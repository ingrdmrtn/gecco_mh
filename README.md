[![arXiv Badge](https://img.shields.io/badge/arXiv-B31B1B?logo=arxiv&logoColor=fff&style=for-the-badge)](https://arxiv.org/abs/2502.00879)

# ğŸ§  GeCCo: Guided Generation of Computational Cognitive Models

Authors: [Milena Rmus](https://github.com/MilenaCCNlab) and [Akshay K. Jagadish](https://akjagadish.github.io/)

## ğŸ“˜ Overview

Guided Generation of Computational Cognitive Models (GeCCo) is a pipeline for automated generation of computational cognitive models using large language models (LLMs).

Given the task instructions, participant data from cognitive tasks, model generation specs and a template function, GeCCo:

1. Prompts an LLM to generate candidate cognitive models as executable Python functions
2. Fits these models offline to the held-out participant data using maximum likelihood estimation (via scipy.optimize)
3. Evaluates the generated model using metrics such as Bayesian Information Criterion (BIC), and uses this performance metric to guide further model generation
4. Refines the generated models over multiple iterations based on structured feedback

![GeCCo Schematic](GeCCo.png)

## ğŸ§© Key Features

- ğŸ§® Task-agnostic design through configurable input data columns
- âš™ï¸ YAML configuration for tasks, data, LLM settings, and evaluation
- ğŸ§± Modular architecture (prompting, fitting, evaluation, feedback)
- ğŸ¤– LLM-driven model generation as interpretable Python functions
- ğŸ“Š Automated fitting with multi-start L-BFGS-B optimization
- ğŸ“ˆ BIC/AIC tracking to identify the best models and iteration results
- ğŸ” Iterative search loop with optional manual or LLM-generated feedback

## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ decision_making.yaml
â”‚   â”œâ”€â”€ schema.py
â”‚   â””â”€â”€ two_step.yaml
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ multi_attribute_decision_making.csv
â”‚   â”œâ”€â”€ rlwm.csv
â”‚   â”œâ”€â”€ standardize_data.py
â”‚   â””â”€â”€ two_step_data.csv
â”œâ”€â”€ gecco/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ run_gecco.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ construct_feedback/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feedback.py
â”‚   â”œâ”€â”€ load_llms/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gpt_backend.py
â”‚   â”‚   â”œâ”€â”€ llama_backend.py
â”‚   â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   â”œâ”€â”€ qwen_backend.py
â”‚   â”‚   â””â”€â”€ r1_backend.py
â”‚   â”œâ”€â”€ offline_evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_structures.py
â”‚   â”‚   â”œâ”€â”€ evaluation_functions.py
â”‚   â”‚   â”œâ”€â”€ fit_generated_models.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ prepare_data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data2text.py
â”‚   â”‚   â””â”€â”€ io.py
â”‚   â””â”€â”€ prompt_builder/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ guardrails.py
â”‚       â””â”€â”€ prompt.py
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ multi_attribute_decision_making/
â”‚   â”‚   â”œâ”€â”€ bics/
â”‚   â”‚   â””â”€â”€ models/
â”‚   â””â”€â”€ two_step_task/
â”‚       â”œâ”€â”€ bics/
â”‚       â””â”€â”€ models/
â””â”€â”€ scripts/
    â”œâ”€â”€ decision_making_demo.py
    â””â”€â”€ two_step_demo.py
```

## ğŸš€ Installation

### Prerequisites

- Python â‰¥ 3.10
- pip or conda

### Install dependencies

```bash
git clone https://github.com/MilenaCCNlab/gecco.git
cd gecco
pip install -r requirements.txt
```

## ğŸ§° Requirements

See `requirements.txt` for full list. Core packages include:

- numpy, pandas, scipy
- torch, transformers
- pyyaml, pydantic
- openai (for OpenAI backend)

Optional (for local LLMs): vllm, accelerate

### API keys

GeCCo reads API keys from environment variables or a `.env` file in the project root (`.env` is gitignored). Create a `.env` file and add whichever keys you need:

```bash
# OpenAI (required if using provider: "openai")
OPENAI_API_KEY=your_openai_api_key_here

# HuggingFace (optional â€” increases rate limits and is required for gated models such as LLaMA)
HF_TOKEN=your_hf_token_here
```

Alternatively, export them in your shell before running:

```bash
export OPENAI_API_KEY=your_openai_api_key_here
export HF_TOKEN=your_hf_token_here
```

A HuggingFace token can be created at huggingface.co/settings/tokens. For gated models (e.g. LLaMA), you must also accept the model licence on the model's HuggingFace page.

### Using local LLMs

GeCCo supports running open-weight models locally via HuggingFace Transformers. The supported providers are:

| Provider value | Models | Example `base_model` |
|----------------|--------|----------------------|
| `llama` | Meta LLaMA family | `meta-llama/Meta-Llama-3.1-70B-Instruct` |
| `qwen` | Alibaba Qwen family | `Qwen/Qwen2-72B-Instruct` |
| `r1` | DeepSeek R1-Distilled | `deepseek-ai/DeepSeek-R1-Distill-Llama-70B` |

To use a local model, set `provider` and `base_model` in your YAML config:

```yaml
llm:
  provider: "llama"
  base_model: "meta-llama/Meta-Llama-3.1-70B-Instruct"
  temperature: 0.2
  max_output_tokens: 2048
```

Models are downloaded from the HuggingFace Hub on first use and loaded with `device_map="auto"` (automatically distributed across available GPUs). bfloat16 precision is used when CUDA is available. You will need:

- A GPU with sufficient VRAM for your chosen model (e.g. ~140 GB for a 70B model in bfloat16, or less with quantisation)
- `torch`, `transformers`, and `accelerate` installed (included in `requirements.txt`)
- For gated models (e.g. LLaMA), log in with `huggingface-cli login` and accept the model license on HuggingFace

**HPC users:** Models are cached in `~/.cache/huggingface/` by default, which may exceed home directory quotas. To use a different location (e.g. a scratch filesystem), set the `HF_HOME` environment variable in your shell or job script before running:

```bash
export HF_HOME=/scratch/$USER/huggingface
python scripts/two_step_demo.py --config config/two_step_local.yaml
```

Note: `HF_HOME` must be set as a shell environment variable â€” putting it in the `.env` file will not work, as HuggingFace reads it at import time before `python-dotenv` loads.

**Lightweight models for testing:** For quick local testing without a large GPU, try a small model such as `Qwen/Qwen2.5-1.5B-Instruct` (~3 GB VRAM) or `meta-llama/Llama-3.2-3B-Instruct` (~6 GB VRAM). Note that model generation quality will be significantly lower than larger models. Qwen models are ungated and can be downloaded without a HuggingFace account or licence agreement, making them the quickest option to get started.

## âš™ï¸ Configuration

All experiment parameters are specified in YAML files under `config/`.

Key sections include:

- `task`: task description and modeling goal for the LLM
- `data`: dataset path/columns and narrative template used for prompting; note that the pipeline currently assumes a trial structure in the data
- `llm`: provider/base model and output constraints/guardrails
- `evaluation`: metric and optimizer options
- `feedback`: feedback mode between iterations (manual or llm-based)
- `loop`: number of model sampling iterations and independent runs

Example (`config/two_step.yaml`):

```yaml
task:
  name: "two_step_task"
  description: "Participants choose between spaceships and interact with aliens for rewards."
  goal: "Propose {models_per_iteration} cognitive models as Python functions: {model_names}"

data:
  path: "data/two_step_data.csv"
  id_column: "participant"
  input_columns: ["choice_1", "state", "choice_2", "reward"]
  data2text_function: "narrative"
  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.
  splits:
    prompt: "[1:3]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "openai"
  base_model: "gpt-4"
  temperature: 0.2
  max_output_tokens: 2048
  models_per_iteration: 3
  include_feedback: true
  guardrails:
    - "Each model must be a standalone Python function"
    - "Function names: cognitive_model1, cognitive_model2, ..."
    - "Return negative log-likelihood of observed choices"
    - "Include clear docstrings with parameter bounds"

evaluation:
  metric: "bic"           # or "aic"
  optimizer: "L-BFGS-B"
  n_starts: 10

feedback:
  type: "manual"          # or "llm"

loop:
  max_iterations: 5
  max_independent_runs: 1

```

## ğŸ¯ Usage

Quick start with demo scripts:

```bash
# Two-step decision task
python scripts/two_step_demo.py

# Multi-attribute decision making
python scripts/decision_making_demo.py
```

Programmatic usage:

```python
from config.schema import load_config
from gecco.prepare_data.io import load_data, split_by_participant
from gecco.prepare_data.data2text import get_data2text_function
from gecco.load_llms.model_loader import load_llm
from gecco.run_gecco import GeCCoModelSearch
from gecco.prompt_builder.prompt import PromptBuilderWrapper

# load config
cfg = load_config("config/two_step.yaml")

# load and prepare data
df = load_data(cfg.data.path, cfg.data.input_columns)
splits = split_by_participant(df, cfg.data.id_column, cfg.data.splits)

# get prompt and eval splits
df_prompt, df_eval = splits["prompt"], splits["eval"]

# convert data to narrative text
data2text = get_data2text_function(cfg.data.data2text_function)
data_text = data2text(
    df_prompt,
    id_col=cfg.data.id_column,
    template=cfg.data.narrative_template,
    value_mappings=getattr(cfg.data, "value_mappings", None),
)

# build prompt
prompt_builder = PromptBuilderWrapper(cfg, data_text)

# load llm
model, tokenizer = load_llm(cfg.llm.provider, cfg.llm.base_model)

# setup GeCCo
search = GeCCoModelSearch(model, tokenizer, cfg, df_eval, prompt_builder)

# run search
best_model, best_bic, best_params = search.run_n_shots(run_idx=0)

# print results: best model code, BIC, params
print("Best Model Code:\n", best_model)
print("Best BIC:", best_bic)
print("Best Parameters:", best_params)
```

## ğŸ§ª How it works

1. Build a structured prompt with task description, example data (as a narrative), guardrails, and optional feedback
2. Generate multiple candidate models per iteration as Python functions
3. Extract function code, parameter names, and bounds
4. Fit each model to each participant via multi-start L-BFGS-B
5. Compute BIC/AIC and track the best model
6. Feed back guidance for the next iteration and repeat

## ğŸ“Š Output

After runs, results are saved under `results/<task_name>/`:

```text
results/two_step_task/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.py
â”‚   â”œâ”€â”€ iter0.py
â”‚   â”œâ”€â”€ iter1.py
â”‚   â””â”€â”€ ...
â””â”€â”€ bics/
    â”œâ”€â”€ iter0.json
    â”œâ”€â”€ iter1.json
    â””â”€â”€ ...
```

Each JSON contains entries like:

```json
[
  {
    "function_name": "cognitive_model1",
    "metric_name": "BIC",
    "metric_value": 245.67,
    "param_names": ["alpha", "beta", "w"],
    "code_file": "results/two_step_task/models/iter0.py"
  }
]
```

## ğŸ¤ Contributing

Contributions are welcome! Please open an issue or pull request.

## ğŸ“„ License

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“š Citation

If you use GeCCo in research, please cite:

```bibtex
@article{rmus2025generating,
  title={Generating Computational Cognitive Models using Large Language Models},
  author={Rmus, Milena and Jagadish, Akshay K. and Mathony, Marvin and Ludwig, Tobias and Schulz, Eric},
  journal={Advances in Neural Information Processing Systems},
  year={2025},
  url={https://arxiv.org/abs/2502.00879},
}
```

For questions or issues, please open a GitHub issue.
