def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, decaying Working Memory (WM) mixture model.

    On each trial, choice probability is a mixture of a softmax over RL Q-values and a
    near-deterministic WM policy. WM weight is reduced when set size is larger (capturing
    capacity limits), and WM contents decay toward uniform over time.

    Parameters (tuple/list):
    - lr: scalar in (0,1], RL learning rate for Q-updates.
    - wm_weight: baseline mixture weight for WM in low load; effective weight is scaled by (3/nS).
    - softmax_beta: inverse temperature (scaled internally by 10) for RL softmax.
    - wm_decay: decay rate of WM contents toward uniform each trial (0=no decay, 1=full reset).
    - lapse: lapse probability (state- and policy-independent uniform choice).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 100  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Handle invalid indices or corrupted trials with lapse-only likelihood and no updates
            if not (0 <= s < nS) or not (0 <= a < nA):
                p_total = lapse * (1.0 / nA) + (1.0 - lapse) * (1.0 / nA)
                log_p += np.log(max(p_total, 1e-12))
                # Global WM decay even if trial corrupted
                w = (1 - wm_decay) * w + wm_decay * w_0
                continue

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM row with very high beta (near one-hot if clear memory)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-limited WM weight scales with set size (reference capacity ~3)
            wm_weight_eff = wm_weight * (3.0 / max(1.0, float(nS)))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Lapse-augmented mixture
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Global decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0
            # - On rewarded trials, store the association deterministically (one-hot with smoothing)
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                # Renormalize to valid probabilities
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size gating and perseveration bias.

    WM contributes strongly in small set size but is gated down in larger set size.
    Adds a within-state perseveration bias that favors repeating the last action chosen
    in that state (often stronger in older adults), applied on top of the RL+WM mixture.

    Parameters (tuple/list):
    - lr: RL learning rate.
    - wm_weight: baseline WM mixture weight in small set size; down-weighted in large set size.
    - softmax_beta: inverse temperature (scaled internally by 10) for RL softmax.
    - pers: perseveration strength (>=0 increases probability of repeating last action in a state).
    - lapse: lapse probability of random choice.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, pers, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 100
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -np.ones(nS, dtype=int)  # track last chosen action per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            if not (0 <= s < nS) or not (0 <= a < nA):
                p_total = lapse * (1.0 / nA) + (1.0 - lapse) * (1.0 / nA)
                log_p += np.log(max(p_total, 1e-12))
                # Decay WM slightly between trials to model maintenance costs
                w = 0.95 * w + 0.05 * w_0
                continue

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Gate WM by set size: strong in small set size, weak in large set size
            wm_gate = 1.0 if nS <= 3 else 0.3
            wm_weight_eff = np.clip(wm_weight * wm_gate, 0.0, 1.0)

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Apply perseveration bias within state by reweighting the chosen action's prob
            if last_action[s] == a:
                # Multiply chosen prob by exp(pers) and renormalize approximately by dividing by Z
                # For likelihood of observed action a, this equals:
                bias_factor = np.exp(pers)
            else:
                bias_factor = 1.0

            # Convert mixture prob of 'a' to biased prob of 'a' under a Luce choice with persev.
            # Approximation: p_biased(a) = [p_mix(a)*exp(pers*I[a==last])] /
            #                              [p_mix(a)*exp(pers*I[a==last]) + (1-p_mix(a))]
            numerator = p_mix * bias_factor
            denominator = numerator + (1.0 - p_mix)
            p_biased = numerator / max(denominator, 1e-12)

            p_total = (1.0 - lapse) * p_biased + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store on reward, mild forgetting otherwise
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Mild drift toward uniform when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Update last action for perseveration on next visit to this state
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + WM with interference (cross-talk) under high load.

    RL uses separate learning rates for positive vs negative outcomes. WM stores rewarded
    associations but suffers from interference across items when set size is large:
    a cross-talk parameter blends each state's WM row toward the average WM across states.

    Parameters (tuple/list):
    - lr_pos: RL learning rate for positive prediction errors (r - Q > 0).
    - lr_neg: RL learning rate for negative prediction errors (r - Q <= 0).
    - wm_weight: baseline WM mixture weight (attenuated by set size via 3/nS).
    - softmax_beta: inverse temperature (scaled internally by 10) for RL softmax.
    - xtalk: WM cross-talk strength in large set size (0=no interference, 1=full averaging).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, xtalk = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 100
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            if not (0 <= s < nS) or not (0 <= a < nA):
                # Assign small uniform likelihood and skip parameter-driven updates
                p_total = 1.0 / nA
                log_p += np.log(max(p_total, 1e-12))
                continue

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity modulation of WM via set size
            wm_weight_eff = np.clip(wm_weight * (3.0 / max(1.0, float(nS))), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            lr = lr_pos if pe > 0 else lr_neg
            q[s, a] += lr * pe

            # WM update with interference:
            # - Encode success deterministically
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

            # - Cross-talk: under higher loads (nS>3), rows blend toward their average
            if nS > 3 and xtalk > 0:
                avg_row = np.mean(w, axis=0, keepdims=True)  # shape (1, nA)
                w = (1.0 - xtalk) * w + xtalk * np.repeat(avg_row, nS, axis=0)
                # Ensure rows remain normalized
                row_sums = np.sum(w, axis=1, keepdims=True)
                w = np.divide(w, row_sums, out=w, where=row_sums > 0)

        blocks_log_p += log_p

    return -blocks_log_p