def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + capacity-limited Working Memory (RLWM) with load-dependent WM availability and lapses.

    Choice policy:
    - Mixture of WM policy and RL softmax policy.
      p(choice) = (1 - xi) * [ w_wm * p_wm(choice) + (1 - w_wm) * p_rl(choice) ] + xi * 1/3
      where w_wm = p_recall * min(1, K_eff / set_size) * 1[state is in WM].

    Working memory:
    - Stores a state-action mapping only when a rewarded association is observed (reward=1).
    - WM has a fixed capacity (K_eff items). When exceeding capacity, the least-recently stored item is evicted.
    - WM retrieval is noisy: if WM is used and contains the state, the stored action is selected with prob (1 - wm_noise),
      and a uniform error among the other actions with prob wm_noise.

    Reinforcement learning:
    - Q-learning with learning rate alpha and softmax inverse temperature beta.

    Parameters (model_parameters):
    - alpha (float): RL learning rate (0..1).
    - beta (float): RL softmax inverse temperature (>0).
    - K_eff (float): Effective WM capacity in number of states (0..6). Internally rounded to nearest int.
    - p_recall (float): Probability of using WM when the state is in WM (0..1).
    - xi (float): Lapse rate; with prob xi choose uniformly at random (0..1).
      Note: wm_noise is set as xi/2 to tie memory slips to lapse tendency so all parameters are used meaningfully.

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, K_eff, p_recall, xi]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, K_eff, p_recall, xi = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    # Memory slip probability tied to lapse to keep parameters compact but all used
    wm_noise = max(0.0, min(1.0, xi / 2.0))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: dictionary state -> stored_action, and recency list to manage eviction
        K = int(np.round(np.clip(K_eff, 0, max(1, nS))))
        wm_map = dict()
        recency = []  # most recent at end

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            # Skip invalid trials
            if s < 0 or s >= nS or a < 0 or a >= nA or r not in (0.0, 1.0):
                continue

            # Compute RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Compute WM policy (only if state is stored)
            wm_has_state = (s in wm_map)
            if wm_has_state:
                a_star = wm_map[s]
                p_wm = np.ones(nA) * (wm_noise / (nA - 1))
                p_wm[a_star] = 1.0 - wm_noise
            else:
                # If no memory for this state, WM policy reduces to uniform (pure guess if accessed)
                p_wm = np.ones(nA) / nA

            # Load-dependent WM availability
            availability = min(1.0, K / max(1, nS))
            w_wm = p_recall * availability * (1.0 if wm_has_state else 0.0)

            # Mixture with lapse
            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_final = (1.0 - xi) * p_mix + xi * (1.0 / nA)

            total_loglik += np.log(max(eps, p_final[a]))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store only when we observe a rewarded mapping
            # If rewarded, we assume the chosen action is correct for this state.
            if r >= 1.0:
                # Insert/update WM
                previously_stored = (s in wm_map)
                wm_map[s] = a
                # Update recency
                if previously_stored:
                    # Move to most recent
                    if s in recency:
                        recency.remove(s)
                    recency.append(s)
                else:
                    recency.append(s)
                    # Evict least recent if over capacity
                    while len(recency) > K:
                        s_old = recency.pop(0)
                        if s_old in wm_map:
                            del wm_map[s_old]
            else:
                # If not rewarded, we do not change WM (leave mapping as is)
                pass

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Q-learning with uncertainty bonus, load-dependent exploration, and a global choice kernel.

    Choice policy:
    - Softmax over augmented values: V[s,a] = Q[s,a] + kappa * U[s,a] + k_kernel * K[a]
      where U[s,a] = 1 / (1 + n_visits[s,a]) captures uncertainty (novel options are boosted).
      The inverse temperature is modulated by set size: beta_eff = beta0 / (1 + gamma_load * (nS - 3)/3).

    Learning:
    - Standard Q-learning with learning rate alpha.

    Choice kernel:
    - Global action kernel K over actions encourages repeating recently chosen actions irrespective of state.
    - After each valid choice, K decays by (1 - tau_kernel) and adds 1 to the chosen action dimension.
      The kernel is incorporated additively into the logits via k_kernel = tau_kernel to ensure parameter usage.

    Parameters (model_parameters):
    - alpha (float): Learning rate (0..1).
    - beta0 (float): Base inverse temperature (>0).
    - kappa (float): Weight of the uncertainty bonus (>=0).
    - gamma_load (float): Degree to which higher set size reduces beta (>=0).
    - tau_kernel (float): Kernel decay/addition strength (0..1). Also sets kernel gain k_kernel=tau_kernel.

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta0, kappa, gamma_load, tau_kernel]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta0, kappa, gamma_load, tau_kernel = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL values and visitation counts
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for uncertainty

        # Global choice kernel
        K = np.zeros(nA)
        k_kernel = tau_kernel  # tie kernel gain to tau to ensure parameter is used twice consistently

        # Load-modulated temperature
        load_factor = 1.0 + gamma_load * max(0.0, (nS - 3) / 3.0)
        beta_eff = beta0 / max(eps, load_factor)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r not in (0.0, 1.0):
                continue

            # Uncertainty bonus
            U = 1.0 / (1.0 + N[s, :])

            # Assemble logits
            V = Q[s, :] + kappa * U + k_kernel * K
            logits = beta_eff * (V - np.max(V))
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            total_loglik += np.log(max(eps, p[a]))

            # Learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update counts, kernel
            N[s, a] += 1.0

            # Decay and reinforce kernel
            K *= (1.0 - tau_kernel)
            K[a] += 1.0

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL with visit-lag-dependent forgetting and load-scaled lapses.

    Choice policy:
    - Softmax over Q-values with inverse temperature beta.
    - Lapse probability increases with set size: p_lapse = lapse_base * (nS / 3).
      Final choice prob: p_final = (1 - p_lapse) * softmax(Q) + p_lapse * 1/3.

    Learning:
    - Asymmetric learning rates: alpha_pos for rewarded outcomes, alpha_neg for non-reward.
    - Visit-lag forgetting: when revisiting a state after lag L trials within the block,
      all Q values of that state are multiplied by (1 - phi) ** L before learning.

    Parameters (model_parameters):
    - alpha_pos (float): Learning rate for positive outcomes (0..1).
    - alpha_neg (float): Learning rate for negative outcomes (0..1).
    - beta (float): Inverse temperature (>0).
    - phi (float): Per-trial forgetting rate applied per lag step (0..1).
    - lapse_base (float): Baseline lapse factor for set size 3; scales linearly with set size (0..1).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha_pos, alpha_neg, beta, phi, lapse_base]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, phi, lapse_base = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        # Track last visit time for each state to compute lag forgetting
        last_visit = -1 * np.ones(nS, dtype=int)

        # Load-scaled lapse
        p_lapse = lapse_base * max(1.0, nS / 3.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r not in (0.0, 1.0):
                continue

            # Apply lag-dependent forgetting upon visiting state s
            if last_visit[s] >= 0:
                lag = t - last_visit[s]
                decay = (1.0 - phi) ** max(0, lag)
                Q[s, :] *= decay

            # Softmax policy with lapse
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)
            p_final = (1.0 - p_lapse) * p + p_lapse * (1.0 / nA)

            total_loglik += np.log(max(eps, p_final[a]))

            # Learning with asymmetric alphas
            alpha = alpha_pos if r >= 1.0 else alpha_neg
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update last visit time
            last_visit[s] = t

    return -float(total_loglik)