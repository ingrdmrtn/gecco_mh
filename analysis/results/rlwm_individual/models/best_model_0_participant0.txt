def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with chunking interference across states and load-dependent drift.

    Mechanism:
    - RL: standard delta-rule with softmax (beta scaled by 10).
    - WM store: per-state action distribution w[s,:] approximating best action.
      On reward, the chosen action is consolidated; on non-reward, a mild
      redistribution occurs away from the chosen action.
    - Chunking interference: under higher load, WM for a state s is contaminated
      by the average of other states' WM (simulating imperfect individuation).
      This is controlled by chunking parameter and increases with set size.
    - Drift: global decay toward uniform, stronger under high load.
    - Policy: mixture of RL and WM with fixed wm_weight.

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_weight: [0,1] mixture weight of WM in the policy.
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - chunking: [0,1], degree that WM rows mix with the across-state mean under load.
    - decay: [0,1], base drift of WM toward uniform each trial.
    - noise_wm: >=0, adds temperature to WM policy (lower precision with larger value).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, chunking, decay, noise_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base WM determinism before noise
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load = float(nS - 1) / max(1.0, float(nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :].copy()
            mean_other = np.mean(w, axis=0) if nS <= 1 else (np.sum(w, axis=0) - W_s) / max(1, nS - 1)
            ch = np.clip(chunking * load, 0.0, 1.0)
            W_eff = (1.0 - ch) * W_s + ch * mean_other

            beta_wm_eff = softmax_beta_wm / (1.0 + noise_wm)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_eff - W_eff[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            d = np.clip(decay * (0.5 + 0.5 * load), 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:

                w[s, :] = 0.8 * w[s, :] + 0.2 * one_hot
            else:

                reduce = 0.15
                w[s, a] = (1.0 - reduce) * w[s, a]
                redistribute = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = w[s, aa] + redistribute

            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p