def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with stay bias + WM with load-dependent confusion and delta learning

    Description:
    - RL: tabular Q-learning with a perseveration (stay) bias that adds a bonus to the
      last action taken in that state when computing the RL softmax.
    - WM: learns via a delta rule toward a target that reflects reward-contingent storage
      with load-dependent "confusion" (probability of mis-binding to other actions).
      On reward, WM targets a distribution that assigns (1 - conf) to the chosen action
      and conf spread uniformly over the non-chosen actions; on no-reward, it targets
      uniform (forgetting).
    - Confusion increases with set size: conf_eff = 1 - (1 - confusion_rate)^nS.
    - Mixture: fixed mixture weight between WM and RL policies (wm_weight).

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight for WM policy (0..1)
    - softmax_beta: RL inverse temperature; internally scaled x10
    - stay_bias: Additive bias applied to the last action tried in a state in the RL policy
    - wm_lr: WM learning rate for delta update toward the target (0..1)
    - confusion_rate: Base confusion rate per item; load compounds it (0..1)
    """
    lr, wm_weight, softmax_beta, stay_bias, wm_lr, confusion_rate = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        conf_eff = 1.0 - (1.0 - np.clip(confusion_rate, 0.0, 1.0)) ** max(1, nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            if last_action[s] >= 0:
                Q_s[last_action[s]] += stay_bias
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            q[s, a] += lr * (r - q[s, a])

            if r > 0.5:
                target = np.ones(nA) * (conf_eff / (nA - 1.0))
                target[a] = 1.0 - conf_eff
            else:
                target = w_0[s, :].copy()  # forget toward uniform
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p