def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM arbitration via prediction error and set-size-dependent WM lapses.

    This model mixes a standard RL policy with a WM policy. WM reliability suffers more
    under higher set sizes (modeled as a lapse that blends WM with a uniform policy).
    Arbitration between RL and WM depends on the magnitude of the RL prediction error (|PE|):
    when PE is small (high certainty), the model leans more on WM; when PE is large (surprise),
    it leans more on RL. RL has asymmetric learning rates for positive and negative outcomes.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr_pos : float
            RL learning rate after rewards (0-1).
        lr_neg : float
            RL learning rate after non-rewards (0-1).
        wm_weight0 : float
            Base mixture weight on WM before arbitration (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        lapse0 : float
            WM lapse in high set size (nS=6). Lapse is 0 at nS=3 and scales linearly up to lapse0.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight0, softmax_beta, lapse0 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        if nS <= 3:
            lapse = 0.0
        else:
            lapse = np.clip(lapse0 * (float(nS - 3) / max(1.0, 6.0 - 3.0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_uniform = 1.0 / nA
            p_wm = (1.0 - lapse) * p_wm_core + lapse * p_uniform

            pe = r - Q_s[a]
            eff_w = np.clip(wm_weight0 * (1.0 - abs(pe)), 0.0, 1.0)

            p_total = eff_w * p_wm + (1.0 - eff_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            if r > Q_s[a]:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            if r > 0.0:

                alpha_pos = 1.0
                w[s, :] = (1.0 - alpha_pos) * w[s, :]
                w[s, a] += alpha_pos
            else:

                alpha_neg = 0.2
                w[s, a] = max(eps, (1.0 - alpha_neg) * w[s, a])

                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p