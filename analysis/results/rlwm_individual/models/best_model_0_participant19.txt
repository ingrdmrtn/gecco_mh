def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-dependent binding interference.

    Idea:
    - RL learns state-action values with a softmax policy.
    - WM stores recent rewarded bindings as probability vectors over actions per state.
    - Larger set sizes increase binding interference (swap-like), diffusing WM contents toward the
      average of other states; and reduce WM arbitration weight.
    - WM policy is near-deterministic (high beta) but sharpened by wm_beta.

    Parameters (6 total):
    - lr: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_weight_base: baseline arbitration weight of WM vs RL at set size 3 (0..1)
    - wm_learn: WM learning rate toward the rewarded action (0..1)
    - binding_noise_base: base interference level; binding_noise scales up with set size (0..1)
    - wm_beta: sharpness multiplier for WM softmax (>=0), higher makes WM more deterministic

    Set-size impact:
    - wm_weight = wm_weight_base * (3 / nS), so larger nS down-weights WM.
    - binding_noise = binding_noise_base * (nS - 3) / 3 increases swap-like diffusion with larger nS.
    """
    lr, softmax_beta, wm_weight_base, wm_learn, binding_noise_base, wm_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base determinism for WM; further scaled by wm_beta
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight = np.clip(wm_weight_base * (3.0 / max(1, nS)), 0.0, 1.0)
        bind = np.clip(binding_noise_base * max(0, nS - 3) / 3.0, 0.0, 1.0)
        beta_wm_eff = softmax_beta_wm * max(0.0, wm_beta)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))


            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta


            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * target
            else:

                down = wm_learn * 0.5
                w[s, a] = max(0.0, w[s, a] * (1 - down))

                w[s, :] = w[s, :] / np.sum(w[s, :])

            if nS > 1:
                mean_other = (np.sum(w, axis=0) - w[s, :]) / (nS - 1)
                w[s, :] = (1 - bind) * w[s, :] + bind * mean_other

            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p