def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Precision-limited WM with set-size–dependent retrieval lapses.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM retrieval is less precise and more failure-prone as set size increases:
      - Effective WM precision beta_wm_eff scales with (3 / nS).
      - WM retrieval lapse lambda_wm increases with set size.
    - WM stores the currently chosen action with a reward-scaled bump, and decays toward a uniform prior.

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate; also used as the base decay step for WM.
        wm_weight : float in [0,1]
            Mixture weight for WM in the final policy. Also shapes WM lapse (higher => more lapses in large sets)
            and WM learning strength.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10). WM precision is fixed high but scaled by set size.

    Set-size impact
    ---------------
    - WM precision decreases with set size: beta_wm_eff = 50 * (3/nS).
    - WM retrieval lapse increases with set size: lambda_wm = wm_weight * max(0, (nS-3)/nS).
    - WM update uses reward-scaled increments and set-size–dependent decay to prior.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))


            beta_wm_eff = softmax_beta_wm * (3.0 / max(1.0, float(nS)))

            lambda_wm = wm_weight * max(0.0, (float(nS) - 3.0) / max(1.0, float(nS)))

            p_wm_soft = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_wm = (1.0 - lambda_wm) * p_wm_soft + lambda_wm * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta


            decay = lr * (1.0 + max(0.0, (float(nS)-3.0)/max(1.0, float(nS))) * wm_weight)
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            w[s, a] += wm_weight * (0.5 + 0.5 * r)

        blocks_log_p += log_p

    return -blocks_log_p