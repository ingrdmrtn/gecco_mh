def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty- and load-modulated temperature + WM familiarity-gated arbitration.

    Policy:
    - RL system: softmax over Q with an effective temperature that increases when RL is confident
      (low entropy) and when set size is small.
    - WM system: softmax over W with high determinism (fixed 50).
    - Arbitration: WM weight increases with WM familiarity in the current state.

    RL mechanisms:
    - Standard delta rule with learning rate lr.
    - Effective inverse temperature:
        beta_eff = softmax_beta*10 * (1 + beta_uncertainty_gain * (H_max - H_rl)/H_max)
                   * (3/nS)^{beta_load_gain}
      where H_rl is the entropy of the RL policy in state s using the base temperature.

    WM mechanisms:
    - Reward strengthens the chosen action toward 1 and suppresses others in the visited state.
    - No reward drifts WM toward uniform in the visited state.
    - Familiarity of WM in a state is m_s = max(W_s) - mean(W_s); arbitration weight is
      wm_weight_eff = clip(wm_weight_base + wm_fam_gain * m_s, 0..1).

    Set-size effects:
    - RL temperature scales with set size via beta_load_gain, making RL more/less deterministic.
    - WM arbitration increases with stronger WM familiarity, which typically grows faster in smaller sets.

    Parameters:
    - lr: RL learning rate (0..1).
    - softmax_beta: Base RL inverse temperature (rescaled by 10).
    - wm_weight_base: Base WM arbitration weight (0..1).
    - beta_uncertainty_gain: Gain on RL determinism as RL uncertainty decreases (>=0).
    - beta_load_gain: Exponent controlling how beta scales with set size (can be positive or negative).
    - wm_fam_gain: Gain mapping WM familiarity to arbitration weight.
    """
    lr, softmax_beta, wm_weight_base, beta_uncertainty_gain, beta_load_gain, wm_fam_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        H_max = np.log(nA)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            logits_base = softmax_beta * (Q_s - np.max(Q_s))
            pi_base = np.exp(logits_base)
            pi_base /= np.sum(pi_base)
            H_rl = -np.sum(pi_base * (np.log(pi_base + 1e-12)))

            beta_conf = 1.0 + float(beta_uncertainty_gain) * (H_max - H_rl) / max(H_max, 1e-12)
            beta_load = (3.0 / nS) ** float(beta_load_gain)
            beta_eff = softmax_beta * beta_conf * beta_load

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            p_wm_row = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            m_s = np.max(W_s) - np.mean(W_s)
            wm_weight_eff = np.clip(float(wm_weight_base) + float(wm_fam_gain) * m_s, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm_row + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            q[s, a] += lr * pe

            if r > 0.0:

                gain = lr
                w[s, a] += gain * (1.0 - w[s, a])
                for a_other in range(nA):
                    if a_other != a:
                        w[s, a_other] += -gain * w[s, a_other] / (nA - 1)
            else:

                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p