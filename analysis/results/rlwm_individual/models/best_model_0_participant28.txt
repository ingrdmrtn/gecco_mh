def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–dependent swap errors and RL temperature load-sensitivity.

    Idea:
    - RL: standard delta rule, but the effective inverse temperature decreases as set size increases
      (more load -> more exploration).
    - WM: one-shot storage of the last rewarded action per state. At retrieval, with a set-size–
      dependent swap probability, WM confuses the target state with other states and averages their
      WM-based policies.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally; then divided by
      (1 + beta_load_slope*(nS-3)) at decision time.
    - swap_base: Controls how swap error grows with set size; swap = 1 - exp(-swap_base*(nS-1)).
    - beta_load_slope: Sensitivity of RL temperature to set size (>=0 makes larger sets more exploratory).

    Set-size impacts:
    - WM: swap probability increases with set size, diluting state-specific WM information.
    - RL: inverse temperature is reduced as set size increases, producing noisier RL choices.
    """
    lr, wm_weight, softmax_beta, swap_base, beta_load_slope = model_parameters
    softmax_beta *= 10  # base scaling; will be downscaled by load inside the block
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        swap = 1.0 - np.exp(-swap_base * max(0.0, float(nS) - 1.0))
        beta_base = softmax_beta

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            beta_eff = beta_base / (1.0 + beta_load_slope * (float(nS) - 3.0))
            beta_eff = max(1e-6, beta_eff)

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))


            W_s = w[s, :].copy()
            p_self = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            if nS > 1:
                p_list = []
                for j in range(nS):
                    if j == s:
                        continue
                    W_j = w[j, :].copy()
                    p_j = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_j - W_j[a])))
                    p_list.append(p_j)
                p_others = np.mean(p_list) if len(p_list) > 0 else (1.0 / nA)
            else:
                p_others = 1.0 / nA

            p_wm = (1.0 - swap) * p_self + swap * p_others

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p