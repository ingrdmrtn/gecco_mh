def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + recall-based WM with lapses and asymmetric RL learning.
    - RL: softmax choice with separate learning rates for positive and negative prediction errors.
    - WM: item-based store of last rewarded action per state.
      Retrieval success decreases with set size; lapse probability adds noise.
      WM policy is a convex combination of uniform and a one-hot policy on the stored action.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr_pos : float in (0,1)
            RL learning rate for positive prediction errors.
        wm_weight : float in [0,1]
            Base mixture weight on WM policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled up internally).
        lr_neg : float in (0,1)
            RL learning rate for negative prediction errors.
        wm_lapse : float in [0,1]
            Lapse probability in WM retrieval (adds uniform noise).
        recall_slope : float >= 0
            Linear decrement of WM recall from set size 3 to 6:
            recall_prob = max(0, 1 - recall_slope*(set_size - 3)).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr_pos, wm_weight, softmax_beta = model_parameters[:3]
    lr_neg = model_parameters[3] if len(model_parameters) > 3 else lr_pos
    wm_lapse = model_parameters[4] if len(model_parameters) > 4 else 0.1
    recall_slope = model_parameters[5] if len(model_parameters) > 5 else 0.3

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # kept to follow template; not directly used for storage
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        has_mem = np.zeros(nS, dtype=bool)
        mem_act = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            set_size_t = int(block_set_sizes[t])
            recall_prob = max(0.0, 1.0 - recall_slope * (set_size_t - 3))
            eff_recall = recall_prob * (1.0 - wm_lapse)

            if has_mem[s]:
                stored_a = int(mem_act[s])
                p_wm_vec = (1.0 - eff_recall) * w_0[s, :]  # uniform noise
                p_wm_vec[stored_a] += eff_recall

                p_wm = p_wm_vec[a]
            else:

                p_wm = w_0[s, a]

            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            if r >= 0.5:
                has_mem[s] = True
                mem_act[s] = a
            else:

                if has_mem[s] and mem_act[s] == a:
                    has_mem[s] = False  # forget incorrect association

        blocks_log_p += log_p

    return -blocks_log_p