def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian-like WM counts with interference and choice perseveration bias.

    Description:
    - RL: single learning rate; softmax policy.
    - WM: maintains Dirichlet-like counts per state (mapped to probabilities) that are decayed toward a symmetric prior.
    - Interference increases WM decay with larger set sizes: wm_decay_eff = wm_decay_base * (nS/3).
    - Perseveration bias: WM policy is biased toward the last action taken in a state.
    - Mixture of WM and RL policies.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values (0..1).
    - model_parameters[1] = wm_weight (float): Base WM mixture weight (0..1).
    - model_parameters[2] = softmax_beta (float): Inverse temperature for RL softmax; internally x10.
    - model_parameters[3] = wm_decay_base (float): Base decay rate of WM counts toward prior (0..1), scaled by nS/3.
    - model_parameters[4] = wm_alpha (float): Symmetric prior concentration for WM counts (>0).
    - model_parameters[5] = pers_bias (float): Additive bias for repeating the last action in a state (>=0).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_alpha, pers_bias = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))

        counts = wm_alpha * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # will hold normalized counts each trial for policy
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_action = -1 * np.ones(nS, dtype=int)

        wm_decay_eff = wm_decay_base * (float(nS) / 3.0)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            c_s = counts[s, :].copy()
            c_sum = np.sum(c_s)
            if c_sum <= 0:
                W_s = w_0[s, :].copy()
            else:
                W_s = c_s / c_sum

            W_logits = W_s.copy()
            if last_action[s] >= 0:
                W_logits[last_action[s]] += pers_bias


            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_logits - W_logits[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            counts[s, :] = (1.0 - wm_decay_eff) * counts[s, :] + wm_decay_eff * wm_alpha

            if r > 0:

                counts[s, a] += 1.0
            else:


                penal = min(0.2, counts[s, a] - 1e-6)
                counts[s, a] -= penal

            last_action[s] = a

            c_sum = counts[s, :].sum()
            if c_sum > 0:
                w[s, :] = counts[s, :] / c_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p