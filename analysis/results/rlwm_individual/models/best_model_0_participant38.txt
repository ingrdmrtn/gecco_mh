def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with asymmetric RL learning rates and set-size-dependent WM interference.
    
    Model summary:
    - Two RL learning rates: lr_pos for rewarded updates, lr_neg for non-rewarded updates.
    - WM is a fast store of rewarded state-action pairs, with decay increased under higher set size.
    - WM mixture weight decreases with set size and more strongly for older adults.
    - Small, fixed age effect reduces RL learning rates and WM weight for older adults.
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight_base: base WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 inside.
    - wm_decay_base: base WM decay toward uniform (0..1).
    - interference_gain: how much WM decay increases from set size 3 to 6.
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, wm_decay_base, interference_gain = model_parameters
    lr_pos = np.clip(lr_pos, 0.0, 1.0)
    lr_neg = np.clip(lr_neg, 0.0, 1.0)
    wm_weight_base = np.clip(wm_weight_base, 0.0, 1.0)
    wm_decay_base = np.clip(wm_decay_base, 0.0, 1.0)
    interference_gain = np.clip(interference_gain, 0.0, 1.0)  # positive increase in decay

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= (1.0 - 0.15 * age_group)
    lr_pos *= (1.0 - 0.2 * age_group)
    lr_neg *= (1.0 - 0.2 * age_group)

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))


        f_set = 1.0 if nS <= 3 else 0.5
        wm_weight_eff = np.clip(wm_weight_base * f_set * (1.0 - 0.4 * age_group), 0.0, 1.0)

        size_factor = 0.0 if nS <= 3 else 1.0
        wm_decay = np.clip(wm_decay_base + interference_gain * size_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - q[s, a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p