def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-adaptive temperature + WM associative counts with set-size misbinding.

    Mechanism
    - RL: delta-rule; inverse temperature is adapted by state-wise uncertainty (variance proxy).
      Higher uncertainty -> lower beta; certainty -> higher beta. Starts from softmax_beta0 and increases with certainty.
    - WM: per-state Dirichlet-like counts over actions (associative memory).
      Positive outcomes increment the chosen action's count; non-rewards increment all actions slightly.
      Misbinding noise proportional to set size spreads a fraction of counts to other states.
    - Mixture: WM and RL are mixed via wm_weight.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float in [0,1]
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy.
        - softmax_beta0: float
            Baseline inverse temperature for RL softmax (scaled internally by 10).
        - beta_gain: float >= 0
            Gain controlling how much certainty increases RL inverse temperature.
        - misbind_rate: float in [0,1]
            Fraction of WM update that is misbound (leaked) to other states, scaled by set size.
        - wm_lr: float in [0,1]
            Step size for WM count updates toward observed outcomes.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta0, beta_gain, misbind_rate, wm_lr = model_parameters
    softmax_beta0 *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))

        counts = np.ones((nS, nA))


        uncert = np.ones(nS)  # higher -> more uncertain

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            certainty = 1.0 / (1.0 + uncert[s])
            softmax_beta = softmax_beta0 * (1.0 + beta_gain * certainty)

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            wm_probs = counts[s, :] / max(np.sum(counts[s, :]), eps)
            prefs_wm = wm_probs - np.mean(wm_probs)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_wm, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            q[s, a] += lr * pe

            uncert[s] = np.clip(0.9 * uncert[s] + 0.1 * (pe ** 2), 0.0, 10.0)


            leak = np.clip(misbind_rate * ((nS - 1) / max(1, nS)), 0.0, 1.0)
            on_target = 1.0 - leak

            incr = np.zeros(nA)
            if r > 0:
                incr[a] = 1.0
            else:
                incr += 0.2  # weak increment for all after non-reward

            counts[s, :] = (1.0 - wm_lr) * counts[s, :] + wm_lr * ((counts[s, :] + on_target * incr))

            if leak > 0 and nS > 1:
                spread = (wm_lr * leak) * incr
                if spread.sum() > 0:
                    per_state = spread / (nS - 1)
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        counts[s2, :] += per_state

            counts = np.clip(counts, 1e-6, 1e6)

        blocks_log_p += log_p

    return -blocks_log_p