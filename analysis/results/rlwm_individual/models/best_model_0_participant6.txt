def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (valence-asymmetric) + WM with prediction-error-based gating and load-dependent WM forgetting.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM contributes a near-deterministic policy from remembered associations.
    - Arbitration weight for WM is down-regulated when current unsigned prediction error is large
      (trust RL exploration when surprised).
    - WM forgetting increases with set size (higher load -> faster decay).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_weight: baseline WM mixture weight (0..1).
    - pe_sensitivity: scales how strongly unsigned PE suppresses WM weight (>0).
    - wm_forgetting: base WM forgetting rate (0..1) that is further increased by load.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, pe_sensitivity, wm_forgetting = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        load_scale = max(1.0, nS / 3.0)
        wm_forgetting_eff = np.clip(wm_forgetting * load_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            pe = r - Q_s[a]
            wm_weight_eff = wm_weight * np.exp(-pe_sensitivity * np.abs(pe))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            if pe >= 0:
                q[s][a] += lr_pos * pe
            else:
                q[s][a] += lr_neg * pe

            w = (1 - wm_forgetting_eff) * w + wm_forgetting_eff * w_0
            if r > 0:

                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p