Below are three standalone cognitive models for the RLWM task. Each function follows the provided template structure, returns the negative log-likelihood of the observed choices, and uses all supplied parameters. Working memory contributions are modulated by set size, and models differ in how WM stores/uses information and how load affects WM.

Note: Assumes numpy as np is already imported. Each function takes: states, actions, rewards, blocks, set_sizes, model_parameters.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited, decaying WM mixture.
    
    Policy:
    - Mixture of RL softmax and WM softmax.
    - WM has a capacity K (effective WM weight scales as wm_weight * min(1, K/nS))
      and decays toward uniform with rate wm_decay each trial.
    - WM stores rewarded associations as near one-hot for the state-action that yielded reward.
    
    Parameters:
    - model_parameters[0]: lr (float) RL learning rate for Q-values.
    - model_parameters[1]: wm_weight (float in [0,1]) Base mixing weight of WM policy.
    - model_parameters[2]: softmax_beta (float) Inverse temperature for RL (internally scaled by 10).
    - model_parameters[3]: K (float) WM capacity; reduces WM influence when set size nS > K.
    - model_parameters[4]: wm_decay (float in [0,1]) Per-trial decay of WM toward uniform baseline.
    
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight_base, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0  # RL beta has a higher upper bound
    softmax_beta_wm = 50.0  # WM assumed to be very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables initialized to uniform
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM baseline for decay

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective WM weight modulated by capacity and set size
            wm_weight = wm_weight_base * min(1.0, K / max(1.0, nS))

            # RL policy: probability of chosen action via softmax
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights for the state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))  # numerical guard
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward baseline (applies to all states each trial)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: store rewarded association (make near one-hot if rewarded),
            # and slightly suppress chosen action on non-reward
            if r > 0.5:
                # Move WM row toward one-hot on action a
                w[s, :] = (1e-6)  # small floor
                w[s, a] = 1.0 - (nA - 1) * 1e-6
            else:
                # Penalize the chosen action a a bit on non-reward
                penalty = 0.1
                give = min(w[s, a], penalty)
                w[s, a] -= give
                redistribute = give / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM with load-induced interference + lapse.
    
    Policy:
    - Mixture of RL softmax and WM softmax.
    - RL has separate learning rates for positive and negative prediction errors.
    - WM suffers interference that scales with set size (nS), mixing each state's WM with the
      average WM of other states; this reduces distinctiveness under high load.
    - After mixture, a stimulus-independent lapse epsilon shifts probability toward uniform.
    
    Parameters:
    - model_parameters[0]: lr_pos (float) RL learning rate for positive PE.
    - model_parameters[1]: lr_neg (float) RL learning rate for negative PE.
    - model_parameters[2]: wm_weight (float in [0,1]) Base mixing weight of WM policy.
    - model_parameters[3]: softmax_beta (float) Inverse temperature for RL (internally scaled by 10).
    - model_parameters[4]: epsilon (float in [0,1]) Lapse probability to uniform responding.
    - model_parameters[5]: interference (float >= 0) WM interference strength with set size.
    
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, epsilon, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective WM weight reduced by load-induced interference
            # Interference proportion grows with set size; bounded to [0,1]
            inter_prop = min(1.0, interference * (max(0, nS - 1) / max(1.0, nS)))
            wm_weight = wm_weight_base * (1.0 - inter_prop)

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM interference: blend the current state's WM with the mean WM of other states
            if nS > 1:
                others = [i for i in range(nS) if i != s]
                mean_others = np.mean(w[others, :], axis=0)
                W_s_vec = (1.0 - inter_prop) * w[s, :] + inter_prop * mean_others
            else:
                W_s_vec = w[s, :]

            # WM policy after interference
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_vec - W_s_vec[a])))

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl

            # Lapse to uniform
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update: if rewarded, store a near one-hot; else small relaxation to baseline
            if r > 0.5:
                w[s, :] = (1e-6)
                w[s, a] = 1.0 - (nA - 1) * 1e-6
            else:
                relax = 0.05
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + WM as win-stay/lose-shift with capacity limit.
    
    Policy:
    - Mixture of RL softmax and a WM policy that implements win-stay/lose-shift:
      if the last outcome for the state was a reward, WM favors repeating that action;
      if it was a loss, WM shifts away from that action with strength lose_shift_bias.
    - WM availability is capacity-limited by K (effective WM weight scales with min(1, K/nS)).
    - RL includes forgetting toward uniform values to capture interference across larger sets.
    
    Parameters:
    - model_parameters[0]: lr (float) RL learning rate.
    - model_parameters[1]: wm_weight (float in [0,1]) Base WM mixture weight.
    - model_parameters[2]: softmax_beta (float) Inverse temperature for RL (internally scaled by 10).
    - model_parameters[3]: K (float) WM capacity; reduces WM influence when set size nS > K.
    - model_parameters[4]: lose_shift_bias (float in [0,1]) Strength of shifting away from last action after loss.
    - model_parameters[5]: forget (float in [0,1]) RL forgetting toward uniform each trial.
    
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight_base, softmax_beta, K, lose_shift_bias, forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # Will hold WM policy distributions per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # Not directly used but maintains template structure

        # Initialize WM memory: unknown at start (uniform)
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1.0 * np.ones(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective WM weight modulated by capacity and set size
            wm_weight = wm_weight_base * min(1.0, K / max(1.0, nS))

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Construct WM policy distribution for this state based on win-stay/lose-shift
            if last_action[s] >= 0 and last_reward[s] >= 0:
                if last_reward[s] > 0.5:
                    # Win-stay: deterministic repeat
                    w_s = np.full(nA, 1e-6)
                    w_s[last_action[s]] = 1.0 - (nA - 1) * 1e-6
                else:
                    # Lose-shift: reduce probability of last action, distribute to others
                    w_s = np.ones(nA) / nA
                    shift = lose_shift_bias
                    take = min(shift, w_s[last_action[s]])
                    w_s[last_action[s]] -= take
                    add = take / (nA - 1)
                    for aa in range(nA):
                        if aa != last_action[s]:
                            w_s[aa] += add
                w[s, :] = w_s
            else:
                # No prior info: uniform
                w[s, :] = np.ones(nA) / nA

            # WM softmax policy for chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply forgetting to the state's Q-values (toward uniform)
            q[s, :] = (1.0 - forget) * q[s, :] + forget * (1.0 / nA)

            # Update WM memory trace for win-stay/lose-shift
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p