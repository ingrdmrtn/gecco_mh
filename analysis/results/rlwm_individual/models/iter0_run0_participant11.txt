def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited working memory mixture model.

    Idea:
    - Decisions are a mixture of RL and WM policies.
    - WM is slot-limited: its contribution scales down as set size exceeds capacity K.
    - WM stores rewarded mappings as near-deterministic one-hots with decay toward uniform.
    - RL updates with a single learning rate.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1), further scaled by set size relative to capacity K
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - K: WM capacity (in number of state-action mappings)
    - wm_alpha: WM encoding strength when rewarded (0..1)
    - wm_decay: WM decay rate toward uniform each trial (0..1)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial
    - Returns: negative log-likelihood of choices under the model
    """
    lr, wm_weight, softmax_beta, K, wm_alpha, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of observed action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM effective weight scaled by capacity K relative to set size
            capacity_scale = min(1.0, max(0.0, K / float(nS)))  # 0..1
            wm_weight_eff = wm_weight * capacity_scale

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on reward: pull toward one-hot at rewarded action
            if r > 0.5:
                # shrink current vector, then add mass to chosen action
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha

            # renormalize to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency-based WM with load-sensitive gating and perseveration.

    Idea:
    - Policy is a mixture of RL and a WM that emphasizes the most recent action in a state,
      irrespective of reward (fast, fragile recency memory).
    - WM influence is gated down as set size increases (logistic function of set size).
    - RL includes a perseveration bias: tendency to repeat the last action in each state.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1), then load-gated
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - gate_slope: Slope of logistic load gating; larger -> stronger reduction at larger set sizes
    - persev: Perseveration bias added to the Q-value of last chosen action in a state
    - wm_decay: WM decay rate toward uniform each trial (0..1)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial
    - Returns: negative log-likelihood of choices under the model
    """
    lr, wm_weight, softmax_beta, gate_slope, persev, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # track last chosen action per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bias to RL values for last chosen action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += persev

            # RL probability of observed action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Load-sensitive WM gating: logistic decrease as set size increases
            # Center around 4.5 to differentiate 3 vs 6 conditions
            gate = 1.0 / (1.0 + np.exp(gate_slope * (float(nS) - 4.5)))
            wm_weight_eff = wm_weight * gate

            # WM recency: emphasize most recent chosen action in this state
            # Decay toward uniform, then inject a recency bump for the current last action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if last_action[s] >= 0:
                # Make WM policy sharply prefer the last action
                rec = np.zeros(nA)
                rec[last_action[s]] = 1.0
                # Blend current WM with a strong one-hot toward last action
                alpha_rec = 1.0  # determinism controlled by softmax_beta_wm; blending via simple overwrite
                w[s, :] = (1.0 - alpha_rec) * w[s, :] + alpha_rec * rec

            # WM policy softmax
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update last action memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Adaptive arbitration by RL uncertainty + WM encoding of correct responses, with asymmetric RL learning.

    Idea:
    - RL updates with separate learning rates for positive vs negative outcomes.
    - WM stores rewarded mappings (one-hot) with encoding strength wm_alpha.
    - Arbitration weight for WM increases when RL is uncertain (high entropy) and decreases with higher set size.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewards (0..1)
    - lr_neg: RL learning rate for non-rewards (0..1)
    - wm_weight: Base WM weight, scaled by RL uncertainty and set size
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - wm_alpha: WM encoding/decay step size (0..1)
    - lapse: Lapse probability mixed uniformly across actions (0..1)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial
    - Returns: negative log-likelihood of choices under the model
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_alpha, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL uncertainty (entropy of softmax over Q with temperature 1)
            # Compute policy pi over actions
            logits = Q_s - np.max(Q_s)
            exps = np.exp(logits)
            pi = exps / np.sum(exps)
            entropy = -np.sum(pi * (np.log(pi + eps)))  # 0..log(nA)
            entropy_norm = entropy / np.log(nA)  # 0..1

            # Set size penalty: heavier load reduces WM weight (relative to 3)
            load_scale = min(1.0, 3.0 / float(nS))

            wm_weight_eff = wm_weight * entropy_norm * load_scale
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = lr_pos if delta > 0 else lr_neg
            q[s, a] += alpha * delta

            # WM update: move toward one-hot on reward, decay toward uniform otherwise
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p