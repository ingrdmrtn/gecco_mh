def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM usage (logistic function of set size) and WM decay.
    The policy mixes a softmax over RL Q-values and a near-deterministic WM lookup table.
    WM is assumed to store only correct (rewarded) state-action associations and decay otherwise.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6 here).
    model_parameters : iterable
        Tuple/list of 6 parameters:
        - lr: RL learning rate (0..1).
        - wm_weight_base: Baseline weight of WM in the policy mixture (0..1).
        - softmax_beta: Inverse temperature for RL softmax (scaled internally).
        - wm_decay: WM decay toward prior (0..1) applied on each trial.
        - wm_capacity_C: WM capacity midpoint parameter controlling set-size effect.
        - wm_k: Steepness of set-size effect on WM (positive => sharp drop from small to large sets).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_capacity_C, wm_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM policy

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM weight via logistic capacity function
        # phi(nS) in (0,1), larger nS -> smaller phi if wm_k > 0 and C near small set size.
        phi = 1.0 / (1.0 + np.exp(wm_k * (nS - wm_capacity_C)))
        wm_weight_eff = np.clip(wm_weight_base * phi, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice probability for chosen action a
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice probability for chosen action a
            # Deterministic softmax over WM table
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward prior
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: store only correct (rewarded) associations
            if r > 0.0:
                # Overwrite the row toward a one-hot on the rewarded action
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and set-size-dependent WM decay.
    WM represents a recency-based one-shot memory for the last chosen action (irrespective of reward),
    with stronger decay under higher set sizes.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6 here).
    model_parameters : iterable
        Tuple/list of 6 parameters:
        - alpha_pos: RL learning rate for rewards (0..1).
        - alpha_neg: RL learning rate for no-reward (0..1).
        - wm_weight: Weight of WM in the policy mixture (0..1).
        - softmax_beta: Inverse temperature for RL softmax (scaled internally).
        - wm_decay_base: Baseline WM decay toward prior (0..1).
        - decay_k: Strength of set-size increase in WM decay (>=0 means more decay with larger sets).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, wm_decay_base, decay_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay: increases with nS
        # Effective decay in (0,1): 1 - (1 - base)^(1 + decay_k*(nS-3))
        decay_factor = 1.0 + decay_k * max(0, nS - 3)
        wm_decay_eff = 1.0 - (1.0 - wm_decay_base) ** decay_factor
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        # WM mixture weight is constant here but could be tuned by set size if desired
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM: recency-based cache for last chosen action in a state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if r > 0.0 else alpha_neg
            q[s, a] += alpha * delta

            # WM decay toward prior
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

            # WM recency insertion: reinforce the chosen action as the most recent
            # Use insertion strength tied to decay (more decay -> stronger refresh needed).
            ins_strength = wm_decay_eff
            # Make state s distribution more peaked on chosen action
            w[s, :] = (1.0 - ins_strength) * w[s, :]
            w[s, a] += ins_strength

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with limited WM slots and action stickiness in RL.
    WM contributes proportionally to available slots relative to set size (phi = min(1, K/nS)).
    RL policy includes a perseveration bonus for repeating the previous action in the same state.
    WM stores only rewarded mappings and decays toward prior.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6 here).
    model_parameters : iterable
        Tuple/list of 6 parameters:
        - lr: RL learning rate (0..1).
        - wm_weight: Baseline WM weight in mixture (0..1).
        - softmax_beta: RL inverse temperature (scaled internally).
        - wm_decay: WM decay toward prior (0..1).
        - K_slots: Effective number of WM slots (capacity).
        - stickiness: Perseveration bonus added to the previous action in the same state (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, K_slots, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM weight via slots: phi = min(1, K/nS)
        phi = min(1.0, float(K_slots) / float(nS)) if nS > 0 else 0.0
        wm_weight_eff = np.clip(wm_weight * phi, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with stickiness bonus for repeating last action in state s
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] = Q_s[last_action[s]] + stickiness

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            # Note: update uses unaugmented q (stickiness is policy-only)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward prior
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: store mapping only when correct (rewarded)
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p