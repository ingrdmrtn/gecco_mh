def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM mixture with recall scaled by set size.
    
    Policy:
    - RL: softmax over Q-values with inverse temperature softmax_beta.
    - WM: rapid one-shot encoding after positive feedback; otherwise decays toward uniform.
           Choice from WM via a (near) deterministic softmax over W-values (softmax_beta_wm).
    - Arbitration: mixture p = wm_recall * p_wm + (1 - wm_recall) * p_rl, where
      wm_recall = wm_weight * (3 / set_size), capturing reduced recall under higher load.
    
    Parameters (model_parameters):
    - lr: learning rate for RL (also used as WM update/decay step-size).
    - wm_weight: base probability of using WM when set size is 3; scaled by 3/set_size.
    - softmax_beta: inverse temperature for RL choice.
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters  # map to template variable name
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over W-values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-dependent recall: higher set size -> lower WM contribution
            wm_recall = wm_weight * (3.0 / float(nS))
            wm_recall = np.clip(wm_recall, 0.0, 1.0)

            p_total = wm_recall * p_wm + (1.0 - wm_recall) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: fast one-shot learning after reward; decay otherwise
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0:
                # Move W_s toward one-hot on the rewarded action
                w[s, :] = (1 - lr) * W_s + lr * one_hot
            else:
                # Forget toward uniform when no reward
                w[s, :] = (1 - lr) * W_s + lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + precision-weighted WM arbitration with inhibition-of-incorrect actions.
    
    Policy:
    - RL: softmax over Q-values with inverse temperature softmax_beta.
    - WM: encodes last correct response sharply; after errors, suppresses the chosen action.
           Choice from WM via near-deterministic softmax over W-values.
    - Arbitration: mixture weight is state-wise and depends on WM precision and load:
         wm_recall = wm_weight * sharpness * (3 / set_size),
       where sharpness = max(W_s) - mean(W_s), capturing how selective WM is.
    
    Parameters (model_parameters):
    - lr: RL learning rate; also controls WM update strength.
    - wm_weight: base arbitration weight scaling WM contribution when sharpness=1 and set_size=3.
    - softmax_beta: inverse temperature of the RL system.
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Precision (sharpness) of WM for this state
            sharpness = max(W_s) - np.mean(W_s)
            wm_recall = wm_weight * sharpness * (3.0 / float(nS))
            wm_recall = np.clip(wm_recall, 0.0, 1.0)

            p_total = wm_recall * p_wm + (1.0 - wm_recall) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update with inhibition on errors and sharpening on correct
            if r > 0:
                # Move toward a one-hot distribution for the correct action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - lr) * W_s + lr * one_hot
            else:
                # Suppress the chosen (incorrect) action; renormalize softly via blend toward uniform
                inhibit = W_s.copy()
                inhibit[a] = (1 - lr) * inhibit[a]  # downweight incorrect action
                w[s, :] = (1 - lr) * inhibit + lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + noisy WM with load-dependent precision (beta) and mixture weight.
    
    Policy:
    - RL: softmax over Q-values with inverse temperature softmax_beta.
    - WM: tracks action propensity via reward-tuned counts (soft values) updated with lr.
          Choice from WM via softmax with load-dependent inverse temperature:
              softmax_beta_wm_eff = softmax_beta_wm_base * (3 / set_size)
          so WM becomes noisier under high load.
    - Arbitration: mixture p = wm_recall * p_wm + (1 - wm_recall) * p_rl with
          wm_recall = wm_weight * (3 / set_size), reducing WM usage under load.
    
    Parameters (model_parameters):
    - lr: shared step-size for RL and WM value updates.
    - wm_weight: base WM mixture weight at set size 3; scaled by 3/set_size.
    - softmax_beta: inverse temperature for RL (scaled internally).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10
    softmax_beta_wm_base = 50
    eps = 1e-12
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Load-dependent WM precision
            beta_wm_eff = softmax_beta_wm_base * (3.0 / float(nS))
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            wm_recall = wm_weight * (3.0 / float(nS))
            wm_recall = np.clip(wm_recall, 0.0, 1.0)

            p_total = wm_recall * p_wm + (1.0 - wm_recall) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-tuned counts (soft values)
            # Increase propensity for chosen action on reward, otherwise slight diffusion toward uniform
            inc = lr * (1.0 if r > 0 else 0.0)
            w[s, a] = (1 - lr) * W_s[a] + inc  # rewarded actions get boosted
            # diffuse the remaining mass slightly toward uniform for stability
            others = [aa for aa in range(nA) if aa != a]
            if len(others) > 0:
                # Blend non-chosen actions toward uniform baseline
                w[s, others] = (1 - lr) * W_s[others] + lr * w_0[s, others]

            # Normalize W_s softly (not strictly necessary due to softmax, but keeps values bounded)
            row = w[s, :]
            if np.any(np.isfinite(row)):
                # keep within [0,1] roughly
                w[s, :] = np.clip(row, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p