def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + reward-gated working memory with set-size-specific weights.

    Idea:
    - A standard model-free RL learns Q-values with learning rate lr.
    - A working-memory (WM) store encodes the rewarded action for each state in a one-shot manner,
      but decays over time within a block.
    - The policy is a mixture of RL and WM policies, with a set-size-dependent WM weight:
      wm_weight_small used in set size 3, wm_weight_large used in set size 6.

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate.
    - wm_weight_small: float in [0,1]. Mixture weight on WM when set size is 3.
    - wm_weight_large: float in [0,1]. Mixture weight on WM when set size is 6.
    - softmax_beta: float >= 0. Inverse temperature for RL; internally scaled up by 10.
    - wm_decay: float in [0,1]. Decay factor applied to WM values each trial for the visited state.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_small, wm_weight_large, softmax_beta, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM is near-deterministic when information is in memory
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM values (prob-like)
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline for WM decay

        # Choose set-size-specific WM mixture weight
        wm_weight_eff = wm_weight_small if nS <= 3 else wm_weight_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy for chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy for chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward baseline, then one-shot write on reward
            # Decay for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # One-shot "write": make chosen action most likely, others low
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite strongly (reward-gated)
                w[s, :] = 0.9 * one_hot + 0.1 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + last-action WM with set-size-dependent noise and weight.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores the most recent action taken in each state (irrespective of reward), capturing short-term
      perseveration/recency. WM retrieval is noisier at larger set size via a lower effective WM temperature.
    - Policy is a mixture of RL and WM with set-size-specific mixture weights.

    Parameters (tuple/list):
    - lr_pos: float in [0,1]. RL learning rate for positive prediction errors (r - Q > 0).
    - lr_neg: float in [0,1]. RL learning rate for negative prediction errors (r - Q < 0).
    - wm_weight_small: float in [0,1]. Mixture weight on WM when set size is 3.
    - wm_weight_large: float in [0,1]. Mixture weight on WM when set size is 6.
    - softmax_beta: float >= 0. RL inverse temperature; internally scaled by 10.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight_small, wm_weight_large, softmax_beta = model_parameters
    softmax_beta *= 10.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM stores "last action" as a peaked distribution

        # Set-size-specific mixture weight and WM temperature (noisier for larger set size)
        wm_weight_eff = wm_weight_small if nS <= 3 else wm_weight_large
        # Higher noise (lower beta) when set size is larger
        softmax_beta_wm_small = 50.0
        softmax_beta_wm_large = 10.0
        softmax_beta_wm = softmax_beta_wm_small if nS <= 3 else softmax_beta_wm_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy prob of chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy prob of chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update: store the most recent chosen action as a peaked distribution
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = 0.8 * one_hot + 0.2 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + capacity-limited WM whose effective weight scales with K / set_size.

    Idea:
    - RL learns Q-values with learning rate lr and includes value forgetting toward a uniform prior.
    - WM encodes rewarded actions using a learning rate (alpha_wm), and decays slightly on each visit.
    - The WM mixture weight is scaled by the fraction of items that can be actively maintained:
        wm_weight_eff = wm_weight_base * min(1, wm_slots / set_size).
      Thus, higher set size reduces WM influence if capacity is limited.

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate.
    - softmax_beta: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_weight_base: float in [0,1]. Base WM mixture weight when capacity is sufficient.
    - wm_slots: float >= 0. Effective WM capacity K (in number of items).
    - decay_q: float in [0,1]. RL forgetting rate toward uniform (applied on each visit to a state).
    - alpha_wm: float in [0,1]. WM learning rate for rewarded one-shot writing.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_weight_base, wm_slots, decay_q, alpha_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL (uniform prior) and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        q_0 = (1.0 / nA) * np.ones((nS, nA))  # RL prior for forgetting
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-modulated WM weight
        cap_factor = min(1.0, wm_slots / float(nS)) if nS > 0 else 1.0
        wm_weight_eff = wm_weight_base * cap_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy (chosen-action prob)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL forgetting toward prior, then TD update
            q[s, :] = (1.0 - decay_q) * q[s, :] + decay_q * q_0[s, :]
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward baseline, then reward-gated write with alpha_wm
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * (0.9 * one_hot + 0.1 * (1.0 / nA))

        blocks_log_p += log_p

    return -blocks_log_p