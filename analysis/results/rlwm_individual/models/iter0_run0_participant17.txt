Below are three distinct cognitive models for the RL-WM task. Each function:

- Follows the requested signature: (states, actions, rewards, blocks, set_sizes, model_parameters).
- Returns the negative log-likelihood of observed choices.
- Uses all parameters meaningfully and respects the template structure (fills in the “FILL IN” parts only conceptually; the outer loop and variables are kept).
- Incorporates set-size effects as requested.

Notes:
- Assumes numpy is already imported as np.
- Uses a stable softmax for the chosen action probability: p(a|s) = 1 / sum(exp(beta * (x - x[a]))).
- Applies a small floor to probabilities to avoid log(0).


def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + decaying WM with capacity-limited mixture weight

    Mechanism:
    - RL: single learning rate (lr), softmax choice with inverse temperature beta.
    - WM: a fast, decaying associative store per state (w), updated toward a one-hot on rewarded actions,
          and decayed toward uniform otherwise. WM uses a near-deterministic policy (beta_wm=50).
    - Mixture: total policy is a convex combination of WM and RL policies.
    - Set-size effect: Effective WM weight is scaled by (wm_capacity / set_size), capped at 1.

    Parameters
    - lr: float in [0,1], RL learning rate for Q values
    - wm_weight: float in [0,1], base weight of the WM policy in the mixture
    - softmax_beta: float >= 0, base inverse temperature (internally scaled by 10)
    - wm_capacity: float > 0, soft capacity controlling how much WM weight survives as set size increases
                   (effective_wm_weight = wm_weight * min(1, wm_capacity / set_size))
    - wm_decay: float in [0,1], decay rate of WM toward uniform when an outcome is not rewarded and mixing during updates

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # capacity-limited WM weight
        wm_weight_eff = wm_weight * min(1.0, float(wm_capacity) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                # Move WM toward a one-hot at the rewarded action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
            else:
                # Decay toward uniform when not rewarded
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-specific learning rates + WM with retrieval failures that increase with set size

    Mechanism:
    - RL: separate learning rates for positive and negative outcomes (lr_pos, lr_neg).
    - WM: stores the last rewarded action for each state deterministically (one-hot),
          but retrieval can fail with probability increasing in set size.
          When retrieval fails, WM contributes uniform policy for that trial.
          WM uses a near-deterministic softmax when retrieval succeeds (beta_wm=50).
    - Mixture: total policy is a convex combination of WM and RL policies per trial.
    - Set-size effect: WM retrieval success probability decreases with set size via
          p_retrieve = 1 / (1 + gamma * (set_size - 3)).

    Parameters
    - lr_pos: float in [0,1], RL learning rate when reward=1
    - lr_neg: float in [0,1], RL learning rate when reward=0
    - wm_weight: float in [0,1], base WM mixture weight
    - softmax_beta: float >= 0, base inverse temperature for RL (internally scaled by 10)
    - gamma: float >= 0, governs how much WM retrieval degrades with set size

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gamma = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM retrieval success decreases with set size
        p_retrieve = 1.0 / (1.0 + gamma * max(0.0, float(nS) - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy with retrieval failures
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_success = 1.0 / denom_wm
            p_wm_fail = 1.0 / nA
            p_wm = p_retrieve * p_wm_success + (1.0 - p_retrieve) * p_wm_fail

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            alpha = lr_pos if r > 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: store last rewarded action deterministically
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            # No change to WM on non-reward to allow persistence of last correct mapping

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM + action perseveration bias within states

    Mechanism:
    - RL: standard delta rule (lr) with softmax (beta).
    - WM: stores a one-hot mapping upon reward; mixture weight is capacity-limited by K vs set size:
          effective_wm_weight = wm_weight * min(1, K_capacity / set_size).
          WM policy is near-deterministic (beta_wm=50).
    - Perseveration: a bias to repeat the last action taken in the same state.
          Implemented by adding pers_bias to the Q-value of the last chosen action in that state.
          The bias state-tracks only the most recent action per state.
    - Mixture: total policy combines WM and RL as a convex mixture.

    Parameters
    - lr: float in [0,1], RL learning rate
    - wm_weight: float in [0,1], base WM mixture weight
    - softmax_beta: float >= 0, base inverse temperature for RL (internally scaled by 10)
    - K_capacity: float > 0, WM capacity: scales WM mixture by min(1, K_capacity / set_size)
    - pers_bias: float >= 0, additive bias to Q for the last chosen action in the current state

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_capacity, pers_bias = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -np.ones(nS, dtype=int)

        # Capacity-limited WM weight
        wm_weight_eff = wm_weight * min(1.0, float(K_capacity) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += pers_bias

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: write on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p