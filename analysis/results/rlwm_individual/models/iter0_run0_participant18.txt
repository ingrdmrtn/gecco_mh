Below are three standalone cognitive models that follow your template structure. Each returns the negative log-likelihood of the observed choices and uses all parameters meaningfully. I implement different working-memory policies and set-size effects across models.

Note: Assume numpy as np is already imported by the environment. I keep the template structure and only fill in the policy and update rules for WM, as requested.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + Reward-gated Working Memory with decay and set-size modulation of WM weight.
    - RL: single learning rate (lr) with softmax choice (softmax_beta).
    - WM: value-like memory W that (a) decays toward uniform and (b) is strengthened toward
      the chosen action when reward is obtained (reward-gated update with wm_learn).
    - Set-size effect: WM contribution is down-weighted as set size increases via k_setsize.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline mixture weight of WM vs RL (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: decay rate of WM toward uniform on each visit to a state (0..1)
    - wm_learn: WM learning rate toward chosen action when reward=1 (0..1)
    - k_setsize: WM weight reduction factor per additional item beyond 3 (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_learn, k_setsize = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight decreases with set size
        wm_weight_eff = wm_weight / (1.0 + k_setsize * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action (softmax via difference trick)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: softmax over W with high beta (deterministic WM)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform at visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM reward-gated update toward chosen action (Hebbian-like)
            if r > 0.0:
                # Move WM distribution toward one-hot at chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Asymmetric RL (positive/negative learning rates) + One-shot WM (episodic-like)
    with lapses that increase with set size.
    - RL: separate learning rates for positive vs negative prediction errors (alpha_pos, alpha_neg),
      softmax choice with softmax_beta.
    - WM: stores the last action taken for a state (one-shot). WM retrieval is noisy; with probability
      epsilon_wm the WM policy lapses to uniform choice; otherwise follows deterministic WM softmax.
    - Set-size effect: WM lapse increases with set size (epsilon_wm = sigmoid_base + k_setsize*(nS-3)
      bounded to [0, 0.9]). The effective WM mixture weight is also reduced with set size.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for positive PE (0..1)
    - alpha_neg: RL learning rate for negative PE (0..1)
    - wm_weight: baseline mixture weight of WM vs RL (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - epsilon_wm_base: base WM lapse at set size 3 (0..0.5 recommended)
    - k_setsize: increase of WM lapse and reduction of WM weight per item beyond 3 (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, epsilon_wm_base, k_setsize = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM holds last action as a one-hot distribution per state (init uniform)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects
        epsilon_wm = np.clip(epsilon_wm_base + k_setsize * max(0, nS - 3), 0.0, 0.9)
        wm_weight_eff = wm_weight / (1.0 + k_setsize * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # Deterministic WM softmax over last action (W_s is near one-hot)
            denom_wm_det = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / denom_wm_det

            # Lapse mixture for WM policy
            p_wm = (1.0 - epsilon_wm) * p_wm_det + epsilon_wm * (1.0 / nA)

            # Total mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update: one-shot store last action regardless of reward
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: RL with stickiness and forgetting + WM with interference scaling with set size.
    - RL: single learning rate, softmax with action stickiness (perseveration) within state, and
      value forgetting (trace) applied at each state visit.
        * Stickiness adds a bias 'persev' to the previously chosen action for that state in the logits.
        * Trace implements forgetting by shrinking Q toward 0 at each visit.
    - WM: reward-gated binding (store rewarded action strongly) with interference-driven decay toward
      uniform, where interference grows with set size.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: mixture weight of WM vs RL (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - trace: RL forgetting factor applied to Q at each visit (0..1)
    - interference: base WM interference rate per extra item beyond 3 (>=0)
    - persev: stickiness bias added to previous action's logit in the RL policy (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, trace, interference, persev = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness; -1 denotes none yet
        last_action = -1 * np.ones(nS, dtype=int)

        # Interference rate increases with set size
        rho_int = np.clip(interference * max(0, nS - 3), 0.0, 0.9)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting at visit
            q[s, :] = (1.0 - trace) * q[s, :]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Apply stickiness bias to RL logits for previous action in this state
            if last_action[s] >= 0:
                Q_eff = Q_s.copy()
                Q_eff[last_action[s]] += persev
            else:
                Q_eff = Q_s

            # RL policy probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: deterministic softmax over W
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture (no explicit set-size modulation of weight; set-size acts via interference)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM interference decay toward uniform at visit (depends on set size)
            w[s, :] = (1.0 - rho_int) * w[s, :] + rho_int * w_0[s, :]

            # WM reward-gated strengthening toward chosen action
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Move halfway toward one-hot to avoid over-saturation; uses lr to couple systems
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects across models:
- Model 1: WM mixture weight is explicitly down-weighted as set size increases via k_setsize; WM also decays each visit, which functionally hurts performance more in larger sets because rewarded visits are rarer.
- Model 2: WM lapse probability increases with set size (poorer retrieval), and WM mixture weight is also reduced with set size via k_setsize.
- Model 3: WM suffers stronger interference as set size increases; RL includes stickiness and forgetting, providing an alternative account where larger sets reduce WM contribution indirectly via interference without changing the mixture weight parameter.