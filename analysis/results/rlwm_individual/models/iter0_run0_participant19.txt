Below are three standalone cognitive models that follow the provided template. Each returns the negative log-likelihood of the observed choices and uses all parameters meaningfully. I keep the RL part as in the template and fill in the WM policy and WM updating, while allowing the WM/RL mixture weight to depend on set size.

Note: Assume numpy as np is already imported. No imports are included inside the code.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + decaying working-memory cache with capacity-modulated arbitration.

    Idea:
    - RL learns Q via a standard delta rule.
    - WM stores action-outcome preferences per state with two processes:
      (i) global decay toward uniform baseline; (ii) direct encoding of the last experienced outcome for the chosen action.
    - Arbitration between WM and RL depends on set size: larger set sizes reduce WM influence via a logistic transformation.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM weight before set-size modulation (logit space via logistic inside)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_decay: global decay rate of WM table toward uniform (0..1)
    - wm_learning: amount WM updates the chosen action toward current outcome (0..1)
    - capacity_slope: slope controlling how set size (3 vs 6) modulates WM weight; positive values decrease WM at larger set size

    Set-size impact:
    - Effective WM weight is computed as sigmoid(wm_weight_base + capacity_slope*(3.5 - nS)), thus WM impact decreases for larger set sizes if capacity_slope > 0.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_learning, capacity_slope = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size-dependent WM weight
        wm_weight_eff = 1 / (1 + np.exp(-(wm_weight_base + capacity_slope * (3.5 - nS))))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy as softmax over W_s with high inverse temperature
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy (WM arbitration depends on set size)
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            alpha = lr
            q[s][a] += alpha * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0
            # Local update toward current outcome for chosen action
            w[s, :] = (1 - wm_learning) * w[s, :]
            w[s, a] += wm_learning * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + one-shot WM with forgetting; arbitration driven by WM confidence and set size.

    Idea:
    - RL uses standard delta learning.
    - WM forms a near-deterministic association for a state-action when rewarded (one-shot storage),
      with global forgetting toward uniform.
    - Arbitration weight increases with WM confidence (how peaked WM's policy is) and decreases with set size.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline logit for WM weight (transformed via sigmoid)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_forget: WM global forgetting rate toward uniform (0..1)
    - wm_reliability_temp: scales how much WM confidence modulates arbitration (>0 increases weight when WM is confident)
    - setsize_penalty: how strongly larger set sizes down-weight WM (>0 reduces WM for nS=6)

    Set-size impact:
    - Arbitration weight decreases linearly in the logit domain with setsize_penalty*(nS-3).
    """
    lr, wm_weight_base, softmax_beta, wm_forget, wm_reliability_temp, setsize_penalty = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy as softmax over W_s with high inverse temperature
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM confidence (max softmax prob under WM), then arbitration weight
            wm_probs = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_probs /= np.sum(wm_probs)
            wm_conf = np.max(wm_probs)  # in [1/nA, 1]

            # Logit for WM weight combines baseline, WM confidence and set size penalty
            logit_w = wm_weight_base + wm_reliability_temp * (wm_conf - 1.0 / nA) - setsize_penalty * (nS - 3)
            wm_weight_eff = 1 / (1 + np.exp(-logit_w))

            # Mixture
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating with forgetting and one-shot storage
            # Global forgetting toward uniform
            w = (1 - wm_forget) * w + wm_forget * w_0
            # If rewarded, store a near-deterministic association; if not, mildly de-emphasize chosen action
            if r >= 1:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Penalize chosen action slightly beyond global forgetting (re-using wm_forget as a mild push)
                w[s, a] = (1 - wm_forget) * w[s, a] + wm_forget * w_0[s, a]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM implementing win-stay/lose-distribute, with set-size scaled arbitration.

    Idea:
    - RL has separate learning rates for positive vs. negative prediction errors.
    - WM implements a fast win-stay/lose-shift-like mechanism: after reward, push WM mass toward the chosen action;
      after no reward, reduce mass on the chosen action and distribute it to alternatives.
    - Arbitration weight scales down with set size using a power law.

    Parameters (tuple):
    - alpha_pos: RL learning rate when delta >= 0 (0..1)
    - alpha_neg: RL learning rate when delta < 0 (0..1)
    - wm_weight_base: base scaling for WM influence (0..1), further modulated by set size
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - win_stay_gain: WM update strength towards win-stay / lose-distribute (0..1)
    - setsize_exponent: exponent controlling how strongly larger set size reduces WM weight (>=0)

    Set-size impact:
    - Effective WM weight = clip(wm_weight_base * (3.0 / nS) ** setsize_exponent, 0, 1); thus larger nS yields lower WM weight.
    """
    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, win_stay_gain, setsize_exponent = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size-scaled WM weight
        wm_weight_eff = wm_weight_base * (3.0 / nS) ** max(0.0, setsize_exponent)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0 else alpha_neg
            q[s][a] += alpha * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Win-stay: push chosen action up; Lose-distribute: reduce chosen and shift to others
            if r >= 1:
                # Move mass toward chosen action
                w[s, :] = (1 - win_stay_gain) * w[s, :]
                w[s, a] += win_stay_gain * 1.0
            else:
                # Reduce chosen action and distribute to the other actions
                others = [i for i in range(nA) if i != a]
                w[s, a] = (1 - win_stay_gain) * w[s, a]
                w[s, others] = (1 - win_stay_gain) * w[s, others] + (win_stay_gain / (nA - 1)) * 1.0

        blocks_log_p += log_p

    return -blocks_log_p