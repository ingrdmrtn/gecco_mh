def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture model with decay and lapses.
    
    On each trial, choice probability is a convex combination of:
    - RL policy from Q-learning.
    - WM policy that stores the last rewarded action per state (fast and deterministic).
    
    The WM contribution is down-weighted under higher set sizes via a capacity factor.

    Parameters
    ----------
    model_parameters : iterable of length 6
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Baseline weight of WM policy in the mixture (0..1).
        softmax_beta : float
            Inverse temperature for RL policy; rescaled by 10 internally.
        capacity_C : float
            Working memory capacity in number of items; scales WM weight as min(1, C / set_size).
        wm_decay : float
            WM update/forgetting rate applied on visited trials. 
            - If r == 1: w[s] <- (1 - wm_decay) * w[s] + wm_decay * one_hot(a)
            - If r == 0: w[s] <- (1 - wm_decay) * w[s] + wm_decay * w0
        lapse : float
            Lapse rate (0..1); with this probability, choice is uniform random.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, capacity_C, wm_decay, lapse = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity factor reduces WM influence when nS > capacity_C
        capacity_factor = min(1.0, capacity_C / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of the chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values; WM is near-deterministic
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight reduced by set size; apply lapse to mixture
            wm_eff = wm_weight * capacity_factor
            p_mix = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with decay: move toward one-hot if rewarded, else toward uniform
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-gated Working Memory (WM), load-sensitive, with decay and lapses.

    Choice policy is a mixture of RL and WM, but the WM weight is trial-by-trial gated by:
    - RL uncertainty (higher RL certainty => higher WM reliance here; complementary effects can be fit).
    - Set size (higher load reduces WM via a power-law penalty).

    WM stores the last rewarded action per state and decays toward uniform when not rewarded.

    Parameters
    ----------
    model_parameters : iterable of length 6
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Baseline WM weight (0..1) scaling the gating signal.
        softmax_beta : float
            Inverse temperature for RL policy; rescaled by 10 internally.
        wm_decay : float
            WM update/forgetting rate on visited trials, as in model 1.
        load_sensitivity : float
            Exponent controlling how WM weight shrinks with set size: factor = nS^(-load_sensitivity).
        lapse : float
            Lapse rate (0..1); with this probability, choice is uniform.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, load_sensitivity, lapse = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load penalty for WM weight
        load_penalty = (nS ** (-load_sensitivity))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL uncertainty via (normalized) entropy of softmax over Q_s
            logits = softmax_beta * (Q_s - np.max(Q_s))  # numeric stability
            probs = np.exp(logits)
            probs /= np.sum(probs)
            entropy = -np.sum(probs * np.log(np.clip(probs, eps, 1.0)))
            max_entropy = np.log(nA)
            norm_certainty = 1.0 - (entropy / max_entropy)  # 0: max uncertain, 1: maximally certain

            # WM policy (near-deterministic over WM contents)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise WM weight: baseline * certainty * load penalty
            wm_eff = wm_weight * norm_certainty * load_penalty
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            p_mix = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with decay toward one-hot if rewarded; else toward uniform
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with capacity-driven availability and maintenance.

    - RL: separate learning rates for positive vs negative prediction errors.
    - WM: per-state memory availability m[s] (0..1) that is set to 1 when rewarded and decays otherwise.
      WM influence is additionally scaled by capacity_C / set_size (bounded at 1).

    WM stores the last rewarded action for a state; if not recently rewarded, it decays toward uniform.

    Parameters
    ----------
    model_parameters : iterable of length 6
        lr_pos : float
            RL learning rate for positive prediction errors.
        lr_neg : float
            RL learning rate for negative prediction errors.
        wm_weight : float
            Baseline WM weight in mixture (0..1).
        softmax_beta : float
            Inverse temperature for RL policy; rescaled by 10 internally.
        capacity_C : float
            WM capacity in number of items; scales WM weight as min(1, C / set_size).
        wm_maint : float
            WM maintenance/decay rate on visited trials (0..1).
            - If r == 1: m[s] <- 1; w[s] <- (1 - wm_maint) * w[s] + wm_maint * one_hot(a)
            - If r == 0: m[s] <- (1 - wm_maint) * m[s]; w[s] <- (1 - wm_maint) * w[s] + wm_maint * w0

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, capacity_C, wm_maint = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Per-state WM availability
        m = np.zeros(nS)

        capacity_factor = min(1.0, capacity_C / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM effective weight depends on capacity and current availability m[s]
            wm_eff = wm_weight * capacity_factor * m[s]
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_use * delta

            # WM availability and content updates
            if r > 0.5:
                m[s] = 1.0
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_maint) * w[s, :] + wm_maint * one_hot
            else:
                m[s] = (1.0 - wm_maint) * m[s]
                w[s, :] = (1.0 - wm_maint) * w[s, :] + wm_maint * w_0

        blocks_log_p += log_p

    return -blocks_log_p