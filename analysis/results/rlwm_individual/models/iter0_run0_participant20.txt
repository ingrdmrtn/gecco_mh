def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited, decaying working memory and supervised WM updates.

    Idea:
    - Choices are a mixture of RL policy and a capacity-limited WM policy.
    - WM weight is downscaled when set size exceeds WM capacity.
    - WM stores action probabilities per state, decays toward uniform across trials,
      and is updated in a supervised fashion (toward the correct/rewarded action).
    - RL updates Q-values via a standard delta rule.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-value updates.
    - wm_weight_base: scalar in [0,1]. Base reliance on WM before capacity scaling.
    - softmax_beta: scalar >= 0. Inverse temperature for the RL policy (scaled internally).
    - wm_decay: scalar in [0,1]. Trial-wise decay toward uniform for WM storage.
    - wm_capacity: scalar >= 0. Effective WM capacity (in number of state-action associations).
                    WM mixture weight is scaled by min(1, wm_capacity / set_size).
    - lr_wm: scalar in [0,1]. WM update learning rate.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_capacity, lr_wm = model_parameters
    softmax_beta *= 10.0  # increase RL beta dynamic range
    softmax_beta_wm = 50.0  # nearly deterministic WM reads

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL (Q) and WM (W) tables and uniform prior
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM weight
        cap_scale = min(1.0, wm_capacity / max(1, nS))
        wm_weight_eff = np.clip(wm_weight_base * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action (softmax over W_s)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM supervised update
            if r > 0.5:
                # Move toward a one-hot at the chosen action
                w[s, :] = (1.0 - lr_wm) * w[s, :]
                w[s, a] += lr_wm
            else:
                # On negative feedback, relax toward uniform
                w[s, :] = (1.0 - lr_wm) * w[s, :] + lr_wm * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and win-stay WM with capacity-scaled weight and decay.

    Idea:
    - RL has separate learning rates for positive and negative outcomes.
    - WM stores the last rewarded action per state (win-stay), otherwise defaults to uniform.
    - WM weight is scaled by WM capacity relative to set size.
    - WM representation decays toward uniform with a decay that increases as capacity is exceeded.
    - Final choice includes a lapse (uniform random) component.

    Parameters (model_parameters):
    - alpha_pos: scalar in [0,1]. RL learning rate for positive prediction errors (rewards).
    - alpha_neg: scalar in [0,1]. RL learning rate for negative prediction errors (no rewards).
    - wm_weight_base: scalar in [0,1]. Base reliance on WM before capacity scaling.
    - softmax_beta: scalar >= 0. Inverse temperature for the RL policy (scaled internally).
    - lapse: scalar in [0,1]. Probability of making a random (uniform) choice.
    - wm_capacity: scalar >= 0. WM capacity in number of mappings; scales WM weight and decay.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, lapse, wm_capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        cap_scale = min(1.0, wm_capacity / max(1, nS))
        wm_weight_eff = np.clip(wm_weight_base * cap_scale, 0.0, 1.0)
        # Set-size dependent WM decay: more decay when capacity is exceeded
        wm_decay = np.clip(1.0 - cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (win-stay): if a rewarded action is stored, policy peaks at it; else uniform
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture + lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM win-stay update: store last rewarded action (otherwise keep current/decayed)
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0  # deterministic preference for the rewarded action

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-specific WM decay and WM noise.

    Idea:
    - RL and WM are mixed. WM uses a softmax readout but is corrupted by a noise (lapse) within WM.
    - WM decay differs between low- and high-load blocks (set size 3 vs 6).
    - RL uses a single learning rate; WM updates via the same learning rate.
    - This model captures set-size effects via WM decay, and WM noisiness via a WM-specific lapse.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. Learning rate for RL updates and WM supervised updates.
    - wm_weight_base: scalar in [0,1]. Base mixture weight on WM (not capacity-scaled here).
    - softmax_beta: scalar >= 0. Inverse temperature for the RL policy (scaled internally).
    - wm_decay_small: scalar in [0,1]. WM decay per trial for set size 3 blocks.
    - wm_decay_large: scalar in [0,1]. WM decay per trial for set size 6 blocks.
    - wm_noise: scalar in [0,1]. WM-specific lapse that mixes WM softmax with uniform.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_decay_small, wm_decay_large, wm_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_decay = wm_decay_small if nS <= 3 else wm_decay_large
        wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax policy
            W_s = w[s, :]
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # WM internal lapse/noise
            p_wm = (1.0 - wm_noise) * p_wm_soft + wm_noise * (1.0 / nA)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM supervised update using the same lr: push toward chosen action on reward, otherwise toward uniform
            if r > 0.5:
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p