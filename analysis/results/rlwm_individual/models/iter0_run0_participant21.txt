def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Mixture RL + decaying working memory (WM) with set-size gating.

    Model overview:
    - RL system: tabular Q-learning with a single learning rate and softmax choice rule.
    - WM system: recency-based memory that boosts the last rewarded action for each state.
      WM decays toward a uniform baseline each trial and is queried via a near-deterministic softmax.
    - Action selection: mixture of WM and RL policies, with WM weight reduced for larger set sizes.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight: scalar in [0,1], baseline mixture weight on WM policy (before set-size gating).
    - softmax_beta: inverse temperature for RL softmax; internally scaled to allow high values.
    - wm_decay: scalar in [0,1], per-trial decay of WM toward a uniform baseline.
    - wm_bonus: scalar >=0, additive boost to the chosen actionâ€™s WM value on rewarded trials.

    Set-size effect:
    - Effective WM weight is scaled by 3/nS so that WM dominates in small sets (nS=3) and is dampened in larger sets (nS=6).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_bonus = model_parameters
    softmax_beta *= 10.0  # extend dynamic range
    softmax_beta_wm = 50.0  # near-deterministic WM
    eps = 1e-12

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM representations
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Policies
            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability for chosen action (stable)
            Q_centered = Q_s - np.max(Q_s)
            exp_Q = np.exp(softmax_beta * Q_centered)
            p_rl = exp_Q[a] / (np.sum(exp_Q) + eps)

            # WM softmax probability for chosen action (near deterministic)
            W_centered = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_centered)
            p_wm = exp_W[a] / (np.sum(exp_W) + eps)

            # Set-size gated mixture
            wm_weight_eff = wm_weight * (3.0 / float(nS))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM update on reward: boost chosen action and renormalize
            if r > 0.0:
                w[s, a] += wm_bonus
                # keep WM as a distribution over actions
                w[s, :] = np.maximum(w[s, :], 0.0)
                z = np.sum(w[s, :])
                if z > 0:
                    w[s, :] /= z
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-specific learning + WM with set-size-dependent access cost.

    Model overview:
    - RL system: tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - WM system: stores the last rewarded action for each state as a sharp distribution; otherwise decays toward uniform.
    - Action selection: mixture of WM and RL policies.
    - Set-size effect: WM weight is exponentially dampened as set size increases.

    Parameters (model_parameters):
    - lr_pos: scalar in [0,1], RL learning rate for positive PE (r - Q > 0).
    - lr_neg: scalar in [0,1], RL learning rate for negative PE (r - Q < 0).
    - wm_weight: scalar in [0,1], baseline mixture weight on WM.
    - softmax_beta: inverse temperature for RL softmax; internally scaled up.
    - gamma: scalar >= 0, exponential damping factor on WM weight as set size grows.

    Set-size effect:
    - wm_weight_eff = wm_weight * exp(-gamma * (nS - 3)), so WM has more influence in nS=3 and less in nS=6.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            Q_centered = Q_s - np.max(Q_s)
            exp_Q = np.exp(softmax_beta * Q_centered)
            p_rl = exp_Q[a] / (np.sum(exp_Q) + eps)

            # WM policy
            W_centered = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_centered)
            p_wm = exp_W[a] / (np.sum(exp_W) + eps)

            # Set-size-dependent WM weight
            wm_weight_eff = wm_weight * np.exp(-gamma * max(0.0, float(nS) - 3.0))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence-specific learning
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM decay toward uniform each trial
            decay = 0.1  # modest fixed decay; gamma separately governs set-size effect
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM storage/update: on reward, set a sharp distribution on the rewarded action
            if r > 0.0:
                sharp = np.zeros(nA)
                sharp[a] = 1.0
                # small smoothing to avoid absolute determinism
                w[s, :] = 0.98 * sharp + 0.02 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + capacity-limited WM (slot-like) + lapse.

    Model overview:
    - RL system: tabular Q-learning with a single learning rate and per-trial forgetting toward uniform (captures drift under load).
    - WM system: stores the most recent rewarded action per state as a one-hot policy; if no stored item or capacity exceeded,
      WM contributes only a noisy/uniform policy.
    - Capacity: WM influence scales smoothly with a capacity parameter relative to set size.
    - Action selection: convex mixture of RL and WM, followed by a lapse that mixes in uniform choice.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: scalar in [0,1], baseline mixture weight on WM.
    - softmax_beta: inverse temperature for RL softmax; internally scaled.
    - forget_rl: scalar in [0,1], per-trial decay of Q-values toward uniform (higher under load).
    - capacity_k: real-valued capacity (around 1-6); higher => more WM influence at larger set sizes.
    - lapse: scalar in [0,1], stimulus-independent lapse rate mixing with uniform over actions.

    Set-size effect:
    - Effective WM weight is scaled by a smooth capacity function: wm_gain = sigmoid(capacity_k - nS).
      Thus WM impact decreases as set size exceeds capacity_k.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, forget_rl, capacity_k, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track if WM has a stored item for each state (-1 means none)
        wm_store = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Build WM distribution from the current store
            if wm_store[s] >= 0:
                W_s = np.zeros(nA)
                W_s[wm_store[s]] = 1.0
                # small smoothing to avoid exact zeros
                W_s = 0.98 * W_s + 0.02 * w_0[s, :]
            else:
                W_s = w[s, :]

            Q_s = q[s, :]

            # RL policy
            Q_centered = Q_s - np.max(Q_s)
            exp_Q = np.exp(softmax_beta * Q_centered)
            p_rl = exp_Q[a] / (np.sum(exp_Q) + eps)

            # WM policy
            W_centered = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_centered)
            p_wm = exp_W[a] / (np.sum(exp_W) + eps)

            # Capacity-limited WM weight via smooth gating
            wm_gain = 1.0 / (1.0 + np.exp(float(nS) - capacity_k))
            wm_weight_eff = np.clip(wm_weight * wm_gain, 0.0, 1.0)

            # Mixture then lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # apply forgetting on the whole row
            q[s, :] = (1.0 - forget_rl) * q[s, :] + forget_rl * w_0[s, :]

            # WM update: store action only on reward, otherwise keep previous
            if r > 0.0:
                wm_store[s] = a
                # also refresh an internal WM distribution slot for unseen states
                w[s, :] = 0.98 * (np.arange(nA) == a).astype(float) + 0.02 * w_0[s, :]
            else:
                # light decay of any residual WM distribution if no stored item
                if wm_store[s] < 0:
                    w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p