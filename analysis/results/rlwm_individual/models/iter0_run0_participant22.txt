def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-scaled WM weight, WM decay, and lapse.
    - The policy is a convex mixture of: (a) RL softmax over Q-values and (b) WM-based policy over a fast-updating cache W.
    - WM weight is reduced as set size increases via a capacity parameter.
    - WM has decay toward uniform and a lapse that mixes WM policy with uniform noise.
    Parameters (6):
      lr: reinforcement-learning learning rate (0..1)
      wm_weight_base: baseline weight of WM policy in the mixture (0..1)
      softmax_beta: inverse temperature for RL softmax (positive; internally scaled)
      phi: WM decay/suppression parameter controlling how fast WM decays and how much a non-rewarded action is suppressed (0..1)
      capacity_k: WM capacity-like parameter (in units of items; >0). WM weight scales by min(1, capacity_k / set_size)
      lapse: WM lapse rate mixing WM policy with uniform noise (0..1)
    Returns:
      Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, phi, capacity_k, lapse = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective WM weight depends on set size via capacity_k
        cap_scale = min(1.0, max(0.0, capacity_k / max(1.0, nS)))
        wm_weight = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay per trial
        phi_eff = 1.0 - np.exp(-max(1e-8, phi) * (nS / max(1.0, capacity_k)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl_soft = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = p_rl_soft

            # WM policy: deterministic softmax over W_s, with lapse toward uniform
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / denom_wm
            p_wm = (1.0 - lapse) * p_wm_det + lapse * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating: decay toward uniform, then incorporate outcome
            # Decay/interference
            w[s, :] = (1.0 - phi_eff) * w[s, :] + phi_eff * w_0[s, :]

            if r >= 1.0:
                # If rewarded, encode chosen action as one-hot memory
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded, suppress the chosen action a bit and renormalize
                w[s, a] = max(0.0, w[s, a] * (1.0 - phi))
                # Renormalize to valid distribution
                total = np.sum(w[s, :])
                if total <= 1e-12:
                    w[s, :] = w_0[s, :]
                else:
                    w[s, :] /= total

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and choice stickiness; WM decays and weight depends on set size.
    - RL uses separate learning rates for positive and negative outcomes and includes a perseveration bias (stickiness).
    - WM policy is a deterministic softmax over a cached mapping that decays toward uniform; on reward, WM stores the action.
    - WM weight decreases with larger set size.
    Parameters (6):
      lr_pos: RL learning rate after reward=1 (0..1)
      lr_neg: RL learning rate after reward=0 (0..1)
      wm_weight_base: baseline weight of WM policy in the mixture (0..1)
      softmax_beta: RL inverse temperature (positive; internally scaled)
      stickiness: choice perseveration parameter added to the last chosen action in the current state (can be +/-)
      wm_decay: per-trial decay of WM toward uniform (0..1)
    Returns:
      Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr_pos, lr_neg, wm_weight, softmax_beta, stickiness, wm_decay = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # WM weight depends on set size: down-weight when nS increases
        wm_weight = np.clip(wm_weight * (3.0 / max(1.0, nS)), 0.0, 1.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        prev_a = -1 * np.ones(nS, dtype=int)  # last chosen action per state

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # Add stickiness bias to the previously chosen action in this state
            if prev_a[s] >= 0:
                Q_s[prev_a[s]] += stickiness

            # RL policy probability of chosen action a (with stickiness bias)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM deterministic policy from W
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            lr = lr_pos if r >= 1.0 else lr_neg
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay, then store only if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r >= 1.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update stickiness memory
            prev_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with recall-dependent WM policy and set-size-dependent lapses.
    - RL softmax is blended with an epsilon-greedy lapse that increases with set size.
    - WM policy is a mixture of a near-deterministic recall of stored action and uniform choice, with recall probability decreasing with set size.
    - WM decays toward uniform each trial; on reward, WM stores a one-hot mapping for the state.
    Parameters (6):
      lr: RL learning rate (0..1)
      wm_weight_base: baseline WM mixture weight (0..1)
      softmax_beta: RL inverse temperature (positive; internally scaled)
      rho_base: base recall probability for set size 3; effective recall scales down with larger set sizes (0..1)
      epsilon_base: base RL lapse at set size 6; effective epsilon increases with set size (0..1)
      decay: WM decay toward uniform per trial (0..1)
    Returns:
      Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, rho_base, epsilon_base, decay = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size effects:
        # - WM weight decreases with larger set sizes
        wm_weight = np.clip(wm_weight * (3.0 / max(1.0, nS)), 0.0, 1.0)
        # - WM recall probability decreases with larger set size
        rho_eff = np.clip(rho_base * (3.0 / max(1.0, nS)), 0.0, 1.0)
        # - RL lapse increases with set size (0 at nS=3, up to epsilon_base at nS=6)
        if nS <= 3:
            epsilon_eff = 0.0
        else:
            frac = (nS - 3.0) / 3.0  # 0 for 3, 1 for 6
            epsilon_eff = np.clip(frac * epsilon_base, 0.0, 1.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL softmax probability of chosen action with epsilon lapse
            p_soft = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = (1.0 - epsilon_eff) * p_soft + epsilon_eff * (1.0 / nA)

            # WM policy: recall with prob rho_eff, otherwise uniform
            W_s = w[s, :]
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = rho_eff * p_wm_det + (1.0 - rho_eff) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay then encode on reward
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r >= 1.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p