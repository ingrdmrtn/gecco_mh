def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity-limited, decaying working memory that blends with model-free RL.
    Working memory reliability scales with set size via an explicit capacity parameter.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate for Q-values (model-free).
    - wm_weight: scalar in [0,1], base mixture weight of WM versus RL.
    - softmax_beta: positive, RL inverse temperature (scaled by 10 internally for range).
    - wm_lr: scalar in [0,1], WM learning rate (how quickly WM stores rewarded associations).
    - phi: scalar in [0,1], WM decay toward uniform each trial (forgetting).
    - K: positive, WM capacity (effective number of items that can be maintained). WM weight is scaled by K/set_size.

    Set size effects:
    - The effective WM mixture weight within a block is wm_weight * min(1, K / nS), making WM more influential for smaller set sizes (e.g., 3) and less so for larger set sizes (e.g., 6).
    """
    lr, wm_weight, softmax_beta, wm_lr, phi, K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM preference table
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior

        # set-size dependent WM mixture
        wm_mix = wm_weight * min(1.0, K / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM preferences for the state
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM learning: store rewarded action quickly; on no reward only decay operates
            if r > 0.0:
                # move W_s towards one-hot on chosen action
                w[s, :] *= (1.0 - wm_lr)
                w[s, a] += wm_lr

            # renormalize WM row to keep a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + win-stay/lose-forget WM.
    WM policy temperature depends on set size; WM mixture weight is scaled by 3/nS.

    Parameters (6 total):
    - lr_pos: scalar in [0,1], RL learning rate for positive prediction errors (rewards).
    - lr_neg: scalar in [0,1], RL learning rate for negative prediction errors (no reward).
    - wm_weight: scalar in [0,1], base mixture weight of WM.
    - softmax_beta: positive, RL inverse temperature (scaled by 10 internally).
    - beta_wm_small: positive, WM inverse temperature when set size is small (nS=3).
    - beta_wm_large: positive, WM inverse temperature when set size is large (nS=6).

    Set size effects:
    - WM policy determinism uses beta_wm_small for nS=3 and beta_wm_large for nS=6.
    - WM mixture weight is scaled by 3/nS, up-weighting WM on small set sizes.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, beta_wm_small, beta_wm_large = model_parameters
    softmax_beta *= 10.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # set-size dependent WM temperature and weight
        softmax_beta_wm = beta_wm_small if nS <= 3 else beta_wm_large
        wm_mix = wm_weight * (3.0 / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: win-stay / lose-forget
            if r > 0.0:
                # fully commit to the rewarded action (deterministic WM trace for that state)
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # forget to uniform on no reward
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian WM with confidence-based mixture that depends on set size via a logistic gate.
    WM stores outcome-weighted action counts with decay; WM policy is the posterior-mean preference.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - softmax_beta: positive, RL inverse temperature (scaled by 10 internally).
    - w0: real, baseline logit for WM mixture gate (higher = more WM).
    - w_slope: real, slope of set-size effect on WM mixture (multiplied by 3.5 - nS).
    - wm_decay: scalar in [0,1], per-trial decay of WM counts toward zero (forgetting).
    - alpha_dirichlet: positive, symmetric Dirichlet prior mass for WM counts (uncertainty).

    Set size effects:
    - WM mixture weight = sigmoid(w0 + w_slope * (3.5 - nS)), so smaller nS increases WM reliance.
    """
    lr, softmax_beta, w0, w_slope, wm_decay, alpha_dirichlet = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM readout from posterior mean
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM represented as non-negative counts with Dirichlet prior
        c = alpha_dirichlet * np.ones((nS, nA))
        w = (c / np.sum(c, axis=1, keepdims=True)).copy()  # posterior mean for initialization
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # logistic gate for WM mixture, depends on set size
        gate = 1.0 / (1.0 + np.exp(-(w0 + w_slope * (3.5 - float(nS)))))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (softmax over posterior mean)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = gate * p_wm + (1.0 - gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay of counts
            c[s, :] *= (1.0 - wm_decay)

            # WM evidence update: add reward as evidence for the chosen action
            # (no evidence added on no reward)
            if r > 0.0:
                c[s, a] += r

            # Recompute WM posterior mean preferences after update
            denom = np.sum(c[s, :])
            if denom > 0:
                w[s, :] = c[s, :] / denom
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p