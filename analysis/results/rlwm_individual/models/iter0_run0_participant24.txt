def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited working memory (WM) mixture model.

    Policy:
    - RL system: softmax over Q-values with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over WM weights W with very deterministic temperature (softmax_beta_wm=50).
    - Arbitration: convex combination p = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.

    WM mechanisms:
    - Capacity-limited reliance: wm_weight_eff = wm_weight * min(1, wm_capacity / nS), so larger set sizes reduce WM influence.
    - Decay-to-uniform per trial at the accessed state with rate wm_decay.
    - One-shot storage when rewarded: with strength wm_store, the WM rows are pushed toward a Kronecker delta at the rewarded action.

    Parameters:
    - lr: RL learning rate for Q updates.
    - wm_weight: baseline WM arbitration weight (before capacity scaling).
    - softmax_beta: RL inverse temperature (rescaled by 10 in the code for a higher upper bound).
    - wm_decay: decay rate of WM toward uniform at the currently accessed state.
    - wm_store: storage strength for WM upon receiving reward (how strongly WM becomes a one-hot for the rewarded action).
    - wm_capacity: effective WM capacity in number of state-action items; reduces WM influence when set size exceeds capacity.

    Set-size effect:
    - wm_weight is scaled by min(1, wm_capacity/nS), so when set size increases, WM influence declines relative to RL.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_store, wm_capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_weight * min(1.0, float(wm_capacity) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (deterministic readout from W)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform at state s
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage when rewarded (one-shot push toward a one-hot for action a)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_store) * w[s, :] + wm_store * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference (swap) errors and lateral inhibition.

    Policy:
    - RL system: softmax over Q-values with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with deterministic temperature (50), but subject to interference.
    - Interference: final WM probability is a mixture between WM softmax and uniform, controlled by
      eps(nS) = clip(eps0 + eps_slope * (nS - 3)/3, 0, 1) to model set-size-dependent swap/encoding errors.
    - Arbitration: convex combination p = wm_weight * p_wm_interf + (1 - wm_weight) * p_rl.

    WM mechanisms:
    - Lateral inhibition update when rewarded: increases W[s,a] and suppresses other actions.
    - No explicit capacity parameter; instead, set-size increases interference eps.

    Parameters:
    - lr: RL learning rate for Q updates.
    - wm_weight: WM arbitration weight (constant across set sizes; set-size effect enters via eps).
    - softmax_beta: RL inverse temperature (rescaled by 10 in the code).
    - eps0: baseline WM interference probability at set size 3.
    - eps_slope: increase in interference from set size 3 to 6 (linear; applied as (nS-3)/3).
    - inhibition: strength of WM lateral inhibition learning when reward is obtained.

    Set-size effect:
    - eps = clip(eps0 + eps_slope * (nS - 3)/3, 0, 1). Larger nS increases interference, mixing WM with uniform and
      thereby reducing WM precision relative to RL.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, eps0, eps_slope, inhibition = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent interference probability
        eps = eps0 + eps_slope * max(0.0, (float(nS) - 3.0) / 3.0)
        eps = max(0.0, min(1.0, eps))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax for chosen action
            p_wm_base = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Interference: mix with uniform distribution over actions
            p_wm_interf = (1.0 - eps) * p_wm_base + eps * (1.0 / nA)

            # Arbitration
            p_total = wm_weight * p_wm_interf + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with lateral inhibition when rewarded
            if r > 0.0:
                # Increase chosen action value toward 1
                w[s, a] += inhibition * (1.0 - w[s, a])
                # Suppress other actions proportionally
                others = [aa for aa in range(nA) if aa != a]
                for aa in others:
                    w[s, aa] -= inhibition * w[s, aa]
                # Optional renormalization to keep values bounded between 0 and 1
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)
            else:
                # On no reward, slight relaxation toward uniform at this state
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + RPE-gated WM with set-size-dependent arbitration.

    Policy:
    - RL system: softmax over Q-values with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with deterministic temperature (50).
    - Arbitration: wm_weight_eff depends on set size via a logistic transform of a linear function of set size:
        wm_weight_eff = sigmoid(wm_weight_base + wm_weight_slope * (3 - nS)),
      so smaller set sizes yield higher WM reliance.

    WM mechanisms:
    - Decay-to-uniform at the accessed state with rate decay_wm each trial.
    - RPE-gated learning: the WM update magnitude scales with |delta| (absolute RL prediction error) via alpha_wm*|delta|.
      Successful outcomes with larger surprise produce stronger WM encoding; failures pull away from the chosen action.

    Parameters:
    - lr: RL learning rate for Q updates.
    - wm_weight_base: baseline term for the logistic arbitration weight (before set-size effect).
    - wm_weight_slope: sensitivity of WM arbitration weight to set size (positive -> more WM for smaller sets).
    - softmax_beta: RL inverse temperature (rescaled by 10 in the code).
    - alpha_wm: base WM learning rate; actual WM step is alpha_wm * |delta|.
    - decay_wm: decay rate of WM toward uniform at the currently accessed state.

    Set-size effect:
    - Arbitration weight increases for smaller nS via the logistic transform of wm_weight_base + wm_weight_slope*(3 - nS).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, wm_weight_slope, softmax_beta, alpha_wm, decay_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent arbitration via logistic transform
        logits = wm_weight_base + wm_weight_slope * (3.0 - float(nS))
        wm_weight_eff = 1.0 / (1.0 + np.exp(-logits))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform at state s
            w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]

            # RPE-gated WM learning step
            step = alpha_wm * abs(delta)
            # Move chosen action toward r, and distribute opposite change to others
            dw = step * (r - w[s, a])
            w[s, a] += dw
            # Distribute negative/positive compensation to others to keep scale bounded
            for aa in range(nA):
                if aa != a:
                    w[s, aa] -= dw / (nA - 1)

            # Bound W values
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p