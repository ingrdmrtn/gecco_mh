def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and decay.
    - The policy is a mixture between RL softmax and WM softmax policies.
    - WM contribution is reduced when set size exceeds a WM capacity K.
    - WM contents decay toward uniform each trial within a block.
    - WM performs one-shot encoding on rewarded trials and partial inhibition on losses.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base weight of WM policy in the mixture (0-1)
    - softmax_beta: Inverse temperature for RL choice (scaled internally)
    - K: WM capacity in number of items; WM weight is scaled by min(1, K/nS)
    - wm_decay: Per-trial decay of WM contents toward uniform (0-1)
    - wm_lose: Degree to suppress chosen action in WM on losses (0-1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, K, wm_decay, wm_lose = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-adjusted WM weight
        wm_weight_eff = wm_weight * min(1.0, max(0.0, K / max(1, nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM contents
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM update: one-shot encoding on reward; suppression on loss
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Suppress the chosen (incorrect) action proportional to wm_lose
                w[s, a] = (1.0 - wm_lose) * w[s, a]
                # Renormalize to avoid degenerate distributions (not strictly necessary for softmax)
                if w[s, :].sum() > 0:
                    w[s, :] /= w[s, :].sum()
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and set-size-gated WM.
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM contribution follows a logistic gating by set size relative to capacity K with slope g.
    - WM does one-shot encoding on rewarded trials and decays otherwise.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for positive prediction errors (0-1)
    - alpha_neg: RL learning rate for negative prediction errors (0-1)
    - wm_weight: Max weight of WM policy (0-1), scaled by logistic gate
    - softmax_beta: Inverse temperature for RL choice (scaled internally)
    - K: WM capacity (items); inflection point of the logistic gate
    - g: Slope of logistic WM gate with set size (higher g => sharper drop with nS>K)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, K, g = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Logistic WM gate as a function of set size
        # wm_weight_eff ~ wm_weight when nS << K; decreases as nS > K
        gate = 1.0 / (1.0 + np.exp(g * (nS - K)))
        wm_weight_eff = wm_weight * gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0 else alpha_neg
            q[s, a] += alpha * delta

            # WM decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # mild default decay
            # One-shot encode only if rewarded
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with binding errors and set-size-dependent noise.
    - RL softmax temperature decreases with set size (more noise with larger nS).
    - WM suffers binding errors that increase with set size, causing random choices.
    - WM decays toward uniform and one-shot encodes on rewarded trials.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base weight of WM policy (0-1)
    - softmax_beta: Base inverse temperature for RL (scaled internally and reduced by nS)
    - epsilon_bind: WM binding error rate scale (0-1); actual error increases with nS
    - wm_decay: Per-trial decay of WM contents toward uniform (0-1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, epsilon_bind, wm_decay = model_parameters
    softmax_beta *= 10  # base high bound
    softmax_beta_wm = 50  # deterministic WM retrieval before binding errors
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-dependent RL temperature: more exploration for larger nS
        beta_rl_eff = softmax_beta * (3.0 / max(1.0, nS))

        # WM binding error increases with set size: zero at nS=1, up to ~epsilon_bind near nS=6
        bind_err = np.clip(epsilon_bind * (nS - 1) / max(1, 6 - 1), 0.0, 1.0)

        wm_weight_eff = wm_weight  # base mixture weight (could also be set-size modulated)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # Baseline WM policy from deterministic softmax over WM contents
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Binding errors: with probability bind_err, WM outputs random action
            p_wm = (1.0 - bind_err) * p_wm_core + bind_err * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM one-shot encoding on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects across models:
- Model 1: WM weight scales by min(1, K/nS), reducing WM influence in larger set sizes.
- Model 2: WM weight is gated by a logistic function of set size relative to capacity K with slope g; larger sets reduce WM contribution.
- Model 3: RL becomes noisier with larger set sizes (beta scales as 3/nS), and WM suffers binding errors that increase with set size.