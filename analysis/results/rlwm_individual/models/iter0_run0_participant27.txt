Below are three standalone cognitive models that follow your template structure. Each function:

- Takes inputs: states, actions, rewards, blocks, set_sizes, model_parameters
- Mixture of RL and Working Memory (WM) policies with a size-dependent WM contribution
- Returns negative log-likelihood of the observed choices
- Uses all parameters meaningfully (<= 6)
- Fills only the “FILL IN” parts in the provided template logic

Model 1: RL + WM with capacity-limited WM and decay
- Idea: WM encodes mappings quickly but is capacity-limited; its influence scales down when set size is large. WM values decay toward uniform.
- Parameters: lr, wm_weight, softmax_beta, alpha_wm, wm_decay, K

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RLWM mixture with capacity-limited WM and decay.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block; e.g., 3 or 6).
    model_parameters : tuple of floats
        (lr, wm_weight, softmax_beta, alpha_wm, wm_decay, K)
        - lr: RL learning rate (0..1).
        - wm_weight: Base WM mixture weight (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled).
        - alpha_wm: WM learning rate (0..1), fast one-shot encoding.
        - wm_decay: WM decay toward uniform per visit (0..1).
        - K: WM capacity (in number of items); scales WM weight by min(1, K/nS).

    Set size effects
    ----------------
    The effective WM influence is wm_weight_eff = wm_weight * min(1, K/nS).
    Thus, WM is stronger when set size is small (3) than when large (6).
    WM values also decay toward uniform using wm_decay on each visit.
    """
    lr, wm_weight, softmax_beta, alpha_wm, wm_decay, K = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-limited WM weight for this block
        cap_scale = min(1.0, float(K) / float(nS))
        wm_weight_eff_block = wm_weight * cap_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax policy using near-deterministic temperature
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with effective WM weight
            p_total = p_wm * wm_weight_eff_block + (1.0 - wm_weight_eff_block) * p_rl

            # Numerical safety
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Local decay toward uniform on this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Fast WM encoding update on chosen action
            w[s, a] += alpha_wm * (r - w[s, a])

            # Renormalize WM row to keep a proper distribution (stability)
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: RL with asymmetric learning, choice perseveration, and WM mixture
- Idea: RL uses different learning rates for positive/negative outcomes; choices show perseveration; WM contributes with decay and reduced weight at larger set sizes.
- Parameters: lr_pos, lr_neg, wm_weight, softmax_beta, tau, wm_decay

def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RLWM mixture with asymmetric RL learning, choice perseveration, and WM decay.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block; e.g., 3 or 6).
    model_parameters : tuple of floats
        (lr_pos, lr_neg, wm_weight, softmax_beta, tau, wm_decay)
        - lr_pos: RL learning rate after reward (0..1).
        - lr_neg: RL learning rate after no reward (0..1).
        - wm_weight: Base WM mixture weight (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled).
        - tau: Choice perseveration strength added to the last chosen action (>0).
        - wm_decay: WM decay toward uniform per visit (0..1).

    Set size effects
    ----------------
    WM influence is scaled by 3/nS: wm_weight_eff = wm_weight * (3/nS),
    so WM weight is halved in set size 6 relative to set size 3.
    Perseveration applies within block regardless of set size.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, tau, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Last chosen action per state for perseveration (initialize to none = -1)
        last_choice = -1 * np.ones(nS, dtype=int)

        # Size-based WM attenuation: stronger in set size 3, weaker in 6
        size_scale = 3.0 / float(nS)
        wm_weight_eff_block = wm_weight * size_scale
        wm_weight_eff_block = min(max(wm_weight_eff_block, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Apply perseveration bias to both controllers: add tau to last chosen action value
            if last_choice[s] >= 0:
                Q_s[last_choice[s]] += tau
                W_s[last_choice[s]] += tau

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight_eff_block + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Local decay toward uniform on this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-driven strengthening of chosen action in WM
            # Single-parameter update strength equals (lr_pos for r=1, lr_neg for r=0) mapped via pe sign
            # To keep within specified params, use a fast WM update proportional to reward r
            w[s, a] += (lr_pos if r > 0.5 else lr_neg) * (r - w[s, a])

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Update perseveration memory
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: RL + WM with time-based WM decay and Q forgetting
- Idea: WM traces decay as a function of time since last visit to a state and set size; RL also forgets slowly over time via decay of Q-values.
- Parameters: lr, wm_weight, softmax_beta, alpha_wm, wm_time_decay, q_decay

def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RLWM mixture with time-since-last-visit WM decay and RL Q-value forgetting.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block; e.g., 3 or 6).
    model_parameters : tuple of floats
        (lr, wm_weight, softmax_beta, alpha_wm, wm_time_decay, q_decay)
        - lr: RL learning rate (0..1).
        - wm_weight: Base WM mixture weight (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled).
        - alpha_wm: WM learning rate (0..1).
        - wm_time_decay: Per-trial WM forgetting factor (0..1) controlling how fast WM reverts to uniform with time since last visit.
        - q_decay: RL Q-value decay toward uniform (0..1) applied per visit to a state.

    Set size effects
    ----------------
    WM influence scales with both set size and recency:
    wm_weight_eff = wm_weight * (3/nS) * (1 / (1 + age_s)),
    where age_s is time since last visit to state s. Larger set sizes and longer intervals reduce WM impact.
    """
    lr, wm_weight, softmax_beta, alpha_wm, wm_time_decay, q_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track time since last visit to each state
        age = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Age update: increment all, reset current after using for weight/decay
            age += 1
            age_s = int(age[s])

            # Effective WM weight: down-weighted by set size and by recency
            size_scale = 3.0 / float(nS)
            recency_scale = 1.0 / (1.0 + float(age_s))
            wm_weight_eff = wm_weight * size_scale * recency_scale
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            # Apply time-based WM decay to all rows based on their ages (before policy)
            # For efficiency, apply only to current state row using its age-driven factor
            # Decay factor increases with age: decay_factor = 1 - (1 - wm_time_decay)^age
            if wm_time_decay < 0.0:
                eff_decay = 0.0
            else:
                eff_decay = 1.0 - (1.0 - wm_time_decay) ** max(age_s, 0)
            eff_decay = min(max(eff_decay, 0.0), 1.0)
            w[s, :] = (1.0 - eff_decay) * w[s, :] + eff_decay * w_0[s, :]

            # Also decay RL Q toward uniform on the visited state
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Fast WM encoding on chosen action, then renormalize
            w[s, a] += alpha_wm * (r - w[s, a])
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Reset age for current state after update
            age[s] = 0

        blocks_log_p += log_p

    return -blocks_log_p