Below are three standalone cognitive models that implement RL+WM mixtures for the RLWM task. Each function follows the requested structure, uses all of its parameters meaningfully, returns the negative log-likelihood of the observed choices, and includes a clear docstring that explains the parameters and how set size can affect them.

Note: These functions assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited Working Memory (WM) with decay.

    Idea:
    - Choices are governed by a mixture between RL and WM policies.
    - WM stores the correct action when rewarded (one-shot storage) but decays toward uniform over time.
    - WM contribution is capacity-limited: declines when set size exceeds an internal capacity K.

    Parameters (5):
    - lr: Scalar learning rate for RL Q-values (0..1).
    - wm_weight: Base WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10).
    - wm_decay: Per-trial decay of WM traces toward uniform (0..1).
    - capacity_K: Capacity parameter controlling how WM mixture scales with set size (positive).
                  Effective WM weight is wm_weight * min(1, capacity_K / nS).

    Set-size impacts:
    - WM mixture weight is scaled by min(1, K/nS), so WM is stronger for smaller set sizes and weaker for larger ones.
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity_K = model_parameters
    softmax_beta *= 10.0  # higher upper bound for RL inverse temperature
    softmax_beta_wm = 50.0  # WM is highly deterministic when engaged

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value states
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaling of WM weight (constant within block)
        wm_weight_eff = wm_weight * min(1.0, max(0.0, capacity_K / max(1.0, nS)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of the chosen action (stable softmax trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (on the current state)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM one-shot storage upon positive feedback
            if r > 0.5:
                # Store the correct action deterministically
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-dependent learning rate + WM with retrieval noise.

    Idea:
    - RL learning rate is attenuated by cognitive load: lr_eff = lr / (nS ** alpha_load).
    - WM stores the last rewarded action (one-shot), but retrieval is noisy.
      WM policy is a mixture of a deterministic softmax and uniform with weight (1 - wm_noise).
    - Choice combines RL and WM via a fixed wm_weight mixture.

    Parameters (5):
    - lr: Base RL learning rate (0..1).
    - wm_weight: Mixture weight of WM in action selection (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - alpha_load: Exponent controlling how set size reduces RL learning rate (>=0).
                  Effective lr is lr / (nS ** alpha_load).
    - wm_noise: WM retrieval noise (0..1). 0 = perfect WM; 1 = uniform random from WM system.

    Set-size impacts:
    - RL learning rate is reduced as set size grows via alpha_load.
    """
    lr, wm_weight, softmax_beta, alpha_load, wm_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent learning rate
        lr_eff = lr / (float(nS) ** max(0.0, alpha_load))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax prob for chosen action
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # WM retrieval noise blends softmax with uniform
            p_wm = (1.0 - wm_noise) * p_wm_soft + wm_noise * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with set-size–dependent lr
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta

            # WM update: one-shot store on correct feedback (no explicit decay here)
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and perseveration + WM with load-suppressed weight.

    Idea:
    - RL uses separate learning rates for positive and negative outcomes.
    - Choice perseveration (stickiness) biases the RL policy toward repeating the last action in that state.
    - WM stores the last rewarded action (one-shot).
    - WM mixture weight is suppressed under higher set sizes via a logistic function.

    Parameters (6):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight: Base WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - stickiness: Perseveration bias added to the value of the previously chosen action in the current state.
                  Positive values increase tendency to repeat; can be in value units (e.g., 0..2).
    - slope_load: Slope controlling how set size reduces WM weight via a logistic:
                  wm_weight_eff = wm_weight / (1 + exp(slope_load * (nS - 4.5)))

    Set-size impacts:
    - WM mixture weight is reduced with larger set sizes via the logistic transformation governed by slope_load.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, stickiness, slope_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-suppressed WM weight via logistic
        wm_weight_eff = wm_weight / (1.0 + np.exp(slope_load * (float(nS) - 4.5)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add stickiness bias to the previously chosen action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL softmax prob for chosen action (with stickiness)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax prob for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: store correct action on reward
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p