def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited working memory (WM) with decay and one-shot learning.
    
    Idea:
    - Choices are a mixture of a model-free RL policy and a WM policy.
    - WM is capacity-limited; its contribution scales down with set size (nS) using a capacity parameter K.
    - WM entries decay toward a uniform prior and learn in a one-shot fashion when rewarded.
    - RL uses a standard delta rule.

    Parameters (model_parameters):
    - lr:           RL learning rate (0..1)
    - wm_weight:    Base weight on WM policy in the mixture (0..1)
    - softmax_beta: Inverse temperature for RL (transformed internally to have a higher range)
    - eta_wm:       WM learning rate for one-shot updates (0..1)
    - wm_decay:     WM decay toward a uniform prior per visit to a state (0..1)
    - capacity_K:   WM capacity (in number of items); scales WM weight as min(1, K / set_size)

    Set-size impact:
    - Effective WM mixture weight per block is: wm_weight_eff = wm_weight * min(1, capacity_K / nS)
      so WM contributes less in larger set-size (nS=6) blocks than in smaller ones (nS=3).
    """
    lr, wm_weight, softmax_beta, eta_wm, wm_decay, capacity_K = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-limited WM mixture weight
        wm_weight_eff = wm_weight * min(1.0, float(capacity_K) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action (stable softmax via difference trick)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over w[s, :]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward prior for this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning: one-shot if rewarded, aversive if not rewarded
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * one_hot
            else:
                # Push probability mass away from the chosen action
                loss = eta_wm * w[s, a]
                w[s, a] = (1.0 - eta_wm) * w[s, a]
                w[s, :] += loss / (nA - 1.0)
                w[s, a] -= loss / (nA - 1.0)  # keep total mass normalized

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning rates + WM that stores last rewarded action, 
    with set-size-dependent decision noise.

    Idea:
    - RL has separate learning rates for positive vs. negative outcomes.
    - WM stores the last rewarded action per state (deterministic one-shot memory) but lapses/forgets over time.
    - Decision noise (inverse temperature) decreases with set size for RL, capturing higher load in larger sets.

    Parameters (model_parameters):
    - lr_pos:        RL learning rate for rewards (0..1)
    - lr_neg:        RL learning rate for non-rewards (0..1)
    - wm_weight:     Base weight on WM policy in the mixture (0..1)
    - softmax_beta:  Base inverse temperature for RL (scaled internally)
    - beta_load:     Load penalty on RL inverse temperature; beta_eff = (softmax_beta*10) / (1 + beta_load*(nS-3))
    - wm_lapse:      WM lapse/decay rate toward uniform (0..1) applied each time a state is visited

    Set-size impact:
    - RL inverse temperature decreases with set size via beta_load, increasing stochasticity for nS=6.
    - WM weight remains as given (wm_weight), but WM itself is imperfect via lapses.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, beta_load, wm_lapse = model_parameters

    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size-dependent RL temperature
        beta_eff = base_beta / (1.0 + beta_load * max(0.0, float(nS - 3)))

        # Initialize RL Q-values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy: softmax over w[s, :]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with valence-asymmetric learning
            if r > 0.5:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            # WM decay toward uniform on each visit to state s
            w[s, :] = (1.0 - wm_lapse) * w[s, :] + wm_lapse * w_0[s, :]

            # WM update: on reward, store the action deterministically (one-shot)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # overwrite as a strong memory trace

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + WM recency and perseveration bias, with set-size-scaled WM weight.

    Idea:
    - RL values decay toward a uniform prior (forgetting), capturing interference across larger sets.
    - WM captures recency of chosen actions per state; rewards strengthen recency, but even unrewarded choices
      leave a weak trace. WM also includes a within-state perseveration bias to repeat the last action.
    - WM mixture weight decreases with set size (fewer WM resources per item).

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - wm_weight:     Base WM mixture weight (0..1)
    - softmax_beta:  RL inverse temperature (scaled internally)
    - rl_forget:     RL forgetting rate toward uniform prior per state visit (0..1)
    - wm_decay:      WM decay rate toward uniform per state visit (0..1)
    - pers_beta:     Perseveration bias added to WM logits for repeating the last action in a state (>=0)

    Set-size impact:
    - WM mixture weight is scaled as wm_weight_eff = wm_weight / (1 + (nS - 3)), reducing WM influence at nS=6.
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_decay, pers_beta = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM structures
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # -1 means none yet

        # Set-size-scaled WM weight
        wm_weight_eff = wm_weight / (1.0 + max(0.0, float(nS - 3)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with perseveration bias in logits
            W_s = w[s, :].copy()
            logits_wm = softmax_beta_wm * W_s

            if last_action[s] >= 0:
                logits_wm[last_action[s]] += pers_beta

            # Compute WM probability of chosen action from logits
            logits_centered = logits_wm - logits_wm[a]
            p_wm = 1.0 / np.sum(np.exp(logits_centered))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL forgetting toward uniform for current state, then TD update
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)
            q[s, a] += lr * (r - Q_s[a])

            # WM decay toward uniform for current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM recency update: reward-weighted boost to the chosen action
            boost = 0.5 + 0.5 * r  # 0.5 if no reward, 1.0 if reward
            w[s, a] = w[s, a] + boost * (1.0 - w[s, a])
            # renormalize to keep probabilities valid
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p