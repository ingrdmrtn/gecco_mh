def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + capacity-limited Working Memory (WM) with decay.
    - RL component: standard delta rule with softmax choice.
    - WM component: stores rewarded action per state; decays toward uniform (interference).
    - Mixture weight is reduced as set size increases (capacity effect).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Base mixture weight on WM policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled up internally).
        wm_decay : float in [0,1]
            Per-trial decay of WM towards uniform (higher = more decay/interference).
        capacity : float in (0, 6]
            WM capacity; effective WM weight is scaled by min(1, capacity / set_size).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    wm_decay = model_parameters[3] if len(model_parameters) > 3 else 0.2
    capacity = model_parameters[4] if len(model_parameters) > 4 else 3.0

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY policy
            # Apply global decay (interference) toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            W_s = w[s, :]
            # Softmax with high beta approximates "retrieve stored action" determinism
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-limited mixture: reduce WM influence for larger set sizes
            current_set_size = int(block_set_sizes[t])
            cap_factor = min(1.0, capacity / max(1, current_set_size))
            wm_weight_eff = np.clip(wm_weight * cap_factor, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WORKING MEMORY value updating
            # Reward-gated storage: if rewarded, store chosen action deterministically.
            if r >= 0.5:
                vec = np.zeros(nA)
                vec[a] = 1.0
                w[s, :] = vec
            # If not rewarded, leave the decayed representation (no new storage)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + recall-based WM with lapses and asymmetric RL learning.
    - RL: softmax choice with separate learning rates for positive and negative prediction errors.
    - WM: item-based store of last rewarded action per state.
      Retrieval success decreases with set size; lapse probability adds noise.
      WM policy is a convex combination of uniform and a one-hot policy on the stored action.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr_pos : float in (0,1)
            RL learning rate for positive prediction errors.
        wm_weight : float in [0,1]
            Base mixture weight on WM policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled up internally).
        lr_neg : float in (0,1)
            RL learning rate for negative prediction errors.
        wm_lapse : float in [0,1]
            Lapse probability in WM retrieval (adds uniform noise).
        recall_slope : float >= 0
            Linear decrement of WM recall from set size 3 to 6:
            recall_prob = max(0, 1 - recall_slope*(set_size - 3)).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr_pos, wm_weight, softmax_beta = model_parameters[:3]
    lr_neg = model_parameters[3] if len(model_parameters) > 3 else lr_pos
    wm_lapse = model_parameters[4] if len(model_parameters) > 4 else 0.1
    recall_slope = model_parameters[5] if len(model_parameters) > 5 else 0.3

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # kept to follow template; not directly used for storage
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM store: for each state, whether we have a stored action and which one
        has_mem = np.zeros(nS, dtype=bool)
        mem_act = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY policy: recall decreases with set size, plus lapse
            set_size_t = int(block_set_sizes[t])
            recall_prob = max(0.0, 1.0 - recall_slope * (set_size_t - 3))
            eff_recall = recall_prob * (1.0 - wm_lapse)

            if has_mem[s]:
                stored_a = int(mem_act[s])
                p_wm_vec = (1.0 - eff_recall) * w_0[s, :]  # uniform noise
                p_wm_vec[stored_a] += eff_recall
                # get probability of chosen action
                p_wm = p_wm_vec[a]
            else:
                # no memory: uniform
                p_wm = w_0[s, a]

            # Mixture weight (kept constant here; set size effect enters via recall_prob)
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM updating rule: store only on rewarded trials; clear on errors
            if r >= 0.5:
                has_mem[s] = True
                mem_act[s] = a
            else:
                # If the stored action was just proven wrong, clear memory to avoid perseveration
                if has_mem[s] and mem_act[s] == a:
                    has_mem[s] = False  # forget incorrect association

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL with eligibility traces + probabilistic WM with leak and set-size-dependent weighting.
    - RL: TD learning with eligibility traces; softmax choice.
    - WM: Dirichlet-like counts over actions per state with leak toward uniform.
      WM policy is softmax over WM distribution with its own temperature.
    - Mixture weight decreases with set size (resource sharing).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight_base : float in [0,1]
            Base mixture weight on WM policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled up internally).
        trace_decay : float in [0,1]
            Eligibility trace decay (lambda).
        wm_leak : float in [0,1]
            Per-trial leak of WM toward uniform (higher = more leak).
        wm_temp_scale : float > 0
            Scales WM temperature: higher -> more stochastic WM policy.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta = model_parameters[:3]
    trace_decay = model_parameters[3] if len(model_parameters) > 3 else 0.5
    wm_leak = model_parameters[4] if len(model_parameters) > 4 else 0.2
    wm_temp_scale = model_parameters[5] if len(model_parameters) > 5 else 1.0

    softmax_beta *= 10.0
    base_wm_beta = 50.0
    softmax_beta_wm = max(1e-6, base_wm_beta / max(1e-6, wm_temp_scale))
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM probability-like store (normalized)
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL over state-action pairs
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY policy: leak toward uniform, then softmax over WM dist
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture weight reduces with set size (resource sharing)
            current_set_size = int(block_set_sizes[t])
            wm_weight_eff = np.clip(wm_weight_base / (1.0 + max(0, current_set_size - 3)), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Update traces: decay all, set current (s,a) to 1
            e *= trace_decay
            e[s, :] *= 0.0
            e[s, a] = 1.0

            pe = r - Q_s[a]
            q += lr * pe * e  # update all SA via traces

            # WM updating: reward-sensitive increment and mild punishment
            if r >= 0.5:
                # Move mass toward chosen action strongly
                incr = np.zeros(nA)
                incr[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * incr
            else:
                # Slightly suppress chosen action probability to avoid perseveration
                w[s, a] = 0.8 * w[s, a]
                # renormalize row to sum to 1
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] = w[s, :] / row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p