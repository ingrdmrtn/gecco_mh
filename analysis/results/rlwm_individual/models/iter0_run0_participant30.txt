Below are three standalone cognitive models that implement reinforcement learning (RL) combined with a working-memory (WM) component for the rlwm task. Each function:

- Follows the requested signature: states, actions, rewards, blocks, set_sizes, model_parameters
- Returns the negative log-likelihood of the observed choices
- Uses all parameters meaningfully
- Implements the provided template structure and fills in the missing policy and updating rules for WM
- Notes how set size (3 vs 6) impacts parameters/weights within each model

Model 1: RL + capacity-weighted WM with decay and learning
- Idea: WM weight scales inversely with set size based on a capacity parameter. WM traces decay toward uniform and are strengthened on reward.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-weighted WM with decay and WM learning.

    Mixture policy: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values
    - wm_weight_base: Base mixture weight for WM
    - softmax_beta: Inverse temperature for RL softmax (scaled up internally)
    - wm_decay: WM decay rate toward uniform per trial (0=no decay, 1=fully reset each trial)
    - wm_capacity: Capacity-like parameter controlling how WM weight scales with set size
                   Effective wm weight per block: wm_weight = wm_weight_base * min(1, wm_capacity / nS)
    - wm_lr: WM learning rate when reward is received; strengthens chosen action’s WM trace

    Set size effect:
    - WM influence decreases as set size increases via the capacity-scaling of wm_weight.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_capacity, wm_lr = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    blocks_log_p = 0.0
    unique_blocks = np.unique(blocks)

    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-weighted WM contribution per block
        # If nS > wm_capacity, WM weight is reduced
        cap_scale = min(1.0, max(0.0, wm_capacity / float(nS)))
        wm_weight_block = wm_weight_base * cap_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy probability of chosen action (softmax over WM weights)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning
            if r > 0.5:
                # Move probability mass toward chosen action
                # Convex combination with a one-hot vector
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * one_hot
            else:
                # On negative feedback, slightly reduce chosen action's weight and renormalize
                # (Keep within probability simplex)
                reduce = min(0.1, w[s, a] * 0.5)
                w[s, a] -= reduce
                redistribute = reduce / (nA - 1)
                for k in range(nA):
                    if k != a:
                        w[s, k] += redistribute
                # numerical tidy to ensure sum to 1
                w[s, :] = np.maximum(w[s, :], 1e-8)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: Asymmetric RL (positive/negative learning rates) + WM cache with set size–dependent forgetting + perseveration bias
- Idea: WM acts as a one-shot cache that is very reliable when set size is small, but forgets more when set size is large. RL uses separate learning rates for gains and losses. Perseveration bias adds a transient preference for the previous action in the same state.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + WM cache with set size–dependent forgetting + perseveration.

    Mixture policy: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors
    - lr_neg: RL learning rate for negative prediction errors
    - wm_weight_base: Base mixture weight for WM
    - softmax_beta: Inverse temperature for RL softmax (scaled up internally)
    - forget_rate6: WM forgetting strength calibrated for set size 6
                    Effective forgetting: forget = forget_rate6 if nS==6 else forget_rate6/2
    - stickiness: Perseveration bias added to RL policy for repeating last action in the same state

    Set size effect:
    - WM forgetting increases with larger set size (higher at 6 than 3), reducing effective WM influence.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, forget_rate6, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    unique_blocks = np.unique(blocks)

    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Set size–dependent forgetting and WM weight
        forget = forget_rate6 if nS == 6 else (forget_rate6 / 2.0)
        wm_weight_block = max(0.0, min(1.0, wm_weight_base * (1.0 - forget)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bonus for repeating last action in this state (RL policy only)
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL policy probability of chosen action with stickiness
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy:
            # Treat WM as a cache: if it's sharply peaked, it acts deterministically;
            # else it approximates uniform. Implement via softmax over W_s with high beta.
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update:
            # On rewarded trial, encode a strong one-hot memory (cache)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # On error, suppress the chosen action and redistribute
                drop = min(0.5, w[s, a])
                w[s, a] -= drop
                w[s, :] += drop / (nA - 1)
                w[s, :] = np.maximum(w[s, :], 1e-8)
                w[s, :] /= np.sum(w[s, :])

            # Apply forgetting toward uniform each trial (stronger at set size 6)
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: Uncertainty-gated WM mixture with set size modulation + win-stay/lose-shift WM
- Idea: The WM mixture weight is dynamically adjusted by RL uncertainty (entropy of RL policy) and set size. WM updates implement a win-stay/lose-shift memory with decay.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-gated WM mixture + win-stay/lose-shift WM.

    Mixture policy: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
    where wm_weight is dynamically modulated by RL uncertainty and set size.

    Parameters (model_parameters):
    - lr: RL learning rate
    - wm_weight_base: Base WM mixture weight (in (0,1)); transformed via logit for gating
    - softmax_beta: Inverse temperature for RL softmax (scaled up internally)
    - wm_decay: WM decay toward uniform each trial
    - uncertainty_slope: Scales influence of RL uncertainty (higher entropy => more WM weight)
    - setsize_slope: Scales influence of set size (larger nS reduces WM weight if negative)

    Set size effect:
    - wm_weight increases when set size is small and/or RL is uncertain; decreases when set size is large.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, uncertainty_slope, setsize_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Helper: stable logit and sigmoid
    def clip01(x, eps=1e-6):
        return min(1.0 - eps, max(eps, x))

    def sigmoid(z):
        return 1.0 / (1.0 + np.exp(-z))

    blocks_log_p = 0.0
    unique_blocks = np.unique(blocks)

    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        # Precompute entropy constant
        H_max = np.log(nA)

        # Logit of base weight for linear gating
        base_logit = np.log(clip01(wm_weight_base) / (1.0 - clip01(wm_weight_base)))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy distribution and chosen-action prob
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)  # numerical stability
            probs_rl = np.exp(logits_rl) / np.sum(np.exp(logits_rl))
            p_rl = max(probs_rl[a], 1e-12)

            # RL uncertainty (entropy)
            H_rl = -np.sum(probs_rl * np.log(np.maximum(probs_rl, 1e-12)))

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            probs_wm = np.exp(logits_wm) / np.sum(np.exp(logits_wm))
            p_wm = max(probs_wm[a], 1e-12)

            # Dynamic wm_weight: base + uncertainty + set size (linear in logit space)
            # Larger nS reduces wm_weight if setsize_slope < 0
            z = base_logit + uncertainty_slope * (H_rl / H_max) + setsize_slope * (3.0 - float(nS))
            wm_weight = sigmoid(z)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM win-stay / lose-shift update (followed by renormalization)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Make WM strongly prefer the chosen action on reward
                w[s, :] = 0.9 * one_hot + 0.1 * w[s, :]
            else:
                # Suppress chosen action, increase others equally
                drop = min(0.5, w[s, a])
                w[s, a] -= drop
                w[s, :] += drop / (nA - 1)
                w[s, :] = np.maximum(w[s, :], 1e-8)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size impacts across the models:
- Model 1: WM mixture weight is downscaled by wm_capacity / nS. Larger set size reduces WM influence.
- Model 2: WM forgetting is stronger when nS = 6, which both erodes WM contents and reduces effective WM weight.
- Model 3: WM weight is dynamically gated by uncertainty and set size; a negative setsize_slope decreases WM reliance when nS is larger.