def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-scaled WM weight and WM decay.
    - The policy is a mixture of a model-free RL softmax and a working-memory (WM) softmax.
    - WM encodes the last rewarded action for a state as a one-hot map (deterministic retrieval),
      but decays toward uniform over trials. The mixture weight of WM is reduced as set size grows.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight: scalar in [0,1], baseline contribution of WM to choice.
    - softmax_beta: scalar >=0, inverse temperature for RL policy (scaled by 10 inside).
    - wm_decay: scalar in [0,1], per-trial decay of WM toward uniform.
    - gamma: scalar >=0, scaling exponent for set-size modulation of WM weight; effective WM weight
             per block is wm_weight * (3 / set_size) ** gamma.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM mixture weight
        wm_weight_eff = wm_weight * (3.0 / float(nS)) ** gamma
        wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) If rewarded, encode perfect association (one-shot)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with asymmetric RL learning rates and uncertainty-gated WM mixture influenced by set size.
    - RL uses separate learning rates for positive vs negative prediction errors.
    - WM stores the last rewarded action per state (win-stay) and penalizes the chosen action on losses (lose-shift).
    - The contribution of WM is dynamically gated by WM confidence and a set-size-dependent threshold.

    Parameters (model_parameters):
    - lr_pos: scalar in [0,1], RL learning rate for positive prediction errors (r > Q).
    - lr_neg: scalar in [0,1], RL learning rate for negative prediction errors (r < Q).
    - wm_weight: scalar in [0,1], maximum WM contribution to the policy.
    - softmax_beta: scalar >=0, inverse temperature for RL policy (scaled by 10 inside).
    - gate_bias: real, baseline threshold for WM gate.
    - gate_slope: real >=0, slope controlling sensitivity of gate to (confidence - threshold).

    Notes on set size:
    - The WM gate threshold increases with set size (harder in 6 than 3), implemented as:
      theta = sigmoid(gate_bias + gate_slope * ((nS - 3) / 3)).
      Effective WM weight at trial t is wm_weight * sigmoid(gate_slope * (wm_conf - theta)),
      where wm_conf is the difference between the top two WM action weights for that state.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gate_bias, gate_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent gate threshold (higher for larger nS)
        theta = sigmoid(gate_bias + gate_slope * ((float(nS) - 3.0) / 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence: gap between best and second-best WM probabilities
            # Compute via values in W_s (already normalized-like). Use difference between top two entries.
            top1 = np.max(W_s)
            # get second best
            second = np.partition(W_s, -2)[-2] if nA > 1 else 0.0
            wm_conf = max(0.0, top1 - second)

            # Gate WM by confidence and set-size threshold
            gate = sigmoid(gate_slope * (wm_conf - theta))
            wm_weight_eff = wm_weight * gate
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update:
            # Decay slightly toward uniform to model interference
            decay = 0.05  # modest fixed decay to avoid overfitting; small regularizer
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                # Win-stay: store the correct action deterministically
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Lose-shift: downweight the chosen action using lr_neg as penalty size
                remove = min(1.0, max(0.0, lr_neg))
                mass = w[s, a] * remove
                w[s, a] -= mass
                # redistribute removed mass uniformly among the other actions
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += mass / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with state-specific forgetting + capacity-limited WM slots.
    - RL includes per-visit forgetting toward uniform (interference) and standard delta-rule learning.
    - WM stores up to K states per block in 'slots'; stored states yield near-perfect retrieval.
      If set size exceeds K, the probability that the current state is in WM is approximately K / set_size,
      implemented by scaling the WM mixture weight accordingly.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q updates.
    - wm_weight: scalar in [0,1], baseline WM weight when the state is in memory.
    - softmax_beta: scalar >=0, inverse temperature for RL policy (scaled by 10 inside).
    - rl_forget: scalar in [0,1], per-visit forgetting toward uniform for the current state's Q.
    - K: scalar >=0, WM capacity (number of states that can be perfectly stored per block).

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, rl_forget, K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track which states are stored in WM (slots)
        stored = np.zeros(nS, dtype=bool)
        slots_used = 0
        K_eff = max(0.0, K)
        # Precompute set-size scaled probability of being in WM
        wm_state_scale = min(1.0, K_eff / float(nS)) if nS > 0 else 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight depends on whether the state is stored and capacity vs set size
            in_memory = stored[s]
            wm_weight_eff = wm_weight * (1.0 if in_memory else 0.0) * wm_state_scale
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL forgetting toward uniform for current state
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)
            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update and slot assignment
            if r > 0.0:
                # If not stored and capacity available, store this state
                if not stored[s] and slots_used < K_eff:
                    stored[s] = True
                    slots_used += 1
                # If stored, encode one-hot association
                if stored[s]:
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                else:
                    # If capacity full, leave WM distribution uniform for this state
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p