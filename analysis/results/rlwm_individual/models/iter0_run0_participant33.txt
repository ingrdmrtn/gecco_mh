Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) to explain choices in the RLWM task. Each function returns the negative log-likelihood of the observed choices. They follow the provided template structure and use up to 6 parameters, all of which are used meaningfully. Importantly, each model specifies how set size (3 vs. 6) impacts behavior.

Note: Assume numpy as np is already imported elsewhere.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity-sensitive WM weight, WM decay, and WM learning on rewarded trials.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight_base : float
            Baseline WM arbitration weight (mixed with RL); scaled by set size via a sigmoidal capacity factor.
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10 for numerical range).
        wm_decay : float
            Per-trial WM decay toward uniform (higher -> faster forgetting).
        wm_learn : float
            Strength with which WM encodes the chosen action on rewarded trials.
        wm_capacity_slope : float
            Controls how strongly set size modulates WM weight; effective WM weight = wm_weight_base * sigmoid(wm_capacity_slope*(3.5 - nS)).
            Thus, smaller sets yield higher WM influence.

    Set-size effects
    ----------------
    - Effective WM weight increases for small set size (3) and decreases for large set size (6) via wm_capacity_slope.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_learn, wm_capacity_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-sensitive WM weight
        cap = 1.0 / (1.0 + np.exp(-wm_capacity_slope * (3.5 - nS)))
        wm_weight_eff = np.clip(wm_weight_base * cap, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (probability of chosen action a under softmax)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (softmax over WM weights; high beta approximates argmax)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then encode rewarded action
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Encode when rewarded
            if r > 0.0:
                # Move mass toward chosen action, then renormalize
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
                # Renormalize to keep a valid distribution
                w[s, :] /= max(np.sum(w[s, :]), 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM as last-action store with decay + set-size-dependent lapse.

    Parameters
    ----------
    model_parameters : tuple
        lr_pos : float
            RL learning rate for positive prediction errors (rewarded trials).
        lr_neg : float
            RL learning rate for negative prediction errors (unrewarded trials).
        wm_weight : float
            Baseline WM arbitration weight (mixed with RL).
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        lapse0 : float
            Baseline lapse logit; higher implies more random responding.
        lapse_setsize_slope : float
            Set-size effect on lapse (logit). Effective lapse = sigmoid(lapse0 + slope*(nS-3));
            larger set size increases lapse if slope > 0.

    Set-size effects
    ----------------
    - Lapse rate increases with set size when lapse_setsize_slope > 0, diluting both RL and WM policies in larger sets.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, lapse0, lapse_setsize_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent lapse (same for all trials in the block)
        lapse_logit = lapse0 + lapse_setsize_slope * (nS - 3.0)
        p_lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
        p_lapse = np.clip(p_lapse, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: store last chosen action with decay, read out deterministically
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixed policy with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - p_lapse) * p_mix + p_lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM update: decay toward uniform and store last chosen action regardless of reward
            # Decay toward uniform
            decay = 0.2  # implicit, small stable decay to keep WM dynamic; tied to set size implicitly via lapse not directly here
            # Use a mild, fixed decay to avoid unused parameter proliferation
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # Store last chosen action deterministically (then renormalize)
            w[s, :] = 0.0 * w[s, :]
            w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with entropy-based arbitration and set-size-dependent WM interference across states.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight_base : float
            Baseline WM arbitration weight (scaled down when WM is uncertain).
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        wm_forget : float
            WM forgetting/decay toward uniform each trial (per observed state).
        interference : float
            Max WM interference strength across states; effective interference scales with set size.
        tau_arbitration : float
            Arbitration temperature: effective WM weight = wm_weight_base * exp(-tau_arbitration * H(W_s));
            higher entropy H reduces WM influence more strongly.

    Set-size effects
    ----------------
    - Interference increases with set size: eff_interf = interference * ((nS - 3) / 3),
      which is 0 at nS=3 and equals 'interference' at nS=6. This blends WM content between states more at larger set sizes.
    - Arbitration also depends on WM uncertainty (entropy of W_s) on each trial.
    """
    lr, wm_weight_base, softmax_beta, wm_forget, interference, tau_arbitration = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent interference scaling
        eff_interf = interference * max((nS - 3.0) / 3.0, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute WM entropy for arbitration
            Pw = np.clip(W_s, 1e-12, 1.0)
            Pw = Pw / np.sum(Pw)
            H = -np.sum(Pw * np.log(Pw)) / np.log(nA)  # normalized entropy in [0,1]

            # Entropy-based arbitration weight (bounded in [0,1])
            wm_weight_eff = wm_weight_base * np.exp(-tau_arbitration * H)
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform for the current state
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Reward-dependent strengthening in WM for chosen action
            if r > 0.0:
                boost = 0.6  # fixed strengthening; balances with wm_forget, keeps params <= 6
                w[s, :] = (1.0 - boost) * w[s, :]
                w[s, a] += boost
                w[s, :] /= max(np.sum(w[s, :]), 1e-12)

            # Set-size-dependent WM interference: blend with average WM across states
            if eff_interf > 0.0 and nS > 1:
                avg_w = np.mean(w, axis=0)
                w[s, :] = (1.0 - eff_interf) * w[s, :] + eff_interf * avg_w
                w[s, :] /= max(np.sum(w[s, :]), 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p

Notes on model differences:
- Model 1: WM weight is explicitly capacity-sensitive with set size; WM encodes only when rewarded, with decay otherwise.
- Model 2: Adds asymmetric RL learning rates and a set-size-dependent lapse that dilutes policies in larger sets; WM stores last action regardless of reward.
- Model 3: Introduces WM interference that increases with set size and arbitration that down-weights WM when its entropy is high, capturing uncertainty and load-dependent interference.