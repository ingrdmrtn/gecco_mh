Below are three standalone cognitive models tailored for the RL-WM task. Each function follows the requested template structure, computes log-likelihood via an RL–WM mixture policy, and includes a clear working-memory policy and update rule. All parameters are used and interpretations, including how set size may modulate parameters, are included in the docstrings.

Note: Assume numpy has already been imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with one-shot WM encoding and decay, and set-size scaling of WM weight.

    Mechanism:
    - RL: Standard Rescorla–Wagner update with learning rate lr and softmax with inverse temperature beta.
    - WM: One-shot storage upon correct feedback (r=1): WM for that state becomes a one-hot vector favoring the chosen action.
          Otherwise, WM decays toward uniform with rate wm_decay. WM policy uses a near-deterministic softmax (beta_wm=50).
    - Mixture: Action probability is a convex combination of WM policy and RL policy with effective WM weight that
               decreases with set size.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: Baseline WM mixture weight in small set size
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: WM decay rate toward uniform (0..1); larger means faster forgetting
    - wm_size_sensitivity: Exponent controlling how WM weight shrinks with set size; wm_eff = wm_weight * (3 / nS) ** wm_size_sensitivity

    Set-size effects:
    - As set size increases (from 3 to 6), wm_eff decreases according to wm_size_sensitivity, capturing reduced WM control
      under higher load.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_size_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        # Effective WM mixture weight for this block given its set size
        wm_eff = wm_weight * (3.0 / float(nS)) ** wm_size_sensitivity
        wm_eff = np.clip(wm_eff, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: softmax over W_s with a very high beta
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r >= 1.0:
                # On rewarded trial: one-shot store the correct action
                w[s, :] = (1.0 - wm_decay) * w[s, :]  # small stabilization before overwrite
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On non-rewarded trial: decay toward uniform template
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with LRU (least-recently-used) eviction and lapse noise.

    Mechanism:
    - RL: Standard RW update with lr and softmax(beta).
    - WM: A discrete associative memory that can hold up to K unique states per block.
          When r=1, store the mapping for that state -> chosen action as a one-hot vector.
          If memory is full and a new state is stored, evict the least-recently-used state.
          If a state is not in WM, WM policy defaults to uniform.
    - Mixture: Weighted sum of WM and RL policies (wm_weight). A small lapse epsilon adds uniform noise to the final policy.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight on WM policy (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - K_capacity: WM capacity (real-valued, rounded/clipped to [0,6])
    - epsilon_lapse: Lapse probability mixed with uniform, applied after RL/WM mixture (0..0.2 suggested)

    Set-size effects:
    - When set size nS > K_capacity, some states cannot be held in WM and default to uniform WM policy, shifting control to RL.
      Thus performance degrades in larger set size blocks when K_capacity is relatively small.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, K_capacity, epsilon_lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM table used only for stored states
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity and LRU bookkeeping
        K = int(np.clip(np.round(K_capacity), 0, nS))
        in_memory = np.zeros(nS, dtype=bool)
        last_used_age = np.zeros(nS)  # 0 means most recently used; higher means older

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Age all entries
            last_used_age[in_memory] += 1.0

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            if in_memory[s]:
                W_s = w[s, :]
                denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = 1.0 / denom_wm
                # refresh recency
                last_used_age[s] = 0.0
            else:
                # Not in WM: WM policy is uniform
                p_wm = 1.0 / nA

            # Mixture then lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store only on rewarded trials
            if r >= 1.0:
                if not in_memory[s]:
                    # Need to insert; if capacity full, evict oldest
                    if np.sum(in_memory) >= K and K > 0:
                        # Evict state with largest age
                        candidates = np.where(in_memory)[0]
                        oldest_idx = candidates[np.argmax(last_used_age[candidates])]
                        in_memory[oldest_idx] = False
                        w[oldest_idx, :] = w_0[oldest_idx, :]

                    if K > 0:
                        in_memory[s] = True
                        w[s, :] = 0.0
                        w[s, a] = 1.0
                        last_used_age[s] = 0.0
                else:
                    # Refresh and overwrite mapping to the rewarded action
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                    last_used_age[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with perseveration bias + leaky WM, both modulated by set size.

    Mechanism:
    - RL: RW update with lr and softmax(beta_eff). The effective RL temperature is reduced under higher set size:
          beta_eff = (softmax_beta * 10) * (3 / nS) ** temp_size_sensitivity.
      Includes a perseveration bias kappa that adds to the last chosen action's value in that state.
    - WM: Leaky associative matrix w with decay toward uniform at rate wm_forget; on each trial:
          w[s, :] = (1 - wm_forget) * w[s, :] + wm_forget * uniform; then add r to the chosen action's WM trace and renormalize.
          WM policy is softmax with high beta_wm.
    - Mixture: WM mixture weight also declines with set size:
          wm_eff = wm_weight * (3 / nS) ** wm_size_sensitivity.

    Parameters:
    - lr: RL learning rate
    - wm_weight: Baseline WM mixture weight (at set size 3)
    - softmax_beta: Base RL inverse temperature (scaled internally by 10)
    - temp_size_sensitivity: Exponent controlling how beta shrinks with set size
    - perseveration: Additive bias applied to the last chosen action in a state for the RL policy
    - wm_forget: WM leak toward uniform (0..1)

    Set-size effects:
    - Larger set sizes reduce both WM influence (via wm_size_sensitivity) and RL determinism (via temp_size_sensitivity),
      capturing broader cognitive load effects.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, temp_size_sensitivity, perseveration, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Keep track of last chosen action per state for perseveration
        last_a = -1 * np.ones(nS, dtype=int)

        # Set-size modulations
        beta_eff_factor = (3.0 / float(nS)) ** temp_size_sensitivity
        wm_eff = wm_weight * (3.0 / float(nS)) ** temp_size_sensitivity  # reuse sensitivity or we could separate
        wm_eff = np.clip(wm_eff, 0.0, 1.0)
        beta_eff_base = softmax_beta * beta_eff_factor

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_a[s] >= 0:
                Q_s[last_a[s]] += perseveration

            denom_rl = np.sum(np.exp(beta_eff_base * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: leaky accumulation toward uniform + add reward to chosen trace
            # Compute current WM policy based on current w (before update)
            W_s = w[s, :]

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: leak toward uniform then add reward evidence to chosen action and renormalize
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            if r > 0.0:
                w[s, a] += r  # add reward evidence
            # Renormalize row to keep scale comparable
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

            # Update perseveration memory
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p