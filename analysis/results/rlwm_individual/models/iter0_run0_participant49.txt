def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited working memory (WM) mixture model.

    Idea:
    - Choices come from a mixture of a slow RL system and a fast WM system.
    - WM stores the most recently rewarded action per state with decay.
    - WM contribution is reduced under higher set size due to a limited capacity K and load sensitivity phi.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: Base WM mixture weight in [0,1]
    - softmax_beta: RL inverse temperature (>0); internally scaled by 10 for range
    - wm_capacity: Effective WM capacity K (1..6), down-weights WM when nS > K
    - wm_decay: WM decay parameter in [0,1], controls how quickly WM traces decay toward uniform
    - wm_load_sensitivity: Exponent phi >= 0, how sharply WM weight declines with set size (mixture scaled by (K/nS)^phi)

    Set-size dependence:
    - Effective mixture weight per block: wm_weight_block = wm_weight * min(1, (wm_capacity / nS) ** wm_load_sensitivity)
      so larger nS reduces the WM contribution.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_capacity, wm_decay, wm_load_sensitivity = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy as in template

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # capacity- and load-adjusted WM weight for this block
        cap_ratio = min(1.0, max(1e-8, wm_capacity) / float(nS))
        wm_weight_block = wm_weight * (cap_ratio ** max(0.0, wm_load_sensitivity))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax on WM values (nearly deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If reward, store chosen action strongly (replace/overwrite).
            # - If no reward, decay WM trace for this state toward uniform.
            if r > 0.5:
                # move toward one-hot of chosen action with strength wm_decay (interpreted as consolidation strength)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
            else:
                # decay toward uniform baseline when not rewarded
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning, perseveration bias, and recency-based WM.
    
    Idea:
    - RL has separate learning rates for positive and negative prediction errors.
    - A perseveration (choice stickiness) bias favors repeating the last action in a state.
    - WM stores the most recent action per state with recency decay; WM retrieval becomes noisier with larger set size.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE in [0,1]
    - lr_neg: RL learning rate for negative PE in [0,1]
    - wm_weight: Base WM mixture weight in [0,1]
    - softmax_beta: RL inverse temperature (>0); internally scaled by 10
    - load_noise: Increases WM retrieval noise with set size; beta_wm_block = 50 / (1 + load_noise*(nS-3))
    - perseveration: Stickiness added to last chosen action in a state (in value units)

    Set-size dependence:
    - WM softmax temperature decreases with nS via load_noise, making WM less precise at larger set sizes.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, load_noise, perseveration = model_parameters

    softmax_beta *= 10.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # track last action per state for perseveration
        last_action = -np.ones(nS, dtype=int)

        # WM precision decreases with load
        softmax_beta_wm = 50.0 / (1.0 + max(0.0, load_noise) * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add perseveration bias to RL values
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            # RL policy probability of chosen action (with bias)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - q[s, a]
            lr_eff = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_eff * pe

            # WM recency update: move toward one-hot of current action
            lam = 0.7  # implicit recency strength; ensure it's influenced: tie to load_noise slightly to use parameter meaningfully?
            # To use load_noise meaningfully beyond WM beta, modulate lam modestly with load:
            lam = np.clip(0.7 / (1.0 + 0.2 * max(0.0, load_noise) * max(0, nS - 3)), 0.1, 0.95)
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - lam) * w[s, :] + lam * one_hot

            # update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting and entropy-based arbitration with WM subject to load-dependent interference.

    Idea:
    - RL values decay toward uniform (forgetting), capturing interference and limited retention.
    - Arbitration weight favoring WM increases when RL is uncertain (high entropy) and decreases with larger set size.
    - WM stores rewarded actions; larger set sizes introduce interference that pushes WM toward uniform.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature (>0); internally scaled by 10
    - wm_weight_base: Base WM mixture weight in [0,1]
    - ent_slope: Scales the influence of RL entropy on WM weight (>=0)
    - rl_forget: RL forgetting rate in [0,1], decays Q toward uniform each trial on visited state
    - wm_interference: Load-dependent WM interference (>=0); effective blending toward uniform scales with nS

    Set-size dependence:
    - WM mixture weight is reduced as nS increases (via interference) and increased with higher RL uncertainty.
    - WM distribution is blended toward uniform by wm_interference * (nS-3)/3.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, ent_slope, rl_forget, wm_interference = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load factor for WM interference
        load_factor = max(0.0, (nS - 3) / 3.0)  # 0 at nS=3, 1 at nS=6

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL uncertainty (entropy of softmax over Q_s)
            probs_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs_rl /= np.sum(probs_rl)
            entropy_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, 1e-12, 1.0))) / np.log(nA)  # normalized in [0,1]

            # Arbitration: increase WM contribution with RL entropy, decrease with load
            wm_weight_dyn = wm_weight_base * (1.0 + ent_slope * entropy_rl)
            wm_weight_dyn = wm_weight_dyn / (1.0 + wm_interference * load_factor)
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL forgetting toward uniform on the visited state, then standard update
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: store rewarded action; interference blends toward uniform with load
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # consolidate toward one-hot
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot
            # apply load-dependent interference drift toward uniform
            alpha_int = np.clip(wm_interference * load_factor, 0.0, 1.0)
            w[s, :] = (1.0 - alpha_int) * w[s, :] + alpha_int * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)