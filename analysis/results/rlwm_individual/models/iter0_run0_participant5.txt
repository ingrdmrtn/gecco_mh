def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity- and noise-limited working memory.
    
    The model mixes a delta-rule RL system with a capacity-limited working-memory (WM) system.
    WM acts like a fast, near-deterministic store of recently rewarded stimulus-action mappings,
    but its effective precision declines with set size (capacity limit) and includes additional noise.
    
    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for updating Q-values after reward prediction errors.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in the final action probability (1-wm_weight for RL).
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10 to span higher upper bounds).
        - wm_decay: float in [0,1]
            Trialwise decay of WM values toward uniform; larger means faster forgetting.
        - wm_capacity: float
            Effective WM capacity (in units of set size). Larger values mitigate set-size degradation.
        - wm_noise: float in [0,1]
            Probability of uniformly random responding within the WM channel (captures internal WM noise).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity, wm_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM precision declines with set size via a capacity-like scaling:
            # wm_scale ~ 1 when nS << wm_capacity, and declines when nS > wm_capacity.
            wm_scale = 1.0 / (1.0 + np.exp((nS - wm_capacity)))  # in (0,1)
            beta_wm_eff = softmax_beta_wm * wm_scale
            # Core WM softmax policy
            p_wm_core = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            # Add WM channel noise mixing with uniform
            p_wm = (1 - wm_noise) * p_wm_core + wm_noise * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM towards uniform
            w[s,:] = (1 - wm_decay) * w[s,:] + wm_decay * w_0[s,:]
            # If rewarded, store chosen action as the likely correct mapping (one-shot-like update)
            if r > 0:
                w[s,:] = (1 - wm_scale) * w[s,:] + wm_scale * w_0[s,:]  # small re-centering with scale
                w[s,:] = (1 - wm_scale) * w[s,:]
                w[s,a] += wm_scale  # push probability mass to chosen action
            # Keep WM probabilities normalized and bounded
            w[s,:] = np.maximum(w[s,:], 1e-8)
            w[s,:] = w[s,:] / np.sum(w[s,:])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (asymmetric learning) + WM (recency-and-reward) mixture with set-size-sensitive WM precision.
    
    RL uses separate learning rates for positive and negative prediction errors.
    WM acts as a fast recency-weighted store that shifts to a one-hot mapping on rewarded trials,
    and drifts toward uniform on non-rewarded trials. WM precision is reduced at larger set sizes.
    
    Parameters
    ----------
    model_parameters: tuple
        - lr_pos: float
            RL learning rate for positive prediction errors (r - Q > 0).
        - lr_neg: float
            RL learning rate for negative prediction errors (r - Q < 0).
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in the final action probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - alpha_wm: float in [0,1]
            WM recency update rate for chosen action; controls how quickly WM adapts.
        - size_threshold: float
            Set-size threshold controlling where WM precision transitions from high to low.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, alpha_wm, size_threshold = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM precision declines when nS exceeds size_threshold via a sigmoid
            wm_precision = 1.0 / (1.0 + np.exp(nS - size_threshold))  # in (0,1)
            beta_wm_eff = softmax_beta_wm * wm_precision
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            if delta >= 0:
                q[s][a] += lr_pos*delta
            else:
                q[s][a] += lr_neg*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Recency-and-reward WM:
            # Move toward one-hot on rewarded trials; drift toward uniform on non-rewarded
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s,:]  # drift back to uncertainty on errors
            w[s,:] = (1 - alpha_wm) * w[s,:] + alpha_wm * target
            # Keep normalized
            w[s,:] = np.maximum(w[s,:], 1e-8)
            w[s,:] = w[s,:] / np.sum(w[s,:])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM (win-stay/lose-devalue) with set-size-dependent forgetting.
    
    RL learns with a single learning rate. WM implements a win-stay/lose-devalue rule:
    - On reward: increase WM weight on chosen action.
    - On no reward: devalue chosen action and drift toward uniform.
    WM forgets faster at larger set sizes (set-size-dependent decay).
    
    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for Q-values.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in the final action probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - wsls_gain: float in (0,1]
            Magnitude of WM update toward/away from the chosen action (win-stay/lose-devalue strength).
        - base_decay: float in [0,1]
            Baseline WM decay toward uniform per trial.
        - size_slope: float >= 0
            Additional WM decay per unit increase in set size (controls load sensitivity).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wsls_gain, base_decay, size_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Set-size-dependent forgetting reduces effective WM precision in larger sets via decay
            # We implement this indirectly by attenuating W discriminability using effective beta
            # proportional to the complement of decay.
            decay = np.clip(base_decay + size_slope * (nS - 1), 0.0, 1.0)
            beta_wm_eff = softmax_beta_wm * (1.0 - decay)
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # First apply decay toward uniform (stronger decay for larger nS)
            w[s,:] = (1 - decay) * w[s,:] + decay * w_0[s,:]
            # Then apply win-stay / lose-devalue
            if r > 0:
                # Increase chosen action weight
                w[s,a] = w[s,a] + wsls_gain * (1.0 - w[s,a])
                # Slightly reduce others to keep normalization stable before renorm
                others = np.arange(w.shape[1]) != a
                w[s,others] = np.maximum(w[s,others] - wsls_gain * (w[s,others]), 1e-8)
            else:
                # Devalue chosen action toward uniform; softly boost others
                w[s,a] = (1 - wsls_gain) * w[s,a] + wsls_gain * (1.0 / nA)
                others = np.arange(w.shape[1]) != a
                # distribute mass to others slightly
                w[s,others] = (1 - 0.5*wsls_gain) * w[s,others] + (0.5*wsls_gain) * ((1.0 - 1.0/nA) / (nA - 1))
            # Normalize and bound
            w[s,:] = np.maximum(w[s,:], 1e-8)
            w[s,:] = w[s,:] / np.sum(w[s,:])

        blocks_log_p += log_p

    return -blocks_log_p