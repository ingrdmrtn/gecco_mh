def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM) mixture model.

    Idea:
    - Choices are a mixture of model-free RL and a one-shot WM store for rewarded S-A pairs.
    - WM contribution is capacity-limited: weight scales down as set size increases beyond capacity.
    - WM traces decay over time toward a uniform prior.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: base mixture weight for WM (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally).
    - wm_decay: decay rate of WM toward uniform (0..1) applied each trial.
    - wm_capacity: effective WM capacity (e.g., around 3); scales WM weight by min(1, K / set_size).
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy as softmax over WM values (deterministic toward stored correct action)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Capacity-scaled WM mixture weight
            wm_weight_eff = wm_weight * min(1.0, wm_capacity / nS)
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global WM decay toward uniform prior
            w = (1 - wm_decay) * w + wm_decay * w_0
            # One-shot storage of rewarded mapping: make chosen action highly preferred
            if r > 0:
                # Reset state row toward baseline then set chosen to 1
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + win-stay/lose-shift working memory, load-sensitive arbitration.

    Idea:
    - RL updates use separate learning rates for positive vs negative outcomes.
    - WM is a fast "win-stay" cache: on reward, store a peaked distribution over the chosen action;
      on no reward, remove preference (shift toward uniform).
    - Arbitration weight for WM decreases with set size via a sigmoid load-sensitivity.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight: base WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally).
    - wm_conf: confidence of WM store (0..1). Determines how peaked WM distribution is when storing.
    - wm_load_sensitivity: slope parameter controlling how WM weight decreases with set size (can be +/-).
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_conf, wm_load_sensitivity = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # not used directly in this model's WM policy; retained to respect template
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        # w stores a probability distribution per state implementing the WM policy directly
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy uses the stored distribution directly (win-stay / lose-shift)
            p_wm = max(W_s[a], 1e-12)
            # Load-sensitive arbitration: WM weight decreases with larger set sizes
            # Sigmoid around midpoint 4.5 to separate 3 vs 6 conditions
            wm_weight_eff = wm_weight / (1.0 + np.exp(wm_load_sensitivity * (nS - 4.5)))
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0 else lr_neg
            q[s][a] += lr * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Win-stay: on reward, store a peaked distribution on chosen action
            if r > 0:
                w[s, :] = (1 - wm_conf) / nA
                w[s, a] = wm_conf + (1 - wm_conf) / nA
            else:
                # Lose-shift: remove preference by moving toward uniform (no favored action)
                w[s, :] = (1 - wm_conf) / nA  # flattens preference

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + retrieval-based WM with lapse and capacity-limited availability.

    Idea:
    - RL includes passive forgetting toward uniform values each trial.
    - WM stores the last rewarded action per state as a near-deterministic distribution
      but suffers from lapses and limited retrieval probability that scales with set size.
    - Arbitration weight equals base WM weight scaled by capacity ratio (K / set size).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_base_weight: base WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally).
    - rl_forgetting: RL forgetting rate toward uniform (0..1) applied each trial.
    - wm_capacity: WM capacity K controlling set-size scaling of WM contribution.
    - wm_lapse: lapse/noise in WM retrieval; stored WM policy assigns 1 - lapse to stored action,
                and lapse mass distributed over other actions (0..1).
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_base_weight, softmax_beta, rl_forgetting, wm_capacity, wm_lapse = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # not used directly; WM policy is a fixed distribution with lapse
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        # w stores a probability distribution that encodes WM policy with lapse
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy is retrieved with capacity-limited weight; p_wm is the stored probability
            p_wm = max(W_s[a], 1e-12)
            wm_weight_eff = wm_base_weight * min(1.0, wm_capacity / nS)
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Store rewarded action with lapse; unrewarded trials mildly relax memory
            if r > 0:
                # Assign 1 - lapse to chosen action, lapse spread across others
                w[s, :] = wm_lapse / (nA - 1)
                w[s, a] = 1.0 - wm_lapse
            else:
                # Without reward, partially relax toward uniform to reflect uncertainty
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

            # RL forgetting toward uniform each trial (applied globally)
            q = (1 - rl_forgetting) * q + rl_forgetting * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p