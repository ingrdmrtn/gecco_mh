def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM).

    Mixture policy:
      p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl

    - RL component uses softmax over Q-values with inverse temperature softmax_beta (scaled by 10).
    - WM component is a rapidly updating map for state->action with near-deterministic retrieval,
      but with an effective capacity limit and decay:
        - Determinism is high (softmax_beta_wm = 50), yet p_wm is blended with uniform according
          to an effective capacity factor cap = min(1, K/nS) to reflect reduced WM control under
          higher set sizes (load).
        - WM traces decay toward a neutral baseline w_0 with rate rho each trial.
        - On rewarded trials, WM moves toward a one-hot mapping for the chosen action with strength eta_wm.

    Parameters (model_parameters):
      - lr: RL learning rate (0..1)
      - wm_weight: mixture weight of WM vs RL in policy (0..1)
      - softmax_beta: inverse temperature for RL softmax; internally multiplied by 10 (>0)
      - K: working memory capacity in number of items (1..6); controls cap = min(1, K/nS)
      - rho: WM decay rate toward baseline per trial (0..1)
      - eta_wm: WM encoding strength toward one-hot after reward (0..1)

    Inputs:
      - states: array of state indices per trial
      - actions: array of chosen action indices per trial (0..2)
      - rewards: array of rewards per trial (0 or 1)
      - blocks: array of block indices per trial
      - set_sizes: array of set size for the current block/trial (3 or 6)
      - model_parameters: list/tuple with parameters as specified above

    Returns:
      - Negative log-likelihood of observed choices under the model.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, K, rho, eta_wm = parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity factor (reduced WM control in larger set sizes)
        cap = np.clip(float(K) / max(1.0, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM policy for chosen action (softmax with high beta)
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Blend WM with uniform by capacity factor (lower cap => more uniform)
            p_wm = cap * p_wm_det + (1.0 - cap) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward baseline
            w = (1.0 - rho) * w + rho * w_0

            # Reward-driven encoding: move the state's WM row toward a one-hot for chosen action
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + error-sensitive WM with load-dependent lapse.

    Mixture policy:
      p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl

    - RL component uses a standard softmax on Q-values (beta scaled by 10).
    - WM component is near-deterministic but subject to a load-dependent lapse:
        p_wm = (1 - epsilon(nS)) * p_wm_det + epsilon(nS) * 1/nA
      where epsilon(nS) = np.clip(epsilon0 * (nS - 1) / 5, 0, 1).
      Thus, higher set size increases the probability of a uniform (noisy) WM policy.

    WM updates are error-sensitive:
      - On reward r=1: move WM for the state toward a one-hot for the chosen action with strength eta_pos.
      - On reward r=0: push away from the chosen action (and slightly boost others) with strength eta_neg.

    Parameters (model_parameters):
      - lr: RL learning rate (0..1)
      - wm_weight: mixture weight of WM vs RL (0..1)
      - softmax_beta: RL inverse temperature; internally scaled by 10 (>0)
      - epsilon0: lapse gain controlling how fast WM lapses with load (>=0)
      - eta_pos: WM learning strength on rewarded trials (0..1)
      - eta_neg: WM repulsion strength on non-rewarded trials (0..1)

    Inputs:
      - states, actions, rewards, blocks, set_sizes as defined in model1
      - model_parameters: list/tuple with the parameters specified above

    Returns:
      - Negative log-likelihood of observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, epsilon0, eta_pos, eta_neg = parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent lapse
        epsilon_load = np.clip(epsilon0 * max(0, nS - 1) / 5.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - epsilon_load) * p_wm_det + epsilon_load * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r >= 0.5:
                # Move toward chosen action (one-hot)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_pos) * w[s, :] + eta_pos * one_hot
            else:
                # Push away from chosen action: decrease chosen, increase others
                decrease = eta_neg * w[s, a]
                w[s, a] = max(0.0, w[s, a] - decrease)
                # Redistribute the removed mass to the other actions
                if nA > 1:
                    add_each = decrease / (nA - 1)
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] += add_each
                # Renormalize row to sum to 1 to maintain a valid preference distribution
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference across states and load-dependent reliability.

    Mixture policy:
      p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl

    - RL component: standard softmax on Q-values with beta*10.
    - WM component:
        p_wm = (1 - psi(nS)) * p_wm_det + psi(nS) * 1/nA
      where psi(nS) = 1 - exp(-gamma / nS). Larger sets increase psi, blending WM toward uniform.

    WM updates include cross-state interference:
      - On each rewarded trial in state s and action a, WM for that state moves toward a one-hot
        with strength eta_wm.
      - Additionally, a small interference fraction chi spreads to other states within the block,
        nudging their WM toward the same chosen action. This interference is stronger, in effect,
        when there are more states, capturing load-induced confusion.

    Parameters (model_parameters):
      - lr: RL learning rate (0..1)
      - wm_weight: mixture weight of WM vs RL (0..1)
      - softmax_beta: RL inverse temperature; internally scaled by 10 (>0)
      - eta_wm: WM learning strength toward one-hot on reward (0..1)
      - chi: cross-state interference weight (0..1)
      - gamma: load-sensitivity controlling WM blending toward uniform with set size (>=0)

    Inputs:
      - states, actions, rewards, blocks, set_sizes as defined above
      - model_parameters: list/tuple with the parameters specified above

    Returns:
      - Negative log-likelihood of observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, eta_wm, chi, gamma = parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM unreliability parameter
        psi = 1.0 - np.exp(-float(gamma) / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - psi) * p_wm_det + psi * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r >= 0.5:
                # Primary WM update for the current state
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * one_hot

                # Cross-state interference: spread a fraction chi toward the chosen action
                if chi > 0.0 and nS > 1:
                    spill = chi * eta_wm
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        w[s_other, :] = (1.0 - spill) * w[s_other, :]
                        w[s_other, a] += spill  # nudge toward the chosen action
                        # Renormalize each affected state's WM row
                        row_sum = np.sum(w[s_other, :])
                        if row_sum > 0:
                            w[s_other, :] /= row_sum
                        else:
                            w[s_other, :] = w_0[s_other, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p