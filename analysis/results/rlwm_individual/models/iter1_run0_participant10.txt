def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited working memory (slots) with decay.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores a sharp mapping for rewarded state-action pairs, but its influence
      is capacity-limited by a slot-like mechanism: effective WM weight scales with K_slots / set_size.
    - WM traces decay toward uniform at each visit when feedback is not rewarding.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base mixture weight for WM contribution (0..1)
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - K_slots: WM capacity in "state slots" (0..6). Effective WM weight scales as min(1, K_slots / set_size).
    - wm_decay: WM decay toward uniform per visit (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, K_slots, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-limited WM weight for this block
        cap_factor = min(1.0, max(0.0, float(K_slots) / float(nS)))
        wm_weight_block = wm_weight_base * cap_factor

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: sharp distribution over current WM weights
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture of WM and RL with capacity scaling
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Decay toward uniform when unrewarded
            # - On reward, commit a sharp one-hot memory for the chosen action
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Normalize to avoid numerical drift
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice stickiness + WM down-weighted by set size and recency-based decay.

    Idea:
    - RL uses a single learning rate but includes a state-wise choice stickiness (perseveration) bias.
      The previous action in a state gains a bias kappa in the softmax.
    - WM contributes a policy derived from a decaying memory trace. WM weight is reduced in larger set sizes
      via a divisive interference term.
    - WM traces decay toward uniform at each visit; reward sets a sharp one-hot trace.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight (0..1)
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - kappa: Choice stickiness bias added to the last chosen action in a state (can be negative/positive)
    - lambda_wm: WM decay toward uniform per visit (0..1)
    - rho: Set-size interference strength scaling WM weight (>=0). Effective WM weight = base / (1 + rho*(nS-3)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, kappa, lambda_wm, rho = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Last action per state for stickiness (-1 indicates none yet)
        last_a = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM weight attenuation
        wm_weight_block = wm_weight_base / (1.0 + rho * max(0.0, float(nS - 3)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Choice stickiness bias vector for this state
            bias = np.zeros(3)
            if last_a[s] >= 0:
                bias[last_a[s]] += kappa

            # RL policy with stickiness: Q + bias
            denom_rl = np.sum(np.exp(softmax_beta * ((Q_s + bias) - (Q_s[a] + bias[a]))))
            p_rl = 1.0 / denom_rl

            # WM policy based on current WM weights
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform on every visit; reward sets a sharp trace
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * w_0[s, :]

            # Normalize
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Update stickiness memory
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + surprise-gated WM and set-size-scaled WM precision.

    Idea:
    - RL uses a single learning rate.
    - WM policy weight is dynamically reduced by (a) larger set sizes and (b) higher absolute prediction error (surprise).
      Intuition: surprising outcomes indicate unreliable WM mapping, shifting control to RL.
    - WM precision (inverse temperature) also scales down with set size.
    - WM traces decay toward uniform; rewarded outcomes write a sharp one-hot memory.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight (0..1)
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - phi_pe: Surprise sensitivity (>0). WM weight is multiplied by exp(-phi_pe * |PE|).
    - wm_beta_scale: Scales WM inverse temperature before set-size adjustment (>0).
    - wm_decay: WM decay toward uniform on unrewarded visits (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, phi_pe, wm_beta_scale, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size scaling factor for WM influence and precision (smaller when set size is larger)
        size_factor = 3.0 / float(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute current PE magnitude (pre-update) to gate WM weight
            pe_abs = abs(r - Q_s[a])

            # WM precision decreases with set size; scaled by wm_beta_scale
            eff_beta_wm = max(1.0, softmax_beta_wm * wm_beta_scale * size_factor)

            denom_wm = np.sum(np.exp(eff_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Dynamic WM weight: base scaled by set size and surprise
            wm_weight_dyn = wm_weight_base * size_factor * np.exp(-phi_pe * pe_abs)
            wm_weight_dyn = max(0.0, min(1.0, wm_weight_dyn))

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Reward: commit a sharp mapping
            # - No reward: decay toward uniform
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p