def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM mix scaled by load + RL forgetting

    Description:
    - RL uses separate learning rates for positive/negative prediction errors and a forgetting
      process that drifts Q-values toward uniform each trial.
    - WM stores a one-shot, reward-gated action association per state (deterministic, via softmax_beta_wm).
    - Arbitration mixes WM and RL policies with a load-sensitive WM weight:
        wm_mix = wm_weight * (3.0 / nS)**gamma
      so WM influence declines as set size increases when gamma > 0.

    Parameters (tuple):
    - alpha_pos: RL learning rate for positive prediction errors (0..1)
    - alpha_neg: RL learning rate for negative prediction errors (0..1)
    - wm_weight: Base WM mixture weight (0..1) before load scaling
    - softmax_beta: Inverse temperature base for RL; internally scaled by 10
    - rho: RL forgetting rate toward uniform (0..1) applied each trial
    - gamma: Load sensitivity exponent for WM mixture (>0 increases WM reduction at higher nS)

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, rho, gamma = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-sensitive WM mixture
        wm_mix = wm_weight * (3.0 / float(nS))**gamma
        wm_mix = np.clip(wm_mix, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy: softmax over WM values
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with asymmetric learning rates and forgetting
            pe = r - q[s, a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe
            # Forgetting toward uniform across all actions in the visited state
            q[s, :] = (1.0 - rho) * q[s, :] + rho * (1.0 / nA)

            # WM update: reward-gated one-shot encoding with gentle decay toward uniform
            # Move all actions slightly toward uniform, then assign mass to chosen action based on reward
            decay_wm = 0.1  # implicit fast WM dynamics; kept fixed to keep parameter count <= 6
            w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]
            target = np.zeros(nA)
            target[a] = r
            w[s, :] += 1.0 * (target - w[s, :])  # one-shot overwrite when r=1, partial when r=0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load-amplified WM decay

    Description:
    - RL: single learning rate Q-learning; standard softmax choice.
    - WM: associative store that decays toward uniform; its decay is amplified by set size,
      capturing interference in larger loads.
    - Arbitration: WM weight is reduced on trials with high WM uncertainty using normalized
      entropy of the WM action distribution in the current state:
        H_norm = -sum(p*log p)/log(nA); wm_effective = wm_weight * (1 - H_norm)
      Thus, when WM for a state is peaked (low entropy), it dominates; when flat (high entropy),
      RL dominates.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture (0..1)
    - softmax_beta: Inverse temperature for RL; internally scaled by 10
    - phi: Load amplification of WM decay (>=0); higher phi increases WM interference as nS grows
    - wm_decay: Base WM decay toward uniform (0..1)
    - rho_wm_reset: Event-based WM consolidation after reward (0..1); higher means stronger push to the rewarded action

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi, wm_decay, rho_wm_reset = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-amplified WM decay: effective decay increases with set size
        wm_decay_eff = 1.0 - (1.0 - wm_decay) ** (1.0 + phi * max(0, nS - 1))
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Entropy-based arbitration for this state
            p_w = np.maximum(eps, W_s / np.sum(W_s))
            H = -np.sum(p_w * np.log(p_w))
            H_norm = H / np.log(nA)
            wm_mix = wm_weight * (1.0 - np.clip(H_norm, 0.0, 1.0))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            # Reward-gated consolidation to the chosen action
            target = np.zeros(nA)
            target[a] = 1.0
            w[s, :] += rho_wm_reset * r * (target - w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent decision noise + capacity-limited WM (state slot model)

    Description:
    - RL: standard Q-learning; inverse temperature decreases with load:
        beta_eff = (softmax_beta*10) / (1 + beta_scale*(nS - 3))
      capturing noisier RL under higher load.
    - WM: capacity-limited store over states. Only up to K states are actively maintained in WM
      within a block; others revert to uniform. When a rewarded trial occurs, the state is
      refreshed/added to the active WM set; if capacity exceeded, evict the least-recently
      refreshed state (LRU). WM values for active states are sharp; inactive states remain flat.
    - Arbitration: fixed wm_weight mixture between WM and RL.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight of WM (0..1)
    - softmax_beta: Base inverse temperature for RL; internally scaled by 10
    - beta_scale: Load sensitivity for RL noise (>=0); larger lowers beta at higher nS
    - cap_fraction: Fraction of states maintainable in WM (0..1); K = max(1, round(1 + cap_fraction*(nS - 1)))

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, beta_scale, cap_fraction = model_parameters

    softmax_beta_base = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        beta_eff = softmax_beta_base / (1.0 + beta_scale * max(0, nS - 3))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity for this block
        K = int(np.clip(np.round(1.0 + cap_fraction * (nS - 1)), 1, nS))
        # Track active WM states with recency timestamps; -1 means inactive
        last_refresh = -1 * np.ones(nS, dtype=int)
        t_global = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with load-dependent beta
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM capacity management
            # If rewarded, (re)store the state with a sharp association; else gently relax toward uniform
            if r > 0.0:
                # Ensure capacity: if adding this state would exceed K active states, evict LRU
                active_mask = (last_refresh >= 0)
                if not active_mask[s]:
                    num_active = int(np.sum(active_mask))
                    if num_active >= K:
                        # Evict the least-recently refreshed active state
                        lru_state = np.argmin(np.where(active_mask, last_refresh, np.inf))
                        last_refresh[lru_state] = -1
                        w[lru_state, :] = w_0[lru_state, :].copy()
                # Store/refresh this state's WM representation to the chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target
                last_refresh[s] = t_global
            else:
                # No reward: let WM for this state drift toward uniform
                drift = 0.2
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

            t_global += 1

        blocks_log_p += log_p

    return -blocks_log_p