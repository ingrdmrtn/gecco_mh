def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size-dependent precision, decay, and reinforcement-driven encoding.

    Policy:
    - RL softmax with inverse temperature softmax_beta*10 (as in template).
    - WM softmax with an effective temperature that degrades with set size: beta_wm_eff = softmax_beta_wm * min(1, wm_capacity / nS).
      Larger set sizes lower WM precision.
    - Mixture policy is given by the template line:
        p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl

    WM dynamics:
    - Global decay toward uniform with rate that increases with set size: decay_eff = wm_decay * (nS / max(1, wm_capacity)).
    - Rewarded trials encode stronger: w[s] moves toward a one-hot on the chosen action with learning rate wm_eta.
    - Unrewarded trials encode weakly: w[s] moves slightly toward the chosen action with rate 0.2*wm_eta.
    - Row normalization maintains w[s] as a probability vector.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_capacity : float
            Effective WM capacity in number of items; caps WM precision improvements up to this capacity.
        wm_decay : float
            Base decay rate of WM toward uniform (0-1).
        wm_eta : float
            WM learning rate toward the chosen (presumed-correct) action (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity, wm_decay, wm_eta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic when capacity is not limiting
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        # Set-size dependent WM precision and decay
        cap_ratio = min(1.0, max(0.0, wm_capacity / max(1.0, nS)))
        beta_wm_eff = softmax_beta_wm * cap_ratio
        decay_eff = wm_decay * (nS / max(1.0, wm_capacity))

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax with capacity-limited precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating: leak toward uniform + reward-modulated encoding
            # Global decay
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # Local encoding toward chosen action
            if r > 0.0:
                alpha = wm_eta
            else:
                alpha = 0.2 * wm_eta  # weaker move on non-reward
            w[s, :] = (1.0 - alpha) * w[s, :]
            w[s, a] += alpha

            # Normalize state row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with choice-trace bias in WM and set-size-dependent interference.

    Policy:
    - RL softmax with inverse temperature softmax_beta*10.
    - WM policy is a softmax over (W_s + kappa * Z_s), where Z_s is a recency/choice-trace for that state.
      This creates a perseveration/recency bias expressed via the WM system.
    - Mixture policy follows the template.

    WM dynamics:
    - Interference/decay increases with set size: decay_eff = wm_decay * (nS / 3).
    - Encoding rate wm_eta toward the chosen action; reward enhances encoding (1.0x) vs nonreward (0.3x).
    - Z (choice trace) is updated with eligibility-like dynamics: Z decays by z_decay and the chosen action gets a boost z_eta.
      Z is bounded to be non-negative and row-normalized for stability.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        kappa : float
            Strength of the WM-expressed choice-trace bias (>=0).
        wm_decay : float
            Base WM decay toward uniform (0-1).
        wm_eta : float
            WM learning rate toward chosen action (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa, wm_decay, wm_eta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Choice trace Z
        Z = np.zeros((nS, nA))

        log_p = 0
        # Set-size dependent decay
        decay_eff = wm_decay * (nS / 3.0)
        z_decay = min(1.0, 0.3 + 0.1 * (nS - 3))  # slightly faster trace decay for larger sets
        z_eta = 1.0  # fixed magnitude for trace boost on chosen action

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy with choice-trace bias
            logits_wm = W_s + kappa * Z[s, :]
            # Implement softmax via difference trick reusing template structure
            # Compute p(choice=a) under WM-softmax
            denom = np.sum(np.exp(softmax_beta_wm * (logits_wm - logits_wm[a])))
            p_wm = 1.0 / denom if denom > 0 else 1.0 / nA

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform (set-size dependent)
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # Reward-modulated encoding
            alpha = wm_eta if r > 0.0 else 0.3 * wm_eta
            w[s, :] = (1.0 - alpha) * w[s, :]
            w[s, a] += alpha
            # Normalize
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs

            # Update choice trace Z
            Z = (1.0 - z_decay) * Z
            Z[s, a] += z_eta
            # Keep non-negative and normalize row for stability
            Z[s, :] = np.maximum(Z[s, :], 0.0)
            ssum = np.sum(Z[s, :])
            if ssum > 0:
                Z[s, :] /= ssum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with retrieval failures that increase with set size, plus deterministic WM encoding on reward.

    Policy:
    - RL softmax with inverse temperature softmax_beta*10.
    - WM policy blends a near-deterministic WM softmax with a uniform lapse reflecting retrieval failure:
        p_wm = (1 - p_fail) * softmax_beta_wm(W_s) + p_fail * (1/nA)
      where p_fail grows with set size via a logistic transform of (nS - 3).

    WM dynamics:
    - On reward, WM for that state is strongly overwritten toward the chosen action with rate alpha_wm.
    - On no-reward, a weak move toward the chosen action (0.2*alpha_wm) captures hypothesis testing.
    - Global decay toward uniform at a small rate (wm_decay), independent base but applied each step.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        fail0 : float
            Baseline retrieval failure level at set size 3 (0-1 after logistic).
        fail_slope : float
            Slope controlling how failure increases with set size.
        alpha_wm : float
            Overwrite strength of WM on rewarded trials (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, fail0, fail_slope, alpha_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    # Logistic to map fail0 into (0,1)
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    base_fail = sigmoid(4.0 * (fail0 - 0.5))  # re-center so 0.5 -> 0.5, extremes map smoothly

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0

        # Set-size dependent retrieval failure (logistic in nS-3)
        fail = base_fail + (1.0 - base_fail) * sigmoid(fail_slope * (nS - 3.0)) - 0.5 * sigmoid(fail_slope * (nS - 3.0))
        # Keep in [0,1]
        fail = min(1.0, max(0.0, fail))
        wm_decay = 0.05  # small base decay

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM softmax (deterministic) mixed with uniform due to retrieval failure
            denom = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / denom if denom > 0 else 1.0 / nA
            p_wm = (1.0 - fail) * p_wm_soft + fail * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Deterministic overwrite on reward, weak move otherwise
            if r > 0.0:
                a_enc = alpha_wm
            else:
                a_enc = 0.2 * alpha_wm
            w[s, :] = (1.0 - a_enc) * w[s, :]
            w[s, a] += a_enc

            # Normalize
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs

        blocks_log_p += log_p

    return -blocks_log_p