def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, decaying working memory with lapses.

    Mixture policy between:
    - RL: tabular Q-learning
    - WM: a one-shot memory that stores the last rewarded action per state with decay toward uniform

    The effective WM contribution scales down with set size via a capacity parameter.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Base mixture weight for WM (0..1). Effective weight scales with set size via capacity_k.
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled up by 10 for expressiveness.
        phi_decay : float
            Working memory decay/leak toward uniform per encounter of a state (0..1).
        capacity_k : float
            WM capacity (in number of items). Effective WM weight is scaled by min(1, capacity_k / set_size).
        epsilon : float
            Lapse rate; with probability epsilon, choices are uniform random.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, phi_decay, capacity_k, epsilon = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # capacity-adjusted weight based on set size
        cap_factor = min(1.0, capacity_k / max(1.0, nS))
        wm_weight_eff = np.clip(wm_weight * cap_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM strengths; WM reflects remembered correct action
            # Use a deterministic softmax with high beta; W_s decays toward uniform each encounter
            W_eff = W_s  # no additional bias here
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform for the current state
            w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * w_0[s, :]
            # If rewarded, write the chosen action strongly into WM (one-shot), then renormalize
            if r > 0.5:
                w[s, a] += phi_decay  # add a boost to the chosen action
            # Normalize to keep as a valid distribution
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with decay and perseveration bias.

    Mixture policy between:
    - RL: tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: a decaying memory distribution over actions per state, updated on every trial and biased toward the last chosen action (perseveration).

    The WM weight scales down with set size as wm_weight * (3 / set_size).

    Parameters
    ----------
    model_parameters : tuple
        lr_pos : float
            RL learning rate for positive prediction errors (0..1).
        lr_neg : float
            RL learning rate for negative prediction errors (0..1).
        wm_weight : float
            Base mixture weight for WM (0..1), scaled by (3 / set_size).
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled up by 10.
        rho_persev : float
            Perseveration bias added to WM strengths for the last chosen action in a state (>=0).
        phi_decay : float
            WM decay/leak toward uniform per encounter (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, rho_persev, phi_decay = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    tiny = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for perseveration bias
        last_choice = -1 * np.ones(nS, dtype=int)

        # Set-size adjusted WM weight
        wm_weight_eff = np.clip(wm_weight * (3.0 / max(1.0, nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy (as given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Add perseveration bias to WM strengths for the last chosen action in this state
            if last_choice[s] >= 0:
                W_s[last_choice[s]] += rho_persev
            # WM softmax choice probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = lr_pos if delta >= 0.0 else lr_neg
            q[s][a] += alpha * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform for the current state
            w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * w_0[s, :]
            # Write chosen action with strength modulated by reward (stronger if rewarded)
            w[s, a] += phi_decay * (0.5 + 0.5 * r)
            # Normalize
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

            # Update perseveration memory
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-weighted arbitration and precision scaling by set size.

    Arbitration:
    - Compute a dynamic WM weight based on relative reliabilities of WM and RL on each trial.
    - WM precision scales inversely with set size.

    Components:
    - RL: tabular Q-learning.
    - WM: decaying distribution over actions per state; rewarded actions are consolidated more.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        wm_weight_base : float
            Base arbitration gain for WM (0..1) that is modulated by relative reliabilities.
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled up by 10.
        wm_precision_base : float
            Base WM precision (>=0); effective WM precision scales as wm_precision_base * (3 / set_size).
        sigma_rl : float
            RL uncertainty scale term (>=0) used in arbitration; larger values down-weight RL reliability.
        epsilon : float
            Lapse rate; with probability epsilon, choices are uniform random.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_precision_base, sigma_rl, epsilon = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    tiny = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM precision scaled by set size
        wm_prec = wm_precision_base * (3.0 / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM strengths
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute arbitration weight based on relative reliabilities
            # RL reliability via inverse entropy-like term
            q_soft = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            q_soft /= np.sum(q_soft)
            entropy_rl = -np.sum(q_soft * np.log(np.maximum(q_soft, tiny)))
            rel_rl = 1.0 / (entropy_rl + sigma_rl)

            # WM reliability proportional to precision
            rel_wm = wm_prec

            # Dynamic WM weight
            if rel_rl + rel_wm > 0:
                wm_weight_dyn = wm_weight_base * (rel_wm / (rel_wm + rel_rl))
            else:
                wm_weight_dyn = 0.0
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            p_mix = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform with rate dependent on (inverse) precision
            kappa = 1.0 / (1.0 + wm_prec)  # more precise WM leaks less
            w[s, :] = (1.0 - kappa) * w[s, :] + kappa * w_0[s, :]
            # Reward-based consolidation toward chosen action scaled by precision
            if r > 0.5:
                w[s, a] += wm_prec * (1.0 - w[s, a])
            # Normalize
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p