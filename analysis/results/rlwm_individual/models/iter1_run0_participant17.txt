def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size窶電ependent interference (noisy WM retrieval) and Hebbian WM updates

    Mechanism:
    - RL: standard delta-rule with learning rate lr and inverse temperature softmax_beta (scaled by 10).
    - WM: per-state action preference vector w[s,:] updated Hebbian-style toward the last rewarded action
          with learning rate eta_wm. Retrieval from WM is imperfect: with probability epsilon_ss (which
          increases with set size), WM returns a uniform (uninformative) policy; otherwise it returns a
          near-deterministic softmax over W_s.
    - Mixture: total policy = wm_weight * p_wm + (1 - wm_weight) * p_rl

    Set-size effect:
    - WM interference increases with set size via epsilon_ss = 1 - exp(-wm_interference * (nS - 1)/nS),
      so larger sets produce more WM retrieval noise.

    Parameters
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight on WM policy.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - eta_wm: float in [0,1], WM learning rate toward one-hot after reward=1.
    - wm_interference: float >= 0, strength of set-size窶電ependent WM retrieval noise.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, eta_wm, wm_interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size窶電ependent WM interference
        epsilon_ss = 1.0 - np.exp(-wm_interference * (max(1, nS) - 1.0) / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            # RL policy (fixed by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s,:]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_focus = 1.0 / max(denom_wm, 1e-12)
            p_wm = (1.0 - epsilon_ss) * p_wm_focus + epsilon_ss * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Hebbian-like WM update toward chosen action when rewarded
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM buffer with capacity-limited maintenance and rehearsal

    Mechanism:
    - RL: standard delta-rule with learning rate lr and inverse temperature softmax_beta (scaled by 10).
    - WM: maintains a recency buffer of states for which the last rewarded action is stored.
          If the current state is in the buffer, WM retrieval succeeds with probability 'rehearsal';
          otherwise, WM contributes an expected-capacity success probability of min(1, K_buffer/nS).
          WM policy when successful is near-deterministic softmax over stored W_s; otherwise uniform.
          WM is updated to one-hot for rewarded actions and the state is moved to the end of the buffer.
          If buffer exceeds K_buffer, evict the oldest state.
    - Mixture: total policy = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Set-size effect:
    - Expected WM availability for non-buffered states scales as K_buffer / nS (worse for nS=6 than nS=3).

    Parameters
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight on WM policy.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - K_buffer: float >= 0, effective WM buffer capacity (will be clipped to [0, nS]).
    - rehearsal: float in [0,1], probability that WM retrieval succeeds when the state is in the buffer.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_buffer, rehearsal = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity for this block clipped to [0, nS]
        Kb = int(np.clip(np.round(K_buffer), 0, nS))
        buffer = []  # recency list of states

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            # RL policy (fixed by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            in_buffer = (s in buffer)
            # Expected availability if not in buffer: capacity ratio
            avail_expected = 0.0 if nS <= 0 else min(1.0, (Kb / float(nS)))
            # Retrieval probability combines actual buffer membership with rehearsal
            p_retrieve = rehearsal * (1.0 if in_buffer else 0.0) + (1.0 - rehearsal) * avail_expected

            W_s = w[s,:]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_success = 1.0 / max(denom_wm, 1e-12)
            p_wm = p_retrieve * p_wm_success + (1.0 - p_retrieve) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # Update WM trace to one-hot and manage buffer membership/recency
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                if in_buffer:
                    # move to end (most recent)
                    buffer.remove(s)
                buffer.append(s)
                # Evict oldest if over capacity
                if len(buffer) > Kb and len(buffer) > 0:
                    evict = buffer.pop(0)
                    # Optional: clear evicted state's WM vector back to baseline for parsimony
                    w[evict, :] = w_0[evict, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Surprise-gated WM with time decay and set-size窶電ependent WM precision

    Mechanism:
    - RL: standard delta-rule with learning rate lr and inverse temperature softmax_beta (scaled by 10).
    - WM storage: when reward arrives, the WM update toward the chosen action is gated by the absolute RL
      prediction error |delta| via a sigmoid with slope gate_slope: larger surprises produce stronger WM encoding.
    - WM decay: each state's WM trace decays exponentially with time since last visit at rate decay_wm.
    - WM precision: WM softmax temperature increases with smaller set size; we model this by adding a bonus
      to the base WM inverse temperature that diminishes with set size: beta_wm_eff = 50 + wm_temp_bonus/(1 + (nS-3)_+).
    - WM retrieval: proportional to current WM trace strength for the state via its L1 norm after decay
      (bounded to [0,1]); fallback is uniform policy.
    - Mixture: total policy = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Set-size effects:
    - WM is more precise (higher beta) for small sets via wm_temp_bonus term.
    - Larger sets indirectly reduce retrieval through faster effective decay between revisits.

    Parameters
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight on WM policy.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - gate_slope: float >= 0, steepness of sigmoid that maps |prediction error| to WM encoding strength.
    - decay_wm: float >= 0, exponential decay rate of WM traces per intervening trial.
    - wm_temp_bonus: float >= 0, additional WM inverse temperature for small set sizes.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_slope, decay_wm, wm_temp_bonus = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm_base = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last-visit time for decay
        last_time = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM beta bonus (bigger bonus for smaller sets)
        ss_term = max(0.0, float(nS) - 3.0)
        beta_wm_eff = softmax_beta_wm_base + (wm_temp_bonus / (1.0 + ss_term))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply decay to WM trace for this state since last visit
            if last_time[s] >= 0:
                dt = max(0, t - last_time[s])
                if decay_wm > 0.0 and dt > 0:
                    decay_factor = np.exp(-decay_wm * float(dt))
                    w[s, :] = (1.0 - decay_factor) * w_0[s, :] + decay_factor * w[s, :]
            last_time[s] = t

            Q_s = q[s,:]
            # RL policy (fixed by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s,:]
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm_focus = 1.0 / max(denom_wm, 1e-12)
            # Retrieval probability proportional to WM strength (L1 distance from uniform)
            wm_strength = max(0.0, min(1.0, 0.5 * np.sum(np.abs(W_s - w_0[s, :]))))
            p_wm = wm_strength * p_wm_focus + (1.0 - wm_strength) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Surprise-gated WM encoding toward chosen action when rewarded
            pe_mag = abs(delta)
            gate = 1.0 / (1.0 + np.exp(-gate_slope * (pe_mag - 0.5)))
            if r > 0.5 and gate > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - gate) * w[s, :] + gate * one_hot

        blocks_log_p += log_p

    return -blocks_log_p