def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity-limited working memory and reward-gated encoding.

    Idea:
    - Choices are generated by a mixture of a reinforcement learning (RL) policy and a
      working memory (WM) policy.
    - WM contribution is down-weighted under higher set-size via a capacity parameter K.
    - WM updates quickly toward the last rewarded action for a state and decays toward uniform.

    Parameters (model_parameters):
    - lr: scalar in (0,1). Learning rate for RL, also used as WM encoding strength on rewarded trials.
    - wm_weight: scalar in (0,1). Base mixture weight of WM vs RL.
    - softmax_beta: positive scalar. Inverse temperature for RL softmax (internally scaled x10).
    - K: positive scalar. Effective WM capacity; WM weight is scaled by min(1, K / set_size).
    - rho: scalar in (0,1). WM decay rate toward uniform per trial.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, K, rho = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables and uniform prior for WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-weighted WM mixture for this block
        cap_scale = min(1.0, K / max(1, nS))
        wm_block_weight = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy: softmax probability of chosen action a
            Q_s = q[s, :]
            Q_centered = Q_s - Q_s[a]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * Q_centered))

            # WM policy: softmax over WM weights for the state
            W_s = w[s, :]
            W_centered = W_s - W_s[a]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * W_centered))

            # Mixture
            wm_eff = wm_block_weight
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then encode rewarded action
            # Decay
            w[s, :] = (1.0 - rho) * w[s, :] + rho * w_0[s, :]
            # Encode only if rewarded: push distribution toward one-hot at chosen action
            if r > 0.5:
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with set-size dependent forgetting and one-shot WM encoding.

    Idea:
    - WM encodes the last rewarded action for a state in a near one-shot manner with strength p_encode.
    - WM forgets toward uniform at a rate that increases with set size (load-dependent forgetting).
    - Choices are a mixture of RL softmax and WM softmax policies.

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate.
    - wm_weight: scalar in (0,1). Base mixture weight of WM vs RL.
    - softmax_beta: positive scalar. Inverse temperature for RL softmax (internally scaled x10).
    - p_encode: scalar in (0,1). Strength of WM encoding toward the rewarded action.
    - phi_base: scalar in (0,1). Base WM forgetting rate; effective forgetting scales with set size.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, p_encode, phi_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent forgetting: scale base forgetting by set size (cap at <1)
        # Reference set size = 3. More items => faster forgetting.
        phi = np.clip(phi_base * (nS / 3.0), 0.0, 0.999)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture (fixed per block, but we let it be constant here)
            wm_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: forgetting then one-shot encoding on reward
            # Forget toward uniform at rate phi
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]
            # Encode rewarded action with strength p_encode (one-shot push toward one-hot)
            if r > 0.5:
                w[s, :] = (1.0 - p_encode) * w[s, :]
                w[s, a] += p_encode

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with uncertainty-gated WM usage and valence-sensitive WM updates.

    Idea:
    - The WM mixture weight is dynamically gated by RL uncertainty and set size:
      when RL is uncertain (small Q-range), rely more on WM; also down-weight WM under higher set size.
    - WM decays toward uniform and is updated positively on reward and negatively on non-reward
      (discouraging recently chosen-but-incorrect actions).

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate; also used as WM update step-size.
    - wm_weight: scalar in (0,1). Base WM weight (midpoint of the gate).
    - softmax_beta: positive scalar. RL inverse temperature (internally scaled x10).
    - gamma: scalar (can be positive). Sensitivity of the WM gate to RL uncertainty.
    - rho: scalar in (0,1). WM decay rate toward uniform.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, gamma, rho = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty gating:
            # RL uncertainty high when Q-range is small.
            q_range = np.max(Q_s) - np.min(Q_s)
            unc = 1.0 - q_range  # in [0,1] early; can be <0 or >1 but typically small range -> ~1
            # Capacity factor: down-weight WM under larger set sizes
            cap = 3.0 / max(3.0, float(nS))
            # Gate WM weight by both factors
            wm_eff = wm_weight * np.exp(gamma * unc) * cap
            wm_eff = float(np.clip(wm_eff, 0.0, 1.0))

            # Mixture
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay then valence-sensitive adjustment
            # Decay toward uniform
            prev = w[s, :].copy()
            w[s, :] = (1.0 - rho) * prev + rho * w_0[s, :]

            if r > 0.5:
                # Reward: push toward chosen action
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                # No reward: discourage chosen action and redistribute mass to others
                loss = lr * prev[a]
                # Apply on top of decayed weights (use current row after decay)
                # First, reduce chosen action (bounded below by 0)
                w[s, a] = max(0.0, w[s, a] - loss)
                # Redistribute equally to non-chosen actions
                others = [i for i in range(nA) if i != a]
                w[s, others] += loss / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p