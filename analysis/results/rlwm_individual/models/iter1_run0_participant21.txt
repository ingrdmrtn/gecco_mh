def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-specific mixture weights and interference-scaled WM decay.

    Model overview:
    - RL: tabular Q-learning with a single learning rate and softmax choice.
    - WM: fast, reward-triggered one-shot encoding that decays toward a uniform baseline.
    - Arbitration: mixture of WM and RL policies, with separate WM mixture weights for small vs. large set size.
    - Load-dependent WM interference: WM decays faster when the set size is larger.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate for Q-values.
    - wm_weight_small: float in [0,1], WM mixture weight when set size is 3.
    - wm_weight_large: float in [0,1], WM mixture weight when set size is 6.
    - softmax_beta: float >= 0, inverse temperature for RL softmax; scaled internally by 10.
    - wm_decay: float in [0,1], base WM decay parameter. Effective decay within a block is:
        decay_eff = 1 - (1 - wm_decay) ** (nS / 3), so larger set sizes increase decay.

    Set-size effect:
    - WM mixture weight switches between wm_weight_small (nS==3) and wm_weight_large (nS==6).
    - WM decays faster at larger set size via decay_eff above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_small, wm_weight_large, softmax_beta, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM decay scales with set size (more interference with larger sets)
        decay_eff = 1.0 - (1.0 - wm_decay) ** (float(nS) / 3.0)
        wm_weight_eff = wm_weight_small if nS <= 3 else wm_weight_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values
            W_centered = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_centered)
            p_wm = exp_W[a] / np.sum(exp_W)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform with load-dependent interference
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            # Reward-gated WM encoding: push distribution toward a one-hot at the rewarded action
            if r > 0.0:
                # One-shot write with strength proportional to base decay (reuse wm_decay)
                enc = wm_decay
                w[s, :] *= (1.0 - enc)
                w[s, a] += enc
                # Normalize row
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with WM noise and state-specific choice stickiness.

    Model overview:
    - RL: tabular Q-learning with a single learning rate and softmax choice.
    - WM: deterministic store that writes one-hot for rewarded actions; otherwise slowly relaxes to uniform.
    - WM policy noise: with probability wm_noise, WM policy is replaced by uniform (slips).
    - Stickiness: WM policy is biased toward the previously chosen action in the same state.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate for Q-values.
    - wm_weight: float in [0,1], mixture weight on the WM policy.
    - softmax_beta: float >= 0, inverse temperature for RL softmax; scaled internally by 10.
    - wm_noise: float in [0,1], injects uniform noise into WM policy and controls WM relaxation rate.
    - sticky: float >= 0, additive bias applied to the last action used in each state (within WM policy).

    Set-size effect:
    - Indirect: with more states (nS=6), prior choices in a given state are more temporally distant,
      reducing the effective influence of stickiness and allowing more WM relaxation between revisits.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_noise, sticky = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # state-specific previous action

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with stickiness: add bias to previous action in this state
            logits_wm = softmax_beta_wm * W_s.copy()
            if last_action[s] >= 0:
                logits_wm[last_action[s]] += sticky
            logits_wm -= np.max(logits_wm)
            exp_W = np.exp(logits_wm)
            p_wm = exp_W[a] / np.sum(exp_W)

            # Inject WM policy noise (slips)
            p_wm = (1.0 - wm_noise) * p_wm + wm_noise * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM relaxation toward uniform each visit to the state
            w[s, :] = (1.0 - wm_noise) * w[s, :] + wm_noise * w_0[s, :]

            # WM one-shot writing on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update last action for this state
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with error-based arbitration and WM lateral inhibition.

    Model overview:
    - RL: tabular Q-learning with a single learning rate and softmax choice.
    - WM: decays to uniform but on rewarded trials sharpens representation by boosting the chosen action
      and suppressing others (lateral inhibition).
    - Arbitration: WM weight is downregulated when the unsigned RL prediction error is large (RL is learning),
      and when the set size is larger.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate for Q-values.
    - wm_weight_base: float in [0,1], baseline WM mixture weight before arbitration.
    - softmax_beta: float >= 0, inverse temperature for RL softmax; scaled internally by 10.
    - kappa: float >= 0, arbitration sensitivity to the unsigned prediction error |delta|.
      Larger kappa reduces WM weight more when |delta| is large.
    - wm_forget: float in [0,1], per-visit WM decay toward uniform; also sets the strength of lateral sharpening.

    Set-size effect:
    - Effective WM weight is scaled by 3/nS, weakening WM influence at nS=6 relative to nS=3.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, kappa, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute delta prior to action selection mixture for arbitration
            delta = r - Q_s[a]
            abs_delta = abs(delta)

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_centered = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_centered)
            p_wm = exp_W[a] / np.sum(exp_W)

            # Error-based and set-size-based arbitration
            wm_weight_eff = wm_weight_base * (3.0 / float(nS)) * (1.0 / (1.0 + kappa * abs_delta))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM lateral inhibition on rewarded trials:
            if r > 0.0:
                # Boost chosen action toward 1 and suppress others proportionally
                boost = (1.0 - w[s, a]) * (1.0 - wm_forget)
                w[s, a] += boost
                if nA > 1:
                    suppress = boost / (nA - 1)
                    for a_other in range(nA):
                        if a_other != a:
                            w[s, a_other] = max(0.0, w[s, a_other] - suppress)
                # Normalize
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p