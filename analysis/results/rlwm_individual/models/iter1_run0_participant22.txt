def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL+Capacity-limited Working Memory (WM) mixture model.

    Idea:
    - Choices are a convex mixture of a model-free RL policy and a fast WM policy.
    - WM is capacity-limited: its effective contribution scales down with set size (3 vs 6).
    - WM rapidly stores the last rewarded action for each state and decays toward a uniform prior.

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate. Also controls the decay speed of WM toward its prior (shared rate).
        wm_weight : float in [0,1]
            Baseline weight on WM policy (maximal in small set sizes).
            Effective WM weight is scaled by 3/nS to capture capacity limits.
        softmax_beta : float >= 0
            Inverse temperature for RL policy; internally scaled by 10 to allow a high upper bound.
            WM uses a very high inverse temperature (fixed) to approximate one-shot use.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Set-size impact
    ---------------
    - Effective WM mixture weight is wm_weight * (3 / nS); thus it is halved in 6-item blocks vs 3-item blocks.
    - RL parameters are not directly changed by set size in this model; the shift in mixture captures load effects.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (probability of chosen action a)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working memory policy: fast, capacity-limited, near-deterministic
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity scaling: WM contribution declines with set size
            wm_eff = wm_weight * (3.0 / max(1.0, float(nS)))

            # Mixture
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Forget toward prior (shared rate lr as decay)
            w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]
            # 2) If rewarded, store the chosen action (one-shot boost)
            if r > 0.0:
                w[s, a] += 1.0  # add a strong preference for the correct action

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Error-driven WM with repulsion from incorrect actions.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM is near-deterministic and encodes both positive and negative outcomes:
      - Rewards move WM toward the chosen action (attraction).
      - Non-rewards move WM away from the chosen action (repulsion).
    - WM influence scales down with set size via wm_weight * (3/nS).

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate. Also controls WM's attraction/repulsion step size and decay to prior.
        wm_weight : float in [0,1]
            Baseline mixture weight on WM. Effective weight scaled by 3/nS.
        softmax_beta : float >= 0
            RL inverse temperature; scaled by 10 internally for dynamic range. WM temperature is fixed high.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Set-size impact
    ---------------
    - WM mixture weight is scaled as wm_weight * (3/nS), reducing WM's control in 6-item blocks.
    - RL parameters are constant across set size; load effects are captured via the mixture scaling.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size scaled mixture
            wm_eff = wm_weight * (3.0 / max(1.0, float(nS)))

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Decay toward prior
            w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]
            # 2) Attraction to chosen action if rewarded; repulsion if not
            if r > 0.0:
                w[s, a] += 1.0
            else:
                # Repel the chosen action slightly and push probability mass to others
                w[s, a] -= 1.0
                # To avoid letting the chosen action dominate negatively, softly boost others
                other_idx = [aa for aa in range(nA) if aa != a]
                for aa in other_idx:
                    w[s, aa] += 0.5

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Uncertainty-weighted WM arbitration.

    Idea:
    - Choices are a mixture of RL and WM, but the arbitration weight is dynamic:
      - When the RL policy is uncertain (high entropy), rely more on WM.
      - When RL is confident, rely less on WM.
    - WM is near-deterministic and rapidly stores the most recent action per state, regardless of reward
      (recency-based). Reward still shapes RL values.

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate. Also controls a small WM decay to prior.
        wm_weight : float in [0,1]
            Base cap on WM influence. Actual WM weight is wm_weight * (3/nS) scaled further by RL uncertainty.
        softmax_beta : float >= 0
            RL inverse temperature; internally scaled by 10. WM has fixed high temperature.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Set-size impact
    ---------------
    - WM base mixture is scaled by (3/nS), reducing WM in larger set sizes.
    - Additionally, arbitration depends on RL uncertainty (entropy), which tends to be higher early on
      and in larger sets, further amplifying set-size effects.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl_a = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Also compute full RL softmax to estimate uncertainty (entropy)
            Qc = Q_s - np.max(Q_s)
            rl_probs = np.exp(softmax_beta * Qc)
            rl_probs = rl_probs / np.sum(rl_probs)
            entropy = -np.sum(rl_probs * np.log(np.maximum(rl_probs, 1e-12))) / np.log(nA)  # normalized [0,1]

            # WM policy
            p_wm_a = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: base WM weight scaled by set size and RL uncertainty
            wm_base = wm_weight * (3.0 / max(1.0, float(nS)))
            wm_eff = wm_base * entropy  # more WM when RL is uncertain

            p_total = wm_eff * p_wm_a + (1.0 - wm_eff) * p_rl_a
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Small decay to prior
            w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]
            # 2) Recency-based storage: encode the last chosen action regardless of reward
            w[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p