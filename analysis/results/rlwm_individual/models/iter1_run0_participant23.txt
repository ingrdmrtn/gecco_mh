Below are three standalone cognitive models that follow your template logic. Each returns the negative log-likelihood of the observed choices and uses at most 6 parameters. I explicitly fill the policy for the WM system and the WM value updating in the [FILL IN] parts while keeping the rest of the skeleton unchanged. I also describe how set size (3 vs 6) affects parameters or mechanisms.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy- and capacity-gated arbitration and WM decay.

    Idea:
    - RL learns Q-values via a single learning rate.
    - WM stores per-state action preferences that are updated quickly when rewarded and
      otherwise relax toward uniform via a decay parameter.
    - Arbitration is governed by a base WM weight scaled by a capacity gate that depends
      on set size via a sigmoidal threshold (c_wm). Thus, when set size exceeds subjective
      capacity, WM contribution collapses toward RL.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate.
    - wm_base: scalar in [0,1], base mixture weight for WM contribution.
    - softmax_beta: positive, RL inverse temperature (scaled by 10 internally).
    - beta_wm: positive, WM inverse temperature.
    - wm_forget: scalar in [0,1], WM decay toward uniform each time it is updated.
    - c_wm: positive, subjective WM capacity threshold used to gate WM with set size.

    Set size effects:
    - WM mixture weight is scaled by a sigmoid gate: gate = 1 / (1 + exp(nS - c_wm)).
      This increases WM influence for small nS and reduces it for large nS.
    """
    lr, wm_base, softmax_beta, beta_wm, wm_forget, c_wm = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity gate based on set size
        gate = 1.0 / (1.0 + np.exp(float(nS) - c_wm))
        wm_mix = np.clip(wm_base * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward consolidates the chosen action; no-reward decays to uniform.
            if r > 0.0:
                # Move W_s toward one-hot at chosen action with strength wm_forget
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * one_hot
            else:
                # Decay toward uniform template
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with WM temperature scaling by set size and lapse noise.

    Idea:
    - RL with a single learning rate.
    - WM acts like a fast look-up table: reward sets a deterministic mapping for that state;
      no-reward resets that state's WM to uniform.
    - WM policy temperature is reduced as set size grows via a power-law factor.
    - Final choice includes a lapse component that selects randomly with probability 'lapse'.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: scalar in [0,1], base mixture weight for WM contribution.
    - softmax_beta: positive, RL inverse temperature (scaled by 10 internally).
    - beta_wm_base: positive, base WM inverse temperature (for nS=3).
    - rho: nonnegative, exponent controlling how WM determinism decreases with set size:
           beta_wm = beta_wm_base * (3.0 / nS) ** rho
    - lapse: scalar in [0,1], probability of random choice independent of policies.

    Set size effects:
    - WM policy inverse temperature beta_wm = beta_wm_base * (3/nS)^rho.
      Thus, WM is more deterministic in small set sizes and more noisy in large set sizes.
    """
    lr, wm_weight, softmax_beta, beta_wm_base, rho, lapse = model_parameters
    softmax_beta *= 10.0
    lapse = np.clip(lapse, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set size scaling for WM temperature
        beta_wm = beta_wm_base * (3.0 / float(nS)) ** max(rho, 0.0)
        wm_mix = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture then lapse
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                # Lucky trial: lock state-action mapping in WM
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Unlucky trial: reset this state's WM to uniform
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with arbitration by RL uncertainty and set-size-scaled WM decay/learning.

    Idea:
    - RL uses a single learning rate.
    - WM values decay toward uniform each trial for the visited state, with a decay rate that
      increases with set size (less stable WM under high load).
    - When rewarded, WM moves toward a one-hot encoding of the chosen action with the same
      'wm_decay' parameter (interpreted as a fast learning rate).
    - Arbitration weight for WM is modulated by RL uncertainty, operationalized as the
      normalized entropy of the RL softmax policy for the current state: more uncertainty
      => rely more on WM. The strength of this modulation is controlled by alpha_unc.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate.
    - wm_base: scalar in [0,1], base WM mixture weight.
    - softmax_beta: positive, RL inverse temperature (scaled by 10 internally).
    - beta_wm: positive, WM inverse temperature (fixed across set sizes).
    - alpha_unc: scalar in [0,1], how strongly RL uncertainty up-weights WM.
    - wm_decay: scalar in [0,1], WM decay/learning rate; effective decay scales with set size.

    Set size effects:
    - Effective WM decay = wm_decay * (nS / 3). Thus, WM is less stable at nS=6 than at nS=3.
    - WM mixture is dynamically scaled by RL uncertainty (entropy of RL policy), which can
      be higher in early/high-load conditions, thereby increasing reliance on WM adaptively.
    """
    lr, wm_base, softmax_beta, beta_wm, alpha_unc, wm_decay = model_parameters
    softmax_beta *= 10.0
    alpha_unc = np.clip(alpha_unc, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM decay/learning
        eff_decay = np.clip(wm_decay * (float(nS) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template)
            p_rl_den = np.exp(softmax_beta * (Q_s - Q_s[a]))
            p_rl = 1.0 / np.sum(p_rl_den)

            # Compute RL uncertainty as normalized entropy of RL softmax over actions
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_probs / np.sum(rl_probs)
            entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            entropy_norm = entropy / np.log(nA)  # in [0,1]

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Dynamic arbitration: WM weight increases with RL uncertainty
            wm_mix = np.clip(wm_base * (1.0 - alpha_unc + alpha_unc * entropy_norm), 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # First, decay the visited state's WM toward uniform (load-dependent)
            w[s, :] = (1.0 - eff_decay) * w[s, :] + eff_decay * w_0[s, :]

            # If rewarded, learn toward one-hot using the same eff_decay as a fast learning rate
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eff_decay) * w[s, :] + eff_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p

Notes on fit and set size impacts:
- Model 1: WM weight is gated by set size relative to a subjective capacity threshold c_wm.
- Model 2: WM determinism decreases with set size via a power scaling; a lapse captures random responding.
- Model 3: WM stability (decay/learning) worsens with set size, and arbitration shifts toward WM when RL is uncertain (high entropy), a situation more likely in larger set sizes or early trials.