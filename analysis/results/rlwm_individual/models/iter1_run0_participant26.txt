def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size–dependent WM leak and negative-tagging of punished actions.

    Mechanisms:
    - RL: standard delta-learning with softmax policy.
    - WM: a cache over state-action weights; on reward, the chosen action is strengthened;
           on non-reward, the chosen action is actively suppressed (negative tag).
    - Set size effect: WM leak grows with set size via a logistic transform of parameters.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline mixture weight of WM vs RL (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>=0).
    - wm_leak_base: baseline WM leak toward uniform per state visit (can be negative/positive; mapped via sigmoid).
    - interference_gain: how much WM leak increases with set size (>=0).
    - eta_neg: strength of negative tagging when r=0 for the chosen action (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_leak_base, interference_gain, eta_neg = model_parameters
    softmax_beta *= 10.0  # keep as template scaling
    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM leak via logistic mapping
        x = wm_leak_base + interference_gain * max(0, nS - 3)
        wm_leak_eff = 1.0 / (1.0 + np.exp(-x))  # in (0,1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax using WM map W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform
            w[s, :] = (1.0 - wm_leak_eff) * w[s, :] + wm_leak_eff * w_0[s, :]

            # Positive reward: strengthen chosen action in WM
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Use lr to control storage strength, consistent with template
                w[s, :] = (1.0 - lr) * w[s, :] + lr * onehot
            else:
                # Negative tagging: suppress the chosen action weight
                # Move chosen action toward zero, renormalize softly by adding uniform
                suppress = np.zeros(nA)
                suppress[a] = 1.0
                w[s, :] = np.clip(w[s, :] - eta_neg * suppress * w[s, a], 0.0, None)
                # Small pull to uniform to keep a proper distribution
                total = w[s, :].sum()
                if total <= 1e-8:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] = 0.9 * (w[s, :] / total) + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + capacity-limited WM mixture (slots).

    Mechanisms:
    - RL: separate learning rates for positive and negative outcomes (alpha+ and alpha-).
    - WM: stores rewarded action per state; policy is near-deterministic over WM.
    - Capacity effect: effective WM contribution scales by min(1, K/nS) and retrieval reliability rho.

    Parameters (tuple):
    - alpha_pos: RL learning rate when r=1 (0..1).
    - alpha_neg: RL learning rate when r=0 (0..1).
    - wm_weight: base mixture weight of WM vs RL (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>=0).
    - K: WM capacity in number of items/states (>=0).
    - rho: WM retrieval/storage reliability (0..1), scales WM weight and storage strength.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, K, rho = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity scaling of WM contribution
        cap = min(1.0, float(K) / max(1.0, nS))
        wm_weight_eff = wm_weight * cap * rho

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with capacity-limited WM
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            alpha = alpha_pos if r > 0.0 else alpha_neg
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Gentle decay toward uniform (implicit leakage via incomplete overwrite)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Storage strength scaled by rho
                w[s, :] = (1.0 - rho) * w[s, :] + rho * onehot
            else:
                # When no reward, do a mild decay toward uniform (forgetting)
                decay = 0.2 * (1.0 - rho)  # more forgetting if rho is low
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with state-wise choice stickiness + short-term WM of last action, both modulated by set size,
    plus a lapse component.

    Mechanisms:
    - RL: delta rule; inverse temperature decreases with set size.
    - Stickiness: adds a bias to repeat the last action chosen in the same state (tau), reduced at larger set size.
    - WM: stores the last chosen action per state (irrespective of reward) with strength phi; contributes via mixture.
    - Lapse: with probability epsilon (growing with set size), choice is uniform.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: mixture weight for WM vs RL (0..1).
    - softmax_beta_base: base inverse temperature before set-size scaling (>=0).
    - tau: stickiness strength added to the value of last action in a state (>=0).
    - beta_setsize_slope: how much RL temperature and stickiness decrease with set size (>=0).
    - lapse_base: base lapse rate at set size 3; scaled up with set size (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta_base, tau, beta_setsize_slope, lapse_base = model_parameters
    # Template scaling; we will compute a per-block beta after this base scaling
    softmax_beta = softmax_beta_base * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_a = -1 * np.ones(nS, dtype=int)

        # Set-size modulation
        scale = 1.0 + beta_setsize_slope * max(0, nS - 3)
        beta_block = softmax_beta / scale
        tau_block = tau / scale
        epsilon = min(0.25, lapse_base * (nS / 3.0))  # capped lapse

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add stickiness bias to last action in this state
            if last_a[s] >= 0:
                Q_s[last_a[s]] += tau_block

            # RL softmax probability
            p_rl = 1.0 / np.sum(np.exp(beta_block * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM keeps last chosen action with strength phi (implemented below in update).
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Add lapse (uniform)
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Short-term memory of last action: move W_s toward one-hot of current choice
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            # Use phi derived from wm_weight to keep param count capped: stronger WM weight => stronger storage
            phi = 0.5 * wm_weight + 0.25  # in [0.25, 0.75] as a monotonic function of wm_weight
            w[s, :] = (1.0 - phi) * w[s, :] + phi * onehot

            # Update last action for stickiness
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p