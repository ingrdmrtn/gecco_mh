Below are three standalone cognitive models that follow your template structure, each exploring a different RLâ€“WM mechanism and set-size dependence. All three return the negative log-likelihood of the observed choices, use no imports inside the function, and use all parameters meaningfully.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited WM with lapses + model-free RL.

    Mechanism
    ---------
    - RL: single learning rate; standard softmax with scaled inverse temperature.
    - WM: slot-limited capacity K. When nS > K, WM impact is reduced proportionally.
           WM is near-deterministic, but includes a small lapse probability epsilon.
           WM updates rapidly toward a one-hot memory for rewarded actions and
           decays (leaks) toward uniform otherwise.

    Set-size effects
    ----------------
    - wm_weight is scaled by min(1, K_slots / nS), so WM influence is weaker in set size 6.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, K_slots, wm_leak, epsilon_lapse)
        - lr: RL learning rate (0..1).
        - wm_weight: Base WM mixture weight before capacity scaling (0..1).
        - softmax_beta: RL inverse temperature (>0, scaled internally by 10).
        - K_slots: WM capacity in number of state-action pairs (>=0).
        - wm_leak: WM leak toward uniform per update (0..1).
        - epsilon_lapse: WM lapse probability mixing in uniform policy (0..1).
    """
    lr, wm_weight, softmax_beta, K_slots, wm_leak, epsilon_lapse = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM mixture weight
        cap_scale = min(1.0, float(K_slots) / float(nS)) if nS > 0 else 1.0
        wm_weight_block = wm_weight * cap_scale
        wm_weight_block = min(max(wm_weight_block, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic, with lapse to uniform
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - epsilon_lapse) * p_wm_det + epsilon_lapse * (1.0 / nA)

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens chosen action; otherwise leak to uniform
            if r > 0.0:
                # Move mass toward chosen action
                w[s, :] = (1.0 - wm_leak) * w[s, :]
                w[s, a] += wm_leak
            else:
                # Leak toward uniform if not rewarded
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-gated WM plus RL with asymmetric learning rates.

    Mechanism
    ---------
    - RL: separate learning rates for positive and negative outcomes; softmax choice.
    - WM: updated toward one-hot for rewarded actions, otherwise decays to uniform.
    - Arbitration: WM weight increases when RL is uncertain (high entropy policy),
      and decreases with larger set sizes via a power-law scaling.

    Set-size effects
    ----------------
    - wm_weight is scaled by (3/nS)^gamma_set, weakening WM in larger sets.
    - Additional dynamic scaling by RL's entropy: higher RL uncertainty => more WM.

    Parameters
    ----------
    model_parameters : tuple
        (alpha_pos, alpha_neg, wm_weight, softmax_beta, gamma_set, wm_alpha)
        - alpha_pos: RL learning rate for rewards (0..1).
        - alpha_neg: RL learning rate for no-reward (0..1).
        - wm_weight: Base WM weight before uncertainty and set-size scaling (0..1).
        - softmax_beta: RL inverse temperature (>0, scaled internally by 10).
        - gamma_set: Exponent controlling set-size effect on WM (>=0).
        - wm_alpha: WM learning rate toward target distributions (0..1).
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, gamma_set, wm_alpha = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probabilities for entropy-based arbitration
            # Compute full softmax policy over actions
            denom = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / max(denom, 1e-12)

            # Entropy of RL policy (normalized)
            log_pi = np.log(np.maximum(pi_rl, 1e-12))
            H = -np.sum(pi_rl * log_pi)          # entropy in nats
            H_max = np.log(nA)                   # max entropy for nA actions
            H_norm = H / max(H_max, 1e-12)       # in [0,1]

            # Set-size scaling of WM weight, and uncertainty gating by RL entropy
            size_scale = (3.0 / float(nS)) ** max(gamma_set, 0.0)
            wm_weight_eff = wm_weight * size_scale * H_norm
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            # Probability of chosen action under RL and WM
            p_rl = pi_rl[a]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta > 0.0 else alpha_neg
            q[s, a] += alpha * delta

            # WM update:
            # Move toward one-hot on chosen action if rewarded; else decay to uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    WM interference across states + RL with choice stickiness.

    Mechanism
    ---------
    - RL: single learning rate; softmax with a perseveration (stickiness) bias for repeating the last action.
    - WM: near-deterministic retrieval of the memorized action, but subject to inter-item interference:
           WM policy is computed from a blend of the current state's WM row and the average of other states.
           Additionally, global interference causes a slight drift of all WM rows toward uniform each trial.

    Set-size effects
    ----------------
    - Interference strength increases with set size via phi_eff = phi_interf * (nS - 1) / nS,
      reducing WM reliability more in larger sets (size 6 > size 3).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, alpha_wm, phi_interf, stickiness)
        - lr: RL learning rate (0..1).
        - wm_weight: Base WM mixture weight (0..1).
        - softmax_beta: RL inverse temperature (>0, scaled internally by 10).
        - alpha_wm: WM learning rate toward one-hot for rewarded actions (0..1).
        - phi_interf: Base WM interference strength across states (0..1).
        - stickiness: Bias added to the value of the previously chosen action in RL (can be +/-).
    """
    lr, wm_weight, softmax_beta, alpha_wm, phi_interf, stickiness = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Last action for RL stickiness (per block)
        last_action = -1

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective interference strength increases with nS
            phi_eff = phi_interf * (float(nS) - 1.0) / float(nS) if nS > 0 else 0.0
            phi_eff = min(max(phi_eff, 0.0), 1.0)

            # RL with stickiness bias for the last chosen action
            Q_s = q[s, :].copy()
            if last_action >= 0 and last_action < nA:
                Q_s[last_action] += stickiness

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM with cross-state interference:
            # Blend current state's WM row with the average of other states' WM rows
            W_s = w[s, :].copy()
            if nS > 1:
                other_indices = [i for i in range(nS) if i != s]
                W_mean_others = np.mean(w[other_indices, :], axis=0)
                W_eff = (1.0 - phi_eff) * W_s + phi_eff * W_mean_others
            else:
                W_eff = W_s

            # WM chosen-action probability (near deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update for current state: rewarded -> toward one-hot; else toward uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target
            else:
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * w_0[s, :]

            # Global interference drift: all WM rows slightly pulled toward uniform
            # (uses phi_eff so it scales with set size)
            if phi_eff > 0.0:
                w = (1.0 - 0.5 * phi_eff) * w + (0.5 * phi_eff) * w_0

            # Renormalize the updated state's row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Update stickiness memory
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p

Notes on parameter-set novelty relative to your prior best model:
- None of these models reuse the exact parameter tuple from your best model. They introduce different mechanisms:
  - Model 1: capacity K with lapses.
  - Model 2: asymmetric RL learning rates and entropy-gated WM with a power-law set-size factor.
  - Model 3: cross-state WM interference and RL choice stickiness.