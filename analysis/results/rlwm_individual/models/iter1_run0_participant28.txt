Below are three standalone cognitive models that follow the provided template structure and constraints. Each returns the negative log-likelihood of the observed choices and uses all specified parameters. I kept the RL/WM mixture policy structure and filled in the WM policy and WM updates in the FILL IN sections. I also highlighted how set size modulates parameters or mechanisms in each model.

Note: Assume numpy as np is already imported. No imports are included inside the functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with per-visit value forgetting and WM with set-size–dependent interference.

    Idea:
    - RL updates with a standard delta rule, but additionally forgets toward uniform on each state visit.
    - WM stores the last rewarded action for a state in a one-shot manner, but storage is corrupted by
      interference that increases with set size.
    - WM and RL policies are mixed by a constant weight.

    Parameters (5):
    - lr: RL learning rate for prediction errors (0..1).
    - wm_weight: Mixture weight for WM policy (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - rl_decay: Per-visit RL decay toward uniform for the visited state row (0..1).
    - wm_interference: Controls how much set size corrupts WM storage; interference = 1 - exp(-wm_interference * (nS-1)).

    Set-size impacts:
    - Larger set sizes increase WM interference, making WM memories more uniform and less informative.
    """
    lr, wm_weight, softmax_beta, rl_decay, wm_interference = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM interference for the block
        interference = 1.0 - np.exp(-wm_interference * max(0.0, float(nS) - 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (Gibbs trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic softmax from the WM matrix
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with per-visit decay toward uniform for the visited state
            # First decay toward uniform on the visited row
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)
            # Then standard delta rule for chosen action
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # One-shot storage on reward with set-size–dependent interference
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - interference) * one_hot + interference * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with count-based exploration bonus (UCB-like) and WM with load-suppressed mixture weight (power law).

    Idea:
    - RL uses a delta rule, but the choice policy includes an uncertainty/exploration bonus based on
      inverse square-root of action visit counts in the current state.
    - WM stores last rewarded action one-shot.
    - WM mixture weight is reduced as a power-law function of set size.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight (0..1) before load suppression.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - ucb_bonus: Magnitude of the exploration bonus added to Q during policy computation (>=0).
    - gamma_load: Exponent controlling load suppression of WM:
                  wm_weight_eff = wm_weight / (1 + (nS / 3.0) ** gamma_load)

    Set-size impacts:
    - WM mixture weight is reduced with larger set sizes via the power law.
    - Exploration bonus encourages sampling less-visited actions especially early/under uncertainty.
    """
    lr, wm_weight, softmax_beta, ucb_bonus, gamma_load = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # action counts per state-action

        # Load-suppressed WM weight
        wm_weight_eff = wm_weight / (1.0 + (float(nS) / 3.0) ** gamma_load)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add count-based exploration bonus for policy computation
            bonus = ucb_bonus / np.sqrt(N[s, :] + 1.0)
            Q_aug = Q_s + bonus

            # RL policy with augmented values
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Update counts
            N[s, a] += 1.0

            # RL value update (standard delta rule on true Q, not augmented)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # One-shot storage on reward
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with a lapse component and WM gated by set-size–dependent storage probability.

    Idea:
    - RL updates with a simple delta rule.
    - WM stores the last rewarded action probabilistically; storage probability is reduced by set size via a logistic gate.
    - A lapse parameter adds uniform-random choice probability on top of the RL/WM mixture.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - gate_slope: Slope controlling how set size reduces the WM storage probability.
    - gate_bias: Bias term for the gating logistic that sets storage probability at small set sizes.
                 p_store = 1 / (1 + exp(-(gate_bias - gate_slope * nS))).
    - lapse: Choice lapse rate added to final policy (0..0.5 typical); final:
             p_final = (1 - lapse) * (wm_weight * p_wm + (1 - wm_weight) * p_rl) + lapse * (1/nA)

    Set-size impacts:
    - Larger set sizes reduce the probability that a rewarded action gets stored in WM via the logistic gate.
    """
    lr, wm_weight, softmax_beta, gate_slope, gate_bias, lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent storage probability
        p_store = 1.0 / (1.0 + np.exp(-(gate_bias - gate_slope * float(nS))))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture then lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Probabilistic storage on reward; if not stored, WM unchanged
            if r > 0.5:
                # Bernoulli draw via thresholding on a uniform random number
                # To keep deterministic and differentiable for fitting, we use expected update:
                # Expected WM after storage attempt: E[w_new] = p_store * one_hot + (1 - p_store) * w_old
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = p_store * one_hot + (1.0 - p_store) * w[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

How set size might impact parameters/mechanisms in these models:
- Model 1: WM interference increases with set size, degrading memory precision; RL includes forgetting that does not directly depend on set size, separating RL dynamics from load effects.
- Model 2: WM mixture weight is suppressed as a power law in set size; additionally, a UCB-like exploration bonus improves early learning in larger sets where uncertainty is higher.
- Model 3: WM storage probability decreases with set size via a logistic gate, and a lapse component captures occasional random responding that may increase under higher cognitive load (though lapse is not directly set-size dependent here).