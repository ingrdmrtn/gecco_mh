Below are three alternative cognitive models for the RL-WM task. Each is a standalone function that computes the negative log-likelihood of the observed choices. They all follow the provided template structure: within each block they initialize RL and WM value tables, compute a mixed policy from RL and WM, and update both systems after observing feedback. All parameters are used, and set size effects are explicitly encoded but in different ways across models.

Note: These functions assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with graded encoding and decay.

    Idea:
    - RL learns state-action values with a single learning rate.
    - WM stores rewarded actions but with graded encoding (not a hard overwrite),
      and WM traces decay toward uniform each time the state is visited.
    - WM mixture weight is capacity-limited: effective WM weight scales as K/nS,
      reflecting reduced WM contribution in larger set sizes.

    Parameters (model_parameters):
    - lr:           RL learning rate (0..1)
    - wm_weight:    Base weight on WM in policy mixture (0..1)
    - softmax_beta: Base RL inverse temperature (scaled internally by *10)
    - K_cap:        WM capacity in number of items; effective WM weight = wm_weight * min(1, K_cap / nS)
    - wm_eta:       WM encoding strength toward the rewarded action upon r=1 (0..1)
    - wm_decay:     WM decay toward uniform each time a state is visited (0..1)

    Set-size impact:
    - WM mixture weight is reduced as set size increases via K_cap/nS.
    """
    lr, wm_weight, softmax_beta, K_cap, wm_eta, wm_decay = model_parameters

    softmax_beta *= 10.0  # follow template scaling
    softmax_beta_wm = 50.0  # deterministic WM policy
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Capacity-limited WM mixture weight
        wm_eff = wm_weight * min(1.0, float(K_cap) / max(1.0, float(nS)))

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (template form)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform on each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding: if rewarded, move toward a one-hot memory trace for the chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with directed exploration bonus + WM with load-dependent weight.

    Idea:
    - RL learns state-action values (single learning rate) and adds an uncertainty-driven
      directed exploration bonus inversely proportional to the number of times an action
      has been tried in that state (count-based bonus).
    - Exploration bonus is stronger in larger set sizes.
    - WM stores rewarded actions and decays toward uniform on each visit.
    - WM's contribution to the policy mixture decreases with set size (load dependence).

    Parameters (model_parameters):
    - lr:           RL learning rate (0..1)
    - wm_weight:    Base WM weight in the mixture (0..1)
    - softmax_beta: Base RL inverse temperature (scaled internally by *10)
    - bonus_gamma:  Base directed exploration bonus scale
    - wm_decay:     WM decay toward uniform on each visit (0..1)
    - wm_load:      Load penalty on WM mixture weight; wm_eff = wm_weight / (1 + wm_load*(nS-3))

    Set-size impact:
    - Directed exploration bonus increases with set size: bonus_eff = bonus_gamma * (nS/3).
    - WM contribution decreases with set size via wm_load.
    """
    lr, wm_weight, softmax_beta, bonus_gamma, wm_decay, wm_load = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Counts for directed exploration
        counts_sa = np.zeros((nS, nA))

        # WM mixture weight decreases with load
        wm_eff = wm_weight / (1.0 + max(0.0, float(wm_load) * float(nS - 3)))
        wm_eff = max(0.0, min(1.0, wm_eff))

        # Exploration bonus scale increases with set size
        bonus_eff = bonus_gamma * (float(nS) / 3.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Directed exploration bonus: larger for less-tried actions
            bonus_vec = bonus_eff / np.sqrt(counts_sa[s, :] + 1.0)
            Q_aug = Q_s + bonus_vec

            # RL policy with augmented values
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update counts AFTER observing the choice
            counts_sa[s, a] += 1.0

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding of rewarded action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # strong episodic WM write on reward

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice stickiness bias + WM with load-invariant weight but lapses increasing with set size.

    Idea:
    - RL learns values with a single learning rate.
    - Add a state-specific choice stickiness bias to RL values, favoring the most recently chosen action
      in that state. The bias decays across visits.
    - WM stores rewarded actions (hard overwrite) and decays using the same decay constant used for
      stickiness decay (shared resource decay).
    - Policy is a mixture of RL and WM. Additionally, there is a load-dependent lapse that mixes uniform
      choice into the final policy (more lapses at larger set sizes).

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - wm_weight:     WM weight in mixture (0..1)
    - softmax_beta:  RL inverse temperature (scaled internally by *10)
    - kappa_stick:   Magnitude of choice stickiness bias added to Q-values upon repeating (>=0)
    - kappa_decay:   Decay rate applied to stickiness trace on each visit; also used as WM decay (0..1)
    - lapse_base:    Base lapse rate scaled by set size: lapse = lapse_base * (nS/3), then clipped to <1

    Set-size impact:
    - Lapse increases linearly with set size (nS/3), making behavior noisier for larger sets.
    - WM weight itself is not directly load-scaled (so that lapse is the dominant load effect here).
    """
    lr, wm_weight, softmax_beta, kappa_stick, kappa_decay, lapse_base = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent lapse
        lapse = lapse_base * (float(nS) / 3.0)
        lapse = max(0.0, min(0.49, lapse))  # keep lapse below 0.5 for stability

        # Initialize RL, WM, and stickiness traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        stick = np.zeros((nS, nA))  # choice bias per state-action

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply decay to stickiness on visiting the state
            stick[s, :] *= (1.0 - kappa_decay)
            # Add stickiness to last chosen action in this state
            stick[s, a] += kappa_stick

            # RL policy with stickiness bias added to Q
            Q_s = q[s, :] + stick[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Lapse to uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay (share decay parameter with stickiness)
            w[s, :] = (1.0 - kappa_decay) * w[s, :] + kappa_decay * w_0[s, :]

            # WM encode reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # strong storage of correct mapping on reward

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes:
- Model1 emphasizes capacity-limited WM mixture contribution that shrinks with set size.
- Model2 emphasizes directed exploration in RL that grows with set size, and WM mixture weight that shrinks with load.
- Model3 emphasizes action perseveration (stickiness) within RL, plus a load-dependent lapse that increases with set size, with a shared decay resource for WM and stickiness.

All three differ in parameter combinations and mechanisms from those you tried previously.