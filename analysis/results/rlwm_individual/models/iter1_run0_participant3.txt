def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited, decaying working memory (WM) + model-free RL mixture.
    - RL: standard delta-rule with softmax choice.
    - WM: associative weights per state that are strengthened by reward and
      decay toward a uniform prior; WM weight scales with an effective capacity.
      Capacity determines how much WM can contribute as set size increases.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Base mixture weight on WM policy (maximal weight at full recall).
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled up internally).
        K_capacity : float >= 0
            WM capacity in number of items; effective WM weight scales as min(1, K_capacity / set_size).
        wm_decay : float in [0,1]
            Per-trial decay of WM weights toward uniform for the attended state.
        alpha_wm : float in [0,1]
            One-shot Hebbian update toward the chosen action when rewarded.

    Set-size impact
    ---------------
    Effective WM contribution on each trial is wm_weight * min(1, K_capacity / set_size).
    Thus, WM is down-weighted in larger set sizes when K_capacity < set_size.
    WM forgetting (wm_decay) is constant across set sizes in this model.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_capacity, wm_decay, alpha_wm = model_parameters[:6]

    softmax_beta *= 10.0  # RL beta scaling per template
    softmax_beta_wm = 50.0  # deterministic WM softmax per template
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1.0 / nA) * np.ones((nS, nA))   # WM associative strengths
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform prior

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights with high beta
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture weight scales with effective capacity vs. set size
            set_size_t = float(block_set_sizes[t])
            recall = min(1.0, float(K_capacity) / max(1.0, set_size_t))
            wm_weight_eff = np.clip(wm_weight * recall, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then Hebbian boost if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r >= 0.5:
                # Move distribution toward a one-hot on chosen action
                w[s, :] = (1.0 - alpha_wm) * w[s, :]
                w[s, a] += alpha_wm

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RPE-gated working memory with set-size-dependent recall and adjustable WM determinism.
    - RL: delta-rule with softmax.
    - WM: stores an association only when a positive prediction error (RPE) exceeds a threshold.
      WM policy is a softmax over stored WM weights with its own determinism scale.
      WM contribution is down-weighted linearly with set size.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Base mixture weight on WM.
        softmax_beta : float >= 0
            RL inverse-temperature (scaled up internally).
        rpe_threshold : float in [0,1]
            Gate: WM encodes association only when RPE > rpe_threshold.
        wm_beta_scale : float >= 0
            Scales WM determinism: effective beta_wm = 50 * (1 + wm_beta_scale).
        recall_slope : float in [0, 1/3]
            Linear decrement in WM availability from set size 3 to 6:
            recall = max(0, 1 - recall_slope * (set_size - 3)).

    Set-size impact
    ---------------
    Effective WM mixture weight each trial is wm_weight * recall, where
    recall = max(0, 1 - recall_slope * (set_size - 3)), reducing WM in larger sets.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rpe_threshold, wm_beta_scale, recall_slope = model_parameters[:6]

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0 * (1.0 + max(0.0, wm_beta_scale))
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))   # RL values
        w = (1.0 / nA) * np.ones((nS, nA))   # WM weights (default uniform)
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform baseline

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over current WM weights
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Set-size-dependent WM weight
            set_size_t = int(block_set_sizes[t])
            recall = max(0.0, 1.0 - recall_slope * (set_size_t - 3))
            wm_weight_eff = np.clip(wm_weight * recall, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # RPE-gated WM update:
            # - If positive RPE exceeds threshold, store a sharp association for (s, a)
            # - Otherwise, relax WM weights toward uniform (forget)
            if pe > rpe_threshold:
                # Encode: move strongly toward one-hot on 'a'
                w[s, :] = 0.0 * w[s, :] + w_0[s, :]  # reset to uniform baseline first
                w[s, a] = 1.0
            else:
                # Forget toward uniform when not strongly supported by RPE
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with action stickiness + fast-learning WM with set-size-dependent forgetting.
    - RL: delta-rule; choice includes an additive state-specific stickiness bias
      toward the last chosen action in that state.
    - WM: fast Hebbian learning on rewarded trials and forgetting that increases with set size.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Base mixture weight on WM policy.
        softmax_beta : float >= 0
            RL inverse-temperature (scaled up internally).
        alpha_wm : float in [0,1]
            WM Hebbian learning rate on rewarded trials.
        wm_forget_base : float in [0,1]
            Baseline WM forgetting; effective forgetting scales up with set size.
        stickiness : float >= 0
            Additive bias to RL values favoring the last chosen action in the same state.

    Set-size impact
    ---------------
    WM forgetting increases with set size:
        wm_forget_eff = wm_forget_base * (1 + 0.5 * (set_size - 3))
    so it is higher in 6-item blocks, reducing WM influence on those trials.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_wm, wm_forget_base, stickiness = model_parameters[:6]

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))   # RL values
        w = (1.0 / nA) * np.ones((nS, nA))   # WM weights
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform
        last_choice = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness bias toward last chosen action in this state
            Q_s = q[s, :].copy()
            if last_choice[s] >= 0:
                Q_s[int(last_choice[s])] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic softmax over WM weights
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Set-size dependent WM forgetting and mixture
            set_size_t = int(block_set_sizes[t])
            wm_forget_eff = np.clip(wm_forget_base * (1.0 + 0.5 * (set_size_t - 3)), 0.0, 1.0)
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (no stickiness in learning)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: forget toward uniform with set-size-dependent rate, then reinforce on reward
            w[s, :] = (1.0 - wm_forget_eff) * w[s, :] + wm_forget_eff * w_0[s, :]
            if r >= 0.5:
                w[s, :] = (1.0 - alpha_wm) * w[s, :]
                w[s, a] += alpha_wm

            # Update last choice for stickiness
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p