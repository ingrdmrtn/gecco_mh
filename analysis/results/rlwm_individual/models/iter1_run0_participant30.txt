def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory mixture with decaying WM traces.

    Description:
    - Choices are generated by a mixture of model-free RL and a rapid Working Memory (WM) system.
    - RL uses a softmax over state-action Q values with a single learning rate.
    - WM stores a probabilistic action map per state that decays toward uniform and is boosted by rewarded outcomes.
    - Effective WM contribution is reduced as set size grows via a capacity exponent.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values (0..1).
    - model_parameters[1] = wm_weight (float): Base mixture weight of WM vs RL (0..1), further modulated by set size.
    - model_parameters[2] = softmax_beta (float): Inverse temperature for RL softmax; internally scaled up by x10.
    - model_parameters[3] = wm_decay (float): Decay rate of WM traces toward uniform per trial (0..1).
    - model_parameters[4] = wm_eta (float): WM update strength applied after reward (0..1), pushes WM toward chosen action.
    - model_parameters[5] = k_capacity (float): Capacity exponent; effective WM weight scales as (3/nS)^k_capacity.

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_eta, k_capacity = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size modulated WM mixture weight
        wm_weight_eff = wm_weight * (3.0 / float(nS)) ** k_capacity
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working Memory policy (softmax over WM weights with high beta)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM update: reward-boost toward chosen action (win-stay-like)
            if r > 0:
                # Pull mass toward chosen action by wm_eta
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            # Normalize to ensure proper probabilities
            wsum = w[s, :].sum()
            if wsum > 0:
                w[s, :] /= wsum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL with forgetting + WM win-stay/lose-switch with lapse; WM scaled by set size.

    Description:
    - RL uses separate learning rates for positive vs negative prediction errors and an RL value decay.
    - WM implements a win-stay / lose-devalue mechanism with stochasticity (epsilon lapse).
    - WM mixture weight is reduced automatically as set size increases: wm_weight_eff = wm_weight * (3/nS).
    - Choice probabilities are a mixture of RL and WM policies.

    Parameters
    - model_parameters[0] = lr_pos (float): RL learning rate for positive PE (0..1).
    - model_parameters[1] = lr_neg (float): RL learning rate for negative PE (0..1).
    - model_parameters[2] = wm_weight (float): Base WM mixture weight (0..1), scaled by 3/nS.
    - model_parameters[3] = softmax_beta (float): Inverse temperature for RL softmax; internally scaled x10.
    - model_parameters[4] = rl_decay (float): Per-trial decay of Q toward uniform (0..1).
    - model_parameters[5] = epsilon_wm (float): WM lapse/exploration probability (0..1); increases effective randomness.

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, rl_decay, epsilon_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # make WM highly deterministic when not lapsing
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # For WM win-stay/lose-switch, w will represent a soft one-hot for last rewarded action
        # Initialize as uniform

        # WM weight scaled by set size (no extra parameter)
        wm_weight_eff = wm_weight * (3.0 / float(nS))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM win-stay/lose-switch policy:
            # - Deterministic softmax over W_s (which encodes last rewarded action), with lapse epsilon_wm
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_uniform = 1.0 / nA
            p_wm = (1.0 - epsilon_wm) * p_wm_core + epsilon_wm * p_uniform

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates and forgetting toward uniform
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0 else lr_neg
            # Decay all action values toward uniform baseline
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)
            q[s][a] += lr_use * pe

            # WM update implementing win-stay / lose-devalue
            if r > 0:
                # Set W to a peaked distribution on chosen action (win-stay)
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Lose: devalue chosen action slightly; mix with uniform to avoid zeros
                devalue = 0.2
                w[s, a] = max(0.0, w[s, a] - devalue)
                # Renormalize by adding back mass uniformly
                wsum = w[s, :].sum()
                if wsum < 1e-8:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= wsum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian-like WM counts with interference and choice perseveration bias.

    Description:
    - RL: single learning rate; softmax policy.
    - WM: maintains Dirichlet-like counts per state (mapped to probabilities) that are decayed toward a symmetric prior.
    - Interference increases WM decay with larger set sizes: wm_decay_eff = wm_decay_base * (nS/3).
    - Perseveration bias: WM policy is biased toward the last action taken in a state.
    - Mixture of WM and RL policies.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values (0..1).
    - model_parameters[1] = wm_weight (float): Base WM mixture weight (0..1).
    - model_parameters[2] = softmax_beta (float): Inverse temperature for RL softmax; internally x10.
    - model_parameters[3] = wm_decay_base (float): Base decay rate of WM counts toward prior (0..1), scaled by nS/3.
    - model_parameters[4] = wm_alpha (float): Symmetric prior concentration for WM counts (>0).
    - model_parameters[5] = pers_bias (float): Additive bias for repeating the last action in a state (>=0).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_alpha, pers_bias = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM "counts" initialized at symmetric prior
        counts = wm_alpha * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # will hold normalized counts each trial for policy
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration bias
        last_action = -1 * np.ones(nS, dtype=int)

        # Interference-driven decay scaling with set size
        wm_decay_eff = wm_decay_base * (float(nS) / 3.0)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # Normalize counts to obtain WM probabilities
            c_s = counts[s, :].copy()
            c_sum = np.sum(c_s)
            if c_sum <= 0:
                W_s = w_0[s, :].copy()
            else:
                W_s = c_s / c_sum

            # Apply perseveration bias to WM logits by boosting last action in this state
            W_logits = W_s.copy()
            if last_action[s] >= 0:
                W_logits[last_action[s]] += pers_bias
                # Renormalize logits to probabilities through softmax with high beta
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from biased logits using softmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_logits - W_logits[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay of counts toward symmetric prior
            counts[s, :] = (1.0 - wm_decay_eff) * counts[s, :] + wm_decay_eff * wm_alpha

            # WM update of counts based on feedback
            if r > 0:
                # Reward: increment chosen action count
                counts[s, a] += 1.0
            else:
                # No reward: mild evidence against chosen action via small decrement toward prior baseline
                # Ensure counts stay positive
                penal = min(0.2, counts[s, a] - 1e-6)
                counts[s, a] -= penal

            # Update last action for perseveration
            last_action[s] = a

            # Keep a mirrored normalized copy in w for consistency with template variables
            c_sum = counts[s, :].sum()
            if c_sum > 0:
                w[s, :] = counts[s, :] / c_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p