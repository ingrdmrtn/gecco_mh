def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + PE-gated WM encoding with set-size–dependent gate.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM store: When a reward is received, the model encodes a one-hot cache of the rewarded action
      for that state only if the unsigned RL prediction error exceeds a gate threshold.
    - WM policy: softmax with very high beta over the WM cache (deterministic when present; uniform otherwise).
    - Mixture: convex combination of WM and RL policies.
    - Set size effect: the PE gate becomes harder to surpass in larger sets (threshold scales up with set size).

    Parameters
    - lr: RL learning rate (0..1).
    - wm_weight: Base weight for WM policy in the mixture (0..1).
    - softmax_beta: Inverse temperature for RL policy; internally scaled by 10 to expand range (>0).
    - pe_gate: Base PE threshold for WM encoding (>=0). Larger means stricter gating.
    - gate_ss_scale: Exponent controlling how gate scales with set size (>=0).
        Effective threshold = pe_gate * (nS / 3) ** gate_ss_scale.
        Larger sets raise the threshold, reducing WM encoding.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, pe_gate, gate_ss_scale = parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight is base; gating impacts whether WM holds informative content
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        # Set-size–dependent gate
        gate_threshold = pe_gate * (max(nS, 1) / 3.0) ** max(gate_ss_scale, 0.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: nearly deterministic softmax over working memory W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: encode rewarded action only if PE exceeds set-size–scaled threshold
            # WM content otherwise persists (no decay), modeling a stable cache when encoded.
            if r > 0 and abs(delta) > gate_threshold:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with state-specific stickiness + WM as win-stay policy + set-size–dependent lapses.

    Mechanism
    - RL: tabular Q-learning with softmax.
      Additionally, a state-specific stickiness bias adds value to repeating the last action in that state.
    - WM: win-stay heuristic. If the last choice in a state was rewarded, WM favors repeating that action;
      otherwise WM is uniform for that state.
    - Mixture: WM and RL policies are mixed, then a set-size–dependent lapse blends with uniform random choice.
    - Set size effects:
        • Lapse rate increases with set size (harder blocks -> more random responding).

    Parameters
    - lr: RL learning rate (0..1).
    - wm_weight: Weight for WM policy in the WM/RL mixture (0..1).
    - softmax_beta: Inverse temperature for RL policy; internally scaled by 10 (>0).
    - stickiness: Additive value bonus applied to the last chosen action in the current state (can be >=0).
    - lapse_base: Base lapse probability of random responding (0..1).
    - lapse_ss_slope: Increment in lapse per unit set size increase from 3 to 6 (>=0).
        Effective lapse = clip(lapse_base + lapse_ss_slope * ((nS - 3) / 3), 0, 1).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, lapse_base, lapse_ss_slope = parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # WM distribution per state
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness and WM win-stay criterion
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Set-size–dependent lapse
        lapse = lapse_base + lapse_ss_slope * ((nS - 3.0) / 3.0)
        lapse = float(np.clip(lapse, 0.0, 1.0))
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Apply state-specific stickiness bias to RL values
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM win-stay policy construction
            if last_action[s] >= 0 and last_reward[s] > 0:
                # Favor repeating last rewarded action
                wm_pref = np.zeros(nA)
                wm_pref[last_action[s]] = 1.0
                W_s = wm_pref
            else:
                # No WM guidance -> uniform
                W_s = w_0[s, :]

            # Use softmax with high beta to convert W_s into a choice probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix WM and RL
            p_mix = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl

            # Apply lapse (blend with uniform)
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update memory traces for next trial
            last_reward[s] = r
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    Dual-rate RL + recency-weighted WM usage via time since last visit (ISI).

    Mechanism
    - RL: tabular Q-learning with separate learning rates for positive and negative outcomes.
    - WM store: one-shot cache of the last rewarded action for each state (if any).
    - WM usage: the mixture weight decays exponentially with the number of trials since the state
      was last observed (ISI). Larger set sizes naturally yield longer ISIs, reducing WM influence.
    - Policy: mixture of WM and RL policies.

    Set size effect
    - Not by explicit parameter; rather, larger sets increase ISI, which lowers WM mixture weight through
      the recency decay.

    Parameters
    - lr_pos: Learning rate for positive prediction errors (0..1).
    - lr_neg: Learning rate for negative prediction errors (0..1).
    - wm_weight: Maximum WM mixture weight at zero ISI (0..1).
    - softmax_beta: Inverse temperature for RL policy; internally scaled by 10 (>0).
    - decay_rate: Exponential decay rate controlling how fast WM weight drops with ISI (>=0).
        Effective WM weight at time t for state s: wm_weight * exp(-decay_rate * ISI_s).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, decay_rate = parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last visit time and build WM cache upon rewards
        last_seen = -1 * np.ones(nS, dtype=int)  # -1 means never seen
        t_global = 0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute ISI in number of trials since last occurrence of state s
            if last_seen[s] < 0:
                isi = 0
            else:
                isi = t_global - last_seen[s]

            # RL policy probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Recency-weighted WM mixture
            wm_weight_eff = np.clip(wm_weight * np.exp(-max(decay_rate, 0.0) * float(isi)), 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with dual learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s][a] += lr_pos * pe
            else:
                q[s][a] += lr_neg * pe

            # WM update: store last rewarded action as a one-hot cache
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            # Update last seen time
            last_seen[s] = t_global
            t_global += 1

        blocks_log_p += log_p

    return -blocks_log_p