def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-gated WM access, WM decay, and global interference.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight : float
            Arbitration weight mixing WM and RL policies (higher -> more WM influence).
        softmax_beta : float
            Inverse temperature for RL policy (internally scaled by 10).
        wm_decay : float
            Per-trial decay of the WM trace toward uniform for the current state.
        wm_interference : float
            Global interference toward uniform across all states on each trial (models load-driven interference).
        gate_slope : float
            Controls how strongly set size gates WM access; gate = sigmoid(gate_slope * (3.5 - nS)).

    Set-size effects
    ----------------
    - WM access is gated by set size: smaller sets increase gate, moving p_wm toward WM policy; larger sets reduce it, adding uniform noise.
    - Global interference increases forgetting across all states irrespective of the current stimulus (models load).
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_interference, gate_slope = model_parameters

    softmax_beta *= 10.0  # RL inverse temperature scaling
    softmax_beta_wm = 50.0  # very deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM gate (higher for small sets)
        gate = 1.0 / (1.0 + np.exp(-gate_slope * (3.5 - nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM softmax over WM trace with a uniform lapse mixed in based on set-size gate.
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / max(denom_wm, 1e-12)
            # Blend WM softmax with uniform according to gate (smaller set -> larger gate -> less lapse)
            p_wm = gate * p_wm_soft + (1.0 - gate) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Global interference toward uniform across all states
            w = (1.0 - wm_interference) * w + wm_interference * w_0

            # 2) State-specific decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 3) Gate-weighted encoding: move current state's WM toward one-hot on chosen action
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - gate) * w[s, :] + gate * one_hot

            # Normalize to keep W_s a proper distribution
            z = np.sum(w[s, :])
            if z > 0:
                w[s, :] /= z

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM-as-choice-kernel with set-size-driven lapse and WM decay.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight : float
            Arbitration weight mixing WM and RL policies (higher -> more WM influence).
        softmax_beta : float
            Inverse temperature for RL policy (internally scaled by 10).
        ck_alpha : float
            Learning rate for the WM choice kernel (how strongly the last choice is stored).
        ck_decay : float
            Per-trial decay of the WM choice kernel toward uniform for the current state.
        ss_lapse_gain : float
            Gain controlling how much set size increases WM lapse; lapse = 1 - exp(-ss_lapse_gain*(nS-3)/3).

    Set-size effects
    ----------------
    - WM policy is blended with uniform via a lapse rate that increases with set size, degrading WM guidance in larger sets.
    """
    lr, wm_weight, softmax_beta, ck_alpha, ck_decay, ss_lapse_gain = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic readout of the choice kernel

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # Interpret w as a choice kernel (recent preference) per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM lapse
        lapse = 1.0 - np.exp(-ss_lapse_gain * max(nS - 3, 0) / 3.0)
        lapse = np.clip(lapse, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM as a choice kernel: softmax over W_s, with set-size lapse to uniform
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / max(denom_wm, 1e-12)
            p_wm = (1.0 - lapse) * p_wm_soft + lapse * (1.0 / nA)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay the choice kernel toward uniform for the current state
            w[s, :] = (1.0 - ck_decay) * w[s, :] + ck_decay * w_0[s, :]

            # Encode the chosen action irrespective of reward (choice perseveration)
            w[s, :] = (1.0 - ck_alpha) * w[s, :]
            w[s, a] += ck_alpha

            # Normalize to maintain a distribution
            z = np.sum(w[s, :])
            if z > 0:
                w[s, :] /= z

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise- and valence-asymmetric WM encoding and set-size-dependent WM decay.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight : float
            Arbitration weight mixing WM and RL policies (higher -> more WM influence).
        softmax_beta : float
            Inverse temperature for RL policy (internally scaled by 10).
        wm_alpha_pos : float
            WM encoding strength for positive outcomes (r=1).
        wm_alpha_neg : float
            WM encoding strength for negative outcomes (r=0); can be weaker/stronger than positive.
        wm_decay_slope : float
            Controls how strongly set size increases WM decay toward uniform; decay = 1 - exp(-wm_decay_slope*(nS-1)).

    Set-size effects
    ----------------
    - WM decay increases with set size, reducing WM fidelity for larger sets.
    - WM encoding is modulated by outcome valence and surprise (prediction error magnitude).
    """
    lr, wm_weight, softmax_beta, wm_alpha_pos, wm_alpha_neg, wm_decay_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay factor (applied each trial on current state)
        wm_decay = 1.0 - np.exp(-wm_decay_slope * (nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Pure WM softmax readout; implicitly degraded by decay that depends on set size
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Set-size dependent decay toward uniform for current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) Surprise- and valence-weighted encoding toward one-hot of chosen action
            # Surprise via |delta|; valence via wm_alpha_pos/neg
            alpha_valence = wm_alpha_pos if r > 0.0 else wm_alpha_neg
            alpha_eff = alpha_valence * abs(delta)
            alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - alpha_eff) * w[s, :] + alpha_eff * one_hot

            # Normalize
            z = np.sum(w[s, :])
            if z > 0:
                w[s, :] /= z

        blocks_log_p += log_p

    return -blocks_log_p