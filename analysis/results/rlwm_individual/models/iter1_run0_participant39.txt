def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with global decay and lapse.

    Mechanism
    - RL: standard Q-learning with softmax policy.
    - WM: associative one-shot store for rewarded state-action pairs, represented as
      a categorical distribution over actions per state that decays toward uniform.
    - Capacity-limited WM usage: the effective contribution of WM is scaled by a
      recall probability that decreases with set size via a capacity parameter.
    - Lapse: with small probability, choices are random.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1)
        - wm_weight: baseline WM mixture weight (0..1), applied when recalled
        - softmax_beta: RL inverse temperature; internally scaled by 10
        - k_capacity: effective WM capacity in items; recall probability = min(1, k_capacity / set_size)
        - wm_decay: WM global decay toward uniform each trial (0..1); also controls write strength (1 - wm_decay)
        - lapse: lapse probability mixing with uniform random policy (0..1)

    Set-size effects
    ----------------
    - WM reliance per block scales with recall probability pr = min(1, k_capacity / set_size).
      Effective WM mixture weight: wm_weight_eff = wm_weight * pr.
      Larger set size lowers pr (and thus WM contribution).
    """
    lr, wm_weight, softmax_beta, k_capacity, wm_decay, lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic WM when available
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM mixture weight for the whole block
        pr = min(1.0, float(k_capacity) / float(nS + 1e-12))
        wm_weight_eff = np.clip(wm_weight * pr, 0.0, 1.0)

        write_strength = 1.0 - np.clip(wm_decay, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (probability of chosen action)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (probability of chosen action)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0

            # WM write on reward (one-shot overwrite blended by write_strength)
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - write_strength) * w[s] + write_strength * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-dependent WM precision and a stay (perseveration) controller.

    Mechanism
    - RL: Q-learning with softmax.
    - WM: one-shot storage on rewarded trials; no explicit learning rate. WM precision
      (inverse temperature) decreases as set size increases, modeling interference.
    - Mixture: fixed WM mixture weight per block, but WM precision depends on set size.
    - Stay controller: an additional mixture that favors repeating the last action chosen
      in the same state, with strength controlled by pers_beta (converted to a mixture weight).

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1)
        - wm_weight: WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature; internally scaled by 10
        - beta_wm0: WM precision baseline at set size 3 (>0)
        - eta_wm_noise: scaling of precision drop with load (>=0); beta_wm = beta_wm0 / (1 + eta*(set_size-3))
        - pers_beta: stay bias strength; transformed via sigmoid to a stay mixture weight

    Set-size effects
    ----------------
    - WM precision beta_wm declines with set size: beta_wm = beta_wm0 / (1 + eta*(nS-3)).
      This makes WM more noisy under higher load without changing WM mixture weight.
    """
    lr, wm_weight, softmax_beta, beta_wm0, eta_wm_noise, pers_beta = model_parameters
    softmax_beta *= 10
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size dependent WM precision
        denom = 1.0 + max(0.0, eta_wm_noise) * max(0, nS - 3)
        softmax_beta_wm = max(1e-6, beta_wm0 / denom)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for stay controller
        last_action = -1 * np.ones(nS, dtype=int)

        # Stay mixture weight via sigmoid transform of pers_beta
        stay_weight = 1.0 / (1.0 + np.exp(-pers_beta))
        stay_weight = np.clip(stay_weight, 0.0, 1.0)

        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM action probabilities (for the chosen action)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Base RL-WM mixture
            p_base = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl

            # Stay component: probability mass for repeating last action in this state
            p_stay = 1.0 if (last_action[s] == a) and (last_action[s] != -1) else 0.0

            # Combine with stay mixture
            p_total = (1 - stay_weight) * p_base + stay_weight * p_stay
            # Add small floor via uniform if p_total becomes zero (e.g., first visit with stay option)
            if p_total <= 0:
                p_total = 1e-12
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM write on reward: store the successful action deterministically
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = onehot
            else:
                # On no reward, leave WM trace as is (implicit interference handled by lower beta_wm at larger set sizes)
                pass

            # Update last action for stay controller
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with load-suppressed weight and passive decay.

    Mechanism
    - RL: Q-learning with different learning rates for positive vs. negative prediction errors.
    - WM: fast associative store for rewarded actions with passive global decay toward uniform each trial.
    - Mixture: WM contribution is suppressed by set size via a logistic load function.

    Parameters
    ----------
    model_parameters : tuple
        - alpha_pos: RL learning rate for positive prediction errors (0..1)
        - alpha_neg: RL learning rate for negative prediction errors (0..1)
        - wm_weight_base: baseline WM mixture weight at low load (0..1)
        - softmax_beta: RL inverse temperature; internally scaled by 10
        - load_slope: positive slope controlling how quickly WM weight drops with set size
                      wm_weight_eff = wm_weight_base / (1 + exp(load_slope*(set_size-3)))
        - decay_wm: WM global decay toward uniform each trial (0..1)

    Set-size effects
    ----------------
    - WM reliance per block is attenuated by set size using a logistic function:
      wm_weight_eff = wm_weight_base / (1 + exp(load_slope * (nS - 3))).
      Larger set sizes (e.g., 6) yield smaller effective WM mixture weights.
    """
    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, load_slope, decay_wm = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # deterministic WM when not decayed/interfered
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Logistic load suppression of WM weight
        wm_weight_eff = wm_weight_base / (1.0 + np.exp(load_slope * (float(nS) - 3.0)))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = alpha_pos if pe >= 0 else alpha_neg
            q[s][a] += lr_use * pe

            # WM global decay toward uniform
            w = (1 - decay_wm) * w + decay_wm * w_0

            # WM write on reward
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - decay_wm) * w[s] + decay_wm * onehot

        blocks_log_p += log_p

    return -blocks_log_p