def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity-limited WM, reward-gated WM writes, and set-size-scaled arbitration.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10 internally).
    - WM system: softmax over W with high inverse temperature (near-deterministic).
    - Mixture: wm_weight is scaled by the fraction of items that can be held in WM, min(1, K/nS).

    Learning:
    - RL: Rescorla-Wagner with learning rate lr.
    - WM: For the current state, decay toward uniform at rate wm_decay, then reward-gated write:
          if reward==1, move W toward a one-hot on the chosen action with strength wm_learn;
          if reward==0, only decay happens (no new write). Interference across states is implicit via capacity scaling.

    Set-size effects:
    - Arbitration weight scales as min(1, K/nS), so WM influence diminishes as set size exceeds K.

    Parameters (6):
    - lr: RL learning rate.
    - wm_weight: Base WM mixture weight (scaled per block by min(1, K/nS)).
    - softmax_beta: RL softmax inverse temperature (internally multiplied by 10).
    - wm_decay: WM decay toward uniform each visit (0=no decay, 1=full reset before any write).
    - wm_learn: Strength of WM write toward the chosen action when rewarded (0=no write, 1=replace).
    - K: WM capacity (continuous, in [0, 6]); effective WM contribution scales with min(1, K/nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_learn, K = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-scaled arbitration weight based on capacity
        cap_scale = min(1.0, max(0.0, K) / max(1, nS))
        wm_weight_block = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W with high beta
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with capacity-scaled WM weight
            p_total = p_wm * wm_weight_block + (1 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform, then reward-gated write toward chosen action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with set-size-specific exploration (beta) and uncertainty-weighted WM arbitration.

    Policy:
    - RL system: softmax over Q with beta chosen by set size (beta_small for nS<=3, beta_large for nS>3).
    - WM system: softmax over W with high inverse temperature.
    - Mixture: per-trial WM weight increases with RL uncertainty (entropy of Q_s), scaled by wm_sensitivity,
               and anchored by base wm_weight. Set size impacts RL certainty through the beta selection.

    Learning:
    - RL: Rescorla-Wagner with learning rate lr.
    - WM: Recency-weighted write each trial with decay wm_decay; write strength is reward-modulated via wm_sensitivity
          using a simple gate g = sigmoid(wm_sensitivity*(r-0.5)), so rewarded trials write more strongly.

    Set-size effects:
    - Exploration temperature depends on set size (beta_small vs beta_large), affecting RL uncertainty and thus the arbitration.

    Parameters (6):
    - lr: RL learning rate.
    - wm_weight: Base WM mixture weight (0..1).
    - beta_small: RL softmax inverse temperature for small set sizes (<=3) before scaling by 10.
    - beta_large: RL softmax inverse temperature for large set sizes (>3) before scaling by 10.
    - wm_decay: WM decay toward uniform on each state visit.
    - wm_sensitivity: Scales (a) the reward-gated WM write strength and (b) the uncertainty-to-arbitration mapping.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, beta_small, beta_large, wm_decay, wm_sensitivity = model_parameters
    # We will assign softmax_beta per block based on set size and then scale by 10
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size-specific RL inverse temperature
        softmax_beta = (beta_small if nS <= 3 else beta_large) * 10

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy of RL policy (over all actions) to gauge uncertainty
            q_centered = Q_s - np.max(Q_s)
            prl_full = np.exp(softmax_beta * q_centered)
            prl_full /= np.sum(prl_full)
            entropy = -np.sum(prl_full * np.log(np.clip(prl_full, eps, 1.0)))
            # Map entropy to [0,1] via a logistic scaled by wm_sensitivity
            ent_norm = entropy / np.log(nA)  # in [0,1]
            wm_dyn = 1 / (1 + np.exp(-wm_sensitivity * (ent_norm - 0.5)))
            wm_dyn = np.clip(wm_dyn, 0.0, 1.0)
            wm_weight_eff = np.clip((1 - wm_weight) * wm_dyn + wm_weight * (1 - wm_dyn), 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay + reward-modulated write strength
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            # Reward gate g in (0,1): larger for r=1 than r=0
            g = 1.0 / (1.0 + np.exp(-wm_sensitivity * (r - 0.5)))
            w[s, :] = (1.0 - g) * w[s, :] + g * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with confidence-based arbitration and set-size-driven WM interference.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W (high inverse temperature).
    - Mixture: wm_weight is dynamically modulated by WM confidence in state s:
        conf_s = softmax(softmax_beta_wm * W_s)[a*] - second_best_prob
      and by interference that increases with set size via psi. Final weight:
        wm_weight_eff = sigmoid(wm_bias + wm_conf_beta * conf_s - psi * (nS - 3)).

    Learning:
    - RL: Rescorla-Wagner with learning rate lr.
    - WM: Per-visit decay toward uniform with rate wm_decay, and recency-based write toward chosen action
          with strength equal to conf_s (confidence-weighted). Hence, high-confidence states are reinforced more.

    Set-size effects:
    - The arbitration weight is reduced as set size increases (via psi*(nS-3)), modeling interference load.

    Parameters (6):
    - lr: RL learning rate.
    - wm_weight: Baseline WM bias term (mapped through sigmoid internally as wm_bias).
    - softmax_beta: RL softmax inverse temperature (internally multiplied by 10).
    - wm_decay: WM decay toward uniform on each visit.
    - wm_conf_beta: Sensitivity of arbitration to WM confidence (and also used to scale confidence-weighted writes).
    - psi: Strength of set-size interference in arbitration (higher psi reduces WM weight more when nS is large).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_conf_beta, psi = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute an additive bias term from baseline wm_weight
        # Map baseline weight in [0,1] to bias domain via logit
        wm_bias = np.log(np.clip(wm_weight, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight, eps, 1 - eps))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM confidence from W_s
            wm_probs = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_probs /= np.sum(wm_probs)
            top2 = np.sort(wm_probs)[-2:]
            conf_s = top2[-1] - top2[-2]  # margin between best and second-best in [0,1]

            # Dynamic arbitration with set-size interference
            logit_w = wm_bias + wm_conf_beta * conf_s - psi * max(0, (nS - 3))
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay + confidence-weighted write toward the chosen action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            write_strength = np.clip(conf_s * wm_conf_beta, 0.0, 1.0)
            w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p