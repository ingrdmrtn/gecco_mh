def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with recency-weighted WM maintenance and interference-driven forgetting.

    Idea:
    - RL learns slowly and always available.
    - WM stores near one-hot action policies per state but is vulnerable to interference from
      unattended states and benefits from recency via an eligibility-like trace.
    - The WM contribution is mixed with RL via a fixed wm_weight, but the quality of WM
      degrades more when set size is large due to stronger interference.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values
    - wm_weight: scalar in [0,1], base mixture weight for WM vs RL
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - lambda_trace: in [0,1], recency trace controlling how strongly recently visited states resist WM decay
    - interference_gamma: >=0, scales WM decay for unattended states; implicitly increases with set size
    - wm_alpha: in [0,1], WM learning rate toward a one-hot policy on the current state

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_trace, interference_gamma, wm_alpha = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Recency eligibility-like trace over states for WM maintenance
        e = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture (fixed base weight; WM quality is handled by maintenance dynamics below)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update recency trace
            e *= lambda_trace
            e[s] = 1.0

            # Interference-driven forgetting for unattended states:
            # decay magnitude increases with set size (more competitors)
            # decay_i = interference_gamma * (1 - e_i) * (nS / max(nS,1)) -> simplifies to interference_gamma*(1 - e_i)
            # We use a convex combination with the uniform prior w_0
            decay_vector = interference_gamma * (1.0 - e)  # shape (nS,)
            # Apply row-wise decay toward uniform for all states
            for i in range(nS):
                d_i = np.clip(decay_vector[i], 0.0, 1.0)
                w[i, :] = (1.0 - d_i) * w[i, :] + d_i * w_0[i, :]

            # WM learning on current state toward a one-hot policy for chosen action
            # This captures rapid WM encoding when the state is attended.
            w[s, :] = (1.0 - wm_alpha) * w[s, :]
            w[s, a] += wm_alpha

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with uncertainty- and load-gated arbitration and soft WM learning.

    Idea:
    - RL learns with standard delta rule; WM holds a fast-updating action distribution per state.
    - Arbitration weight for WM is higher when (a) set size is small (lower load) and
      (b) RL is uncertain (high choice entropy), reflecting a strategic shift to WM when RL is not confident.
    - WM policy has adjustable effective temperature (wm_temp) even though a high base temperature is set,
      by scaling the logits.
    - WM updates softly toward a one-hot vector for the chosen action.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate
    - wm_alpha: in [0,1], WM learning rate toward one-hot policy
    - wm_weight: real-valued bias term controlling baseline WM reliance (before gating)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_temp: positive scalar scaling WM determinism; higher => more deterministic WM
    - gate_bias: real scalar that scales how RL uncertainty reduces WM reliance (higher => less WM when uncertain)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_alpha, wm_weight, softmax_beta, wm_temp, gate_bias = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic (we further modulate via wm_temp)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL choice probabilities (full vector) to compute entropy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl)
            probs_rl /= max(np.sum(probs_rl), 1e-12)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # RL entropy (0..log(nA)); normalize to [0,1] by dividing by log(nA)
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, 1e-12, 1.0)))
            H_rl_norm = H_rl / np.log(nA)

            # WM policy with adjustable determinism via wm_temp
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * wm_temp * (W_s - W_s[a])
            denom_wm = np.sum(np.exp(logits_wm))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: WM reliance increases with lower load (1/nS) and RL uncertainty (H_rl_norm),
            # combined with a baseline bias (wm_weight) and a gate that penalizes uncertainty via gate_bias.
            # We map the linear combination through a sigmoid to keep 0..1.
            arb_input = wm_weight + (1.0 / max(nS, 1)) + (1.0 - gate_bias) * H_rl_norm
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_input))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM soft update toward one-hot; mild re-centering toward uniform for stability
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + (wm_alpha) * w_0[s, :]
            w[s, a] += wm_alpha * (1.0 - w_0[s, a])  # net effect: pulls mass to chosen action

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size scaling and action stickiness.

    Idea:
    - WM has a slot-like capacity; effective WM usage scales approximately with wm_capacity_slots/nS.
    - WM traces decay over trials; when a correct action is reinforced, WM writes a strong one-hot.
    - In addition to RL/WM, choices show perseveration: bias toward repeating the last action (stickiness).
    - Mixture weight is scaled by capacity relative to set size.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate
    - wm_weight: base WM reliance (scaled by capacity relative to set size)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_capacity_slots: positive scalar, effective number of WM slots
    - wm_decay: in [0,1], per-trial WM decay toward uniform
    - stickiness: real scalar added to the previously chosen action's logit in both RL and WM policies

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity_slots, wm_decay, stickiness = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight scaled by capacity relative to set size
        cap_ratio = min(1.0, wm_capacity_slots / max(1.0, float(nS)))
        wm_weight_eff = wm_weight * cap_ratio

        log_p = 0.0
        last_action = -1  # for stickiness

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add stickiness bias to the previously selected action (if defined)
            if last_action >= 0 and last_action < nA:
                Q_s[last_action] += stickiness
                W_s[last_action] += stickiness

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture using capacity-scaled WM weight
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM maintenance: global decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM write: if rewarded, write a strong one-hot; if not, a weak, noisy imprint
            if r > 0.5:
                # Strong write to chosen action
                w[s, :] = (1.0 - (1.0 - wm_decay)) * w[s, :]  # ensure boundedness
                w[s, a] = max(w[s, a], 1.0 - 1e-6)
            else:
                # Weak imprint: slight shift toward the chosen action to capture exploration memory
                weak = 0.25 * wm_decay
                w[s, :] = (1.0 - weak) * w[s, :]
                w[s, a] += weak

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p