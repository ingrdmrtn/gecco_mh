def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-sizeâ€“scaled precision and global interference + lapse.

    Idea:
    - RL learns Q-values with learning rate lr (kept fixed).
    - WM stores action preferences per state; when rewarded, WM sharpens toward the chosen action.
    - WM precision and contribution decrease with larger set size (capacity limit).
    - Global interference flattens WM across all states each trial (cross-talk).
    - Mixture policy between WM and RL plus a lapse component.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight before load scaling (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_beta_base: Base WM inverse temperature; scaled down by set size (0..100)
    - interference: Global WM interference per trial toward uniform (0..1)
    - lapse: Lapse probability mixed uniformly across actions (0..1)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial (constant within block)
    - Returns: negative log-likelihood of choices under the model
    """
    lr_base, wm_weight_base, softmax_beta, wm_beta_base, interference, lapse = model_parameters
    softmax_beta *= 10.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load-scaled WM parameters
        load_scale = min(1.0, 3.0 / float(nS))  # <=1 for larger sets
        wm_weight_block = max(0.0, min(1.0, wm_weight_base * load_scale))
        softmax_beta_wm = max(1.0, wm_beta_base * load_scale)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Optional adaptive lr (keep constant here to respect parameter interpretation)
            lr = lr_base

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global interference each trial: w drifts toward uniform across all states
            w = (1.0 - interference) * w + interference * w_0

            # State-specific WM update:
            if r > 0.5:
                # Strengthen the chosen action for this state by moving mass toward the chosen action
                # Using a simple sharpening: convex combination toward one-hot
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Use a moderate fixed step scaled by load (more precise with smaller sets)
                wm_step = 0.5 * load_scale
                w[s, :] = (1.0 - wm_step) * w[s, :] + wm_step * one_hot
            else:
                # No reward: slight forgetting toward uniform for this state
                wm_forget = 0.2
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Normalize to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Win-Stay/Lose-Shift working memory with asymmetric bias and decay.

    Idea:
    - RL updates Q-values with learning rate lr and softmax policy.
    - WM encodes a fast win-stay rule: after reward, bias strongly to the same action in that state.
    - After loss, WM biases away from the same action (lose-shift/avoidance), with controllable strength.
    - WM weight is mixed with RL (no lapse), and WM decays over time.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight of WM vs RL (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wsls_bias: Strength of WM win-stay/lose-shift bias (maps to WM softmax beta, >0)
    - wm_decay: Per-trial leak of WM toward uniform (0..1)
    - avoidance_bias: Additional multiplicative boost for shifting away after losses (>=0)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial
    - Returns: negative log-likelihood of choices under the model
    """
    lr_base, wm_weight, softmax_beta, wsls_bias, wm_decay, avoidance_bias = model_parameters
    softmax_beta *= 10.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM precision increases with smaller set sizes (more reliable WSLS under low load)
        load_scale = min(1.0, 3.0 / float(nS))
        softmax_beta_wm = max(1.0, wsls_bias * (1.0 + 2.0 * load_scale))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Use fixed RL lr
            lr = lr_base

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy is softmax over W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture without lapse
            wm_w = max(0.0, min(1.0, wm_weight))
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform for all states (leak)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WSLS state-specific update:
            if r > 0.5:
                # Win: push probability mass to the chosen action (win-stay)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                gain = 0.6  # strong consolidation after win
                w[s, :] = (1.0 - gain) * w[s, :] + gain * one_hot
            else:
                # Loss: bias away from chosen action (lose-shift)
                # Reduce chosen action weight and redistribute to others
                avoid = min(0.6 * (1.0 + avoidance_bias), 0.95)
                w[s, a] = (1.0 - avoid) * w[s, a]
                redistribute = avoid * (1.0 / (nA - 1))
                for a2 in range(nA):
                    if a2 != a:
                        w[s, a2] = (1.0 - avoid) * w[s, a2] + redistribute

            # Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + novelty-gated WM with visit-based arbitration and set-size sensitivity.

    Idea:
    - RL updates Q-values with learning rate lr (per template).
    - WM stores fast mappings when evidence arrives (reward), decays otherwise.
    - Arbitration weight for WM is stronger for novel/rarely visited states and under low set size.
      This captures reliance on WM early in learning and when capacity allows.
    - WM has its own precision (softmax beta).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: Base WM weight before novelty/load gating (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_beta: WM inverse temperature (e.g., 10..100 for near-deterministic WM)
    - count_decay: Pseudocount that softens novelty effect (>=0)
    - cap_sensitivity: Exponent controlling how strongly set size reduces WM weight (>=0)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial
    - Returns: negative log-likelihood of choices under the model
    """
    lr_base, wm_weight0, softmax_beta, wm_beta, count_decay, cap_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = max(1.0, wm_beta)
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track state visit counts to compute novelty
        visits = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective RL learning rate per trial equals parameter (template)
            lr = lr_base

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Novelty- and capacity-gated WM arbitration
            # Novelty in [0,1]: high when visits low
            novelty = 1.0 / np.sqrt(visits[s] + 1.0 + count_decay)
            # Capacity scaling: (3/nS)^cap_sensitivity in [0,1]
            cap_scale = (3.0 / float(nS)) ** max(0.0, cap_sensitivity)
            wm_w = wm_weight0 * novelty * cap_scale
            wm_w = max(0.0, min(1.0, wm_w))

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # Increment visits after computing novelty
            visits[s] += 1.0

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform for this state (more decay as visits grow; less WM needed)
            decay = 1.0 - (1.0 / (visits[s] + 1.0))
            decay = min(max(decay, 0.0), 0.9)  # keep stable
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # If rewarded, encode chosen action strongly (one-shot-like)
            if r > 0.5:
                encode = 0.7
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - encode) * w[s, :] + encode * one_hot

            # Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p