def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference across states that scales with set size.

    Idea
    - RL learns action values per state with a standard delta rule.
    - WM stores recent action choices and decays toward uniform, but when writing to WM,
      some of the memory "spills over" (interferes) into other states, and this interference
      increases with set size (load).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (e.g., 3 or 6).
    model_parameters : iterable
        Tuple/list of 6 parameters:
        - lr: RL learning rate (0..1).
        - wm_weight: Mixture weight of WM in the policy (0..1).
        - softmax_beta: RL inverse temperature (scaled internally x10).
        - wm_decay: Decay of WM toward uniform per trial (0..1).
        - interf_base: Baseline WM interference (0..1).
        - interf_k: Increase of interference with set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, interf_base, interf_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent interference: more spillover with larger sets
        rho = np.clip(interf_base + interf_k * max(0, nS - 3), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM write with interference: a fraction (1 - rho) boosts correct row,
            # and a fraction rho is distributed to the same action column across other states.
            ins = 1.0  # one-shot write strength
            # Strength to the current state's chosen action
            w[s, :] = (1.0 - ins * (1.0 - rho)) * w[s, :]
            w[s, a] += ins * (1.0 - rho)

            # Spillover to other states on the same action column
            if nS > 1 and rho > 0.0:
                spill = ins * rho / (nS - 1)
                for s_other in range(nS):
                    if s_other != s:
                        w[s_other, a] += spill

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and reward-gated WM learning.

    Idea
    - RL learns values per state.
    - WM learns the last rewarded action per state (reward-gated Hebbian update).
    - The mixture weight of WM is computed on-the-fly from RL policy uncertainty (entropy)
      and is modulated by set size: higher set size reduces reliance on WM.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (e.g., 3 or 6).
    model_parameters : iterable
        Tuple/list of 6 parameters:
        - lr: RL learning rate (0..1).
        - wm_bias: Baseline logit for WM weight (real).
        - softmax_beta: RL inverse temperature (scaled internally x10).
        - wm_eta: WM learning rate for rewarded actions (0..1).
        - arb_k: Gain linking RL entropy to WM weight (real).
        - set_k: Penalty on WM weight per extra item beyond 3 (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_bias, softmax_beta, wm_eta, arb_k, set_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        set_penalty = set_k * max(0, nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL action probabilities for entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits)
            pi_rl /= np.sum(pi_rl)
            # Chosen-action prob under RL (using the provided form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL uncertainty (entropy)
            H = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))

            # Arbitration: higher RL entropy -> more WM; larger set -> less WM
            wm_weight_eff = sigmoid(wm_bias + arb_k * H - set_penalty)

            # WM policy probability of chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Reward-gated WM update: only strengthen memory on rewarded trials
            if r > 0.0:
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            # Optional mild passive drift toward uniform to avoid runaway (small, tied to wm_eta)
            drift = 0.1 * wm_eta
            w = (1.0 - drift) * w + drift * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recall-and-lapse mechanism: larger set sizes reduce WM recall,
    increasing lapses toward uniform choice.

    Idea
    - RL learns action values per state.
    - WM stores the last rewarded action per state.
    - At choice time, the WM policy is a mixture of a recalled mapping and a uniform lapse.
      The recall probability decays with set size, capturing capacity limitations.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (e.g., 3 or 6).
    model_parameters : iterable
        Tuple/list of 6 parameters:
        - lr: RL learning rate (0..1).
        - wm_weight: Baseline mixture weight for WM vs RL (0..1).
        - softmax_beta: RL inverse temperature (scaled internally x10).
        - wm_recall_base: Baseline WM recall probability at set size 3 (0..1).
        - lapse: Degree to which WM failure yields uniform responding (0..1).
        - size_k: How strongly recall drops with set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_recall_base, lapse, size_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 1.0)
    wm_recall_base = np.clip(wm_recall_base, 0.0, 1.0)

    blocks_log_p = 0.0
    eps = 1e-12
    nA_global = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent recall probability
        # Exponential drop from base as set size increases
        p_recall = wm_recall_base * np.exp(-size_k * max(0, nS - 3))
        p_recall = np.clip(p_recall, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy under recall (deterministic softmax on W_s)
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM lapse mixture: with probability (1 - p_recall) lapse to uniform
            p_wm = p_recall * p_wm_det + (1.0 - p_recall) * (1.0 / nA_global)

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store last rewarded action; also forget more when recall is low
            if r > 0.0:
                k = 1.0  # one-shot consolidation of rewarded mapping
                w[s, :] = (1.0 - k) * w[s, :]
                w[s, a] += k

            # Forgetting tied to recall failure and lapse
            forget = (1.0 - p_recall) * lapse
            w = (1.0 - forget) * w + forget * w_0

        blocks_log_p += log_p

    return -blocks_log_p