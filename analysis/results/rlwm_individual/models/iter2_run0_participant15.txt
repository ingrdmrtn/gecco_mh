def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, gated WM with decay.

    Mechanism
    - RL: tabular Q-learning with a single learning rate.
    - WM: a decaying distribution per state that becomes sharply peaked for rewarded actions when gated-in.
    - Arbitration: fixed wm_weight but scaled by a probabilistic capacity factor that depends on set size.

    Set-size dependence
    - WM contribution is down-weighted as set size grows: p_capacity = min(1, capacity_k / set_size).
    - WM precision is set by a high softmax beta, but its influence is scaled by gating and capacity.
    - WM decays each trial toward uniform with rate phi_decay, stronger decay harms large sets.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Base weight of WM in arbitration (0..1).
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled up by 10.
        phi_decay : float
            WM decay rate toward uniform on each trial (0..1).
        capacity_k : float
            Effective WM capacity in number of items; WM usage scales as min(1, capacity_k / set_size).
        wm_gate : float
            Probability/strength of successfully storing a rewarded action in WM (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, phi_decay, capacity_k, wm_gate = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity factor based on set size
        p_capacity = min(1.0, capacity_k / max(1.0, nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM distribution
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight scaled by capacity
            wm_weight_eff = np.clip(wm_weight * p_capacity, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * w_0[s, :]

            # WM storage (gated) on rewarded trials: sharpen toward chosen action
            if r > 0.5:
                # Move probability mass toward the chosen action with strength wm_gate
                w[s, a] += wm_gate * (1.0 - w[s, a])
                other_idx = [i for i in range(nA) if i != a]
                if len(other_idx) > 0:
                    scale = (1.0 - w[s, a]) / max(tiny, np.sum(w[s, other_idx]))
                    w[s, other_idx] *= (1.0 - wm_gate) * scale

            # Normalize to avoid drift
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + WM with set-size-dependent interference + perseveration bias.

    Mechanism
    - RL: Q-learning with eligibility traces (lambda) to generalize credit across recently chosen pairs.
    - WM: stores the most recently rewarded action for each state as a peaked distribution; interference adds noise
      that increases with set size.
    - Arbitration: fixed wm_weight mixture.
    - Perseveration: adds a choice stickiness bias to the most recent action, implemented in the RL softmax logits.

    Set-size dependence
    - WM distribution is mixed with uniform by an interference noise factor that scales with set size:
      noise = clip(wm_noise_base * (set_size / 3), 0..0.99).

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Weight of WM in arbitration (0..1).
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled up by 10.
        lambda_elig : float
            Eligibility trace decay parameter (0..1).
        rho_sticky : float
            Choice perseveration weight added to the last chosen action's logit (>=0).
        wm_noise_base : float
            Base WM interference level mixed with uniform; effective noise scales as wm_noise_base * (set_size/3).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_elig, rho_sticky, wm_noise_base = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        # State-dependent WM interference noise
        noise = np.clip(wm_noise_base * (nS / 3.0), 0.0, 0.99)

        last_action = None

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL logits with perseveration added
            Q_s = q[s, :].copy()
            if last_action is not None:
                Q_s[last_action] += rho_sticky

            # WM distribution with interference
            W_s_clean = w[s, :]
            W_s = (1.0 - noise) * W_s_clean + noise * w_0[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Update eligibilities: decay and set current SA to 1
            e *= lambda_elig
            e[s, a] = 1.0
            delta = r - q[s, a]
            q += lr * delta * e

            # WM update: if rewarded, store one-hot; if not, mild relaxation toward uniform
            if r > 0.5:
                w[s, :] = tiny * np.ones(nA)
                w[s, a] = 1.0
            else:
                # small relaxation toward uniform (prevents stale incorrect memories)
                relax = 0.1
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

            # Normalize
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-modulated learning rate + WM with retrieval failures that increase with set size + action bias.

    Mechanism
    - RL: learning rate increases with surprise |r - Q(s,a)| (Pearce-Hall style).
    - WM: stores rewarded actions as peaked distributions; retrieval may fail with probability rising with set size.
    - Arbitration: WM weight scaled by retrieval success probability on each trial.
    - Motor/choice bias: adds a constant bias to action 0 in both RL and WM policies.

    Set-size dependence
    - WM retrieval success: p_retrieve = clip(1 - fail_base * ((set_size - 3)/3), 0..1). Larger sets reduce WM use.

    Parameters
    ----------
    model_parameters : tuple
        lr0 : float
            Baseline RL learning rate (0..1).
        wm_weight : float
            Base weight of WM in arbitration (0..1).
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled up by 10.
        kappa_surprise : float
            Scale on surprise for modulating the RL learning rate (>=0), lr_eff = clip(lr0 + kappa*|delta|, 0..1).
        fail_base : float
            Base WM retrieval failure rate (>=0); effective failure grows with set size.
        bias0 : float
            Additive bias to action 0 logits (can be positive or negative).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr0, wm_weight, softmax_beta, kappa_surprise, fail_base, bias0 = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Retrieval success probability from set size
        p_retrieve = np.clip(1.0 - fail_base * ((nS - 3.0) / 3.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL logits with action 0 bias
            Q_s = q[s, :].copy()
            Q_s[0] += bias0

            # WM logits with action 0 bias
            W_s = w[s, :].copy()
            if 0 < len(W_s):
                # Convert to logits by taking log probabilities baseline; we keep softmax over W_s distribution
                # but add bias by shifting the preferred action 0
                W_s[0] = min(1.0 - tiny, W_s[0] + (1.0 - W_s[0]) * (1.0 / (1.0 + np.exp(-bias0))) - 0.5 * (1.0 / (1.0 + np.exp(-bias0))))

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration with retrieval probability
            wm_weight_eff = np.clip(wm_weight * p_retrieve, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update with surprise-modulated learning rate
            delta = r - q[s, a]
            lr_eff = np.clip(lr0 + kappa_surprise * abs(delta), 0.0, 1.0)
            q[s, a] += lr_eff * delta

            # WM update: consolidate rewarded actions, mild decay otherwise
            if r > 0.5:
                w[s, :] = tiny * np.ones(nA)
                w[s, a] = 1.0
            else:
                decay = 0.05
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Normalize
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p