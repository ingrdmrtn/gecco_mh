def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + reward-gated WM with set-size-dependent WM precision.

    Idea:
    - RL: standard delta rule over state-action values with softmax policy.
    - WM: represents a short-term action tendency per state, updated only when rewarded
      (reward-gated Hebbian update). When rewarded, WM moves toward the chosen action;
      otherwise WM is left unchanged (no decay parameter).
    - Policy: convex mixture of RL and WM.
    - Set-size effect: WM precision decreases with larger set size via a logistic mapping
      from set size into WM inverse temperature. Thus, WM is more precise in set size 3
      and more noisy in set size 6.

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate for Q updates.
    - eta_wm: float in [0,1]. WM learning rate used on rewarded trials.
    - wm_weight: float in [0,1]. Mixture weight on WM policy (state-independent baseline).
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_beta_base: float. Baseline for WM inverse temperature in a logistic function.
    - wm_beta_sens: float. Sensitivity of WM inverse temperature to set size (negative values make WM less precise at larger set sizes).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, eta_wm, wm_weight, beta_rl, wm_beta_base, wm_beta_sens = model_parameters
    softmax_beta = beta_rl * 10.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM inverse temperature via logistic on set size
        # softmax_beta_wm(nS) = 50 * sigmoid(wm_beta_base + wm_beta_sens*(nS-3))
        def wm_beta_from_setsize(ss):
            z = wm_beta_base + wm_beta_sens * (ss - 3.0)
            sig = 1.0 / (1.0 + np.exp(-z))
            return 50.0 * sig + eps

        softmax_beta_wm = wm_beta_from_setsize(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Reward-gated WM update toward chosen action (no change when r=0)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice stickiness + error-correcting WM and set-size-dependent WM weight.

    Idea:
    - RL: delta rule with softmax policy plus an additive choice stickiness term that biases
      repeating the most recent action in that state.
    - WM: stores the most recently rewarded action for each state (error-correcting mapping).
      When a choice is rewarded, WM is overwritten to that action; otherwise it stays as is.
      WM policy is deterministic (high inverse temperature).
    - Policy: mixture of RL and WM. The WM mixture weight depends on set size through a
      logistic function over set size (more weight in small sets, less in large sets).

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - stickiness: float. Additive bias favoring repeating the last action taken in a state.
    - wm_weight_intercept: float. Intercept for the logistic mapping of WM mixture weight over set size.
    - wm_weight_slope: float. Slope for the logistic mapping of WM mixture weight over set size.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta_rl, stickiness, wm_weight_intercept, wm_weight_slope = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # stores last rewarded action distribution
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # last action taken in each state (for stickiness)
        lastA = -np.ones(nS, dtype=int)

        # Set-size-dependent WM weight via logistic over set size
        z = wm_weight_intercept + wm_weight_slope * (nS - 3.0)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-z))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with stickiness added to that state's last action
            Q_s = q[s, :].copy()
            if lastA[s] >= 0:
                Q_s[lastA[s]] += stickiness

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (most recently rewarded action)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (without stickiness in learning)
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update only on rewarded trials: encode the rewarded action
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update last action for stickiness
            lastA[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall associability, Q forgetting, and capacity-limited WM.

    Idea:
    - RL: prediction-error-weighted learning via a Pearce-Hall associability per state.
      Associability tracks recent absolute PE and scales the effective learning rate.
      Q-values also forget toward uniform to capture interference across larger choice sets.
    - WM: capacity-limited store that encodes rewarded actions. WM influence is downscaled
      when set size exceeds capacity.
    - Policy: mixture of RL and WM. WM policy is sharp; RL uses softmax.

    Parameters (tuple/list):
    - base_lr: float in [0,1]. Baseline RL learning rate.
    - assoc_alpha: float in [0,1]. Update rate for associability from absolute PE.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_weight: float in [0,1]. Baseline mixture weight for WM.
    - wm_capacity_items: float > 0. Effective number of items WM can hold; downscales weight when nS > capacity.
    - forget_q: float in [0,1]. Forgetting rate toward uniform for Q-values and diffusion for WM on errors.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    base_lr, assoc_alpha, beta_rl, wm_weight, wm_capacity_items, forget_q = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Pearce-Hall associability per state, initialized to moderate value
        assoc = 0.5 * np.ones(nS)

        # Capacity scaling of WM weight
        cap_scale = min(1.0, max(eps, wm_capacity_items) / float(nS))
        wm_weight_eff = wm_weight * cap_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # Q forgetting toward uniform for the visited state
            q[s, :] = (1.0 - forget_q) * q[s, :] + forget_q * (1.0 / nA)

            # RL update with associability-scaled learning rate
            pe = r - Q_s[a]
            assoc[s] = (1.0 - assoc_alpha) * assoc[s] + assoc_alpha * abs(pe)
            lr_eff = base_lr * assoc[s]
            q[s, a] += lr_eff * pe

            # WM: rewarded overwrites to chosen action; unrewarded diffuses toward uniform
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = (1.0 - forget_q) * w[s, :] + forget_q * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects:
- Model 1: WM precision (inverse temperature) decreases with larger set size via wm_beta_base and wm_beta_sens.
- Model 2: WM mixture weight decreases with set size via a logistic function parameterized by wm_weight_intercept and wm_weight_slope.
- Model 3: WM influence is capacity-limited: effective WM weight scales by wm_capacity_items / nS when nS exceeds capacity; additionally, Q forgetting (forget_q) can exacerbate performance drops at larger set sizes.