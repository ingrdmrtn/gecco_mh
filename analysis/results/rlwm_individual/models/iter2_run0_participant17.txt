Below are three standalone cognitive models tailored to the RL-WM task. Each follows the provided structure, uses up to 6 parameters, and returns the negative log-likelihood of the observed choices. I highlight how set size (3 vs 6) modulates behavior in each model.

Note: Assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-adaptive learning + WM with set-size–dependent temperature

    Mechanism:
    - RL: learning rate adapts to unsigned prediction error (Pearce-Hall style):
          alpha_t = clip(lr0 + kappa * |delta_t|, 0, 1).
    - WM: stores last rewarded action per state with a one-hot trace updated by alpha_wm.
          WM policy uses a near-deterministic softmax, but its effective temperature
          degrades with set size via:
              beta_wm_eff = 50 / (1 + wm_temp_drop * max(0, nS - 3)).
    - Mixture: convex combination of RL and WM policies using wm_weight.

    Parameters
    - lr0: float in [0,1], base RL learning rate.
      Set-size impact: none directly, but effective alpha grows with surprise which can be larger with bigger sets due to sparser learning.
    - kappa: float >= 0, scales how much unsigned prediction error boosts RL learning.
    - wm_weight: float in [0,1], mixture weight of WM policy.
    - softmax_beta: float >= 0, base RL inverse temperature (internally scaled by 10).
    - alpha_wm: float in [0,1], update rate for WM one-hot mapping when rewarded.
    - wm_temp_drop: float >= 0, increases WM noisiness as set size increases (reduces WM beta).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr0, kappa, wm_weight, softmax_beta, alpha_wm, wm_temp_drop = model_parameters

    softmax_beta *= 10.0  # RL inverse temperature scaling
    softmax_beta_wm_base = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1.0 / nA) * np.ones((nS, nA))   # WM value table
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # WM baseline (unused but kept for consistency)

        # Set-size dependent WM temperature
        size_factor = max(0.0, float(nS) - 3.0)
        softmax_beta_wm = softmax_beta_wm_base / (1.0 + wm_temp_drop * size_factor)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for chosen action probability
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy for chosen action probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with surprise-adaptive learning rate
            delta = r - Q_s[a]
            alpha_t = lr0 + kappa * abs(delta)
            if alpha_t < 0.0:
                alpha_t = 0.0
            if alpha_t > 1.0:
                alpha_t = 1.0
            q[s, a] += alpha_t * delta

            # WM update: move towards one-hot of rewarded action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with novelty bonus and set-size–dependent lapse + WM mapping

    Mechanism:
    - RL: standard delta rule plus a novelty/exploration bonus added at decision time:
          bonus_s,a = novelty_bonus / (1 + N_s,a),
          where N_s,a counts how often action a was selected in state s.
          The bonus encourages early exploration and decays with visits.
    - Lapse: with probability epsilon, a uniform random choice is made; epsilon increases with set size:
          epsilon = min(0.5, lapse_base * (nS / 3)).
    - WM: stores last rewarded action per state deterministically (one-hot) and provides a near-deterministic policy.
    - Mixture: combine WM and RL policies, then apply the lapse mixture.

    Parameters
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM policy.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - beta_size_penalty: float >= 0, reduces RL inverse temperature as set size increases:
          beta_eff = softmax_beta*10 / (1 + beta_size_penalty * max(0, nS - 3)).
    - novelty_bonus: float >= 0, magnitude of novelty exploration bonus at decision time.
    - lapse_base: float in [0,1], base lapse rate; actual lapse increases with set size.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, beta_size_penalty, novelty_bonus, lapse_base = model_parameters

    beta_base = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visit_counts = np.zeros((nS, nA))  # counts of selections for novelty bonus

        # Set-size effects
        size_factor = max(0.0, float(nS) - 3.0)
        beta_eff = beta_base / (1.0 + beta_size_penalty * size_factor)
        epsilon = min(0.5, lapse_base * (float(nS) / 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with novelty bonus at decision time
            Q_s = q[s, :]
            bonus_s = novelty_bonus / (1.0 + visit_counts[s, :])
            Q_aug = Q_s + bonus_s
            denom_rl = np.sum(np.exp(beta_eff * (Q_aug - Q_aug[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture (WM and RL), then lapse to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update novelty counts
            visit_counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay and UCB exploration + WM gated by RL uncertainty (entropy)

    Mechanism:
    - RL: Q-values decay toward uniform after each visit of a state (forgetting):
          q[s,:] = (1 - decay_q) * q[s,:] + decay_q * uniform.
      At decision time, we add an uncertainty bonus (UCB-like):
          Q_aug = Q + ucb_bonus / sqrt(1 + N_s,a), where N_s,a is the action count in state s.
    - WM: stores last rewarded action per state deterministically (one-hot).
    - Adaptive mixture: WM weight increases when RL is uncertain (high entropy), but this boost
      is attenuated at larger set sizes:
          wm_eff = clip(wm_weight_base + entropy_sensitivity * H(Q_aug) * (3/nS), 0, 1),
      where H is the entropy of the RL softmax policy over actions in the current state.

    Parameters
    - lr: float in [0,1], RL learning rate.
    - decay_q: float in [0,1], forgetting rate applied to the visited state's Qs after each trial.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight_base: float in [0,1], baseline WM weight.
    - entropy_sensitivity: float >= 0, scales how much WM is up-weighted by RL entropy (uncertainty).
      Set-size impact: the entropy bonus is multiplied by (3/nS), thus smaller at set size 6.
    - ucb_bonus: float >= 0, magnitude of UCB exploration bonus at decision time.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, decay_q, softmax_beta, wm_weight_base, entropy_sensitivity, ucb_bonus = model_parameters

    beta = softmax_beta * 10.0
    beta_wm = 50.0
    nA = 3
    uniform = (1.0 / nA) * np.ones(nA)

    def softmax_probs(vals, beta_local):
        # Return action probabilities for the given state's values
        z = vals - np.max(vals)
        ex = np.exp(beta_local * z)
        s = np.sum(ex)
        if s <= 0.0:
            return np.ones_like(vals) / len(vals)
        return ex / s

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with UCB bonus
            Q_s = q[s, :]
            bonus = ucb_bonus / np.sqrt(1.0 + counts[s, :])
            Q_aug = Q_s + bonus

            # Compute RL chosen-action probability
            denom_rl = np.sum(np.exp(beta * (Q_aug - Q_aug[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # RL entropy for gating WM
            pi_rl = softmax_probs(Q_aug, beta)
            H = -np.sum(pi_rl * np.log(np.maximum(pi_rl, 1e-12)))
            wm_boost = entropy_sensitivity * H * (3.0 / float(nS))
            wm_eff = wm_weight_base + wm_boost
            if wm_eff < 0.0:
                wm_eff = 0.0
            if wm_eff > 1.0:
                wm_eff = 1.0

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with adaptive WM weight
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and decay
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply decay towards uniform on the visited state's Q-values
            q[s, :] = (1.0 - decay_q) * q[s, :] + decay_q * uniform

            # WM update: one-hot on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update counts for UCB
            counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p

How set size affects parameters/mechanisms:
- Model 1: Larger set size reduces effective WM temperature via wm_temp_drop, making WM less deterministic; RL adapts to surprise regardless of set size.
- Model 2: Larger set size increases lapse probability and reduces RL beta via beta_size_penalty; also the novelty bonus is intrinsic and not directly size-linked except through slower revisits.
- Model 3: Larger set size reduces the WM gating boost because the entropy-based boost is scaled by 3/nS; RL uses UCB bonus which naturally promotes exploration when visits are sparse, which is more pronounced in larger sets.