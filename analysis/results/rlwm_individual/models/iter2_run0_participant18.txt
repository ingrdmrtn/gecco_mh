def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Surprise-gated Working Memory mixture with RL
    - Arbitration: Trial-by-trial gating of WM vs RL based on unsigned RL prediction error (surprise).
      A logistic gate assigns more weight to WM when surprise exceeds a set-size-shifted center.
    - RL: Single learning rate (lr) and softmax inverse temperature (softmax_beta).
    - WM store: Value-like table W updated toward the chosen action when rewarded and relaxed toward
      uniform when unrewarded (no explicit decay parameter; learning via wm_alpha).
    - Set-size effect: The gate's center increases with set size via k_setsize, making WM engagement
      harder for larger sets.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_alpha: WM learning rate toward target (0..1). On reward=1, toward one-hot; on reward=0, toward uniform.
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - gate_slope: Steepness of logistic gate mapping surprise to WM weight (>=0)
    - gate_center: Surprise level at which WM and RL are equally weighted when set size=3
    - k_setsize: Linear increase added to gate_center per additional item beyond 3 (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_alpha, softmax_beta, gate_slope, gate_center, k_setsize = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Surprise-gated WM weight (unsigned PE), set-size shifted
            pe = abs(r - Q_s[a])
            gate_center_eff = gate_center + k_setsize * max(0, nS - 3)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_slope * (pe - gate_center_eff)))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens chosen action; no-reward relaxes toward uniform
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Entropy-based arbitration + WM decay with capacity-limited strength
    - Arbitration: Higher RL uncertainty (softmax entropy) increases WM weighting via a sigmoid.
    - RL: Single learning rate (lr) with softmax choice (softmax_beta).
    - WM: Value-like memory W decays toward uniform each visit (wm_decay) and learns from reward with
      rate wm_learn (tied to wm_strength). WM contribution is scaled down when set size exceeds a
      capacity parameter.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_strength: Baseline WM influence and learning rate (0..1); higher means stronger WM and faster WM learning
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: Per-visit decay of WM toward uniform (0..1)
    - arbitration_slope: Steepness mapping RL entropy to WM weight (>=0)
    - wm_capacity: Capacity-like parameter; effective WM weight scales by max(0, 1 - (nS-3)/wm_capacity)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_strength, softmax_beta, wm_decay, arbitration_slope, wm_capacity = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    nA = 3
    log_nA = np.log(nA)  # maximum entropy for uniform over 3 actions

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity scaling of WM weight with set size
        if wm_capacity <= 0:
            cap_scale = 0.0
        else:
            cap_scale = max(0.0, 1.0 - max(0, nS - 3) / wm_capacity)
        wm_weight_base = wm_strength * cap_scale
        wm_learn = wm_strength  # tie WM learning to strength

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy and entropy
            probs_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs_rl = probs_rl / max(np.sum(probs_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)
            H_rl = -np.sum(probs_rl * np.log(np.maximum(probs_rl, 1e-12)))

            # WM policy
            probs_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            probs_wm = probs_wm / max(np.sum(probs_wm), 1e-12)
            p_wm = max(probs_wm[a], 1e-12)

            # Entropy-based arbitration weight (scaled to [0,1] by logistic around mid-entropy)
            # Normalize entropy by max (log_nA) and center at 0.5
            H_norm = np.clip(H_rl / max(log_nA, 1e-12), 0.0, 1.0) - 0.5
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arbitration_slope * H_norm))
            wm_weight_eff = wm_weight_eff * wm_weight_base  # cap by base WM strength under set size

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning from reward
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: WM with interference across states + RL
    - RL: Single learning rate (lr) with softmax (softmax_beta).
    - WM: Reward-locked update toward one-hot for chosen action with rate wm_learn.
      After each trial, WM representations are corrupted by interference that mixes each state's
      memory with the average memory across states. Interference grows with set size.
    - Arbitration: Fixed WM/RL mixture weight scaled down by interference (i.e., less WM with larger sets).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Baseline mixture weight for WM contribution (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_learn: WM learning rate toward one-hot when reward=1 (0..1)
    - interference_base: Baseline WM interference at set size=3 (0..1)
    - interference_slope: Increase in interference per additional item beyond 3 (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_learn, interference_base, interference_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference level and WM weight scaling with set size
        interference = np.clip(interference_base + interference_slope * max(0, nS - 3), 0.0, 1.0)
        wm_weight_eff_block = wm_weight * (1.0 - interference)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with interference-scaled WM weight
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-dependent strengthening
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot

            # Interference: mix each state's memory with the average memory across states
            avg_w = np.mean(w, axis=0)
            for si in range(nS):
                w[si, :] = (1.0 - interference) * w[si, :] + interference * avg_w

        blocks_log_p += log_p

    return -blocks_log_p