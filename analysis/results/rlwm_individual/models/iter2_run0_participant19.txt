Here are three distinct cognitive models, each as a standalone Python function. Each returns the negative log-likelihood of the observed choices. They follow the provided template structure and fill in the WM policy and WM updating mechanisms, with explicit, meaningful set-size effects.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + confidence-gated WM that weakens with set size.

    Idea:
    - RL: single learning rate with value forgetting toward uniform (captures drift/interference).
    - WM: stores action-specific beliefs per state; decays toward uniform faster for larger set sizes.
    - Arbitration weight is dynamically gated by WM "confidence" (how peaked W is) and penalized by set size.

    Parameters:
    - lr: RL learning rate for prediction error (0..1)
    - rl_forget: RL forgetting rate toward uniform each trial (0..1)
    - wm_base: baseline arbitration bias toward WM (real; passed through sigmoid)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - conf_temp: sensitivity of arbitration to WM confidence (max(W_s)-mean(W_s))
    - setsize_drop: increases WM forgetting and reduces arbitration weight as set size grows

    Set-size impact:
    - WM per-trial decay toward uniform: wm_decay = sigmoid(setsize_drop * (nS - 4.5)); larger nS -> stronger decay.
    - Arbitration: wm weight per decision decreases with nS via a - setsize_drop*(nS-3) shift.
    """
    lr, rl_forget, wm_base, softmax_beta, conf_temp, setsize_drop = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # deterministic WM policy when confidence is high
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size-driven WM decay
        wm_decay = 1.0 / (1.0 + np.exp(-setsize_drop * (nS - 4.5)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Apply RL forgetting (toward uniform) before choice to reflect ongoing interference
            q = (1 - rl_forget) * q + rl_forget * (1.0 / nA)

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice probability for observed action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values (deterministic-like)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence as peakedness of W_s; higher -> more WM weight
            conf = np.max(W_s) - np.mean(W_s)
            wm_bias = wm_base + conf_temp * conf - setsize_drop * (nS - 3)
            wm_w = 1.0 / (1.0 + np.exp(-wm_bias))

            p_total = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform (set-size dependent)
            w = (1 - wm_decay) * w + wm_decay * w_0
            # WM encoding of current outcome: move W_s toward one-hot for chosen action with strength r
            # Encode both positive and negative outcomes but with reward-weighted strength
            encode_strength = 0.5 + 0.5 * r  # stronger consolidation for reward
            w[s, :] = (1 - encode_strength) * w[s, :]  # shrink others slightly
            w[s, a] += encode_strength  # boost chosen action

            # Renormalize W_s to stay a proper distribution
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay + noisy WM under set-size-dependent interference; fixed arbitration.

    Idea:
    - RL: single learning rate; Q-values experience decay toward uniform (interference).
    - WM: fast associative store that suffers interference proportional to set size and has noise that scales with set size.
    - Arbitration: constant across trials, but WM's effective reliability drops as set size grows (via interference and noise).

    Parameters:
    - lr: RL learning rate (0..1)
    - rl_decay: RL decay toward uniform per trial (0..1)
    - wm_weight: mixing weight for WM vs RL (real; passed through sigmoid)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_interference_slope: controls how fast WM decays to uniform as set size increases
    - wm_noise: scales WM temperature as a function of set size (higher -> noisier WM)

    Set-size impact:
    - WM decay per trial: wm_decay = sigmoid(wm_interference_slope * (nS - 4.5)).
    - WM temperature: beta_wm_eff = 50 / (1 + wm_noise * (nS - 1)), lowering determinism for larger nS.
    """
    lr, rl_decay, wm_weight, softmax_beta, wm_interference_slope, wm_noise = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # base, will be downscaled by set size
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_decay = 1.0 / (1.0 + np.exp(-wm_interference_slope * (nS - 4.5)))
        beta_wm_eff = softmax_beta_wm / (1.0 + wm_noise * max(0, nS - 1))
        wm_w = 1.0 / (1.0 + np.exp(-wm_weight))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Apply RL decay to reflect interference
            q = (1 - rl_decay) * q + rl_decay * (1.0 / nA)

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice prob
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with effective temperature decreasing with set size
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM interference/decay
            w = (1 - wm_decay) * w + wm_decay * w_0
            # WM encoding: move the chosen action toward the observed outcome
            # Reward-modulated learning: stronger consolidation on reward
            encode_strength = 0.5 + 0.5 * r
            w[s, :] = (1 - encode_strength) * w[s, :]
            w[s, a] += encode_strength
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WSLS-style WM policy with set-size-dependent noise.

    Idea:
    - RL: standard delta rule.
    - WM: implements a win-stay/lose-shift heuristic per state. After a rewarded choice, it favors repeating the
      previous action in that state; after non-reward, it favors shifting to other actions. The strength of this
      heuristic is weakened by larger set sizes.
    - Arbitration: fixed mixture between RL softmax and WM-WSLS probability.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: mixture weight for WM vs RL (real; passed through sigmoid)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - ws_gain: gain for win-stay tendency (real; passed through sigmoid for probability mass)
    - ls_gain: gain for lose-shift tendency (real; passed through sigmoid for probability mass)
    - setsize_wsls_noise: increases WM stochasticity as set size increases (reduces stay/shift strength)

    Set-size impact:
    - WM win/shift probability mass is reduced by factor 1/(1 + setsize_wsls_noise*(nS-1)), making WM less decisive at nS=6.
    """
    lr, wm_weight, softmax_beta, ws_gain, ls_gain, setsize_wsls_noise = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # not directly used since WM is WSLS probabilistic; kept for consistency
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # we will store WM policy distribution here
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action and last reward per state for WSLS
        last_a = -1 * np.ones(nS, dtype=int)
        last_r = np.zeros(nS)

        wm_w = 1.0 / (1.0 + np.exp(-wm_weight))
        win_mass = 1.0 / (1.0 + np.exp(-ws_gain))
        lose_mass = 1.0 / (1.0 + np.exp(-ls_gain))
        setsize_scale = 1.0 / (1.0 + setsize_wsls_noise * max(0, nS - 1))
        win_mass *= setsize_scale
        lose_mass *= setsize_scale

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # RL choice probability
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM WSLS policy construction for this state
            if last_a[s] == -1:
                # No history: uniform
                wm_policy = np.ones(nA) / nA
            else:
                wm_policy = np.ones(nA) / nA
                if last_r[s] > 0:
                    # Win: allocate mass to stay on last action
                    mass = min(1.0, win_mass)
                    wm_policy *= (1 - mass)
                    wm_policy[last_a[s]] += mass
                else:
                    # Loss: allocate mass to shift (uniformly over other actions)
                    mass = min(1.0, lose_mass)
                    wm_policy *= (1 - mass)
                    other_actions = [aa for aa in range(nA) if aa != last_a[s]]
                    for oa in other_actions:
                        wm_policy[oa] += mass / (nA - 1)

            # p_wm is the probability assigned by the WSLS WM to the chosen action
            p_wm = max(wm_policy[a], 1e-12)

            # Mixture
            p_total = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM "updating": store the current WSLS policy into w for continuity and potential readout
            w[s, :] = wm_policy

            # Update last action/reward traces
            last_a[s] = a
            last_r[s] = r

        blocks_log_p += log_p

    return -blocks_log_p

Notes on the submitted models:
- Model 1 introduces confidence-gated arbitration that down-weights WM with larger set sizes and when WM is uninformative.
- Model 2 focuses on set-size-dependent WM interference and noise, plus RL decay, with a fixed arbitration weight.
- Model 3 instantiates a heuristic WM mechanism (WSLS) whose decisiveness weakens as set size increases.

All parameters are used, and set-size effects are explicitly integrated but via mechanisms not repeating the exact parameter combinations listed in the feedback.