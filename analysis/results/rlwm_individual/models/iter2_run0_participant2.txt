def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with asymmetric RL learning and load-dependent WM contribution.

    Idea:
    - RL learns Q-values with separate learning rates for positive and negative prediction errors.
    - WM stores rewarded action for each state, decays toward uniform at rate phi_wm.
    - The mixture weight of WM vs RL is attenuated by a capacity parameter C relative to set size (nS):
        wm_eff = wm_weight * min(1, C / nS).
      Thus, WM contributes less under higher load if C < nS.

    Parameters (model_parameters):
    - alpha_pos: scalar in (0,1). RL learning rate for positive reward prediction errors (r - Q > 0).
    - alpha_neg: scalar in (0,1). RL learning rate for negative reward prediction errors (r - Q < 0).
    - wm_weight: scalar in (0,1). Base mixture weight of WM policy relative to RL in low load.
    - softmax_beta: positive scalar. Inverse temperature for RL softmax (internally scaled x10).
    - phi_wm: scalar in (0,1). WM forgetting rate toward uniform per trial (leak).
    - C: positive scalar. WM capacity proxy; effective WM weight is scaled by min(1, C / set_size).

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, phi_wm, C = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_eff = np.clip(wm_weight * min(1.0, C / max(1.0, float(nS))), 0.0, 1.0)
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0.0 else alpha_neg
            q[s, a] += alpha * delta

            # WM forgetting toward uniform
            w[s, :] = (1.0 - phi_wm) * w[s, :] + phi_wm * w_0[s, :]

            # Reward-gated WM encoding toward the chosen action
            if r > 0.5:
                # Move distribution toward a one-hot on action a with strength proportional to (1 - current prob)
                # Simple one-step consolidation: increase mass on a by taking from others
                inc = (1.0 - w[s, a]) * (1.0 - phi_wm)
                # Distribute decrease proportionally across non-a actions
                if nA > 1:
                    dec_per_other = inc / (nA - 1)
                    for aa in range(nA):
                        if aa == a:
                            w[s, aa] += inc
                        else:
                            w[s, aa] = max(0.0, w[s, aa] - dec_per_other)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL(Î») with eligibility traces + WM with error-driven reset and load-dependent WM noise.

    Idea:
    - RL uses eligibility traces: an eligibility matrix e decays by lambda_trace each trial and is set to 1 for the chosen (s,a).
      Q updates are proportional to e, so recent choices (especially in the same state) get credit/blame.
    - WM forms a near one-shot memory on rewarded trials with strength p_store and is reset on errors for that state.
    - WM precision deteriorates with load: effective beta_wm = 50 / (1 + sigma_load * (nS - 3)).
    - Choices are a mixture of RL and WM policies with fixed wm_weight.

    Parameters (model_parameters):
    - lr: scalar in (0,1). Step size for RL value updates.
    - wm_weight: scalar in (0,1). Mixture weight for WM vs RL.
    - softmax_beta: positive scalar. RL inverse temperature (internally scaled x10).
    - lambda_trace: scalar in [0,1). Eligibility trace decay rate (higher = longer-lasting traces).
    - p_store: scalar in (0,1). Strength of WM encoding toward the rewarded action on r=1.
    - sigma_load: nonnegative scalar. Increases WM noise with set size: larger -> weaker WM under high load.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, lambda_trace, p_store, sigma_load = model_parameters
    softmax_beta *= 10.0
    base_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility

        # Load-dependent WM precision
        softmax_beta_wm = base_beta_wm / (1.0 + sigma_load * max(0.0, float(nS) - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Decay all eligibilities
            e *= lambda_trace
            # Replacing trace for current (s,a)
            e[s, :] *= 0.0
            e[s, a] = 1.0
            delta = r - Q_s[a]
            q += lr * delta * e  # credit assignment via eligibility

            # WM dynamics: small leak and error-driven reset/encoding
            # Mild generic leak toward uniform that also scales a bit with load
            phi = np.clip(0.05 + 0.05 * sigma_load * max(0.0, float(nS) - 3.0), 0.0, 0.5)
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            if r > 0.5:
                # Encode rewarded action strongly
                w[s, :] = (1.0 - p_store) * w[s, :]
                w[s, a] += p_store
            else:
                # On an error, reset WM for this state toward uniform (drop the wrong association)
                reset_strength = min(1.0, 2.0 * p_store)
                w[s, :] = (1.0 - reset_strength) * w[s, :] + reset_strength * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with load penalty and WM Hebbian learning.

    Idea:
    - RL updates with a single learning rate. Policy uses softmax with beta*10.
    - WM is a fast Hebbian learner: on rewarded trials, move w[s] toward one-hot on chosen action with rate eta_wm;
      otherwise small decay toward uniform. WM policy uses a near-deterministic softmax.
    - Arbitration is trial-wise and depends on:
        * RL uncertainty at state s: entropy of RL softmax (higher = more uncertain).
        * WM confidence at state s: 1 - entropy of w[s] (higher = more confident).
      The mixture weight is a logistic function of (WM_conf - RL_unc) with bias b0 and slope theta,
      penalized by set size via kappa_load.

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate.
    - softmax_beta: positive scalar. RL inverse temperature (internally scaled x10).
    - eta_wm: scalar in (0,1). WM learning rate (Hebbian consolidation on rewards).
    - theta: positive scalar. Slope of the arbitration logistic on (WM_conf - RL_unc).
    - b0: real scalar. Bias term of the arbitration logistic (WM preference at zero evidence).
    - kappa_load: nonnegative scalar. Linear penalty on WM usage with set size (applied to nS-3).

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, softmax_beta, eta_wm, theta, b0, kappa_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def softmax_probs(vals, beta):
        # Stable softmax
        v = vals - np.max(vals)
        e = np.exp(beta * v)
        Z = np.sum(e)
        return e / max(Z, eps)

    def entropy(pvec):
        p = np.clip(pvec, eps, 1.0)
        p = p / np.sum(p)
        return -np.sum(p * np.log(p))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy and uncertainty
            Q_s = q[s, :]
            pi_rl = softmax_probs(Q_s, softmax_beta)
            p_rl = max(pi_rl[a], eps)
            rl_unc = entropy(pi_rl) / np.log(nA)  # normalize to [0,1]

            # WM policy and confidence
            W_s = w[s, :]
            pi_wm = softmax_probs(W_s, softmax_beta_wm)
            p_wm = max(pi_wm[a], eps)
            wm_conf = 1.0 - (entropy(W_s) / np.log(nA))  # higher when peaked

            # Arbitration: logistic of evidence difference minus load penalty
            load_pen = kappa_load * max(0.0, float(nS) - 3.0)
            evidence = wm_conf - rl_unc
            wm_eff = sigmoid(b0 + theta * evidence - load_pen)
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM dynamics: mild decay, reward-gated Hebbian increment
            decay = 0.05  # small fixed leak per trial for the visited state
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = (1.0 - eta_wm) * w[s, :]
                w[s, a] += eta_wm

        blocks_log_p += log_p

    return -blocks_log_p