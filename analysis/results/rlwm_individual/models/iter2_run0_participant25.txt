def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-gated WM with per-visit decay and one-shot encoding on reward.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate and softmax action selection.
    - WM: a fast, near-deterministic lookup that is updated to a one-hot code on rewarded
      visits and decays toward uniform each time the state is revisited.
    - Mixture: choice probability is a convex combination of WM and RL policies.
    - Set size effect: the effective WM weight is reduced by a capacity-like gate
      min(1, c_wm / nS^psi), capturing limited WM capacity under higher set size.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0-1)
    - wm_weight: Base mixture weight on WM policy (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - c_wm: Capacity scaling constant for WM weight; higher c_wm sustains WM under load
    - psi: Exponent controlling how quickly WM capacity falls with set size nS
    - mu: WM per-visit decay rate toward uniform (0-1); applied when a state is visited

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, c_wm, psi, mu = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-like gating: higher nS reduces effective WM contribution
        gate = min(1.0, c_wm / (max(1, nS) ** psi))
        wm_weight_eff = wm_weight * gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Per-visit WM decay toward uniform baseline for the visited state
            w[s, :] = (1 - mu) * w[s, :] + mu * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (near-deterministic softmax over WM weights)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-driven one-shot encoding toward chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Overwrite toward one-hot; no change on errors
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + uncertainty-weighted WM mixture.

    Mechanism:
    - RL: tabular Q-learning with per-trial forgetting (shrinkage toward uniform) and softmax policy.
    - WM: fast associative store updated on every trial toward the observed outcome:
        * If reward: move toward one-hot for chosen action.
        * If no reward: move toward uniform (ambiguous mapping).
    - Mixture: WM weight is higher when RL is uncertain about the chosen action.
      Uncertainty is modeled by 1/(N+u0) where N is visit count to (s,a), scaled by exponent chi.
      Effective WM weight also mildly scales with set size via the uncertainty term through N growth.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base WM mixture weight (0-1), modulated by uncertainty
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - u0: Pseudocount controlling initial uncertainty (>=0)
    - chi: Exponent shaping how strongly uncertainty gates WM (>=0)
    - rho: RL forgetting rate per visit to a state (0-1); higher = more decay toward uniform

    Returns:
    - Negative log-likelihood of observed choices.
    Notes on set size:
    - Larger set size slows down visits, keeping N smaller longer; thus the uncertainty term
      stays larger, increasing effective WM weight under larger nS early in a block.
    """
    lr, wm_weight, softmax_beta, u0, chi, rho = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per (s,a)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting toward uniform for the visited state s
            q[s, :] = (1 - rho) * q[s, :] + rho * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty of the chosen action under RL (lower N -> higher uncertainty)
            u = 1.0 / (N[s, a] + u0 + 1e-12)
            gate = (u ** chi)
            wm_weight_eff = wm_weight * gate
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Update counts
            N[s, a] += 1.0

            # RL learning
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM learning
            if r > 0.5:
                # Move toward one-hot choice when rewarded
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * (w[s, :] + target)
            else:
                # Move toward uniform when no reward (uncertain mapping)
                w[s, :] = 0.5 * (w[s, :] + w_0[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + inhibitory WM (error-tagging) with logistic set-size gating and sticky bias.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate.
    - WM: stores "avoid" tags for actions that produced errors in a state.
      The WM policy favors actions with lower avoid tags via a softmax over negative tags.
      Avoid tags decay toward zero on each visit to the state.
    - Mixture: WM weight is gated by a logistic function of set size, capturing a sharp drop
      in WM contribution as nS increases: gate = 1/(1 + exp(g*(nS - 4.5))).
    - Sticky choice bias: RL policy includes a positive bias toward repeating the last action
      within a state.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base WM mixture weight (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - g: Logistic slope controlling how strongly set size reduces WM weight (can be >0)
    - d_w: Per-visit decay (0-1) of inhibitory WM tags toward zero; also sets error update magnitude as (1 - d_w)
    - stick: Sticky choice bias added to the Q-value of the last chosen action in that state

    Returns:
    - Negative log-likelihood of observed choices.
    Notes on set size:
    - Larger nS yields a smaller logistic gate, reducing the WM contribution; the transition width is set by g.
    """
    lr, wm_weight, softmax_beta, g, d_w, stick = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # Inhibitory WM tags: higher = more avoidance pressure
        w = np.zeros((nS, nA))
        w_0 = np.zeros((nS, nA))

        # Keep track of last action per state for sticky bias
        last_action = -np.ones(nS, dtype=int)

        # Logistic gate based on set size (midpoint fixed at 4.5)
        gate = 1.0 / (1.0 + np.exp(g * (nS - 4.5)))
        wm_weight_eff = wm_weight * gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Decay inhibitory tags toward zero on each visit to the state
            w[s, :] = (1 - d_w) * w[s, :] + d_w * w_0[s, :]

            # RL policy with sticky bias toward repeating last action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stick

            # WM inhibitory policy: prefer actions with lower avoid tags
            W_s = w[s, :]
            # RL policy probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            # WM policy based on negative tags (lower tag -> higher preference)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (-(W_s) + W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Update last action
            last_action[s] = a

            # RL learning (without sticky term in the update)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Inhibitory WM update on error: increase avoid tag for chosen action
            if r < 0.5:
                # Error update magnitude is tied to (1 - d_w): stronger when decay is small
                w[s, a] += (1.0 - d_w)
                # Clip to keep tags non-negative and bounded
                w[s, a] = max(0.0, min(5.0, w[s, a]))
            else:
                # On reward, relieve avoidance for the chosen action
                w[s, a] = max(0.0, w[s, a] - d_w)

        blocks_log_p += log_p

    return -blocks_log_p