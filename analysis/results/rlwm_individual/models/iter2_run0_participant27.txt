Below are three alternative cognitive models tailored to the RL–WM task. Each function follows the requested interface and computes the negative log-likelihood of the observed choices. I keep the RL choice-probability line as in the template and fill in the WM policy and WM updates in the marked locations. I also include explicit set-size effects via principled parameter scalings, as requested.

Note: Assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–scaled WM temperature and WM leak.

    Mechanism
    ---------
    - RL: single learning rate; softmax choice (line kept from template).
    - WM: a value table w updated toward one-hot for rewarded actions, and toward uniform otherwise.
           WM also leaks (decays) toward uniform on each trial in the visited state.
    - Arbitration: fixed wm_weight, but the WM inverse temperature increases in smaller sets
      and decreases in larger sets via a power-law scaling.

    Set-size effects
    ----------------
    - WM temperature is scaled by (3/nS)^gamma_beta, making WM more deterministic in small sets (nS=3)
      and less in large sets (nS=6).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, beta_wm_scale, wm_decay, gamma_beta)
        - lr: RL learning rate (0..1).
        - wm_weight: Mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - beta_wm_scale: Scales WM inverse temperature (>0).
        - wm_decay: WM leak toward uniform per visit (0..1).
        - gamma_beta: Exponent controlling set-size effect on WM temperature (>=0).
    """
    lr, wm_weight, softmax_beta, beta_wm_scale, wm_decay, gamma_beta = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base WM temperature; we will scale it below
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # precompute set-size scaling for WM temperature
        size_scale_beta = (3.0 / float(nS)) ** max(gamma_beta, 0.0)
        beta_wm_eff = softmax_beta_wm * max(beta_wm_scale, 0.0) * size_scale_beta

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax probability of chosen action with effective temperature beta_wm_eff
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # First: associative update toward one-hot on reward, or toward uniform otherwise
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Second: leak WM toward uniform (capacity/maintenance limit)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce–Hall associability and WM arbitration by associability and set size.

    Mechanism
    ---------
    - RL: baseline learning rate scaled by a learned, state-wise associability kappa[s].
           kappa increases with unsigned PE and decays otherwise (Pearce–Hall).
    - WM: updated toward one-hot on reward, otherwise toward uniform with gain parameter.
    - Arbitration: WM weight increases when RL associability is low (i.e., RL is less tuned),
      and decreases with larger set size (linear factor 3/nS).

    Set-size effects
    ----------------
    - wm_weight_eff = wm_weight_base * (3/nS) * (1 - kappa[s]), so WM is favored in small sets
      and when RL associability is low for the current state.

    Parameters
    ----------
    model_parameters : tuple
        (alpha_base, kappa0, softmax_beta, eta_asso, wm_weight_base, wm_gain)
        - alpha_base: Baseline RL learning rate (0..1).
        - kappa0: Initial associability per state (0..1).
        - softmax_beta: RL inverse temperature (>0, scaled internally by 10).
        - eta_asso: Associability learning rate (0..1) controlling how fast kappa tracks |PE|.
        - wm_weight_base: Base WM mixture weight before arbitration (0..1).
        - wm_gain: WM update gain toward target distributions (0..1).
    """
    alpha_base, kappa0, softmax_beta, eta_asso, wm_weight_base, wm_gain = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        kappa = np.clip(np.full(nS, kappa0), 0.0, 1.0)  # per-state associability

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Standard WM softmax on W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight depends on set size and associability
            size_factor = 3.0 / float(nS)  # in (0.5, 1] for nS in {6,3}
            wm_weight_eff = wm_weight_base * size_factor * (1.0 - kappa[s])
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with associability-scaled learning rate
            delta = r - Q_s[a]
            alpha_eff = alpha_base * np.clip(kappa[s], 0.0, 1.0)
            q[s][a] += alpha_eff * delta

            # Update associability kappa[s] toward |delta|
            kappa[s] = (1.0 - eta_asso) * kappa[s] + eta_asso * abs(delta)
            kappa[s] = np.clip(kappa[s], 0.0, 1.0)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_gain) * w[s, :] + wm_gain * target
            else:
                w[s, :] = (1.0 - wm_gain) * w[s, :] + wm_gain * w_0[s, :]

            # normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size–dependent forgetting and WM with separate temperature.

    Mechanism
    ---------
    - RL: single learning rate; additionally, Q-values forget toward uniform when a state is visited.
           Forgetting increases with set size via a power factor.
    - WM: associative update toward one-hot on reward, toward uniform on no reward.
           WM has its own temperature parameter (beta_wm_param) independent of RL.
    - Arbitration: fixed wm_weight across trials.

    Set-size effects
    ----------------
    - RL forgetting rate: rho_eff = rho_q * (nS/3)^zeta_set, so larger set sizes induce more forgetting.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, beta_wm_param, rho_q, zeta_set)
        - lr: RL learning rate (0..1).
        - wm_weight: Mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, scaled internally by 10).
        - beta_wm_param: Multiplier on WM inverse temperature (>0).
        - rho_q: Baseline RL forgetting rate toward uniform on state visit (0..1).
        - zeta_set: Exponent controlling how forgetting scales with set size (>=0).
    """
    lr, wm_weight, softmax_beta, beta_wm_param, rho_q, zeta_set = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent forgetting rate
        rho_eff = max(rho_q, 0.0) * (float(nS) / 3.0) ** max(zeta_set, 0.0)
        rho_eff = min(max(rho_eff, 0.0), 1.0)

        # WM temperature parameter
        beta_wm_eff = softmax_beta_wm * max(beta_wm_param, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply RL forgetting toward uniform for the current state
            q[s, :] = (1.0 - rho_eff) * q[s, :] + rho_eff * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on parameter identifiability and fit hints:
- cognitive_model1 isolates set-size effects into WM temperature and includes a WM maintenance leak (wm_decay).
- cognitive_model2 introduces a distinct arbitration signal (associability) that can capture trial-by-trial shifts between WM and RL without relying on entropy (different from the prior best model).
- cognitive_model3 places set-size effects into RL forgetting, which should reduce performance selectively in larger set sizes even with intact WM, offering a complementary explanation.

All parameters are used in the likelihood and have interpretable set-size impacts as described.