def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–dependent WM lapse and error-driven WM erasure.

    Idea:
    - RL: standard delta rule.
    - WM: one-shot storage of rewarded action per state (deterministic template), but retrieval is
      imperfect due to a set-size–dependent lapse that mixes WM with a uniform policy.
    - On negative feedback, WM partially erases the memory for that state toward uniform.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: RL inverse temperature; multiplied by 10 internally to increase range.
    - lapse0: WM lapse intercept; lapse = sigmoid(lapse0 + lapse_slope*(nS-3)).
    - lapse_slope: WM lapse sensitivity to set size (positive means larger sets increase WM lapse).
    - erase_prob: On non-reward, WM row moves toward uniform by this fraction (0..1).

    Set-size impacts:
    - Larger set sizes increase WM lapse via the logistic transform, reducing WM reliability and thus
      down-weighting effective WM influence within p_wm.
    """
    lr, wm_weight, softmax_beta, lapse0, lapse_slope, erase_prob = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM lapse
        lapse = 1.0 / (1.0 + np.exp(-(lapse0 + lapse_slope * (float(nS) - 3.0))))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with lapse-to-uniform (set-size–dependent)
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse) * p_wm_det + lapse * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # store rewarded mapping deterministically
            else:
                # Partial erasure toward uniform on errors
                w[s, :] = (1.0 - erase_prob) * w[s, :] + erase_prob * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + globally decaying WM traces with set-size–dependent decay and reward sharpening.

    Idea:
    - RL: standard delta rule.
    - WM: each state has a probability distribution over actions that decays toward uniform every trial.
      The decay rate increases with set size (greater interference). Upon reward, the visited state’s
      WM row is sharpened toward the rewarded action.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: RL inverse temperature; multiplied by 10 internally to increase range.
    - wm_decay_base: Base WM decay factor; effective decay per trial is 1 - exp(-wm_decay_base * nS).
    - wm_sharpness: On reward, fraction moving the WM row toward the one-hot rewarded action (0..1).

    Set-size impacts:
    - Larger set sizes increase the per-trial WM decay toward uniform, weakening WM representations and
      reducing their decision impact through p_wm.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_sharpness = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM decay toward uniform
        decay = 1.0 - np.exp(-wm_decay_base * float(nS))

        log_p = 0.0
        for t in range(len(block_states)):

            # Global WM decay each trial (interference/scarcity)
            w = (1.0 - decay) * w + decay * w_0

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from current WM row
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: sharpen toward rewarded action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_sharpness) * w[s, :] + wm_sharpness * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–dependent swap errors and RL temperature load-sensitivity.

    Idea:
    - RL: standard delta rule, but the effective inverse temperature decreases as set size increases
      (more load -> more exploration).
    - WM: one-shot storage of the last rewarded action per state. At retrieval, with a set-size–
      dependent swap probability, WM confuses the target state with other states and averages their
      WM-based policies.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally; then divided by
      (1 + beta_load_slope*(nS-3)) at decision time.
    - swap_base: Controls how swap error grows with set size; swap = 1 - exp(-swap_base*(nS-1)).
    - beta_load_slope: Sensitivity of RL temperature to set size (>=0 makes larger sets more exploratory).

    Set-size impacts:
    - WM: swap probability increases with set size, diluting state-specific WM information.
    - RL: inverse temperature is reduced as set size increases, producing noisier RL choices.
    """
    lr, wm_weight, softmax_beta, swap_base, beta_load_slope = model_parameters
    softmax_beta *= 10  # base scaling; will be downscaled by load inside the block
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute set-size–dependent factors
        swap = 1.0 - np.exp(-swap_base * max(0.0, float(nS) - 1.0))
        beta_base = softmax_beta

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # RL policy with load-adjusted temperature
            beta_eff = beta_base / (1.0 + beta_load_slope * (float(nS) - 3.0))
            beta_eff = max(1e-6, beta_eff)
            # Temporarily use beta_eff for this computation
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy with swap errors:
            # - p_self: WM policy from the current state's WM row
            W_s = w[s, :].copy()
            p_self = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # - p_others: average WM policy across other states
            if nS > 1:
                p_list = []
                for j in range(nS):
                    if j == s:
                        continue
                    W_j = w[j, :].copy()
                    p_j = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_j - W_j[a])))
                    p_list.append(p_j)
                p_others = np.mean(p_list) if len(p_list) > 0 else (1.0 / nA)
            else:
                p_others = 1.0 / nA

            p_wm = (1.0 - swap) * p_self + swap * p_others

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store last rewarded action deterministically
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p