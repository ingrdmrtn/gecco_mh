def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load-dependent WM noise.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores recently rewarded action per state, decaying toward uniform.
    - Arbitration between RL and WM depends on the relative certainty of each system:
      the mixture weight shifts toward the system with lower entropy (higher certainty).
    - WM is noisier under higher set size (load), implemented by mixing WM weights
      toward uniform before action selection.

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - wm_weight0:    Baseline WM mixture weight before arbitration (0..1)
    - softmax_beta:  Base RL inverse temperature (scaled internally by *10)
    - wm_decay:      WM decay toward uniform per state visit (0..1)
    - arb_slope:     Slope for entropy-difference arbitration; larger favors low-entropy system
    - wm_noise:      Load-dependent WM noise strength; effective WM noise = wm_noise * ((nS-3)/3), clipped to [0,1]

    Set-size impact:
    - WM noise increases with set size (nS): WM policy is blended toward uniform more in nS=6 than nS=3.
    - Arbitration dynamically favors the lower-entropy (more certain) system trial-by-trial.
    """
    lr, wm_weight0, softmax_beta, wm_decay, arb_slope, wm_noise = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def softmax_probs(vec, beta):
        z = vec - np.max(vec)
        ez = np.exp(beta * z)
        denom = np.sum(ez)
        return ez / max(denom, eps)

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    def logit(x):
        x = np.clip(x, 1e-6, 1.0 - 1e-6)
        return np.log(x / (1.0 - x))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM noise level
        wm_noise_eff = np.clip(wm_noise * max(0.0, (nS - 3) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Full RL policy for entropy computation
            pi_rl = softmax_probs(Q_s, softmax_beta)
            H_rl = entropy(pi_rl)

            # WM readout with load-dependent noise (blend toward uniform)
            W_s = (1.0 - wm_noise_eff) * w[s, :] + wm_noise_eff * w_0[s, :]
            # WM probability of chosen action (near-deterministic softmax)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Full WM policy for entropy computation
            pi_wm = softmax_probs(W_s, softmax_beta_wm)
            H_wm = entropy(pi_wm)

            # Entropy-based arbitration: favor system with lower entropy
            base = logit(wm_weight0)
            wm_eff = sigmoid(base + arb_slope * (H_rl - H_wm))
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # WM write on rewarded trials
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice-trace bias + WM mixture (trace is load-sensitive).

    Idea:
    - RL learns Q-values with a single learning rate.
    - A recency-based choice-trace (per state) biases RL toward repeating recent choices.
    - Trace bias is weaker in larger set sizes (more states dilute recency influence).
    - WM stores rewarded actions per state and decays toward uniform; mixed with RL.

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - wm_weight:     WM mixture weight (0..1)
    - softmax_beta:  Base RL inverse temperature (scaled internally by *10)
    - trace_gain:    Strength of choice-trace added to RL Q-values
    - trace_decay:   Per-visit decay of choice-trace (0..1)
    - wm_decay:      WM decay toward uniform per state visit (0..1)

    Set-size impact:
    - Effective trace gain is scaled by 3/nS, so the trace influence is weaker in larger sets:
      trace_gain_eff = trace_gain * (3.0 / nS).
    """
    lr, wm_weight, softmax_beta, trace_gain, trace_decay, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Choice trace per state-action
        ctrace = np.zeros((nS, nA))

        trace_gain_eff = trace_gain * (3.0 / float(nS))
        wm_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with choice-trace bias
            Q_s = q[s, :]
            T_s = ctrace[s, :]
            Q_aug = Q_s + trace_gain_eff * T_s

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update choice trace: decay then reinforce chosen action
            ctrace[s, :] *= (1.0 - trace_decay)
            ctrace[s, a] += 1.0

            # WM decay and write on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with volatility-adaptive learning rate + WM suppressed by volatility.

    Idea:
    - RL learning rate adapts to a per-state volatility estimate driven by absolute PEs.
      When recent PEs are large, volatility is high and RL updates faster.
    - WM stores rewarded actions and decays toward uniform.
    - Arbitration down-weights WM when volatility is high (when mappings seem unstable).

    Parameters (model_parameters):
    - lr_base:       Base RL learning rate (0..1)
    - wm_weight:     Baseline WM mixture weight (0..1)
    - softmax_beta:  Base RL inverse temperature (scaled internally by *10)
    - vol_kappa:     Volatility update rate (0..1); higher tracks PEs more quickly
    - wm_decay:      WM decay toward uniform per state visit (0..1)
    - vol_to_weight: Strength of volatility-to-WM suppression; wm_eff = wm_weight / (1 + vol_to_weight * vol_s)

    Set-size impact:
    - Indirect: larger sets often yield larger early PEs, increasing estimated volatility and thereby
      boosting RL learning and suppressing WM contribution when the environment feels unstable.
    """
    lr_base, wm_weight, softmax_beta, vol_kappa, wm_decay, vol_to_weight = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        vol = np.zeros(nS)  # per-state volatility estimate in [0,1]-ish

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Volatility suppresses WM weight
            wm_eff = wm_weight / (1.0 + vol_to_weight * vol[s])
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with volatility-adaptive learning rate
            delta = r - Q_s[a]
            vol[s] = (1.0 - vol_kappa) * vol[s] + vol_kappa * abs(delta)
            lr_eff = np.clip(lr_base * (1.0 + vol[s]), 0.0, 1.0)
            q[s, a] += lr_eff * delta

            # WM decay and write on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p