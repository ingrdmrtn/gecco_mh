def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + graded WM distribution with set-size-driven interference.

    Mechanism
    - RL: standard delta-rule with softmax choice.
    - WM: per-state action distribution w[s,:] that is updated toward a one-hot
      vector for the chosen action on rewarded trials (fast associative update),
      and globally decays toward uniform due to interference that increases with set size.
    - Choice: convex mixture of RL policy and WM policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate for Q-updates.
        wm_weight : float in [0,1]
            Mixture weight placed on WM policy.
        softmax_beta : float >= 0
            Inverse temperature for RL softmax (scaled up internally).
        kappa_wm : float in [0,1]
            Strength of WM update toward the chosen action on rewarded trials.
        interference_sensitivity : float >= 0
            Magnitude of WM decay toward uniform driven by set size on each trial.
            Effective decay per trial is interference_sensitivity * (set_size - 3) / 3.
            Thus, larger sets (6) produce stronger decay than smaller sets (3).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa_wm, interference_sensitivity = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM readout
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM action distribution per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior for WM

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action (efficient 1D softmax evaluation)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM distribution (here WM is already a distribution; softmax just sharpens it)
            W_s = w[s, :]
            wm_logits = W_s  # already normalized; sharpening below makes it near-argmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on reward, push w[s,:] toward one-hot at chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - kappa_wm) * w[s, :] + kappa_wm * one_hot

            # WM interference/decay: toward uniform, stronger at larger set sizes
            set_size_t = int(block_set_sizes[t])
            decay = interference_sensitivity * max(0.0, (set_size_t - 3) / 3.0)
            if decay > 0.0:
                w = (1.0 - decay) * w + decay * w_0  # global interference within block

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-adaptive learning + WM recall with set-size-specific mixing.

    Mechanism
    - RL: learning rate is dynamically modulated by surprise (|PE|) via a sigmoid map.
      This allows faster learning when outcomes are unexpected.
    - WM: stores the last rewarded action per state; retrieval is imperfect (lapse epsilon_wm).
      Mixture weight for WM differs between small and large set sizes.
    - Choice: mixture of RL softmax and WM recall policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr0 : float (unconstrained real; internally passed through sigmoid)
            Baseline logit for RL learning rate. Effective lr = sigmoid(lr0 + kappa_pe*|PE|).
        softmax_beta : float >= 0
            Inverse temperature for RL softmax (scaled up internally).
        wm_weight_small : float in [0,1]
            Mixture weight for WM policy when set size == 3.
        wm_weight_large : float in [0,1]
            Mixture weight for WM policy when set size == 6.
        kappa_pe : float (can be positive)
            Sensitivity of RL learning rate to surprise (|PE|).
        epsilon_wm : float in [0,1]
            WM lapse probability (probability mass assigned to uniform instead of recalled action).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr0, softmax_beta, wm_weight_small, wm_weight_large, kappa_pe, epsilon_wm = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # not directly used; keep per template
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        has_mem = np.zeros(nS, dtype=bool)
        mem_act = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if memory exists, choose stored action with prob 1-epsilon_wm, else uniform
            if has_mem[s]:
                p_vec = epsilon_wm * w_0[s, :].copy()
                p_vec[mem_act[s]] += (1.0 - epsilon_wm)
                # Use very sharp softmax to obtain action probability
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (p_vec - p_vec[a])))
            else:
                p_wm = w_0[s, a]  # uniform when no memory

            # Set-size-specific mixture weight
            wm_w = wm_weight_small if int(block_set_sizes[t]) <= 3 else wm_weight_large
            wm_w = np.clip(wm_w, 0.0, 1.0)

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with surprise-adaptive learning rate
            pe = r - Q_s[a]
            lr_eff = sigmoid(lr0 + kappa_pe * abs(pe))
            q[s, a] += lr_eff * pe

            # WM storage on reward
            if r > 0.5:
                has_mem[s] = True
                mem_act[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with confidence-gated WM encoding and set-size-dependent WM decay.

    Mechanism
    - RL: standard delta-rule with softmax choice.
    - WM: per-state action strengths W_s updated only when RL is confident about the
      chosen action (large advantage over alternatives). WM also decays toward uniform,
      with stronger decay at larger set sizes.
    - Choice: mixture of RL softmax and WM softmax.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight on WM policy.
        softmax_beta : float >= 0
            Inverse temperature for RL softmax (scaled up internally).
        conf_threshold : float >= 0
            Minimum Q-advantage (Q[a] - max(Q[others])) required to encode into WM.
        wm_decay_base : float in [0,1]
            Baseline WM decay per trial toward uniform; scaled up by set size as:
            decay = wm_decay_base * (set_size / 6).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, conf_threshold, wm_decay_base = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM strengths over actions per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM strengths for current state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Confidence-gated WM encoding: update W only if chosen action is clearly best
            # Advantage of chosen action over the best alternative:
            alt_best = np.max(np.delete(Q_s, a))
            advantage = Q_s[a] - alt_best
            if advantage >= conf_threshold:
                # Encode chosen action; reward boosts the encoding by using its value as a gain
                gain = 1.0 if r <= 0.5 else 1.0 + r  # small extra boost when rewarded (=2.0)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Move W toward the one-hot pattern with a modest fraction controlled by gain
                alpha_enc = min(1.0, 0.5 * gain)
                w[s, :] = (1.0 - alpha_enc) * w[s, :] + alpha_enc * one_hot

            # Set-size-dependent WM decay toward uniform
            set_size_t = int(block_set_sizes[t])
            decay = np.clip(wm_decay_base * (set_size_t / 6.0), 0.0, 1.0)
            if decay > 0.0:
                w = (1.0 - decay) * w + decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size impacts:
- Model 1: interference_sensitivity directly scales WM decay per trial proportionally to (set_size - 3)/3, so larger set size increases interference and lowers WMâ€™s influence over time.
- Model 2: wm_weight_small vs wm_weight_large explicitly adjusts the WM mixture weight across set sizes; typically wm_weight_large < wm_weight_small to reflect reduced WM reliability in larger sets.
- Model 3: wm_decay_base is scaled by set size (set_size/6), increasing WM decay in larger sets, thereby reducing WM efficacy when cognitive load is higher.