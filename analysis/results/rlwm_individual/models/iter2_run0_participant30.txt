def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with lateral inhibition + one-shot WM with load-dependent decay.

    Description
    - RL: tabular Q-learning with learning rate; includes lateral inhibition/forgetting
      of unchosen actions within the visited state via lambda_et.
    - WM: fast "one-shot" Hebbian store of rewarded associations per state (w table),
      with decay toward uniform that increases with set size (load_sensitivity).
    - Policy: mixture of RL and WM softmaxes. WM softmax is near-deterministic.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values (0..1).
    - model_parameters[1] = wm_weight (float): Mixture weight on WM policy (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally x10.
    - model_parameters[3] = lambda_et (float): Lateral inhibition/forgetting of unchosen actions in the visited state (0..1).
                                              Applied as a shrink toward zero on Q[s,:] on each visit before the Q update.
    - model_parameters[4] = wm_conf (float): Strength of WM encoding for a rewarded action (0..1).
                                             Larger values push WM toward a one-hot code on rewarded choices.
    - model_parameters[5] = load_sensitivity (float): Increases WM decay as set size grows; mapped via a sigmoid on (nS-3).

    Set-size effects
    - WM decay increases with nS via: decay = sigmoid(load_sensitivity * (nS - 3)).
      Thus, larger set sizes produce stronger drift of WM toward uniform, degrading WM policy.
    """
    lr, wm_weight, softmax_beta, lambda_et, wm_conf, load_sensitivity = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay factor (per visit)
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        wm_decay = sigmoid(load_sensitivity * (nS - 3.0))
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: compute choice prob for chosen action via softmax trick
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (near-deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with lateral inhibition/forgetting of unchosen actions
            # Shrink all Q in current state toward 0 before applying TD update
            q[s, :] = (1.0 - lambda_et) * q[s, :]
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (stronger under higher load)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM one-shot Hebbian update:
            # If rewarded, push toward one-hot on the chosen action
            if r > 0:
                w[s, :] = (1.0 - wm_conf) * w[s, :]
                w[s, a] += wm_conf
            else:
                # If not rewarded, suppress the chosen action slightly
                w[s, a] = (1.0 - wm_conf) * w[s, a]

            # Normalize to keep within simplex (numerical safety)
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-scaled forgetting + WM recency buffer with noisy retrieval.

    Description
    - RL: Q-learning with learning rate; includes state-wise forgetting whose strength
      increases with set size (rl_forget_base).
    - WM: a recency buffer that stores the most recently rewarded action per state with a
      strength that decays over time and with load (wm_time_decay). Retrieval is noisy (wm_noise).
    - Policy: mixture of RL and WM softmaxes. WM softmax uses a distribution formed by
      mixing a point mass at the remembered action with uniform noise.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values (0..1).
    - model_parameters[1] = wm_weight (float): Mixture weight on WM policy (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally x10.
    - model_parameters[3] = rl_forget_base (float): Base forgetting rate for Q-values in the visited state (0..1),
                                                    scaled by nS/6 on each visit.
    - model_parameters[4] = wm_time_decay (float): Base decay of WM strength per trial (0..1).
                                                   Effective decay scales with nS/6 as load increases.
    - model_parameters[5] = wm_noise (float): Retrieval noise in WM policy (0..1). 0 = pure point mass, 1 = uniform.

    Set-size effects
    - RL forgetting per visit = rl_forget_base * (nS / 6), so larger sets accelerate Q decay.
    - WM strength decay per visit = wm_time_decay * (nS / 6), so larger sets accelerate forgetting in WM.
    """
    lr, wm_weight, softmax_beta, rl_forget_base, wm_time_decay, wm_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # will hold WM policy vectors constructed each trial
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM recency memory: last rewarded action and its strength per state
        last_a = -1 * np.ones(nS, dtype=int)
        strength = np.zeros(nS)  # in [0,1]

        rl_forget = np.clip(rl_forget_base * (nS / 6.0), 0.0, 1.0)
        wm_decay_eff = np.clip(wm_time_decay * (nS / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Construct WM policy for current state s from recency buffer
            if last_a[s] >= 0 and strength[s] > 0:
                wm_vec = wm_noise * (1.0 / nA) * np.ones(nA)
                wm_vec[last_a[s]] += (1.0 - wm_noise) * strength[s]
                # Renormalize (in case wm_noise + strength != 1)
                wm_vec = np.maximum(wm_vec, eps)
                wm_vec /= wm_vec.sum()
            else:
                wm_vec = (1.0 / nA) * np.ones(nA)

            w[s, :] = wm_vec.copy()

            Q_s = q[s, :]
            W_s = w[s, :]

            # Policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL forgetting in visited state, then standard TD update
            q[s, :] = (1.0 - rl_forget) * q[s, :]
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM strength decay
            strength[s] *= (1.0 - wm_decay_eff)

            # WM update based on feedback
            if r > 0:
                last_a[s] = a
                # Set strength to max (1.0), reflecting a fresh, reliable memory
                strength[s] = 1.0
            else:
                # Negative outcome weakens memory and, if it pertained to the same action, clears it
                if last_a[s] == a:
                    strength[s] *= (1.0 - wm_time_decay)
                    if strength[s] < 1e-3:
                        last_a[s] = -1  # drop the memory

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-based arbitration between RL and probabilistic WM.

    Description
    - RL: standard Q-learning.
    - WM: probability table updated by a delta-rule toward a one-hot on rewarded actions,
      with decay toward uniform that grows with set size.
    - Arbitration: the WM mixture weight is adjusted on each trial by comparing the
      entropies of the RL and WM action distributions (lower entropy => higher confidence).
      A sigmoid maps the entropy difference to a trial-wise WM weight.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate (0..1).
    - model_parameters[1] = base_wm_weight (float): Baseline WM mixture weight before arbitration (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally x10.
    - model_parameters[3] = arb_gain (float): Gain on entropy difference (H_rl - H_wm) in the arbitration sigmoid.
    - model_parameters[4] = wm_eta (float): WM learning rate toward one-hot when rewarded (0..1).
    - model_parameters[5] = wm_decay_load (float): Base WM decay toward uniform, scaled by nS/3 (0..1).

    Set-size effects
    - WM decay per visit = wm_decay_load * (nS / 3), making WM less stable in larger sets.
    - Arbitration indirectly responds to set size via WM entropy (WM becomes less certain under higher load).
    """
    lr, base_wm_weight, softmax_beta, arb_gain, wm_eta, wm_decay_load = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12
    nA = 3

    def softmax(logits):
        z = logits - np.max(logits)
        ez = np.exp(z)
        return ez / np.sum(ez)

    def entropy(p):
        p = np.clip(p, eps, 1.0)
        return -np.sum(p * np.log(p))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_decay = np.clip(wm_decay_load * (nS / 3.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # Full RL and WM distributions for arbitration
            p_rl_vec = softmax(softmax_beta * Q_s)
            p_wm_vec = softmax(softmax_beta_wm * W_s)

            H_rl = entropy(p_rl_vec)
            H_wm = entropy(p_wm_vec)

            # Arbitration: increase WM weight when WM is more certain than RL
            # wmix in [0,1]
            wmix = sigmoid(np.log(base_wm_weight + eps) - np.log(1 - base_wm_weight + eps) + arb_gain * (H_rl - H_wm))

            # Choice probabilities for the chosen action
            p_rl = p_rl_vec[a]
            p_wm = p_wm_vec[a]

            p_total = wmix * p_wm + (1.0 - wmix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM delta-rule: move toward one-hot on rewarded action; slight repulsion otherwise
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] += wm_eta * (target - w[s, :])
            else:
                # Soft anti-association on chosen action only
                w[s, a] += wm_eta * (0.0 - w[s, a])

            # Keep WM row normalized
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p