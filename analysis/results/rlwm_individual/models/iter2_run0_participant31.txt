def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + WM with interference-limited encoding and leaky cache.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM cache: a cached action-probability vector per state, updated toward a one-hot
      vector of the chosen action when rewarded, but WM encoding strength is reduced
      by set-size-dependent interference. The cache also leaks toward uniform on each visit.
    - Policy: mixture of WM and RL action probabilities.

    Parameters
    - lr: Q-learning rate (0..1).
    - wm_weight: Mixture weight for WM policy (0..1); 0=RL only, 1=WM only.
    - softmax_beta: Inverse temperature for RL policy; internally scaled by 10 (>0).
    - enc_strength: Base WM encoding strength toward one-hot on rewarded trials (0..1).
    - wm_leak: Leak toward uniform baseline applied on each state visit (0..1).
    - interference_ss_slope: How much set size reduces effective encoding (>0).
        Effective encoding = enc_strength / (1 + interference_ss_slope * max(nS - 3, 0)).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, enc_strength, wm_leak, interference_ss_slope = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent effective encoding strength
        inter = max(interference_ss_slope, 0.0)
        enc_eff = np.clip(enc_strength / (1.0 + inter * max(nS - 3, 0)), 0.0, 1.0)
        wm_w = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over the WM cache; if uniform, behaves near-uniform
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform on each visit of the state
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # WM encoding on rewarded trials with set-size–limited strength
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - enc_eff) * w[s, :] + enc_eff * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + adaptive WM gating driven by RL uncertainty and set-size penalty.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM cache: one-shot overwriting toward a one-hot vector on rewarded trials
      with probability controlled by overwrite_prob.
    - Adaptive WM usage: WM mixture weight is dynamic per trial,
      computed via a logistic transform of RL entropy (higher entropy -> more WM),
      with a linear penalty for larger set sizes.
    - Policy: mixture of WM and RL action probabilities using the dynamic WM weight.

    Parameters
    - lr: Q-learning rate (0..1).
    - wm_base: Base logit bias for WM usage (unbounded).
    - softmax_beta: Inverse temperature for RL policy; internally scaled by 10 (>0).
    - wm_gain: Gain on RL entropy (>=0); higher values increase WM usage when RL is uncertain.
    - ss_penalty: Linear penalty on WM usage per extra item beyond 3 (>=0).
    - overwrite_prob: Probability to overwrite WM cache toward the rewarded action on r=1 (0..1).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, wm_gain, ss_penalty, overwrite_prob = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    overwrite_prob = np.clip(overwrite_prob, 0.0, 1.0)
    wm_gain = max(wm_gain, 0.0)
    ss_penalty = max(ss_penalty, 0.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # For adaptive gating, compute RL action distribution and entropy
            # Use the same softmax that defines p_rl but across all actions
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / np.sum(pi_rl)
            # Shannon entropy in nats
            entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))

            # Set-size penalty for WM usage
            ss_term = ss_penalty * max(nS - 3, 0)

            # Dynamic WM weight via logistic transform
            wm_logit = wm_base + wm_gain * entropy - ss_term
            wm_w = 1.0 / (1.0 + np.exp(-wm_logit))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: probabilistic overwrite toward the rewarded action
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - overwrite_prob) * w[s, :] + overwrite_prob * one_hot
            else:
                # On non-reward, keep current cache (no decay here); w_0 available if you want to reset elsewhere
                w[s, :] = w[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + WM with reward-driven attraction and error-driven repulsion; retention harmed by set size.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM cache: a probability vector per state updated on each visit by:
        - Rewarded trials: attraction toward one-hot of chosen action (rate enc_prob).
        - Non-rewarded trials: repulsion away from chosen action by redistributing mass to other actions (rate anti_prob).
      Additionally, WM retention decays faster in larger sets: on each state visit, the cache contracts toward uniform
      with a decay factor that increases with set size.
    - Policy: mixture of WM and RL action probabilities.

    Parameters
    - lr: Q-learning rate (0..1).
    - wm_weight: Mixture weight for WM policy (0..1).
    - softmax_beta: Inverse temperature for RL policy; internally scaled by 10 (>0).
    - enc_prob: WM attraction rate toward chosen action on reward (0..1).
    - anti_prob: WM repulsion rate away from chosen action on non-reward (0..1).
    - retention_ss_scale: Exponent controlling how retention worsens with set size (>=0).
        Decay per visit = 1 - exp(-(nS/3) ** retention_ss_scale).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, enc_prob, anti_prob, retention_ss_scale = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    enc_prob = np.clip(enc_prob, 0.0, 1.0)
    anti_prob = np.clip(anti_prob, 0.0, 1.0)
    retention_ss_scale = max(retention_ss_scale, 0.0)
    wm_w_base = np.clip(wm_weight, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent retention decay per visit
        decay = 1.0 - np.exp(- (max(nS, 1) / 3.0) ** retention_ss_scale)
        decay = np.clip(decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with fixed WM weight
            p_total = wm_w_base * p_wm + (1.0 - wm_w_base) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Retention decay toward uniform, worsened by set size
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            if r > 0.0:
                # Reward: attract toward one-hot of chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - enc_prob) * w[s, :] + enc_prob * one_hot
            else:
                # Non-reward: repulse chosen action by moving mass to other actions
                repulse = w[s, :].copy()
                take = anti_prob * repulse[a]
                repulse[a] -= take
                repulse[np.arange(nA) != a] += take / (nA - 1)
                w[s, :] = repulse

        blocks_log_p += log_p

    return -blocks_log_p