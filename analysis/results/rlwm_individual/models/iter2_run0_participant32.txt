Here are three distinct cognitive models that follow your template and guardrails. Each implements a different WM–RL interaction and set-size mechanism, uses all parameters, and returns the negative log-likelihood.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with dynamic WM gating by WM strength, linear set-size penalty, perseveration,
    and a lapse process.

    Mechanism:
    - RL: Q-learning with softmax.
    - WM: stores a one-hot action for a state when rewarded; otherwise drifts toward uniform with
      an interference rate that increases with set size (no extra parameter). WM policy uses a very
      deterministic softmax.
    - Dynamic arbitration: WM weight is gated by the instantaneous WM strength in that state
      (max(W_s)), passed through a sigmoid with sensitivity. Base WM weight is penalized linearly
      by set size: wm_weight * (3 / set_size).
    - Perseveration: adds a bias to repeat the previous action within state in the RL policy.
    - Lapse: with probability lapse, choices are uniform, independent of state.

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate.
    - wm_weight0: [0,1] baseline WM mixture weight before gating and set-size penalty.
    - softmax_beta: >=0 base inverse temperature for RL (scaled by 10 internally).
    - pers_bias: real, log-bias added to the logit of the previous action within state in the RL policy.
    - gate_sensitivity: >=0, controls how strongly WM strength gates the WM mixture weight via sigmoid.
    - lapse: [0,1], probability of a stimulus-independent lapse to uniform choice.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight0, softmax_beta, pers_bias, gate_sensitivity, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state previous action for perseveration bias (-1 means none yet)
        prev_act = -1 * np.ones(nS, dtype=int)

        # Set-size interference rate for WM (no extra parameter): heavier decay for larger set size
        # For nS=3 => 0, for nS=6 => 0.5
        wm_interference = max(0.0, min(1.0, 1.0 - (3.0 / float(nS))))

        # Base set-size penalty on WM mixture
        base_wm_weight = wm_weight0 * (3.0 / float(nS))
        base_wm_weight = max(0.0, min(1.0, base_wm_weight))

        log_p = 0.0

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Perseveration bias: add bias to previous action logit in RL policy
            if prev_act[s] >= 0:
                # Adjust Q logits by adding pers_bias on prev action
                Q_s_biased = Q_s.copy()
                Q_s_biased[prev_act[s]] += pers_bias / max(softmax_beta, 1e-8)
            else:
                Q_s_biased = Q_s

            # Recompute RL probability of chosen action under biased logits
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s_biased - Q_s_biased[a])))

            # WM softmax for chosen action (very sharp)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Gate WM by its current strength and sensitivity
            wm_strength = float(np.max(W_s))  # in [1/3, 1]
            gate = 1.0 / (1.0 + np.exp(-gate_sensitivity * (wm_strength - 1.0 / nA)))
            wm_weight_eff = base_wm_weight * gate
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            # Lapse mixture to uniform
            p_mixture = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mixture + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform with set-size–dependent interference
            w[s, :] = (1.0 - wm_interference) * w[s, :] + wm_interference * w_0[s, :]

            # WM overwrite only on rewarded trials
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update previous action for perseveration
            prev_act[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + valence-tagged WM with set-size–dependent WM suppression.

    Mechanism:
    - RL: Q-learning with softmax.
    - WM: maintains a valence-tag vector V_s over actions for each state:
          +1 tag for the last rewarded action; a negative tag (-neg_tag_weight) for the last
          unrewarded action. Untagged actions have 0. WM policy is a very sharp softmax over V_s.
          This implements both "go to last rewarded" and "avoid last punished".
    - Arbitration: WM mixture weight is suppressed exponentially with set size.

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] baseline WM mixture weight before set-size suppression.
    - softmax_beta: >=0, inverse temperature for RL (scaled by 10 internally).
    - neg_tag_weight: >=0, magnitude of negative WM tag after no reward.
    - ss_suppression: >=0, exponential suppression factor: wm_weight_eff = wm_weight * exp(-ss_suppression*(set_size-3)).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, neg_tag_weight, ss_suppression = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # We'll store WM tags in w as real-valued scores (not probabilities); initialize to 0
        w = np.zeros((nS, nA))
        # Not used as probabilities here, but keep w_0 to satisfy template expectations
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size suppression of WM mixture
        wm_weight_eff_block = wm_weight * np.exp(-ss_suppression * (float(nS) - 3.0))
        wm_weight_eff_block = max(0.0, min(1.0, wm_weight_eff_block))

        log_p = 0.0

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            V_s = w[s, :]  # WM valence scores

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability from valence tags (sharp softmax over V_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (V_s - V_s[a])))

            # Mixture (no lapse term here; mixture weight already ss-modulated)
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: set valence tags
            if r > 0.0:
                # Strong positive tag for the rewarded action; reset others to 0
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Negative tag to discourage the unrewarded action; leave others at 0
                w[s, :] = 0.0
                w[s, a] = -abs(neg_tag_weight)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size penalty, plus uncertainty learning.

    Mechanism:
    - RL: Q-learning with softmax.
    - WM: stores the last rewarded action as a one-hot entry per state; deterministic WM softmax.
    - Uncertainty: each state's RL uncertainty u_s is tracked as an exponentially smoothed absolute
      prediction error. When uncertainty is low, the controller trusts WM more. Arbitration weight:
      wm_weight_eff = wm_weight_base * (3 / set_size) * (1 - u_s) ** eta.
    - This yields stronger WM use in small set sizes and when the RL estimate is confident.

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate.
    - wm_weight_base: [0,1] base WM weight before set-size and uncertainty gating.
    - softmax_beta: >=0, inverse temperature for RL (scaled by 10 internally).
    - alpha_u: [0,1], learning rate for updating state-wise RL uncertainty u_s from |PE|.
    - eta: >=0, exponent controlling how strongly low uncertainty boosts WM weight.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, alpha_u, eta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize state-wise RL uncertainty u_s to high (0.5)
        u = 0.5 * np.ones(nS)

        log_p = 0.0

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-based WM weight with set-size penalty
            ss_penalty = (3.0 / float(nS))
            wm_weight_eff = wm_weight_base * ss_penalty * (max(0.0, 1.0 - u[s]) ** eta)
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Uncertainty update from absolute PE
            u[s] = (1.0 - alpha_u) * u[s] + alpha_u * abs(pe)

            # WM overwriting on reward; mild passive decay otherwise toward uniform (set-size driven)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Passive decay with interference stronger for larger set size (no extra parameter)
                decay = max(0.0, min(1.0, 1.0 - (3.0 / float(nS))))
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set size effects:
- Model 1: WM mixture weight is linearly penalized by set size (3/set_size), and WM interference/decay increases with set size (decay = 1 - 3/set_size).
- Model 2: WM mixture weight is exponentially suppressed by set size via ss_suppression, and WM uses valence tags to encourage rewarded and avoid punished actions.
- Model 3: WM mixture weight is penalized by set size (3/set_size) and additionally modulated by state-wise RL uncertainty; WM decays more under larger set sizes.