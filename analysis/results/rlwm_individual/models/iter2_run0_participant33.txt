def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with set-size-dependent exploration and RL forgetting; WM reward-gated encoding.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight : float
            Mixture weight between WM and RL policies (0..1).
        softmax_beta : float
            Base inverse temperature for RL policy; scaled internally by 10 and further modulated by set size.
        rl_forget : float
            Per-trial RL forgetting toward uniform (0..1). Scaled by set size (more forgetting for larger sets).
        wm_learn : float
            Strength with which WM encodes the chosen action on rewarded trials (0..1).
        beta_ss_slope : float
            Set-size modulation for RL inverse temperature; beta_eff = beta_base * sigmoid(beta_ss_slope*(3.5 - nS)).

    Set-size effects
    ----------------
    - RL inverse temperature decreases with larger set size via beta_ss_slope (more exploration at nS=6).
    - RL forgetting scales with set size: effective forgetting = rl_forget * (nS / 6.0).
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_learn, beta_ss_slope = model_parameters

    softmax_beta_base = softmax_beta * 10.0
    softmax_beta_wm = 50.0  # deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent RL temperature
        cap_beta = 1.0 / (1.0 + np.exp(-beta_ss_slope * (3.5 - nS)))
        beta_eff = max(softmax_beta_base * cap_beta, 1e-8)

        # Set-size scaled RL forgetting
        rl_forget_eff = np.clip(rl_forget * (nS / 6.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over WM values
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q[s, :] = (1.0 - rl_forget_eff) * q[s, :] + rl_forget_eff * w_0[s, :]

            # WM update: reward-gated encoding (push probability mass to chosen action)
            if r > 0.0:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
                w[s, :] /= max(np.sum(w[s, :]), 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with asymmetric RL learning and adaptive WM gating based on recent success and set size.

    Parameters
    ----------
    model_parameters : tuple
        lr_pos : float
            RL learning rate for positive prediction errors (rewards).
        lr_neg : float
            RL learning rate for negative prediction errors (no reward).
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        wm_weight_base : float
            Baseline WM arbitration weight.
        gate_bias : float
            Bias term for WM gate (logistic). Higher -> more WM usage.
        gate_ss_slope : float
            Set-size slope for WM gate; gate increases for small set sizes and decreases for large set sizes.

    Mechanism
    ---------
    - WM gate g_t = sigmoid(gate_bias + gate_ss_slope*(3.5 - nS) + 2*(recent_correct - 0.5)).
      Effective WM weight = wm_weight_base * g_t.
    - WM store is one-shot on rewarded trials for the current state.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, gate_bias, gate_ss_slope = model_parameters

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Recent performance tracker (fixed time scale)
        recent_correct = 0.5
        tau = 0.2  # fixed smoothing for reward rate proxy

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute adaptive WM gate
            gate_drive = gate_bias + gate_ss_slope * (3.5 - nS) + 2.0 * (recent_correct - 0.5)
            g_t = 1.0 / (1.0 + np.exp(-gate_drive))
            wm_weight_eff = np.clip(wm_weight_base * g_t, 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: one-shot store on rewarded trials (overwrite to one-hot)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # Update recent performance estimate
            recent_correct = (1.0 - tau) * recent_correct + tau * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with state-specific choice stickiness and set-size-dependent WM decay.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        wm_weight_base : float
            Baseline WM mixture weight.
        stickiness : float
            Choice perseveration bias added to the previously chosen action within a state.
        wm_decay_base : float
            Baseline WM decay toward uniform per trial.
        wm_decay_ss_slope : float
            Set-size slope for WM decay; larger set sizes induce stronger decay.

    Set-size effects
    ----------------
    - WM decay is increased with larger set sizes via wm_decay_eff = wm_decay_base * sigmoid(wm_decay_ss_slope*(nS - 3)).
    """
    lr, softmax_beta, wm_weight_base, stickiness, wm_decay_base, wm_decay_ss_slope = model_parameters

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Previous action memory for stickiness (per state)
        prev_a = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM decay
        wm_decay_eff = wm_decay_base * (1.0 / (1.0 + np.exp(-wm_decay_ss_slope * (nS - 3.0))))
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Stickiness bias: add to last chosen action within the state
            bias = np.zeros(3)
            if prev_a[s] >= 0:
                bias[prev_a[s]] += stickiness

            # RL policy with stickiness in the softmax
            denom_rl = np.sum(np.exp(softmax_beta * ((Q_s + bias) - (Q_s[a] + bias[a]))))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (set-size dependent)
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM reward-driven strengthening using same decay factor as learning gain
            if r > 0.0:
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :]
                w[s, a] += wm_decay_eff
                w[s, :] /= max(np.sum(w[s, :]), 1e-12)

            # Update stickiness memory
            prev_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p