def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with reward-gated WM writes and set-size-scaled WM decay.

    Policy:
    - RL system: softmax over Q with softmax_beta (scaled internally by 10).
    - WM system: softmax over W with very high inverse temperature (deterministic).
    - Mixture: fixed wm_weight_base, but WM memory quality degrades with set size
      via a set-size-scaled decay applied on each WM visit.

    Learning:
    - RL: single learning rate lr for Q-learning updates.
    - WM: on each visit, decay W toward uniform with wm_decay_eff, where
          wm_decay_eff = clip(wm_decay_base + gamma_setsize * (nS - 3) / 3, 0, 1).
          Then, if reward==1, write toward a one-hot for the chosen action with rate wm_learn_rate.

    Set-size effects:
    - Larger set size increases WM decay via gamma_setsize, reducing WM fidelity and, implicitly,
      the efficacy of the WM component in the mixture.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight_base: mixture weight for WM (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_learn_rate: WM write rate when reward==1 (0..1).
    - wm_decay_base: baseline WM decay per visit (0..1).
    - gamma_setsize: how much additional WM decay scales with set size (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_learn_rate, wm_decay_base, gamma_setsize = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-scaled WM decay for the whole block
        wm_decay_eff = np.clip(wm_decay_base + gamma_setsize * max(0, (nS - 3) / 3.0), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over W with high beta
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff* p_wm + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating: decay toward uniform, then reward-gated write
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn_rate) * w[s, :] + wm_learn_rate * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with dynamic arbitration based on entropy (uncertainty) of WM and RL policies.

    Policy:
    - RL system: softmax over Q with softmax_beta (scaled by 10).
    - WM system: softmax over W with high inverse temperature.
    - Mixture: wm_weight_t = sigmoid(k_wm * (Hmax - H_wm) - k_rl * H_rl)
        where H_wm is the entropy of the WM policy at the current state,
              H_rl is the entropy of the RL policy at the current state,
              Hmax = log(nA).
      Thus, WM dominates when its policy is sharp (low entropy) and RL is uncertain.

    Learning:
    - RL: learning rate lr for Q updates.
    - WM: exponential decay toward uniform with wm_decay; recency-based write with rate wm_eta (agnostic to reward).

    Set-size effects:
    - Larger set sizes tend to keep WM more uniform (higher H_wm), which reduces wm_weight_t.
      This provides an implicit load effect without explicit set-size parameters.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_eta: WM write rate toward chosen action each visit (0..1).
    - wm_decay: WM decay toward uniform each visit (0..1).
    - k_wm: sensitivity of arbitration to WM certainty (>=0).
    - k_rl: sensitivity of arbitration to RL uncertainty (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_eta, wm_decay, k_wm, k_rl = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    def softmax_probs(x, beta):
        z = x - np.max(x)
        ex = np.exp(beta * z)
        return ex / np.sum(ex)

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        Hmax = np.log(nA)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy probability of chosen action under high beta
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Full distributions for arbitration entropies
            p_rl_full = softmax_probs(Q_s, softmax_beta)
            p_wm_full = softmax_probs(W_s, softmax_beta_wm)
            H_rl = entropy(p_rl_full)
            H_wm = entropy(p_wm_full)

            # Dynamic arbitration weight
            wm_weight_t = 1.0 / (1.0 + np.exp(-(k_wm * (Hmax - H_wm) - k_rl * H_rl)))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating: decay toward uniform, then recency-based write
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and WM that stores last rewarded action,
    with set-size-scaled WM mixture weight via a capacity ratio.

    Policy:
    - RL system: softmax over Q with softmax_beta (scaled by 10).
    - WM system: softmax over W with high inverse temperature.
    - Mixture: wm_weight_eff = wm_weight_base * capacity_ratio * (3.0 / nS),
      so WM influence decreases with larger set size.

    Learning:
    - RL: separate learning rates for positive and negative outcomes (lr_pos, lr_neg).
    - WM: overwrites the state's memory toward the chosen action when reward==1 with rate wm_write.
          Otherwise, W only decays slightly toward uniform with wm_forget.

    Set-size effects:
    - WM mixture weight scales inversely with nS via capacity_ratio*(3/nS), mimicking limited WM capacity.

    Parameters (6):
    - lr_pos: RL learning rate for rewarded trials (0..1).
    - lr_neg: RL learning rate for non-rewarded trials (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: base WM mixture weight (0..1).
    - wm_write: WM write rate when reward==1 (0..1).
    - capacity_ratio: scales WM contribution with set size (0..1), used as wm_forget on no-reward.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_write, capacity_ratio = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-scaled WM mixture weight
        scale = 3.0 / max(1.0, float(nS))
        wm_weight_eff = np.clip(wm_weight_base * np.clip(capacity_ratio, 0.0, 1.0) * scale, 0.0, 1.0)

        # Use capacity_ratio also as a mild forget rate on no-reward (ties parameter to learning)
        wm_forget = np.clip(capacity_ratio, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff* p_wm + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, eps))
      
            # RL update with asymmetric learning rates
            alpha = lr_pos if r > 0 else lr_neg
            delta = r - Q_s[a]
            q[s][a] += alpha * delta

            # WM update: reward-contingent overwrite; otherwise mild forgetting
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_write) * w[s, :] + wm_write * one_hot
            else:
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p