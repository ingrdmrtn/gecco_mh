def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and WM decay.

    Mechanism
    - RL: single learning rate with standard softmax choice.
    - WM: fast, near-deterministic cache that decays toward uniform each trial.
    - Arbitration: the WM mixture weight is reduced by two factors:
        (a) load factor that decreases with set size (sigmoid of set size),
        (b) RL-uncertainty factor (higher WM contribution when RL is more uncertain).

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM mixture weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        kappa_decay: float
            WM decay toward uniform per visit (0..1).
        zeta_unc: float
            Arbitration sensitivity to RL uncertainty (>=0). Higher values increase WM use when RL is uncertain.
        lam_load: float
            Load sensitivity (>=0) controlling how WM weight declines with set size.

    Returns
    -------
    nll: float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, kappa_decay, zeta_unc, lam_load = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-based baseline scaling for WM mixture (lower under higher load)
        # Using a sigmoid from set size around 3.5 with slope lam_load
        load_scale = 1.0 / (1.0 + np.exp(lam_load * (nS - 3.5)))
        wm_mix_base = np.clip(wm_weight * load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy (softmax expressed as 1/sum exp(beta*(Q - Q[a])))
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: more WM when RL is more uncertain (high entropy, low max gap)
            # Use normalized entropy of RL policy as uncertainty proxy (0..1)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits - np.max(logits))
            prl_vec = exp_logits / np.sum(exp_logits)
            entropy = -np.sum(np.clip(prl_vec, 1e-12, 1.0) * np.log(np.clip(prl_vec, 1e-12, 1.0)))
            max_entropy = np.log(nA)
            H_norm = entropy / max_entropy if max_entropy > 0 else 0.0
            wm_unc_scale = 1.0 / (1.0 + np.exp(-zeta_unc * (H_norm - 0.5)))  # 0..1 increasing with uncertainty

            wm_mix = np.clip(wm_mix_base * wm_unc_scale, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform each visit
            w[s, :] = (1.0 - kappa_decay) * w[s, :] + kappa_decay * w_0[s, :]
            # On reward, cache the chosen action (overwrite toward one-hot)
            if r > 0.5:
                # Blend toward a one-hot at chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - kappa_decay) * w[s, :] + kappa_decay * target

            # Renormalize
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Win-Stay/Lose-Shift WM with load-dependent WM failures and WM memory decay.

    Mechanism
    - RL: standard delta rule and softmax choice.
    - WM: implements a WSLS heuristic per state:
        * If last outcome in a state was rewarded, favor repeating last action.
        * If last outcome was not rewarded, avoid last action.
      This is implemented via a WM value vector that strongly boosts or suppresses the last action.
    - Load effect: WM mixture is reduced as set size increases (exponential drop-off).
    - WM decay: the WM value for a state decays toward uniform on each visit.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM mixture weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        wsls_gain: float
            Strength of WSLS coding in WM (>=0). Higher values make WM policy more deterministic.
        rho_forget: float
            WM decay rate toward uniform on each visit (0..1).
        phi_load: float
            Load sensitivity (>=0). Higher values produce stronger WM failure with set size.

    Returns
    -------
    nll: float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, wsls_gain, rho_forget, phi_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Load-induced failure factor: exponential decrease with set size relative to 3
        fail = 1.0 - np.exp(-phi_load * max(nS - 3, 0))
        fail = np.clip(fail, 0.0, 1.0)
        wm_mix_base = wm_weight * (1.0 - fail)
        wm_mix_base = np.clip(wm_mix_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Construct WSLS WM values for current state
            W_s = w[s, :].copy()
            # Decay WM toward uniform whenever state is visited
            W_s = (1.0 - rho_forget) * W_s + rho_forget * w_0[s, :]

            if last_action[s] >= 0:
                if last_reward[s] > 0.5:
                    # Win-Stay: boost last action
                    W_s[last_action[s]] += wsls_gain
                else:
                    # Lose-Shift: suppress last action (boost others)
                    W_s[last_action[s]] -= wsls_gain
                    for aa in range(nA):
                        if aa != last_action[s]:
                            W_s[aa] += wsls_gain / (nA - 1)

            # Ensure positivity and renormalize a WM preference vector for softmax
            W_s = np.maximum(W_s, 1e-8)
            W_s = W_s / np.sum(W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Commit the decayed WM values back
            w[s, :] = (1.0 - rho_forget) * w[s, :] + rho_forget * w_0[s, :]
            # Update memory traces based on WSLS outcome
            if r > 0.5:
                # Strengthen the chosen action's memory
                w[s, a] += wsls_gain
            else:
                # Penalize the chosen action, distribute preference to others
                w[s, a] = max(w[s, a] - wsls_gain, 1e-8)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += wsls_gain / (nA - 1)
            # Normalize
            w[s, :] /= np.sum(w[s, :])

            # Update last action/outcome memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM with confidence gating and action-trace bias.

    Mechanism
    - RL: standard delta rule with softmax.
    - WM: a recallable cache of correct S-A; success probability scales with capacity K:
        p_success = min(1, K / nS). On success WM is nearly deterministic.
      Working memory decays toward uniform when not reinforced.
    - Confidence gating: WM mixture is further scaled up when RL is uncertain (small Q-gap).
    - Action-trace bias (recency): an exponentially decaying per-state action trace biases RL values.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM mixture weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        K_capacity: float
            WM capacity (0..6). Effective WM success is min(1, K_capacity / set_size).
        lambda_trace: float
            Action-trace decay (0..1). Larger means longer-lasting recency bias.
        eta_conf: float
            Confidence-gating strength (>=0) scaling WM more when RL is uncertain.

    Returns
    -------
    nll: float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, K_capacity, lambda_trace, eta_conf = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state action trace for recency bias in RL
        trace = np.zeros((nS, nA))

        # Capacity-based WM success probability
        p_success = np.clip(K_capacity / max(nS, 1e-6), 0.0, 1.0)
        wm_mix_base = np.clip(wm_weight * p_success, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with action-trace bias
            Q_s = q[s, :] + trace[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from cached vector
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence gating: stronger WM when RL is uncertain (small Q-gap)
            q_gap = np.max(q[s, :]) - np.partition(q[s, :], -2)[-2]  # top-1 minus top-2
            # Map small gaps to higher WM scaling via decreasing exponential
            conf_scale = np.exp(-eta_conf * q_gap)
            wm_mix = np.clip(wm_mix_base * conf_scale, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform slightly each visit
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.5:
                # On reward, store deterministic association for this state
                w[s, :] = 1e-6 * np.ones(nA)
                w[s, a] = 1.0 - (nA - 1) * 1e-6
            else:
                # On no-reward, soften the chosen action's memory
                w[s, a] = 0.5 * w[s, a] + 0.5 * (1.0 / nA)
                # Renormalize slightly toward uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Normalize WM vector
            w[s, :] /= np.sum(w[s, :])

            # Update action trace: decay and add bump to chosen action
            trace[s, :] *= (1.0 - lambda_trace)
            trace[s, a] += lambda_trace

        blocks_log_p += log_p

    return -blocks_log_p