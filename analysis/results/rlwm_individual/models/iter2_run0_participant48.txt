Below are three standalone cognitive models that implement different RL+WM mechanisms suited to the RLWM task. Each returns the negative log-likelihood of the observed choices and uses all parameters meaningfully. They follow the provided template structure, filling in the WM policy and WM updating logic.

Note: Assume numpy as np is already imported. No imports are included in the functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-sizeâ€“scaled WM weight, WM decay, and RL forgetting.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate (lr) and global forgetting (phi)
      that decays Q-values toward uniform each trial (captures interference/load).
    - WM: item-specific store that decays toward uniform (wm_decay), and only encodes on rewarded trials.
    - Mixture: baseline WM weight (wm_weight) is downscaled by capacity overload:
        wm_block = wm_weight * min(1, capacity / nS)
      so WM contributes less when the set-size exceeds capacity.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: WM decay rate toward uniform each trial (0..1)
    - phi: RL forgetting rate toward uniform each trial (0..1)
    - capacity: WM capacity (in number of items); reduces WM influence when nS > capacity

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, phi, capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM weight per block
        scale = min(1.0, capacity / max(1.0, float(nS)))
        wm_block = wm_weight * scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM values; WM decays to uniform globally each trial)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_block * p_wm + (1.0 - wm_block) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update with forgetting toward uniform after learning
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q = (1.0 - phi) * q + phi * (1.0 / nA) * np.ones_like(q)

            # WM decay and encoding only when rewarded
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with uncertainty-based arbitration and WM decay.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate.
    - WM: item-specific store with decay toward uniform (wm_decay), encoding on rewarded trials.
    - Arbitration: mixture weight is a sigmoid of (H_rl - H_wm) and set-size factor,
        wm_t = sigmoid(omega0 + omega1*(H_rl - H_wm) + omega2*(size_factor - 0.5)),
      where H_rl and H_wm are the entropies of the RL and WM policies for the current state,
      and size_factor = 3 / nS (1.0 for set-size 3, 0.5 for set-size 6). Higher WM confidence
      (lower H_wm) or smaller set-size increases WM control, modulated by omegas.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: WM decay rate toward uniform each trial (0..1)
    - omega0: arbitration bias (real)
    - omega1: arbitration sensitivity to H_rl - H_wm (real)
    - omega2: arbitration sensitivity to set-size factor (real)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_decay, omega0, omega1, omega2 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        size_factor = 3.0 / float(nS)  # 1.0 (nS=3) vs 0.5 (nS=6)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy: compute full softmax for entropy
            Q_s = q[s, :]
            z_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = z_rl / np.sum(z_rl)
            p_rl = pi_rl[a]
            H_rl = -np.sum(pi_rl * np.log(pi_rl + 1e-15))

            # WM policy: compute full softmax for entropy
            W_s = w[s, :]
            z_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = z_wm / np.sum(z_wm)
            p_wm = pi_wm[a]
            H_wm = -np.sum(pi_wm * np.log(pi_wm + 1e-15))

            # Arbitration weight via sigmoid
            z = omega0 + omega1 * (H_rl - H_wm) + omega2 * (size_factor - 0.5)
            wm_t = 1.0 / (1.0 + np.exp(-z))

            p_total = wm_t * p_wm + (1.0 - wm_t) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and encoding on rewarded trials
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with choice perseveration, lapse, and power-law set-size scaling of WM influence.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate (lr).
      Choice perseveration is modeled by adding a state-specific last-choice trace to Q
      before the softmax: Q_eff = Q + kappa * (trace - 1/nA), where trace is one-hot for
      the last chosen action in that state (0 otherwise).
    - WM: item-specific one-hot store that encodes only when reward is received (no decay).
      Under larger set-sizes, WM is down-weighted by a power-law: wm_block = wm_weight * nS^{-gamma}.
    - Lapse: with probability epsilon, choice is random; otherwise follow the RL-WM mixture.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - kappa: strength of perseveration bias added to RL values (real, >=0)
    - epsilon: lapse rate mixing in uniform random choice (0..1)
    - gamma: exponent controlling how WM weight declines with set size (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa, epsilon, gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Last-choice trace per state for perseveration
        trace = np.zeros((nS, nA))

        # Set-size scaled WM weight (power-law)
        wm_block = wm_weight * (max(1.0, float(nS)) ** (-gamma))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_base = q[s, :]
            Q_eff = Q_base + kappa * (trace[s, :] - (1.0 / nA))
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy (no decay here; relies on set-size down-weighting)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_block * p_wm + (1.0 - wm_block) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - Q_base[a]
            q[s, a] += lr * delta

            # Update perseveration trace to last chosen action for this state
            trace[s, :] = 0.0
            trace[s, a] = 1.0

            # WM update: encode only when rewarded
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p

How these differ from your prior best model:
- Model 1 introduces RL forgetting (phi) and a simple capacity-scaled WM mixture (not entropy-based, not asymmetric learning).
- Model 2 removes fixed wm_weight and instead arbitrates via entropies (uncertainty) plus an explicit set-size factor, yielding dynamic trial-by-trial mixture.
- Model 3 adds a choice perseveration bias and a lapse component, and uses a power-law set-size scaling for WM weight with no WM decay, differing mechanistically and parametrically from the prior capacity+decay+asymLR model.