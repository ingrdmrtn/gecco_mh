def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size weighted mixture and lapses.

    Idea:
      - RL: standard Q-learning with softmax.
      - WM: one-shot associative memory that encodes rewarded mappings,
            used with a strong, near-deterministic policy.
      - The WM contribution is down-weighted when set size exceeds a capacity-like threshold.
      - A lapse component increases with set size, degrading WM policy.

    Parameters
    ----------
    model_parameters : tuple/list with 5 entries
        lr : float
            RL learning rate (0-1).
        wm_base : float
            Baseline WM mixture weight (0-1), modulated by capacity and set size.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        capacity_k : float
            WM capacity parameter (>0). WM mixture weight scales as min(1, capacity_k / nS).
        lapse : float
            Lapse probability amplitude (0-1) that grows with set size (uniform random action in WM policy).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_base, softmax_beta, capacity_k, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # highly deterministic WM policy baseline

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables to uniform
        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight given set size
        wm_weight_eff = wm_base * min(1.0, capacity_k / max(1.0, nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # - baseline deterministic softmax from WM
            # - lapse increases with set size to reflect higher load
            lapse_eff = lapse * (nS / 6.0)
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse_eff) * p_wm_core + lapse_eff * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - On reward, "write" a near one-hot code for the state-action mapping.
            # - On no reward, add mild drift toward uniform (to avoid over-commitment to errors).
            if r > 0:
                w[s, :] = 1e-8  # nearly zero elsewhere
                w[s, a] = 1.0
                w[s, :] /= w[s, :].sum()
            else:
                # mild drift back to uniform when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                w[s, :] /= w[s, :].sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic encoding WM with set-size-dependent decay and choice perseveration.

    Idea:
      - RL: standard Q-learning with softmax.
      - WM: associative memory that encodes rewarded actions with a probability
            that increases in smaller set sizes; WM decays toward uniform at a rate
            that increases with set size (via a logistic slope).
      - Choice perseveration: a recency bias to repeat the last action for the same state,
            applied as a multiplicative bias on the final mixture policy.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight between WM and RL policies (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_encode_prob : float
            Baseline probability of encoding a rewarded action into WM per visit (0-1),
            which is scaled up in smaller set sizes.
        decay_slope : float
            Slope (>0) controlling how WM decay (toward uniform) accelerates with set size.
        perseveration : float
            Strength of choice stickiness for repeating the last action in the same state (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_encode_prob, decay_slope, perseveration = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size-dependent WM decay (logistic mapping scaled to modest range)
        # Maps nS in [3,6] through sigmoid to [~0, ~0.3], increasing with nS.
        def wm_decay_rate(nS_local):
            sig = 1.0 / (1.0 + np.exp(-decay_slope * (nS_local - 3.0)))
            return 0.3 * sig

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL full softmax vector and chosen prob
            exp_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM full softmax vector and chosen prob
            exp_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture
            p_mix_vec = wm_weight * p_wm_vec + (1.0 - wm_weight) * p_rl_vec

            # Apply perseveration bias toward repeating last action in this state
            if last_action[s] >= 0:
                bias = np.ones(nA)
                bias[last_action[s]] = np.exp(perseveration)
                p_biased_vec = p_mix_vec * bias
                p_biased_vec /= p_biased_vec.sum()
            else:
                p_biased_vec = p_mix_vec

            p_total = p_biased_vec[a]
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (set-size dependent)
            f = wm_decay_rate(nS)
            w[s, :] = (1.0 - f) * w[s, :] + f * w_0[s, :]

            # Probabilistic encoding in expectation when rewarded
            # Encoding probability increases with smaller set size
            p_enc = wm_encode_prob * (6.0 / max(1.0, nS))
            p_enc = np.clip(p_enc, 0.0, 1.0)
            if r > 0:
                # Move distribution toward one-hot on the chosen action,
                # by allocating p_enc mass to a and removing proportionally.
                w[s, :] = (1.0 - p_enc) * w[s, :]
                w[s, a] += p_enc
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= w[s, :].sum()

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM gated by RL uncertainty and retrieval noise.

    Idea:
      - RL: Q-learning with separate learning rates for positive and negative prediction errors.
      - WM: stores rewarded state-action mappings with a sharp softmax. Retrieval suffers
            from noise that increases with set size.
      - Mixture weight: WM's contribution increases when RL is uncertain (high entropy),
            via a sigmoid mapping controlled by an entropy gain parameter.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr_pos : float
            RL learning rate for positive prediction errors (0-1).
        lr_neg : float
            RL learning rate for negative prediction errors (0-1).
        wm_weight_base : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_noise : float
            Retrieval noise amplitude (0-1); mixes WM policy with uniform,
            scaling up with set size.
        entropy_gain : float
            Sensitivity (>0) of WM mixture gating to RL entropy (uncertainty).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, wm_noise, entropy_gain = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Entropy reference (midpoint) for gating; max entropy is ln(nA)
        H_ref = 0.5 * np.log(nA)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax vector and chosen prob
            exp_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # RL entropy for gating
            H = -np.sum(p_rl_vec * np.log(p_rl_vec + 1e-12))

            # WM policy with retrieval noise increasing with set size
            exp_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_wm_core = exp_wm / np.sum(exp_wm)
            rho = np.clip(wm_noise * (nS / 6.0), 0.0, 1.0)
            p_wm_vec = (1.0 - rho) * p_wm_core + rho * (1.0 / nA)
            p_wm = p_wm_vec[a]

            # Uncertainty-gated mixture weight (higher when RL is more uncertain)
            gate = 1.0 / (1.0 + np.exp(-entropy_gain * (H - H_ref)))
            wm_weight_eff = wm_weight_base * gate

            p_mix_vec = wm_weight_eff * p_wm_vec + (1.0 - wm_weight_eff) * p_rl_vec
            p_total = p_mix_vec[a]
            log_p += np.log(p_total + 1e-12)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: write rewarded associations; mild drift otherwise
            if r > 0:
                w[s, :] = 1e-8
                w[s, a] = 1.0
                w[s, :] /= w[s, :].sum()
            else:
                # gentle drift toward uniform when not rewarded
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
                w[s, :] /= w[s, :].sum()

        blocks_log_p += log_p

    return -blocks_log_p