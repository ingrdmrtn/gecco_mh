def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with load-dependent decay and soft recall

    Description:
    - RL: tabular Q-learning as given by the template (lr, softmax_beta).
    - WM: stores per-state action weights w[s,:], which decay faster under higher set size nS,
      mimicking interference. WM policy sharpness is scaled by an effective "slot coverage"
      p_hit = min(1, cap_k / nS), capturing capacity limitations.
    - The WM softmax becomes more/less deterministic according to p_hit; when capacity is
      exceeded (nS >> cap_k), WM becomes less diagnostic.
    - Update: after reward, WM shifts towards a one-hot code; without reward, it mildly
      suppresses the chosen action. Between trials, WM decays toward uniform with a rate
      that increases with nS.

    Parameters (tuple):
    - lr: RL learning rate for Q update (0..1)
    - wm_weight: Mixture weight between WM and RL policies (0..1)
    - softmax_beta: Inverse temperature for RL softmax (internally scaled x10)
    - cap_k: WM capacity in number of items (>0)
    - wm_decay_base: Base WM decay factor per trial (0..1); effective decay = wm_decay_base^(1 + nS/cap_k)
      increases with nS, modeling interference
    """
    lr, wm_weight, softmax_beta, cap_k, wm_decay_base = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic baseline
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load-dependent WM decay factor per step in this block
        decay_exp = 1.0 + (nS / max(cap_k, eps))
        wm_decay = max(0.0, min(1.0, wm_decay_base)) ** decay_exp

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: capacity-limited sharpness via p_hit = min(1, cap_k / nS)
            p_hit = min(1.0, max(0.0, cap_k) / max(1.0, nS))
            beta_eff = softmax_beta_wm * max(eps, p_hit)
            denom_wm = np.sum(np.exp(beta_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Between-trial decay toward uniform, stronger under larger nS
            w = wm_decay * w + (1.0 - wm_decay) * w_0

            if r > 0.5:
                # Rewarded: one-shot strengthening toward a one-hot code
                w[s, :] = 0.1 * w[s, :]  # compress others
                w[s, a] = 1.0
            else:
                # Unrewarded: mild suppression of the chosen action
                w[s, a] = 0.8 * w[s, a] + 0.2 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + entropy-gated WM with load-sensitive decay and WM learning rate

    Description:
    - RL: tabular Q-learning (lr, softmax_beta).
    - WM: maintains per-state action weights w[s,:] that learn with a WM learning rate
      toward a one-hot target when rewarded, and away from the chosen action when not rewarded.
    - Reliability gating: the WM policy sharpness is modulated by the entropy of W_s.
      When memory for the state is concentrated (low entropy), WM is sharp; when diffuse (high
      entropy), WM becomes noisy. A 'wm_sharp' exponent controls this sensitivity.
    - Load sensitivity: both the WM decay toward uniform and the WM learning rate are scaled
      by set size via 'load_sens' (greater nS -> more decay, less effective WM learning).

    Parameters (tuple):
    - lr: RL learning rate for Q update (0..1)
    - wm_weight: Mixture weight between WM and RL policies (0..1)
    - softmax_beta: Inverse temperature for RL softmax (internally scaled x10)
    - wm_lr_base: Base WM learning rate (0..1) before load scaling
    - wm_decay_base: Base WM decay toward uniform (0..1) before load scaling
    - wm_sharp: Entropy-to-sharpness exponent (>0); higher = more sensitivity to entropy
    """
    lr, wm_weight, softmax_beta, wm_lr_base, wm_decay_base, wm_sharp = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-sensitive scaling for WM learning and decay
        load_scale = 1.0 / max(1.0, nS)  # larger nS -> smaller scale
        wm_lr = max(0.0, min(1.0, wm_lr_base)) * load_scale
        wm_decay = max(0.0, min(1.0, wm_decay_base)) + (1.0 - max(0.0, min(1.0, wm_decay_base))) * (1.0 - load_scale)
        wm_decay = max(0.0, min(1.0, wm_decay))  # clamp

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Entropy of WM distribution for state s
            W_pos = np.maximum(W_s, eps)
            W_pos = W_pos / np.sum(W_pos)
            H = -np.sum(W_pos * np.log(W_pos))
            Hmax = np.log(nA)
            reliability = (1.0 - H / max(eps, Hmax))
            beta_eff = softmax_beta_wm * (max(eps, reliability) ** max(eps, wm_sharp))

            denom_wm = np.sum(np.exp(beta_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform, stronger under higher load
            w = wm_decay * w + (1.0 - wm_decay) * w_0

            # WM learning step
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target
            else:
                # Move probability mass away from the chosen action
                not_a = (1.0 - np.eye(nA)[a]) / (nA - 1.0)
                w[s, :] = (1.0 - 0.5 * wm_lr) * w[s, :] + 0.5 * wm_lr * not_a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recall-failure WM with load- and memory-strengthâ€“dependent access

    Description:
    - RL: tabular Q-learning (lr, softmax_beta).
    - WM: maintains a fast-binding associative map w[s,:] plus a separate scalar memory
      strength m[s] that tracks how reliable the binding is for each state.
    - Access model (recall failure): with probability p_rec(s) = sigmoid(gain*m[s] - load*sens*nS),
      WM retrieves its policy; otherwise WM yields a uniform distribution (a WM-specific lapse).
      This models failures to retrieve under high load and weak memory.
    - Memory strength m[s] increases on reward, decays each step, and is bounded [0,1].
    - Update: rewarded trials set a sharp one-hot WM trace; unrewarded trials leave the trace but
      reduce its strength via m[s] decay and slight chosen-action suppression.

    Parameters (tuple):
    - lr: RL learning rate for Q update (0..1)
    - wm_weight: Mixture weight between WM and RL policies (0..1)
    - softmax_beta: Inverse temperature for RL softmax (internally scaled x10)
    - gain: Scales the influence of memory strength on recall probability (>0)
    - load_sens: Scales the penalty of set size on recall probability (>0)
    - m_decay: Memory strength decay per trial (0..1)
    """
    lr, wm_weight, softmax_beta, gain, load_sens, m_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Memory strength per state
        m = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Recall probability based on memory strength and load
            lin = gain * m[s] - load_sens * nS
            # Sigmoid with soft clipping for stability
            p_rec = 1.0 / (1.0 + np.exp(-np.clip(lin, -20.0, 20.0)))

            # Retrieved WM policy if recall succeeds; otherwise uniform lapse
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_retrieved = 1.0 / max(eps, denom_wm)
            p_wm = p_rec * p_wm_retrieved + (1.0 - p_rec) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay of memory strength
            m = np.clip(m * max(0.0, min(1.0, m_decay)), 0.0, 1.0)

            if r > 0.5:
                # Rewarded: strengthen memory and set a sharp one-hot pattern
                m[s] = np.clip(m[s] + 0.5 * (1.0 - m[s]), 0.0, 1.0)
                w[s, :] = 0.05 * w[s, :]
                w[s, a] = 1.0
            else:
                # Unrewarded: weaken memory and suppress the chosen action slightly
                m[s] = np.clip(m[s] * 0.8, 0.0, 1.0)
                w[s, a] = 0.85 * w[s, a] + 0.15 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p