def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size-gated WM contribution and reward-sensitive WM imprint.

    Idea:
    - RL learns action values with a single learning rate.
    - WM holds a fast, near-deterministic guess; its influence is down-weighted as set size increases
      via a logistic gating function.
    - WM updates imprint the chosen action strongly after reward, and partially after no-reward.
    - WM globally drifts toward uniform more when the set is larger (capacity pressure).

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of {0,1}
        Feedback.
    blocks : array-like of int
        Block indices.
    set_sizes : array-like of int
        Set size (3 or 6).
    model_parameters : iterable of 6 floats
        - lr: RL learning rate (0..1).
        - wm_gain_base: Base WM weight in the policy mixture (0..1).
        - softmax_beta: RL inverse temperature (scaled internally).
        - retr_slope: Slope (>0) of the logistic gating over set size.
        - retr_center: Center of the logistic gating in set-size units (e.g., 4.5).
        - wm_imprint: Strength of WM imprint (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_gain_base, softmax_beta, retr_slope, retr_center, wm_imprint = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size gated WM weight using logistic gating: higher nS -> lower WM weight
        # wm_weight_eff in [0,1]
        gate = 1.0 / (1.0 + np.exp(-retr_slope * (retr_center - float(nS))))
        wm_weight_eff = np.clip(wm_gain_base * gate, 0.0, 1.0)

        # Global WM drift toward uniform increases with set size (no free param here)
        # Larger nS => more drift
        wm_drift = 1.0 - (1.0 / float(nS))
        wm_drift = np.clip(wm_drift, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global drift toward uniform (set-size dependent)
            w = (1.0 - wm_drift) * w + wm_drift * w_0

            # WM local imprint: stronger after reward, partial after no-reward
            ins = wm_imprint if r > 0.0 else 0.5 * wm_imprint
            ins = np.clip(ins, 0.0, 1.0)
            w[s, :] = (1.0 - ins) * w[s, :]
            w[s, a] += ins

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size-specific RL temperature and WM reset on errors.

    Idea:
    - RL has separate inverse temperatures for small vs large sets (capturing exploration control).
    - WM provides a fast guess; its weight is constant across set sizes.
    - WM is "reset" toward uniform after errors and set deterministically to the chosen action after rewards.
      This captures participants clearing incorrect memory traces when feedback is negative.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of {0,1}
        Feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    model_parameters : iterable of 6 floats
        - lr: RL learning rate (0..1).
        - beta_small: RL inverse temperature for set size 3 (scaled internally).
        - beta_large: RL inverse temperature for set size 6 (scaled internally).
        - wm_weight: Weight of WM in the policy mixture (0..1).
        - wm_reset: Strength (0..1) of WM reset toward uniform after no-reward.
        - lapse: Lapse probability (0..1) that mixes in a uniform random choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_small, beta_large, wm_weight, wm_reset, lapse = model_parameters
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size-specific RL temperature
        if nS <= 3:
            softmax_beta = beta_small * 10.0
        else:
            softmax_beta = beta_large * 10.0

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
        lapse = np.clip(lapse, 0.0, 1.0)
        wm_reset = np.clip(wm_reset, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            # Lapse adds uniform noise
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If rewarded: move deterministically toward the chosen action (one-shot memory)
            # - If not rewarded: "reset" toward uniform to clear incorrect association
            if r > 0.0:
                w[s, :] = (1.0 - 1.0) * w[s, :]
                w[s, a] += 1.0
            else:
                # Reset toward uniform with strength wm_reset
                w[s, :] = (1.0 - wm_reset) * w[s, :] + wm_reset * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with surprise-gated WM and set-size-modulated RL forgetting.

    Idea:
    - RL updates with learning rate and includes value forgetting that increases with set size.
    - WM contribution to choice is dynamically gated by surprise (absolute prediction error) and reduced
      under higher set sizes.
    - WM encodes the chosen action more strongly when feedback is both rewarding and surprising.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of {0,1}
        Feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    model_parameters : iterable of 6 floats
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled internally).
        - wm_base: Baseline WM weight (0..1).
        - wm_surprise_gain: Weight (>=0) of surprise in WM gating.
        - rl_forget_base: Baseline RL forgetting (0..1) toward uniform.
        - size_sensitivity: How much larger set sizes increase forgetting and reduce WM weight (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_base, wm_surprise_gain, rl_forget_base, size_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects:
        # - RL forgetting increases with set size
        # - WM baseline weight decreases with set size
        size_factor = max(0.0, float(nS) - 3.0)
        rl_forget_eff = np.clip(rl_forget_base + size_sensitivity * size_factor, 0.0, 1.0)
        wm_base_eff = np.clip(wm_base / (1.0 + size_sensitivity * size_factor), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # Surprise = absolute prediction error before update
            delta = r - Q_s[a]
            surprise = abs(delta)

            # Surprise-gated WM weight
            wm_weight_eff = np.clip(wm_base_eff + wm_surprise_gain * surprise, 0.0, 1.0)

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL forgetting toward uniform values for the state, then standard delta update
            # Apply forgetting to the entire state-action row (decay toward uniform value)
            q[s, :] = (1.0 - rl_forget_eff) * q[s, :] + rl_forget_eff * (1.0 / nA)
            # RL update for chosen action
            q[s, a] += lr * delta

            # WM update: imprint scaled by reward and surprise
            ins = surprise * r  # only if rewarded and more surprising
            ins = np.clip(ins, 0.0, 1.0)
            w[s, :] = (1.0 - ins) * w[s, :]
            w[s, a] += ins

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects:
- Model 1: WM influence is reduced by a logistic gate as set size increases; WM also drifts more toward uniform under larger set sizes (capacity pressure).
- Model 2: RL temperature differs between set sizes (more exploration in larger sets); WM reset behavior is independent of set size, isolating RLâ€™s contribution to set-size effects.
- Model 3: Larger set sizes increase RL forgetting and reduce baseline WM weight; WM contribution on each trial is boosted by surprise, partially compensating under uncertainty.