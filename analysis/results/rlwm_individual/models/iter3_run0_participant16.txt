def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-gated WM with set-size-dependent access and decay.

    Idea:
    - RL: single learning rate and softmax choice.
    - WM: stores a peaked distribution over the last action taken in a state, with decay toward uniform.
           WM retrieval weight is gated by a sigmoid of set size (captures reduced WM contribution at larger set size).
    - Policy: mixture of RL and WM; WM retrieval itself uses a high inverse temperature.

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate for Q-updates.
    - softmax_beta: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_decay: float in [0,1]. Per-trial decay of WM traces toward uniform (higher = faster forgetting).
    - wm_gate_intercept: float. Intercept of the set-size WM gate (logit space).
    - wm_gate_slope: float. Slope of the set-size WM gate (logit space); negative values reduce WM weight with larger set size.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_decay, wm_gate_intercept, wm_gate_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM retrieval is highly deterministic
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM distribution over actions per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform baseline for decay

        # Set-size dependent WM weight (gating)
        wm_weight = sigmoid(wm_gate_intercept + wm_gate_slope * (nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then incorporate current choice evidence
            # Rewarded choices are encoded more strongly; unrewarded still leave a weak trace
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            encode_strength = 0.7 * r + 0.3 * (1 - r)  # stronger when rewarded, weaker when not
            w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * (0.85 * one_hot + 0.15 * (1.0 / nA))

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM storage.

    Idea:
    - RL: single learning rate with softmax policy and eligibility traces over state-action pairs to propagate credit
      when the same state reoccurs (eligibility accumulates on the visited pair and decays otherwise).
    - WM: stores reward-validated actions but only with probability proportional to capacity K relative to set size.
      WM retrieval is deterministic; no WM if the item was not stored. WM traces decay toward uniform each trial.
    - Policy: mixture of RL and WM with a fixed mixture weight (independent of set size), but capacity K makes WM
      less available in larger set size blocks.

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - lambda_e: float in [0,1]. Eligibility trace decay per trial (higher = more persistent traces).
    - wm_weight: float in [0,1]. Mixture weight on WM vs RL when WM is available.
    - K_slots: float in [0,6]. Effective WM capacity; probability of storing an item is min(1, K_slots / set_size).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta_rl, lambda_e, wm_weight, K_slots = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 30.0
    eps = 1e-12
    rng = np.random.RandomState(0)  # deterministic pseudo-randomness for storage decisions in likelihood computation

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        # Probability to store a rewarded association in WM
        p_store = min(1.0, K_slots / max(1, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # If WM is near-uniform (i.e., not stored), its contribution is effectively uniform.
            # We still mix by wm_weight to reflect retrieval attempts.
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL with eligibility traces
            # Update eligibility: decay all, increment the visited pair
            e *= lambda_e
            e[s, :] *= 0.0
            e[s, a] = 1.0
            delta = r - Q_s[a]
            q += lr * delta * e  # credit assignment via eligibility

            # WM decay toward baseline
            w = (1.0 - 0.1) * w + 0.1 * w_0  # small passive decay across all states

            # WM storage on reward with capacity limit (stochastic, but seeded for determinism)
            if r > 0:
                if rng.rand() < p_store:
                    one_hot = np.zeros(nA)
                    one_hot[a] = 1.0
                    w[s, :] = 0.9 * one_hot + 0.1 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + error-driven WM with power-law set-size modulation of WM weight.

    Idea:
    - RL: single learning rate and softmax policy.
    - WM: rapidly encodes rewarded actions and actively "inhibits" recently unrewarded actions by shifting probability
      mass away from the chosen action. WM retrieval has its own temperature.
    - Set-size effect: WM mixture weight scales as c / (set_size^gamma), capped to [0,1], producing less WM reliance
      in larger set sizes.

    Parameters (tuple/list):
    - lr: float in [0,1]. RL learning rate.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - alpha_wm: float in [0,1]. WM plasticity; fraction of the current WM state replaced by new evidence per trial.
    - c_wm: float >= 0. Base WM weight constant before set-size scaling.
    - gamma_size: float >= 0. Exponent controlling how WM weight declines with set size.
    - beta_wm: float >= 0. WM inverse temperature controlling the sharpness of WM policy.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta_rl, alpha_wm, c_wm, gamma_size, beta_wm = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = max(1e-6, beta_wm)  # ensure non-negative
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM weight via power law
        wm_weight = min(1.0, max(0.0, c_wm / (max(1, nS) ** max(0.0, gamma_size))))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Error-driven WM update:
            # - If rewarded: move WM toward chosen action.
            # - If not rewarded: move WM away from chosen action toward the other two actions (equal mass).
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0:
                target = 0.9 * one_hot + 0.1 * (1.0 / nA)
            else:
                avoid = np.ones(nA) / (nA - 1)
                avoid[a] = 0.0  # puts zero on the chosen (erroneous) action
                target = 0.9 * avoid + 0.1 * (1.0 / nA)

            # Include mild drift toward baseline to prevent saturation
            w[s, :] = (1.0 - alpha_wm) * (0.9 * w[s, :] + 0.1 * w_0[s, :]) + alpha_wm * target

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects:
- Model 1: WM mixture weight is a logistic function of set size (parameters wm_gate_intercept and wm_gate_slope). Negative slope yields less WM influence at set size 6 vs 3.
- Model 2: WM effectiveness is reduced at larger set sizes through a lower probability of storage p_store = K_slots / set_size (capped at 1). Thus, even with a fixed mixture weight, WM contributes less often in larger sets.
- Model 3: WM mixture weight follows a power-law decay with set size, wm_weight = c_wm / set_size^gamma, directly capturing reduced WM influence in the larger set.