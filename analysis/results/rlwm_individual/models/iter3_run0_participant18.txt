def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Capacity-limited WM with set-size-dependent forgetting and reward-driven strengthening
    - Arbitration: Mixture of WM and RL policies. The base mixture parameter (wm_weight) is reduced
      as set size increases via a linear penalty on the logit scale (converted back to 0..1).
    - RL: Standard delta-rule with a single learning rate (lr) and softmax choice rule (softmax_beta).
    - WM store: A value-like table W that
        (a) decays toward uniform each trial with a set-size-dependent rate,
        (b) is strengthened toward a one-hot code for the chosen action on rewarded trials.
      No additional push on non-reward trials beyond decay.
    - Set-size effect: Two levers
        1) mixture arbitration: larger set size reduces the WM mixture weight,
        2) memory maintenance: larger set size increases WM decay.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight in [0,1] when set size = 3
    - softmax_beta: RL inverse temperature; scaled internally by 10 to allow a wide range
    - wm_decay_base: Baseline WM decay toward uniform at set size = 3 (0..1)
    - decay_slope: Additional decay per extra item beyond 3 (>=0). Effective decay is
                   clip(wm_decay_base + decay_slope * max(0, nS - 3), 0, 1)
    - wm_boost: Reward-driven WM learning rate toward one-hot on rewarded trials (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, decay_slope, wm_boost = model_parameters

    softmax_beta *= 10.0  # RL temperature scaling
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent mixture: shrink WM weight with larger sets on the logit scale
        def inv_logit(x):
            return 1.0 / (1.0 + np.exp(-x))

        def logit(p):
            p = np.clip(p, 1e-6, 1 - 1e-6)
            return np.log(p) - np.log(1 - p)

        # Effective decay for this block
        decay_eff = np.clip(wm_decay_base + decay_slope * max(0, nS - 3), 0.0, 1.0)
        # Effective WM mixture for this block
        wm_weight_block = inv_logit(logit(wm_weight) - decay_slope * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (softmax over W)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            wm_mix = wm_weight_block
            p_total = p_wm * wm_mix + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: global decay toward uniform, plus reward-driven boost toward one-hot
            # Decay
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]
            # Reward-driven strengthening
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_boost) * w[s, :] + wm_boost * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Entropy-weighted WM arbitration with WM learning and leak
    - Arbitration: Mixture weight decreases with WM uncertainty at the queried state.
      Specifically, wm_weight is scaled by a logistic transform of the negative entropy of W(s).
      As the WM distribution becomes flatter (higher entropy), WM is trusted less.
      Larger set sizes tend to induce higher entropy, thereby implicitly lowering WM usage.
    - RL: Single learning rate (lr) and softmax choice rule (softmax_beta).
    - WM store: Delta-rule toward one-hot on reward and toward uniform on no-reward (wm_alpha),
      with a small uniform leak/decay each trial (wm_decay).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight in [0,1] when WM is certain (low entropy)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - entropy_slope: Positive slope controlling how strongly entropy reduces WM influence
    - wm_alpha: WM learning rate toward target distribution (0..1). On reward=1: toward one-hot,
                on reward=0: toward uniform.
    - wm_decay: Additional WM decay toward uniform at every trial (0..1)

    Set-size note:
    - Larger set sizes generally increase entropy of W(s), which in turn reduces the effective
      WM mixture weight via entropy_slope (no explicit set-size parameter is required here).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, entropy_slope, wm_alpha, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Entropy of WM at state s
            P = np.clip(W_s / max(np.sum(W_s), 1e-12), 1e-12, 1.0)
            P = P / np.sum(P)
            entropy = -np.sum(P * np.log(P))  # in nats; max ~ ln(nA)

            # Map entropy to an adjustment on the logit scale of wm_weight
            wm_weight_eff = inv_logit(logit(wm_weight) - entropy_slope * entropy)

            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: small leak + target toward one-hot (reward) or uniform (no reward)
            # Leak toward uniform (applies regardless of reward)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Meta-learned WM engagement with set-size penalty and WM forgetting
    - Arbitration: A per-state confidence variable c(s) is updated from experience and used to
      gate WM usage. The base wm_weight is adjusted by +c(s) (promoting WM when it proves useful)
      and penalized by set size. Effective weight is logistic(logit(wm_weight) + c(s) - set_penalty * max(0, nS-3)).
    - RL: Standard delta rule with single learning rate and softmax policy.
    - WM store: Episodic-like strengthening on rewards (toward one-hot), and forgetting toward uniform
      with parameter wm_forget each time the state is visited. Unrewarded trials rely on forgetting only.
    - Set-size effect: Larger set sizes explicitly reduce WM engagement via set_penalty and indirectly
      degrade performance via WM forgetting that interacts with s visit frequency.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight in [0,1] at set size = 3 and zero confidence
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - meta_lr: Meta-learning rate for confidence c(s) (0..1). c increases when WM's predicted
               action aligns with reward, and decays otherwise.
    - wm_forget: WM forgetting toward uniform on each visit (0..1)
    - set_penalty: Linear penalty applied per additional item beyond 3 to the WM mixture logit (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, meta_lr, wm_forget, set_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Per-state meta-confidence for WM engagement
        c = np.zeros(nS)

        # Precompute set-size penalty on logit scale
        ss_pen = set_penalty * max(0, nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Confidence-adjusted and set-size-penalized WM mixture weight
            wm_weight_eff = inv_logit(logit(wm_weight) + c[s] - ss_pen)

            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting toward uniform on each visit
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Reward-based episodic strengthening
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # One-shot-like push (use wm_forget as the complementary factor to avoid extra params):
                alpha_wm = min(1.0, 2.0 * wm_forget)  # tie strength to forgetting scale to ensure use of all params
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

            # Meta-confidence update: compare WM's selected action probability to outcome
            # Use W_s[a] as a proxy for WM's confidence in the executed action.
            wm_conf = np.clip(W_s[a], 0.0, 1.0)
            # Target meta-signal: higher when reward is obtained and WM supported that choice
            target_c = (2.0 * r - 1.0) * (wm_conf - 1.0 / nA)  # centered around 0
            c[s] = (1.0 - meta_lr) * c[s] + meta_lr * target_c

        blocks_log_p += log_p

    return -blocks_log_p