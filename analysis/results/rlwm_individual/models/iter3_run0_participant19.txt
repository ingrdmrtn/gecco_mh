Below are three alternative cognitive models tailored to the RLâ€“WM task. Each returns the negative log-likelihood of the observed choices. They all follow the provided template logic, filling in the WM policy and WM updating while preserving the RL softmax and TD update structure shown.

Note: Assume numpy as np is already imported in your environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM gating.

    Idea:
    - RL uses TD learning with replacing eligibility traces to propagate prediction errors within a block.
    - WM behaves like a capacity-limited, near-deterministic store of recently reinforced mappings.
      Arbitration weight scales down when set size exceeds WM slot capacity.
    - WM decays toward uniform within a block unless refreshed by reward on the current state.

    Parameters:
    - lr: RL learning rate (0..1)
    - lam: eligibility trace decay (0..1), controls how widely TD error spreads over past state-actions
    - wm_weight_base: base arbitration weight mapped through sigmoid; scaled by capacity factor (0..1)
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wm_slot_capacity: effective number of state-action pairs WM can support per block (>=1)
    - wm_refresh: WM decay rate toward uniform per trial (0..1); larger -> faster forgetting

    Set-size impact:
    - WM arbitration weight is reduced by a factor min(1, wm_slot_capacity / set_size), capturing limited WM slots.
    """
    lr, lam, wm_weight_base, softmax_beta, wm_slot_capacity, wm_refresh = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility traces
        e = np.zeros((nS, nA))

        # Capacity-scaled WM arbitration weight (constant within block)
        wm_base = 1.0 / (1.0 + np.exp(-wm_weight_base))
        capacity_factor = min(1.0, wm_slot_capacity / max(1.0, float(nS)))
        wm_w = wm_base * capacity_factor

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (provided form)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (near-deterministic softmax over WM map)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces (replacing traces)
            pe = r - q[s, a]
            e *= lam
            e[s, :] *= 0  # clear other actions for replacing trace
            e[s, a] = 1.0
            q += lr * pe * e

            # WM decay towards uniform then refresh if reward is informative
            w = (1 - wm_refresh) * w + wm_refresh * w_0
            if r > 0:
                # Store reinforced action as the WM mapping for this state
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + uncertainty-weighted WM arbitration + interference-driven WM decay.

    Idea:
    - RL values drift toward uniform each trial (forgetting), to capture interference across larger sets.
    - WM is a near-deterministic store of last reinforced action per state, subject to interference-based decay.
    - Arbitration weight depends on current WM certainty (sharper W_s -> more WM), transformed by a bias.

    Parameters:
    - alpha: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wm_confidence_temp: sensitivity to WM certainty (larger -> stronger reliance when W_s is sharp)
    - rl_forget: RL forgetting rate toward uniform per trial (0..1)
    - interference_rate: increases WM decay with set size; larger set sizes produce stronger decay
    - wm_reliability_bias: bias term (in logit) for WM arbitration (positive -> more WM, negative -> more RL)

    Set-size impact:
    - WM decay rate per trial is decay = 1 - exp(-interference_rate * nS): larger nS => faster WM interference.
    - RL forgetting applies uniformly (indirectly impacting larger sets due to less frequent repetition).
    """
    alpha, softmax_beta, wm_confidence_temp, rl_forget, interference_rate, wm_reliability_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM interference-driven decay per trial (constant within block)
        wm_decay = 1.0 - np.exp(-interference_rate * max(1.0, float(nS)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL forgetting toward uniform prior
            q = (1 - rl_forget) * q + rl_forget * (1.0 / nA) * np.ones_like(q)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence from sharpness of W_s (use max probability minus chance)
            maxp = np.max(W_s)
            conf_signal = maxp - (1.0 / nA)

            # Arbitration weight: logistic of bias + temperature * confidence
            wm_logit = wm_reliability_bias + wm_confidence_temp * conf_signal
            wm_w = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM interference decay then update on rewarded trials
            w = (1 - wm_decay) * w + wm_decay * w_0
            if r > 0:
                # Commit near-deterministic mapping for this state
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Meta-control: WM vs RL arbitration driven by performance feedback and set size + lapse.

    Idea:
    - RL uses standard TD learning.
    - WM is a fast, near-deterministic cache updated by positive feedback and decays with set size.
    - Arbitration to WM increases with recent performance for that state and decreases with larger set size.
    - Includes a lapse parameter that mixes in uniform random choice.

    Parameters:
    - alpha: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wm_base: base arbitration logit favoring WM (>0) or RL (<0)
    - error_gain: weight translating recent performance signal into WM logit (positive => rely more on WM when doing well)
    - wm_size_penalty: penalty per unit set size in WM logit and WM decay (larger -> weaker WM in larger sets)
    - lapse: probability of a uniform-random lapse (0..1), applied after WM/RL mixture

    Set-size impact:
    - WM arbitration logit includes -wm_size_penalty * (nS - 3).
    - WM decay toward uniform increases with set size: decay = 1 - exp(-wm_size_penalty * nS).
    - Larger sets reduce WM weight and accelerate WM forgetting.
    """
    alpha, softmax_beta, wm_base, error_gain, wm_size_penalty, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track recent performance per state (EWMA of reward)
        perf = 0.5 * np.ones(nS)  # initialized at chance-correctness

        # WM decay depends on set size
        wm_decay = 1.0 - np.exp(-wm_size_penalty * max(1.0, float(nS)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: logistic of base + performance - size penalty
            wm_logit = wm_base + error_gain * (perf[s] - 0.5) - wm_size_penalty * (nS - 3.0)
            wm_w = 1.0 / (1.0 + np.exp(-wm_logit))

            # Mixture then lapse
            p_mix = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM decay and reward-based update
            w = (1 - wm_decay) * w + wm_decay * w_0
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

            # Update recent performance trace (EWMA with fixed smoothing)
            perf[s] = 0.7 * perf[s] + 0.3 * r

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size influence:
- Model 1: WM arbitration weight is scaled by a slots-per-set factor wm_slot_capacity / nS.
- Model 2: WM decay increases with set size via an interference process; arbitration depends on WM certainty.
- Model 3: Both arbitration weight and WM decay penalized by set size; also includes a lapse process.