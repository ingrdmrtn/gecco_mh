def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall associability and WM with capacity gating by set size.

    Idea:
    - RL: Rescorla-Wagner with dynamic, stimulus-action-specific associability (Pearce-Hall).
      Associability tracks unsigned prediction error and gates the effective learning rate.
    - WM: one-shot storage on rewarded trials; no overwrite on errors.
    - Set-size effect: WM mixture weight is capacity-gated by K/nS (capped at 1).

    Parameters (6 total):
    - lr0: [0,1], base RL learning rate before associability scaling.
    - wm_weight: [0,1], base WM mixture weight (further gated by capacity).
    - softmax_beta: >0, RL inverse temperature (internally scaled by 10).
    - kappa_ph: [0,1], associability learning rate; larger -> faster updates of associability.
    - K: >=0, WM capacity in number of items; scales wm_weight by min(1, K/nS).
    - phi0: >=0, initial associability value for all state-action pairs.

    Set size effect:
    - Mixture: wm_mix = wm_weight * min(1, K/nS), thus larger sets reduce WM influence.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr0, wm_weight, softmax_beta, kappa_ph, K, phi0 = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Pearce-Hall associability per state-action
        phi = np.full((nS, nA), float(phi0))

        # Capacity-gated WM mixture
        cap_gate = min(1.0, float(K) / max(nS, 1))
        wm_mix_base = np.clip(wm_weight, 0.0, 1.0) * cap_gate

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax over WM weights)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall associability-gated learning rate
            delta = r - Q_s[a]
            lr_eff = lr0 * max(phi[s, a], 0.0)
            q[s, a] += lr_eff * delta
            # Update associability toward |PE|
            phi[s, a] = (1.0 - kappa_ph) * phi[s, a] + kappa_ph * abs(delta)

            # WM update: one-shot storage on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with intra-state credit spread and WM with set-size-dependent misbinding (swap errors).

    Idea:
    - RL: standard Rescorla-Wagner for chosen action, plus suppression (credit spread) on
      non-chosen actions within the same state when prediction error is positive.
    - WM: on rewarded trials, memory is updated (blended overwrite). At choice time, WM
      may misbind the remembered action to a wrong state with probability increasing
      with set size (swap errors). Likelihood mixes the correct-state WM distribution with
      the average of other-state WM distributions.

    Parameters (6 total):
    - lr: [0,1], RL learning rate for chosen action.
    - wm_weight: [0,1], base WM mixture weight.
    - softmax_beta: >0, RL inverse temperature (internally scaled by 10).
    - xi: >=0, credit-spread strength; when PE>0, non-chosen actions are suppressed by xi*lr*PE.
    - rho_swap: >=0, controls growth of WM misbinding with set size: p_swap = 1 - exp(-rho_swap*(nS-1)).
    - wm_blend: [0,1], blending factor for WM storage on reward: w <- (1-wm_blend)*w + wm_blend*onehot.

    Set size effects:
    - WM misbinding increases with set size via p_swap = 1 - exp(-rho_swap*(nS-1)),
      reducing the effective precision of WM in larger sets.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, xi, rho_swap, wm_blend = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent swap probability
        p_swap = 1.0 - np.exp(-max(rho_swap, 0.0) * max(nS - 1, 0))

        wm_mix_base = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with misbinding: mixture of correct-state WM and average other-state WM
            W_s = w[s, :]
            pw_correct = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            if nS > 1:
                other_states = [j for j in range(nS) if j != s]
                pw_others = 0.0
                for j in other_states:
                    W_j = w[j, :]
                    pw_others += 1.0 / np.sum(np.exp(softmax_beta_wm * (W_j - W_j[a])))
                pw_others /= (nS - 1)
                p_wm = (1.0 - p_swap) * pw_correct + p_swap * pw_others
            else:
                p_wm = pw_correct

            p_total = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL updates: chosen action
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Credit spread: suppress alternatives within state when PE positive
            if delta > 0.0 and xi > 0.0:
                spread = xi * lr * delta
                for a_alt in range(nA):
                    if a_alt != a:
                        q[s, a_alt] -= spread / (nA - 1)

            # WM update: blended overwrite on reward
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_blend) * w[s, :] + wm_blend * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value forgetting and WM with error-driven maintenance plus surprise-gated mixture.

    Idea:
    - RL: standard Rescorla-Wagner with per-visit forgetting toward uniform on the active state.
    - WM: one-shot overwrite on reward; on errors, WM drifts back toward prior (w0) with leak.
    - Mixture: WM weight is scaled both by set size (3/nS) and by surprise (unsigned PE)
      via a sigmoid gate, emphasizing WM under high surprise.

    Parameters (6 total):
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], base WM mixture weight before gates.
    - softmax_beta: >0, RL inverse temperature (internally scaled by 10).
    - forget: [0,1], RL forgetting rate toward uniform for the current state's action values.
    - wm_leak: [0,1], WM leak toward prior on error trials.
    - pe_gain: >=0, gain of surprise gating on mixture: gate = sigmoid(pe_gain * |PE|).

    Set size effects:
    - WM mixture is down-weighted by (3/nS), reducing WM contribution at set size 6.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, forget, wm_leak, pe_gain = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set size scaling for WM mixture
        ss_gate = 3.0 / max(nS, 1)
        wm_mix_base = np.clip(wm_weight, 0.0, 1.0) * ss_gate

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL forgetting toward uniform for current state
            q[s, :] = (1.0 - forget) * q[s, :] + forget * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise (unsigned PE) for mixture gating (computed from current Q)
            delta = r - Q_s[a]
            gate_surprise = 1.0 / (1.0 + np.exp(-max(pe_gain, 0.0) * abs(delta)))
            wm_mix = np.clip(wm_mix_base * gate_surprise, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM maintenance: reward -> overwrite; error -> leak toward prior
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p