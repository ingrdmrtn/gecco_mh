def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-specific arbitration and WM confusion under high load.

    Policy:
    - RL system: softmax over Q-values with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with deterministic temperature (50).
      Under high set size (6), WM is noisy: a confusion probability mixes WM with uniform.
    - Arbitration: wm_weight depends on set size (wm_weight3 for size=3, wm_weight6 for size=6).

    WM mechanisms:
    - Passive decay toward prior template w_0 at every trial with rate wm_decay.
    - Rewarded strengthening using RL learning rate lr (Hebbian towards one-hot), and slight suppression on errors.

    Parameters:
    - lr: RL learning rate for Q updates; also controls WM strengthening.
    - wm_weight3: Arbitration weight of WM for set size 3 blocks.
    - wm_weight6: Arbitration weight of WM for set size 6 blocks.
    - softmax_beta: RL inverse temperature (rescaled by 10 inside the function).
    - wm_decay: WM passive decay rate toward uniform template per trial (0..1).
    - wm_confusion: Additional WM confusion probability used only at set size 6 (0..1),
      mixing WM policy with uniform: p_wm = (1 - wm_confusion) * softmax(W) + wm_confusion * uniform.

    Set-size effect:
    - Directly via wm_weight3 vs wm_weight6 arbitration.
    - Additional WM confusion penalty (wm_confusion) only when nS=6.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight3, wm_weight6, softmax_beta, wm_decay, wm_confusion = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration weight by set size
        this_wm_weight = wm_weight3 if nS == 3 else wm_weight6
        # Confusion applies only under high load
        confusion = 0.0 if nS == 3 else max(0.0, min(1.0, wm_confusion))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (base)
            p_wm_base = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # WM confusion under load: mix with uniform
            p_wm = (1.0 - confusion) * p_wm_base + confusion * (1.0 / nA)

            # Arbitration
            p_total = this_wm_weight * p_wm + (1.0 - this_wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive decay toward prior
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: reward-based Hebbian and mild suppression on errors
            if r > 0.0:
                w[s, a] += lr * (1.0 - w[s, a])
                # Normalize row softly into [0,1]
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)
            else:
                # On error, slightly depress the chosen association
                w[s, a] -= 0.5 * lr * w[s, a]
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-modulated learning + WM with limited lifespan and retrieval failures at size 6.

    Policy:
    - RL system: softmax over Q-values with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with deterministic temperature (50).
      Retrieval failure at set size 6: mix WM with uniform by wm_fail6.
    - Arbitration: fixed wm_weight.

    RL mechanism:
    - Trial-wise learning rate: alpha_t = clip(lr_base + surprise_gain * |r - Q(s,a)|, 0, 1).

    WM mechanisms:
    - Episodic-like storage on reward: move row W[s,:] toward a one-hot at a.
    - Limited lifespan: exponential decay toward uniform each trial with rate d = 1 - exp(-1/wm_lifespan).
      Smaller wm_lifespan => faster decay. wm_lifespan > 0.
    - Retrieval failure under load nS=6: p_wm = (1 - wm_fail6) * softmax(W) + wm_fail6 * uniform.

    Parameters:
    - lr_base: baseline RL learning rate.
    - wm_weight: arbitration weight for WM.
    - softmax_beta: RL inverse temperature (rescaled by 10 inside the function).
    - surprise_gain: scales learning-rate boost by absolute prediction error.
    - wm_lifespan: controls WM decay speed (positive). Higher => slower decay.
    - wm_fail6: WM retrieval failure probability at set size 6 (0..1).

    Set-size effect:
    - WM retrieval failure penalty (wm_fail6) only for nS=6, reducing WM policy precision.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_base, wm_weight, softmax_beta, surprise_gain, wm_lifespan, wm_fail6 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    wm_lifespan = max(1e-6, wm_lifespan)  # avoid zero/negative
    # Per-trial decay rate derived from lifespan
    decay_rate = 1.0 - np.exp(-1.0 / wm_lifespan)
    decay_rate = max(0.0, min(1.0, decay_rate))
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Retrieval failure at size 6 only
        fail = max(0.0, min(1.0, wm_fail6)) if nS == 6 else 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with retrieval failure
            p_wm_base = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - fail) * p_wm_base + fail * (1.0 / nA)

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update with surprise-modulated learning rate
            pe = r - Q_s[a]
            alpha_t = lr_base + surprise_gain * abs(pe)
            alpha_t = max(0.0, min(1.0, alpha_t))
            q[s, a] += alpha_t * pe

            # WM passive decay (lifespan-based)
            w = (1.0 - decay_rate) * w + decay_rate * w_0

            # WM update: one-shot consolidation on reward with strength tied to decay_rate
            if r > 0.0:
                k = min(1.0, 3.0 * decay_rate)  # stronger when memory is volatile
                # Move row toward one-hot at chosen action
                w[s, :] = (1.0 - k) * w[s, :]
                w[s, a] += k
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and set-size-dependent exploration + WM with choice stickiness.

    Policy:
    - RL system: softmax over Q-values with inverse temperature beta_rl.
      beta_rl is reduced under set size 6 by multiplicative factor (1 - beta_drop6).
    - WM system: softmax over W with deterministic temperature (50).
    - Both systems include a stickiness bias toward the last chosen action in the same state.
    - Arbitration: fixed wm_weight.

    RL mechanism:
    - Asymmetric learning rates: lr_pos for rewards (r=1), lr_neg for non-rewards (r=0).

    WM mechanisms:
    - Reward-based strengthening using lr_pos; error-based weakening using lr_neg.
    - Mild leak toward uniform each trial scaled by lr_neg (captures interference/forgetting).

    Parameters:
    - lr_pos: RL learning rate on rewarded trials.
    - lr_neg: RL learning rate on non-rewarded trials (also scales WM leak).
    - wm_weight: arbitration weight for WM.
    - softmax_beta_base: base RL inverse temperature (rescaled by 10 inside the function).
    - beta_drop6: fractional drop of RL beta at set size 6 (0..1).
    - stickiness: bias added to the last chosen action in state for both RL and WM policies.

    Set-size effect:
    - RL exploration increases for nS=6 via beta_rl = softmax_beta_base * (1 - beta_drop6).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta_base, beta_drop6, stickiness = model_parameters
    # RL beta with scaling and potential drop at set size 6
    softmax_beta_base *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # per-state last chosen action

        # RL inverse temperature adjusted by set size
        if nS == 6:
            beta_rl = softmax_beta_base * max(0.0, 1.0 - beta_drop6)
        else:
            beta_rl = softmax_beta_base

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add stickiness bias to both policies for previous action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
                W_s[last_action[s]] += stickiness

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr_pos if r > 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM leak toward uniform scaled by lr_neg (captures interference)
            leak = 0.1 * max(0.0, min(1.0, lr_neg))
            w = (1.0 - leak) * w + leak * w_0

            # WM update mirroring asymmetry: strengthen on reward, weaken on error
            if r > 0.0:
                w[s, a] += lr_pos * (1.0 - w[s, a])
            else:
                w[s, a] -= lr_neg * w[s, a]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p