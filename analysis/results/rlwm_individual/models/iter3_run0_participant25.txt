def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with asymmetric RL learning and WM forgetting.

    Mechanism:
    - RL: Q-learning with separate learning rates for positive and negative feedback.
    - WM: Stores rewarded action as a one-hot distribution per state. Between visits to a state,
      WM decays toward uniform at a per-trial forgetting rate f.
    - Capacity-limited gating: The effective WM weight is downscaled by the fraction of items
      that fit into capacity K: wm_weight_eff = wm_weight * min(1, K / nS).
      Thus WM contributes more when set size is small relative to capacity.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate after reward = 1 (0-1)
    - alpha_neg: RL learning rate after reward = 0 (0-1)
    - wm_weight: Base mixture weight on WM policy before capacity gating (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - K: WM capacity in slots; determines set-size gating via min(1, K/nS) (>=0)
    - f: WM forgetting rate per intervening trial for a given state (0-1); higher = faster decay

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, K, f = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last visit time per state for decay computation
        last_visit = -np.ones(nS, dtype=int)

        # Capacity-limited WM gating
        cap_gate = min(1.0, float(K) / max(1, nS))
        wm_weight_eff = wm_weight * cap_gate

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Time-based WM decay toward uniform for the current state
            if last_visit[s] >= 0:
                isi = t - last_visit[s]
                retention = (1.0 - f) ** isi
                w[s, :] = retention * w[s, :] + (1.0 - retention) * w_0[s, :]
            last_visit[s] = t

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy via near-deterministic softmax on WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # Asymmetric RL learning
            if r > 0.5:
                lr = alpha_pos
            else:
                lr = alpha_neg
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating: encode rewarded action as one-hot; otherwise just keep decay
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target  # strong one-shot encoding on reward

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with state-specific forgetting + WM with bidirectional error-correcting updates.
    
    Mechanism:
    - RL: Standard Q-learning with single learning rate, but Q-values decay toward uniform
      between visits to a state. Decay rate increases with set size: 
      kappa_eff = kappa0 * (nS/3)^gamma.
    - WM: On reward, encode one-hot toward the chosen action with strength eta_pos.
      On no reward, reduce probability on chosen action by eta_neg and redistribute the
      removed mass equally to the other actions (error-correcting).
    - Mixture: Fixed wm_weight blends WM and RL policies.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Mixture weight on WM policy (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - kappa0: Base RL forgetting rate per intervening trial (0-1)
    - gamma: Set-size sensitivity of forgetting; higher gamma -> stronger forgetting in larger sets
    - eta_neg: WM negative update strength on unrewarded trials (0-1); eta_pos = 1 - eta_neg

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa0, gamma, eta_neg = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective RL forgetting rate scaling with set size
        kappa_eff = np.clip(kappa0 * (max(1, nS) / 3.0) ** gamma, 0.0, 1.0)
        eta_pos = max(0.0, min(1.0, 1.0 - eta_neg))

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_visit = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL decay toward prior for this state based on ISI and kappa_eff
            if last_visit[s] >= 0:
                isi = t - last_visit[s]
                retention = (1.0 - kappa_eff) ** isi
                q[s, :] = retention * q[s, :] + (1.0 - retention) * (1.0 / nA)
            last_visit[s] = t

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy via softmax on WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: bidirectional
            if r > 0.5:
                # Positive update toward one-hot
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - eta_pos) * w[s, :] + eta_pos * target
            else:
                # Negative update: push mass away from chosen action and redistribute
                reduce = eta_neg * w[s, a]
                w[s, a] -= reduce
                w[s, :] += reduce / (nA - 1.0)
                w[s, a] -= reduce / (nA - 1.0)  # remove the part that was added back to chosen
                # Numerical safety and renormalize
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence- and set-sizeâ€“based arbitration.

    Mechanism:
    - RL: Standard Q-learning with single learning rate.
    - WM: Decays exponentially toward uniform with per-trial retention lam_wm.
    - Arbitration: WM contribution is dynamically scaled by
        wm_weight_eff = wm_weight * sigmoid(a_set * (3.5 - nS)) * sigmoid(conf / tau_conf),
      where conf = max(W_s) - second_max(W_s) measures WM confidence, and the set-size term
      downweights WM as nS increases (if a_set > 0).
    - Mixture: Trial-wise wm_weight_eff blends WM and RL policies.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base mixture weight (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - a_set: Set-size arbitration slope; positive values reduce WM weight as set size grows
    - tau_conf: Confidence temperature (>0); larger values flatten the confidence effect
    - lam_wm: WM retention per intervening trial for a given state (0-1); higher = slower decay

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, a_set, tau_conf, lam_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_visit = -np.ones(nS, dtype=int)

        # Set-size arbitration component (constant within a block)
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))
        set_gate = sigmoid(a_set * (3.5 - nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # WM decay toward prior for this state (time since last visit)
            if last_visit[s] >= 0:
                isi = t - last_visit[s]
                decay = lam_wm ** isi
                w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]
            last_visit[s] = t

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy and confidence
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Confidence as margin between top two WM probabilities
            sorted_W = np.sort(W_s)[::-1]
            if len(sorted_W) >= 2:
                conf = max(0.0, sorted_W[0] - sorted_W[1])
            else:
                conf = 0.0
            conf_gate = sigmoid(conf / max(1e-6, tau_conf))

            wm_weight_eff = wm_weight * set_gate * conf_gate

            p_total = wm_weight_eff*p_wm + (1-wm_weight_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating: encode rewarded action toward one-hot modestly
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use a small, outcome-driven consolidation proportional to confidence complement
                eta = 0.5  # fixed moderate consolidation
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target

        blocks_log_p += log_p

    return -blocks_log_p