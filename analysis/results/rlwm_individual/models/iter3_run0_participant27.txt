def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM mixture with set-size–scaled arbitration and WM forgetting.

    Mechanism
    - RL: single learning rate; softmax choice (line kept from template).
    - WM: state-action table w moves toward one-hot for rewarded actions and toward uniform for non-rewarded actions.
           WM additionally forgets toward uniform on each visit.
    - Arbitration: the WM weight is scaled by a capacity term that degrades with set size.

    Set-size effects
    - The effective WM contribution is down-weighted when set size exceeds capacity:
      wm_weight_eff = wm_weight * (min(1, capacity / nS) ** gamma_mix).
      Thus, WM contributes more in small sets (nS=3) than in large sets (nS=6).

    Parameters
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, capacity, gamma_mix, wm_forget)
        - lr: RL learning rate (0..1); also used for WM encoding strength.
        - wm_weight: Base mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - capacity: Effective WM capacity in number of states (>0).
        - gamma_mix: Exponent controlling how strongly capacity scales arbitration (>=0).
        - wm_forget: Per-visit WM forgetting rate toward uniform (0..1); stronger forgetting harms WM especially at larger set sizes.
    """
    lr, wm_weight, softmax_beta, capacity, gamma_mix, wm_forget = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–scaled WM weight via capacity
        cap_scale = min(1.0, max(1e-6, float(capacity)) / float(nS))
        wm_weight_eff = wm_weight * (cap_scale ** max(0.0, gamma_mix))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over WM values (deterministic table lookup)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: move toward one-hot when rewarded, uniform otherwise; then apply forgetting toward uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Per-visit forgetting toward uniform (capacity limits)
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Normalize row to avoid drift
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and set-size–scaled prior WM bias.

    Mechanism
    - RL: single learning rate; softmax choice (from template).
    - WM: tabular values updated with a separate WM learning rate toward one-hot when rewarded,
           and toward uniform when not rewarded.
    - Arbitration: mixture weight is dynamic via a logistic function of uncertainty difference:
           wm_weight_eff = sigmoid(b_eff + kappa * (H_rl - H_wm)),
      where H_rl and H_wm are entropies of the RL and WM policies. A set-size term modifies the bias b_eff.

    Set-size effects
    - b_eff = logit(wm_weight0) + gamma_size * log(3 / nS): larger sets reduce the effective WM prior,
      smaller sets enhance it, shifting arbitration toward RL when nS increases.

    Parameters
    model_parameters : tuple
        (lr, wm_weight0, softmax_beta, kappa, wm_eta, gamma_size)
        - lr: RL learning rate (0..1).
        - wm_weight0: Base prior weight on WM (0..1); transformed to logit for arbitration bias.
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - kappa: Sensitivity of arbitration to entropy difference (>=0).
        - wm_eta: WM learning rate (0..1).
        - gamma_size: Strength of set-size scaling of WM prior (can be positive or negative; typical >=0).
    """
    lr, wm_weight0, softmax_beta, kappa, wm_eta, gamma_size = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–scaled bias for WM arbitration
        wm_weight0 = np.clip(wm_weight0, 1e-6, 1 - 1e-6)
        b0 = np.log(wm_weight0 / (1 - wm_weight0))
        b_eff = b0 + gamma_size * np.log(3.0 / float(nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Full policies for entropy computation
            # RL policy
            z_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = z_rl / np.sum(z_rl)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            # WM policy
            z_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = z_wm / np.sum(z_wm)
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, 1e-12, 1.0)))

            # Entropy-based arbitration
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(b_eff + max(0.0, kappa) * (H_rl - H_wm))))
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update with its own learning rate; reward -> one-hot, no reward -> uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

            # Normalize row to avoid numerical drift
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–dependent WM interference and lapse.

    Mechanism
    - RL: single learning rate; softmax choice (from template).
    - WM: updated toward one-hot on reward, and away from the chosen action on non-reward
           (redistributes probability to the non-chosen actions).
    - Interference: larger set sizes induce additional WM flattening (confusability), mixing WM with uniform.
    - Lapse: mixture policy is contaminated by a small uniform lapse probability.

    Set-size effects
    - Interference strength scales with set size: xi_eff = xi_interf * ((nS - 3) / 3), so zero at nS=3 and
      increasing toward nS=6. This weakens WM in larger sets.

    Parameters
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, xi_interf, recency, eps_lapse)
        - lr: RL learning rate (0..1).
        - wm_weight: Fixed mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - xi_interf: Base WM interference coefficient (0..1), scaled up by set size as above.
        - recency: WM learning rate (0..1) controlling how strongly WM shifts toward the target.
        - eps_lapse: Lapse probability that chooses uniformly random among actions (0..1).
    """
    lr, wm_weight, softmax_beta, xi_interf, recency, eps_lapse = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–dependent interference scaling
        xi_eff = np.clip(xi_interf * max(0.0, (float(nS) - 3.0) / 3.0), 0.0, 1.0)
        eps_lapse = np.clip(eps_lapse, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax from WM table
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture and lapse
            p_mix = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = (1.0 - eps_lapse) * p_mix + eps_lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update with recency and interference
            if r > 0.0:
                # Reward: move toward one-hot of chosen action
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # No reward: move away from chosen action (redistribute to other actions)
                target = np.ones(nA)
                target[a] = 0.0
                target = target / np.sum(target)

            # Primary WM learning step
            w[s, :] = (1.0 - recency) * w[s, :] + recency * target

            # Set-size–dependent interference: flatten toward uniform
            w[s, :] = (1.0 - xi_eff) * w[s, :] + xi_eff * w_0[s, :]

            # Normalize to avoid numerical drift
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p