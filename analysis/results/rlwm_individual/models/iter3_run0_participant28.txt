def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic WM encoding that decreases with set size (encoding threshold).

    Idea:
    - RL: standard delta rule (fixed learning rate and softmax).
    - WM: stores a one-shot association for a state only when (a) reward is positive and
      (b) an internal encoding succeeds. The encoding probability follows a logistic
      function that declines with set size (higher load -> lower chance to commit to WM).
      Retrieval uses a highly deterministic WM softmax over the stored association.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - encode_bias: Bias term for WM encoding probability (higher -> more encoding).
    - encode_slope: Load sensitivity for encoding; effective p_enc ~ sigmoid(encode_bias - encode_slope*(nS-3)).
                    Larger encode_slope means stronger drop in encoding under larger set sizes.

    Set-size impacts:
    - WM: as set size increases, the probability of committing a rewarded association to WM drops
      via the logistic encoding rule (fewer reliable WM entries under load).
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, encode_bias, encode_slope = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # unused baseline kept for completeness

        log_p = 0.0
        # Precompute encoding probability given set size
        load_term = float(nS) - 3.0
        p_enc_set = 1.0 / (1.0 + np.exp(-(encode_bias - encode_slope * load_term)))
        p_enc_set = np.clip(p_enc_set, 0.0, 1.0)

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic softmax over WM vector for this state
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Encode into WM only on rewarded trials, with probability that declines with set size.
            if r > 0.5:
                if np.random.rand() < p_enc_set:
                    one_hot = np.zeros(nA)
                    one_hot[a] = 1.0
                    w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-sizeâ€“dependent crosstalk (global interference).

    Idea:
    - RL: standard delta rule.
    - WM: stores last rewarded action per state (one-shot overwrite). After each trial,
      the WM content undergoes global crosstalk, i.e., blending towards the across-state
      mean policy. The crosstalk rate increases with set size (more items -> more interference).

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - crosstalk_base: Base rate controlling strength of WM crosstalk (>0).
    - crosstalk_exp: Exponent shaping how crosstalk scales with set size; rate ~ 1-exp(-crosstalk_base * nS^crosstalk_exp).

    Set-size impacts:
    - WM: interference (crosstalk) grows with set size, diluting state-specific WM vectors toward
      their global mean and reducing WM distinctiveness under higher load.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, crosstalk_base, crosstalk_exp = parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Crosstalk rate as a function of set size
        ct_rate = 1.0 - np.exp(-crosstalk_base * (float(nS) ** crosstalk_exp))
        ct_rate = np.clip(ct_rate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-gated overwrite of WM for this state
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Global crosstalk/interference toward the block-wise mean
            mean_w = np.mean(w, axis=0, keepdims=True)
            w = (1.0 - ct_rate) * w + ct_rate * mean_w

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with valence-coded last-outcome rule: recall-to-choose vs. recall-to-avoid,
    and WM precision degrades with set size.

    Idea:
    - RL: standard delta rule.
    - WM: stores the last action and its valence per state. On retrieval:
        - If last outcome was rewarded, WM recommends that last rewarded action.
        - If last outcome was unrewarded, WM recommends avoiding the last chosen action
          by assigning probability mass to the other actions.
      WM policy sharpness is scaled by avoid_strength and reduced by load_scale as set size increases.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - avoid_strength: Controls determinism of WM policy (higher -> more deterministic both for choose and avoid).
    - load_scale: Set-size sensitivity that reduces WM sharpness: beta_wm_eff = base/(1 + load_scale*(nS-3)).

    Set-size impacts:
    - WM: effective WM inverse temperature decreases with set size, making WM choices noisier under higher load.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, avoid_strength, load_scale = parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will be used to hold last-valence-coded WM vector
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM sharpness as function of set size
        wm_beta_eff = (softmax_beta_wm * max(1e-6, avoid_strength)) / (1.0 + max(0.0, load_scale) * (float(nS) - 3.0))
        wm_beta_eff = max(1e-6, wm_beta_eff)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Valence-coded WM update: choose-on-reward, avoid-on-no-reward.
            one_hot = np.zeros(nA)
            if r > 0.5:
                # Store that this action is the target
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Avoid last action: put zero on the last action, and distribute mass on others
                others = np.ones(nA)
                others[a] = 0.0
                if np.sum(others) > 0:
                    others /= np.sum(others)
                    w[s, :] = others
                else:
                    # Fallback (should not occur with nA>=2)
                    w[s, :] = (1.0 / nA) * np.ones(nA)

        blocks_log_p += log_p

    return -blocks_log_p