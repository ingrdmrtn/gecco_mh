def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + associative WM with swap interference and set-size–specific WM weights.

    Core ideas
    - RL: standard delta rule with softmax choice.
    - WM: each state's WM stores an action-probability vector. WM suffers "swap" interference,
      where on retrieval some probability mass is borrowed from other states (binding errors).
    - Arbitration: WM mixture weight depends on set size (separate weights for 3 vs 6).

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial; constant within a block.
    model_parameters : sequence of floats
        lr : float in (0,1)
            RL learning rate.
        wm_weight_3 : float in [0,1]
            WM mixture weight when set size = 3.
        wm_weight_6 : float in [0,1]
            WM mixture weight when set size = 6.
        softmax_beta : float >= 0
            RL inverse-temperature (scaled internally by 10).
        wm_decay : float in (0,1)
            WM decay/update step size toward stored associations.
        swap_rate : float in [0,1]
            Degree of swap/interference; effective swap increases with set size.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_3, wm_weight_6, softmax_beta, wm_decay, swap_rate = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # WM associative weights per state (distribution over actions)
        w_0 = (1 / nA) * np.ones((nS, nA)) # uniform baseline

        # Set-size specific WM mixture
        wm_weight_block = wm_weight_3 if nS == 3 else wm_weight_6

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            # RL policy: softmax prob of chosen action (efficiently via normalization by chosen)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy with swap interference
            # Normalize WM vector for current state
            W_s = w[s, :].copy()
            sum_Ws = np.sum(W_s)
            if sum_Ws <= 0:
                W_s = w_0[s, :].copy()
            else:
                W_s = W_s / sum_Ws

            # Mean WM vector from other states (source of swap/binding errors)
            if nS > 1:
                others = [j for j in range(nS) if j != s]
                W_others_mean = np.mean(w[others, :], axis=0)
                sum_Wo = np.sum(W_others_mean)
                if sum_Wo <= 0:
                    W_others_mean = w_0[s, :].copy()
                else:
                    W_others_mean = W_others_mean / sum_Wo
            else:
                W_others_mean = w_0[s, :].copy()

            # Set-size dependent swap interference
            swap_eff = swap_rate * ((int(block_set_sizes[t]) - 3) / 3.0 if int(block_set_sizes[t]) > 3 else 0.0)
            swap_eff = np.clip(swap_eff, 0.0, 1.0)

            W_mix = (1.0 - swap_eff) * W_s + swap_eff * W_others_mean
            # Deterministic WM readout (via high beta softmax over log-probs)
            # Equivalent probability of chosen action (no need to compute full softmax vector)
            X = np.log(np.clip(W_mix, 1e-12, 1.0))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (X - X[a])))

            # Mixture policy
            wm_w = np.clip(wm_weight_block, 0.0, 1.0)
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform, then reward-based imprint toward the chosen action
            # 1) decay toward uniform baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) if rewarded, add associative bump toward the chosen action (one-hot)
            if r >= 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration (set-size–dependent recall) and WM noise.

    Core ideas
    - RL: delta rule + softmax.
    - WM: stores a distribution over actions per state; updated with a noise-controlled step.
      When unrewarded, WM drifts toward uniform (forgetting).
    - Arbitration: WM weight scales with a sigmoid recall probability that decreases with set size.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence of floats
        lr : float in (0,1)
            RL learning rate.
        softmax_beta : float >= 0
            RL inverse-temperature (scaled internally by 10).
        wm_weight_base : float in [0,1]
            Base WM mixture weight, modulated by recall probability.
        wm_noise : float in (0,1)
            WM update/forgetting step size (higher = noisier but faster imprinting).
        k_slope : float >= 0
            Slope of the recall-vs-set-size sigmoid.
        c0_mid : float
            Midpoint of the recall sigmoid in set-size units; recall = 1/(1+exp(k*(S - c0))).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_noise, k_slope, c0_mid = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # WM distribution over actions
        w_0 = (1 / nA) * np.ones((nS, nA)) # uniform baseline

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # Set-size dependent recall probability and WM mixture
            S_t = float(block_set_sizes[t])
            recall_prob = 1.0 / (1.0 + np.exp(k_slope * (S_t - c0_mid)))
            wm_w = np.clip(wm_weight_base * recall_prob, 0.0, 1.0)

            # WM policy: softmax over log of WM distribution for determinism
            W_s = w[s, :].copy()
            sum_Ws = np.sum(W_s)
            if sum_Ws <= 0:
                W_s = w_0[s, :].copy()
            else:
                W_s = W_s / sum_Ws
            X = np.log(np.clip(W_s, 1e-12, 1.0))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (X - X[a])))

            # Mixture
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: imprint toward chosen action on reward, otherwise drift to uniform
            if r >= 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_noise) * w[s, :] + wm_noise * onehot
            else:
                w[s, :] = (1.0 - wm_noise) * w[s, :] + wm_noise * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with global choice-kernel influence and set-size–driven WM interference.

    Core ideas
    - RL: standard delta rule + softmax.
    - WM: state-specific associative distribution, but its effective precision is reduced as set size grows
      (interference). A global, across-state choice kernel biases WM readout toward recently chosen actions.
    - Arbitration: fixed base WM weight, but the WM readout is weakened by set-size interference.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence of floats
        lr : float in (0,1)
            RL learning rate.
        wm_weight_base : float in [0,1]
            Base mixture weight on WM policy (before interference).
        softmax_beta : float >= 0
            RL inverse-temperature (scaled internally by 10).
        tau_ck : float > 1
            Choice-kernel time constant; larger = slower decay of global recency.
        kappa_ck : float >= 0
            Strength of the choice-kernel influence on WM readout.
        gamma_interf : float in [0,1]
            Set-size interference strength that shrinks WM precision as set size increases.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, tau_ck, kappa_ck, gamma_interf = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # WM associative distribution per state
        w_0 = (1 / nA) * np.ones((nS, nA)) # uniform baseline

        # Global choice kernel (action recency across states)
        K = (1.0 / nA) * np.ones(nA)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # Set-size interference factor: reduces WM effective precision as set size increases
            S_t = int(block_set_sizes[t])
            interf = gamma_interf * ((S_t - 3) / 3.0 if S_t > 3 else 0.0)
            interf = np.clip(interf, 0.0, 1.0)

            # WM state distribution (normalized)
            W_s = w[s, :].copy()
            sum_Ws = np.sum(W_s)
            if sum_Ws <= 0:
                W_s = w_0[s, :].copy()
            else:
                W_s = W_s / sum_Ws

            # Combine WM with uniform due to interference
            W_eff = (1.0 - interf) * W_s + interf * w_0[s, :]

            # Add choice-kernel bias to WM readout
            CK_centered = K - (1.0 / nA)
            X = np.log(np.clip(W_eff, 1e-12, 1.0)) + kappa_ck * CK_centered
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (X - X[a])))

            # Mixture policy
            wm_w = np.clip(wm_weight_base, 0.0, 1.0)
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-imprinting toward chosen action; mild decay otherwise
            if r >= 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                alpha_wm = 0.5  # implicit fixed imprint strength (kept constant to respect param cap)
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * onehot
            else:
                alpha_forget = 0.2  # mild decay toward uniform on no reward
                w[s, :] = (1.0 - alpha_forget) * w[s, :] + alpha_forget * w_0[s, :]

            # Update global choice kernel
            eta_ck = 1.0 / max(tau_ck, 1.0001)
            onehot_a = np.zeros(nA)
            onehot_a[a] = 1.0
            K = (1.0 - eta_ck) * K + eta_ck * onehot_a

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set size effects
- Model 1: The WM mixture weight is explicitly different for S=3 vs S=6 (wm_weight_3 vs wm_weight_6). Additionally, swap interference increases linearly with set size, degrading WM precision in larger sets.
- Model 2: WM mixture is scaled by a sigmoid recall probability that decreases with set size, via parameters k_slope and c0_mid.
- Model 3: WM precision is reduced as set size increases through gamma_interf, blending the WM state distribution toward uniform when S=6. The global choice-kernel bias can partially compensate but does not depend on set size directly.