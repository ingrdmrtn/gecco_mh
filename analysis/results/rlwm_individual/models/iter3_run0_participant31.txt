def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Uncertainty-gated WM with set-size scaling and WM leak.

    Mechanism
    - RL: tabular Q-learning with softmax policy (beta scaled by 10 internally).
    - WM store: one-hot cache for the rewarded action in each state; the cache leaks toward uniform.
    - Arbitration: the WM weight increases when RL policy is uncertain (high entropy).
      The entropy influence is further scaled by set size (larger sets reduce the WM weight).
    - Set size effect: the entropy gain is multiplied by (nS/3)**ss_entropy_scale.

    Parameters
    - lr: RL learning rate (0..1).
    - wm_base: base contribution of WM to the mixture (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>0).
    - wm_leak: leak rate toward uniform for the WM store each trial (0..1).
    - entropy_slope: slope of the sigmoid that maps RL entropy to a WM gain (>0).
        Larger values make WM weight more sensitive to uncertainty.
    - ss_entropy_scale: exponent that scales the entropy gain by set size (>=0).
        Effective scaling factor = (nS / 3) ** ss_entropy_scale.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_base, softmax_beta, wm_leak, entropy_slope, ss_entropy_scale = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling factor for arbitration
        ss_scale = (max(nS, 1) / 3.0) ** max(ss_entropy_scale, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            z = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(z) / np.sum(np.exp(z))
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy (deterministic when a clear cache exists)
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            # RL entropy (natural log)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            # Normalize entropy to [0, log(nA)] -> map around mid (heuristic center at ~1.0)
            entropy_gain = 1.0 / (1.0 + np.exp(-entropy_slope * (H_rl - 1.0) * ss_scale))

            wm_weight_eff = np.clip(wm_base * entropy_gain, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global leak toward uniform
            w = (1.0 - wm_leak) * w + wm_leak * w_0

            # WM encoding on rewarded trials: store one-hot cache for this state
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with asymmetric learning rates + WM with set-size–specific mixture weights.

    Mechanism
    - RL: tabular Q-learning with separate learning rates for positive vs negative prediction errors.
    - WM store: one-hot cache of the last rewarded action per state (no leak).
    - Arbitration: mixture of WM and RL policies with weights that depend on set size.
      A distinct WM weight is used for small (3) vs large (6) set sizes.

    Parameters
    - lr_pos: learning rate for positive prediction errors (0..1).
    - lr_neg: learning rate for negative prediction errors (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>0).
    - wm_weight_small: WM mixture weight when set size is 3 (0..1).
    - wm_weight_large: WM mixture weight when set size is 6 (0..1).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_small, wm_weight_large = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = np.clip(wm_weight_small if nS <= 3 else wm_weight_large, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            z = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(z) / np.sum(np.exp(z))
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr * delta

            # WM encoding on rewarded trials (overwrite with one-hot)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Hebbian WM with interference from number of distinct states (set-size–sensitive).

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM store: Hebbian-like update toward the chosen action on rewarded trials; otherwise mild decay.
    - Interference: both WM encoding rate and its arbitration weight decrease as more distinct
      states are encountered within the block, with an additional scaling by set size.
    - Set size effect: interference grows with set size via an exponent on nS.

    Parameters
    - lr: RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>0).
    - wm_base: base WM mixture weight (0..1).
    - interference_rate: governs how strongly additional distinct states suppress WM (>=0).
        Effective suppression factor per step ~ exp(-interference_rate * seen_factor).
    - ss_exponent: exponent on set size for interference scaling (>=0).
        seen_factor uses (nS / 3) ** ss_exponent.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_base, interference_rate, ss_exponent = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        seen_states = set()
        ss_scale = (max(nS, 1) / 3.0) ** max(ss_exponent, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update seen states and compute interference factor
            if s not in seen_states:
                seen_states.add(s)
            # The more distinct states observed, the larger the effective interference
            interference = np.exp(-interference_rate * len(seen_states) * ss_scale)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            z = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(z) / np.sum(np.exp(z))
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            wm_weight_eff = np.clip(wm_base * interference, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM Hebbian-like update:
            # - If rewarded: move W_s toward the chosen action (one-hot) with rate tied to interference.
            # - If not rewarded: mild decay toward uniform.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                eta = 1.0 - np.exp(-interference_rate * ss_scale)  # stronger interference -> smaller step size
                eta = np.clip(eta, 0.0, 1.0)
                # Convert to effective encoding rate that diminishes when interference_rate is small
                enc_rate = max(1e-6, 1.0 - eta) * interference
                w[s, :] = (1.0 - enc_rate) * W_s + enc_rate * one_hot
            else:
                decay = 0.1 * (1.0 - interference)  # more interference -> more decay
                w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects
- cognitive_model1: WM arbitration weight depends on RL entropy scaled by (nS/3)**ss_entropy_scale.
- cognitive_model2: Distinct WM weights are used for small (3) vs large (6) set sizes.
- cognitive_model3: Interference increases with the number of distinct states seen, scaled by (nS/3)**ss_exponent, reducing both WM weight and WM encoding strength.