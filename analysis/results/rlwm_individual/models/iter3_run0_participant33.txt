def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + decaying WM with set-size-dependent WM retrieval reliability.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate for Q-values.
        wm_weight_base : float
            Baseline mixture weight on WM policy (0..1).
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        wm_decay : float
            Trial-wise decay of WM rows toward uniform (0..1).
        wm_recall_ss_slope : float
            Slope controlling WM recall probability as a function of set size nS.
            p_recall = sigmoid(wm_recall_ss_slope * (3.5 - nS)); smaller sets -> higher recall.
        wm_binding_strength : float
            Strength of WM "one-shot" binding on rewarded trials toward chosen action (0..1).

    Mechanism
    ---------
    - RL: standard delta rule on Q(s,a), softmax policy.
    - WM storage: decays toward uniform each time step; on reward, moves toward one-hot of the chosen action.
    - WM retrieval: noisy recall with probability p_recall depending on set size; if recall fails,
      WM policy defaults to uniform. Mixture of WM and RL uses wm_weight_base.

    Set-size effects
    ----------------
    - wm_recall_ss_slope directly modulates recall probability with set size nS (3 vs 6).
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_recall_ss_slope, wm_binding_strength = model_parameters

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (base)
            denom_wm_base = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_base = 1.0 / max(denom_wm_base, 1e-12)

            # Set-size-dependent WM recall probability
            recall_drive = wm_recall_ss_slope * (3.5 - nS)
            p_recall = 1.0 / (1.0 + np.exp(-recall_drive))

            # Effective WM policy with recall failures defaulting to uniform
            p_wm = p_recall * p_wm_base + (1.0 - p_recall) * (1.0 / nA)

            # Mixture
            wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM binding on rewarded trials
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_binding_strength) * w[s, :] + wm_binding_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + capacity-limited probabilistic WM storage.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate.
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        wm_weight_max : float
            Maximum weight assigned to WM when capacity is not exceeded.
        K_capacity : float
            Effective WM capacity in number of items; WM weight scales as min(1, K_capacity/nS).
        trace_decay : float
            Eligibility trace decay (0..1); higher keeps traces longer.
        wm_learn_prob : float
            Probability of WM committing the rewarded association; implemented as expected
            update magnitude toward one-hot on reward.

    Mechanism
    ---------
    - RL: delta rule with eligibility traces e(s,a). After each choice, e decays by trace_decay
      and the chosen e[s,a] is set to 1. Update Q for all actions via alpha * PE * e.
    - WM: capacity-limited; mixture weight scales as K_capacity/nS.
      On reward, WM row moves toward one-hot with step wm_learn_prob (expected update),
      otherwise no directed learning (but WM persists).

    Set-size effects
    ----------------
    - Capacity-limited arbitration: wm_weight_eff = wm_weight_max * min(1, K_capacity/nS), thus smaller sets -> greater WM influence.
    """
    lr, softmax_beta, wm_weight_max, K_capacity, trace_decay, wm_learn_prob = model_parameters

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Capacity-limited arbitration
            cap_scale = min(1.0, max(0.0, K_capacity / float(nS)))
            wm_weight_eff = np.clip(wm_weight_max * cap_scale, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            pe = r - Q_s[a]

            # Decay traces globally, then set chosen trace
            e *= trace_decay
            e[s, :] *= 0.0
            e[s, a] = 1.0

            # Update all Q via traces
            q += lr * pe * e

            # WM expected learning on reward
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn_prob) * w[s, :] + wm_learn_prob * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with confidence-based arbitration (entropy contrast) and WM delta learning toward outcome.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate.
        softmax_beta : float
            Inverse temperature for RL policy (scaled internally by 10).
        kappa : float
            Sensitivity of arbitration to entropy contrast (H_RL - H_WM).
            Larger kappa increases reliance on the sharper distribution.
        wm_bias : float
            Baseline bias of arbitration toward WM (logistic intercept).
        ss_gamma : float
            Set-size term in arbitration; positive values increase WM use in small sets (3) and
            decrease it in large sets (6).
        wm_alpha : float
            WM learning rate toward outcome: on reward -> toward one-hot(a); on no-reward -> toward uniform.

    Mechanism
    ---------
    - Compute softmax over Q and over W to get distributions; estimate their entropies.
    - Arbitration weight = sigmoid(wm_bias + ss_gamma*(3.5 - nS) + kappa*(H_RL - H_WM)).
      If WM sharper than RL (lower entropy), weight increases.
    - RL: standard delta rule.
    - WM: delta rule toward outcome; reward drives toward one-hot, no-reward drives toward uniform.

    Set-size effects
    ----------------
    - ss_gamma directly modulates the arbitration gate with set size nS.
    """
    lr, softmax_beta, kappa, wm_bias, ss_gamma, wm_alpha = model_parameters

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy distribution
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / max(np.sum(rl_probs), 1e-12)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy distribution
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / max(np.sum(wm_probs), 1e-12)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Entropy-based arbitration
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            H_wm = -np.sum(wm_probs * np.log(np.clip(wm_probs, 1e-12, 1.0)))
            gate_drive = wm_bias + ss_gamma * (3.5 - nS) + kappa * (H_rl - H_wm)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_drive))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture likelihood
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM learning toward outcome
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]  # uniform on no-reward
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target

        blocks_log_p += log_p

    return -blocks_log_p