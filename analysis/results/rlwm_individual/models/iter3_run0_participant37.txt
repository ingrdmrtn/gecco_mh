def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with entropy-gated WM and set-size-scaled WM precision and learning.

    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled x10 internally).
    - WM channel: softmax over WM weights W with inverse temperature that decreases with set size.
    - Mixture: mixture weight g_t is larger when RL is uncertain (higher entropy).

    WM dynamics:
    - Leaky integration toward one-hot of chosen action with learning rate that scales with reward.
      w[s,:] <- (1 - wm_alpha_eff*r) * w[s,:] ; w[s,a] += wm_alpha_eff * r
      (No strengthening on errors; only decay toward current w.)

    Set-size effects:
    - WM precision: softmax_beta_wm_eff = 1 + wm_precision_base / (1 + size_precision_drop * (nS - 3))
      Larger set size reduces the sharpness of WM policy.
    - WM learning rate: wm_alpha_eff = wm_alpha / (1 + size_precision_drop * (nS - 3))

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight0: Baseline WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL channel (scaled by 10 internally).
    - wm_precision_base: Baseline precision of WM softmax (>0).
    - size_precision_drop: Strength of set-size penalty on WM precision/learning (>=0).
    - wm_alpha: WM learning rate (0..1).
    """
    lr, wm_weight0, softmax_beta, wm_precision_base, size_precision_drop, wm_alpha = model_parameters
    softmax_beta *= 10.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects on WM precision and learning
        denom_size = 1.0 + size_precision_drop * max(0, nS - 3)
        softmax_beta_wm_eff = 1.0 + wm_precision_base / denom_size
        wm_alpha_eff = wm_alpha / denom_size

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # RL softmax distribution (for entropy computation)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits)
            probs_rl /= np.sum(probs_rl)
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, 1e-12, 1.0)))
            H_max = np.log(nA)
            H_norm = H_rl / max(H_max, 1e-12)

            # Entropy-gated WM mixture: more WM when RL is uncertain
            g = wm_weight0 * (H_norm ** 1.0)  # exponent fixed to 1 for identifiability

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = g * p_wm + (1.0 - g) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Reward-gated WM learning toward one-hot of chosen action
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha_eff * r) * w[s, :]
                w[s, a] += wm_alpha_eff * r

            # Renormalize WM row to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with probability-redistribution WM and set-size-scaled WM learning.

    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (x10 internally).
    - WM channel: softmax over WM weights W with very high inverse temperature (nearly deterministic).
    - Mixture: fixed wm_weight across trials within block.

    WM dynamics:
    - On reward: move WM distribution toward a one-hot at chosen action with learning rate wm_eta_eff.
      w[s,:] <- (1 - wm_eta_eff) * w[s,:]; w[s,a] += wm_eta_eff
    - On non-reward: redistribute probability mass away from chosen action equally to the other actions:
      loss = wm_eta_eff * min(1.0, w[s,a])
      w[s,a] -= loss; w[s,others] += loss / (nA - 1)
    - Always followed by small leak toward uniform to prevent degeneracy.

    Set-size effect:
    - WM learning rate decreases with set size: wm_eta_eff = wm_eta_base / (1 + size_eta_gain * (nS - 3))

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - softmax_beta: Inverse temperature for RL (scaled by 10 internally).
    - wm_eta_base: Baseline WM learning rate (0..1).
    - size_eta_gain: Strength of set-size penalty on WM learning (>=0).
    """
    lr, wm_weight, softmax_beta, wm_eta_base, size_eta_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_eta_eff = wm_eta_base / (1.0 + size_eta_gain * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                # Pull toward one-hot at chosen action
                w[s, :] = (1.0 - wm_eta_eff) * w[s, :]
                w[s, a] += wm_eta_eff
            else:
                # Move probability away from chosen action and share with others
                loss = wm_eta_eff * min(1.0, w[s, a])
                w[s, a] -= loss
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += loss / (nA - 1)

            # Small leak toward uniform baseline
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

            # Renormalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with surprise-adaptive gating and set-size penalty on gate.

    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (x10 internally).
    - WM channel: softmax over WM weights W with inverse temperature wm_temp_eff.
    - Mixture: dynamic g_t = sigmoid(z_t), where z is updated by surprise (|delta|) and penalized by set size.

    WM dynamics:
    - Leaky integration toward one-hot of chosen action each trial:
      w[s,:] <- (1 - w_learn) * w[s,:]; w[s,a] += w_learn
      where w_learn is tied to wm_temp_eff (higher temp -> stronger learning).

    Set-size effects:
    - WM temp: wm_temp_eff = wm_temp / (1 + 0.5 * (nS - 3))
    - Gate penalty: each trial z <- z - size_penalty * (nS - 3) / max(1, T_block), reducing g in larger set sizes.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight0: Initial WM mixture weight (0..1) used to initialize z via logit.
    - softmax_beta: Inverse temperature for RL (scaled by 10 internally).
    - wm_temp: Base inverse temperature for WM policy (>0).
    - gate_lr: Learning rate for gate updates driven by surprise |delta| (>=0).
    - size_penalty: Magnitude of per-trial penalty on the gate in larger set sizes (>=0).
    """
    lr, wm_weight0, softmax_beta, wm_temp, gate_lr, size_penalty = model_parameters
    softmax_beta *= 10.0
    blocks_log_p = 0.0

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])
        T_block = max(1, len(block_states))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_temp_eff = wm_temp / (1.0 + 0.5 * max(0, nS - 3))
        softmax_beta_wm = max(1.0, wm_temp_eff)

        # Initialize gate latent variable
        z = logit(wm_weight0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            g = 1.0 / (1.0 + np.exp(-z))  # mixture weight

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = g * p_wm + (1.0 - g) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leaky move toward one-hot
            w_learn = 1.0 - np.exp(-softmax_beta_wm / 50.0)
            w[s, :] = (1.0 - w_learn) * w[s, :]
            w[s, a] += w_learn

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Gate update: surprise-driven and set-size penalized
            z += gate_lr * abs(delta)
            z -= size_penalty * max(0, nS - 3) / T_block

        blocks_log_p += log_p

    return -blocks_log_p