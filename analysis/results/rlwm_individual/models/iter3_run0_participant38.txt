def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size-dependent WM forgetting.

    Idea:
    - RL: standard delta rule.
    - WM: leaky probability table over actions per state, updated more by reward.
    - Arbitration: weight of WM vs RL depends on relative uncertainty (entropy) between WM and RL,
      plus a set-size bias. If WM is more certain (lower entropy) than RL, the model relies more on WM.
    - Set size affects:
      - Arbitration bias linearly with set size (parameter arb_size_slope).
      - WM forgetting increases with set size via wm_forget_eff = 1 - (1 - wm_forget) ** nS.

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[2] = wm_eta (float in [0,1]): WM learning gain toward chosen action on reward.
    - model_parameters[3] = wm_forget (float in [0,1]): Base WM forgetting toward uniform each trial (scaled by set size).
    - model_parameters[4] = arb_bias (float): Arbitration bias term (logit space), positive favors WM.
    - model_parameters[5] = arb_size_slope (float): Additional arbitration bias per extra item beyond 3.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_eta, wm_forget, arb_bias, arb_size_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-scaled forgetting: increases with nS
        wm_forget_eff = 1.0 - (1.0 - np.clip(wm_forget, 0.0, 1.0)) ** nS

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration with set-size bias
            eps = 1e-12
            # Convert values to a softmax distribution to compute entropy proxies
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / np.sum(pi_rl)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))

            pi_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = pi_wm / np.sum(pi_wm)
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, eps, 1.0)))

            # Arbitration logit: lower entropy WM relative to RL -> higher WM weight
            size_bias = arb_size_slope * (nS - 3)
            arb_logit = arb_bias + size_bias + (H_rl - H_wm)
            wm_weight = 1.0 / (1.0 + np.exp(-arb_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global forgetting toward uniform (scaled by set size)
            w = (1.0 - wm_forget_eff) * w + wm_forget_eff * w_0

            # WM local update: reward-driven strengthening toward chosen action
            if r > 0.5:
                # Move W_s toward the one-hot vector for action a by wm_eta
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            else:
                # On no reward, only passive forgetting has acted (above).
                pass

            # Normalize the row to stay a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-scaled learning rate + WM with set-size-scaled precision.

    Idea:
    - RL: learning rate decreases with set size (harder to assign credit when more items).
      lr_eff = lr_base / (1 + lr_size_slope * (nS - 1)).
    - WM: policy precision (inverse temperature) decreases with set size:
      beta_wm_eff = wm_beta_base / (1 + wm_beta_slope * (nS - 1)).
      WM is updated via leaky integration toward the observed correct action on reward,
      and simply decays on no reward.
    - Mixture weight is implicit in combining RL and WM via the template's convex mixture.

    Parameters
    - model_parameters[0] = lr_base (float in [0,1]): Base RL learning rate at set size 1-3.
    - model_parameters[1] = lr_size_slope (float >= 0): How strongly set size reduces RL learning.
    - model_parameters[2] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_decay (float in [0,1]): WM leaky integration/decay rate.
    - model_parameters[4] = wm_beta_base (float > 0): Base WM inverse temperature.
    - model_parameters[5] = wm_beta_slope (float >= 0): How strongly set size reduces WM precision.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr_base, lr_size_slope, softmax_beta, wm_decay, wm_beta_base, wm_beta_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (unused as we compute beta_wm_eff per set size)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-scaled parameters
        lr_eff = lr_base / (1.0 + max(0.0, lr_size_slope) * (nS - 1))
        beta_wm_eff = wm_beta_base / (1.0 + max(0.0, wm_beta_slope) * (nS - 1))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with set-size-scaled precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Use a fixed mixture weight implicitly; here we set wm_weight via WM confidence (norm)
            # For this model, derive wm_weight from how peaked W_s is:
            peak = np.max(W_s)
            wm_weight = np.clip((peak - 1.0 / nA) / (1.0 - 1.0 / nA), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with set-size-scaled learning rate
            delta = r - Q_s[a]
            q[s][a] += lr_eff * delta

            # WM leaky update: on reward, move toward one-hot; on no reward, just decay
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Renormalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-dependent exploration + WM with lag-dependent recall probability.

    Idea:
    - RL: inverse temperature decreases with set size:
      beta_rl_eff = softmax_beta_base / (1 + beta_size_slope * (nS - 1)).
    - WM: retrieval is noisy and degrades with time since the state was last seen (lag).
      The effective WM row is a mixture of the stored row and uniform:
        W_eff = p_recall * W_s + (1 - p_recall) * uniform,
      where p_recall = exp(-wm_recall_decay * lag).
      WM is updated toward chosen action on reward and repelled on no reward.
    - Mixture weight is constant (wm_weight_base), capturing reliance on WM.

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = softmax_beta_base (float > 0): Base RL inverse temperature.
    - model_parameters[2] = beta_size_slope (float >= 0): How strongly set size reduces RL precision.
    - model_parameters[3] = wm_weight_base (float in [0,1]): Constant mixture weight for WM.
    - model_parameters[4] = wm_recall_decay (float >= 0): Memory decay per trial lag (higher -> faster forgetting).
    - model_parameters[5] = wm_gain (float in [0,1]): WM update strength per trial.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_base, beta_size_slope, wm_weight_base, wm_recall_decay, wm_gain = model_parameters

    softmax_beta = softmax_beta_base * 10  # base scaling
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last seen time for lag-dependent recall
        last_seen = -1 * np.ones(nS, dtype=int)

        # RL precision scaled by set size
        beta_rl_eff = softmax_beta / (1.0 + max(0.0, beta_size_slope) * (nS - 1))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with set-size-scaled precision
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM recall based on lag
            if last_seen[s] >= 0:
                lag = t - last_seen[s]
            else:
                lag = t + 1  # unseen yet; treat as long lag

            p_recall = np.exp(-max(0.0, wm_recall_decay) * max(0, lag))
            uniform_row = w_0[s, :]
            W_eff = p_recall * W_s + (1.0 - p_recall) * uniform_row

            # WM policy based on effective row
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture weight constant
            wm_weight = np.clip(wm_weight_base, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward -> push toward chosen action; no reward -> push away
            if r > 0.5:
                w[s, :] = (1.0 - wm_gain) * w[s, :]
                w[s, a] += wm_gain
            else:
                # Repulsion: move some mass from chosen action to others
                dec = min(wm_gain, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

            # Update last seen time
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p