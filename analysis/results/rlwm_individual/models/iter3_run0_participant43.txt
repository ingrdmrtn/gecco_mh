Below are three alternative cognitive models that instantiate different mechanisms for how working memory (WM) and reinforcement learning (RL) contribute to decisions under varying set sizes (load). Each returns the negative log-likelihood of the observed choices.

Note:
- All models adhere to the requested template logic: RL softmax with beta scaled by x10, WM softmax with high beta (50), per-block resets of values, and a mixture of RL and WM policies.
- All parameters are used meaningfully, total â‰¤ 6.
- Load (set size) modulates WM contributions differently across models.
- Assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with load-based decay and lapses.

    Idea:
    - RL learns via standard delta rule and softmax choice.
    - WM stores rewarded associations as near one-hot; under higher load than capacity,
      WM traces decay toward uniform (load-based decay).
    - The WM mixture weight is reduced when set size exceeds WM capacity.
    - A lapse parameter injects uniform random responding on a fraction of trials.

    Parameters:
    - lr: RL learning rate (0..1).
    - wm_weight: Base mixture weight of WM vs RL (0..1).
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - wm_capacity: Effective WM capacity (>=1). Higher capacity preserves WM influence under load.
    - lapse: Lapse probability to choose uniformly at random (0..0.2 typical).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-adjusted WM weight and decay strength
        cap_ratio = min(1.0, float(wm_capacity) / max(1.0, float(nS)))
        eff_wm_weight = wm_weight * cap_ratio
        # Load-based WM decay strength: more decay when load exceeds capacity
        decay_strength = max(0.0, (float(nS) - float(wm_capacity))) / max(1.0, float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability of observed action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax probability of observed action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If rewarded, move toward one-hot on chosen action (strong imprint).
            # - If not rewarded, decay toward uniform; decay scales with load beyond capacity.
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Strong attraction to target, but not fully overwriting
                w[s, :] = 0.3 * w[s, :] + 0.7 * target
            else:
                # Load-driven decay toward uniform
                w[s, :] = (1.0 - decay_strength) * w[s, :] + decay_strength * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with action perseveration and load-induced WM interference.

    Idea:
    - RL uses delta rule and softmax.
    - WM encodes rewarded pairs; under higher load, WM suffers interference that
      blurs the stored distribution toward the across-state average (action interference).
    - Action perseveration bias (per-state) favors repeating the last chosen action.
      This bias affects the policy by shifting effective logits for both RL and WM paths.

    Parameters:
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM (0..1).
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - pers_beta: Strength of per-state perseveration bias added to policy computation (>=0).
                 Implemented as an additive bias to the chosen action's value at choice time.
    - interference: Base WM interference level (0..1). Scales with load above 3.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, pers_beta, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration bias
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-scaled WM interference weight: mixes each state's WM with across-state mean
        load_scale = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for 3, 1 for 6
        eff_interf = np.clip(interference * load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Apply perseveration bias as an additive bump to the last chosen action's value
            if last_action[s] >= 0:
                bias_vec = np.zeros(nA)
                bias_vec[last_action[s]] = pers_beta
            else:
                bias_vec = np.zeros(nA)

            # RL probability with bias
            Q_eff = Q_s + bias_vec
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM interference: mix with average WM of other states under load
            if nS > 1:
                other_idx = [i for i in range(nS) if i != s]
                W_other_mean = np.mean(w[other_idx, :], axis=0)
                W_eff = (1.0 - eff_interf) * W_s + eff_interf * W_other_mean
            else:
                W_eff = W_s

            # WM probability with bias
            W_eff_biased = W_eff + bias_vec
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff_biased - W_eff_biased[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: rewarded -> imprint toward one-hot; unrewarded -> light forgetting
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.4 * w[s, :] + 0.6 * target
            else:
                # Gentle leak toward uniform (fixed small rate); interference will further blur at choice time
                leak = 0.1
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with directed exploration bonus + slot-based WM storage under load.

    Idea:
    - RL learns via delta rule. Choice uses softmax over Q plus an uncertainty bonus
      (directed exploration): actions with fewer visits get an additive boost.
    - WM stores rewarded associations stochastically, with probability decreasing with load.
      If a state's association is not in WM, WM policy is near-uniform; if stored, it's sharp.
    - The WM mixture weight is constant, but the probability of a state being stored depends on load.

    Parameters:
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM (0..1).
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - expl_bonus: Magnitude of uncertainty bonus added to Q for less-visited actions (>=0).
    - store_scale: Scale (0..1) that controls probability of WM storage given load.
                   Effective p_store = store_scale * min(1, 3/nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, expl_bonus, store_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track whether a state's association is currently stored in WM (0/1)
        wm_has_state = np.zeros(nS, dtype=float)

        # Visit counts for exploration bonus
        counts = np.zeros((nS, nA))

        # Storage probability decreases with load; reference capacity ~3
        p_store_base = store_scale * min(1.0, 3.0 / max(1.0, float(nS)))
        p_store_base = np.clip(p_store_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Directed exploration bonus: less-visited actions get higher bonus
            bonus = expl_bonus / np.sqrt(1.0 + counts[s, :])
            Q_bonus = Q_s + bonus

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_bonus - Q_bonus[a])))

            # WM policy depends on whether the state is stored
            if wm_has_state[s] >= 0.5:
                W_s = w[s, :].copy()
            else:
                # If not stored, WM is nearly uniform (weak, noisy trace)
                W_s = 0.7 * w_0[s, :] + 0.3 * w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update counts
            counts[s, a] += 1.0

            # WM update:
            # - If rewarded: imprint and (re)store with load-dependent probability
            # - If unrewarded: slight leak toward uniform and possibly drop from WM
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.3 * w[s, :] + 0.7 * target
                # Store with probability depending on load
                # Deterministic surrogate: accumulate toward 1.0 using p_store_base
                wm_has_state[s] = min(1.0, wm_has_state[s] + p_store_base * (1.0 - wm_has_state[s]))
            else:
                leak = 0.05
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]
                # Under non-reward, storage commitment weakens slightly
                wm_has_state[s] = max(0.0, wm_has_state[s] - 0.2 * (1.0 - p_store_base))

        blocks_log_p += log_p

    return -blocks_log_p