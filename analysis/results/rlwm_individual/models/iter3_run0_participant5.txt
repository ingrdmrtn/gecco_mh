def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + interference-prone WM cache.

    Mechanism
    - RL: delta-rule Q-learning with an accumulating eligibility trace over state-actions, enabling
      faster propagation within a block.
    - WM: item-specific fast memory table that moves toward a target distribution after feedback.
      It suffers set-size-dependent interference/decay toward uniform.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            Learning rate for RL Q-values.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - wm_lr: float in [0,1]
            WM learning rate toward the target distribution after feedback.
        - interference: float in [0,1]
            Scales WM decay toward uniform as a function of set size.
        - trace_lambda: float in [0,1]
            Eligibility trace decay factor for RL (accumulating traces).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, interference, trace_lambda = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # near-deterministic WM readout
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # RL eligibility traces over state-action pairs
        e_tr = np.zeros((nS, nA))

        # Set-size dependent WM interference (decay toward uniform)
        decay = np.clip(interference * (nS - 1) / max(nS, 1), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over w[s,:]
            W_s = w[s, :].copy()
            # Stabilize by centering
            prefs = W_s - np.mean(W_s)
            exp_prefs = np.exp(np.clip(softmax_beta_wm * prefs, -50, 50))
            denom = np.maximum(np.sum(exp_prefs), eps)
            W_prob = exp_prefs / denom
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_prob - W_prob[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - q[s, a]
            # Decay traces and set current SA to 1 (accumulating)
            e_tr *= trace_lambda
            e_tr[s, a] += 1.0
            q += lr * delta * e_tr

            # WM decay toward uniform due to interference
            w[s, :] = (1 - decay) * w[s, :] + decay * w_0[s, :]

            # WM learning toward a target distribution driven by outcome
            # If rewarded, target is one-hot on chosen action; else, revert toward uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = np.ones(nA) / nA
            w[s, :] = (1 - wm_lr) * w[s, :] + wm_lr * target

            # Keep WM in the simplex
            w[s, :] = np.clip(w[s, :], eps, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-gated WM precision.

    Mechanism
    - RL: standard delta-rule Q-learning with softmax.
    - WM: fast system whose policy precision (softmax beta) and learning strength depend on set size
      relative to an internal capacity K. When set size exceeds capacity, WM becomes noisy and learns
      less from outcomes; when below capacity, WM is precise and learns faster.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - K: float
            Working memory capacity (in number of items).
        - k_slope: float
            Steepness of the capacity gating function (higher -> sharper transition around K).
        - wm_lr: float in [0,1]
            Baseline WM learning rate (scaled by capacity gating on each block).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K, k_slope, wm_lr = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    # Helper: smooth capacity gate g in [0,1], ~1 when nS << K, ~0 when nS >> K
    def capacity_gate(nS, K, k):
        # sigmoid(K - nS) with slope k, mapped to [0,1]
        x = k * (K - nS)
        # clip exponent to avoid overflow
        return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = float(block_set_sizes[0])  # use float for gate; also needed as int for arrays
        nS_int = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS_int, nA))
        w = (1 / nA) * np.ones((nS_int, nA))
        w_0 = (1 / nA) * np.ones((nS_int, nA))

        # Capacity gate for this block
        g = capacity_gate(nS, K, k_slope)  # 0..1
        beta_wm_eff = max(1.0, softmax_beta_wm * g)
        wm_lr_eff = wm_lr * g
        # When overloaded, also induce decay toward uniform (interference)
        overload = np.clip(1.0 - g, 0.0, 1.0)
        decay = 0.5 * overload  # bounded decay rate

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (precision gated by capacity)
            W_s = w[s, :].copy()
            prefs = W_s - np.mean(W_s)
            exp_prefs = np.exp(np.clip(beta_wm_eff * prefs, -50, 50))
            denom = np.maximum(np.sum(exp_prefs), eps)
            W_prob = exp_prefs / denom
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_prob - W_prob[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM interference (overload -> more decay)
            w[s, :] = (1 - decay) * w[s, :] + decay * w_0[s, :]

            # WM learning toward outcome-dependent target (faster when under capacity)
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # when no reward, move modestly toward uniform
                target = np.ones(nA) / nA
            w[s, :] = (1 - wm_lr_eff) * w[s, :] + wm_lr_eff * target

            # Normalize
            w[s, :] = np.clip(w[s, :], eps, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice perseveration + WM recency with noise and load scaling.

    Mechanism
    - RL: delta-rule Q-values but the softmax includes a perseveration bias that favors
      repeating the last action taken in that state.
    - WM: recency-based memory of the last rewarded action in each state; decays over time and
      is more weakly updated under higher set size. WM policy is a noisy softmax mixture
      (epsilon-like) to capture lapses within WM.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - stickiness: float
            Perseveration bias added to the last chosen action in a state for RL policy.
        - wm_recency: float in [0,1]
            Recency update strength for WM (scaled by 1/set size).
        - wm_noise: float in [0,1]
            Probability of uniform responding within WM (noisy WM policy).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, wm_recency, wm_noise = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # recency scores start neutral (all zeros)
        w_0 = (1 / nA) * np.ones((nS, nA))  # not directly used but kept for template consistency

        # Track last chosen action per state for perseveration
        last_a = -1 * np.ones(nS, dtype=int)

        # Scale WM update by load (higher nS -> smaller effective update)
        load_scale = 1.0 / max(nS, 1)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_a[s] >= 0:
                bias[last_a[s]] = stickiness
            Q_biased = Q_s + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_biased - Q_biased[a])))

            # WM policy: softmax over recency scores, with internal lapse (wm_noise)
            W_s_scores = w[s, :].copy()
            prefs = W_s_scores - np.mean(W_s_scores)
            exp_prefs = np.exp(np.clip(softmax_beta_wm * prefs, -50, 50))
            prob_soft = exp_prefs / np.maximum(np.sum(exp_prefs), eps)
            prob_wm = (1.0 - wm_noise) * prob_soft + wm_noise * (np.ones(nA) / nA)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (prob_wm - prob_wm[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM recency update (decay + reward-gated strengthening), scaled by load
            # Decay all actions' recency for this state
            w[s, :] *= (1.0 - wm_recency * load_scale)
            # If rewarded, boost the chosen action's recency
            if r > 0.0:
                w[s, a] += wm_recency * load_scale

            # Update last action for perseveration
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p