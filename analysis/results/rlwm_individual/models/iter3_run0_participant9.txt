def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM slots with item-dependent arbitration and lapse.

    Idea:
    - RL learns action values per state via a standard delta rule.
    - WM stores recently rewarded state-action pairs with a capacity limit (K slots).
      Under larger set sizes, the chance that a given state is in WM is reduced.
    - Arbitration weight on WM is scaled by the probability that the current state is in WM.
    - A small lapse rate mixes the final choice probability with uniform to capture random errors.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline weight on WM when item is available in WM (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 for identifiability.
    - wm_K_slots: Effective number of WM slots (>=0); item-in-WM probability ~ min(1, K/nS).
    - wm_refresh: One-shot WM consolidation when reward=1; how strongly WM is updated toward the rewarded action (0..1).
    - lapse: Lapse rate mixing with uniform policy on each trial (0..0.2 recommended).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_K_slots, wm_refresh, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM readout
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Probability that a given state is actively represented in WM given capacity K
        p_item_in_wm = float(min(1.0, max(0.0, wm_K_slots) / max(1, nS)))
        wm_weight = float(np.clip(wm_weight_base * p_item_in_wm, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (provided template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic readout from W_s
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            # Arbitration and lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot consolidation on reward, without global decay
            if r > 0.5:
                # Move W_s toward a one-hot at chosen action by wm_refresh
                w[s, :] = (1.0 - wm_refresh) * w[s, :]
                w[s, a] += wm_refresh

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice stickiness + WM with decay; set-size-scaled WM arbitration and action bias.

    Idea:
    - RL learns via a delta rule and includes a choice-stickiness term favoring repetition of the last action.
    - WM stores rewarded associations and decays toward uniform each trial.
    - Arbitration weight on WM scales down with set size (proportional to 3/nS), capturing load effects.
    - A global action bias term adds a fixed preference for action 0 (applies to both RL and WM channels).

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: Baseline WM weight at set size 3 (0..1).
    - wm_decay: Per-trial WM decay toward uniform (0..1).
    - stickiness: Bonus added to the last chosen action's logit (>=0).
    - bias0: Additive bias for action 0 applied to both RL and WM logits.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_decay, stickiness, bias0 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM contribution scales with 3/nS (1.0 at nS=3; 0.5 at nS=6)
        scale_ns = 3.0 / max(1, nS)
        wm_weight = float(np.clip(wm_weight_base * scale_ns, 0.0, 1.0))

        # Track last action for stickiness
        last_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy with stickiness and bias
            rl_logits = softmax_beta * Q_s
            if last_action is not None:
                rl_logits[last_action] += stickiness
            rl_logits[0] += bias0
            rl_logits -= np.max(rl_logits)
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = pi_rl[a]

            # WM policy with strong readout, bias, and stickiness (applies similarly)
            wm_logits = softmax_beta_wm * W_s
            if last_action is not None:
                wm_logits[last_action] += stickiness
            wm_logits[0] += bias0
            wm_logits -= np.max(wm_logits)
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM learning on reward: reinforce chosen action within the state
            if r > 0.5:
                # Move W_s a bit away from others toward chosen action (use wm_decay as learning step for parsimony)
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

            # Update stickiness memory
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-based arbitration + WM with forgetting.

    Idea:
    - RL learns via delta rule. Arbitration gives more weight to WM when:
        (a) set size is small, and
        (b) RL uncertainty (entropy of the RL policy) is high.
    - WM stores rewarded associations and globally forgets toward uniform each trial.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - theta0: Arbitration intercept (logit space).
    - theta1: Effect of set size on arbitration (multiplies 3/nS; positive favors WM under small set size).
    - theta2: Effect of RL uncertainty (entropy) on arbitration (positive favors WM when RL is uncertain).
    - wm_forget: Per-trial WM forgetting rate toward uniform (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, theta0, theta1, theta2, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability for chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Also compute full RL policy to estimate uncertainty (entropy)
            rl_logits = softmax_beta * Q_s
            rl_logits -= np.max(rl_logits)
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / max(np.sum(exp_rl), eps)
            entropy_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0))) / np.log(nA)  # normalized 0..1

            # WM policy
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            # Uncertainty-based arbitration in logit space
            x = theta0 + theta1 * (3.0 / max(1, nS)) + theta2 * float(entropy_rl)
            wm_weight = 1.0 / (1.0 + np.exp(-x))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM forgetting and learning
            w = (1.0 - wm_forget) * w + wm_forget * w_0
            if r > 0.5:
                # Fast imprint of correct response in WM row
                w[s, :] = (1.0 - wm_forget) * w[s, :]
                w[s, a] += wm_forget

        blocks_log_p += log_p

    return -blocks_log_p