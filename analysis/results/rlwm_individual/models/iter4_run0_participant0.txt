def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with lateral inhibition and load-dependent decay.

    Mechanism:
    - RL: standard delta-rule with softmax policy (beta scaled by 10).
    - WM: maintains near one-hot action distributions per state (row w[s,:]).
      WM is capacity-limited: when set size exceeds a capacity threshold, WM
      becomes noisier by blending toward uniform and suffers stronger decay.
      A lateral inhibition parameter sharpens WM contrasts within the state.
    - Action policy: mixture of RL and WM using fixed wm_weight (template).

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_weight: [0,1] mixing weight of WM in the policy (1=all WM, 0=all RL).
    - softmax_beta: inverse temperature for RL (internally scaled by 10).
    - capacity_c: effective WM capacity (in number of states). Overload (nS > capacity_c)
                  drives WM toward uniform and increases forgetting.
    - inhibition: [0,1] lateral inhibition/contrast gain within WM policy. Larger values
                  sharpen WM by emphasizing deviations from the mean preference.
    - decay_base: [0,1] base decay rate of WM toward uniform each trial, scaled by overload.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_c, inhibition, decay_base = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Overload factor based on set size relative to capacity
        overload = max(0.0, float(nS) - float(capacity_c)) / max(float(nS), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy with capacity noise and lateral inhibition
            W_s = w[s, :].copy()

            # Blend WM toward uniform under overload
            W_eff = (1.0 - overload) * W_s + overload * (1.0 / nA) * np.ones(nA)

            # Lateral inhibition / contrast enhancement around the mean
            mean_w = np.mean(W_eff)
            W_eff = (1.0 - inhibition) * W_eff + inhibition * (W_eff - mean_w)

            # Softmax WM policy (high precision)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global WM decay scaled by overload
            d = np.clip(decay_base * (0.5 + 0.5 * overload), 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # WM learning toward one-hot for current state/action
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            # Stronger update when not overloaded, weaker when overloaded
            wm_alpha = np.clip(1.0 - 0.75 * overload, 0.0, 1.0)
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + error-gated WM with PE-dependent learning and load-dependent leak.

    Mechanism:
    - RL: standard delta rule with softmax policy (beta scaled by 10).
    - WM: updates are gated by the unsigned RL prediction error (|PE|).
      The larger the surprise (|PE|), the larger the WM update rate (nonlinear by pe_power).
      On rewarded trials WM moves toward a one-hot policy; on unrewarded trials WM relaxes
      toward uniform (to unlearn incorrect associations). A global leak (decay) increases
      with set size (load sensitivity).
    - Action policy: mixture of RL and WM using fixed wm_weight (template).

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] mixing weight of WM in the policy (1=all WM).
    - softmax_beta: inverse temperature for RL (internally scaled by 10).
    - wm_gain: >=0, scales the WM learning rate as wm_gain * |PE|^pe_power.
    - pe_power: >=0, nonlinearity exponent on |PE|.
    - leak: [0,1], base WM leak toward uniform per trial; scaled up by set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_gain, pe_power, leak = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled leak factor
        leak_scale = (float(nS) / 6.0)  # in [0.5,1.0] for nS in {3,6}
        leak_eff = np.clip(leak * leak_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Global WM leak toward uniform (stronger in larger sets)
            w = (1.0 - leak_eff) * w + leak_eff * w_0

            # Error-gated WM learning
            pe_mag = abs(pe)
            alpha_w = wm_gain * (pe_mag ** pe_power)
            alpha_w = np.clip(alpha_w, 0.0, 1.0)

            if r > 0.5:
                # Rewarded: move toward one-hot
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # Unrewarded: relax toward uniform to unlearn the chosen action
                target = (1.0 / nA) * np.ones(nA)

            w[s, :] = (1.0 - alpha_w) * w[s, :] + alpha_w * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM implementing win-stay/lose-shift bias with load-dependent lapses.

    Mechanism:
    - RL: standard delta rule with softmax policy (beta scaled by 10).
    - WM: stores a one-hot tendency for the last chosen action per state and
      updates more when rewarded. WM policy is then biased by win-stay/lose-shift:
        * If the last choice on a state was rewarded, increase its WM probability.
        * If it was not rewarded, decrease it and redistribute to alternatives.
      Additionally, a lapse probability increases with set size, blending WM with uniform.
    - Action policy: mixture of RL and WM using fixed wm_weight (template).

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] mixing weight of WM in the policy (1=all WM).
    - softmax_beta: inverse temperature for RL (internally scaled by 10).
    - wsls_bias: [0,1], strength of win-stay/lose-shift modulation applied to WM policy.
    - lapse_low: [0,1], baseline lapse probability at low set size.
    - lapse_slope: real, controls how lapse increases with set size (logistic in nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wsls_bias, lapse_low, lapse_slope = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Memory of last action and reward per state for WSLS bias
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1.0 * np.ones(nS)  # -1 indicates unknown

        # Lapse probability as a logistic function of set size
        # nS in {3,6} -> center at 3, slope controls growth toward 6
        x = float(nS - 3)
        lapse = lapse_low + (1.0 - lapse_low) / (1.0 + np.exp(-lapse_slope * x))
        lapse = np.clip(lapse, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # Base WM policy from stored distribution
            W_s = w[s, :].copy()
            # Softmax WM
            base_den = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_base = 1.0 / max(base_den, 1e-12)

            # Construct full WM probability vector (not just p of chosen) to apply WSLS
            logits = softmax_beta_wm * W_s
            # For numerical stability, shift by max
            logits -= np.max(logits)
            probs = np.exp(logits)
            probs /= max(np.sum(probs), 1e-12)

            # Apply WSLS bias if we have a previous action on this state
            la = int(last_action[s])
            lr_prev = float(last_reward[s])
            if la >= 0 and lr_prev >= 0.0:
                if lr_prev > 0.5:
                    # Win-stay: boost probability of last action
                    boost = wsls_bias
                    probs = (1.0 - boost) * probs
                    probs[la] += boost
                else:
                    # Lose-shift: reduce probability of last action and redistribute mass
                    reduce = wsls_bias
                    take = min(reduce, probs[la])
                    redistribute = take / (nA - 1)
                    probs[la] -= take
                    for ai in range(nA):
                        if ai != la:
                            probs[ai] += redistribute
                # Renormalize
                probs = np.maximum(probs, 1e-12)
                probs /= np.sum(probs)

            # Load-dependent lapse on WM: blend with uniform
            probs = (1.0 - lapse) * probs + lapse * (1.0 / nA) * np.ones(nA)

            # Extract p_wm for the chosen action a
            p_wm = float(probs[a])

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: stronger consolidation when rewarded, mild when not
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            alpha_pos = 0.7 + 0.3 * wsls_bias  # use wsls_bias to also scale update strength
            alpha_neg = 0.2 * (1.0 - wsls_bias)
            alpha = alpha_pos if r > 0.5 else alpha_neg
            alpha = np.clip(alpha, 0.0, 1.0)
            w[s, :] = (1.0 - alpha) * w[s, :] + alpha * one_hot

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p