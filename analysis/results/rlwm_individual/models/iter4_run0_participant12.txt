def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (dual learning rates) + WM with load-weakened precision and uncertainty-gated arbitration

    Description:
    - RL: tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: state-action weight matrix that decays toward uniform, more quickly under higher load (nS).
      Its softmax precision is reduced when nS is large (capacity pressure).
    - Arbitration: the WM/RL mixture weight is reduced when RL is confident (low entropy) and when load is high.

    Parameters (tuple):
    - lr_pos: RL learning rate when prediction error is positive (0..1)
    - lr_neg: RL learning rate when prediction error is negative (0..1)
    - base_mix: Baseline WM mixture weight (0..1)
    - softmax_beta: Inverse temperature for RL (scaled internally x10)
    - wm_beta: Base inverse temperature for WM policy (before load-scaling)
    - wm_forget: Base WM forgetting factor (0..1). Effective decay per trial increases with nS as wm_forget^(nS/3)
    """
    lr_pos, lr_neg, base_mix, softmax_beta, wm_beta, wm_forget = model_parameters

    softmax_beta *= 10.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled WM decay toward uniform
        wm_decay = max(0.0, min(1.0, wm_forget)) ** (nS / 3.0)
        wm_decay = max(0.0, min(1.0, wm_decay))

        # Load reduces WM precision
        load_precision_scale = 3.0 / max(1.0, nS)  # 1 at nS=3, 0.5 at nS=6
        beta_wm_eff = max(eps, wm_beta * load_precision_scale)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probability for the chosen action (via softmax trick)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # Full RL policy to compute entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            rl_entropy = -np.sum(rl_probs * np.log(np.maximum(eps, rl_probs)))
            rl_entropy_norm = rl_entropy / np.log(nA)  # in [0,1]

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration: down-weight WM when RL is confident (low entropy) and when load is high
            load_mix_scale = 3.0 / max(1.0, nS)  # 1 at nS=3, 0.5 at nS=6
            wm_weight = base_mix * load_mix_scale * rl_entropy_norm
            wm_weight = max(0.0, min(1.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with dual learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0.0 else lr_neg
            # Update only chosen state-action
            q[s, a] += lr * delta

            # WM global decay to uniform (load-dependent)
            w = wm_decay * w + (1.0 - wm_decay) * w_0

            # WM local update at current state
            alpha_wm = 1.0 - wm_decay  # faster update when decay is stronger
            if r >= 0.5:
                # Move toward one-hot on chosen action
                w[s, :] = (1.0 - alpha_wm) * w[s, :]
                w[s, a] += alpha_wm
            else:
                # Mildly reduce chosen action toward uniform
                w[s, a] = (1.0 - alpha_wm) * w[s, a] + alpha_wm * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + WM with interference-driven decay and state-wise perseveration

    Description:
    - RL: tabular Q-learning augmented with eligibility traces, enabling credit assignment that
      spans recent choices. The trace decays by lambda_tr each trial and is set to 1 for the chosen pair.
    - WM: per-state action weights decay more under larger set sizes (interference parameter).
      WM policy includes a "stay" bias that boosts the last action taken in the same state.
    - Arbitration: baseline WM mixture weight scaled by capacity factor (3/nS).

    Parameters (tuple):
    - lr: RL learning rate for Q updates via eligibility traces (0..1)
    - lambda_tr: Eligibility trace decay (0..1), controls how long traces persist
    - mix0: Baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally x10)
    - interference: Controls WM decay to uniform with load; higher -> stronger decay with nS (>=0)
    - stay_bias: Additive bias favoring repeating the last action in a state within WM policy (in logits space, >=0)
    """
    lr, lambda_tr, mix0, softmax_beta, interference, stay_bias = model_parameters

    softmax_beta *= 10.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces
        e = np.zeros((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay increases with load via exponential interference
        wm_decay = np.exp(-max(0.0, interference) * nS)
        wm_decay = max(0.0, min(1.0, wm_decay))

        # Capacity factor for arbitration
        cap_factor = 3.0 / max(1.0, nS)
        wm_weight = max(0.0, min(1.0, mix0 * cap_factor))

        # Track last chosen action per state for perseveration bias
        last_act = -np.ones(nS, dtype=int)

        softmax_beta_wm = 50.0  # baseline precision for WM
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # WM stay bias: add to the logit of the last action taken in this state
            if last_act[s] >= 0:
                W_s[last_act[s]] += max(0.0, stay_bias)

            # RL policy prob
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy prob
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # Decay traces and set chosen trace to 1
            e *= lambda_tr
            e[s, a] = 1.0
            # Update all q by traces
            q += lr * delta * e

            # WM global decay due to interference/load
            w = wm_decay * w + (1.0 - wm_decay) * w_0

            # WM local update
            if r >= 0.5:
                # Strong cache of correct mapping
                w[s, :] = 0.1 * w[s, :]
                w[s, a] = 1.0
            else:
                # Mild suppression of the chosen action toward uniform
                w[s, a] = 0.8 * w[s, a] + 0.2 * (1.0 / nA)

            # Update last action for perseveration bias
            last_act[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with distractor suppression and load-dependent state confusion (generalization)

    Description:
    - RL: standard tabular Q-learning with single learning rate.
    - WM: on rewarded trials, moves toward a one-hot code for the chosen action; on unrewarded trials,
      it suppresses the chosen action by a tunable amount. WM learning uses a dedicated rate (wm_lr).
    - Load-dependent confusion: when nS is large, WM partially generalizes across states by blending
      each state's weights with the across-state mean, controlled by 'confusion'. This induces
      interference in the 6-item condition.
    - Arbitration: WM weight diminishes with load (scaled by 3/nS). WM precision also softens with confusion.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - mix: Baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally x10)
    - wm_lr: WM learning rate toward target codes (0..1)
    - neg_suppress: Degree of WM suppression of the chosen action after no reward (0..1)
    - confusion: Load-dependent state-averaging weight (>=0). Effective g grows with nS, causing interference.
    """
    lr, mix, softmax_beta, wm_lr, neg_suppress, confusion = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm_base = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration weight scales with load
        wm_weight_base = max(0.0, min(1.0, mix)) * (3.0 / max(1.0, nS))

        # Confusion grows with load (0 at nS=3, up to ~confusion at nS=6)
        g = max(0.0, confusion) * max(0.0, (nS - 3.0) / 3.0)
        g = min(1.0, g)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen action prob
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM precision reduced by confusion
            beta_wm_eff = softmax_beta_wm_base * (1.0 - 0.7 * g)
            beta_wm_eff = max(eps, beta_wm_eff)

            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM local update toward target code
            T = np.ones(nA) / nA
            if r >= 0.5:
                T = np.zeros(nA)
                T[a] = 1.0
            else:
                # Suppress chosen action relative to uniform, then renormalize
                T[a] = T[a] * (1.0 - max(0.0, min(1.0, neg_suppress)))
                T = np.maximum(eps, T)
                T = T / np.sum(T)

            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * T

            # Load-dependent state confusion: blend with across-state average
            mean_w = np.mean(w, axis=0, keepdims=True)
            w = (1.0 - g) * w + g * mean_w

            # Gentle drift to maintain numerical stability and prevent collapse
            w = 0.999 * w + 0.001 * w_0

        blocks_log_p += log_p

    return -blocks_log_p