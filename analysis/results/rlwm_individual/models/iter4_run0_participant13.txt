def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + gated WM with set-size scaling and dual decay.

    Policy is a mixture of:
      - RL softmax over Q-values.
      - WM softmax over a working-memory matrix that is updated primarily after positive outcomes.
    WM storing is "gated" by reward and by set size (less likely to store when set size is large).
    Both RL and WM decay toward uniform baselines, capturing interference/forgetting.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        wm_gate_base : float
            Base probability of committing an item to WM after reward (0-1).
        wm_decay : float
            Per-trial decay rate of WM toward uniform (0-1).
        rl_decay : float
            Per-trial decay rate of RL Q-values toward uniform (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_gate_base, wm_decay, rl_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        q_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM commit gate (expected update strength after reward)
        # More items -> lower commit probability.
        gate_scale = min(1.0, 3.0 / max(1.0, nS))
        wm_gate = np.clip(wm_gate_base * gate_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply RL decay globally (interference/forgetting)
            q = (1.0 - rl_decay) * q + rl_decay * q_0

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-gated commit to WM (expected update: convex combo with one-hot)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_gate) * w[s, :] + wm_gate * one_hot
                # Ensure normalization
                w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)
            else:
                # After no reward, leave WM mostly unchanged beyond decay
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RLâ€“WM arbitration via entropy difference with set-size bias.

    The mixture weight assigned to WM is not fixed: it is computed on each trial
    via a logistic function of the entropy difference between RL and WM policies,
    plus a set-size bias that down-weights WM when set size is large.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight0 : float
            Base bias for WM in the logistic arbitration (0-1, used as prior logit anchor).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        arb_slope : float
            Slope controlling sensitivity to (H_RL - H_WM).
        arb_bias : float
            Additive bias term in arbitration logit (positive favors WM).
        wm_decay : float
            Per-trial WM decay toward uniform (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, arb_slope, arb_bias, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    def softmax_probs(vals, beta):
        z = vals - np.max(vals)
        e = np.exp(beta * z)
        p = e / np.sum(e)
        return p

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    def logit(x):
        x = np.clip(x, eps, 1 - eps)
        return np.log(x) - np.log(1 - x)

    base_logit = logit(np.clip(wm_weight0, 0.0, 1.0))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size penalty term (discourage WM as set size grows)
        size_bias = np.log(max(1.0, 3.0) / max(1.0, nS))  # <= 0 when nS >= 3

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob of chosen action (template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute full distributions for arbitration entropy terms
            pi_rl = softmax_probs(Q_s, softmax_beta)
            pi_wm = softmax_probs(W_s, softmax_beta_wm)
            H_rl = entropy(pi_rl)
            H_wm = entropy(pi_wm)

            # Arbitration weight is logistic of entropy difference and set-size bias
            # Intuition: when RL is more uncertain (higher entropy) than WM, favor WM.
            wm_logit = base_logit + arb_bias + arb_slope * (H_rl - H_wm) + size_bias
            eff_wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Reward-strengthened row update (sharpen on reward, mild nudge otherwise)
            if r > 0.0:
                alpha = 1.0
            else:
                alpha = 0.2
            w[s, :] = (1.0 - alpha) * w[s, :]
            w[s, a] += alpha
            w[s, :] /= max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-diluted WM with per-state recency bias (perseveration) and RL.

    WM effectiveness is diluted as set size grows via a state-specific mixture between
    a uniform distribution and the WM trace, controlled by a capacity-like parameter.
    A recency bias increases the WM probability for the last action chosen in that state.
    Policy is a mixture between RL and this WM component.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        phi_slots : float
            Capacity-like parameter; higher values reduce dilution under larger set sizes.
        wm_decay : float
            Per-trial WM decay toward uniform (0-1).
        perseveration : float
            Strength of per-state recency bias added to WM for the last chosen action (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi_slots, wm_decay, perseveration = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for recency bias
        last_choice = -1 * np.ones(nS, dtype=int)

        # Capacity dilution factor for this block/state size
        # m_s is the fraction of WM signal retained (rest mixed with uniform)
        m_s = np.clip(phi_slots / max(1.0, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # Add recency bias toward last choice in this state into WM scores
            if last_choice[s] >= 0:
                bias_vec = np.zeros(nA)
                bias_vec[last_choice[s]] = perseveration
                W_biased = W_s + bias_vec
                # Normalize biased WM vector to a probability simplex for softmax input
                # Keep it as scores; scale doesn't matter for softmax, but ensure non-negativity
                W_biased = np.maximum(W_biased, 0.0)
            else:
                W_biased = W_s

            # Apply capacity dilution: mix WM scores with uniform before softmax
            W_eff = (1.0 - m_s) * w_0[s, :] + m_s * W_biased

            # RL policy (template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform globally
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-dependent sharpening of WM trace for the chosen action
            alpha_pos = 1.0
            alpha_neg = 0.2
            alpha = alpha_pos if r > 0.0 else alpha_neg
            w[s, :] = (1.0 - alpha) * w[s, :]
            w[s, a] += alpha
            w[s, :] /= max(np.sum(w[s, :]), eps)

            # Update last choice for recency bias
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p