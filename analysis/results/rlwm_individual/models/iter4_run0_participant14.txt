def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with reward-contingent WM encoding and set-size-dependent WM lapses.

    Idea
    - RL learns slowly and stochastically via delta rule (as in the template).
    - WM stores a sharp, one-shot preference for the most recently chosen action in a state,
      with stronger encoding when rewarded vs. unrewarded.
    - Under higher set size (6), WM retrieval suffers more "lapses" (mixture with uniform),
      reducing the effective influence of WM on policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6 here).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight_base: Baseline weight of WM in the mixture (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - enc_pos: WM encoding strength when reward=1 (0..1). For reward=0, encoding strength is 1-enc_pos.
        - lapse_base: Baseline WM lapse probability (0..1).
        - lapse_k: How much WM lapse increases with set size (>=0; larger nS -> larger lapse).

    Set-size impact
    - WM lapses increase with set size via: lapse_eff = 1 - (1 - lapse_base)^(1 + lapse_k*(nS-3)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, enc_pos, lapse_base, lapse_k = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM lapse (higher with larger nS)
        ss_factor = 1.0 + max(0, nS - 3) * max(0.0, lapse_k)
        lapse_eff = 1.0 - (1.0 - np.clip(lapse_base, 0.0, 1.0)) ** ss_factor
        lapse_eff = np.clip(lapse_eff, 0.0, 1.0)

        wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM map, with lapses to uniform
            pdet_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse_eff) * pdet_wm + lapse_eff * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update (template)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - Reward-contingent encoding strength. When reward=1, use enc_pos; when 0, use 1-enc_pos.
            # - Move current state's WM row toward the chosen action (one-hot) by that strength.
            enc_strength = enc_pos if r > 0.0 else (1.0 - enc_pos)
            enc_strength = np.clip(enc_strength, 0.0, 1.0)

            # First, keep other actions proportionally scaled (to maintain a distribution)
            w[s, :] = (1.0 - enc_strength) * w[s, :]
            w[s, a] += enc_strength

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size-specific temperatures and WM weights.

    Idea
    - RL is standard delta-rule with a softmax.
    - WM stores the most recent action per state (sharp, with decay toward uniform each trial).
    - Both WM weight and RL temperature depend on set size: separate parameters for nS=3 vs. nS=6.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6 here).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight_small: WM weight when nS=3 (0..1).
        - softmax_beta_small: RL inverse temperature for nS=3 (scaled internally by 10).
        - softmax_beta_large: RL inverse temperature for nS=6 (scaled internally by 10).
        - wm_weight_large: WM weight when nS=6 (0..1).
        - wm_decay: Per-trial WM decay toward uniform (0..1).

    Set-size impact
    - Uses separate RL temperatures and WM weights for small vs. large set sizes.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_small, softmax_beta_small, softmax_beta_large, wm_weight_large, wm_decay = model_parameters
    # We'll still scale the chosen beta by 10 in keeping with the template spirit
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size contingent parameters
        if nS <= 3:
            wm_weight_eff = np.clip(wm_weight_small, 0.0, 1.0)
            softmax_beta = np.clip(softmax_beta_small, 1e-6, 1e6) * 10.0
        else:
            wm_weight_eff = np.clip(wm_weight_large, 0.0, 1.0)
            softmax_beta = np.clip(softmax_beta_large, 1e-6, 1e6) * 10.0

        wd = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform, then imprint chosen action
            w = (1.0 - wd) * w + wd * w_0
            w[s, :] = (1.0 - wd) * w[s, :]
            w[s, a] += wd

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with global-action tendency component that grows with set size.

    Idea
    - RL learns via delta rule.
    - WM has a state-specific map (w[s,:]) that decays to uniform and is imprinted by the chosen action.
    - Additionally, there is a global action tendency g (across all states) updated every trial.
      Under higher set size, WM retrieval blends more with this global tendency, reflecting increased
      reliance on state-agnostic heuristics when WM is overloaded.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6 here).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight: Weight of WM in the policy mixture (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - wm_decay: Per-trial WM decay toward uniform and imprint step (0..1).
        - gmix_base: Baseline mixing between state-specific WM and global tendency (0..1).
        - gmix_k: Increase in this mixing with set size (>=0; larger nS -> more global blending).

    Set-size impact
    - The blend between state-specific WM and global tendency increases with nS via:
      gmix_eff = clip(gmix_base + gmix_k*(nS-3), 0, 1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, gmix_base, gmix_k = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global action tendency vector (starts uniform)
        g = (1.0 / nA) * np.ones(nA)

        # Set-size dependent blend between state-specific WM and global tendency
        gmix_eff = np.clip(gmix_base + max(0, nS - 3) * max(0.0, gmix_k), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
        wd = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]

            # Effective WM preference combines state-specific WM with global tendency
            W_s_state = w[s, :]
            W_s_eff = (1.0 - gmix_eff) * W_s_state + gmix_eff * g

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy using effective WM preference
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_eff - W_s_eff[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - Decay whole WM map toward uniform
            w = (1.0 - wd) * w + wd * w_0
            # - Imprint chosen action for the visited state
            w[s, :] = (1.0 - wd) * w[s, :]
            w[s, a] += wd

            # Global tendency update (analogous imprint toward last chosen action)
            g = (1.0 - wd) * g
            g[a] += wd

        blocks_log_p += log_p

    return -blocks_log_p