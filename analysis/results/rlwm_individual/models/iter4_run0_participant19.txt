def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + interference-prone WM + reliability-based arbitration.

    Idea:
    - RL uses an eligibility trace per (state, action) to distribute credit over time.
    - WM stores a fast, leaky value table that suffers more interference when set size is larger.
    - Arbitration weight is computed on each trial from an approximate reliability signal:
      higher WM distinctiveness (spread) and lower RL surprise increase WM control.

    Parameters (6 total):
    - alpha: RL learning rate for prediction error (0..1)
    - wm_plasticity: WM learning rate for updating the chosen action's WM value (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - elig: eligibility decay factor (0..1), controls how credit spreads across recent choices
    - interference: scales WM leak toward prior; higher => faster WM decay, esp. for larger set size
    - arb_gain: gain for the arbitration logistic; higher => more sensitivity to reliability differences

    Set-size impact:
    - WM interference increases linearly with set size: wm_decay = interference * (nS - 3) / 3
      so the WM table decays faster when nS=6 vs 3.
    - Arbitration depends on WM distinctiveness in the current state, which tends to be lower under larger set sizes.
    """
    alpha, wm_plasticity, softmax_beta, elig, interference, arb_gain = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # RL eligibility traces and running surprise per state
        e = np.zeros((nS, nA))
        surprise = 0.5 * np.ones(nS)  # running |PE| per state

        # Set-size-dependent WM decay per step
        wm_decay = max(0.0, min(1.0, interference * (nS - 3) / 3.0))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY: softmax on WM values (deterministic)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight from reliability:
            # - WM reliability ~ distinctiveness in this state's WM values
            wm_distinct = np.max(W_s) - np.min(W_s)
            # - RL unreliability ~ running absolute PE (surprise)
            rl_unrel = surprise[s]
            wm_weight = 1 / (1 + np.exp(-arb_gain * (wm_distinct - rl_unrel)))
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            pe = r - q[s, a]
            # decay all traces
            e *= elig
            # activate trace for chosen
            e[s, a] += 1.0
            # update all Q-values by eligibility
            q += alpha * pe * e

            # Update running surprise (exponential moving average of |pe|)
            surprise[s] = 0.8 * surprise[s] + 0.2 * abs(pe)

            # WORKING MEMORY UPDATE:
            # - global leak toward prior (interference), stronger for larger set sizes
            w = (1 - wm_decay) * w + wm_decay * w_0
            # - plastic update on chosen entry toward immediate outcome
            w[s, a] += wm_plasticity * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Surprise-gated WM arbitration + leaky WM with set-size-dependent decay.

    Idea:
    - RL: basic delta rule.
    - WM: fast but leaky value table; leak grows exponentially with set size via a base decay.
    - Arbitration is dynamic: when recent surprise (|PE|) is high, shift more toward WM
      to exploit quickly updated information; otherwise rely on RL.

    Parameters (6 total):
    - alpha: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_decay_base: base for WM decay per trial; WM decay = 1 - wm_decay_base**nS (0..1)
    - wm_learn: WM learning rate for chosen action (0..1)
    - mix_bias: bias term for arbitration logistic (can be negative/positive)
    - surprise_gain: scales influence of surprise on arbitration (>=0)

    Set-size impact:
    - WM leak increases with set size by 1 - wm_decay_base**nS; larger nS => faster WM decay.
    - Arbitration depends on surprise (|PE|); set size indirectly matters by reducing WM stability at larger nS.
    """
    alpha, softmax_beta, wm_decay_base, wm_learn, mix_bias, surprise_gain = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Surprise tracker per state
        surprise = 0.5 * np.ones(nS)

        # Set-size-dependent WM decay (0..1)
        wm_decay = 1.0 - (wm_decay_base ** max(1, nS))
        wm_decay = max(0.0, min(1.0, wm_decay))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: almost deterministic softmax on WM table
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise-gated arbitration
            gate = mix_bias + surprise_gain * surprise[s]
            wm_weight = 1.0 / (1.0 + np.exp(-gate))
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # Update surprise trace
            surprise[s] = 0.8 * surprise[s] + 0.2 * abs(pe)

            # WM update: leak + fast update
            w = (1 - wm_decay) * w + wm_decay * w_0
            w[s, a] += wm_learn * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited slot WM + RL with set-size-dependent leak.

    Idea:
    - WM behaves like a limited-capacity store: probability the current state is in WM
      scales with capacity C relative to set size nS: p_inWM = clip(C / nS, 0, 1).
    - If the state is in WM, its WM policy is near-deterministic toward the stored correct action
      when last rewarded; otherwise WM is near-uniform.
    - RL runs in parallel and slowly forgets more when set size is larger.

    Parameters (6 total):
    - alpha: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - capacity_C: effective WM slots available for these mappings (0..6)
    - eta: reliability of WM when a rewarded association is stored (0.33..1). Higher => more deterministic.
    - rl_leak: RL leak toward uniform per trial, scaled by set size (0..1)
    - wm_noise: softens WM policy when not confidently stored; 0 => perfectly uniform, >0 adds slight structure

    Set-size impact:
    - WM arbitration weight equals p_inWM = min(1, capacity_C / nS); thus larger set size reduces WM control.
    - RL leak per step is leak = rl_leak * (nS - 3) / 3; larger set size increases forgetting.
    """
    alpha, softmax_beta, capacity_C, eta, rl_leak, wm_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM store: for each state, track a stored action and a confidence flag
        stored_action = -1 * np.ones(nS, dtype=int)
        stored_conf = np.zeros(nS)  # 0..1, becomes 1 after a rewarded observation

        # Set-size effects
        p_inWM = max(0.0, min(1.0, capacity_C / max(1, nS)))
        rl_forget = max(0.0, min(1.0, rl_leak * (nS - 3) / 3.0))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Compose a WM vector for this state from the slot store
            if stored_action[s] >= 0 and stored_conf[s] > 0.5:
                # High-confidence stored mapping: put mass eta on stored action, rest uniformly with wm_noise
                wm_vec = (wm_noise / nA) * np.ones(nA)
                wm_vec += (1 - wm_noise) * ((1 - eta) / (nA - 1)) * np.ones(nA)
                wm_vec[stored_action[s]] = (wm_noise / nA) + (1 - wm_noise) * eta
            else:
                # No reliable memory: near-uniform with slight structure (wm_noise)
                wm_vec = (1.0 / nA) * np.ones(nA)

            # Set WM table row to this vector to define a softmax policy
            w[s, :] = wm_vec

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy on the fly from W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight equals probability state is in WM
            wm_weight = p_inWM
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with set-size-dependent leak toward uniform
            pe = r - q[s, a]
            # Leak entire Q-table slightly toward uniform
            q = (1 - rl_forget) * q + rl_forget * (1.0 / nA)
            q[s, a] += alpha * pe

            # WM update of the slot store
            if r == 1:
                stored_action[s] = a
                stored_conf[s] = 1.0
            else:
                # failed outcome reduces confidence
                stored_conf[s] = 0.7 * stored_conf[s]

        blocks_log_p += log_p

    return -blocks_log_p