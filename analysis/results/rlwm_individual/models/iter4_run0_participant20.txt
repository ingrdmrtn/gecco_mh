Here are three alternative cognitive models, each expressed as a standalone Python function. All three follow the requested template structure, mixing an RL policy with a WM policy, and they return the negative log-likelihood of observed choices. They differ in how WM is encoded/decays, how RL updates are structured, and how load (set size) modulates behavior.

Note: Assume numpy (np) is already imported in the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with RL eligibility traces, load-driven WM interference, and a global lapse.

    Idea:
    - Choices are a mixture of an RL policy and a WM policy.
    - RL uses an eligibility-trace that decays across all state-action pairs, allowing broader credit assignment.
    - WM stores rewarded actions but is subject to interference/decay that grows with set size (load).
    - A small lapse probability mixes in a uniform-random policy.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: baseline mixture weight for WM in [0,1].
    - softmax_beta: inverse temperature (scaled by 10 internally) for RL policy.
    - lambda_trace: eligibility-trace persistence in [0,1]; higher -> slower trace decay.
    - interference_rate: positive; increases WM decay/interference with set size.
    - lapse: small probability [0, 0.2] to choose uniformly at random.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_trace, interference_rate, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL, WM and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # Load-dependent WM decay/interference strength
        load = max(0, nS - 3)
        wm_decay_strength = 1.0 - np.exp(-interference_rate * (1.0 + load))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture + lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            # Decay all traces
            e *= lambda_trace
            # Set current trace to 1 for chosen state-action
            e[s, a] = 1.0
            # Compute PE and update all Q with eligibility
            pe = r - q[s, a]
            q += lr * pe * e

            # WM global decay toward uniform (interference)
            w = (1.0 - wm_decay_strength) * w + wm_decay_strength * w_0

            # WM encoding: on reward, strengthen the chosen action; on no reward, slight reset toward uniform
            if r > 0.5:
                # Encode toward one-hot for the rewarded pair with strength tied to remaining non-decayed mass
                enc = (1.0 - wm_decay_strength)
                w[s, :] = (1.0 - enc) * w[s, :]
                w[s, a] += enc
            else:
                # Non-reward: additional drift toward uniform for that state
                drift = 0.5 * wm_decay_strength
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with PE-nonlinearity and forgetting + WM with confidence-gated reliance.

    Idea:
    - RL uses a nonlinear prediction error: PE' = sign(PE) * |PE|^eta_pe, allowing sensitivity to large errors.
    - RL also forgets (decays towards uniform) each trial by q_forget.
    - WM mixture weight is boosted by a state-specific confidence variable that grows with recent rewards.
    - WM encodes rewarded actions (supervised) and minimally resets on non-rewards.

    Parameters (model_parameters):
    - lr: base RL learning rate in [0,1].
    - wm_weight: baseline WM weight in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally).
    - eta_pe: PE nonlinearity exponent (>0); 1 = linear PE, >1 emphasizes big errors.
    - q_forget: RL forgetting rate in [0,1], decaying Q toward uniform.
    - wm_confidence_gain: scales how much confidence boosts the WM mixture weight.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, eta_pe, q_forget, wm_confidence_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-level confidence that grows with recent rewards and decays otherwise
        confidence = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence-boosted WM weight for this state
            # Confidence is mapped through a saturating transform to [0,1), then scaled
            conf_term = confidence[s] / (1.0 + abs(confidence[s]))
            wm_w_eff = np.clip(wm_weight + wm_confidence_gain * conf_term, 0.0, 1.0)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update: forgetting + nonlinear PE update on chosen action
            # Forgetting towards uniform
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA)
            pe = r - q[s, a]
            pe_nl = np.sign(pe) * (abs(pe) ** max(1e-6, eta_pe))
            q[s, a] += lr * pe_nl

            # WM update
            if r > 0.5:
                # Strong supervised update toward the chosen action
                w[s, :] *= 0.0
                w[s, a] = 1.0
            else:
                # On non-reward, slight reset towards uniform for that state
                reset = 0.25
                w[s, :] = (1.0 - reset) * w[s, :] + reset * w_0[s, :]

            # Update confidence: accumulate rewards and softly decay
            confidence *= (1.0 - q_forget)
            confidence[s] += r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Gated RL-WM hybrid: WM-gated Q bonus + load-dependent WM noise and WM learning rate.

    Idea:
    - Two policies combined by mixture, but additionally WM "gates" RL by adding a bonus to the RL value
      of the action currently preferred by WM (argmax in W_s).
    - WM precision decreases with set size (load), implemented as a reduction of the WM inverse temperature.
    - WM encoding rate is a free parameter; on reward, W encodes the chosen action, otherwise drifts to uniform.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: WM mixture weight in [0,1].
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - wm_noise_load: non-negative; higher values reduce WM precision more at larger set sizes.
    - wm_encode: WM supervised encoding step size in [0,1].
    - gate_bonus: non-negative; additive bonus to RL Q for WMâ€™s currently preferred action.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_noise_load, wm_encode, gate_bonus = model_parameters
    softmax_beta *= 10.0
    base_softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM temperature scaling
        load = max(0, nS - 3)
        softmax_beta_wm = base_softmax_beta_wm / (1.0 + wm_noise_load * (1.0 + load))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # WM policy and preferred action
            W_s = w[s, :]
            # If WM is flat, argmax is stable due to initialization; OK for gating
            wm_pref = int(np.argmax(W_s))

            # RL policy with WM-gated bonus on WM-preferred action
            Q_s = q[s, :].copy()
            if gate_bonus > 0.0:
                Q_s[wm_pref] += gate_bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy softmax with load-reduced precision
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: supervised encoding with tunable rate; otherwise drift to uniform
            if r > 0.5:
                # Move W_s toward one-hot on chosen action by wm_encode
                w[s, :] = (1.0 - wm_encode) * w[s, :]
                w[s, a] += wm_encode
            else:
                # Drift back toward uniform slightly on errors
                drift = 0.25 * (1.0 / (1.0 + load))  # slightly less drift when load is high
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on load effects:
- Model 1: WM interference/decay increases with set size via interference_rate and load.
- Model 2: Load is implicit through the RL forgetting and confidence dynamics; no direct load term, making it a useful contrast.
- Model 3: WM precision decreases with set size by lowering the WM inverse temperature using wm_noise_load; also WM drift on errors is slightly modulated by load.