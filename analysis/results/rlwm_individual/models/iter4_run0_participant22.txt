def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Error-driven WM gating with set-size scaling.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM contribution is down-weighted in larger set sizes and WM updates are gated by "surprise":
      larger unsigned prediction errors make WM more likely to store the just-experienced (state, action)
      association, especially when reward is received.
    - RL learns via a standard delta rule.

    Parameters
    ----------
    parameters : tuple/list of length 4
        lr : float in [0,1]
            RL learning rate. Also used as a small WM decay rate toward the prior.
        wm_weight : float in [0,1]
            Baseline WM mixture weight applied at set size 3; scaled by (3/nS) for arbitration.
        softmax_beta : float >= 0
            RL inverse temperature; internally scaled by 10. WM has fixed high temperature.
        gate_sensitivity : float >= 0
            Controls the steepness of the logistic gating on unsigned prediction error for WM storage.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Set-size impact
    ---------------
    - WM mixture weight is scaled by (3/nS), reducing WM influence in larger sets.
    - Because RL errors tend to be larger in larger sets early on, gating can transiently boost WM updates,
      but arbitration still limits their influence when nS is large.
    """
    lr, wm_weight, softmax_beta, gate_sensitivity = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy for chosen action: near-deterministic softmax over W
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            # Arbitration: scale WM by set-size
            wm_eff = wm_weight * (3.0 / max(1.0, float(nS)))
            wm_eff = min(max(wm_eff, 0.0), 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - Always decay slightly toward prior
            w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # - Error-driven gating for storage, emphasizing rewarded trials
            #   Gate on unsigned PE around a 0.5 reference scale
            gate = 1.0 / (1.0 + np.exp(-gate_sensitivity * (abs(delta) - 0.5)))
            if r > 0.5:
                w[s, a] += gate  # store strongly when rewarded and surprising
            else:
                # weak storage on non-reward to avoid locking in wrong mapping
                w[s, a] += 0.2 * gate

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    Asymmetric RL + WM with set-size-dependent lapse.

    Idea:
    - Choices are a mixture of RL and WM, with an additional lapse process that increases with set size.
    - RL has asymmetric learning rates for positive vs. negative prediction errors.
    - WM is updated on every trial (recency-based), with a smaller boost when the outcome is negative.

    Parameters
    ----------
    parameters : tuple/list of length 5
        lr_pos : float in [0,1]
            RL learning rate for positive prediction errors (better-than-expected).
        wm_weight : float in [0,1]
            Baseline WM mixture weight at set size 3; scaled by (3/nS).
        softmax_beta : float >= 0
            RL inverse temperature; internally scaled by 10.
        lr_neg : float in [0,1]
            RL learning rate for negative prediction errors (worse-than-expected).
        lapse_k : float (can be negative or positive)
            Controls how lapse probability grows with set size (higher -> more lapses for larger nS).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Set-size impact
    ---------------
    - WM mixture is scaled by (3/nS), reducing WM reliance in larger sets.
    - Lapse probability increases with set size via a logistic function, further degrading performance at nS=6.
    """
    lr_pos, wm_weight, softmax_beta, lr_neg, lapse_k = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent lapse probability (small for nS=3, larger for nS=6)
        # Centered at 4.5 so that nS=3 < 0.5, nS=6 > 0.5 in the logistic argument
        lapse = 1.0 / (1.0 + np.exp(-lapse_k * (float(nS) - 4.5)))
        lapse = min(max(lapse, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            # Arbitration: WM down-weighted with set size
            wm_eff = wm_weight * (3.0 / max(1.0, float(nS)))
            wm_eff = min(max(wm_eff, 0.0), 1.0)
            p_mix = wm_eff * p_wm + (1.0 - wm_eff) * p_rl

            # Lapse mixture to uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / 3.0)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            if delta >= 0.0:
                q[s][a] += lr_pos * delta
            else:
                q[s][a] += lr_neg * delta

            # WM update: recency-based with reward modulation
            # Decay toward prior mildly proportional to the average of the two learning rates
            lr_avg = 0.5 * (lr_pos + lr_neg)
            w[s, :] = (1.0 - lr_avg) * w[s, :] + lr_avg * w_0[s, :]
            # Boost chosen action; stronger if rewarded
            w[s, a] += (1.0 if r > 0.5 else 0.3)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Dynamic arbitration from surprise with WM decay and size penalty.

    Idea:
    - Arbitration weight for WM is dynamic and depends on:
        - Base WM weight (at set size 3),
        - Trial-by-trial unsigned RL prediction error (surprise),
        - A linear penalty with set size.
      Combined via a logistic transform to remain in [0,1].
    - WM decays toward a prior and stores rewarded actions by boosting chosen and inhibiting alternatives.
    - RL updates via a standard delta rule.

    Parameters
    ----------
    parameters : tuple/list of length 6
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in (0,1)
            Baseline WM weight that is transformed via logit for arbitration; interpretable at nS=3 and zero surprise.
        softmax_beta : float >= 0
            RL inverse temperature; internally scaled by 10.
        wm_decay : float in [0,1]
            Per-visit decay of WM toward the uniform prior.
        surprise_slope : float >= 0
            Sensitivity of arbitration to unsigned prediction error magnitude.
        size_penalty : float >= 0
            Linear penalty term applied per additional item beyond 3 in the set (reduces WM reliance when nS increases).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Set-size impact
    ---------------
    - WM arbitration is reduced by size_penalty * (nS - 3).
    - Larger sets tend to yield larger early PEs, which can transiently increase WM arbitration via surprise_slope,
      but the size penalty keeps overall WM influence smaller for nS=6.
    """
    lr, wm_weight, softmax_beta, wm_decay, surprise_slope, size_penalty = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy probability for the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy for the chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            # Compute current PE for arbitration and updates
            delta = r - Q_s[a]

            # Dynamic arbitration via a logistic function
            eps = 1e-12
            base_logit = np.log(max(wm_weight, eps)) - np.log(max(1.0 - wm_weight, eps))
            size_term = - size_penalty * (float(nS) - 3.0)
            surprise_term = surprise_slope * abs(delta)
            wm_eff = 1.0 / (1.0 + np.exp(-(base_logit + size_term + surprise_term)))
            wm_eff = min(max(wm_eff, 0.0), 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s][a] += lr * delta

            # WM update: decay toward prior, then reward-contingent boost and inhibition
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # strengthen chosen action and inhibit others
                w[s, a] += 1.0
                others = [aa for aa in range(nA) if aa != a]
                for aa in others:
                    w[s, aa] -= 0.5
            else:
                # on errors, only mild recency push to the chosen action
                w[s, a] += 0.2

        blocks_log_p += log_p

    return -blocks_log_p