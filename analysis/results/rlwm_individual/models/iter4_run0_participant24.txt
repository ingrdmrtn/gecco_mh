def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated arbitration and set-size bias.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10 internally).
    - WM system: softmax over W with WM-specific inverse temperature wm_beta.
    - Arbitration: trial-wise wm_weight_t derived from RL uncertainty (Shannon entropy of the RL policy at the current state).
      A set-size-specific bias shifts arbitration under higher load.

    WM mechanisms:
    - Passive decay toward uniform template w_0 at each trial with rate wm_decay.
    - Rewarded strengthening (Hebbian) toward a one-hot for the chosen action; small anti-Hebbian on errors.

    Parameters:
    - lr: RL learning rate for Q updates; also used to scale WM strengthening.
    - softmax_beta: RL inverse temperature (rescaled by 10 inside the function).
    - wm_beta: WM inverse temperature (determinism of WM policy).
    - wm_decay: Passive decay rate of WM towards uniform each trial (0..1).
    - wm_gain: Gain mapping RL uncertainty (entropy) to arbitration weight (higher gain -> more WM when RL is uncertain).
    - bias6: Additional bias added to the arbitration when set size is 6 (typically negative to down-weight WM at high load).

    Set-size effect:
    - Arbitration gets an additive bias when nS == 6 (bias6), shifting reliance away/towards WM under load.
    - Indirectly, larger sets typically produce higher RL uncertainty early, increasing or decreasing WM usage depending on wm_gain and bias.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_beta, wm_decay, wm_gain, bias6 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = float(wm_beta)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        size_bias = 0.0 if nS == 3 else float(bias6)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of the observed action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL policy to estimate uncertainty (entropy)
            # Softmax with the same temperature used for p_rl computation
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            pi_rl = exp_logits / np.sum(exp_logits + 1e-12)
            entropy_rl = -np.sum(pi_rl * np.log(np.maximum(pi_rl, 1e-12)))

            # WM choice probability for action a
            p_wm_base = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-gated arbitration weight (sigmoid of entropy + set-size bias)
            # Normalize entropy to approx [0, ln(3)] -> center near 1.0 for stability
            wm_weight_t = 1.0 / (1.0 + np.exp(-(wm_gain * (entropy_rl - 1.0) + size_bias)))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm_base + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM learning: reward strengthens chosen association; small weakening on errors
            if r > 0.0:
                w[s, a] += lr * (1.0 - w[s, a])
            else:
                w[s, a] -= 0.25 * lr * w[s, a]

            # Keep WM values in [0,1]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with probabilistic WM encoding that depends on set size.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with WM inverse temperature wm_beta.
    - Arbitration: fixed wm_weight, but effective WM contribution is reduced by the probability
      that the item is encoded in WM, which depends on set size (encode3 vs encode6).

    WM mechanisms:
    - On rewarded trials, WM strengthens the chosen state-action association proportionally to the encoding probability.
    - No explicit WM decay parameter here; instead, limited encoding probability keeps WM from saturating.
      (Note: WM still starts from uniform and only strengthens when encoded and rewarded.)

    Parameters:
    - lr: RL learning rate for Q updates; also scales WM strengthening on rewarded trials.
    - softmax_beta: RL inverse temperature (rescaled by 10 inside the function).
    - wm_beta: WM inverse temperature (determinism of WM policy).
    - wm_weight: Base arbitration weight for WM contribution when encoded.
    - encode3: Probability (0..1) that a 3-set item is encoded/available in WM on a given trial.
    - encode6: Probability (0..1) that a 6-set item is encoded/available in WM on a given trial.

    Set-size effect:
    - WM contribution is directly reduced by encode6 relative to encode3 under high load.
    - WM learning strength on rewarded trials is also scaled by the set-size-specific encoding probability.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_beta, wm_weight, encode3, encode6 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = float(wm_beta)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # not explicitly used, but kept for structural parity

        enc_prob = float(encode3) if nS == 3 else float(encode6)
        enc_prob = np.clip(enc_prob, 0.0, 1.0)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)
        eff_wm_weight = wm_weight * enc_prob

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration with effective WM weight reduced by encoding probability
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM learning only when reward is present; scaled by encoding probability
            if r > 0.0:
                w[s, a] += (lr * enc_prob) * (1.0 - w[s, a])
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with prediction-error-adaptive learning rate + WM with set-size-dependent interference.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with high determinism (fixed 50).
    - Arbitration: fixed wm_weight blending WM and RL.

    WM mechanisms:
    - Passive decay toward uniform w_0 at rate wm_decay.
    - Rewarded strengthening toward one-hot for the chosen action.
    - Interference under higher load: WM policy is mixed with uniform by an amount that grows with set size.

    RL mechanisms:
    - Learning rate adapts to unsigned prediction error: lr_t = lr0 + (lr_max - lr0) * |PE|.
      This increases learning when outcomes are surprising.

    Parameters:
    - lr0: Baseline RL learning rate (lower bound).
    - lr_max: Max RL learning rate when |PE| = 1.
    - softmax_beta: RL inverse temperature (rescaled by 10 inside the function).
    - wm_weight: Arbitration weight on WM (0..1).
    - wm_decay: Passive decay rate of WM toward uniform (0..1).
    - interference_slope: Scales WM interference with set size: interference = max(0, interference_slope * (nS - 3) / 3).

    Set-size effect:
    - WM interference increases when moving from set size 3 to 6 via interference_slope.
    - RL dynamics are global but interact with set size via resulting WM blend.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr0, lr_max, softmax_beta, wm_weight, wm_decay, interference_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent interference level for WM policy
        interference = max(0.0, float(interference_slope) * (nS - 3) / 3.0)
        interference = min(interference, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference toward uniform at higher set sizes
            p_wm_base = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - interference) * p_wm_base + interference * (1.0 / nA)

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with PE-adaptive learning rate
            pe = r - Q_s[a]
            lr_t = lr0 + (lr_max - lr0) * abs(pe)
            lr_t = np.clip(lr_t, 0.0, 1.0)
            q[s, a] += lr_t * pe

            # WM decay and learning
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.0:
                w[s, a] += lr0 * (1.0 - w[s, a])  # use lr0 as WM strengthening speed
            else:
                w[s, a] -= 0.25 * lr0 * w[s, a]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p