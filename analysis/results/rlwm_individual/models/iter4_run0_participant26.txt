def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with asymmetric RL learning and capacity-weighted WM.
    - RL: Asymmetric learning rates for positive vs negative RPEs.
    - WM: Cached action per state with leak; rewarded choices are stored,
           errors weaken the chosen action in WM.
    - Arbitration: Fixed WM weight scaled down by set size (capacity cost).

    Parameters (tuple):
    - alpha_pos: RL learning rate for positive RPEs (0..1).
    - alpha_neg: RL learning rate for negative RPEs (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 for range.
    - wm_weight_base: Baseline WM mixture weight in [0,1].
    - wm_leak: WM leak toward uniform on each visit (0..1).
    - ss_cost: Nonnegative scaling of WM degradation with larger set sizes.
               Effective WM weight = wm_weight_base / (1 + ss_cost*(nS-3)).

    Set-size impacts:
    - The WM weight is explicitly degraded with larger set sizes via ss_cost.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, wm_leak, ss_cost = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_weight_base / (1.0 + ss_cost * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM weights)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0 else alpha_neg
            q[s, a] += alpha * delta

            # WM decay toward uniform on each visit
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # WM update: rewarded actions strengthened; errors weaken chosen
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - alpha_pos) * w[s, :] + alpha_pos * onehot
            else:
                # Down-weight the chosen action and renormalize to maintain a distribution
                w[s, a] = max(0.0, (1.0 - alpha_neg) * w[s, a])
                total = np.sum(w[s, :])
                if total > 0:
                    w[s, :] /= total
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM arbitration driven by RL uncertainty (entropy) and set size.
    - RL: Softmax policy with learning rate lr.
    - WM: Deterministic cache attracted to last rewarded action; decays otherwise.
    - Arbitration: Trial-wise WM weight is a logistic function of RL entropy
      (higher entropy -> more WM) penalized by larger set size.

    Parameters (tuple):
    - lr: RL learning rate (0..1) and WM storage step size after reward.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_beta: WM inverse temperature (higher -> more deterministic WM).
    - wm_base: Baseline bias toward WM in the logistic arbitration.
    - k_uncertainty: Sensitivity of WM weight to RL entropy (>=0).
    - ss_slope: Penalty on WM weight with larger set sizes (>=0).

    Set-size impacts:
    - The WM weight decreases with set size through ss_slope*(nS-3).
    """
    lr, softmax_beta, wm_beta, wm_base, k_uncertainty, ss_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = float(wm_beta)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax probabilities for entropy
            Q_s = q[s, :]
            # Stable softmax
            z = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * z)
            pi_rl = pi_rl / np.sum(pi_rl)
            # Entropy (natural log)
            H_rl = -np.sum(pi_rl * (np.log(pi_rl + 1e-12)))

            # WM policy distribution from current WM map
            W_s = w[s, :]
            z_w = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * z_w)
            pi_wm = pi_wm / np.sum(pi_wm)

            # Arbitration weight: logistic of (wm_base + k_unc*H - ss_slope*(nS-3))
            H0 = np.log(nA)  # maximum entropy for reference
            x = wm_base + k_uncertainty * (H_rl - 0.5 * H0) - ss_slope * max(0, nS - 3)
            wm_weight_t = 1.0 / (1.0 + np.exp(-x))

            # Choice probability
            p_rl = pi_rl[a]
            p_wm = pi_wm[a]
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Reward-gated WM storage: on reward, move toward one-hot
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with novelty bonus + WM recency trace mixture.
    - RL: Standard delta-rule; policy includes a novelty bonus that is larger
          for rarely chosen actions and is attenuated by larger set sizes.
    - WM: Recency-weighted action histogram per state (eligibility-like trace).
    - Arbitration: Fixed mixture weight.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - lambda_recency: Recency parameter for WM traces (0..1); higher -> longer memory.
    - novelty: Magnitude of RL novelty bonus (>0).
    - wm_forget: Per-visit forgetting of WM traces (0..1).

    Set-size impacts:
    - Novelty bonus is scaled by 3/nS, reducing its impact in larger set sizes.
    """
    lr, softmax_beta, wm_weight, lambda_recency, novelty, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM represented via trace z; w will hold normalized distribution for policy
        z = np.ones((nS, nA)) / nA
        w = np.copy(z)
        w_0 = np.ones((nS, nA)) / nA

        # Visit counts for novelty computation
        counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute novelty bonus scaled by set size
            bonus_scale = 3.0 / float(nS)
            bonus_vec = novelty * bonus_scale / (1.0 + counts[s, :])

            # RL policy uses Q plus novelty bonus
            Q_s_base = q[s, :]
            Q_s = Q_s_base + bonus_vec

            # WM policy from normalized trace w[s]
            W_s = w[s, :]

            # Compute RL and WM choice probabilities
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (no bonus in learning, only in policy)
            delta = r - Q_s_base[a]
            q[s, a] += lr * delta

            # Update counts for novelty
            counts[s, a] += 1.0

            # WM trace update:
            # - global forgetting toward zero
            z[s, :] = (1.0 - wm_forget) * z[s, :]
            # - eligibility-like recency update
            z[s, :] *= lambda_recency
            z[s, a] += 1.0
            # - normalize to get a distribution for policy
            total = np.sum(z[s, :])
            if total > 0:
                w[s, :] = z[s, :] / total
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p