def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent learning rate + episodic WM write (confidence-controlled).

    Idea:
    - RL learns state-action values with a learning rate that decreases with set size
      (to reflect reduced effective learning under higher load).
    - WM stores the most recently rewarded action for a state with a confidence-controlled
      overwrite and decays toward uniform otherwise.
    - Policy is a mixture of WM and RL, with a fixed WM weight parameter.
    
    Parameters (model_parameters):
    - lr:           Base RL learning rate (0..1)
    - wm_weight:    Mixture weight of WM policy (0..1)
    - softmax_beta: Base RL inverse temperature (scaled internally by *10)
    - lr_load:      Load penalty on RL learning rate; lr_eff = lr / (1 + lr_load*(nS-3))
    - wm_decay:     WM decay toward uniform on each visit (0..1)
    - wm_conf:      WM confidence for overwriting on reward (0..1), higher -> stronger one-hot write
    
    Set-size impact:
    - RL learning rate decreases with set size via lr_load.
    - WM policy is not directly load-modulated here; the contrast isolates the RL-side load impact.
    """
    lr, wm_weight, softmax_beta, lr_load, wm_decay, wm_conf = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-adjusted learning rate
        lr_eff = lr / (1.0 + max(0.0, float(lr_load) * float(nS - 3)))
        lr_eff = max(0.0, min(1.0, lr_eff))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy: softmax on WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1.0 - wm_weight)*p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr_eff * delta

            # WM updating: decay toward uniform; on reward, confidence-weighted one-hot write
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_conf) * w[s, :] + wm_conf * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent WM decay and loss-driven inhibition (WSLS-like WM dynamics).

    Idea:
    - RL is standard delta-rule.
    - WM decays toward uniform with a decay rate that increases with set size (more interference).
    - When rewarded, WM writes a one-hot for the chosen action (win-store).
    - When not rewarded, WM inhibits the chosen action (lose-inhibit) by moving probability away
      from that action toward other actions in WM.
    - Policy is a mixture with a fixed WM weight.

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - wm_weight:     Mixture weight of WM policy (0..1)
    - softmax_beta:  Base RL inverse temperature (scaled internally by *10)
    - wm_base_decay: Base WM decay (0..1) at set size 3
    - wm_load_scale: Additional WM decay per +3 items of set size; wm_decay_eff = clamp(wm_base_decay + wm_load_scale*(nS-3), 0, 1)
    - loss_inhibit:  Strength of WM inhibition for un-rewarded chosen action (0..1)

    Set-size impact:
    - WM decay increases with set size via wm_load_scale, implementing load-based interference in WM.
    """
    lr, wm_weight, softmax_beta, wm_base_decay, wm_load_scale, loss_inhibit = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        wm_decay_eff = wm_base_decay + max(0.0, float(wm_load_scale)) * float(nS - 3)
        wm_decay_eff = max(0.0, min(1.0, wm_decay_eff))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy: softmax on WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1.0 - wm_weight)*p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating:
            # 1) decay toward uniform with load-dependent decay
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            if r > 0.5:
                # Win: store the chosen action as a one-hot
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Loss: inhibit the chosen action in WM by shifting probability to others
                redistribute = np.ones(nA)
                redistribute[a] = 0.0
                if redistribute.sum() > 0:
                    redistribute = redistribute / redistribute.sum()
                w[s, :] = (1.0 - loss_inhibit) * w[s, :]
                w[s, :] += loss_inhibit * redistribute

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent forgetting + adaptive WM mixture weight driven by recent RPE.

    Idea:
    - RL uses a standard learning rate, but Q-values undergo forgetting toward uniform that
      increases with set size (to reflect larger interference under load).
    - WM is standard: decay toward uniform; on reward, write a one-hot for the chosen action.
    - Mixture weight of WM vs RL is adapted online based on the magnitude of the recent RPE:
      higher absolute RPE -> rely more on RL; lower absolute RPE -> rely more on WM.
      wm_eff = sigmoid(wm_bias - rpe_sens * |delta|), with wm_bias scaled by set size.

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - wm_bias:       Bias term for WM mixture (higher -> more WM); transformed by set size
    - softmax_beta:  Base RL inverse temperature (scaled internally by *10)
    - rpe_sens:      Sensitivity of WM weight to absolute RPE (>=0)
    - rl_forget:     Base RL forgetting rate toward uniform (0..1) at set size 3
    - forget_load:   Load multiplier on forgetting per +3 items; forget_eff = clamp(rl_forget*(1 + forget_load*(nS-3)), 0, 1)

    Set-size impact:
    - RL forgetting increases with set size (forget_eff).
    - WM mixture bias is reduced with set size: wm_bias_eff = wm_bias / (1 + (nS-3)).
    """
    lr, wm_bias, softmax_beta, rpe_sens, rl_forget, forget_load = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        forget_eff = rl_forget * (1.0 + max(0.0, float(forget_load)) * float(nS - 3))
        forget_eff = max(0.0, min(1.0, forget_eff))

        wm_bias_eff = wm_bias / (1.0 + float(max(0, nS - 3)))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize last absolute RPE per state for adaptive mixture; start neutral
        last_abs_rpe = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # WM policy: softmax on WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Adaptive mixture weight based on last abs RPE for this state
            wm_eff = sigmoid(wm_bias_eff - rpe_sens * last_abs_rpe[s])

            p_total = wm_eff*p_wm + (1.0 - wm_eff)*p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # RL forgetting toward uniform (per visit), stronger under load
            q[s, :] = (1.0 - forget_eff) * q[s, :] + (forget_eff) * (1.0 / nA)

            # WM updating: decay to uniform; on reward, write one-hot
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # mild base decay every visit
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update recent absolute RPE trace for mixture adaptation
            last_abs_rpe[s] = abs(delta)

        blocks_log_p += log_p

    return -blocks_log_p