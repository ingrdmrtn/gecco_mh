def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated arbitration and set-size–dependent WM decay.

    Idea
    ----
    - RL system: standard Q-learning with softmax policy.
    - WM system: fast-learning item-based values per state that decay toward uniform.
      Decay accelerates with larger set size (more interference).
    - Arbitration: the weight on WM is dynamically gated by RL uncertainty (entropy)
      and penalized by set size; higher RL entropy increases reliance on WM, larger
      set sizes reduce it.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of float (0/1)
        Reward feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_alpha : float in (0,1)
            WM learning rate for the chosen action (fast encoding).
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled internally).
        wm_decay : float >= 0
            Baseline decay rate of WM toward uniform each trial (higher = faster decay).
        gate_bias : float
            Bias term in the logistic arbitration toward WM.
        setsize_penalty : float >= 0
            Linear penalty of WM arbitration with larger set size.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Notes on set size impact
    ------------------------
    - WM decay rate increases with set size via: d_t = 1 - exp(-wm_decay * (1 + setsize_penalty*(set_size-3))).
    - Arbitration weight uses a logistic transform of RL entropy and a negative term
      proportional to (set_size-3), reducing WM reliance for larger sets.
    """
    lr, wm_alpha, softmax_beta, wm_decay, gate_bias, setsize_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            set_size_t = int(block_set_sizes[t])

            # RL policy: compute full softmax to obtain entropy and chosen prob
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            # RL uncertainty (entropy)
            H_rl = -np.sum(pi_rl * (np.log(pi_rl + 1e-12)))

            # WM policy: softmax over WM values (very sharp)
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = pi_wm[a]

            # Arbitration: logistic of entropy, penalized by set size
            gate_input = gate_bias + H_rl - setsize_penalty * (set_size_t - 3)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform; faster decay with larger set size
            decay_t = 1.0 - np.exp(-wm_decay * (1.0 + setsize_penalty * (set_size_t - 3)))
            w[s, :] = (1.0 - decay_t) * w[s, :] + decay_t * w_0[s, :]

            # WM fast encoding on chosen action
            pe_wm = r - W_s[a]
            w[s, a] += wm_alpha * pe_wm

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with perseveration + capacity-limited WM store (LRU) and set-size–scaled WM mixture.

    Idea
    ----
    - RL system: Q-learning with softmax; includes action perseveration (stickiness).
    - WM system: slot-based memory of rewarded state-action pairs (capacity K).
      If a state is in WM, the policy is near-deterministic to its stored action,
      with lapses to uniform.
    - Arbitration: base WM weight scaled by relative capacity usage (K / set_size).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of float (0/1)
        Reward feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled internally).
        wm_weight_base : float in [0,1]
            Base mixture weight on WM (before capacity scaling).
        K_slots : float >= 0
            Effective WM capacity (can be non-integer; scaling uses K/set_size).
        lapse_rate : float in [0,1]
            Probability that WM outputs uniform noise instead of the stored action.
        beta_pers : float
            Perseveration strength added to the value of the previously chosen action.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Notes on set size impact
    ------------------------
    - Effective WM mixture is wm_weight_base * min(1, K_slots / set_size).
      Larger set sizes dilute the influence of WM when capacity is limited.
    """
    lr, softmax_beta, wm_weight_base, K_slots, lapse_rate, beta_pers = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM storage: whether a state is stored and its stored action
        in_mem = np.zeros(nS, dtype=bool)
        mem_act = np.zeros(nS, dtype=int)
        # LRU queue of states currently in memory
        lru_list = []

        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # RL policy with perseveration bias toward previous action
            Q_s = q[s, :].copy()
            if prev_action is not None:
                Q_s[prev_action] += beta_pers

            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            # WM policy
            if in_mem[s]:
                # near-deterministic on stored action with lapses to uniform
                p_wm_vec = (lapse_rate) * w_0[s, :].copy()
                p_wm_vec[mem_act[s]] += (1.0 - lapse_rate)
                p_wm = p_wm_vec[a]
            else:
                # if not in WM, policy is uniform (no information)
                p_wm = w_0[s, a]

            # Arbitration: scale WM weight by capacity fraction
            cap_frac = min(1.0, K_slots / max(1.0, float(set_size_t)))
            wm_weight_eff = np.clip(wm_weight_base * cap_frac, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM "values" are not used for policy directly, but keep w updated to follow template
            # Encode rewarded associations into WM with LRU capacity
            if r >= 0.5:
                # store state s with action a
                if not in_mem[s]:
                    # add to memory, respect capacity by evicting oldest
                    in_mem[s] = True
                    mem_act[s] = a
                    lru_list.append(s)
                    # evict if capacity exceeded; capacity is the integer floor of K_slots
                    while len(lru_list) > int(max(0, np.floor(K_slots))):
                        evict_s = lru_list.pop(0)
                        if evict_s != s:
                            in_mem[evict_s] = False
                else:
                    # refresh recency and update action
                    mem_act[s] = a
                    if s in lru_list:
                        lru_list.remove(s)
                    lru_list.append(s)
                # reflect in w as a near one-hot with lapses (for completeness)
                w[s, :] = (lapse_rate) * w_0[s, :]
                w[s, mem_act[s]] += (1.0 - lapse_rate)
            else:
                # if incorrect and the stored action matches the wrong action, remove it
                if in_mem[s] and mem_act[s] == a:
                    in_mem[s] = False
                    if s in lru_list:
                        lru_list.remove(s)
                    w[s, :] = w_0[s, :]

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with swap/interference errors increasing with set size.

    Idea
    ----
    - RL system: Q-learning with softmax choice.
    - WM system: fast item-based encoding of the best action per state.
      Retrieval is noisy due to swap/interference: with a set-size–dependent probability,
      WM produces an action from another memorized state (modeled as mass spread over
      the two non-stored actions).
    - Arbitration: fixed WM weight (learned), WM learning rate controls how quickly
      WM locks onto rewarded actions.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of float (0/1)
        Reward feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_alpha : float in (0,1)
            WM learning rate for chosen action (fast encoding).
        softmax_beta : float >= 0
            Inverse-temperature for RL softmax (scaled internally).
        wm_weight : float in [0,1]
            Mixture weight on WM policy (constant across trials).
        swap_base : float in [0,1]
            Baseline probability of WM swap/interference at set size 3.
        swap_slope : float >= 0
            Increment in swap probability per +1 item in set size:
            swap_prob = clip(swap_base + swap_slope*(set_size - 3), 0, 1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.

    Notes on set size impact
    ------------------------
    - Swap/interference probability increases linearly with set size, degrading WM policy.
    """
    lr, wm_alpha, softmax_beta, wm_weight, swap_base, swap_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track whether we have a stored action per state (for constructing WM policy)
        has_mem = np.zeros(nS, dtype=bool)
        mem_act = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            # WM policy with swap/interference
            if has_mem[s]:
                stored = int(mem_act[s])
                swap_prob = np.clip(swap_base + swap_slope * (set_size_t - 3), 0.0, 1.0)
                p_wm_vec = np.full(3, swap_prob / (nA - 1))
                p_wm_vec[stored] = 1.0 - swap_prob
                p_wm = p_wm_vec[a]
            else:
                # if nothing is stored for this state, WM offers no information (uniform)
                p_wm = w_0[s, a]

            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM value updating: fast encoding on reward, mild extinction on error
            # Use w as value-like store (also maintain a binary memory flag)
            if r >= 0.5:
                has_mem[s] = True
                mem_act[s] = a
                # sharpen WM value for stored action
                W_s = w[s, :]
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]  # mild relaxation
                w[s, a] += wm_alpha * (1.0 - w[s, a])
            else:
                # if wrong and WM believes this action, weaken it slightly
                if has_mem[s] and mem_act[s] == a:
                    # reduce confidence
                    w[s, a] += wm_alpha * (0.0 - w[s, a])
                    # optional: keep the flag; interference modeled in policy via swap_prob

        blocks_log_p += log_p

    return -blocks_log_p