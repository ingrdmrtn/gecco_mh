def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + decaying WM with set-size–dependent leak and noise.

    Mechanism
    - RL: tabular Q-learning with a single learning rate.
    - WM store: one-hot cache of the last rewarded action per state, but with
      trial-by-trial leak toward uniform and additional choice noise. Both
      leak and noise increase with set size (6 > 3), capturing load effects.
    - Arbitration: fixed mixture between WM and RL policies.

    Parameters
    - lr: learning rate for RL Q-updates (0..1).
    - wm_weight: mixture weight for WM policy in the final choice (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>0).
    - wm_decay0: baseline WM leak toward uniform per trial (0..1).
    - wm_noise: baseline WM choice noise mixing with uniform (0..1).
    - ss_decay_mult: multiplicative exponent controlling how much leak/noise
      grow with set size; larger values amplify differences between 3 vs 6.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay0, wm_noise, ss_decay_mult = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent leak and noise (larger for nS=6 than for nS=3)
        scale_ss = (nS / 3.0) ** ss_decay_mult
        decay_eff = np.clip(wm_decay0 * scale_ss, 0.0, 1.0)
        noise_eff = np.clip(wm_noise * scale_ss, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given by the template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute a sharp softmax from WM map, then inject set-size–dependent noise.
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = (1.0 - noise_eff) * pi_wm[a] + noise_eff * (1.0 / nA)

            # Mixture policy and log-likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rewarded actions overwrite WM for that state; then global leak toward uniform.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            # Leak toward uniform (larger leak when set size is larger)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + capacity-limited WM with probabilistic encoding and leak.

    Mechanism
    - RL: tabular Q-learning with a single learning rate.
    - WM store: when reward is received, the correct action is encoded into WM for that state.
      Encoding is probabilistic and capacity-limited: p_enc = min(1, C / set_size).
      WM decays (leaks) toward uniform each trial.
    - Arbitration: fixed mixture between WM and RL policies; effective WM policy is attenuated
      by encoding probability, yielding a load effect without changing the explicit mixture weight.

    Parameters
    - lr: learning rate for RL Q-updates (0..1).
    - wm_weight: mixture weight for WM policy in the final choice (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>0).
    - capacity_C: effective WM capacity (in number of state–action pairs; >0).
    - wm_leak: WM leak toward uniform per trial (0..1).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_C, wm_leak = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent encoding probability (expected, used both in policy and update)
        p_enc = np.clip(capacity_C / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given by the template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM softmax, attenuated by probability of successful encoding.
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            # If the state is not encoded (with prob 1-p_enc), WM defaults to uniform.
            p_wm = p_enc * pi_wm[a] + (1.0 - p_enc) * (1.0 / nA)

            # Mixture policy and log-likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Expected (non-stochastic) encoding update plus global leak toward uniform.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Expected update: move a fraction p_enc toward the one-hot
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * one_hot
            # Leak toward uniform regardless of reward
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Bayesian-like WM counts with set-size–dependent WM reliability and lapse.

    Mechanism
    - RL: tabular Q-learning with a single learning rate.
    - WM store: accumulates 'evidence' counts for actions per state when rewarded; the WM policy
      is derived from normalized counts with a prior concentration (wm_conc0).
    - WM reliability: when set size is larger, WM policy is diluted toward uniform in proportion
      to a slope parameter. Additionally, a WM-specific lapse mixes WM policy with uniform.
    - Arbitration: fixed mixture between WM and RL policies; WM policy itself embeds the
      set-size effect and lapse.

    Parameters
    - lr: learning rate for RL Q-updates (0..1).
    - wm_weight: mixture weight for WM policy in the final choice (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>0).
    - wm_conc0: prior concentration added to WM counts to prevent overconfidence (>0).
    - ss_weight_slope: slope controlling how much WM policy is diluted as set size increases
      (higher => stronger dilution when nS > 3; can be negative to reduce dilution).
    - lapse: WM-specific lapse rate mixing with uniform (0..1).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_conc0, ss_weight_slope, lapse = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # w will store accumulated evidence (counts-like, starting near uniform)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent dilution factor toward uniform for WM policy
        # Map slope to [0,1] via tanh and scale by relative size (0 for nS=3, >0 for nS=6)
        size_term = max(0.0, (nS - 3.0) / 3.0)
        dilate = np.clip(0.5 * (1.0 + np.tanh(ss_weight_slope)) * size_term, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as given by the template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Convert accumulated evidence + prior into a WM policy, then sharpen and dilute.
            counts = np.maximum(W_s, 0.0) + (wm_conc0 / nA)
            counts /= np.sum(counts)
            z_wm = softmax_beta_wm * (counts - np.max(counts))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            # Dilute by set size and include WM-specific lapse
            pi_wm_diluted = (1.0 - dilate) * pi_wm + dilate * (1.0 / nA)
            p_wm = (1.0 - lapse) * pi_wm_diluted[a] + lapse * (1.0 / nA)

            # Mixture policy and log-likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward increases evidence for the chosen action (accumulation).
            # No explicit decay term; dilution is handled in the policy level.
            if r > 0.0:
                w[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p