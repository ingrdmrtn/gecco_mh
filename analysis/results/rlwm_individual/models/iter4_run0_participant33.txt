Below are three standalone cognitive models that instantiate different RL+WM mechanisms while adhering to your template. Each returns the negative log-likelihood of the observed choices, uses all parameters meaningfully, and includes a clear docstring noting set-size effects. I avoid parameter combinations you listed as already tried.

Note: Assume numpy (np) is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with set-size-modulated epsilon-greedy lapses and WM decay toward uniform.

    Mechanism
    ---------
    - RL: standard delta rule with inverse temperature softmax.
    - WM: fast, near-deterministic softmax over a WM value table W that decays toward uniform
      on each visit (capturing memory drift/interference).
    - Arbitration: convex combination of WM and RL with a constant gate controlled by wm_weight_base.
    - Lapses: final mixture with a uniform policy via epsilon-greedy; epsilon increases with set size.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        wm_weight_base : float
            Logistic parameter, transformed to [0,1] gate weight for WM over RL.
        softmax_beta : float
            Inverse temperature for RL policy, scaled internally by 10.
        epsilon_base : float
            Baseline lapse rate (logit space), mixed with uniform at the policy output.
        epsilon_ss_slope : float
            Sensitivity of lapse to set size; positive => larger set size increases lapses.
        wm_decay_per_step : float
            Per-visit decay of WM values toward uniform (0..1). Larger => faster WM drift.

    Set-size effects
    ----------------
    - Lapse rate epsilon depends on set size via: sigmoid(epsilon_base + epsilon_ss_slope*(nS-3)).
      Thus larger sets can induce more random responding.
    """
    lr, wm_weight_base, softmax_beta, epsilon_base, epsilon_ss_slope, wm_decay_per_step = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = sigmoid(wm_weight_base)  # constant gate

        # set-size modulated lapse
        epsilon = sigmoid(epsilon_base + epsilon_ss_slope * (nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply WM decay toward uniform for visited state before computing policy
            w[s, :] = (1.0 - wm_decay_per_step) * w[s, :] + wm_decay_per_step * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probability of chosen action (per template)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Combine WM and RL
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Epsilon-greedy lapse to uniform
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: toward correct action if rewarded; otherwise toward uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            # Use same decay parameter as a learning step-size toward the target
            w[s, :] = (1.0 - wm_decay_per_step) * w[s, :] + wm_decay_per_step * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-adaptive learning rate + WM with action-binding errors (swap/leakage).

    Mechanism
    ---------
    - RL: delta rule with learning rate adapted by set size (meta-learning on capacity).
    - WM: near-deterministic softmax; however, action-binding errors are modeled as probability
      mass leaking from each intended WM action to other actions (a "swap" process).
    - Arbitration: constant WM gate controlled by wm_weight_base.

    Parameters
    ----------
    model_parameters : tuple
        lr_base : float
            Baseline RL learning rate in logit space; mapped to (0,1) by sigmoid.
        lr_ss_slope : float
            Set-size slope on RL learning rate (larger sets can decrease/increase lr).
        softmax_beta : float
            Inverse temperature for RL policy, scaled internally by 10.
        wm_weight_base : float
            Logistic parameter mapped to [0,1] gate giving WM's contribution.
        wm_alpha : float
            WM learning rate toward outcome: reward -> one-hot(a), no-reward -> uniform.
        wm_binding_error : float
            Probability of action-binding swap/leak in WM policy (0..1); increases ambiguity.

    Set-size effects
    ----------------
    - RL learning rate depends on set size via sigmoid(lr_base + lr_ss_slope*(3.5 - nS)).
      Positive lr_ss_slope increases lr for small sets and decreases it for large sets.
    - WM binding error is global here (no explicit set-size term) to keep mechanisms distinct.
    """
    lr_base, lr_ss_slope, softmax_beta, wm_weight_base, wm_alpha, wm_binding_error = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size adaptive RL learning rate
        lr_eff = sigmoid(lr_base + lr_ss_slope * (3.5 - nS))
        wm_weight_eff = sigmoid(wm_weight_base)

        # Constrain binding error
        leak = np.clip(wm_binding_error, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM base softmax distribution
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / max(np.sum(wm_probs), 1e-12)

            # Action-binding leakage: each action keeps (1-leak) of its mass, leaking leak equally to others
            wm_probs_leaky = (1.0 - leak) * wm_probs + leak * (1.0 / (nA - 1)) * (1.0 - wm_probs)

            # Extract chosen action prob from leaky WM
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_base = 1.0 / max(denom_wm, 1e-12)  # per-template chosen-prob
            # Adjust to match leaky distribution at chosen action index
            # Compute p for chosen a from wm_probs_leaky directly:
            p_wm = wm_probs_leaky[a]
            # Ensure consistency with the chosen-prob computation path
            p_wm = float(p_wm)

            # Arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta

            # WM update toward outcome
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with recency-gated arbitration: WM reliability decays with time since state visit,
    amplified by set size.

    Mechanism
    ---------
    - RL: standard delta rule.
    - WM: delta rule toward outcome; near-deterministic policy.
    - Arbitration: WM weight is the product of a baseline gate (sigmoid(wm_bias)) and a recency-based
      reliability term exp(-recency_slope * age_s * (nS/3)), where age_s is the number of trials since
      the state was last visited. Larger set sizes increase the effective age scaling, favoring RL.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            Inverse temperature for RL policy, scaled internally by 10.
        wm_bias : float
            Baseline WM gate in logit space; mapped via sigmoid to [0,1].
        recency_slope : float
            Sensitivity of WM reliability to state age. Higher => faster WM reliability decay.
        wm_alpha : float
            WM learning rate toward outcome: reward -> one-hot(a), no-reward -> uniform.
        wm_decay : float
            Passive WM decay toward uniform at each visit (captures interference even without time).

    Set-size effects
    ----------------
    - Arbitration depends on set size via the term exp(-recency_slope * age_s * (nS/3)):
      larger sets inflate effective age, reducing WM reliance more strongly.
    """
    lr, softmax_beta, wm_bias, recency_slope, wm_alpha, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track age since last visit for each state
        age = np.zeros(nS, dtype=float)
        base_gate = sigmoid(wm_bias)

        log_p = 0.0
        for t in range(len(block_states)):

            # Increment age for all states by 1 at each trial
            age += 1.0

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Passive WM decay on visit (interference)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM chosen-action probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Recency-based reliability scaled by set size
            reliab = np.exp(-recency_slope * age[s] * (nS / 3.0))
            reliab = float(np.clip(reliab, 0.0, 1.0))
            wm_weight_eff = base_gate * reliab
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update toward outcome
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target

            # Reset age of the visited state
            age[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p