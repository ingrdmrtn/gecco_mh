def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Load-modulated WM decay with rehearsal; arbitration weight decreases with set size.

    Idea
    - RL: standard delta rule on Q(s,a) with softmax choice.
    - WM: probability table w(s,a) that decays toward uniform each trial, plus a rehearsal update
      on the current state that increases the chosen action after reward and redistributes after no reward.
    - Arbitration: a base WM mixture weight is compressed by set size via a 1/(1+phi*nS) rule.

    Set-size effects
    - Larger set size increases WM decay: wm_decay_block = wm_decay_base * (nS/3).
    - Larger set size reduces WM's contribution to choice: wm_weight_block = wm_weight_base / (1 + wm_weight_size_phi * nS).

    Parameters (model_parameters)
    - lr (float in [0,1]): RL learning rate.
    - softmax_beta (float > 0): Base RL inverse temperature (internally scaled x10).
    - wm_weight_base (float in [0,1]): Base mixture weight for WM contribution at nS=0; gets reduced by set size.
    - wm_weight_size_phi (float >= 0): Strength of the set-size penalty on WM weight (larger = smaller WM use at large nS).
    - wm_decay_base (float in [0,1]): Base WM decay toward uniform per trial at nS=3; scales with nS.
    - wm_rehearsal_gain (float in [0,1]): Strength of WM update toward chosen action (rewarded) or redistribution (unrewarded).
    
    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_weight_base, wm_weight_size_phi, wm_decay_base, wm_rehearsal_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent parameters for this block
        wm_decay_block = min(1.0, wm_decay_base * (nS / 3.0))
        wm_weight_block = wm_weight_base / (1.0 + wm_weight_size_phi * nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax on WM row; soft and close to argmax due to large beta)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy with load-modulated WM weight
            wm_w = np.clip(wm_weight_block, 0.0, 1.0)
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global decay toward uniform (set-size amplified)
            w = (1.0 - wm_decay_block) * w + wm_decay_block * w_0

            # WM rehearsal update on current state
            if r > 0.5:
                # Move probability mass toward chosen action
                add = wm_rehearsal_gain
                # Reduce all entries proportionally, then add to chosen
                w[s, :] = (1.0 - add) * w[s, :]
                w[s, a] += add
            else:
                # Penalize chosen action; redistribute to others
                dec = min(wm_rehearsal_gain, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Renormalize row to sum to 1
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with novelty bonus + capacity-limited WM slots with decay.

    Idea
    - RL: standard delta rule on Q(s,a) and an intrinsic novelty bonus that favors less-visited actions in a state.
    - WM: capacity-limited slots K; each state maintains a WM distribution, but only the top-K states by WM strength
      are effectively stored. States outside capacity act as near-uniform. WM traces decay each trial.
    - Arbitration: fixed mixture weight (wm_weight), independent of set size; load affects which states get into WM
      because with larger nS, fewer states are within K, so WM helps fewer states.

    Set-size effects
    - Larger nS means a smaller fraction of states can be stored (K/nS), reducing WM influence in large-set blocks.

    Parameters (model_parameters)
    - lr (float in [0,1]): RL learning rate.
    - softmax_beta (float > 0): RL inverse temperature (internally scaled x10).
    - wm_weight (float in [0,1]): Mixture weight for WM contribution.
    - K_capacity (int-like >=1): Number of states that can be held in WM; will be clipped to [1, nS] per block.
    - wm_decay (float in [0,1]): Global decay of WM rows toward uniform each trial.
    - novelty_eta (float >= 0): Strength of novelty bonus in RL policy based on inverse visit counts per state-action.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_weight, K_capacity, wm_decay, novelty_eta = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Visit counts for novelty bonus per state-action
        visits = 1e-6 * np.ones((nS, nA))  # small prior to avoid div by zero

        # Effective capacity for this block
        K_eff = int(np.clip(np.round(K_capacity), 1, nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute set of states currently "in WM" as the top-K by row concentration (max prob)
            row_strength = np.max(w, axis=1)  # how peaked each state's WM row is
            top_idx = np.argsort(-row_strength)[:K_eff]
            in_wm = (s in top_idx)

            Q_s = q[s, :]

            # RL policy with novelty bonus (favor less-visited actions)
            novelty = novelty_eta * (1.0 / np.sqrt(visits[s, :] + 1e-8))
            Q_aug = Q_s + novelty
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy
            if in_wm:
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                # If state not held in capacity, WM is effectively uniform
                p_wm = 1.0 / nA

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL updates
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # Update visits after observing choice
            visits[s, a] += 1.0

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM local update on current state
            if r > 0.5:
                add = 0.5  # make a strong imprint when rewarded (fixed strong imprint; decay controls persistence)
                add = min(add, 1.0)  # safety
                w[s, :] = (1.0 - add) * w[s, :]
                w[s, a] += add
            else:
                # push away from chosen action slightly on errors
                dec = min(0.25, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian-inspired arbitration using WM entropy and RL spread; WM learning tied to set size.

    Idea
    - RL: standard delta rule on Q(s,a).
    - WM: per-state categorical distribution w(s,a) updated toward one-hot of the chosen action when rewarded,
      and pushed away when not; the learning strength is tied to set size via decay/learn coupling.
    - Arbitration: trial-wise WM weight computed from a logistic of (negative WM entropy) and (negative RL variance),
      modulated by set size (larger nS reduces WM influence).

    Set-size effects
    - WM learning strength: wm_learn_block = max(0, 1 - wm_decay_block), with wm_decay_block = wm_decay_base * (nS/3).
    - Arbitration intercept and features scaled by 1/(1 + nS): wm_weight_t = sigmoid(k0 + k_wm * H_term + k_rl * V_term) / (1 + nS/3).

    Parameters (model_parameters)
    - lr (float in [0,1]): RL learning rate.
    - softmax_beta (float > 0): RL inverse temperature (internally scaled x10).
    - k0 (float): Arbitration bias term (logit space).
    - k_wm (float): Weight on WM certainty (negative entropy) in arbitration.
    - k_rl (float): Weight on RL certainty (negative variance) in arbitration.
    - wm_decay_base (float in [0,1]): Base WM decay rate; scaled up by set size.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, k0, k_wm, k_rl, wm_decay_base = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    def entropy(p):
        p_safe = np.clip(p, 1e-12, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_decay_block = min(1.0, wm_decay_base * (nS / 3.0))
        wm_learn_block = max(0.0, 1.0 - wm_decay_block)
        # Additional scaling that reduces arbitration weight as set size grows
        arb_scale = 1.0 / (1.0 + nS / 3.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration based on certainty signals
            # WM certainty: negative entropy (higher when WM row is peaked)
            H = entropy(W_s)
            H_term = -(H / np.log(nA))  # normalize to [-1,0], then negate -> [0,1]
            # RL certainty: negative variance of Q-values (more spread => larger certainty)
            V = np.var(Q_s)
            # Normalize RL variance by its maximal spread for probabilities ~ [0,1]
            V_term = -V  # lower variance (flat Q) => less certain; more negative; we will weight appropriately

            logit = k0 + k_wm * H_term + k_rl * V_term
            wm_weight_t = 1.0 / (1.0 + np.exp(-logit))
            wm_weight_t *= arb_scale
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global decay
            w = (1.0 - wm_decay_block) * w + wm_decay_block * w_0

            # WM local learning tied to wm_learn_block
            if r > 0.5:
                add = wm_learn_block
                w[s, :] = (1.0 - add) * w[s, :]
                w[s, a] += add
            else:
                dec = min(wm_learn_block / 2.0, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p