def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-modulated WM reliance and reward-gated WM writes,
    plus RL choice stickiness (per-state perseveration).

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta, plus a stickiness bias
      toward the last chosen action in the current state (strength stickiness).
    - WM system: softmax over W with a very high inverse temperature (near-deterministic).
    - Mixture: wm_weight is a logistic function of set size: wm_weight = sigmoid(wm_bias + wm_size_slope*(3 - nS)),
      so that larger set sizes reduce WM reliance when wm_size_slope > 0.

    Learning:
    - RL: Rescorla-Wagner with learning rate lr.
    - WM: per-visit decay toward uniform (wm_decay), and reward-gated overwrite: on rewarded trials (r=1),
      the chosen action is written strongly (full overwrite); on unrewarded trials, only decay happens.

    Set-size effects:
    - Enter via the mixture weight logistic term (wm_bias, wm_size_slope). Larger nS reduces WM weight when wm_size_slope>0.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_bias: intercept for WM mixture weight (logit space).
    - wm_size_slope: slope for how set size modulates WM mixture weight in logit space via (3 - nS).
    - wm_decay: WM decay toward uniform on each visit (0=no decay, 1=full reset toward uniform before writing).
    - stickiness: strength of per-state perseveration bias added to the RL policy for the last chosen action.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_bias, wm_size_slope, wm_decay, stickiness = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # set-size dependent WM weight (constant within block)
        logit_wm = wm_bias + wm_size_slope * (3 - nS)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_wm))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # add stickiness to RL values for policy (bias last chosen action in this state)
            if last_action[s] >= 0:
                bias = np.zeros(nA)
                bias[last_action[s]] = stickiness
                Q_eff = Q_s + bias
            else:
                Q_eff = Q_s

            # RL choice probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy: softmax over W with very high beta
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform, then reward-gated write
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # full overwrite on reward (maximally simple reward-gated write)
                w[s, :] = one_hot

            # update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with power-law set-size scaling of WM reliance, asymmetric RL learning,
    and RL value forgetting toward uniform.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta.
    - WM system: softmax over W with high inverse temperature.
    - Mixture: wm_weight = clip(kappa / (nS ** alpha), 0, 1), so larger set sizes reduce WM reliance
      via a power-law with parameters kappa and alpha.

    Learning:
    - RL: asymmetric learning rates for positive vs negative prediction errors (lr_pos, lr_neg).
      Additionally, per-visit RL forgetting toward uniform at rate rl_forget (captures interference in larger sets).
    - WM: decay toward uniform each visit; reward-locked store (only on r=1) for recency of correct mappings.

    Set-size effects:
    - Enter through wm_weight power-law in nS and via increased need for RL (WM reduced at large nS).
      RL forgetting applies on every visit (can capture poorer retention in larger sets implicitly through behavior).

    Parameters (6):
    - lr_pos: RL learning rate for positive prediction errors.
    - lr_neg: RL learning rate for negative prediction errors.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - kappa: WM weight numerator in power-law mixture (overall WM strength).
    - alpha: exponent of set-size effect on WM mixture (higher alpha reduces WM more strongly at large nS).
    - rl_forget: RL forgetting rate toward uniform each visit (0=no forgetting, 1=reset toward uniform each visit).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, kappa, alpha, rl_forget = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size dependent WM weight via power-law
        wm_weight_eff = np.clip(kappa / (float(nS) ** max(alpha, 0.0)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM action probabilities
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetry
            pe = r - q[s, a]
            lr_eff = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_eff * pe

            # RL forgetting toward uniform
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # WM decay and reward-locked store
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # light decay each visit to avoid brittleness without extra param
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with error-driven WM learning and decay.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta.
    - WM system: softmax over W with a very high inverse temperature.
    - Arbitration: dynamic wm_weight based on relative uncertainty (entropy) of RL vs WM at the current state:
        wm_weight = sigmoid(arbitration_bias + arbitration_slope * (H_rl - H_wm)),
      where H_rl is the entropy of the RL policy at the state, and H_wm is the entropy of the WM policy.
      If RL is more uncertain than WM (H_rl > H_wm), arbitration shifts weight toward WM.

    Learning:
    - RL: Rescorla-Wagner (lr).
    - WM: delta-rule toward the chosen action with learning rate wm_learn_rate scaled by reward r,
      plus per-visit decay toward uniform with wm_decay (captures fragility/interference).

    Set-size effects:
    - Implicit: larger sets typically increase RL entropy and reduce WM certainty, dynamically reducing wm_weight.
      Additionally, higher visit spacing at large nS promotes WM decay between visits.

    Parameters (6):
    - lr: RL learning rate.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_learn_rate: WM learning rate toward chosen action on rewarded trials (0..1).
    - wm_decay: WM decay toward uniform on each visit (0..1).
    - arbitration_slope: sensitivity of arbitration to entropy difference (H_rl - H_wm).
    - arbitration_bias: baseline bias toward WM (logit space).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_learn_rate, wm_decay, arbitration_slope, arbitration_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probabilities over all actions (needed for entropy)
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)

            # WM softmax probabilities
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)

            # chosen-action probabilities
            p_rl = pi_rl[a]
            p_wm = pi_wm[a]

            # entropies (natural log)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, eps, 1.0)))

            # arbitration weight by relative uncertainty
            logit_w = arbitration_bias + arbitration_slope * (H_rl - H_wm)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # mixture probability
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay and error-driven learning (reward-gated)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            # Move W toward the chosen action proportional to reward and wm_learn_rate
            w[s, :] = (1.0 - wm_learn_rate * r) * w[s, :] + (wm_learn_rate * r) * one_hot

        blocks_log_p += log_p

    return -blocks_log_p

Notes on using these models for set-size effects in this dataset:
- Model 1 uses an explicit logistic function of set size to change WM reliance; it can capture better performance in small sets due to higher WM weight and stickiness-driven reuse.
- Model 2 uses a power-law WM weight in set size and introduces asymmetric RL learning plus RL forgetting, allowing steeper performance drops when sets are larger.
- Model 3 uses dynamic trial-by-trial arbitration driven by entropy; larger sets typically increase RL uncertainty and reduce WM certainty, thereby changing the mixture in a data-driven way without explicit set-size parameters.