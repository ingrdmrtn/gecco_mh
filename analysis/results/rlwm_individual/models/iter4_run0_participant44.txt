def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Count-based exploration + WM capacity gated by set size (logistic) + WM decay.

    Rationale:
    - RL learns action values per state.
    - A count-based exploration bonus encourages sampling less-tried actions (helps early learning).
    - WM stores recently rewarded mappings with high precision but its influence is reduced in larger set sizes
      via a logistic gating function of set size.
    - WM traces decay toward uniform over time.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr              : RL learning rate (0..1).
      wm_base         : Base WM mixture weight (0..1), scaled down by set size via logistic gating.
      softmax_beta    : RL inverse temperature; internally scaled by 10.
      size_sensitivity: Positive value controlling how sharply WM weight drops with set size (higher => stronger drop for nS=6 vs nS=3).
      wm_decay        : Per-trial WM decay toward uniform (0..1).
      explore_bonus   : Strength of count-based exploration bonus added to RL values (>0).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, size_sensitivity, wm_decay, explore_bonus = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Value tables
        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Counts for exploration
        N   = np.zeros((nS, nA))

        # Logistic capacity gate: high for small nS, low for large nS
        # center between 3 and 6; positive size_sensitivity => lower weight for nS=6
        gate = 1.0 / (1.0 + np.exp(size_sensitivity * (nS - 4.5)))
        wm_mix_block = np.clip(wm_base * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with count-based exploration bonus
            bonus = explore_bonus / np.sqrt(1.0 + N[s, :])
            logits_rl = softmax_beta * (q[s, :] + bonus)
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy (high precision over w)
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_mix_block * p_wm + (1.0 - wm_mix_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: reward strengthens chosen association; nonreward slightly weakens
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong encoding on reward
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot
            else:
                # Gentle push away from the chosen action on nonreward
                reduce = 0.2 * w[s, a]
                w[s, a] -= reduce
                others = [i for i in range(nA) if i != a]
                w[s, others] += reduce / (nA - 1)
                # Normalize safety
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

            # Increment counts after observing the choice
            N[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM implementing Win-Stay/Lose-Shift (WSLS) with size-dependent WM weight (power law).

    Rationale:
    - RL updates action values as usual.
    - WM implements a heuristic WSLS policy: after a win in a state, strongly repeat the same action; after a loss, shift away
      from that action, with a tunable bias for how strongly to avoid it and how to distribute probability over alternatives.
    - WM weight decreases with set size via a power law: (3/nS)^size_exponent
      (so WM dominates more for small set sizes).
    - No separate lapse parameter; stochasticity is handled by the softmax precisions.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr             : RL learning rate (0..1).
      wm_weight0     : Base WM mixture weight; scaled by (3/nS)^size_exponent.
      softmax_beta   : RL inverse temperature; internally scaled by 10.
      size_exponent  : Controls set-size penalty on WM (>=0). 0 => no penalty; larger => stronger penalty for nS=6.
      ws_strength    : Strength of WSLS preferences (>=0). Higher => more deterministic WM.
      shift_bias     : Bias toward distributing probability to alternatives after a loss (0..1). 0.5 => equal split.

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, size_exponent, ws_strength, shift_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))  # will mirror the WSLS policy vector for transparency
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and last outcome per state for WSLS
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # WM mixture weight scaled by set size (power law)
        wm_mix_block = np.clip(wm_weight0 * (3.0 / float(nS))**max(0.0, size_exponent), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = softmax_beta * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # Build WSLS WM policy vector
            pref = np.zeros(nA)
            if last_action[s] >= 0:
                la = int(last_action[s])
                if last_reward[s] > 0.0:
                    # Win => stay: raise preference for last action
                    pref[:] = 0.0
                    pref[la] = ws_strength
                else:
                    # Lose => shift: suppress last action, distribute to others
                    pref[:] = 0.0
                    pref[la] = -ws_strength
                    others = [i for i in range(nA) if i != la]
                    # Allocate positive preference to alternatives, possibly biased
                    pref[others[0]] = ws_strength * shift_bias
                    pref[others[1]] = ws_strength * (1.0 - shift_bias)
            else:
                # No memory yet for this state: uniform
                pref[:] = 0.0

            # Convert WSLS preferences to a sharp softmax policy
            logits_wm = softmax_beta_wm * pref
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]
            # Keep w aligned to current WM policy (not strictly necessary, but aligns with template variables)
            w[s, :] = p_wm_vec

            # Mixture
            p_total = wm_mix_block * p_wm + (1.0 - wm_mix_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update WSLS memory traces
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Confidence-gated WM + size-scaled forgetting + suppression of incorrect WM associations.

    Rationale:
    - RL learns values in the usual way.
    - WM stores recently rewarded associations; its contribution is gated by RL's confidence (low entropy => higher WM weight),
      reflecting that when RL is certain, WM uses this certainty to stabilize mappings.
    - WM forgetting accelerates with set size (more interference).
    - When WM-predicted action yields no reward, that association is actively suppressed (penalized within WM).

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr               : RL learning rate (0..1).
      wm_base          : Base WM weight (0..1).
      softmax_beta     : RL inverse temperature; internally scaled by 10.
      wm_conf_gain     : Scales WM weight around base according to RL confidence (can be positive or negative).
      wm_forget        : Base WM forgetting rate toward uniform (0..1); scaled by set size.
      wm_suppress      : Strength of suppression applied to the chosen WM association on errors (0..1).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, wm_conf_gain, wm_forget, wm_suppress = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Size-scaled forgetting: more forgetting at larger nS
        forget_rate = np.clip(wm_forget * (float(nS) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy vector and confidence (1 - normalized entropy)
            logits_rl = softmax_beta * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # Entropy-based confidence
            eps = 1e-12
            H = -np.sum(p_rl_vec * np.log(p_rl_vec + eps))
            H_max = np.log(nA)
            confidence = 1.0 - (H / (H_max + eps))  # in [0,1]

            # WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Confidence-gated mixture weight around base
            wm_mix = np.clip(wm_base + wm_conf_gain * (confidence - 0.5), 0.0, 1.0)

            # Mixture probability
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM forgetting toward uniform (size-scaled)
            w = (1.0 - forget_rate) * w + forget_rate * w_0

            # WM encoding and suppression
            if r > 0.0:
                # Encode strong association for rewarded action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.3 * w[s, :] + 0.7 * one_hot
            else:
                # Suppress the chosen association within WM on error
                decay_amt = wm_suppress * w[s, a]
                w[s, a] -= decay_amt
                others = [i for i in range(nA) if i != a]
                w[s, others] += decay_amt / (nA - 1)
                # Safety normalization
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p