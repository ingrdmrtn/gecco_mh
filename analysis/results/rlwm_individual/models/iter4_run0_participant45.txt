def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-scaled learning and precision-limited WM plus lapses.

    Mechanism
    - RL: standard delta-rule; learning rate decreases with set size (higher load slows RL).
    - WM: softmax over a WM weight vector; its effective precision decreases with set size.
      WM mixture weight also decreases with set size via a power-law.
    - Lapse: with small probability epsilon, choices are uniform.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr_base: float
            Base RL learning rate (0..1). Effective lr scales with 3/nS.
        wm_weight_base: float
            Base weight of WM contribution (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        psi_precision: float
            Base WM precision scaling (>=0). Higher yields more deterministic WM; also scales with 3/nS.
        wm_load_slope: float
            Exponent controlling how WM mixture decreases with load; wm_mix ~ (3/nS)^wm_load_slope.
        epsilon_lapse: float
            Load-independent lapse probability (0..0.2) for uniform random responding.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr_base, wm_weight_base, softmax_beta, psi_precision, wm_load_slope, epsilon_lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled parameters for this block
        load_scale = 3.0 / max(1.0, nS)  # 1 for nS=3, 0.5 for nS=6
        lr_eff = lr_base * load_scale
        wm_mix_base = np.clip(wm_weight_base * (load_scale ** wm_load_slope), 0.0, 1.0)
        beta_wm_eff = softmax_beta_wm * np.maximum(1e-6, psi_precision * load_scale)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (precision decreases with load via beta_wm_eff)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            p_total = (1.0 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr_eff * delta

            # WM update: state-specific decay tied to precision and load, with potentiation on reward
            # Decay toward uniform; stronger decay at higher load (via load_scale) or lower precision
            decay_rate = np.clip(0.3 * (1.0 - np.tanh(psi_precision * load_scale)), 0.0, 0.3)
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            if r > 0.5:
                # Potentiate near one-hot memory when rewarded
                tiny = 1e-6
                w[s, :] = tiny * np.ones(nA)
                w[s, a] = 1.0 - (nA - 1) * tiny
            else:
                # On errors, nudge chosen action toward uniform
                w[s, a] = 0.5 * w[s, a] + 0.5 * (1.0 / nA)

            # Renormalize for numerical stability
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces and interference-prone WM.

    Mechanism
    - RL: delta-rule with state-action eligibility traces; traces decay over time,
      spreading credit temporally for repeated same S-A selections.
    - WM: stores near one-hot S-A associations but decays toward uniform each trial.
      Decay and arbitration weight both worsen with larger set sizes via an interference parameter.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM arbitration weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        theta_elig: float
            Eligibility trace persistence (0..1). Larger means slower decay.
        mu_wm_decay: float
            Base WM decay rate toward uniform per trial (0..1).
        xi_interf: float
            Interference sensitivity to set size (>=0), increases WM decay and reduces WM weight as nS grows.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, theta_elig, mu_wm_decay, xi_interf = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for all state-actions
        e = np.zeros((nS, nA))

        # Set-size dependent WM decay and arbitration weight
        decay_wm = np.clip(mu_wm_decay + xi_interf * max(0, nS - 3) / 6.0, 0.0, 1.0)
        wm_mix = np.clip(wm_weight * (1.0 - xi_interf * max(0, nS - 3) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces:
            # Increment trace for chosen S-A, decay all traces
            e *= (1.0 - theta_elig)
            e[s, a] += 1.0
            # Temporal difference delta for chosen S-A
            delta = r - q[s, a]
            # Apply update proportionally to eligibility traces
            q += lr * delta * e

            # WM decay toward uniform
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM reinforcement: if rewarded, write near one-hot; if not, mild smoothing
            if r > 0.5:
                tiny = 1e-6
                w[s, :] = tiny * np.ones(nA)
                w[s, a] = 1.0 - (nA - 1) * tiny
            else:
                # On error, push chosen action toward uniform a bit more
                w[s, a] = 0.5 * w[s, a] + 0.5 * (1.0 / nA)

            # Normalize the updated state row for numerical stability
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Early-WM/late-RL arbitration with set-size gated WM and previous-action bias.

    Mechanism
    - RL: delta-rule with softmax; includes a bias for repeating the last action in that state.
    - WM: near one-shot encoding on reward; used more heavily early in learning for each state,
      with the duration of WM dominance shrinking as set size increases.
    - Arbitration: WM mixture weight depends on the visit count of the current state
      relative to a gate threshold that scales with set size.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Peak WM mixture weight (0..1) when within the gate window.
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        trial_gate: float
            Baseline number of within-state visits for which WM dominates (>=1).
        kappa_set: float
            Set-size sensitivity; effective gate ~ trial_gate * (3/nS)^kappa_set.
        bias_prev: float
            Additive value bonus to the previously chosen action in the same state (>=0).

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, trial_gate, kappa_set, bias_prev = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track within-state visit counts and last action per state
        visits = np.zeros(nS, dtype=int)
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective WM gate length decreases with set size
        gate_len = max(1, int(np.round(trial_gate * (3.0 / max(1.0, nS)) ** kappa_set)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            # Add previous-action bias in RL values for that state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += bias_prev

            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM dominates for the first 'gate_len' visits of this state
            wm_mix = wm_weight if visits[s] < gate_len else 0.2 * wm_weight

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # Update counters
            visits[s] += 1
            last_action[s] = a

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: mild global decay each step plus state-specific potentiation
            w = 0.95 * w + 0.05 * w_0
            if r > 0.5:
                tiny = 1e-6
                w[s, :] = tiny * np.ones(nA)
                w[s, a] = 1.0 - (nA - 1) * tiny
            else:
                # On errors, reduce confidence in the chosen action
                w[s, a] = 0.6 * w[s, a] + 0.4 * (1.0 / nA)

            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p