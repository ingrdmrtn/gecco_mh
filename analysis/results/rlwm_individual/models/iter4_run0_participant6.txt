def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-driven arbitration and load penalty.

    Idea:
    - Choices are a mixture of model-free RL and a WM trace that stores rewarded S-A pairs.
    - Arbitration is dynamic: WM weight increases when WM for that state is sharp (high certainty),
      and decreases when RL policy is sharp (low uncertainty) and/or when set size is large.
    - WM traces decay toward a uniform prior each trial; rewarded trials write a one-hot memory.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_decay: WM decay rate toward uniform (0..1) applied each trial.
    - arb_slope: slope controlling sensitivity of WM weight to WM sharpness vs RL sharpness.
                 Positive values strengthen WM when WM is sharp; negative would do the opposite.
    - arb_bias: baseline bias toward WM use (can be negative to penalize WM).
    - load_penalty: linear penalty per extra item beyond 3 on WM weight (>=0 reduces WM at larger set sizes).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_decay, arb_slope, arb_bias, load_penalty = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy via near-deterministic softmax over W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: sigmoid of WM-vs-RL sharpness, penalized by load
            wm_sharp = np.max(W_s) - (1.0 / nA)                     # 0 when uniform, up to 1-1/nA when one-hot
            rl_sharp = np.max(Q_s) - np.min(Q_s)                    # larger spread -> more confident RL
            # Combine with slope and bias; subtract load penalty proportional to excess items beyond 3
            logits = arb_bias + arb_slope * (wm_sharp - rl_sharp) - load_penalty * max(0, nS - 3)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logits))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform, then overwrite on reward
            w = (1 - wm_decay) * w + wm_decay * w_0
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + time-based decaying WM trace with probabilistic encoding.

    Idea:
    - RL updates action values per state.
    - WM stores the last rewarded action per state as a one-hot prototype, but its effective precision
      at decision time decays with the number of intervening trials since last reward for that state.
    - Encoding is probabilistic and folded into a deterministic strength term, avoiding stochastic simulation.
    - Mixture weight is fixed (implicitly via WM precision) and independent of load; load influences WM implicitly
      through longer inter-visit intervals when nS is large, which increases decay.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - tau_wm: time constant (>0) controlling WM decay with trials since last reward for the state.
    - enc_prob: probability (0..1) that a reward encodes into WM; scales the peakiness of WM.
    - wm_precision: gain applied to the WM softmax; higher = more deterministic WM when strong.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, tau_wm, enc_prob, wm_precision = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = max(1e-6, wm_precision)  # use provided precision for WM
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        # w will hold the last rewarded action prototype (one-hot when encoded), otherwise uniform
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track age since last successful encoding per state (in trials)
        age = np.full(nS, np.inf)
        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Increment ages
            age = age + 1

            Q_s = q[s,:]
            # Build an effective WM distribution that mixes uniform with the stored prototype
            # Strength decays with age; enc_prob scales the asymptotic height
            if np.isfinite(age[s]) and tau_wm > 0:
                strength = enc_prob * np.exp(-age[s] / max(1e-6, tau_wm))
            else:
                strength = 0.0
            W_proto = w[s, :]  # one-hot prototype if previously encoded, else uniform
            W_s = (1 - strength) * w_0[s, :] + strength * W_proto

            # Policies
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture: let WM compete directly with RL via fixed convex combination implied by strength
            # The same "strength" acts as mixture weight to reflect usable WM reliability at this moment.
            wm_weight_eff = strength
            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM encoding on reward: write a one-hot prototype with expected strength enc_prob
            if r > 0:
                # store prototype deterministically; its influence at choice time is via 'strength'
                w[s, :] = 0.0
                w[s, a] = 1.0
                age[s] = 0  # reset age on reward

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Win-Stay/Lose-Shift working-memory heuristic with load-scaled arbitration.

    Idea:
    - RL updates values as usual.
    - WM implements a simple heuristic for each state:
        - After reward, bias strongly toward repeating the chosen action (win-stay).
        - After no reward, bias against the chosen action and toward alternatives (lose-shift).
      The heuristic is stored per state and persists until updated by a new outcome.
    - Arbitration weight scales with set size as (3/nS)^load_exponent: WM dominates at nS=3, weakens at nS=6.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - stay_strength: weight (0..1) assigned to the chosen action in WM after reward.
    - lose_shift_strength: weight (0..1) that shifts probability mass from the chosen action
                           to alternatives after non-reward.
    - load_exponent: exponent >= 0 controlling how fast WM weight drops with set size (higher = stronger drop).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, stay_strength, lose_shift_strength, load_exponent = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        # WM heuristic values; start uniform
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute WM weight from load
        wm_weight_eff = min(1.0, (3.0 / float(nS)) ** max(0.0, load_exponent))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]

            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy (heuristic) via deterministic softmax of W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Update WM heuristic for this state based on reward outcome
            if r > 0:
                # Win-Stay: assign 'stay_strength' to chosen action; spread remainder uniformly
                remainder = max(0.0, 1.0 - stay_strength)
                w[s, :] = remainder / nA
                w[s, a] = remainder / nA + stay_strength
            else:
                # Lose-Shift: remove mass from chosen action and distribute to others
                # Start from uniform
                base = 1.0 / nA
                # amount to move from chosen to each alternative
                shift_total = min(1.0, lose_shift_strength)
                # chosen receives less than uniform; others receive extra mass
                w[s, :] = base
                w[s, a] = max(0.0, base * (1.0 - shift_total))
                add_to_others = (base * shift_total) / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = base + add_to_others

        blocks_log_p += log_p

    return -blocks_log_p