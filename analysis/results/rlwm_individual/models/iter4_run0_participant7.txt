def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + precision-scaled WM mixture with load-dependent sharpening and graded encoding.

    Idea
    - RL: tabular Q-learning with a single learning rate and softmax choice.
    - WM: maintains a probabilistic action distribution per state (row of w), combined with RL by a fixed mixture.
      WM acts like a fast system whose precision is reduced under higher load (larger set size).
      WM encodes toward the chosen action on every trial, but the target depends on outcome:
        - if rewarded, WM moves toward a one-hot on the chosen action;
        - if not rewarded, WM reverts toward uniform for that state.
      This yields rapid capture after reward and "unlearning" after negative outcomes.
    - Load effect: WM precision is scaled by (3/nS)^load_exponent, reducing WM determinism in larger sets.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_precision_base, load_exponent, wm_eta)
        - lr: RL learning rate in [0,1]
        - wm_weight: WM/RL mixture weight in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - wm_precision_base: baseline WM precision (>0) that sharpens WM policy
        - load_exponent: exponent controlling how WM precision drops with set size
        - wm_eta: WM encoding step size per trial in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_precision_base, load_exponent, wm_eta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision (reduces as nS grows)
        precision_scale = (3.0 / float(nS)) ** load_exponent
        wm_precision = max(0.0, wm_precision_base) * precision_scale

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: sharpen around WM contents via precision scaling
            # Shift to zero-mean around uniform before sharpening to avoid biasing toward higher baseline values
            W_centered = W_s - (1.0 / nA)
            W_eff = (wm_precision * W_centered)  # higher precision => more deterministic
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: graded move toward onehot if rewarded, else toward uniform
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            target = r * onehot + (1.0 - r) * w_0[s, :]
            w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + surprise-gated WM encoding with load-damped WM influence and passive decay.

    Idea
    - RL: tabular Q-learning with softmax choice.
    - WM: probabilistic store that decays toward uniform each trial.
      WM writes (encodes a one-hot for the chosen action) only when surprise (unsigned RPE) exceeds a threshold.
    - Load effect: the contribution of WM to choice is damped as set size increases (exponential load bias).

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, surprise_gate, load_bias, wm_decay)
        - lr: RL learning rate in [0,1]
        - wm_weight: baseline WM/RL mixture weight in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - surprise_gate: threshold on unsigned RPE for WM encoding (>=0)
        - load_bias: scales how WM influence decreases with set size (>=0)
        - wm_decay: per-trial decay of WM toward uniform in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, surprise_gate, load_bias, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-damped WM mixture weight
        wm_weight_eff = wm_weight * np.exp(-max(0.0, load_bias) * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic-ish softmax over WM distribution)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Passive WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Surprise-gated WM encoding (overwrite for state s)
            unsigned_rpe = abs(delta)
            if unsigned_rpe > surprise_gate:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Encode strongly when gated
                w[s, :] = onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency-weighted WM with load-dependent lapses and within-state perseveration.

    Idea
    - RL: tabular Q-learning with softmax policy.
    - WM: captures the most recently rewarded action for each state with exponential recency weighting;
      WM weakens over time via decay toward uniform. WM is also prone to lapses that increase with load.
    - Additionally, within each state, WM policy includes a perseveration bias toward the state's last chosen action.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, recency_tau, load_offset, wm_stickiness)
        - lr: RL learning rate in [0,1]
        - wm_weight: WM/RL mixture weight in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - recency_tau: WM recency time constant (>0). Larger => slower decay (more persistent WM)
        - load_offset: controls how WM lapses increase with set size (slope of logistic; can be >0)
        - wm_stickiness: additive bias to WM values for the state's last chosen action (can be >=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, recency_tau, load_offset, wm_stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Convert recency_tau to a decay fraction per trial toward uniform
    recency_tau = max(1e-6, recency_tau)
    decay = 1.0 - np.exp(-1.0 / recency_tau)  # in (0,1)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action_state = -np.ones(nS, dtype=int)

        # Load-dependent WM lapse within WM policy via logistic centered between 3 and 6
        # Higher set size -> larger lapse
        lapse_eff = 1.0 / (1.0 + np.exp(-load_offset * (float(nS) - 4.5)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Add perseveration bias to WM values for last action in this state
            if last_action_state[s] >= 0:
                W_s[last_action_state[s]] += wm_stickiness

            # WM policy: softmax over biased WM distribution, then lapse to uniform
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse_eff) * p_wm_soft + lapse_eff * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform each trial (recency forgetting)
            w = (1.0 - decay) * w + decay * w_0

            # WM encoding on rewarded trials with strength tied to recency (use 'decay' as encoding rate)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                enc = decay  # tie encoding magnitude to the same timescale as forgetting
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot

            # Update last action for this state
            last_action_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p