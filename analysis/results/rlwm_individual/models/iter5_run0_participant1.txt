def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-gated arbitration and WM decay.

    Idea:
    - RL learns state-action values with a standard delta rule.
    - WM stores fast mappings that are highly deterministic (beta_wm=50) but subject to decay.
    - Arbitration weight for WM depends on load (set size): a logistic gate shifts the baseline
      WM weight as a function of set size, yielding stronger WM use in low load (nS=3) and
      reduced WM use in high load (nS=6).

    Policy:
    - p_total = wm_weight(nS) * p_wm + (1 - wm_weight(nS)) * p_rl
    - RL uses a softmax with inverse temperature softmax_beta (scaled by 10).
    - WM uses a softmax with softmax_beta_wm=50 (near-deterministic).

    WM updates:
    - On reward: fast attraction of w[s] toward the chosen action (one-shot like), with wm_eta.
    - Per trial decay: w[s] relaxes toward uniform with rate depending on forget_rate. Decay
      applies regardless of outcome.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_base: float in (0,1), baseline WM mixture weight at nS=3 (center of logistic gate).
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_eta: float in [0,1], WM learning rate on rewarded trials (one-shot like).
    - gate_slope: float, slope of logistic gate controlling WM weight change per unit load.
                  Positive gate_slope reduces WM usage as set size increases.
    - forget_rate: float in [0,1], per-trial WM decay toward uniform.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_base, softmax_beta, wm_eta, gate_slope, forget_rate = model_parameters
    softmax_beta *= 10.0  # as specified by template
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    # helper for stable logistic
    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    # map baseline weight to logit and gate by set size
    wm_base = float(np.clip(wm_base, 1e-6, 1 - 1e-6))
    base_logit = np.log(wm_base) - np.log(1 - wm_base)

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # compute WM mixture weight for this block from load
        # shift argument so that when nS=3, wm_weight ~= wm_base
        gate_input = base_logit + gate_slope * (nS - 3)
        wm_weight_block = float(inv_logit(gate_input))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # Outcome-dependent attraction toward chosen action, then decay
            if r > 0.5:
                # move toward one-hot on rewarded action
                # convex combination ensures normalization
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            # decay toward uniform each trial (outcome-independent)
            f = float(np.clip(forget_rate, 0.0, 1.0))
            w[s, :] = (1.0 - f) * w[s, :] + f * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + limited-capacity WM (slot-like) with load-driven interference.

    Idea:
    - WM holds only K_eff state-action mappings in a block (slot-like capacity).
      If nS > K_eff, some states are not represented in WM and default to uniform WM policy.
      K_eff shrinks with load via a divisive factor.
    - RL always learns and supplies a softmax policy.
    - Arbitration: fixed wm_weight between WM and RL.
    - WM updates only for stored states; on reward, update toward the rewarded action.
      Stored WM traces also slowly relax toward uniform with interference that increases with load.

    Policy:
    - p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
    - WM policy:
        - If state s is stored in WM: softmax with beta_wm=50 over w[s]
        - Else: uniform over actions (no WM info)

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight: float in [0,1], mixture weight for WM policy.
    - k_capacity: float >= 0, baseline WM capacity in slots (will be rounded and clipped).
    - load_interference: float >= 0, controls reduction in effective capacity and extra decay with load.
    - wm_eta: float in [0,1], WM learning rate toward rewarded action for stored states.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, k_capacity, load_interference, wm_eta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # determine effective capacity
        denom = 1.0 + load_interference * max(0, nS - 1)
        K_eff = int(np.clip(np.round(k_capacity / denom), 0, nS))

        stored = np.zeros(nS, dtype=bool)  # which states are in WM
        n_stored = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy depends on whether s is stored
            if stored[s]:
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                p_wm = 1.0 / nA  # no WM information for this state

            # mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updates and storage policy
            # On reward, attempt to store s if capacity allows
            if (r > 0.5) and (not stored[s]) and (n_stored < K_eff):
                stored[s] = True
                n_stored += 1

            # If stored, update toward chosen action on reward
            if stored[s] and (r > 0.5):
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta

            # Interference/decay: stored items decay more as load increases
            if stored[s]:
                extra_decay = 1.0 - np.exp(-load_interference * max(1, nS))
                extra_decay = float(np.clip(extra_decay, 0.0, 1.0))
                w[s, :] = (1.0 - extra_decay) * w[s, :] + extra_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load-scaled RL precision; RL decay.

    Idea:
    - Arbitration between WM and RL depends on their relative uncertainty at the current state:
      higher WM weight when WM is more certain (lower entropy) than RL.
    - RL precision (effectively, inverse temperature) decreases with load; we implement this by
      scaling Q values before the softmax so the existing beta still applies.
    - RL values also undergo small decay toward an uninformative baseline to capture forgetting.
    - WM updates fast toward rewarded actions.

    Policy:
    - Compute full action distributions for RL and WM, get their entropies H_rl, H_wm.
    - wm_weight_t = sigmoid(arb_sensitivity * (H_rl - H_wm)): favors WM when WM is more precise.
    - p_total = wm_weight_t * p_wm + (1 - wm_weight_t) * p_rl.
    - Load-scaled RL precision: Q_s is multiplied by beta_factor = exp(-beta_scale_load * (nS - 3))
      prior to the softmax to emulate lower precision at higher load.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta_base: float >= 0, base RL inverse temperature (will be scaled by 10 as per template).
    - beta_scale_load: float >= 0, controls how much RL precision drops with load.
    - wm_alpha: float in [0,1], WM learning rate toward rewarded action.
    - arb_sensitivity: float, sensitivity of the arbitration gate to entropy difference.
    - q_decay: float in [0,1], per-trial decay of Q toward uniform baseline.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, beta_scale_load, wm_alpha, arb_sensitivity, q_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def softmax_probs(beta, vals):
        z = vals - np.max(vals)
        e = np.exp(beta * z)
        p = e / np.sum(e)
        return p

    def entropy(p):
        p_safe = np.clip(p, 1e-12, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # load-based scaling factor for RL precision (applied to Q)
        beta_factor = np.exp(-beta_scale_load * (nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy: scale Q_s to emulate load-sensitivity of precision
            Q_s = q[s, :].copy()
            Q_s_scaled = Q_s * beta_factor
            # compute full RL probability vector for arbitration and chosen prob for likelihood
            p_rl_vec = softmax_probs(softmax_beta, Q_s_scaled)
            p_rl = float(np.clip(p_rl_vec[a], 1e-12, 1.0))

            # WM policy: near-deterministic softmax over WM values
            W_s = w[s, :]
            p_wm_vec = softmax_probs(softmax_beta_wm, W_s)
            p_wm = float(np.clip(p_wm_vec[a], 1e-12, 1.0))

            # Entropy-based arbitration
            H_rl = entropy(p_rl_vec)
            H_wm = entropy(p_wm_vec)
            wm_weight_t = float(inv_logit(arb_sensitivity * (H_rl - H_wm)))

            # mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with decay toward baseline
            delta = r - q[s, a]
            q[s, a] += lr * delta
            if q_decay > 0.0:
                q = (1.0 - q_decay) * q + q_decay * w_0  # decay toward uniform values

            # WM update: fast attraction on reward, mild relaxation otherwise
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # slight relaxation toward uniform to prevent overcommitment on unrewarded trials
                relax = min(0.05, wm_alpha)  # tie to wm_alpha to ensure parameter is used meaningfully
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p