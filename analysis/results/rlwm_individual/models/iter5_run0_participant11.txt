def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-gated arbitration and reward-gated WM encoding.

    Rationale:
    - RL learns action values with a standard delta rule.
    - WM stores recently rewarded mappings; it decays toward uniform.
    - Arbitration emphasizes WM when its policy is confident (low entropy) and under low load.
      The confidence gating is controlled by an entropy slope parameter.
    - WM encoding strengthens the chosen action only when rewarded.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: Base WM mixture weight (0..1); scaled by load and WM confidence
    - softmax_beta: Base RL inverse temperature (scaled internally by 10)
    - wm_encode: Magnitude of WM encoding into the chosen action when rewarded (0..1)
    - wm_decay: Per-trial WM decay toward uniform (0..1)
    - entropy_slope: Slope of the sigmoid gating based on WM confidence (>=0). Larger = sharper gating

    Returns: negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, wm_encode, wm_decay, entropy_slope = model_parameters
    softmax_beta *= 10.0  # RL beta scaled
    softmax_beta_wm = 50.0  # deterministic WM readout
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax with high beta over WM probabilities)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration: confidence = 1 - normalized entropy
            H = -np.sum(W_s * np.log(W_s + eps)) / np.log(nA)
            confidence = max(0.0, min(1.0, 1.0 - H))
            # Load scaling
            load_scale = min(1.0, 3.0 / float(nS))
            # Sigmoid gating around confidence=0.5
            conf_gate = 1.0 / (1.0 + np.exp(-entropy_slope * (confidence - 0.5)))
            wm_weight = wm_weight0 * load_scale * conf_gate
            wm_weight = max(0.0, min(1.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM encoding
            if r > 0.5:
                w[s, :] = (1.0 - wm_encode) * w[s, :]
                w[s, a] += wm_encode

            # Renormalize WM row to a probability distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + Dirichlet-like WM counts with load-power arbitration.

    Rationale:
    - RL uses eligibility traces to carry credit for recent choices, enabling faster propagation.
    - WM maintains action counts per state (Dirichlet-like), updated only on reward and diffused by noise.
    - Arbitration weight decays with set size as a power-law (3/nS)^gamma.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1), scaled by (3/nS)^gamma
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - trace_lambda: Eligibility trace decay (0..1). Larger = longer memory of past choices
    - wm_noise: Diffusion/noise on WM counts toward being uniform across actions (0..1)
    - gamma: Load sensitivity exponent for WM arbitration (>=0). Larger = stronger down-weight in high load

    Returns: negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, trace_lambda, wm_noise, gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will mirror Dirichlet mean
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for Q
        e = np.zeros((nS, nA))

        # Dirichlet-like counts (start with symmetric prior = 1 per action)
        counts = np.ones((nS, nA))

        # Load-based WM weight scaling
        load_scale = (3.0 / float(nS)) ** gamma
        wm_weight_block = wm_weight * load_scale
        wm_weight_block = max(0.0, min(1.0, wm_weight_block))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update WM probability view from counts (mean of Dirichlet)
            row_sum_counts = np.sum(counts[s, :])
            if row_sum_counts > 0:
                w[s, :] = counts[s, :] / row_sum_counts
            else:
                w[s, :] = w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL with eligibility traces
            delta = r - Q_s[a]
            # decay all eligibilities, then set current
            e *= trace_lambda
            e[s, a] = 1.0
            q += lr * delta * e

            # WM diffusion/noise toward uniform counts (keeps them well-behaved)
            mean_c = np.mean(counts[s, :])
            counts[s, :] = (1.0 - wm_noise) * counts[s, :] + wm_noise * mean_c

            # Reward-driven WM increment of the chosen action
            if r > 0.5:
                counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    PE-gated WM arbitration + uncertainty-tempered RL exploitation.

    Rationale:
    - WM is relied upon more when RL is confident (small absolute prediction error) and under low load.
    - RL becomes more exploitative as state familiarity grows (lower uncertainty), via a visit-based beta.
    - WM stores rewarded associations and decays with a load-dependent rate.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight (0..1), modulated by PE-gating and load
    - softmax_beta: Base RL inverse temperature (scaled by 10 internally)
    - pe_gain: Sensitivity of WM gate to absolute prediction error. Larger = stronger suppression when PE is large
    - wm_alpha: WM encoding increment for the chosen action when rewarded (0..1)
    - wm_decay_load: Base WM decay rate factor that scales with set size (>=0)

    Returns: negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, pe_gain, wm_alpha, wm_decay_load = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track state visit counts to temper RL beta by uncertainty
        visits = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Uncertainty-tempered RL beta: lower when the state is less familiar
            u = 1.0 / (1.0 + max(0.0, visits[s]))  # 1 early, -> 0 with more visits
            beta_eff = base_beta * (1.0 - 0.8 * u)  # cap reduction by 80% early

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute current absolute PE for gating (using current Q)
            delta = r - Q_s[a]
            abs_pe = abs(delta)

            # Load scaling and PE-based gating (more WM when abs PE is small)
            load_scale = min(1.0, 3.0 / float(nS))
            pe_gate = 1.0 / (1.0 + np.exp(-pe_gain * (0.25 - abs_pe)))
            wm_weight = wm_weight_base * load_scale * pe_gate
            wm_weight = max(0.0, min(1.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM decay rate increases with load (set size)
            decay_rate = 1.0 - np.exp(-wm_decay_load * (float(nS) / 3.0))
            decay_rate = max(0.0, min(1.0, decay_rate))
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            # Reward-gated WM encoding
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Update visit count
            visits[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p