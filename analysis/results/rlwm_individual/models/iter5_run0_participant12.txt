def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-gated WM under load

    Description:
    - RL: tabular Q-learning.
    - WM: maintains per-state action weights w[s,:] (probability-like), producing a policy via a sharp softmax.
    - Gating: the effective WM mixture weight is larger when the RL policy is uncertain and smaller under high load.
      Specifically, g = sigmoid(gate_gain * (H_rl_norm - load_penalty * load_norm)), where H_rl_norm is the
      normalized entropy of the RL policy at the current state and load_norm rescales set size to [0,1].
      The final mixture is wm_weight_eff = wm_weight0 * g.
    - WM update: decays toward uniform between trials, then shifts toward a one-hot code for rewarded actions,
      and gently suppresses chosen action on no-reward.
      The decay is also load-sensitive: stronger at higher load to mimic interference.

    Parameters (tuple):
    - lr: RL learning rate for Q update (0..1)
    - wm_weight0: Baseline WM mixture weight (0..1) multiplied by the uncertainty gate
    - softmax_beta: Inverse temperature for RL softmax (rescaled x10 internally)
    - gate_gain: Gain on uncertainty-load gating for WM mixture (higher -> more sensitive gating)
    - load_penalty: How much larger set size reduces effective WM reliance and increases WM decay (>=0)
    """
    lr, wm_weight0, softmax_beta, gate_gain, load_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # sharp WM policy
    eps = 1e-12

    # helper: sigmoid
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load normalization in [0,1] if set sizes are 3 and 6 (generalizable)
        load_min, load_max = 1.0, max(1.0, float(nS))
        # For typical 3 vs 6, map 3->0 and 6->1
        if nS in [3, 6]:
            load_norm = (nS - 3.0) / 3.0
        else:
            load_norm = (nS - load_min) / max(eps, (load_max - load_min))

        # WM decay increases with load to mimic interference
        base_decay = 0.10 + 0.70 * np.clip(load_penalty * load_norm, 0.0, 1.0)  # in [0.1,0.8] approximately
        wm_decay = np.clip(base_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax and entropy (uncertainty)
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl + eps)
            p_rl = max(eps, pvec_rl[a])

            # Normalized entropy of RL policy in [0,1]
            H_rl = -np.sum(pvec_rl * np.log(pvec_rl + eps))
            H_rl_norm = H_rl / np.log(nA)

            # Uncertainty-load gate for WM mixture
            gate = sigmoid(gate_gain * (H_rl_norm - load_penalty * load_norm))
            wm_weight_eff = np.clip(wm_weight0 * gate, 0.0, 1.0)

            # WM softmax policy
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm + eps)
            p_wm = max(eps, pvec_wm[a])

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: rewarded -> push to one-hot; unrewarded -> gentle suppression of chosen
            if r > 0.5:
                # Sharpen memory for the rewarded association
                w[s, :] = 0.05 * w[s, :]
                w[s, a] = 1.0
            else:
                # Mildly suppress the chosen action, drift others slightly up toward uniform
                w[s, a] = 0.85 * w[s, a] + 0.15 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with WM-driven directed exploration bonus

    Description:
    - RL: tabular Q-learning, but choice probabilities are computed from Q' = Q + exploration_bonus.
    - WM: maintains per-state action weights w[s,:] that reflect recent evidence about correct action.
    - Directed exploration: when WM is uncertain at a state (high entropy of W_s), RL receives a bonus
      toward actions that are underrepresented in WM (uniform - W_s). The bonus is amplified under
      higher load (set size), encouraging exploration when WM is less reliable.
    - WM update: decays toward uniform each trial and shifts toward chosen rewarded action; on no reward,
      small attraction toward uniform.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight between WM and RL policies (0..1)
    - softmax_beta: RL inverse temperature (scaled x10 internally)
    - eta_novelty: Scale of directed exploration bonus added to RL values (>=0)
    - load_scale: Multiplier controlling how exploration bonus grows with load (>=0)
    """
    lr, wm_weight, softmax_beta, eta_novelty, load_scale = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 40.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load factor that boosts exploration under high load
        if nS in [3, 6]:
            load_factor = 1.0 + load_scale * ((nS - 3.0) / 3.0)
        else:
            load_factor = 1.0 + load_scale * max(0.0, (nS - 1.0) / max(1.0, nS - 1.0))

        # WM decay slightly stronger under higher load
        wm_decay = np.clip(0.08 + 0.12 * (load_factor - 1.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # WM policy
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm + eps)
            p_wm = max(eps, pvec_wm[a])

            # Compute WM uncertainty (entropy)
            H_wm = -np.sum(pvec_wm * np.log(pvec_wm + eps))
            H_wm_norm = H_wm / np.log(nA)  # [0,1]

            # Directed exploration bonus for RL: push toward actions underrepresented in WM
            # Bonus vector points toward (uniform - W_s), scaled by WM uncertainty and load
            novelty_dir = (1.0 / nA) * np.ones(nA) - W_s
            bonus = eta_novelty * H_wm_norm * load_factor * novelty_dir

            Q_eff = Q_s + bonus

            # RL policy with exploration bonus
            logits_rl = softmax_beta * (Q_eff - np.max(Q_eff))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl + eps)
            p_rl = max(eps, pvec_rl[a])

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and update
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            if r > 0.5:
                # Strengthen evidence for chosen action
                w[s, :] = 0.1 * w[s, :]
                w[s, a] = 1.0
            else:
                # Small attraction toward uniform to prevent overconfidence on errors
                w[s, a] = 0.9 * w[s, a] + 0.1 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Slot-like WM with deterministic coverage and load-dependent slips

    Description:
    - RL: tabular Q-learning with standard softmax.
    - WM: implements a slot-like mechanism with deterministic state coverage within a block:
      Let K = round(cap_frac * nS). The first K states (s = 0..K-1) are assigned "in WM" slots.
      For in-slot states, WM yields a highly deterministic policy favoring the last rewarded action.
      For out-of-slot states, WM is near-uniform with load-dependent slip probability.
    - Slip: WM policy is mixed with uniform at rate slip = min(0.5, slip_load * nS / max(3, nS)),
      increasing with load. This captures greater distractibility/forgetting under higher set sizes.
    - WM update: for in-slot states, rewarded actions are stored as near one-hot; otherwise
      the WM trace remains near uniform.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight between WM and RL (0..1)
    - softmax_beta: RL inverse temperature (scaled x10 internally)
    - cap_frac: Fraction of states that can be actively maintained in WM per block (0..1)
    - slip_load: Coefficient controlling how much WM policy is mixed with uniform as load increases (>=0)
    """
    lr, wm_weight, softmax_beta, cap_frac, slip_load = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm_in = 60.0  # very sharp for in-slot states
    softmax_beta_wm_out = 1.0  # almost flat for out-of-slot states
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Determine deterministic slot coverage for this block
        K = int(np.clip(np.round(cap_frac * nS), 0, nS))

        # Load-dependent slip that mixes WM policy with uniform
        slip = np.clip(slip_load * (nS / max(3.0, float(nS))), 0.0, 0.5)

        # Mild global decay to keep traces from saturating; stronger for larger load implicitly via slip mixture
        wm_decay = 0.05

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            in_slot = (s < K)

            # WM policy depends on slot membership
            if in_slot:
                beta_wm = softmax_beta_wm_in
            else:
                beta_wm = softmax_beta_wm_out

            logits_wm = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm + eps)

            # Apply slip mixture with uniform (more under load)
            pvec_wm = (1.0 - slip) * pvec_wm + slip * (1.0 / nA)
            p_wm = max(eps, pvec_wm[a])

            # RL policy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl + eps)
            p_rl = max(eps, pvec_rl[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update based on slot
            if in_slot:
                if r > 0.5:
                    # Store near one-hot for rewarded action
                    w[s, :] = 0.05 * w[s, :]
                    w[s, a] = 1.0
                else:
                    # Slight suppression of chosen action on error
                    w[s, a] = 0.9 * w[s, a] + 0.1 * (1.0 / nA)
            else:
                # Out-of-slot states remain close to uniform; tiny nudge on positive feedback
                if r > 0.5:
                    w[s, a] = 0.8 * w[s, a] + 0.2 * 1.0  # mild bump that will decay back

        blocks_log_p += log_p

    return -blocks_log_p