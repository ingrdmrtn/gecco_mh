Below are three standalone cognitive models that follow your template structure and constraints. Each function returns the negative log-likelihood of the observed choices and uses all of its parameters. I indicate how set size (3 vs 6) affects behavior for each model in the docstrings.

Note: These functions assume numpy as np is already imported (per your guardrails).

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration, asymmetric RL learning, WM decay, and lapse.

    Mechanism
    - RL: tabular Q-learning with asymmetric learning rates; negative PE rate is scaled by m_neg.
    - WM: probability table over actions per state; on reward it stores a peaked distribution; otherwise it decays toward uniform.
    - Arbitration: based on WM uncertainty (entropy). More certain WM yields higher WM weight via a sigmoid with slope kappa_uncert.
    - Lapse: small epsilon mixing with uniform.

    Set-size dependence
    - The WM entropy that drives arbitration is naturally higher when set size is larger (harder to maintain precise WM),
      thus reducing the WM weight in larger (6) vs smaller (3) set sizes without needing a size-specific parameter.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            Baseline RL learning rate for positive PEs (0..1).
        m_neg : float
            Multiplicative factor for negative PE learning rate: lr_neg = lr * m_neg (0..2 typical).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        decay_wm : float
            WM decay toward uniform after non-rewarded trials (0..1).
        kappa_uncert : float
            Slope for uncertainty-based arbitration (higher = stronger WM when certain).
        epsilon : float
            Lapse probability mixing uniform choice into the final policy (0..0.2 typical).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, m_neg, softmax_beta, decay_wm, kappa_uncert, epsilon = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-based arbitration: WM certainty via normalized entropy
            # Entropy H in [0, log(nA)]; normalize to [0,1] by dividing by log(nA)
            H = -np.sum(np.clip(W_s, tiny, 1.0) * np.log(np.clip(W_s, tiny, 1.0)))
            H_norm = H / np.log(nA)
            certainty = 1.0 - H_norm  # higher when WM more peaked
            wm_weight = 1.0 / (1.0 + np.exp(-kappa_uncert * (certainty - 0.5)))

            # Mixture and lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)

            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rate
            pe = r - Q_s[a]
            alpha = lr if pe >= 0 else lr * m_neg
            q[s, a] += alpha * pe

            # WM update: on reward, store deterministic association; else decay toward uniform
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]
                # renormalize
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with size-sensitive WM weight (logistic) and a WSLS heuristic mixture.

    Mechanism
    - RL: tabular Q-learning with softmax.
    - WM: probability table; on reward it stores a peaked distribution; on non-reward, decays toward uniform.
    - Arbitration RL/WM: WM weight is a logistic function of set size via intercept z_w and slope eta_size.
      This yields higher WM influence for small set sizes and lower influence for large set sizes.
    - WSLS: a heuristic policy mixed into the final choice with weight gamma_wsls.
      Win: repeat last action for the state; Lose: shift uniformly to the other two actions.

    Set-size dependence
    - WM mixture weight depends directly on set size through wm_weight = sigmoid(z_w + eta_size * (size_centered)),
      where size_centered is positive for small sets and negative for large sets. This explicitly captures load effects.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        z_w : float
            Intercept for WM mixture weight logistic (can be any real).
        eta_size : float
            Slope for WM weight vs set size (negative values reduce WM at larger set sizes).
        decay_wm : float
            WM decay toward uniform on non-rewarded trials (0..1).
        gamma_wsls : float
            Mixture weight of WSLS heuristic into final policy (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, z_w, eta_size, decay_wm, gamma_wsls = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action and reward per state for WSLS
        last_act = -1 * np.ones(nS, dtype=int)
        last_rew = np.zeros(nS)

        # Compute WM weight based on set size (center size at 4.5 so that size 3 > 0, size 6 < 0)
        size_centered = 4.5 - nS  # +1.5 for size 3, -1.5 for size 6
        wm_weight = 1.0 / (1.0 + np.exp(-(z_w + eta_size * size_centered)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL/WM mixture
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # WSLS policy for this state
            if last_act[s] == -1:
                # No history for this state yet
                p_wsls = 1.0 / nA
            else:
                if last_rew[s] > 0.5:
                    # Win: repeat
                    if a == last_act[s]:
                        p_wsls = 1.0
                    else:
                        p_wsls = 0.0
                else:
                    # Lose: shift uniformly to other actions
                    if a != last_act[s]:
                        p_wsls = 0.5  # two alternatives equally likely
                    else:
                        p_wsls = 0.0

            # Final mixture with WSLS
            p_total = (1.0 - gamma_wsls) * p_mix + gamma_wsls * p_wsls
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum
                else:
                    w[s, :] = w_0[s, :]

            # Update WSLS memory
            last_act[s] = a
            last_rew[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with count-based exploration bonus + WM with reward-streak gating + lapse.

    Mechanism
    - RL: tabular Q-learning with softmax; action selection uses an optimism bonus inversely proportional to sqrt(visit count).
    - WM: probability table; on reward it stores a peaked distribution; otherwise it decays toward uniform.
    - Arbitration: WM weight depends on a per-state reward streak (consecutive rewards) via a sigmoid gate.
      Longer streaks increase WM control, reflecting stronger, more recent episodic bindings.
    - Lapse: small epsilon mixing with uniform.

    Set-size dependence
    - In larger set sizes (6), states are sampled less frequently, producing shorter streaks on average, thereby reducing WM weight.
      Thus, load affects arbitration indirectly via streak dynamics; exploration bonus also interacts with sparser sampling.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        bonus_eta : float
            Strength of count-based exploration bonus (added to Q for choice only).
        gate_gain : float
            Slope of sigmoid mapping from reward streak length to WM weight.
        decay_wm : float
            WM decay toward uniform after non-rewarded trials (0..1).
        epsilon : float
            Lapse probability mixing uniform choice into the final policy (0..0.2 typical).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, bonus_eta, gate_gain, decay_wm, epsilon = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Count-based exploration: visit counts per state-action
        N = tiny * np.ones((nS, nA))

        # Reward streak per state
        streak = np.zeros(nS, dtype=float)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Exploration bonus for action selection only
            bonus = bonus_eta / np.sqrt(np.maximum(N[s, :], tiny))
            Q_bonus = Q_s + bonus

            # RL policy uses bonus-adjusted values
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_bonus - Q_bonus[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight from reward streak via sigmoid; cap streak to avoid overflow
            streak_capped = min(streak[s], 5.0)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_gain * (streak_capped - 1.0)))

            # Mixture and lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)

            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # Update counts for exploration (after observing action)
            N[s, a] += 1.0

            # RL update without bonus (bonus only biases choice)
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update and streak update
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
                streak[s] += 1.0
            else:
                w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum
                else:
                    w[s, :] = w_0[s, :]
                streak[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p

Notes on the three models
- Model 1 emphasizes dynamic arbitration from WM certainty, plus asymmetric RL learning; set size affects WM certainty.
- Model 2 makes WM weight an explicit logistic function of set size and adds a WSLS heuristic.
- Model 3 adds a count-based exploration bonus to RL and uses reward streaks to gate WM control; set size affects streak accumulation.