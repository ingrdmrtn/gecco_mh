def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity- and recency-limited working memory

    Mechanism
    - RL: standard delta-rule with learning rate lr and softmax with inverse temperature softmax_beta*10.
    - WM: stores a one-shot mapping from state to action after rewarded trials (one-hot for the rewarded action).
      On non-rewarded trials, WM softly decays toward uniform.
    - Mixture: probability of choice is a convex combination of RL and WM policies.
    - Capacity and recency limits:
        * WM contribution is attenuated by set size via min(1, K_capacity / set_size).
        * WM contribution also decays with time since the state was last seen: exp(-wm_decay_time * lag).
      These jointly define an effective WM mixture weight on each trial.

    Parameters
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], base mixture weight on WM before capacity/recency attenuation.
    - softmax_beta: >=0, base inverse temperature for RL (internally scaled by 10).
    - K_capacity: >=0, WM capacity (effective number of state-action mappings WM can hold with full strength).
                   WM influence scales by min(1, K_capacity / set_size). Impacted by set size.
    - wm_decay_time: >=0, recency decay rate per trial for WM contribution as a function of lag since last visit.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_capacity, wm_decay_time = model_parameters

    softmax_beta *= 10.0  # RL inverse temperature scaling
    softmax_beta_wm = 50.0  # near-deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))     # RL values
        w = (1.0 / nA) * np.ones((nS, nA))     # WM weights (policy-like)
        w_0 = (1.0 / nA) * np.ones((nS, nA))   # WM baseline

        # Track last time each state was seen to compute lag
        last_seen = -1 * np.ones(nS, dtype=int)

        # Capacity attenuation due to set size
        cap_attn = min(1.0, float(K_capacity) / float(nS)) if nS > 0 else 1.0

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute recency-based attenuation from lag
            if last_seen[s] == -1:
                lag = 0
            else:
                lag = t - last_seen[s]
            rec_attn = np.exp(-wm_decay_time * float(lag))

            # Effective, trial-wise WM mixture weight
            wm_w_eff = wm_weight * cap_attn * rec_attn
            wm_w_eff = max(0.0, min(1.0, wm_w_eff))

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (1/sumexp trick)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # One-shot encoding on reward; soft decay toward uniform on non-reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Soft decay toward uniform baseline
                decay_strength = 0.2  # implicit fixed decay toward baseline when no reward
                w[s, :] = (1.0 - decay_strength) * w[s, :] + decay_strength * w_0[s, :]

            # Update last seen
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size–dependent inverse temperature + WM stickiness within state

    Mechanism
    - RL: delta-rule with learning rate lr. Inverse temperature decreases with set size:
          beta_eff = (softmax_beta*10) * exp(-beta_size_penalty * (set_size - 3)).
    - WM: encodes a tendency to repeat the last chosen action within the same state ("stickiness").
          After each trial in state s, WM vector is updated toward one-hot(last_action) with rate stickiness.
    - Mixture: convex combination of WM and RL policies with fixed wm_weight.

    Parameters
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], mixture weight on WM stickiness policy.
    - softmax_beta: >=0, base inverse temperature for RL before set-size scaling.
    - stickiness: [0,1], strength of WM update toward last chosen action in that state.
    - beta_size_penalty: >=0, how much RL inverse temperature decreases as set size increases (impacted by set size).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, beta_size_penalty = model_parameters

    softmax_beta_base = softmax_beta * 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))     # RL values
        w = (1.0 / nA) * np.ones((nS, nA))     # WM stickiness policy over actions
        w_0 = (1.0 / nA) * np.ones((nS, nA))   # baseline (not directly used but kept for template consistency)

        # Set-size–dependent RL inverse temperature
        size_factor = np.exp(-beta_size_penalty * max(0.0, float(nS) - 3.0))
        beta_eff = softmax_beta_base * size_factor

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with size-adjusted beta
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Sticky update toward last chosen action irrespective of reward
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - stickiness) * w[s, :] + stickiness * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM elimination with surprise-gated WM weighting and set-size penalty

    Mechanism
    - RL: standard delta-rule with learning rate lr, softmax with inverse temperature softmax_beta*10.
    - WM (Elimination/Episodic): maintains a viability vector over actions per state.
        * After no reward, eliminate the chosen action for that state (set its weight to 0, renormalize others).
        * After reward, one-shot commit to the chosen action (one-hot).
      Policy is a softmax over WM viability with high beta.
    - Mixture: WM mixture weight is dynamically gated by surprise (|prediction error|) and penalized by set size:
        wm_weight_t = sigmoid( logit(wm_weight_base) + gate_surprise_slope*|delta| - size_wm_penalty*(set_size-3) )
      so WM gets more weight on surprising trials and less weight in larger sets.

    Parameters
    - lr: [0,1], RL learning rate.
    - wm_weight_base: (0,1), baseline mixture weight for WM, transformed via logit for gating.
    - softmax_beta: >=0, base inverse temperature for RL (internally scaled by 10).
    - gate_surprise_slope: >=0, strength of surprise-based gating of WM by |delta|.
    - size_wm_penalty: >=0, decrease of WM influence as set size increases (impacted by set size).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, gate_surprise_slope, size_wm_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Helper: stable logistic / logit transforms
    def logistic(x):
        # clip to avoid overflow
        x = np.clip(x, -60.0, 60.0)
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1.0 - 1e-6)
        return np.log(p) - np.log(1.0 - p)

    base_logit = logit(wm_weight_base)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))     # RL values
        w = (1.0 / nA) * np.ones((nS, nA))     # WM viability/episodic weights
        w_0 = (1.0 / nA) * np.ones((nS, nA))   # baseline (kept for completeness)

        # Set-size penalty term (constant within block)
        size_term = size_wm_penalty * max(0.0, float(nS) - 3.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # Surprise (unsigned prediction error based on RL)
            delta = r - Q_s[a]
            surprise = abs(delta)

            # Dynamic WM weight via gated logistic
            wm_logit_t = base_logit + gate_surprise_slope * surprise - size_term
            wm_w_eff = logistic(wm_logit_t)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # Commit to the rewarded action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Eliminate the failed action by setting its viability to 0 and renormalizing others
                w[s, a] = 0.0
                mass = np.sum(w[s, :])
                if mass <= 1e-8:
                    # If everything collapsed (edge case), reset to uniform
                    w[s, :] = 1.0 / nA
                else:
                    w[s, :] /= mass

        blocks_log_p += log_p

    return -blocks_log_p