def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size dependent recall and global WM decay.

    Overview:
    - RL: tabular Q-learning with a single learning rate and softmax policy (beta scaled by 10).
    - WM: per-state action distribution that aims to store the correct action after reward.
      Retrieval is probabilistic and depends on set size:
        * Higher recall in small set size (wm_recall3) than large set size (wm_recall6).
      When recall succeeds, the WM policy selects the WM-best action almost deterministically;
      when recall fails, WM contributes a uniform guess.
      WM contents decay toward uniform at rate wm_decay each trial.
    - Policy: convex mixture of WM and RL controlled by wm_weight.

    Set-size impact:
    - Directly through different recall probabilities (wm_recall3 vs wm_recall6).
    - Indirectly via WM decay that erodes stored associations regardless of set-size, but the
      lower recall at large set-size makes the WM contribution weaker.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in the action policy.
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10.
    - wm_recall3: float in [0,1], probability of recalling WM entry when set size is 3.
    - wm_recall6: float in [0,1], probability of recalling WM entry when set size is 6.
    - wm_decay: float in [0,1], per-trial decay of WM rows toward uniform.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_recall3, wm_recall6, wm_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent recall parameter
        recall_p = wm_recall3 if nS == 3 else wm_recall6

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            # RL choice probability for the chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy:
            W_s = w[s,:]
            # If recall succeeds, choose WM-argmax with near-delta; else uniform guess
            argmax_w = int(np.argmax(W_s))
            if argmax_w == a:
                p_wm_recall = 1.0  # deterministic softmax to argmax
            else:
                p_wm_recall = 0.0
            p_wm_guess = 1.0 / nA
            p_wm = recall_p * p_wm_recall + (1.0 - recall_p) * p_wm_guess

            # Mixture
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM consolidation: move row toward one-hot of chosen action
            if r > 0.0:
                # Consolidate strongly upon reward, respecting normalization
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend current with one-hot; reuse wm_decay as consolidation complement
                alpha_wm = 1.0 - (1.0 - wm_decay)  # ensures usage of wm_decay parameter
                alpha_wm = min(max(alpha_wm, 0.0), 1.0)
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot
                # Normalize for numerical stability
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with load-dependent WM precision and reward-gated WM learning.

    Overview:
    - RL: tabular Q-learning with learning rate lr and softmax policy (beta scaled by 10).
    - WM: per-state probability vector updated toward the chosen action on rewarded trials,
      with learning rate wm_learn. WM policy uses a softmax over WM contents with a
      load-dependent inverse temperature (higher precision at set size 3 than 6).
    - Policy: convex mixture of WM and RL controlled by wm_weight.

    Set-size impact:
    - WM precision (inverse temperature) differs between set sizes (wm_precision3 vs wm_precision6),
      making WM choices more/less deterministic depending on load.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in the action policy.
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10.
    - wm_precision3: float >= 0, WM inverse temperature when set size is 3.
    - wm_precision6: float >= 0, WM inverse temperature when set size is 6.
    - wm_learn: float in [0,1], WM learning rate toward the chosen action upon reward.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_precision3, wm_precision6, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (unused here; maintained for template completeness)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM inverse temperature
        beta_wm_block = wm_precision3 if nS == 3 else wm_precision6

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            # RL choice probability for chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over WM contents (treated as action preferences)
            W_s = w[s,:]
            # To ensure numerical stability, center by chosen action value
            p_wm = 1 / np.sum(np.exp(beta_wm_block * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: reward-gated learning toward chosen action
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot
                # Normalize to keep as a probability vector
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()
            else:
                # Gentle drift toward uniform when not rewarded (prevents overcommitment)
                drift = 0.1 * wm_learn
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with perseveration bias and refresh.

    Overview:
    - RL: tabular Q-learning with lr and softmax (beta scaled by 10).
    - WM: capacity-limited store. Only up to capacity_k states per block receive WM encoding/refresh.
      WM encodes recent chosen action (regardless of reward) via refresh_rate and tends to
      bias toward repeating the last action for encoded states (perseveration).
    - Perseveration: implemented within WM policy via an additional softmax bias toward the last action
      with strength sticky_beta.
    - Policy: convex mixture of WM and RL controlled by wm_weight.

    Set-size impact:
    - When set size exceeds capacity_k, some states do not get WM encoding and thus contribute a near-uniform WM policy,
      effectively shifting control to RL in large-set blocks.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in the action policy.
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10.
    - capacity_k: int or float >= 0, number of states per block that can be actively encoded in WM.
    - sticky_beta: float >= 0, strength of perseveration bias within WM toward the last action for that state.
    - refresh_rate: float in [0,1], rate at which WM refreshes toward the last chosen action on each visit.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_k, sticky_beta, refresh_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (not used directly; perseveration implemented below)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track which states are within capacity and last action per state
        encoded = np.zeros(nS, dtype=bool)
        last_action = -1 * np.ones(nS, dtype=int)
        max_encoded = int(min(max(capacity_k, 0), nS))
        n_encoded = 0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Assign capacity to new states upon first encounter until capacity is filled
            if not encoded[s] and n_encoded < max_encoded:
                encoded[s] = True
                n_encoded += 1

            Q_s = q[s,:]
            # RL choice probability for chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy with perseveration:
            W_s = w[s,:].copy()

            if encoded[s]:
                # Add perseveration bias in logits toward last action if defined
                logits = W_s.copy()
                if last_action[s] >= 0:
                    logits[last_action[s]] += sticky_beta
                # Softmax probability for chosen action a
                p_wm = 1 / np.sum(np.exp(logits - logits[a]))
            else:
                # Not encoded -> near-uniform WM
                p_wm = 1.0 / nA

            # Mixture
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: refresh toward last/just-chosen action for encoded states
            if encoded[s]:
                # Update last action and refresh WM toward current choice (recency-based WM)
                last_action[s] = a
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - refresh_rate) * w[s, :] + refresh_rate * one_hot
                # Normalize to keep probabilities stable
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()
            else:
                # Drift toward uniform for non-encoded states
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p