def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Precision-limited WM with set-size–dependent retrieval lapses.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM retrieval is less precise and more failure-prone as set size increases:
      - Effective WM precision beta_wm_eff scales with (3 / nS).
      - WM retrieval lapse lambda_wm increases with set size.
    - WM stores the currently chosen action with a reward-scaled bump, and decays toward a uniform prior.

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate; also used as the base decay step for WM.
        wm_weight : float in [0,1]
            Mixture weight for WM in the final policy. Also shapes WM lapse (higher => more lapses in large sets)
            and WM learning strength.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10). WM precision is fixed high but scaled by set size.

    Set-size impact
    ---------------
    - WM precision decreases with set size: beta_wm_eff = 50 * (3/nS).
    - WM retrieval lapse increases with set size: lambda_wm = wm_weight * max(0, (nS-3)/nS).
    - WM update uses reward-scaled increments and set-size–dependent decay to prior.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Set-size–dependent WM precision and lapse
            beta_wm_eff = softmax_beta_wm * (3.0 / max(1.0, float(nS)))
            # Lapse grows with set size; bounded to [0,1)
            lambda_wm = wm_weight * max(0.0, (float(nS) - 3.0) / max(1.0, float(nS)))
            # Softmax WM policy for chosen action
            p_wm_soft = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            # Retrieval failure leads to uniform choice
            p_wm = (1.0 - lambda_wm) * p_wm_soft + lambda_wm * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward prior with rate that modestly increases with set size
            decay = lr * (1.0 + max(0.0, (float(nS)-3.0)/max(1.0, float(nS))) * wm_weight)
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # Reward-weighted storage bump on chosen action, scaled by wm_weight
            w[s, a] += wm_weight * (0.5 + 0.5 * r)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Confidence-gated WM (thresholded retrieval) with reward-gated storage.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM contribution is computed as usual, but if WM is not confident (low margin between best
      and second-best WM value), we defer to RL by setting p_wm = p_rl (confidence gate).
    - Confidence threshold increases with set size, making WM less likely to engage in larger sets.
    - WM storage is reward-gated: positive feedback yields stronger storage; negative yields decay.

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate and WM decay base step.
        wm_weight : float in [0,1]
            Mixture weight; also scales the WM confidence threshold and storage strength.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10). WM has fixed high precision.

    Set-size impact
    ---------------
    - WM confidence threshold theta grows with set size: theta = wm_weight * (nS-3)/nS.
      Higher nS -> more deferral to RL.
    - WM storage uses reward r to scale the bump; lack of reward triggers more decay.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute WM confidence: margin between best and second best
            sort_idx = np.argsort(W_s)[::-1]
            top = W_s[sort_idx[0]]
            second = W_s[sort_idx[1]] if nA > 1 else W_s[sort_idx[0]]
            margin = top - second
            # Set-size–dependent confidence threshold
            theta = wm_weight * max(0.0, (float(nS) - 3.0) / max(1.0, float(nS)))
            # Base WM policy (deterministic softmax)
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Confidence gate: if margin too small, fall back to RL for WM component
            p_wm = p_wm_soft if (margin > theta) else p_rl

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-gated storage: stronger bump when rewarded, otherwise more decay
            bump = wm_weight * (0.25 + 0.75 * r)  # in [0.25, 1.0]
            decay = lr * (0.75 + 0.25 * (1.0 - r))  # slightly larger decay when r=0
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            w[s, a] += bump

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Chunked WM: state-specific WM blended with a block-level action prior.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM retrieval combines:
      - State-specific memory W_s.
      - A block-level action prior (capturing chunking/heuristics across states), approximated by the
        per-row prior w_0[s,:] that we adapt across states during the block.
    - The influence of the global prior grows when set size is larger (harder WM), reducing reliance
      on purely state-specific WM content.

    Parameters
    ----------
    parameters : tuple/list of length 3
        lr : float in [0,1]
            RL learning rate; also used to softly update the global WM prior.
        wm_weight : float in [0,1]
            Mixture weight; also controls how strongly the global prior enters WM retrieval.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10). WM is near-deterministic.

    Set-size impact
    ---------------
    - Global WM prior gain gamma_prior = wm_weight * (nS/3) increases with set size.
    - Global prior is updated (small step) toward the chosen action across all states, enabling chunking.
    - State-specific WM still gets a reward-scaled bump and mild decay to the evolving prior.
    """
    lr, wm_weight, softmax_beta = parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Combine state-specific WM and a set-size–weighted global prior
            gamma_prior = wm_weight * (float(nS) / 3.0)  # larger for bigger sets
            combined_pref = W_s + gamma_prior * (w_0[s, :] - (1.0 / nA))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (combined_pref - combined_pref[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) State-specific WM: decay to current prior and reward-scaled storage bump
            w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]
            w[s, a] += (0.5 + 0.5 * r) * wm_weight

            # 2) Update global prior across all states toward the chosen action (chunking)
            eta_global = 0.25 * lr * wm_weight * (float(nS) / 3.0)
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            w_0 = (1.0 - eta_global) * w_0 + eta_global * onehot[None, :]

        blocks_log_p += log_p

    return -blocks_log_p