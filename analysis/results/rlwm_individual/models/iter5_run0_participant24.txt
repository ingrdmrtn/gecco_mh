def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall associability and WM with load-dependent interference.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with high determinism (fixed 50).
    - Arbitration: fixed wm_weight blending WM and RL.

    RL mechanisms:
    - State-wise associability alpha_s tracks recent unsigned prediction errors (Pearce-Hall):
        alpha_s <- (1 - kappa)*alpha_s + kappa*|PE|
      The effective learning rate is lr_eff = lr_base * alpha_s (0..lr_base).
      This increases learning when outcomes are surprising for that state.

    WM mechanisms:
    - Passive global decay toward uniform at rate wm_decay each trial.
    - Rewarded strengthening of the chosen action entry in W.
    - Load-dependent interference: WM policy is mixed with uniform by
      interference = clip(wm_load_sensitivity * (nS - 3) / 3, 0, 1).

    Parameters:
    - lr_base: Baseline RL learning rate (0..1).
    - kappa: Associability update rate (0..1); larger => faster adaptation of alpha.
    - softmax_beta: RL inverse temperature (rescaled by 10).
    - wm_weight: Arbitration weight on WM (0..1).
    - wm_decay: Global WM decay toward uniform each trial (0..1).
    - wm_load_sensitivity: Scales the increase in WM interference from set size 3 to 6.

    Set-size effect:
    - WM interference increases with set size via wm_load_sensitivity, reducing WM precision more in the 6-item condition.
    """
    lr_base, kappa, softmax_beta, wm_weight, wm_decay, wm_load_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        alpha = np.ones(nS) * 0.5  # initial associability per state

        interference = np.clip(float(wm_load_sensitivity) * (nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL choice probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute WM choice probability with load interference
            W_s = w[s, :]
            p_wm_base = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - interference) * p_wm_base + interference * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with associability-modulated learning rate
            pe = r - Q_s[a]
            alpha[s] = (1.0 - kappa) * alpha[s] + kappa * abs(pe)
            lr_eff = np.clip(lr_base * alpha[s], 0.0, 1.0)
            q[s, a] += lr_eff * pe

            # WM global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Reward-contingent strengthening of chosen action memory
            if r > 0.0:
                w[s, a] += lr_base * (1.0 - w[s, a])
            else:
                w[s, a] -= 0.25 * lr_base * w[s, a]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with fixed learning rate and entropy-based arbitration; WM with ISI-dependent decay.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over an effective WM row W_eff that decays with time since last presentation (ISI).
    - Arbitration: wm_weight_t = sigmoid(a_w + b_w * (H_rl - H_wm)),
      where H_rl and H_wm are the entropies (in nats) of the current RL and WM choice distributions.
      This shifts weight toward the less-entropic (more certain) system.

    RL mechanisms:
    - Single learning rate lr on chosen state-action.

    WM mechanisms:
    - Each state's memory decays toward uniform as a function of the number of trials since it was last seen:
        W_eff[s] = (1 - d_s)*W[s] + d_s*w_0[s], with d_s = 1 - exp(-wm_lambda * ISI_s).
    - Rewarded strengthening of W[s,a] for positive outcomes with gain wm_gain; mild weakening on zero reward.

    Parameters:
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (rescaled by 10).
    - wm_gain: Strength of WM strengthening on rewarded trials.
    - wm_lambda: Rate of ISI-based WM decay (>=0); larger => faster decay with longer lags.
    - a_w: Arbitration bias (logit) toward WM (>0 favors WM).
    - b_w: Arbitration sensitivity to entropy difference (H_rl - H_wm).

    Set-size effect:
    - Larger set sizes naturally increase ISIs for each state, amplifying the impact of wm_lambda
      and thus reducing WM reliability in 6-item blocks without explicitly using set size in parameters.
    """
    lr, softmax_beta, wm_gain, wm_lambda, a_w, b_w = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track time since last seen for each state (ISI in trials)
        isi = np.ones(nS) * 0  # initialize zero; we will increment at the start of each trial

        log_p = 0.0
        for t in range(len(block_states)):
            # Increment ISI for all states at the start of the trial
            isi += 1

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective WM row with ISI-dependent decay
            d_s = 1.0 - np.exp(-wm_lambda * float(isi[s]))
            d_s = np.clip(d_s, 0.0, 1.0)
            W_s_eff = (1.0 - d_s) * w[s, :] + d_s * w_0[s, :]

            # RL distribution for state s
            Q_s = q[s, :]
            # Compute full distributions to get entropies
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / np.sum(pi_rl)
            # WM distribution
            pi_wm = np.exp(softmax_beta_wm * (W_s_eff - np.max(W_s_eff)))
            pi_wm = pi_wm / np.sum(pi_wm)

            # Entropies (nats)
            H_rl = -np.sum(np.clip(pi_rl, 1e-12, 1.0) * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            H_wm = -np.sum(np.clip(pi_wm, 1e-12, 1.0) * np.log(np.clip(pi_wm, 1e-12, 1.0)))

            # Arbitration weight toward WM based on relative certainty
            wm_weight_t = 1.0 / (1.0 + np.exp(-(a_w + b_w * (H_rl - H_wm))))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Choice probabilities for chosen action a
            p_rl = pi_rl[a]
            p_wm = pi_wm[a]
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: reset ISI for the visited state, then apply reward-based modification
            isi[s] = 0
            if r > 0.0:
                w[s, a] += wm_gain * (1.0 - w[s, a])
            else:
                w[s, a] -= 0.25 * wm_gain * w[s, a]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with across-state generalization and WM with simple decay.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with high determinism (fixed 50).
    - Arbitration: fixed wm_weight blending WM and RL.

    RL mechanisms:
    - Standard learning on visited state-action with learning rate lr.
    - Across-state generalization: a fraction of the RL update for (s,a) spreads to the same action a
      in other states s' != s at rate gen_eff = gen_base + gen_slope * (nS - 3) / 3, evenly divided.
      This captures interference or shared representations that increase under higher load.

    WM mechanisms:
    - Passive decay toward uniform at rate wm_decay each trial.
    - Rewarded strengthening for chosen action.

    Parameters:
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (rescaled by 10).
    - wm_weight: Arbitration weight on WM (0..1).
    - wm_decay: WM decay toward uniform (0..1).
    - gen_base: Baseline across-state generalization (0..1).
    - gen_slope: Increase in generalization from set size 3 to 6.

    Set-size effect:
    - RL generalization gen_eff increases with set size via gen_slope, causing more interference
      (or beneficial transfer) across the 6-item set than the 3-item set.
    """
    lr, softmax_beta, wm_weight, wm_decay, gen_base, gen_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        gen_eff = np.clip(float(gen_base) + float(gen_slope) * (nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with across-state generalization
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            if nS > 1:
                spread = gen_eff * lr * pe
                if nS - 1 > 0:
                    q[np.arange(nS) != s, a] += spread / (nS - 1)

            # WM decay and strengthening
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.0:
                w[s, a] += lr * (1.0 - w[s, a])
            else:
                w[s, a] -= 0.25 * lr * w[s, a]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p