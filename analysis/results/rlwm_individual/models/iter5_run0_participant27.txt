def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–modulated arbitration and within-state choice stickiness.

    Mechanism
    ---------
    - RL: single learning rate; softmax choice as in template.
    - WM: a fast one-shot mapping that shifts toward a one-hot vector for rewarded actions
          and toward uniform for non-rewarded outcomes.
    - Stickiness: a state-specific perseveration bonus added to the last action taken in that state.
                  This bias influences both RL and WM policies.
    - Arbitration: fixed base WM weight transformed by a logistic and modulated by set size,
      making WM more influential in small sets and less in large sets.

    Set-size effects
    ----------------
    - The WM mixture weight is scaled by a linear term in (3 - nS): wm_weight_eff = sigmoid(wm_base + delta_size*(3 - nS)).
      Thus, WM has higher weight in set size 3 and lower in set size 6.
    - WM inverse temperature (beta_wm) is a free parameter independent of RL beta.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_base, softmax_beta, beta_wm, delta_size, stickiness)
        - lr: RL learning rate (0..1), also used as WM learning rate.
        - wm_base: Base tendency to rely on WM (mapped via sigmoid to [0,1]).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - beta_wm: WM inverse temperature (>0), controls determinism of WM policy.
        - delta_size: Set-size sensitivity of WM mixture weight; positive values favor WM for nS=3.
        - stickiness: Perseveration strength added to last chosen action value within a state.
    """
    lr, wm_base, softmax_beta, beta_wm, delta_size, stickiness = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # unused constant from template base; we use beta_wm below
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -np.ones(nS, dtype=int)

        # Set-size–modulated WM weight (sigmoid)
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))
        wm_weight_eff = sigmoid(wm_base + delta_size * (3 - nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add stickiness bias to last action for both policies
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
                W_s[last_action[s]] += stickiness

            # RL policy (kept template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values with its own temperature
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: rewarded -> toward one-hot; not rewarded -> toward uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Normalize WM row to a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-driven arbitration and set-size–scaled WM temperature.

    Mechanism
    ---------
    - RL: single learning rate with softmax choice.
    - WM: value table that shifts toward one-hot on reward and toward uniform on no reward,
          with a per-visit WM leak toward uniform.
    - Arbitration: WM weight is computed from RL uncertainty at the current state using
      the entropy of the RL softmax distribution. Higher RL uncertainty -> higher WM reliance.
    - Set size affects WM temperature via a power-law scaling.

    Set-size effects
    ----------------
    - WM inverse temperature scales as beta_wm_eff = beta_wm * (3/nS)^zeta_size,
      making WM more deterministic in smaller sets and less in larger sets.

    Parameters
    ----------
    model_parameters : tuple
        (lr, softmax_beta, beta_wm, wm_gain, leak_wm, zeta_size)
        - lr: RL learning rate (0..1), reused for WM update strength.
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - beta_wm: Base WM inverse temperature (>0), scaled by set size.
        - wm_gain: Gain mapping RL entropy to WM weight (>=0).
        - leak_wm: WM leak toward uniform on each visit (0..1).
        - zeta_size: Exponent controlling set-size scaling of WM temperature (>=0).
    """
    lr, softmax_beta, beta_wm, wm_gain, leak_wm, zeta_size = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base from template; we use beta_wm with scaling below
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling for WM inverse temperature
        size_scale = (3.0 / float(nS)) ** max(zeta_size, 0.0)
        beta_wm_eff = max(beta_wm, 0.0) * size_scale

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL uncertainty (entropy of RL softmax over actions)
            logits = softmax_beta * Q_s
            logits = logits - np.max(logits)
            exp_logits = np.exp(logits)
            prob_rl_vec = exp_logits / np.sum(exp_logits)
            # Entropy in nats
            entropy = -np.sum(prob_rl_vec * np.log(np.maximum(prob_rl_vec, 1e-12)))
            # Normalize entropy by max entropy (log nA) and map to WM weight
            H_norm = entropy / np.log(nA)
            wm_weight = 1.0 / (1.0 + np.exp(-wm_gain * (H_norm - 0.5)))  # centered at 0.5 uncertainty

            # WM policy as softmax over WM values
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform on each visit
            w[s, :] = (1.0 - leak_wm) * w[s, :] + leak_wm * w_0[s, :]

            # WM update toward one-hot on reward, toward uniform on no reward
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay + episodic WM with binding noise that increases with set size.

    Mechanism
    ---------
    - RL: single learning rate and per-visit decay of Q toward uniform (forgetting).
    - WM: when rewarded, store a near-one-hot association with binding noise; when not rewarded,
          WM drifts toward uniform. WM policy uses the stored distribution directly (no softmax).
    - Arbitration: fixed WM weight, independent of trial-wise uncertainty.
    - Set size increases WM binding noise, reducing WM precision in larger sets.

    Set-size effects
    ----------------
    - Binding noise increases linearly with set size:
      bind_eff = clip(bind_noise + size_slope * (nS - 3) / 3, 0, 1).
      This yields more precise WM in set size 3 and noisier WM in set size 6.

    Parameters
    ----------
    model_parameters : tuple
        (lr, softmax_beta, wm_weight, q_decay, bind_noise, size_slope)
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - wm_weight: Fixed mixture weight on WM policy (0..1).
        - q_decay: Per-visit decay of Q toward uniform (0..1).
        - bind_noise: Base WM binding noise (0..1), fraction of probability spread to non-chosen actions.
        - size_slope: How much binding noise increases with set size (can be negative or positive).
    """
    lr, softmax_beta, wm_weight, q_decay, bind_noise, size_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # not used; WM policy is a stored distribution
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective binding noise as a function of set size
        bind_eff = bind_noise + size_slope * (float(nS) - 3.0) / 3.0
        bind_eff = min(max(bind_eff, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: use stored WM distribution directly
            p_wm = max(W_s[a], 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            # First decay Q on visited state
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * w_0[s, :]
            # Then apply learning step
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.0:
                # Store a noisy one-hot: mass (1 - bind_eff) on chosen, rest spread uniformly
                w[s, :] = (bind_eff / (nA - 1)) * np.ones(nA)
                w[s, a] = 1.0 - bind_eff
            else:
                # Drift toward uniform on no reward
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p