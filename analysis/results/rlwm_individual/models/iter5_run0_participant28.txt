def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited, decaying WM with set-size–attenuated contribution.

    Mechanism:
    - RL: delta-rule Q-learning.
    - WM: per-state one-shot storage of the last rewarded action (one-hot), with trial-by-trial decay
      toward uniform and a limited number of states that can be held with high fidelity (capacity).
    - Arbitration: mixture of WM and RL, where the WM mixture weight is attenuated as set size increases.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight: Base mixture weight of WM in decision (0..1).
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - K_cap: Effective WM capacity (number of states that can be sharply represented).
    - wm_decay_lambda: Decay rate of WM toward uniform per trial (0..1).
    - wm_load_slope: How strongly WM weight is reduced as set size increases (>=0).
      wm_weight_eff = wm_weight / (1 + wm_load_slope*(nS-3)).

    Set-size impacts:
    - WM: weight is reduced as nS increases; capacity limits make more states fall back to uniform WM.
      Larger sets therefore rely more on RL.
    """
    lr, wm_weight, softmax_beta, K_cap, wm_decay_lambda, wm_load_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track which states are stored in capacity-limited WM
        stored = np.zeros(nS, dtype=bool)
        max_slots = int(np.round(K_cap))

        # Effective WM contribution reduced with load
        wm_weight_eff = wm_weight / (1.0 + wm_load_slope * max(0.0, float(nS) - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM decay toward uniform each trial (global)
            if wm_decay_lambda > 0:
                w = (1.0 - wm_decay_lambda) * w + wm_decay_lambda * w_0

            # WM policy: high fidelity if within capacity and previously stored; else uniform
            if stored[s]:
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                # Not stored or out of capacity: WM contributes as uniform
                p_wm = 1.0 / nA

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store one-hot on rewarded trials, with capacity gating
            if r > 0.5:
                # If not stored and capacity available, allocate a slot
                if not stored[s]:
                    if np.sum(stored) < max_slots:
                        stored[s] = True
                # Update the WM trace to one-hot
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size–dependent learning rate + WM with binding noise.

    Mechanism:
    - RL: delta-rule with learning rate attenuated under higher set size (more items -> slower RL updating).
    - WM: one-shot storage of last rewarded action, but retrieval suffers action-binding noise: with
      probability eps, WM suggests a random action; otherwise it retrieves the stored action.
    - Arbitration: fixed mixture weight for WM vs RL.

    Parameters (5):
    - lr_base: Base RL learning rate (0..1).
    - wm_weight: Mixture weight for WM (0..1).
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - lr_load_slope: Load sensitivity of RL learning rate (>=0), lr_eff = lr_base / (1 + lr_load_slope*(nS-3)).
    - wm_binding_noise: Probability that WM retrieval yields a random action (0..1).

    Set-size impacts:
    - RL: learning rate is reduced as set size increases, slowing learning in 6-item blocks.
    - WM: binding noise is constant here, but the relative contribution to behavior depends on the
      RL update efficacy (indirectly influenced by set size).
    """
    lr_base, wm_weight, softmax_beta, lr_load_slope, wm_binding_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic for the non-noisy WM channel
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL learning rate reduced with load
        lr = lr_base / (1.0 + lr_load_slope * max(0.0, float(nS) - 3.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with binding noise
            W_s = w[s, :]
            p_wm_clean = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - wm_binding_noise) * p_wm_clean + wm_binding_noise * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated storage; non-reward causes gentle reversion to uniform
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # small decay toward uniform when incorrect
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reliability-based arbitration and set-size–dependent WM hazard.

    Mechanism:
    - RL: standard delta-rule.
    - WM: one-shot storage of last rewarded action; traces are forgotten via a hazard that increases
      with set size (more items -> faster forgetting toward uniform).
    - Arbitration: dynamic, based on relative reliability of WM vs RL on the current state.
      Reliability for WM is low entropy (peaky distribution), while for RL it’s the Q-value spread.
      The mixture weight is the sigmoid of their difference, tempered by arbitration_tau and blended
      with a base wm_weight.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight: Baseline WM weight (0..1) combined with reliability-driven arbitration.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_beta: WM inverse temperature controlling how deterministic WM policy is (replaces fixed 50).
    - hazard_wm: Base WM hazard; effective hazard = 1 - exp(-hazard_wm*(nS-3 if nS>3 else 0)).
    - arbitration_tau: Sensitivity of the arbitration to reliability differences (>=0).

    Set-size impacts:
    - WM: forgetting hazard increases with set size, causing faster reversion to uniform in 6-item blocks.
    - Arbitration: when WM becomes less reliable (e.g., under higher load), the model shifts toward RL.
    """
    lr, wm_weight, softmax_beta, wm_beta, hazard_wm, arbitration_tau = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = max(1e-6, wm_beta)  # use provided WM sharpness
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM hazard
        load_term = max(0.0, float(nS) - 3.0)
        hazard_eff = 1.0 - np.exp(-hazard_wm * load_term)
        hazard_eff = np.clip(hazard_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM hazard forgetting toward uniform (state-specific, applied each time state is visited)
            if hazard_eff > 0:
                w[s, :] = (1.0 - hazard_eff) * w[s, :] + hazard_eff * w_0[s, :]

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Reliability-based arbitration
            # WM reliability: inverse entropy proxy via max probability
            wm_probs = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_probs = wm_probs / np.sum(wm_probs)
            wm_rel = np.max(wm_probs)  # higher when distribution is sharp

            # RL reliability: spread of Q-values (normalized)
            Q_centered = Q_s - np.mean(Q_s)
            rl_rel = np.sqrt(np.mean(Q_centered**2))  # RMS spread
            # Normalize RL reliability to [0,1] approximately via soft saturation
            rl_rel = rl_rel / (1.0 + rl_rel)

            # Dynamic WM weight via sigmoid of reliability difference
            rel_diff = wm_rel - rl_rel
            wm_dyn = 1.0 / (1.0 + np.exp(-arbitration_tau * rel_diff))
            # Blend with base wm_weight to ensure parameter is used meaningfully
            wm_mix = 0.5 * wm_weight + 0.5 * wm_dyn

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated one-shot storage
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p