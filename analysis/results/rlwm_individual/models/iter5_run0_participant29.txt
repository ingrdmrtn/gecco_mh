def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + leaky graded working memory with set-size-dependent decay and exploration.

    Idea:
    - RL learns Q-values with a standard learning rate.
    - WM stores a graded one-hot trace of the last chosen action weighted by outcome (reward),
      and decays toward uniform with a decay that increases with set size (more interference).
    - Policy is a mixture of RL and WM; RL exploration (softmax_beta) decreases under load.

    Parameters (model_parameters):
    - lr:           RL learning rate (0..1)
    - wm_weight:    Mixture weight of WM policy (0..1)
    - softmax_beta: Base RL inverse temperature; scaled by 10 internally
    - beta_shrink:  Load effect on RL inverse temperature; beta_eff = beta / (1 + beta_shrink*(nS-3))
    - wm_eta:       WM learning rate toward a reward-weighted one-hot of the chosen action (0..1)
    - interfer_k:   Load-dependent WM decay strength; wm_decay = 1 - 1 / (1 + interfer_k*(nS-3)+1e-8)

    Set-size impact:
    - RL: inverse temperature decreases with set size via beta_shrink (more exploration at nS=6).
    - WM: decay toward uniform increases with set size via interfer_k (greater interference at nS=6).
    """
    lr, wm_weight, softmax_beta, beta_shrink, wm_eta, interfer_k = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        beta_eff = softmax_beta / (1.0 + max(0.0, float(beta_shrink) * float(nS - 3)))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay
        wm_decay = 1.0 - 1.0 / (1.0 + max(0.0, float(interfer_k) * float(nS - 3)) + 1e-8)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action a (ratio-stable form)
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM softmax probability based on WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM graded write toward reward-weighted one-hot of chosen action
            target = np.full(nA, 0.0)
            target[a] = r  # if r=1, move to one-hot; if r=0, no strengthening
            w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric WM gating by recent outcomes and load-dampened WM reliance.

    Idea:
    - RL learns Q-values as usual.
    - WM stores the most recently rewarded action per state (one-hot when reward occurs),
      and otherwise weakly decays toward uniform.
    - The policy mixture weight for WM is dynamic: it is updated by recent outcomes
      via separate gains for positive and negative outcomes, and scaled down under higher set size.

    Parameters (model_parameters):
    - lr:            RL learning rate (0..1)
    - softmax_beta:  RL inverse temperature; scaled by 10 internally
    - wm_w_base:     Baseline WM weight (0..1)
    - wm_gain_pos:   Increase of WM reliance after reward (0..1)
    - wm_gain_neg:   Decrease of WM reliance after no reward (0..1)
    - wm_load_shrink:Load penalty on WM reliance; wm_w = wm_w * (1 / (1 + wm_load_shrink*(nS-3)))

    Set-size impact:
    - WM reliance is scaled down with set size via wm_load_shrink.
    - RL temperature is not load-modulated here; load effect is isolated to WM usage.
    """
    lr, softmax_beta, wm_w_base, wm_gain_pos, wm_gain_neg, wm_load_shrink = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-scaled baseline WM weight
        wm_w = wm_w_base / (1.0 + max(0.0, float(wm_load_shrink) * float(nS - 3)))
        wm_w = np.clip(wm_w, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Small fixed WM decay for stability; tied to negative gain to keep parameter count
        wm_decay = np.clip(0.25 * wm_gain_neg, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with current wm_w
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, overwrite toward one-hot of chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_gain_pos) * w[s, :] + wm_gain_pos * one_hot

            # Adapt WM reliance based on outcome (meta-control)
            if r > 0.5:
                wm_w = wm_w + wm_gain_pos * (1.0 - wm_w)
            else:
                wm_w = wm_w - wm_gain_neg * wm_w
            wm_w = np.clip(wm_w, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL combined with state-specific choice-kernel (habit-like WM) under load.

    Idea:
    - RL updates Q-values for state-action pairs.
    - WM is implemented as a state-specific choice kernel that boosts recently chosen actions,
      capturing short-term repetition tendencies that can aid learning in small set sizes.
    - The choice kernel decays more under higher set sizes (greater interference).
    - RL learning is mildly suppressed after negative outcomes with a load-sensitive factor.

    Parameters (model_parameters):
    - lr:              Base RL learning rate (0..1)
    - wm_weight:       Mixture weight of WM (choice-kernel) policy (0..1)
    - softmax_beta:    RL inverse temperature; scaled by 10 internally
    - ck_eta:          Choice-kernel learning rate (0..1) toward one-hot of chosen action
    - ck_load:         Load effect on choice-kernel decay; decay = 1 - 1/(1 + ck_load*(nS-3)+1e-8)
    - neg_suppress:    Negative-outcome suppression of RL update scaled by load
                       lr_eff = lr * (1 - neg_suppress * (nS-3)) on negative prediction errors

    Set-size impact:
    - WM/choice-kernel decays more with set size via ck_load.
    - RL learning after negative outcomes is suppressed more under higher load via neg_suppress.
    """
    lr, wm_weight, softmax_beta, ck_eta, ck_load, neg_suppress = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent decay for choice kernel
        ck_decay = 1.0 - 1.0 / (1.0 + max(0.0, float(ck_load) * float(nS - 3)) + 1e-8)
        ck_decay = np.clip(ck_decay, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # here w is the choice kernel per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with load-sensitive negative suppression
            delta = r - Q_s[a]
            if delta < 0.0:
                lr_eff = lr * (1.0 - max(0.0, float(neg_suppress) * float(nS - 3)))
                lr_eff = np.clip(lr_eff, 0.0, 1.0)
            else:
                lr_eff = lr
            q[s, a] += lr_eff * delta

            # Choice-kernel decay toward uniform
            w[s, :] = (1.0 - ck_decay) * w[s, :] + ck_decay * w_0[s, :]

            # Choice-kernel update toward chosen action (independent of reward)
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - ck_eta) * w[s, :] + ck_eta * one_hot

        blocks_log_p += log_p

    return -blocks_log_p