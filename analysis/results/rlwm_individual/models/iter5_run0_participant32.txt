def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Associative WM mixture with set-size-driven WM interference.
    - Choice policy is a mixture between model-free RL and a working-memory (WM) controller.
    - WM is an associative probability map over actions that updates on every trial
      (Hebbian-like), decays toward uniform, and suffers more interference when set size is large.
    - Set size reduces the effective precision of WM by contracting its action preference
      toward uniform before the WM softmax.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Mixture weight of WM in the policy (0-1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_eta: WM learning rate for associative update toward the experienced outcome.
              On reward, increases prob. of chosen action; on no-reward, decreases it.
    - wm_decay: Per-trial decay of WM toward uniform (0-1).
    - wm_interf: Set-size interference scaling; contracts WM preferences by factor
                 c = 1 / (1 + wm_interf * (nS - 3)) before the WM softmax.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_eta, wm_decay, wm_interf = model_parameters
    softmax_beta *= 10.0  # higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent contraction factor for WM precision
        c_ss = 1.0 / (1.0 + wm_interf * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: contract preferences toward uniform by c_ss before softmax
            mu = 1.0 / nA
            W_tilde = mu + c_ss * (W_s - mu)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_tilde - W_tilde[a])))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Associative WM update: push probability toward the observed outcome
            target = np.full(nA, (1.0 - r) / (nA - 1))
            if r > 0.0:
                target[:] = 0.0
                target[a] = 1.0
            else:
                target[a] = 0.0  # chosen action is discouraged on no-reward

            w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            # Renormalize and clip for numerical stability
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Episodic WM with confidence gating and set-size-scaled learning of confidence.
    - WM stores a near one-hot action distribution when rewarded (episodic memory),
      otherwise decays toward uniform.
    - A per-state confidence variable gates the sharpness of WM policy: higher confidence
      makes WM more deterministic. Confidence increases after reward and decreases after no reward.
    - Set size weakens confidence learning by scaling both positive and negative confidence updates.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Mixture weight of WM in the policy (0-1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - c_pos: Confidence learning rate after reward (0-1).
    - c_neg: Confidence learning rate after no reward (0-1).
    - ss_scale: Set-size scaling of confidence learning; effective update rates are
                c_pos * (3/nS)^ss_scale and c_neg * (3/nS)^ss_scale.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, c_pos, c_neg, ss_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base determinism
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state confidence, initialized lower as set size grows
        conf = np.full(nS, (3.0 / float(nS)))
        # Set-size scaling of confidence learning
        conf_scale = (3.0 / float(nS)) ** ss_scale

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: confidence shrinks or amplifies deviation from uniform
            mu = 1.0 / nA
            W_tilde = mu + np.clip(conf[s], 0.0, 1.0) * (W_s - mu)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_tilde - W_tilde[a])))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]  # mild base decay for stability

            # Episodic WM update on reward: store one-hot; else just decay
            if r > 0.0:
                w[s, :] = eps
                w[s, a] = 1.0
                w[s, :] = w[s, :] / np.sum(w[s, :])

            # Confidence update with set-size scaling
            if r > 0.0:
                conf[s] = conf[s] + conf_scale * c_pos * (1.0 - conf[s])
            else:
                conf[s] = conf[s] - conf_scale * c_neg * (conf[s] - 0.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with decay + WM gated by prediction-error magnitude, with set-size threshold shift.
    - RL values decay toward uniform each trial (forgetting).
    - WM is episodic: after reward, WM stores the chosen action as one-hot; otherwise it decays.
    - The contribution of WM on a given trial is effectively reduced when RL signals large surprise:
      we shrink WM sharpness based on the absolute prediction error |PE|. Larger |PE| -> weaker WM.
    - Set size increases the PE threshold at which WM sharpness is reduced.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Mixture weight of WM in the policy (0-1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - rl_decay: Per-trial decay of Q-values toward uniform (0-1).
    - gate_k: Slope of the gating function mapping |PE| to WM sharpness (>=0).
    - ss_theta: Set-size threshold factor; effective threshold is theta = ss_theta * (nS/3).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rl_decay, gate_k, ss_theta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base for WM softmax
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        theta = ss_theta * (float(nS) / 3.0)  # higher threshold under higher set size

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting toward uniform
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute RL likelihood and PE for gating
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            pe = r - Q_s[a]
            # Map |PE| to a WM sharpness factor g in [0,1] via a smooth gate:
            # g = 1 / (1 + exp(k*(|PE|-theta))) -> high g when |PE| < theta (trust WM), low otherwise
            g = 1.0 / (1.0 + np.exp(gate_k * (abs(pe) - theta)))

            # Shrink WM preferences toward uniform based on g
            mu = 1.0 / nA
            W_tilde = mu + g * (W_s - mu)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_tilde - W_tilde[a])))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with PE
            q[s, a] += lr * pe

            # WM decay and episodic write on reward
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.0:
                w[s, :] = eps
                w[s, a] = 1.0
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p