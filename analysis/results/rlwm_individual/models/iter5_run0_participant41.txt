def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited gated working memory with decay and lapses, mixed with RL.

    Idea:
    - RL: standard delta-rule Q-learning.
    - WM: stores rewarded action as a one-hot distribution for a state; its strength m[s] decays.
           Storage is capacity-limited; total WM strength across states is constrained by K.
    - Arbitration: WM contribution scales with the current memory strength for the queried state and
      a base gate parameter; also reduced by lapse. WM availability also effectively scales down when
      set size exceeds capacity (via normalization of m to sum<=K).
    - Lapse: with probability 'lapse', respond uniformly random.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_gate_base: baseline WM policy weight (0..1).
    - wm_decay: per-trial decay of memory strength m and stored WM distribution (0..1).
    - wm_capacity: capacity K (>=0) controlling the total WM strength across states.
    - lapse: lapse probability (0..1), mixed with uniform choice.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_gate_base, wm_decay, wm_capacity, lapse = model_parameters
    softmax_beta *= 10.0  # higher upper bound as specified
    softmax_beta_wm = 50.0  # very deterministic WM when stored
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # State-specific WM strength in [0,1], subject to capacity normalization
        m = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Capacity-constrained WM strength normalization
            total_m = np.sum(m)
            if total_m > wm_capacity and wm_capacity >= 0.0:
                scale = wm_capacity / total_m if total_m > 0 else 1.0
                m = m * scale

            # WM policy with near-deterministic softmax over W_s
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM policy weight depends on memory strength for this state
            wm_avail = np.clip(wm_gate_base * m[s], 0.0, 1.0)

            # Mixture with lapse to uniform
            p_mix = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updates
            # Decay memory strength and WM distributions slightly toward uniform
            m = (1.0 - wm_decay) * m
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Gate WM on rewarded outcomes for this state
            if r > 0.5:
                # Increase memory strength for this state (bounded by 1, later capacity-normalized)
                m[s] = np.clip(m[s] + (1.0 - m[s]) * 0.5, 0.0, 1.0)  # saturating increase
                # Store rewarded action as one-hot distribution
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size–sensitive inverse temperature and WM with swap interference.

    Idea:
    - RL: delta-rule Q-learning. The effective inverse temperature decreases with set size,
      capturing noisier policy under higher cognitive load.
    - WM: updated by a delta rule toward one-hot on rewards and toward uniform on non-rewards.
      WM suffers swap-like interference: with set-size–dependent probability, the effective WM
      policy is mixed with the average of other states' WM distributions.
    - Arbitration: fixed WM weight scaled by set size (weaker when nS is large).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: base RL inverse temperature; internally scaled by 10 and made set-size sensitive.
    - wm_weight_base: baseline WM weight in the policy mixture (0..1).
    - swap_rate: base swap/interference rate (>=0); grows with set size beyond 3.
    - beta_size_slope: slope controlling how much RL beta decreases with set size (>=0).
    - wm_learn: WM learning rate toward targets (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, swap_rate, beta_size_slope, wm_learn = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling
        size_term = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for 3, 1 for 6
        beta_eff = softmax_beta / (1.0 + beta_size_slope * size_term)
        wm_weight = np.clip(wm_weight_base * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
        swap_eff = np.clip(swap_rate * size_term, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with set-size–adjusted beta
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM with swap interference: mix with average of other states
            if nS > 1:
                mean_other = (np.sum(w, axis=0) - w[s, :]) / max(1, nS - 1)
            else:
                mean_other = w[s, :].copy()
            W_eff = (1.0 - swap_eff) * w[s, :] + swap_eff * mean_other
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update rule: toward one-hot when rewarded, toward uniform when not
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = w[s, :] + wm_learn * (target - w[s, :])
            # ensure normalization
            w[s, :] = np.maximum(w[s, :], 0.0)
            w[s, :] /= np.sum(w[s, :]) if np.sum(w[s, :]) > 0 else 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Entropy-based arbitration with WM hypothesis store, set-size–dependent WM decay,
    and RL stickiness bias.

    Idea:
    - RL: delta-rule Q-learning; action preferences include a stickiness term favoring the
      last action taken in the same state.
    - WM: hypothesis-like store of last rewarded action per state (one-hot). When not rewarded,
      WM decays toward uniform; decay increases with set size.
    - Arbitration: WM weight increases when WM is confident (low entropy) and RL is uncertain
      (high entropy). Weight is passed through a sigmoid controlled by a base term and an
      entropy-contrast gain.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_base: base term inside the sigmoid for WM reliance.
    - ent_weight: gain multiplying (H_rl - H_wm) inside the sigmoid; larger favors WM when
      RL is more uncertain than WM (>=0).
    - decay_size_slope: slope controlling how WM decay increases with set size (>=0).
    - kappa: RL stickiness bias added to the previous action's logit within the state (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_base, ent_weight, decay_size_slope, kappa = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def softmax_vec(x):
        x = x - np.max(x)
        ex = np.exp(x)
        return ex / np.sum(ex)

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track previous action per state for stickiness
        prev_action = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM decay
        lam = np.clip(decay_size_slope * (float(nS) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness in logits
            Qb = q[s, :].copy()
            if prev_action[s] >= 0:
                Qb[prev_action[s]] += kappa
            # Convert to probability of chosen action using softmax definition
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Qb - Qb[a])))

            # WM policy over actions
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute full distributions to estimate entropies
            pi_rl = softmax_vec(softmax_beta * Qb)
            pi_wm = softmax_vec(softmax_beta_wm * W_s)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, eps, 1.0)))

            # Entropy-based arbitration via sigmoid
            wm_weight = sigmoid(wm_base + ent_weight * (H_rl - H_wm))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: reward => store one-hot; otherwise decay toward uniform
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            # Global decay toward uniform, stronger in larger set sizes
            w = (1.0 - lam) * w + lam * w_0

            # Update stickiness state-wise
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p