def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size-dependent interference.

    Core ideas:
    - RL learns action values per state with a single learning rate and softmax choice.
    - WM stores the most recent rewarded action per state as a high-precision policy.
    - WM's decision weight and stability depend on set size (nS) relative to capacity (K):
        * Effective WM weight: wm_base * min(1, K / nS).
        * Interference/forgetting increases when nS > K by blending WM toward uniform each trial.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
      lr            : RL learning rate (0..1).
      wm_base       : Base contribution of WM to choice (0..1).
      softmax_beta  : RL inverse temperature; internally scaled by 10.
      wm_capacity   : WM capacity (in "slots", positive real).
      beta_wm       : WM inverse temperature (precision of WM policy).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_base, softmax_beta, wm_capacity, beta_wm = model_parameters
    softmax_beta *= 10.0  # as per template scaling
    softmax_beta_wm = 50.0  # not used directly; we use beta_wm below
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent parameters
        cap_factor = min(1.0, float(wm_capacity) / max(1.0, float(nS)))
        wm_weight_block = np.clip(wm_base * cap_factor, 0.0, 1.0)
        # Interference toward uniform increases when nS > capacity
        interference = max(0.0, (float(nS) - float(wm_capacity)) / max(1.0, float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL softmax probability of chosen action (stable form as in template)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over WM weights with its own precision (beta_wm)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            wm_weight = wm_weight_block  # per-template variable name
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay due to set-size interference
            w = (1.0 - interference) * w + interference * w_0

            # WM update: store rewarded action with high fidelity (fast binding)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty bonus (UCB) + WM with precision and weight both degrading with set size.

    Core ideas:
    - RL uses asymmetric learning rates and an uncertainty-driven exploration bonus (UCB).
    - WM stores the last rewarded action per state, but its precision and decision weight decrease
      with set size via a precision factor precision = 1 / (1 + wm_noise_scale * (nS - 1)).
    - The WM and RL are mixed according to a size-adjusted WM weight.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr_pos         : RL learning rate for positive prediction errors (0..1).
      lr_neg         : RL learning rate for negative prediction errors (0..1).
      softmax_beta   : RL inverse temperature; internally scaled by 10.
      wm_weight0     : Base WM mixing weight (0..1) before size adjustment.
      ucb_bonus      : UCB exploration bonus magnitude (>=0).
      wm_noise_scale : How quickly WM precision degrades with set size (>=0).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight0, ucb_bonus, wm_noise_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # baseline WM precision constant
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM state
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        n_counts = np.zeros((nS, nA))  # visitation counts per state-action for UCB

        # Size-dependent WM precision and mixing
        precision = 1.0 / (1.0 + wm_noise_scale * max(0.0, float(nS) - 1.0))
        beta_wm_eff = softmax_beta_wm * precision
        wm_weight_block = np.clip(wm_weight0 * precision, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with UCB bonus on the choice policy (does not bias value updates)
            bonus = ucb_bonus / np.sqrt(n_counts[s, :] + 1.0)
            Q_aug = q[s, :] + bonus
            denom_rl = np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy with reduced precision under higher set size
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            wm_weight = wm_weight_block  # per-template variable name
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            delta = r - q[s, a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # Increment count for chosen action (affects future UCB)
            n_counts[s, a] += 1.0

            # WM regularization toward uniform depends on low precision (more noise => more forgetting)
            forget = 1.0 - precision
            if forget > 0.0:
                w = (1.0 - forget) * w + forget * w_0

            # WM update: bind rewarded action; weakly erase on non-reward
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # On errors push WM slightly toward uniform to reflect uncertainty
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM arbitration by confidence and set size with WM learning from errors.

    Core ideas:
    - RL learns with a single learning rate and softmax policy.
    - WM policy is high precision but learns from both rewards (toward chosen action) and errors (toward uniform).
    - Arbitration weight is computed by a sigmoid over: (lower RL entropy - lower WM entropy) and capacity pressure.
      Concretely: wm_weight = sigmoid(wm_weight_bias + gate_slope * ((H_rl - H_wm) + (wm_capacity - nS)/wm_capacity)).
      Thus:
        * When WM policy is sharper than RL (H_wm < H_rl), WM is weighted more.
        * When set size exceeds capacity, the capacity term reduces WM weight.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr              : RL learning rate (0..1).
      softmax_beta    : RL inverse temperature; internally scaled by 10.
      wm_weight_bias  : Bias term for WM weight (can be negative/positive).
      wm_learning     : WM learning rate toward target (0..1).
      gate_slope      : Slope of the arbitration sigmoid (>=0).
      wm_capacity     : WM capacity (in slots, >0).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, softmax_beta, wm_weight_bias, wm_learning, gate_slope, wm_capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM is high precision
    blocks_log_p = 0.0

    def _entropy(p):
        p_safe = np.clip(p, 1e-12, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity pressure term (constant within block)
        cap_term = (float(wm_capacity) - float(nS)) / max(1.0, float(wm_capacity))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL softmax vector for entropy and chosen prob (stable)
            logits_rl = softmax_beta * Q_s
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM softmax vector and chosen prob
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration based on relative confidence (entropy difference) and capacity pressure
            H_rl = _entropy(p_rl_vec)
            H_wm = _entropy(p_wm_vec)
            z = wm_weight_bias + gate_slope * ((H_rl - H_wm) + cap_term)
            wm_weight = 1.0 / (1.0 + np.exp(-z))  # sigmoid in [0,1]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: learn from outcomes
            # Target for WM is one-hot on reward, uniform on error
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]

            w[s, :] = (1.0 - wm_learning) * w[s, :] + wm_learning * target

        blocks_log_p += log_p

    return -blocks_log_p