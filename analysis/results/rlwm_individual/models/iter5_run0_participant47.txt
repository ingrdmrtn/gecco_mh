def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL with directed exploration + capacity-limited WM mixture via interference-by-load.
    - RL: Q-learning (single learning rate) with an action-specific novelty bonus that
      diminishes with the number of times each state-action is sampled (directed exploration).
    - WM: stores rewarded actions per state as a sharp distribution; subject to leak toward
      uniform. Arbitration weight for WM declines exponentially with set size (load-induced
      interference).

    Parameters:
      lr:              RL learning rate (0..1)
      softmax_beta:    RL inverse temperature (scaled internally by 10; >=0)
      wm_base:         Baseline WM mixture weight at low load (0..1)
      interference:    Load interference rate controlling how much WM weight decays with set size
                       w_eff = wm_base * exp(-interference * (nS-3))  (>=0)
      novelty_bonus:   Directed exploration bonus added to RL values: bonus/(1+count_{s,a}) (>=0)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_base, interference, novelty_bonus = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))  # visitation counts for directed exploration

        # Effective WM weight decreases with load because of interference
        wm_weight_eff = np.clip(wm_base * np.exp(-max(0.0, interference) * max(0, nS - 3)), 0.0, 1.0)
        # Storage strength is tied to wm_base to keep parameter usage meaningful
        store_strength = np.clip(0.5 + 0.5 * wm_base, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy with directed exploration bonus
            bonus_vec = novelty_bonus / (1.0 + counts[s, :])
            Q_aug = Q_s + bonus_vec
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy from current working-memory weights
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM dynamics: leak toward uniform, then store if rewarded
            # Mild leak each encounter to allow gradual forgetting
            leak = 0.1 * (1.0 + max(0, nS - 3) / 3.0)  # slightly more leak at higher load
            leak = np.clip(leak, 0.0, 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            if r > 0:
                # Store a sharp memory of the rewarded action
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

            # Update directed exploration counts
            counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Adaptive-exploitation RL + gated WM with decay.
    - RL: Q-learning (single learning rate) with an adaptive inverse temperature that
      increases with recent correct streak within a state (meta-control of exploitation).
    - WM: Stores rewarded actions only when recent performance for that state exceeds a
      gating threshold; WM decays to uniform on each visit.

    Parameters:
      lr:             RL learning rate (0..1)
      softmax_beta:   Base RL inverse temperature (scaled internally by 10; >=0)
      beta_gain:      Multiplicative gain on beta per unit of recent correct streak (>=0)
      wm_weight:      Fixed mixture weight for WM contribution (0..1)
      gate_threshold: Integer-like threshold; WM stores only if recent_correct >= gate_threshold
      wm_decay:       Per-visit decay of WM toward uniform (0..1)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, beta_gain, wm_weight, gate_threshold, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track recent correct streak within each state
        recent_correct = np.zeros(nS, dtype=float)

        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
        gate_threshold_eff = max(0.0, gate_threshold)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Adaptive beta based on recent streak for this state
            beta_mult = 1.0 + max(0.0, beta_gain) * min(5.0, recent_correct[s])
            beta_eff = softmax_beta * beta_mult

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update recent streak
            if r > 0:
                recent_correct[s] += 1.0
            else:
                recent_correct[s] = 0.0

            # WM decay toward uniform on every visit
            decay = np.clip(wm_decay, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Gate-dependent storage: only store if performance criterion met and reward obtained
            if (r > 0) and (recent_correct[s] >= gate_threshold_eff):
                # Store a strong memory trace when gate opens
                store_strength = 0.8  # strong storage when gated
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Uncertainty-based arbitration between RL and WM with load-sensitive WM leak.
    - RL: Q-learning (single learning rate).
    - WM: probability map over actions per state that is updated toward the rewarded action
      and leaks toward uniform. Leak increases with set size (load).
    - Arbitration: trial-wise mixture weight is a sigmoid of the difference in action-selection
      uncertainty (entropy) between RL and WM for the current state.

    Parameters:
      lr:            RL learning rate (0..1)
      softmax_beta:  RL inverse temperature (scaled internally by 10; >=0)
      arb_bias:      Arbitration bias term; positive favors WM, negative favors RL
      arb_slope:     Arbitration slope; how strongly entropy difference drives WM weighting (>=0)
      wm_leak:       Baseline WM leak; effective leak grows with set size

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, arb_bias, arb_slope, wm_leak = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-sensitive leak scaling
        leak_scale = 1.0 + max(0, nS - 3) / 3.0  # 1 at nS=3, ~2 at nS=6
        base_leak = np.clip(wm_leak, 0.0, 1.0)
        leak_eff = 1.0 - (1.0 - base_leak) ** leak_scale  # increases with set size
        # WM update strength derived from wm_leak to use parameter fully
        store_strength = 1.0 - (1.0 - np.clip(wm_leak, 0.0, 1.0)) ** 2

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL choice probabilities (for entropy computation)
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            prl_vec = np.exp(logits_rl)
            prl_vec /= np.sum(prl_vec)
            p_rl = prl_vec[a]

            # WM choice probabilities
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pwm_vec = np.exp(logits_wm)
            pwm_vec /= np.sum(pwm_vec)
            p_wm = pwm_vec[a]

            # Entropy-based arbitration
            def entropy(p):
                p = np.clip(p, 1e-12, 1.0)
                return -np.sum(p * np.log(p))

            H_rl = entropy(prl_vec)
            H_wm = entropy(pwm_vec)
            # Higher WM weight when WM is more certain (lower entropy than RL)
            x = arb_bias + max(0.0, arb_slope) * (H_rl - H_wm)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-x))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # Reward-based WM update (sharpen distribution on chosen action)
            if r > 0:
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p