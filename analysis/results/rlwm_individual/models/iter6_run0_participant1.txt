def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and load-scaled WM decay.

    Idea:
    - RL updates use an asymmetry for negative prediction errors via a multiplicative factor (neg_mult).
    - WM stores rewarded associations as near one-hot and decays toward uniform when not rewarded.
    - WM effectiveness decays with load (set size) via a power-law scaling of both the mixture weight and the WM decay.
      Small set sizes support stronger WM, large set sizes weaken it.

    Parameters:
    - model_parameters[0]: lr (float in [0,1]) Base RL learning rate for positive prediction errors.
    - model_parameters[1]: wm_weight (float in [0,1]) Base mixture weight on WM policy (modulated by set size).
    - model_parameters[2]: softmax_beta (float >= 0) RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: neg_mult (float >= 0) Multiplier for learning from negative prediction errors: lr_neg = lr * neg_mult.
    - model_parameters[4]: wm_decay_base (float >= 0) Base WM decay rate that increases with load.
    - model_parameters[5]: load_power (float >= 0) Exponent controlling how strongly set size scales WM strength and decay.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, neg_mult, wm_decay_base, load_power = model_parameters
    softmax_beta *= 10.0  # RL beta rescaling per template
    softmax_beta_wm = 50.0  # near-deterministic WM softmax
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-modulated weights
        # WM weight attenuates with set size via power-law relative to baseline at nS=3
        wm_weight_block = wm_weight_base * (3.0 / max(3.0, float(nS)))**max(0.0, load_power)
        wm_weight_block = float(np.clip(wm_weight_block, 0.0, 1.0))

        # WM decay increases with load
        wm_decay = 1.0 - np.exp(-wm_decay_base * (float(nS) ** max(0.0, load_power)))
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action (softmax trick as in template)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM row for this state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight_block + (1.0 - wm_weight_block) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with asymmetric learning rate
            pe = r - q[s, a]
            lr_eff = lr if pe >= 0.0 else lr * neg_mult
            q[s, a] += lr_eff * pe

            # WM update:
            # - If rewarded: store near one-hot association
            # - If not rewarded: decay row toward uniform with load-scaled decay
            if r > 0.5:
                eps = 1e-6
                w[s, :] = eps
                w[s, a] = 1.0 - (nA - 1) * eps
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL meta-temperature and logistic WM arbitration under load.

    Idea:
    - RL softmax temperature adapts to ongoing reward rate: when reward rate is high, choices become more deterministic.
      Specifically, beta_eff = beta * (1 + beta_gain * (rew_avg - 0.5)).
    - WM mixture weight is determined by a logistic function of set size:
      wm_weight_eff = sigmoid(wm_bias - (nS - load_mid)).
      This yields high WM reliance for small set sizes and low reliance for large ones.
    - WM stores rewarded associations; on errors, WM is penalized for the chosen action and softly renormalized.

    Parameters:
    - model_parameters[0]: lr (float in [0,1]) RL learning rate.
    - model_parameters[1]: wm_weight (float in [0,1]) Base WM weight (used as a gain inside the logistic output).
    - model_parameters[2]: softmax_beta (float >= 0) Base RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: beta_gain (float) Strength of reward-rate modulation on RL beta.
    - model_parameters[4]: wm_bias (float) Bias term for logistic arbitration favoring WM at small set sizes.
    - model_parameters[5]: load_mid (float) Set size midpoint shaping the logistic transition.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, beta_gain, wm_bias, load_mid = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running reward average for meta-temperature (fixed step-size)
        rew_avg = 0.5
        alpha_avg = 0.2  # fixed smoothing for reward average

        # Logistic WM arbitration by load, then scaled by base wm_weight
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        wm_weight_by_load = sigmoid(wm_bias - (float(nS) - load_mid))
        wm_weight_block = float(np.clip(wm_weight_base * wm_weight_by_load, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with meta-temperature
            beta_eff = base_beta * (1.0 + beta_gain * (rew_avg - 0.5))
            beta_eff = float(max(0.0, beta_eff))
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm_clean = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_wm_clean  # no extra noise here; arbitration handles load

            # Mixture
            p_total = p_wm * wm_weight_block + (1.0 - wm_weight_block) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update:
            if r > 0.5:
                # Store one-hot on success
                eps = 1e-6
                w[s, :] = eps
                w[s, a] = 1.0 - (nA - 1) * eps
            else:
                # Penalize the chosen action in WM and softly renormalize towards uniform
                # Penalty magnitude increases slightly with set size
                penalty = (float(nS) - 3.0) / max(1.0, float(nS))
                penalty = float(np.clip(penalty, 0.0, 0.5))
                w[s, a] = (1.0 - penalty) * w[s, a]
                # Renormalize with a light pull to uniform
                renorm = w[s, :].sum()
                if renorm <= 0.0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= renorm
                mix = 0.05 + 0.1 * penalty
                w[s, :] = (1.0 - mix) * w[s, :] + mix * w_0[s, :]

            # Update running reward average
            rew_avg = (1.0 - alpha_avg) * rew_avg + alpha_avg * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Volatility-gated arbitration and confidence-weighted WM precision.

    Idea:
    - Arbitration weight on WM decreases with prediction-error volatility.
      Volatility v is an exponentially smoothed absolute PE: v <- (1 - alpha_v) * v + alpha_v * |PE|.
      wm_weight_eff = wm_weight * exp(-v).
    - WM precision is state-specific: a confidence c[s] increases after reward and decreases after no reward.
      The WM softmax inverse temperature is scaled by c[s], and a set size lapse adds uniform noise.
    - RL is standard delta-rule.

    Parameters:
    - model_parameters[0]: lr (float in [0,1]) RL learning rate.
    - model_parameters[1]: wm_weight (float in [0,1]) Base WM mixture weight before volatility gating.
    - model_parameters[2]: softmax_beta (float >= 0) RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: pe_volatility (float) Controls the smoothing of volatility via alpha_v = sigmoid(pe_volatility).
    - model_parameters[4]: wm_conf_gain (float in [0,1]) Step size for updating WM confidence per state.
    - model_parameters[5]: wm_lapse (float >= 0) Load-driven WM lapse coefficient that mixes WM with uniform as set size increases.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, pe_volatility, wm_conf_gain, wm_lapse = model_parameters
    softmax_beta *= 10.0
    base_beta_wm = 50.0
    blocks_log_p = 0.0

    # Map pe_volatility to [0,1] smoothing coefficient
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    alpha_v = sigmoid(pe_volatility)

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific WM confidence in [0,1]
        c = 0.5 * np.ones(nS)

        # Initialize volatility
        v = 0.0

        # Load-induced WM lapse (more lapse with higher set size)
        lapse = wm_lapse * (max(0.0, float(nS) - 3.0)) / max(1.0, float(nS))
        lapse = float(np.clip(lapse, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with confidence-weighted precision and load lapse
            W_s = w[s, :]
            beta_wm_eff = base_beta_wm * float(np.clip(c[s], 0.0, 1.0))
            if beta_wm_eff <= 0.0:
                p_wm_clean = 1.0 / nA
            else:
                p_wm_clean = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = (1.0 - lapse) * p_wm_clean + lapse * (1.0 / nA)

            # Volatility-gated arbitration
            wm_weight_eff = float(np.clip(wm_weight_base * np.exp(-v), 0.0, 1.0))

            # Mixture policy
            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update and volatility
            pe = r - q[s, a]
            q[s, a] += lr * pe
            v = (1.0 - alpha_v) * v + alpha_v * abs(float(pe))

            # WM update and confidence
            if r > 0.5:
                # Strengthen memory and increase confidence
                eps = 1e-6
                w[s, :] = eps
                w[s, a] = 1.0 - (nA - 1) * eps
                c[s] = c[s] + wm_conf_gain * (1.0 - c[s])
            else:
                # Mild decay toward uniform and reduce confidence
                decay = 0.05 + 0.2 * lapse
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                c[s] = (1.0 - 0.5 * wm_conf_gain) * c[s]

        blocks_log_p += log_p

    return -blocks_log_p