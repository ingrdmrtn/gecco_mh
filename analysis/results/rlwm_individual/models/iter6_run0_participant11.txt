def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Certainty-gated Working Memory + RL with value decay.

    Rationale:
    - WM acts as a fast system that, when confident about a state-action mapping,
      takes control; otherwise RL drives choices.
    - RL includes value decay to capture forgetting/interference within a block.
    - WM encodes rewarded mappings and decays toward uniform.
    - WM control is load-sensitive indirectly: confidence is harder to obtain with larger sets.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_theta: Confidence threshold for WM control (0..1); if max WM prob > wm_theta, WM controls
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - rl_decay: Per-trial RL value decay toward 0 (0..1)
    - wm_alpha: WM encoding step size on reward (0..1)
    - wm_decay: WM decay toward uniform each trial (0..1)

    Returns:
    - Negative log-likelihood of observed choices
    """
    lr, wm_theta, softmax_beta, rl_decay, wm_alpha, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL choice probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute WM choice probability
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Certainty-gated arbitration: WM if confident else RL
            maxp = np.max(W_s)
            wm_gate = 1.0 if maxp > wm_theta else 0.0
            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with decay
            q *= (1.0 - rl_decay)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and encoding on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # Move probability mass toward chosen action
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
                # Renormalize
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Load-sigmoid arbitration with adjustable WM precision.

    Rationale:
    - WM/RL mixture weight is modulated by set size via a sigmoid: more WM at low load (nS=3), less at high load (nS=6).
    - WM precision (inverse temperature) is a free parameter to capture retrieval noise.
    - WM decays toward uniform; rewarded outcomes write into WM.
    - RL is standard Rescorla-Wagner.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: Base WM weight (0..1), scaled by load via a sigmoid
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_temp: WM precision scaling factor; WM inverse temperature = 50*wm_temp (wm_temp >= 0)
    - load_slope: Slope of the load sigmoid; positive values increase WM reliance for small sets
    - wm_decay: WM decay toward uniform and strength of WM overwrite on reward (0..1)

    Returns:
    - Negative log-likelihood of observed choices
    """
    lr, wm_weight0, softmax_beta, wm_temp, load_slope, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = max(0.0, wm_temp) * 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Sigmoid-based load scaling centered between 3 and 6 (using 4.5 as midpoint implicitly)
        # scale = sigmoid(load_slope*(3 - nS)) -> higher when nS=3, lower when nS=6 if load_slope>0
        x = load_slope * (3.0 - float(nS))
        load_scale = 1.0 / (1.0 + np.exp(-x))
        wm_weight_block = np.clip(wm_weight0 * load_scale, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # Use the same wm_decay to control overwrite strength toward one-hot
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Risk-sensitive RL with exploration bonus + success-streak-gated WM.

    Rationale:
    - RL learns on a non-linear utility of reward (risk sensitivity), capturing different sensitivity to gains.
    - Policy uses an uncertainty/exploration bonus based on action visit counts (optimism for less tried actions).
    - WM is gated by recent success streaks for each state and down-weighted under high load.
    - WM decays to uniform and encodes when rewarded.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1), scaled by success streak and load (3/nS)
    - softmax_beta: Base RL inverse temperature (scaled internally by 10)
    - risk_gamma: Reward utility exponent (>0); u(r)=r**risk_gamma
    - bonus: Exploration bonus magnitude added to less-visited actions in policy (>=0)
    - wm_decay: WM decay toward uniform and overwrite strength on reward (0..1)

    Returns:
    - Negative log-likelihood of observed choices
    """
    lr, wm_weight, softmax_beta, risk_gamma, bonus, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    risk_gamma = max(risk_gamma, 1e-6)  # avoid zero
    bonus = max(bonus, 0.0)
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track visits and success streak per state
        visits = np.zeros((nS, nA))
        streak = np.zeros(nS, dtype=float)

        # Load scaling for WM
        load_scale = min(1.0, 3.0 / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective RL policy values with exploration bonus (policy only)
            Q_s = q[s, :]
            bonus_s = bonus / np.sqrt(1.0 + np.maximum(0.0, visits[s, :]))
            Q_eff = Q_s + bonus_s

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy prob
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight gated by success streak and load
            # streak factor in [0,1): f = streak/(streak+1)
            f_streak = streak[s] / (streak[s] + 1.0)
            wm_weight_s = np.clip(wm_weight * f_streak * load_scale, 0.0, 1.0)

            p_total = wm_weight_s * p_wm + (1.0 - wm_weight_s) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with risk-sensitive utility
            u = r ** risk_gamma
            delta = u - Q_s[a]
            q[s, a] += lr * delta

            # Update visits and streak
            visits[s, a] += 1.0
            if r > 0.5:
                streak[s] += 1.0
            else:
                streak[s] = 0.0

            # WM decay and encoding on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p