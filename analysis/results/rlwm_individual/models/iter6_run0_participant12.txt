def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-gated mixture and load-accelerated decay

    Description:
    - RL: tabular Q-learning with a softmax decision (lr, softmax_beta).
    - WM: fast, item-specific store that encodes the rewarded action for a state.
      WM has its own softmax policy with precision wm_precision.
    - Mixture: the WM vs RL mixture weight is dynamically modulated by load via a logistic gate:
        wm_mix = sigmoid(logit(wm_base) - load_slope*(nS-3)),
      so increasing set size reduces reliance on WM when load_slope > 0.
    - WM decay: the WM table decays towards uniform each trial with an effective rate that
      accelerates with load: wm_decay_eff = 1 - (1 - wm_decay)**(nS/3).
    - WM update: on reward, w[s,:] is driven to one-hot on the chosen action; on no-reward,
      the chosen action is mildly suppressed towards uniform.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_base: baseline WM mixture weight at set size 3, before load modulation (0..1)
    - softmax_beta: RL inverse temperature; internally scaled by x10 for numerical range
    - load_slope: strength of the negative effect of set size on WM mixture (>=0)
    - wm_precision: WM softmax inverse temperature (>0) for policy from WM table
    - wm_decay: base WM decay per trial (0..1); effective decay increases with nS
    """
    lr, wm_base, softmax_beta, load_slope, wm_precision, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = max(1e-6, float(wm_precision))
    eps = 1e-12

    # helper: logit and sigmoid
    def logit(x):
        x = np.clip(x, eps, 1.0 - eps)
        return np.log(x) - np.log(1.0 - x)

    def sigmoid(z):
        return 1.0 / (1.0 + np.exp(-z))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # load-dependent mixture and decay
        wm_mix = sigmoid(logit(wm_base) - load_slope * (nS - 3.0))
        wm_decay_eff = 1.0 - (1.0 - np.clip(wm_decay, 0.0, 1.0)) ** (nS / 3.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global WM decay toward uniform, accelerated by load
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

            # WM update: reward => store chosen action; no-reward => suppress chosen slightly
            if r > 0.5:
                w[s, :] *= 0.0
                w[s, a] = 1.0
            else:
                # nudge chosen toward uniform (reduces its relative weight)
                w[s, a] = 0.8 * w[s, a] + 0.2 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + confidence-weighted WM with load- and practice-sensitive decay

    Description:
    - RL: tabular Q-learning with separate learning rates for positive/negative outcomes.
    - WM: stores per-state action weights; builds a one-hot trace on reward, otherwise
      lightly relaxes the chosen action toward uniform. WM policy is sharp (beta=50).
    - Mixture: dynamic, state-specific WM weight equals the current WM "strength":
        strength_s = (max(W_s) - 1/nA) / (1 - 1/nA) in [0,1].
      This means WM contributes more when it confidently encodes a mapping.
    - WM decay: when a state is visited, its WM row decays toward uniform with an
      effective decay factor:
        eff_decay = base_decay ** ( (nS ** load_exponent) / (1 + visits[s]/sat_count) ).
      Larger set size accelerates decay (more interference), while more visits reduce
      decay (practice protects memory).

    Parameters (tuple):
    - lr_pos: RL learning rate for rewards (0..1)
    - lr_neg: RL learning rate for non-rewards (0..1)
    - softmax_beta: RL inverse temperature; internally scaled x10
    - base_decay: baseline WM decay factor per visit of a state (0..1)
    - sat_count: visit count at which decay is substantially reduced (>0)
    - load_exponent: exponent controlling how strongly load speeds decay (>=0)
    """
    lr_pos, lr_neg, softmax_beta, base_decay, sat_count, load_exponent = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Dynamic state-specific WM weight based on WM strength
            strength = (np.max(W_s) - (1.0 / nA)) / (1.0 - 1.0 / nA)
            wm_weight_s = np.clip(strength, 0.0, 1.0)

            p_total = wm_weight_s * p_wm + (1.0 - wm_weight_s) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with asymmetric learning rates
            if r > 0.5:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            # Visit count update
            visits[s] += 1.0

            # Load- and practice-modulated decay of the visited state's WM row
            denom = 1.0 + (visits[s] / max(eps, sat_count))
            exponent = (nS ** max(0.0, load_exponent)) / denom
            eff_decay = np.clip(base_decay, 0.0, 1.0) ** exponent
            w[s, :] = eff_decay * w[s, :] + (1.0 - eff_decay) * w_0[s, :]

            # WM update: reward => one-hot store; no-reward => soften chosen toward uniform
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, a] = 0.8 * w[s, a] + 0.2 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with stay bias + WM with load-dependent confusion and delta learning

    Description:
    - RL: tabular Q-learning with a perseveration (stay) bias that adds a bonus to the
      last action taken in that state when computing the RL softmax.
    - WM: learns via a delta rule toward a target that reflects reward-contingent storage
      with load-dependent "confusion" (probability of mis-binding to other actions).
      On reward, WM targets a distribution that assigns (1 - conf) to the chosen action
      and conf spread uniformly over the non-chosen actions; on no-reward, it targets
      uniform (forgetting).
    - Confusion increases with set size: conf_eff = 1 - (1 - confusion_rate)^nS.
    - Mixture: fixed mixture weight between WM and RL policies (wm_weight).

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight for WM policy (0..1)
    - softmax_beta: RL inverse temperature; internally scaled x10
    - stay_bias: Additive bias applied to the last action tried in a state in the RL policy
    - wm_lr: WM learning rate for delta update toward the target (0..1)
    - confusion_rate: Base confusion rate per item; load compounds it (0..1)
    """
    lr, wm_weight, softmax_beta, stay_bias, wm_lr, confusion_rate = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-dependent confusion
        conf_eff = 1.0 - (1.0 - np.clip(confusion_rate, 0.0, 1.0)) ** max(1, nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # RL policy with stay bias
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stay_bias
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM delta update toward reward-contingent target with load-dependent confusion
            if r > 0.5:
                target = np.ones(nA) * (conf_eff / (nA - 1.0))
                target[a] = 1.0 - conf_eff
            else:
                target = w_0[s, :].copy()  # forget toward uniform
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target

            # Update last action trace for stay bias
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p