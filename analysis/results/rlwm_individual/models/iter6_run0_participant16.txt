def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Surprise-gated WM arbitration with load penalty and RL delta rule.

    Idea:
    - RL: standard delta rule with softmax action selection.
    - WM: leaky cache of rewarded actions per state that is refreshed by reward and decays toward uniform.
    - Arbitration: the mixture weight on WM increases with trial-wise surprise (absolute prediction error),
      but is penalized by larger set sizes (load).
    
    Parameters:
    - lr: float in [0,1]. RL learning rate.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_bias: float (unbounded). Baseline bias toward WM in the mixture (logit space).
    - surprise_gain: float >= 0. How strongly the absolute RL prediction error increases WM weight.
    - load_penalty: float >= 0. Linear penalty on WM weight with set size (applied per block using nS-3).
    - wm_refresh: float in [0,1]. Strength with which a rewarded action overwrites WM cache.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_bias, surprise_gain, load_penalty, wm_refresh = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load penalty term (relative to set size 3)
        load_term = load_penalty * max(0, nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: chosen-action probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: chosen-action probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Surprise-gated mixture weight (logistic)
            delta = r - Q_s[a]
            wm_logit = wm_bias + surprise_gain * abs(delta) - load_term
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM decay toward uniform each trial (mild, load-insensitive for parsimony)
            w = 0.98 * w + 0.02 * w_0

            # WM reward-based refresh (reward strengthens action in WM map)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces and time-based WM decay modulated by set size.

    Idea:
    - RL: delta rule propagated via replacing eligibility traces e(s,a) that decay each step.
      Larger set sizes increase interference, accelerating trace decay.
    - WM: caches a precise mapping per state but decays as a function of time since last visit.
      The time-based decay accelerates with larger set sizes (more intervening items).
    - Policy: mixture of WM and RL softmax policies with a fixed mixture weight.

    Parameters:
    - lr: float in [0,1]. RL learning rate on prediction error times eligibility trace.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_weight: float in [0,1]. Mixture weight on WM policy vs RL policy.
    - lambda_trace_base: float in [0,1]. Base eligibility trace persistence per step (replacing trace).
    - time_decay_base: float >= 0. Baseline per-trial WM time-decay rate.
    - size_sensitivity: float >= 0. Scales both eligibility and WM decay with set size (nS-3).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_weight, lambda_trace_base, time_decay_base, size_sensitivity = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL, per state-action
        e = np.zeros((nS, nA))

        # Time since last visit to each state (for WM decay)
        last_visit = -1 * np.ones(nS, dtype=int)

        # Load-modulated decays
        load = max(0, nS - 3)
        lambda_trace = np.clip(lambda_trace_base / (1.0 + size_sensitivity * load), 0.0, 1.0)
        wm_time_rate = time_decay_base * (1.0 + size_sensitivity * load)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Before choice: WM decays with time since last visit
            if last_visit[s] >= 0:
                dt = t - last_visit[s]
                # Exponential decay toward uniform with rate scaled by dt and load
                decay_factor = np.exp(-wm_time_rate * max(0, dt))
                w[s, :] = decay_factor * w[s, :] + (1.0 - decay_factor) * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility traces (replacing)
            delta = r - Q_s[a]
            # Decay traces
            e *= lambda_trace
            # Replacing at chosen state-action
            e[s, :] = 0.0
            e[s, a] = 1.0
            # Update all Qs via eligibility
            q += lr * delta * e

            # WM update: reward-driven refresh for the chosen state
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong overwrite on rewarded choice
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot

            # Update last visit time
            last_visit[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Conflict-based arbitration with load-dependent WM precision.

    Idea:
    - RL: delta rule with softmax.
    - WM: cache updated by reward with a precision (gain) parameter that controls how sharp the WM map is.
      Precision decreases with larger set sizes (load), flattening WM policy.
    - Arbitration: WM weight diminishes when RL and WM disagree (policy conflict); otherwise WM is trusted more.

    Parameters:
    - lr: float in [0,1]. RL learning rate.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_weight_base: float in [0,1]. Baseline mixture weight on WM when no conflict.
    - conflict_sensitivity: float >= 0. Scales reduction of WM weight with policy conflict.
    - wm_precision_base: float >= 0. Base gain applied to WM map updates (effective WM sharpness).
    - load_precision_drop: float >= 0. How much precision decreases per +3 items (nS-3).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_weight_base, conflict_sensitivity, wm_precision_base, load_precision_drop = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision (scales how peaked W becomes after reward)
        load = max(0, nS - 3)
        wm_precision = wm_precision_base / (1.0 + load_precision_drop * load)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Mild WM decay toward uniform each trial
            w = 0.99 * w + 0.01 * w_0

            # Policies
            Q_s = q[s, :]
            # For conflict, we need full distributions
            logits_rl = softmax_beta * Q_s
            prl_vec = np.exp(logits_rl - np.max(logits_rl))
            prl_vec = prl_vec / np.sum(prl_vec)

            W_s = w[s, :]
            logits_wm = softmax_beta_wm * W_s
            pwm_vec = np.exp(logits_wm - np.max(logits_wm))
            pwm_vec = pwm_vec / np.sum(pwm_vec)

            p_rl = prl_vec[a]
            p_wm = pwm_vec[a]

            # Conflict measure: total variation distance between policies
            conflict = 0.5 * np.sum(np.abs(prl_vec - pwm_vec))

            # Conflict-based arbitration (reduce WM weight under conflict)
            wm_weight = np.clip(wm_weight_base * np.exp(-conflict_sensitivity * conflict), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with load-dependent precision (sharpen on reward)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Scale the update magnitude by wm_precision (effective WM beta)
                mix = np.tanh(wm_precision)  # bounded in [0,1), maps precision to update strength
                w[s, :] = (1.0 - mix) * w[s, :] + mix * one_hot

        blocks_log_p += log_p

    return -blocks_log_p