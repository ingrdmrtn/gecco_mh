def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + Noisy Working Memory with set-size–dependent WM precision and asymmetric WM learning
    - Choice policy: Mixture of RL softmax and WM softmax.
    - RL: Single learning rate (lr) and inverse temperature (softmax_beta scaled by 10).
    - WM policy: Softmax over a WM table W with an effective inverse temperature that decreases
      with set size (more noise/less precision at larger set sizes).
    - WM learning: Asymmetric updates:
        * On reward=1, move W toward a one-hot vector for the chosen action with wm_alpha_pos.
        * On reward=0, relax W toward uniform with wm_alpha_neg.
    - Set-size effect: WM precision is reduced as set size increases via a linear noise rule:
        beta_wm_eff = softmax_beta_wm / (1 + wm_noise_base + wm_noise_slope * max(0, nS - 3))

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_alpha_pos: WM learning rate on rewarded trials (0..1)
    - wm_alpha_neg: WM learning rate on unrewarded trials (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_noise_base: Baseline WM noise term lowering WM precision (>=0)
    - wm_noise_slope: Additional WM noise per item beyond set size 3 (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_alpha_pos, wm_alpha_neg, softmax_beta, wm_noise_base, wm_noise_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM inverse temperature (lower precision for larger sets)
        extra_items = max(0, nS - 3)
        beta_wm_eff = softmax_beta_wm / (1.0 + wm_noise_base + wm_noise_slope * extra_items)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM choice probability of chosen action with set-size–dependent precision
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Fixed mixture weight implicitly captured by WM precision change; use equal mixing
            # but precision adjusts WM's impact. To ensure both systems contribute, use 0.5 mix.
            wm_weight = 0.5
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: asymmetric learning toward one-hot on reward, toward uniform on no-reward
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha_pos) * w[s, :] + wm_alpha_pos * one_hot
            else:
                w[s, :] = (1.0 - wm_alpha_neg) * w[s, :] + wm_alpha_neg * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Entropy-based arbitration between RL and WM with set-size penalty
    - Choice policy: Mixture of RL and WM probabilities with a dynamic weight determined by
      relative uncertainty (entropy) of their policies on the current state.
      If WM is more certain than RL (lower entropy), weight shifts to WM, and vice versa.
    - RL: Single learning rate (lr) and inverse temperature (softmax_beta scaled by 10).
    - WM: High-precision softmax policy; values updated toward one-hot when rewarded and toward
      uniform when unrewarded (wm_alpha).
    - Arbitration: wm_weight = sigmoid(arb_bias + arb_slope * (H_rl - H_wm) - size_bias * max(0, nS-3))
      where H denotes Shannon entropy of each system's policy.
    - Set-size effect: Larger set size reduces WM engagement via the size_bias term.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_alpha: WM learning rate toward the target distribution (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - arb_slope: Steepness of the entropy-difference mapping to WM weight (>=0)
    - arb_bias: Baseline bias favoring WM (positive) or RL (negative)
    - size_bias: Additional negative shift per item beyond 3 that reduces WM engagement (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_alpha, softmax_beta, arb_slope, arb_bias, size_bias = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        extra_items = max(0, nS - 3)
        size_shift = size_bias * extra_items

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM probability of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Compute full policy distributions for entropy
            Q_centered = Q_s - np.max(Q_s)
            pi_rl_unnorm = np.exp(softmax_beta * Q_centered)
            pi_rl = pi_rl_unnorm / max(np.sum(pi_rl_unnorm), 1e-12)
            W_centered = W_s - np.max(W_s)
            pi_wm_unnorm = np.exp(softmax_beta_wm * W_centered)
            pi_wm = pi_wm_unnorm / max(np.sum(pi_wm_unnorm), 1e-12)

            # Entropies
            eps = 1e-12
            H_rl = -np.sum(pi_rl * np.log(np.maximum(pi_rl, eps)))
            H_wm = -np.sum(pi_wm * np.log(np.maximum(pi_wm, eps)))

            # Arbitration weight based on relative certainty and set size
            logits = arb_bias + arb_slope * (H_rl - H_wm) - size_shift
            wm_weight = 1.0 / (1.0 + np.exp(-logits))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Meta-learned arbitration per state based on likelihood advantage, slowed by set size
    - Choice policy: Mixture of RL and WM with a state-specific meta-weight m[s] that is learned
      online from which system better predicts the chosen action and outcome.
    - RL: Single learning rate (lr) and inverse temperature (softmax_beta scaled by 10).
    - WM: High-precision softmax; values updated toward one-hot on reward and toward uniform on no-reward (wm_alpha).
    - Meta-arbitration:
        * Compute advantage adv = log p_wm(chosen) - log p_rl(chosen).
        * Update a state-specific meta variable m[s] by a signed rule scaled by outcome:
              m[s] <- m[s] + meta_lr * (2*r - 1) * adv / (1 + setsize_temp * max(0, nS - 3))
          so rewarded trials increase m[s] if WM predicted better, and decrease it if RL predicted better;
          opposite for unrewarded trials.
        * Mixture weight wm_weight = sigmoid(meta_bias + m[s]).
    - Set-size effect: Meta-learning rate is reduced for larger set sizes via setsize_temp.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_alpha: WM learning rate toward the target distribution (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - meta_lr: Learning rate for the arbitration variable m (>=0)
    - meta_bias: Baseline bias favoring WM (positive) or RL (negative)
    - setsize_temp: Strength of slowing factor for meta-learning as set size increases (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_alpha, softmax_beta, meta_lr, meta_bias, setsize_temp = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        m = np.zeros(nS)  # state-specific arbitration variable

        log_p = 0.0
        extra_items = max(0, nS - 3)
        meta_scale = 1.0 / (1.0 + setsize_temp * extra_items)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL and WM chosen-action probabilities
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Current arbitration weight from meta variable
            wm_weight = 1.0 / (1.0 + np.exp(-(meta_bias + m[s])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

            # Meta-learning update: outcome-signed likelihood advantage
            adv = np.log(max(p_wm, 1e-12)) - np.log(max(p_rl, 1e-12))
            signed_outcome = 2.0 * r - 1.0
            m[s] += meta_lr * meta_scale * signed_outcome * adv

        blocks_log_p += log_p

    return -blocks_log_p