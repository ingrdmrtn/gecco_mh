def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-dependent binding interference.

    Idea:
    - RL learns state-action values with a softmax policy.
    - WM stores recent rewarded bindings as probability vectors over actions per state.
    - Larger set sizes increase binding interference (swap-like), diffusing WM contents toward the
      average of other states; and reduce WM arbitration weight.
    - WM policy is near-deterministic (high beta) but sharpened by wm_beta.

    Parameters (6 total):
    - lr: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_weight_base: baseline arbitration weight of WM vs RL at set size 3 (0..1)
    - wm_learn: WM learning rate toward the rewarded action (0..1)
    - binding_noise_base: base interference level; binding_noise scales up with set size (0..1)
    - wm_beta: sharpness multiplier for WM softmax (>=0), higher makes WM more deterministic

    Set-size impact:
    - wm_weight = wm_weight_base * (3 / nS), so larger nS down-weights WM.
    - binding_noise = binding_noise_base * (nS - 3) / 3 increases swap-like diffusion with larger nS.
    """
    lr, softmax_beta, wm_weight_base, wm_learn, binding_noise_base, wm_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base determinism for WM; further scaled by wm_beta
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight = np.clip(wm_weight_base * (3.0 / max(1, nS)), 0.0, 1.0)
        bind = np.clip(binding_noise_base * max(0, nS - 3) / 3.0, 0.0, 1.0)
        beta_wm_eff = softmax_beta_wm * max(0.0, wm_beta)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (softmax trick)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM distribution with high effective beta
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Reward-driven WM learning toward one-hot of chosen action
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # On error, reduce belief in the chosen action slightly (renormalize)
                down = wm_learn * 0.5
                w[s, a] = max(0.0, w[s, a] * (1 - down))
                # renormalize to sum to 1
                w[s, :] = w[s, :] / np.sum(w[s, :])

            # 2) Binding interference: mix with average WM of other states (swap-like)
            if nS > 1:
                mean_other = (np.sum(w, axis=0) - w[s, :]) / (nS - 1)
                w[s, :] = (1 - bind) * w[s, :] + bind * mean_other

            # 3) Ensure valid distribution
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + Win-Stay/Lose-Shift WM with set-size-dependent decay.

    Idea:
    - RL uses separate learning rates for positive and negative outcomes.
    - WM implements a short-term policy per state:
      - On reward: move toward repeating the chosen action (win-stay).
      - On no reward: shift probability away from the chosen action toward the others,
        governed by shift_bias.
    - WM decays toward uniform faster when set size is larger; WM arbitration also shrinks with set size.

    Parameters (6 total):
    - alpha_pos: RL learning rate for rewards (0..1)
    - alpha_neg: RL learning rate for non-rewards (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_weight_base: baseline WM weight at set size 3 (0..1)
    - decay_base: base WM decay toward uniform per trial; scales up with set size (0..1)
    - shift_bias: how strongly WM avoids the last losing action (0..1); 1 => fully avoid, 0 => ignore loss

    Set-size impact:
    - wm_weight = wm_weight_base * (3 / nS), reducing WM control for larger nS.
    - decay = decay_base * (nS - 3) / 3 increases forgetting/decay with set size.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, decay_base, shift_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # WM is near-deterministic when it has a strong belief
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight = np.clip(wm_weight_base * (3.0 / max(1, nS)), 0.0, 1.0)
        decay = np.clip(decay_base * max(0, nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            if r > 0:
                q[s, a] += alpha_pos * pe
            else:
                q[s, a] += alpha_neg * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform increases with set size
            w[s, :] = (1 - decay) * w[s, :] + decay * w_0[s, :]

            if r == 1:
                # Win-stay: move toward one-hot of chosen action
                learn = 1.0  # full overwrite tendency
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - learn) * w[s, :] + learn * target
            else:
                # Lose-shift: push probability away from chosen action toward the alternatives
                # Reduce chosen action by shift_bias fraction, redistribute to others uniformly
                lose_mass = shift_bias * w[s, a]
                w[s, a] -= lose_mass
                w[s, :] += lose_mass / (nA - 1)
                w[s, a] -= lose_mass / (nA - 1)  # remove the portion that was added to chosen
                # The above ensures net flow from chosen to others

            # Normalize to a valid distribution
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-gated WM arbitration with set-size-dependent WM forgetting.

    Idea:
    - RL learns standard Q-values.
    - WM maintains a probabilistic mapping per state; reward sharpens WM toward the chosen action.
    - WM forgets toward uniform more when set size is larger.
    - Arbitration weight depends both on set size and WM certainty (low entropy => higher WM control).

    Parameters (6 total):
    - lr: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_weight_base: baseline WM influence at set size 3 (0..1)
    - entropy_temp: sensitivity of arbitration to WM entropy (>=0); higher penalizes uncertain WM
    - wm_learn: WM learning rate toward the rewarded action (0..1)
    - wm_forget_base: base WM forgetting toward uniform; scales with set size (0..1)

    Set-size impact:
    - wm_weight_setsize = wm_weight_base * (3 / nS).
    - wm_forget = wm_forget_base * (nS - 3) / 3 increases forgetting for larger sets.
    - Effective WM weight = wm_weight_setsize * sigmoid(-entropy_temp * entropy(W_s)).
    """
    lr, softmax_beta, wm_weight_base, entropy_temp, wm_learn, wm_forget_base = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight_setsize = np.clip(wm_weight_base * (3.0 / max(1, nS)), 0.0, 1.0)
        wm_forget = np.clip(wm_forget_base * max(0, nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-gated arbitration
            entropy = -np.sum(W_s * np.log(np.clip(W_s, 1e-12, 1.0)))
            gate = 1.0 / (1.0 + np.exp(entropy_temp * entropy))  # higher entropy => smaller gate
            wm_weight = wm_weight_setsize * gate

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Forget toward uniform depends on set size
            w[s, :] = (1 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Reward-driven sharpening toward chosen action
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # On error, mildly reduce chosen probability
                drop = 0.5 * wm_learn
                w[s, a] = max(1e-12, w[s, a] * (1 - drop))

            # Renormalize
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p