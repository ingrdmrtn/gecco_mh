Below are three standalone cognitive models that implement distinct RL+WM mechanisms for the rlwm task. Each returns the negative log-likelihood of the observed choices and uses up to six parameters. I follow the provided template structure and fill in the WM policy and WM updating rules, while allowing set-size (3 vs 6) to modulate key mechanisms.

Note: Assume numpy as np is already imported; no imports are included inside functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with entropy-gated arbitration and load-degraded WM precision.

    Overview:
    - RL: tabular Q-learning with single learning rate and softmax (beta scaled by 10 per template).
    - WM policy: softmax over WM weights with an effective inverse temperature that decreases with set size.
    - Arbitration: the mixture weight for WM is dynamically adjusted from the base wm_weight using RL-policy entropy:
        * When RL is uncertain (high entropy), WM receives more weight if strong; when RL is confident, WM weight decreases.
        * Entropy gating effect increases with set size (higher load).
    - WM update: decay toward uniform at a load-inflated rate; reward-gated consolidation toward chosen action.

    Set-size impact:
    - WM precision (inverse temperature) decreases as set size increases.
    - WM decay increases with set size.
    - Arbitration sensitivity to RL uncertainty grows with load.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], base mixture weight of WM in the action policy (will be entropy-gated).
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10 (template).
    - wm_precision_base: float >= 0, base WM precision scaling the WM inverse temperature.
    - entropy_sensitivity: float >= 0, scales how strongly RL entropy modulates wm_weight (stronger under larger set sizes).
    - wm_decay_rate: float in [0,1], base per-visit WM decay toward uniform, amplified by load.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_precision_base, entropy_sensitivity, wm_decay_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic baseline for WM
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load factor: 0 for set size 3, 1 for set size 6
        load = max(0.0, (float(nS) - 3.0) / 3.0)

        # Effective WM inverse temperature decreases with load
        beta_wm_eff = max(1.0, softmax_beta_wm * wm_precision_base * (1.0 - 0.6 * load))

        # Effective WM decay increases with load
        wm_decay_eff = np.clip(1.0 - (1.0 - wm_decay_rate) ** (1.0 + load), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            # Compute RL softmax probabilities (for entropy gating)
            z = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * z)
            pi_rl /= np.sum(pi_rl)
            # Probability of chosen action under RL
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL entropy
            eps = 1e-12
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))

            # Entropy-gated mixture weight (logit transform for stability)
            def sigmoid(x):
                return 1.0 / (1.0 + np.exp(-x))
            def logit(p):
                p = np.clip(p, 1e-6, 1 - 1e-6)
                return np.log(p / (1.0 - p))

            gate = entropy_sensitivity * H_rl * (1.0 + load)
            wm_weight_eff = sigmoid(logit(wm_weight) + (-gate))  # more RL entropy -> reduce reliance on RL -> increase WM weight
            wm_weight = wm_weight_eff  # use effective weight in mixture

            # WM policy: softmax over WM weights with load-degraded precision
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture and likelihood accumulation
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Reward-gated consolidation toward chosen action
            if r > 0.0:
                alpha_wm = 0.6 * wm_precision_base  # stronger base consolidation when WM is precise
                w[s, :] *= (1.0 - alpha_wm)
                w[s, a] += alpha_wm
                # normalize row
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with spacing-dependent WM recall and reward-tagged storage.

    Overview:
    - RL: tabular Q-learning with single learning rate and softmax (beta scaled by 10).
    - WM policy: recall is probabilistic and depends on the time since last visit to the same state (spacing/age).
        * Retention probability decays exponentially with the state's age and is further reduced by larger set size.
        * When recall occurs, choose the WM argmax deterministically; otherwise, uniform guessing.
    - WM update: reward-tagged consolidation of the chosen action; without reward, mild redistribution away from the chosen action.
      State age is reset on visit, capturing spacing effects across visits.

    Set-size impact:
    - Recall (retention) base is reduced more rapidly with larger set size via a load-amplified decay with age.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in the action policy.
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10 (template).
    - retention_base: float in [0,1], base probability of successful WM recall at zero age and small set size.
    - spacing_sensitivity: float >= 0, controls how quickly recall decays with state age and load.
    - wm_conf_learn: float in [0,1], strength of WM consolidation toward chosen action on reward.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, retention_base, spacing_sensitivity, wm_conf_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic if recall occurs (via argmax)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track age (time since last visit) for spacing-dependent recall
        ages = np.zeros(nS, dtype=float)

        # Load factor: 0 for 3, 1 for 6
        load = max(0.0, (float(nS) - 3.0) / 3.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Increment ages and reset current state's age (we compute recall with pre-visit age)
            # For recall, use pre-visit age value
            pre_age = ages[s]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Probabilistic recall based on spacing and load
            # retention = retention_base * exp(-spacing_sensitivity * pre_age * (1+load))
            retention = retention_base * np.exp(-spacing_sensitivity * pre_age * (1.0 + load))
            retention = float(np.clip(retention, 0.0, 1.0))

            # WM recall policy: deterministic choice of WM argmax when recall succeeds; else uniform guess
            W_s = w[s, :]
            argmax_w = int(np.argmax(W_s))
            if argmax_w == a:
                p_wm_recall = 1.0
            else:
                p_wm_recall = 0.0
            p_wm_guess = 1.0 / nA
            p_wm = retention * p_wm_recall + (1.0 - retention) * p_wm_guess

            # Mixture and likelihood accumulation
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-tagged consolidation; mild redistribution on non-reward
            if r > 0.0:
                alpha = wm_conf_learn
                w[s, :] *= (1.0 - alpha)
                w[s, a] += alpha
            else:
                # Slight move away from the chosen action to other actions
                beta = 0.25 * wm_conf_learn
                give = min(beta, w[s, a])  # ensure non-negative
                w[s, a] -= give
                redistribute = give / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            # Normalize to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

            # Update ages: increment all, reset visited state's age to 0 after visit
            ages += 1.0
            ages[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with load-adaptive RL temperature, WM softmax, and visit-triggered WM refresh.

    Overview:
    - RL: tabular Q-learning. RL inverse temperature softens with higher set size (load).
    - WM policy: softmax over WM weights with an inverse temperature that decreases with load.
    - WM update: per-visit refresh toward the chosen action (even without reward), with extra top-up on reward.
      This models active rehearsal of current state-response bindings at retrieval, more challenged under higher load.

    Set-size impact:
    - RL inverse temperature decreases with load (more randomness at nS=6).
    - WM softmax precision decreases with load.
    - The refresh mechanism operates per visit but its impact interacts with load via the reduced WM precision.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in the action policy.
    - softmax_beta: float >= 0, base RL inverse temperature; internally scaled by 10 (template).
    - beta_load_slope: float >= 0, rate at which RL inverse temperature decays with load.
    - wm_beta_base: float >= 0, scales WM inverse temperature at low load.
    - wm_refresh: float in [0,1], per-visit WM refresh amount added to chosen action; reward provides an additional top-up.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, beta_load_slope, wm_beta_base, wm_refresh = model_parameters
    softmax_beta *= 10  # base RL inverse temperature
    softmax_beta_wm = 50  # baseline for WM, modulated below
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load factor: 0 for 3, 1 for 6
        load = max(0.0, (float(nS) - 3.0) / 3.0)

        # Prepare per-block adjusted RL beta (softer under load)
        # Use an exponential decay with slope parameter
        rl_beta_eff = softmax_beta * np.exp(-beta_load_slope * load)

        # WM beta decreases with load
        wm_beta_eff = max(1.0, softmax_beta_wm * wm_beta_base * (1.0 - 0.5 * load))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with load-adjusted temperature
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(rl_beta_eff * (Q_s - Q_s[a])))

            # WM policy with load-adjusted temperature
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Mixture and likelihood accumulation
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM refresh toward chosen action on every visit (even without reward)
            # 1) mild decay toward uniform to avoid runaway
            decay = 0.1 * (1.0 + 0.0 * load)  # small fixed decay; interacts indirectly with load via reduced precision
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # 2) refresh: shift mass toward chosen action
            give_total = wm_refresh
            # remove from others proportionally
            others = [aa for aa in range(nA) if aa != a]
            taken = min(give_total, np.sum(w[s, others]))
            if len(others) > 0 and np.sum(w[s, others]) > 0:
                frac = taken / np.sum(w[s, others])
                for aa in others:
                    w[s, aa] -= frac * w[s, aa]
                w[s, a] += taken
            else:
                # If numerical issues, just add and renormalize
                w[s, a] += give_total

            # Extra top-up when rewarded
            if r > 0.0:
                topup = min(0.5 * wm_refresh, 1.0 - w[s, a])
                # take proportionally from others
                other_mass = np.sum(w[s, others])
                if other_mass > 0:
                    frac2 = topup / other_mass
                    for aa in others:
                        w[s, aa] -= frac2 * w[s, aa]
                    w[s, a] += topup

            # Normalize row to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p