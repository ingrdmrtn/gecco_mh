def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with per-visit Q-decay + WM with set-size-driven binding confusion.

    Mechanism:
    - RL: standard delta-rule learning with learning rate lr; on each visit to a state,
      Q-values decay toward a uniform prior by rl_decay to capture forgetting under load.
    - WM: a cached one-hot-like map W updated toward the chosen action on rewarded trials
      with strength wm_eta. WM readout is corrupted by a binding confusion that increases
      with set size: effective WM policy mixes the WM map with uniform noise proportional
      to set size via bind = clip(bind_slope * (nS-3)/3, 0, 0.9).
    - Policy: mixture of RL and WM policies, with base wm_weight0. All choices are evaluated
      via a softmax; WM softmax is made near-deterministic (beta_wm=50).

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0-1)
    - wm_weight0: Base weight on WM policy in the mixture (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - rl_decay: Per-visit decay of Q(s,·) toward uniform prior (0-1)
    - bind_slope: Slope controlling increase in WM binding confusion with set size (>=0)
    - wm_eta: WM encoding strength toward one-hot on rewarded trials (0-1)

    Returns:
    - Negative log-likelihood of observed choices across all blocks.
    """
    lr, wm_weight0, softmax_beta, rl_decay, bind_slope, wm_eta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-dependent binding confusion
        bind = bind_slope * max(nS - 3, 0) / 3.0
        bind = min(max(bind, 0.0), 0.9)  # clamp

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (softmax centered on chosen action)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with binding confusion (mix with uniform)
            W_s_clean = w[s, :]
            W_s_eff = (1 - bind) * W_s_clean + bind * (1.0 / nA)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_eff - W_s_eff[a])))

            # Mixture policy
            p_total = wm_weight0 * p_wm + (1.0 - wm_weight0) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with per-visit decay toward uniform
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: encode rewarded associations
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # On errors, softly revert toward baseline WM prior
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL with choice perseveration + WM weight gated by ongoing reward rate and set size.

    Mechanism:
    - RL: separate learning rates for positive and negative outcomes (alpha_pos, alpha_neg).
    - Choice perseveration: soft bias toward repeating the last action within a state
      via parameter kappa (added to the softmax preference for the last action).
    - WM: one-hot-like map W updated on both rewarded and non-rewarded trials with
      different step sizes tied to RL asymmetry (alpha_pos for wins toward chosen action,
      alpha_neg for losses away from chosen action toward baseline).
    - WM gating: WM weight is scaled online by the exponentially smoothed recent reward
      rate c_t (with time constant tau_c) and inversely by set size (3/nS), capturing
      both motivation/success-based control and load.
    - Policy: mixture of RL and WM softmaxes.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for positive prediction errors (0-1)
    - alpha_neg: RL learning rate for negative prediction errors (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_weight0: Base WM weight before dynamic gating (0-1)
    - kappa: Choice perseveration strength within each state (>=0)
    - tau_c: Reward-rate smoothing factor (0-1), higher = slower updates

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight0, kappa, tau_c = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # State-specific last action for perseveration, -1 means none
        last_action = -np.ones(nS, dtype=int)

        # Running reward rate for the block
        c = 0.5  # initialize at chance-level success

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bias to Q preferences for softmax
            if last_action[s] >= 0:
                pref = Q_s.copy()
                pref[last_action[s]] += kappa
            else:
                pref = Q_s

            # RL policy under softmax centered on chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (pref - pref[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic WM gating by reward rate and set size
            gate_ns = 3.0 / max(3.0, float(nS))
            wm_weight_eff = wm_weight0 * c * gate_ns
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += alpha_pos * pe
            else:
                q[s, a] += alpha_neg * pe

            # WM update uses similar asymmetry:
            if r > 0.5:
                tgt = np.zeros(nA)
                tgt[a] = 1.0
                w[s, :] = (1 - alpha_pos) * w[s, :] + alpha_pos * tgt
            else:
                # On losses, push away from the chosen action toward baseline
                w[s, a] = (1 - alpha_neg) * w[s, a] + alpha_neg * (w_0[s, a])

            # Update reward-rate gate
            c = tau_c * c + (1 - tau_c) * r

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with entropy bonus exploration + capacity-limited WM gating and WM decay.

    Mechanism:
    - RL: delta-rule learning with learning rate lr. The action selection uses the
      standard softmax but is mixed with an uncertainty-seeking component: higher
      entropy over Q(s,·) increases effective stochasticity via an additive 'bonus'
      term in the mixture (implemented as a convex combination with a uniform policy
      scaled by the state's entropy).
    - WM: capacity-limited gating using a capacity K_capacity. If nS <= K, WM weight
      remains high; otherwise it is downscaled by K/nS. WM values decay toward uniform
      by wm_forget on each visit (to capture rapid WM leakage under load) and are
      updated toward a one-hot for the chosen action on rewarded trials.
    - Policy: WM and RL mixture with base wm_weight, then an additional uncertainty
      bonus mixing with uniform proportional to entropy(H) with strength 'bonus'.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base WM weight in the RL/WM mixture (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - K_capacity: WM capacity in number of items; gates WM by min(1, K/nS)
    - wm_forget: WM decay toward uniform per visit (0-1)
    - bonus: Weight on entropy-based uniform exploration (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_capacity, wm_forget, bonus = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-based WM gate
        cap_gate = min(1.0, float(K_capacity) / max(1.0, float(nS)))
        wm_weight_eff = wm_weight * cap_gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax term
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM with per-visit decay toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Base mixture
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Entropy over Q(s,·) to scale a uniform exploration component
            # Compute softmax over Q with same beta to estimate policy entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            entropy = -np.sum(pi * np.log(np.maximum(pi, 1e-12))) / np.log(nA)  # normalized to [0,1]

            p_uniform = 1.0 / nA
            p_total = (1.0 - bonus * entropy) * p_mix + (bonus * entropy) * p_uniform
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update on reward
            if r > 0.5:
                tgt = np.zeros(nA)
                tgt[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * tgt  # modest encoding to avoid overwriting

        blocks_log_p += log_p

    return -blocks_log_p