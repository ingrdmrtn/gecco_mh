def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with probabilistic recall and set-size–dependent WM fidelity.

    Mechanism
    ---------
    - RL: standard delta-rule with softmax (beta scaled by 10 as in template).
    - WM: a fast table w updated toward one-hot after rewards and toward uniform after no-reward.
          WM fidelity is reduced by action-confusion noise; WM contributes only when "recalled".
    - Arbitration: fixed wm_weight mixes RL and WM policies.
    - Lapse: final mixture is blended with a uniform lapse.

    Set-size effects
    ----------------
    - Recall probability: p_recall = min(1, phi_recall * (3/nS)). Smaller set (nS=3) → stronger recall.
    - Confusion noise increases with set size: the effective WM policy is flattened toward uniform
      with strength xi_conf * (nS/3).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, phi_recall, xi_conf, tau_lapse)
        - lr: RL learning rate (0..1).
        - wm_weight: Mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - phi_recall: Scales WM recall probability; p_recall = min(1, phi_recall * 3/nS).
        - xi_conf: WM action-confusion toward uniform, scaled by set size (>=0).
        - tau_lapse: Final lapse probability mixing with uniform (0..1).
    """
    lr, wm_weight, softmax_beta, phi_recall, xi_conf, tau_lapse = model_parameters

    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0  # per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent recall and confusion scaling
        p_recall = min(1.0, max(0.0, float(phi_recall) * (3.0 / float(nS))))
        conf_scale = max(0.0, float(xi_conf) * (float(nS) / 3.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template-prescribed form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM softmax
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confusion toward uniform (set-size scaled)
            p_wm_confused = (1.0 - conf_scale) * p_wm_det + conf_scale * (1.0 / nA)
            p_wm = p_recall * p_wm_confused + (1.0 - p_recall) * (1.0 / nA)

            # Mixture and lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - tau_lapse) * p_total + tau_lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Move toward one-hot if rewarded; toward uniform if not
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Normalize to guard against drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-gated arbitration and WM learning rate; arbitration is set-size sensitive.

    Mechanism
    ---------
    - RL: standard delta-rule and softmax.
    - WM: table w updated with its own learning rate eta_w toward one-hot when rewarded and toward uniform otherwise.
    - Arbitration: WM weight increases when WM is certain (low entropy in W_s). This gating is
      modulated by set size: the effect of entropy on arbitration is scaled by a size_bias.

    Set-size effects
    ----------------
    - The entropy gate is scaled by size_factor = (6/nS)^size_bias, so smaller sets (nS=3) amplify
      the influence of WM certainty on the mixture weight more than larger sets.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight_base, softmax_beta, eta_w, kappa_w, size_bias)
        - lr: RL learning rate (0..1).
        - wm_weight_base: Baseline WM mixture floor (0..1); arbitration never goes below this.
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - eta_w: WM learning rate (0..1) for updating w.
        - kappa_w: Sensitivity of arbitration to WM certainty (>=0).
        - size_bias: Set-size scaling exponent for arbitration (can be negative or positive).
    """
    lr, wm_weight_base, softmax_beta, eta_w, kappa_w, size_bias = model_parameters

    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0  # per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size factor for arbitration gain
        size_factor = (6.0 / float(nS)) ** float(size_bias)
        H_max = np.log(nA)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template line)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy of WM row
            W_safe = np.clip(W_s, 1e-12, 1.0)
            W_safe = W_safe / np.sum(W_safe)
            H = -np.sum(W_safe * np.log(W_safe))

            # Entropy-gated arbitration: higher weight when entropy is low
            certainty = H_max - H  # 0..H_max
            gate = 1.0 / (1.0 + np.exp(-kappa_w * size_factor * certainty))  # sigmoid
            wm_weight_eff = wm_weight_base + (1.0 - wm_weight_base) * gate

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - eta_w) * w[s, :] + eta_w * target
            else:
                w[s, :] = (1.0 - eta_w) * w[s, :] + eta_w * w_0[s, :]

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM win–stay/lose–shift with set-size–scaled WM strength and state-conditioned stickiness.

    Mechanism
    ---------
    - RL: standard delta-rule and softmax.
    - WM: emphasizes a win–stay update; rewards push w[s] toward a’s one-hot, non-rewards suppress a.
          Additionally, a state-conditioned stickiness bonus favors repeating the last chosen action in that state.
    - Arbitration: fixed wm_weight.

    Set-size effects
    ----------------
    - WM update strength is scaled by (3/nS)^psi_size, making WM stronger in smaller sets.
    - Stickiness contributes via an additive bonus to W_s on the last action in that state;
      this mechanism interacts with set-size via the scaled WM strength.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, mu_wm, psi_size, sigma_stick)
        - lr: RL learning rate (0..1).
        - wm_weight: Mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - mu_wm: Base WM update strength (0..1), scaled by set size per trial.
        - psi_size: Exponent controlling set-size scaling of WM strength (>=0).
        - sigma_stick: Strength of stickiness bonus added to the last action in the state (>=0).
    """
    lr, wm_weight, softmax_beta, mu_wm, psi_size, sigma_stick = model_parameters

    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0  # per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -np.ones(nS, dtype=int)

        # Set-size scaling for WM update
        size_scale = (3.0 / float(nS)) ** max(0.0, float(psi_size))
        mu_eff = np.clip(mu_wm * size_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s_base = w[s, :].copy()

            # Add stickiness bonus to WM values for current state
            if last_action[s] >= 0 and 0 <= last_action[s] < nA:
                stick_vec = np.zeros(nA)
                stick_vec[last_action[s]] = sigma_stick
                W_s = W_s_base + stick_vec
            else:
                W_s = W_s_base

            # RL policy (template line)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Win–stay: move toward one-hot(a) on reward
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - mu_eff) * w[s, :] + mu_eff * target
            else:
                # Lose–shift: suppress chosen action and renormalize
                w[s, a] = (1.0 - mu_eff) * w[s, a]
                # Redistribute the removed mass proportionally to others plus a small uniform to avoid zeros
                remainder = 1.0 - np.sum(w[s, :])
                if remainder > 0:
                    redistribution = remainder / (nA - 1)
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] += redistribution

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p