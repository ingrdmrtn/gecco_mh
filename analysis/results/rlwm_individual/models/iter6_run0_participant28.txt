def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reward-gated WM encoding and set-sizeâ€“dependent WM decay.

    Idea:
    - RL: delta-rule learning of action values.
    - WM: stores one-shot associations upon reward; encoding strength is gated by reward magnitude.
           WM memories decay faster as set size increases (higher load -> faster decay).
    - Action selection: mixture of RL and WM policies.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - gate_strength: Controls WM encoding strength when reward is received; g = 1 - exp(-gate_strength * r).
    - wm_forgetting: Controls load-dependent WM decay; per-trial decay f = 1 - exp(-wm_forgetting*(nS-1)).

    Set-size impacts:
    - WM decay increases with set size via wm_forgetting, making WM less reliable in set size 6 than 3.
    """
    lr, wm_weight, softmax_beta, gate_strength, wm_forgetting = model_parameters
    softmax_beta *= 10  # higher range for RL temperature
    softmax_beta_wm = 50  # sharp WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay rate
        f_decay = 1.0 - np.exp(-wm_forgetting * max(0.0, float(nS) - 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM encoding gate: only strengthen on reward, with gated strength
            g = 1.0 - np.exp(-gate_strength * r)  # g=0 when r=0; increases with r
            if g > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - g) * w[s, :] + g * one_hot

            # WM decay toward uniform (set-size dependent) for the current state
            w[s, :] = (1.0 - f_decay) * w[s, :] + f_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with WM leak and load-driven action swap noise.

    Idea:
    - RL: delta-rule value learning.
    - WM: on reward, encode the chosen action strongly; on no reward, weakly suppress the chosen action.
           WM traces also leak toward uniform over time with a time constant.
    - Load-dependent action swap: as set size increases, WM retrieval is noisier, modeled as a probability
      of swapping to a random action (dilution of WM policy toward uniform).
    - Action selection: mixture of RL and noisy WM policies.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - wm_decay_tau: WM leak time constant; larger -> slower leak. Effective per-trial leak increases with nS.
    - action_swap_base: Controls how WM action swap grows with set size;
                        swap_prob = 1 - exp(-action_swap_base * max(0, nS-3)).

    Set-size impacts:
    - WM leak is faster when nS is larger (via 1 - exp(-(nS)/wm_decay_tau)).
    - WM action swap increases with set size, making WM choices noisier in larger sets.
    """
    lr, wm_weight, softmax_beta, wm_decay_tau, action_swap_base = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-driven parameters
        wm_leak = 1.0 - np.exp(-float(nS) / max(1e-6, wm_decay_tau))
        swap_prob = 1.0 - np.exp(-action_swap_base * max(0.0, float(nS) - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Deterministic WM policy from current WM content
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Action swap noise (toward uniform) increases with set size
            p_wm = (1.0 - swap_prob) * p_wm_det + swap_prob * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            if r > 0.5:
                # Strong encode chosen action on reward
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Weak suppression of the chosen (move mass from chosen to others evenly), scaled by leak
                kappa = wm_leak  # reuse leak as a negative-update gain
                w_s = w[s, :].copy()
                take = kappa * w_s[a]
                w_s[a] -= take
                w_s += (take / (nA - 1)) * (1 - np.eye(nA)[a])
                w[s, :] = w_s

            # WM leak toward uniform for current state
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM availability with load-driven interference.

    Idea:
    - RL: standard delta-rule learning (constant learning rate).
    - WM: stores rewarded state-action mappings, but access availability is limited by a capacity
      parameter (mem_slots). When the set size exceeds capacity, interference reduces availability.
      WM availability per state c[s] in [0,1] gates how much the WM policy contributes versus uniform.
    - Action selection: RL policy mixed with WM policy diluted by availability.

    Parameters (5):
    - lr_base: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM pathway in choice (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - mem_slots: WM capacity in number of states (>=1). Non-integer values are allowed and interpreted
                 as a soft capacity.
    - interference_rate: Strength of load-driven interference when nS > mem_slots.
                         Availability decay per trial: d = 1 - exp(-interference_rate * overload),
                         overload = max(0, nS - mem_slots) / max(1, nS).

    Set-size impacts:
    - When nS > mem_slots, availability decays more each trial, reducing WM influence for all states.
    - When nS <= mem_slots, availability is preserved more strongly.
    """
    lr_base, wm_weight, softmax_beta, mem_slots, interference_rate = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM availability per state (0..1); starts at 0.5 baseline to allow some early influence
        c = 0.5 * np.ones(nS)

        overload = max(0.0, float(nS) - float(mem_slots)) / max(1.0, float(nS))
        decay_avail = 1.0 - np.exp(-interference_rate * overload)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with availability gating toward uniform
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = c[s] * p_wm_det + (1.0 - c[s]) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_base * delta

            # WM update: on reward, write association and increase availability; else slight decay
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                # Increase availability toward 1
                c[s] = 1.0 - (1.0 - c[s]) * 0.2  # quick boost; deterministic and bounded
            else:
                # Mild drift toward uniform when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                # Slightly reduce availability
                c[s] *= 0.9

            # Global interference when overloaded: reduce availability for all states
            if decay_avail > 0.0:
                c = (1.0 - decay_avail) * c

            # Keep c within [0,1]
            c = np.clip(c, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p