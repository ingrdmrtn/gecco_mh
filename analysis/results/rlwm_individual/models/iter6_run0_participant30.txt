def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + surprise-gated WM with set-size-dependent decay and WM noise.

    Description:
    - RL: single learning rate and softmax policy.
    - WM: a probabilistic table w[state, action] approximating a one-hot mapping for each state.
      WM updates are gated by surprise (unsigned prediction error) and by reward: large surprise
      and positive feedback triggers encoding toward the chosen action. Otherwise, WM only decays.
      WM decays faster in larger set sizes (greater interference).
    - Mixture: action probability is a convex combination of RL and WM policies, with the WM
      mixture weight reduced as set size increases.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate (0..1).
    - model_parameters[1] = wm_weight0 (float): Base WM mixture weight at set size 3 (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally x10.
    - model_parameters[3] = wm_conf_threshold (float): Surprise gate; WM encodes if |PE| > threshold (>=0).
    - model_parameters[4] = wm_decay_ss (float): Base WM decay scaling by set size; larger => faster decay (>=0).
    - model_parameters[5] = wm_noise (float): Noise in WM encoding toward one-hot; 0=perfect, >0 adds smoothing (0..1).

    Set-size effects
    - WM decay increases linearly with set size: decay = clip(wm_decay_ss * (nS/3), 0, 1).
    - WM mixture weight is reduced with set size: wm_weight_eff = wm_weight0 * (3 / nS).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, wm_conf_threshold, wm_decay_ss, wm_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size dependent decay and mixture
        wm_decay = np.clip(wm_decay_ss * (float(nS) / 3.0), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight0 * (3.0 / float(nS)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action (deterministic softmax on W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Surprise-gated WM encoding: encode only if large unsigned PE and reward positive
            if (abs(delta) > wm_conf_threshold) and (r > 0):
                # Encode toward a noisy one-hot on chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Smooth one-hot with wm_noise
                smooth = (1.0 - wm_noise) * one_hot + wm_noise * (1.0 / nA)
                # Move W toward smooth mapping strongly (overwrite-like)
                w[s, :] = 0.5 * w[s, :] + 0.5 * smooth  # conservative merge to avoid numerical lock-in

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + WM with capacity-limited strength and lapse.

    Description:
    - RL: separate positive and negative learning rates; softmax policy.
    - WM: table of action probabilities per state updated quickly toward the chosen action
      when rewarded; otherwise only decays. WM strength (mixture weight) decreases with set size.
    - Lapse: WM retrieval can fail with a set-size-scaled lapse, mixing in uniform choice.

    Parameters
    - model_parameters[0] = lr_pos2 (float): RL learning rate for positive PEs (0..1).
    - model_parameters[1] = lr_neg2 (float): RL learning rate for negative PEs (0..1).
    - model_parameters[2] = wm_strength_base (float): Base WM mixture weight at set size 3 (0..1).
    - model_parameters[3] = softmax_beta (float): RL inverse temperature; internally x10.
    - model_parameters[4] = lapse_base (float): Base lapse probability at set size 3 (0..1).
    - model_parameters[5] = ss_interf (float): Set-size interference scaling both WM decay and lapse (>=0).

    Set-size effects
    - Effective WM weight: wm_weight_eff = wm_strength_base / (1 + ss_interf * (nS - 3)).
    - WM decay: decay_ss = clip(ss_interf * (nS / 6), 0, 1).
    - Lapse: lapse_eff = clip(lapse_base * (nS / 3), 0, 1).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos2, lr_neg2, wm_strength_base, softmax_beta, lapse_base, ss_interf = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_strength_base / (1.0 + ss_interf * max(0.0, float(nS) - 3.0))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)
        decay_ss = np.clip(ss_interf * (float(nS) / 6.0), 0.0, 1.0)
        lapse_eff = np.clip(lapse_base * (float(nS) / 3.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax prob
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Apply lapse to WM policy by mixing with uniform
            p_wm = (1.0 - lapse_eff) * p_wm_det + lapse_eff * (1.0 / nA)

            # Mixture of WM and RL
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetry
            pe = r - Q_s[a]
            eta = lr_pos2 if pe >= 0 else lr_neg2
            q[s][a] += eta * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - decay_ss) * w[s, :] + decay_ss * w_0[s, :]

            # WM encoding mostly when rewarded: move probability mass toward chosen action
            if r > 0:
                alpha_wm = 0.7  # strong attraction toward the chosen action when rewarded
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + WM with rehearsal-driven encoding and set-size arbitration.

    Description:
    - RL: single learning rate with within-state eligibility traces over actions. The chosen action
      gets trace=1; other actions in the same state decay by lambda. Q updates use delta * trace.
    - WM: table of action probabilities per state. A latent rehearsal activation m[state] increases
      when the state is encountered and decays over trials, yielding stronger WM encoding on repeated
      or closely spaced presentations. WM mixture weight is reduced with set size via a smooth scaling.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate (0..1).
    - model_parameters[1] = wm_mix0 (float): Base WM mixture weight at set size 3 (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally x10.
    - model_parameters[3] = trace_lambda (float): Eligibility trace decay for non-chosen actions (0..1).
    - model_parameters[4] = rehearsal_eff (float): Controls how strongly WM encodes per unit activation (>=0).
    - model_parameters[5] = ss_sensitivity (float): Set-size sensitivity reducing WM weight and increasing activation decay (>=0).

    Set-size effects
    - WM mixture: wm_weight_eff = wm_mix0 / (1 + ss_sensitivity * (nS - 3)).
    - Activation decay per trial: act_decay = clip(ss_sensitivity * (nS / 6), 0, 1).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_mix0, softmax_beta, trace_lambda, rehearsal_eff, ss_sensitivity = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility traces per state/action
        e = np.zeros((nS, nA))

        # Rehearsal activation per state
        m = np.zeros(nS)

        wm_weight_eff = np.clip(wm_mix0 / (1.0 + ss_sensitivity * max(0.0, float(nS) - 3.0)), 0.0, 1.0)
        act_decay = np.clip(ss_sensitivity * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces (within current state)
            # Update traces: decay all actions in state, set chosen to 1
            e[s, :] *= trace_lambda
            e[s, a] = 1.0

            delta = r - Q_s[a]
            # Update all actions in the current state proportionally to their trace
            q[s, :] += lr * delta * e[s, :]

            # Rehearsal activation dynamics: global decay, then activate current state
            m *= (1.0 - act_decay)
            m[s] += 1.0

            # WM encoding strength grows with activation of the current state
            alpha_wm = 1.0 - np.exp(-rehearsal_eff * m[s])
            alpha_wm = np.clip(alpha_wm, 0.0, 1.0)

            # Move WM toward chosen action by alpha_wm; otherwise slight decay to uniform to prevent lock-in
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - 0.1) * w[s, :] + 0.1 * w_0[s, :]  # mild ongoing decay/interference
            w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p