Below are three standalone cognitive models that follow your template and return the negative log-likelihood of the observed choices. Each model keeps the specified components intact and fills in the missing WM policy and updating rules. I also indicate how parameters are impacted by set size (3 vs 6).

Note: Assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with set-size–specific learning rates + WM with set-size–dependent arbitration (logistic).

    Mechanism
    - RL: tabular Q-learning with learning rate depending on set size (small vs large).
    - WM store: deterministic cache of the last rewarded action per state (no decay).
    - Arbitration: mixture weight is a logistic function of set size via intercept and slope.

    Parameters
    - lr_small: RL learning rate used when set size == 3 (0..1). Impacted by set size.
    - lr_large: RL learning rate used when set size == 6 (0..1). Impacted by set size.
    - softmax_beta: base inverse temperature for RL; internally scaled by 10 (>0).
    - wm_logit_intercept: intercept term for logistic WM weight (can be negative/positive).
    - wm_logit_slope: slope on set size (the function uses nS directly: 3 or 6).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr_small, lr_large, softmax_beta, wm_logit_intercept, wm_logit_slope = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent parameters
        lr_eff = lr_small if nS <= 3 else lr_large
        wm_weight_eff = 1.0 / (1.0 + np.exp(-(wm_logit_intercept + wm_logit_slope * nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (kept as in template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM cache (deterministic given softmax_beta_wm)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta

            # WM update: write one-hot on rewarded outcomes; otherwise keep as is
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with set-size–dependent exploration (inverse temperature) + WM with noisy retrieval scaling with set size.

    Mechanism
    - RL: single learning rate; inverse temperature depends on set size (beta_small vs beta_large).
    - WM store: incremental encoding toward chosen action on rewards (strength wm_eta).
    - WM interference: retrieval noise increases with set size, implemented by mixing WM with uniform before policy.

    Parameters
    - lr: RL learning rate (0..1).
    - beta_small: multiplicative factor on the (scaled) softmax_beta when set size == 3 (>0). Impacted by set size.
    - beta_large: multiplicative factor on the (scaled) softmax_beta when set size == 6 (>0). Impacted by set size.
    - wm_weight: mixture weight between WM and RL policies (0..1).
    - wm_eta: WM encoding strength toward the chosen action upon reward (0..1).
    - ss_noise: strength of WM retrieval noise per added load unit (>=0); effective noise grows with set size. Impacted by set size.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, beta_small, beta_large, wm_weight, wm_eta, ss_noise = parameters
    softmax_beta = 10.0  # base will be scaled by beta_small/large per block
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects
        beta_scale = beta_small if nS <= 3 else beta_large
        softmax_beta_eff = softmax_beta * beta_scale
        # WM retrieval noise grows with set size (0 at 3, up to ss_noise at 6)
        load_units = max(nS - 3, 0) / 3.0  # 0 for 3, 1 for 6
        wm_noise = np.clip(ss_noise * load_units, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template form but with block-specific beta)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta_eff * (Q_s - Q_s[a])))

            # WM policy with retrieval noise: mix with uniform before softmax
            W_s_noisy = (1.0 - wm_noise) * W_s + wm_noise * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: encode toward chosen action only when rewarded
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * W_s + wm_eta * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + PE-gated WM arbitration with set-size shift and reward-contingent WM clearing.

    Mechanism
    - RL: standard Q-learning (single lr).
    - WM store: if rewarded, write one-hot; if not rewarded, clear memory for that state to uniform.
    - Arbitration: WM weight depends on last prediction error magnitude for that state (PE-gating)
      with a set-size-specific shift. Larger surprise decreases WM reliance.

      wm_weight(s) = sigmoid(wm_base + ss_shift*(nS>3) - pe_sens*|last_PE_state|)

    Parameters
    - lr: RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>0).
    - wm_base: baseline WM weight logit intercept (free real).
    - ss_shift: additive shift on WM weight logit when set size > 3. Impacted by set size.
    - pe_sens: sensitivity of arbitration to absolute last prediction error for that state (>=0).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_base, ss_shift, pe_sens = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_pe = np.zeros(nS)  # track last PE per state for arbitration

        ss_term = ss_shift if nS > 3 else 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # PE-gated arbitration using last PE for this state
            wm_logit = wm_base + ss_term - pe_sens * np.abs(last_pe[s])
            wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            last_pe[s] = delta

            # WM update: reward-contingent write, otherwise clear
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes on set size effects:
- Model 1: Both the RL learning rate and the WM arbitration weight depend on set size (lr_small vs lr_large; logistic WM weight with slope on nS).
- Model 2: Exploration temperature and WM retrieval noise are modulated by set size (beta_small vs beta_large and ss_noise scaled by load).
- Model 3: Arbitration is PE-gated with an additive shift when set size is large (ss_shift), reducing WM reliance under higher load.