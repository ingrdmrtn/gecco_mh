def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Win-stay WM with reward-tagged strengthening and set-size interference.

    Idea:
    - RL: standard Rescorla-Wagner with softmax choice.
    - WM: a per-state cache of the last rewarded action; when a reward occurs, WM stores a
      sharpened one-hot trace of the chosen action. In the absence of reward, the WM trace
      decays toward uniform. Larger set size increases interference, accelerating decay.
    - Policy: mixture of RL softmax and WM softmax.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Mixture weight (0-1) for WM policy in the action selection
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wm_decay: Base WM decay toward uniform each trial (0-1)
    - wm_bonus: Strengthening amount for the WM trace on rewarded trials (>=0)
    - ss_interf_rate: Additional WM decay per unit increase in set size beyond 3 (>=0)
      effective_decay = wm_decay + ss_interf_rate * max(0, (nS - 3) / 3)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_bonus, ss_interf_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size dependent interference
        interf = ss_interf_rate * max(0.0, (float(nS) - 3.0) / 3.0)
        eff_decay = np.clip(wm_decay + interf, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: sharpen the stored trace around the currently strong action preference
            # Move W_s away from uniform by wm_bonus effect already encoded into W_s via updates.
            # Softmax over W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) decay toward uniform with set-size interference
            w[s, :] = (1.0 - eff_decay) * w[s, :] + eff_decay * w_0[s, :]

            # 2) if rewarded, strengthen the chosen action trace
            if r > 0.0:
                # add a directed boost to the chosen action, then renormalize
                w[s, a] += wm_bonus
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] = w[s, :] / w_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM retrieval with set-size scaling.

    Idea:
    - RL: standard Rescorla-Wagner with softmax.
    - WM: stores the last rewarded action per state as a one-hot trace.
    - Retrieval probability is capacity-limited and declines with set size:
      recall_prob = sigmoid(kappa_wm * (capacity_C / nS - 1))
      Larger nS -> smaller capacity_C / nS -> lower recall.
    - WM policy: if retrieval succeeds, WM offers a sharp one-hot preference;
      otherwise it is close to uniform controlled by wm_noise.
    - Policy: mixture of RL softmax and WM softmax.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Mixture weight (0-1) for WM in the policy
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - capacity_C: Effective WM capacity (in "items", >0)
    - kappa_wm: Sensitivity of recall to capacity ratio (>=0). Higher -> steeper drop with set size.
    - wm_noise: When recall fails, amount of bias away from uniform (0 = perfectly uniform; higher -> mild structure)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_C, kappa_wm, wm_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # will hold last rewarded action traces per state
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size dependent recall probability
        cap_ratio = (capacity_C / max(1.0, float(nS)))
        recall_prob = 1.0 / (1.0 + np.exp(-kappa_wm * (cap_ratio - 1.0)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy construction:
            # If recall succeeds (with probability recall_prob), use sharp one-hot trace from W_s;
            # otherwise use a near-uniform vector with small structure governed by wm_noise.
            # In likelihood, we can't marginalize over a latent Bernoulli easily without sampling,
            # but we can construct an "effective" preference vector:
            # Effective W_tilde = recall_prob * W_s_onehot + (1 - recall_prob) * U_noise
            # where W_s_onehot is the current w[s], and U_noise is uniform with slight bias via wm_noise.
            U = w_0[s, :].copy()
            # Introduce controllable noise structure: add wm_noise to the currently stored best action index
            best_idx = int(np.argmax(W_s))
            U[best_idx] += wm_noise
            U = U / np.sum(U)

            W_tilde = recall_prob * W_s + (1.0 - recall_prob) * U

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_tilde - W_tilde[a])))

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # If rewarded, set a sharp one-hot trace for the chosen action
            if r > 0.0:
                w[s, :] = eps
                w[s, a] = 1.0
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # If not rewarded, leave as is but add mild diffusion toward uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Choice-kernel WM (recency-based perseveration) with set-size weighted arbitration.

    Idea:
    - RL: standard Rescorla-Wagner with softmax.
    - WM: tracks recent actions per state regardless of reward via an exponential recency kernel;
      this implements state-specific perseveration (win/lose stay tendency driven by recency).
    - The influence of WM on policy declines hyperbolically with set size:
      wm_weight_eff = wm_weight0 / (1 + ss_alpha * max(0, nS - 3))
    - WM policy: softmax over a recency-trace sharpened by pers_gain.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight0: Base WM weight at set size 3 (0-1)
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - tau_wm: Recency rate for WM choice kernel (0-1). Higher -> more weight on last action.
    - pers_gain: How strongly WM recency deviations from uniform are emphasized (>=0)
    - ss_alpha: Set-size penalty on WM influence (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight0, softmax_beta, tau_wm, pers_gain, ss_alpha = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # recency-based choice kernel per state
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent effective WM weight
        ss_penalty = 1.0 + ss_alpha * max(0.0, float(nS) - 3.0)
        wm_weight_eff = wm_weight0 / ss_penalty
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: emphasize deviations from uniform with pers_gain
            mu = 1.0 / nA
            W_tilde = mu + pers_gain * (W_s - mu)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_tilde - W_tilde[a])))

            # Mixture policy with set-size adjusted weight
            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: exponential recency update toward the last chosen action (reward-agnostic)
            # Move W_s toward one-hot(a) with rate tau_wm
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - tau_wm) * w[s, :] + tau_wm * one_hot
            # Ensure normalization
            w_sum = np.sum(w[s, :])
            if w_sum > 0:
                w[s, :] = w[s, :] / w_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p