def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM uncertainty-gated mixture with set-size-scaled WM mixture and error-based suppression.
    
    Mechanism:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled by 10).
    - WM channel: softmax over WM weights W with high inverse temperature (50).
    - Mixture weight is uncertainty-gated: higher WM influence when RL is uncertain (high entropy).
    - WM dynamics:
        - Decay toward uniform each trial (wm_forget).
        - Positive reward: overwrite with one-hot on the chosen action (strong encoding).
        - Negative reward: suppress the chosen action in WM (reduce its weight by wm_suppress), then renormalize.
    
    Set-size effect:
    - WM mixture is scaled down as set size increases: wm_mix_eff = base_wm_mix / (1 + size_alpha * (sqrt(nS) - sqrt(3))).
      This imposes a sublinear decline with set size (3 vs 6).
    
    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - base_wm_mix: Baseline WM vs RL mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - gate_slope: Strength of uncertainty gating by RL entropy (>=0). Larger => more WM under uncertainty.
    - wm_forget: WM decay rate toward uniform per visit (0..1).
    - size_alpha: Set-size penalty on WM mixture (>=0), scales with sqrt(nS).
    """
    lr, base_wm_mix, softmax_beta, gate_slope, wm_forget, size_alpha = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled baseline WM weight
        wm_mix_base_eff = base_wm_mix / (1.0 + size_alpha * (np.sqrt(nS) - np.sqrt(3.0)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action via denominator trick
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Uncertainty gating based on RL entropy
            # Compute full RL action probs for state s to measure entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0))) / np.log(nA)  # normalized to [0,1]
            wm_gate = 1.0 / (1.0 + np.exp(-gate_slope * (entropy - 0.5)))  # sigmoid centered at 0.5 entropy

            wm_weight_eff = np.clip(wm_mix_base_eff * wm_gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for visited state
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Reward-contingent WM update: overwrite on reward, suppress on no reward
            if r > 0.5:
                # Overwrite to one-hot
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Suppress chosen action, renormalize to keep a distribution
                wm_suppress = 0.5  # implicit suppression magnitude by rule; uses gating via wm_forget already
                w[s, a] = max(w[s, a] * (1.0 - wm_suppress), 0.0)
                # Renormalize to sum to 1
                total = np.sum(w[s, :])
                if total < eps:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= total

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + probabilistic WM binding and set-size-reduced WM precision.
    
    Mechanism:
    - RL channel: softmax over Q-values; asymmetric learning rates for positive and negative outcomes.
    - WM channel: maintains associative weights W; on each trial, binds the chosen action to the state with a
      probability that depends on reward (stronger when rewarded) and a baseline gate.
      WM retrieval is via softmax with an effective inverse temperature that decreases with set size.
    - Mixture: fixed mixture weight between WM and RL on each trial.
    - WM leakage toward uniform each visit and limited reliability (lapse into uniform policy).
    
    Set-size effect:
    - WM precision is lower for larger set sizes: beta_wm_eff = softmax_beta_wm / (1 + size_prec_gain*(nS-3)).
      This makes WM policy more diffuse at set size 6 than 3.
    
    Parameters (tuple):
    - tau_pos: RL learning rate after reward = 1 (0..1).
    - tau_neg: RL learning rate after reward = 0 (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_gate_base: Baseline WM binding probability on any trial (0..1).
    - wm_lapse: Lapse rate in WM policy mixing uniform action probabilities (0..1).
    - size_prec_gain: Magnitude of set-size penalty on WM precision (>=0).
    """
    tau_pos, tau_neg, softmax_beta, wm_gate_base, wm_lapse, size_prec_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM precision
        beta_wm_eff = softmax_beta_wm / (1.0 + size_prec_gain * max(0, nS - 3))

        # Fixed mixture between WM and RL (kept constant across trials here)
        wm_mix = 0.5  # implicit balance; WM reliability is further modulated by lapse below

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(denom_wm, eps)

            # WM lapse toward uniform
            p_uniform = 1.0 / nA
            p_wm = (1.0 - wm_lapse) * p_wm_det + wm_lapse * p_uniform

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr = tau_pos if r > 0.5 else tau_neg
            q[s, a] += lr * delta

            # WM leak toward uniform on the visited state
            leak = 0.2  # fixed small leak for stability; distinct from wm_lapse (which affects policy)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Probabilistic WM binding with reward modulation
            # Binding probability increases with reward: p_bind = wm_gate_base * (1 + r)
            p_bind = np.clip(wm_gate_base * (1.0 + r), 0.0, 1.0)
            if np.random.rand() < p_bind:
                # Reward-weighted strengthening of chosen action, soft overwrite
                eta = 0.8 * r + 0.2 * (1.0 - r)  # stronger update when r=1
                w[s, :] = (1.0 - eta) * w[s, :]
                w[s, a] += eta
                # Renormalize
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :]) if np.sum(w[s, :]) > eps else 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with priority gating by prediction error magnitude.
    
    Mechanism:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled by 10).
    - WM channel: only a fraction of states (slots) receive high-fidelity WM storage at any time.
      Priority for WM storage is proportional to recent unsigned prediction error; higher PE -> higher chance to be in WM.
    - Mixture: if the current state is in WM (gated), use higher WM weight; otherwise rely more on RL.
    - WM dynamics:
        - Decay toward uniform each trial (wm_decay).
        - When gated, reward=1 causes overwrite to one-hot; reward=0 causes partial move toward uniform.
    
    Set-size effect:
    - Number of effective WM slots scales with set size: K = max(1, round(slot_frac * nS)).
      Thus WM coverage per block expands with more states, but remains fractional (capacity-like).
    
    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight when a state is currently in WM (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - slot_frac: Fraction of states that can be in WM concurrently (0..1); K = slot_frac * nS.
    - wm_decay: WM decay rate toward uniform (0..1).
    - pe_priority_temp: Temperature for converting unsigned PEs into WM priority (>=0).
    """
    lr, wm_weight, softmax_beta, slot_frac, wm_decay, pe_priority_temp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        pe_abs = np.zeros(nS)  # running measure of unsigned PE for priority

        # Capacity in number of states
        K = max(1, int(round(np.clip(slot_frac, 0.0, 1.0) * nS)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Derive WM gating set for this trial based on priorities
            if pe_priority_temp <= 0:
                priorities = np.ones(nS)
            else:
                priorities = np.exp(pe_priority_temp * (pe_abs - np.max(pe_abs)))
            # Select top-K states (ties arbitrary)
            topK_idx = np.argsort(-priorities)[:K]
            in_wm = s in topK_idx

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            wm_w_eff = wm_weight if in_wm else 0.0
            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update and PE tracking
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Update unsigned PE trace for priority (leaky integration)
            pe_abs[s] = 0.7 * pe_abs[s] + 0.3 * abs(delta)

            # WM decay on visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM updating only if the state is within WM set
            if in_wm:
                if r > 0.5:
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                else:
                    # Move partially toward uniform (forget incorrect mapping)
                    eta = 0.5
                    w[s, :] = (1.0 - eta) * w[s, :] + eta * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p