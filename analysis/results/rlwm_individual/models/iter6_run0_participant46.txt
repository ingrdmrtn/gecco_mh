Below are three standalone cognitive models that follow the provided template logic, combining an RL learner with a working-memory (WM) component. Each function:

- Accepts states, actions, rewards, blocks, set_sizes, model_parameters
- Uses all parameters meaningfully (≤ 6 per model)
- Returns the negative log-likelihood of observed choices
- Fills in WM policy and WM updates while keeping the given RL policy and update
- Uses load-dependent mechanisms so WM contributes differently for set size 3 vs 6

Note: Assume numpy is available as np. No imports are included inside the functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + Bayesian WM (Dirichlet counts) with entropy-based arbitration and load-dependent forgetting

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: per-state Dirichlet counts over actions; the WM policy is the mean of a Dirichlet, i.e., normalized counts.
      WM forgets more when set size is large.
    - Arbitration: WM weight scales with (a) a baseline wm_weight0, (b) certainty of WM for the current state
      via 1 - normalized entropy, and (c) a set-size factor (3/nS)^ss_penalty.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight0: float in [0,1], baseline WM mixture weight.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - kappa0: float > 0, initial Dirichlet concentration per action (prior strength) for WM.
    - ss_penalty: float >= 0, governs how WM availability/retention degrades with set size. Larger values penalize nS=6 more.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight0, softmax_beta, kappa0, ss_penalty = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic for WM softmax
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM as Dirichlet counts; start with symmetric prior kappa0
        counts = kappa0 * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM distribution for state s as normalized counts
            C_s = counts[s, :]
            sumC = np.sum(C_s)
            if sumC <= 0:
                W_s = np.ones(nA) / nA
            else:
                W_s = C_s / sumC

            # WM policy probability of chosen action a (softmax with high beta on W)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size factor
            scale_ss = (3.0 / float(nS)) ** max(0.0, ss_penalty)

            # WM certainty via 1 - normalized entropy
            eps = 1e-12
            entropy = -np.sum(np.clip(W_s, eps, 1.0) * np.log(np.clip(W_s, eps, 1.0)))
            entropy_max = np.log(nA)
            certainty = 0.0 if entropy_max <= 0 else (1.0 - entropy / entropy_max)
            wm_weight_eff = np.clip(wm_weight0 * certainty * scale_ss, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM forgetting: shrink counts toward prior more when set size is large
            # Forgetting fraction f in [0,1): larger for nS=6 when ss_penalty>0
            # Here we define retention = scale_ss, so forgetting f = 1 - retention
            retention = scale_ss
            forgetting = np.clip(1.0 - retention, 0.0, 1.0)
            counts = (1.0 - forgetting) * counts + forgetting * (kappa0 * np.ones_like(counts))

            # WM update: add evidence for chosen action if rewarded
            if r > 0.5:
                counts[s, a] += 1.0
            # If not rewarded, we leave counts unchanged (Dirichlet prior remains the hedge)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL + capacity-limited WM store with load-dependent recall and lapse

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: discrete associative store that keeps the last rewarded action for a state, if any.
    - Load dependence:
      * Probability of successful WM recall p_recall = min(1, capacity_K / nS).
      * WM lapse/noise increases with set size: epsilon_ss = min(0.49, noise_ss_gain * (nS-3)/3).
    - Arbitration: mixture weight = wm_weight0 * p_recall.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight0: float in [0,1], baseline WM mixture weight.
    - capacity_K: float >= 0, effective WM capacity in number of items (used to scale p_recall vs set size).
    - noise_ss_gain: float >= 0, scales how WM lapse grows from set size 3 to 6.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight0, capacity_K, noise_ss_gain = model_parameters
    softmax_beta *= 10
    # We won’t use a WM softmax here; we define WM choice probabilities directly via lapse.
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM memory: -1 means no association stored for this state; otherwise store action index in [0..nA-1]
        mem = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Load-dependent recall probability and lapse
            p_recall = np.clip(capacity_K / max(1.0, float(nS)), 0.0, 1.0)
            epsilon_ss = min(0.49, max(0.0, noise_ss_gain * (float(nS) - 3.0) / 3.0))

            # Compute WM policy probability of chosen action a
            if mem[s] >= 0:
                # If we recall the stored association, probability mass (1 - epsilon) on stored action
                if a == mem[s]:
                    p_wm = (1.0 - epsilon_ss)
                else:
                    p_wm = epsilon_ss / (nA - 1)
            else:
                # No stored association => uniform
                p_wm = 1.0 / nA

            # Arbitration: WM weight scales with recall probability
            wm_weight_eff = np.clip(wm_weight0 * p_recall, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: store rewarded association; clear wrong stored association if it yields 0
            if r > 0.5:
                mem[s] = a
            else:
                # If the currently stored action for this state was chosen and failed, clear it
                if mem[s] == a:
                    mem[s] = -1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: RL + surprise-gated WM strength with deterministic WM policy and load-dependent decay

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: graded associative matrix over actions that is drawn toward one-hot for rewarded pairs
      and decays toward uniform more under higher load.
    - Arbitration: WM weight scales with
        wm_weight0 * gate(surprise) * (3/nS),
      where surprise = |reward - Q(s,a)| and gate is a logistic with slope gate_beta and threshold theta.
      Thus, positive surprise opens the WM gate more; larger set size dampens the effect.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight0: float in [0,1], baseline WM mixture weight.
    - gate_beta: float >= 0, slope of surprise gate (higher => sharper gating).
    - theta: float >= 0, surprise threshold around which the gate turns on.
    - wm_alpha: float in [0,1], WM learning/decay rate (used for updating toward one-hot and for decay magnitude).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight0, gate_beta, theta, wm_alpha = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy via near-deterministic softmax on W
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise and gating
            delta = r - Q_s[a]
            surprise = abs(delta)
            gate = 1.0 / (1.0 + np.exp(-(surprise - theta) * max(0.0, gate_beta)))
            # Load factor (no extra parameter cost)
            load_scale = 3.0 / float(nS)
            wm_weight_eff = np.clip(wm_weight0 * gate * load_scale, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s][a] += lr * delta

            # WM decay increases with load (less decay at nS=3, more at nS=6)
            # Use a decay rate tied to wm_alpha and load: decay = wm_alpha * (1 - load_scale)
            decay = np.clip(wm_alpha * (1.0 - load_scale), 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # WM learning toward one-hot for rewarded outcomes; mild repulsion for non-reward
            if r > 0.5:
                # Move row s toward one-hot on chosen action with step wm_alpha
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # Push slightly away from the chosen action to reduce its WM salience
                w[s, a] = (1.0 - wm_alpha) * w[s, a]
                # Renormalize state row to sum to 1 to keep W a valid distribution
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p