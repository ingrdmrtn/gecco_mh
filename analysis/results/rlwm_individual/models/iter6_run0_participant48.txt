def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + WM with set-sizeâ€“scaled WM decay and state-specific choice stickiness.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate.
    - WM: one-shot storage of rewarded state-action associations; continuous decay toward uniform that accelerates with set size.
    - Choice stickiness: within each state, a bias toward repeating the last chosen action (implemented in the WM policy).
    - Arbitration: fixed WM mixture weight.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - omega0: baseline WM mixture weight in choice (0..1)
    - rho: base WM decay factor per trial (0..1); effective decay increases with set size
    - kappa_stick: strength of within-state choice stickiness added to WM policy (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, omega0, rho, kappa_stick = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM decay increases with set size
        wm_forget = 1.0 - (1.0 - rho) ** max(1, (nS - 1))

        # Track last chosen action per state for stickiness in WM
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with stickiness bias on last chosen action in this state
            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                # Additive bias implemented by shifting W_s before softmax
                bias_vec = np.zeros(nA)
                bias_vec[last_action[s]] = kappa_stick
                W_s = W_s + bias_vec
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = omega0 * p_wm + (1.0 - omega0) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform at the block level
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            # WM learning: on rewarded trials, store one-hot
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited WM slots with probabilistic engagement and precision; hybrid with RL.

    Mechanism:
    - RL: tabular Q-learning (single learning rate).
    - WM: stores rewarded pairs with decay; WM is engaged with probability proportional to slots/nS.
      When engaged, WM policy is noisier/less precise according to xi, implemented by mixing with uniform.
    - Arbitration: WM weight scales with engagement probability.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - slots: number of WM slots (>=0), controls engagement probability p_in = min(1, slots/nS)
    - decay: WM decay rate toward uniform per trial (0..1)
    - omega0: baseline WM weight when engaged (0..1)
    - xi: WM precision (0..1); xi=1 is precise WM, xi=0 reduces WM to uniform

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, slots, decay, omega0, xi = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM engagement probability based on capacity
        p_in = min(1.0, float(slots) / max(1.0, float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with reduced precision xi (blend with uniform before softmax)
            W_s = w[s, :].copy()
            W_s = xi * W_s + (1.0 - xi) * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM contributes in proportion to engagement probability
            wm_weight = omega0 * p_in
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - decay) * w + decay * w_0

            # WM learning: reinforce rewarded association
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    EWA-style RL with interference-controlled WM and perseveration bias.

    Mechanism:
    - RL: experience-weighted attraction (EWA-like). Values are partially decayed each trial
      (decay_e), then updated toward recent outcomes with learning rate lr_e.
    - WM: rewarded associations stored one-shot; global interference grows with set size and
      increases forgetting and reduces arbitration weight.
    - Perseveration: tendency to repeat the last action within each state biases the RL policy.

    Parameters:
    - lr_e: RL updating strength toward recent outcomes (0..1)
    - decay_e: RL value decay per trial (0..1), implements experience discounting
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - omega0: baseline WM mixture weight in choice (0..1)
    - kappa_interf: interference sensitivity to set size (>=0); larger kappa => more forgetting, less WM reliance
    - persev: perseveration bias added to the last chosen action (>=0), applied in RL policy

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_e, decay_e, softmax_beta, omega0, kappa_interf, persev = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference factor grows with set size
        alpha = 1.0 - np.exp(-kappa_interf * max(1.0, float(nS)))

        # Track last action for perseveration in RL policy
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias on last action for this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += persev
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight reduced by interference
            wm_weight = omega0 * (1.0 - alpha)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL EWA-like update: decay then move chosen value toward outcome
            q[s, :] = (1.0 - decay_e) * q[s, :]
            q[s, a] += lr_e * (r - q[s, a])

            # WM interference-driven forgetting toward uniform
            w = (1.0 - alpha) * w + alpha * w_0

            # WM learning: on reward, store one-hot association
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p