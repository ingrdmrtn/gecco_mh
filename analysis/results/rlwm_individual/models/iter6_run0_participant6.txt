def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and WM learning/decay.

    Idea:
    - Choices are a mixture of model-free RL and a probabilistic WM store.
    - Arbitration weight for WM depends on WM certainty (low entropy => higher WM weight)
      and implicitly on load via the entropy computed from the current WM state distribution.
    - WM traces learn from reward (graded learning rate) and decay toward uniform each trial.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_entropy_sensitivity: gain that increases WM weight when WM distribution is sharp (low entropy).
    - load_bias: bias term (logit scale) that shifts the WM weight globally (can absorb load effects).
    - wm_decay: WM decay rate toward uniform (0..1) applied each trial.
    - wm_learn_rate: WM learning rate for rewarded trials (0..1) and mild unlearning for unrewarded trials.
    
    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_entropy_sensitivity, load_bias, wm_decay, wm_learn_rate = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy via softmax over WM weights
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration: sharper WM (lower entropy) => larger WM weight
            # Compute entropy of W_s (add epsilon for stability)
            eps = 1e-12
            H = -np.sum(W_s * np.log(np.clip(W_s, eps, 1.0)))
            H_max = np.log(nA)
            certainty = 1.0 - H / H_max  # 0=uniform, 1=delta
            logit_w = load_bias + wm_entropy_sensitivity * certainty
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0

            # WM learning: reward strengthens the chosen association; no-reward weakly suppresses it
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn_rate) * w[s, :] + wm_learn_rate * target
            else:
                # mild suppression of the chosen action probability, with renormalization
                w[s, a] = max(w[s, a] * (1 - 0.5 * wm_learn_rate), 0.0)
                # renormalize row to sum to 1
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM with load-induced interference and confidence gating.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores one-shot rewarded associations, but suffers interference that grows with set size.
      Interference is modeled as mixing the WM distribution with uniform as nS increases.
    - Arbitration weight for WM is confidence-gated: higher when WM is sharp (large max prob).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_gate: gain for converting WM confidence to a mixture weight (0..5 typical).
    - interference_gamma: controls how quickly WM is diluted toward uniform with set size (>0).
    - wm_decay: per-trial decay of WM toward uniform (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_gate, interference_gamma, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # load-dependent WM interference factor (0 at low load -> approaches 1 at high load)
        k_interf = 1 - np.exp(-interference_gamma * max(nS - 1, 0))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]

            # RL choice prob
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # Effective WM row after load-induced interference
            W_s = w[s,:]
            W_eff = (1 - k_interf) * W_s + k_interf * w_0[s, :]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Confidence-gated mixture: transform max probability into a weight via wm_gate
            conf = np.max(W_eff) - 1.0 / nA  # 0 if uniform, up to 1 - 1/nA
            wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_gate * conf))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0 else lr_neg
            q[s][a] += lr * pe

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0

            # WM update: one-shot write on reward; otherwise gentle relaxation
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
            else:
                # small nudge toward uniform if incorrect to avoid false memories
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated WM arbitration with WM learning/decay.

    Idea:
    - WM learns from reward and decays; as in a one-shot memory but probabilistic.
    - Arbitration weight for WM depends on recent RL surprise (unsigned PE) for the same state:
      when RL is surprised (large |PE|), the model shifts toward WM guidance.
    - Load reduces the influence of surprise on WM arbitration.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - kappa_pe: gain on unsigned prediction error for WM arbitration (logit scale).
    - lambda_load: load penalty on WM arbitration per additional item beyond 3 (>=0).
    - wm_decay: WM decay rate toward uniform per trial (0..1).
    - wm_eta: WM learning rate toward rewarded action (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, kappa_pe, lambda_load, wm_decay, wm_eta = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last unsigned PE per state for arbitration; initialize to 0
        last_abs_pe = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # PE-gated arbitration: larger recent surprise => more WM, penalized by load
            load_pen = lambda_load * max(nS - 3, 0)
            logit_w = kappa_pe * last_abs_pe[s] - load_pen
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            pe = r-Q_s[a]
            q[s][a] += lr*pe

            # Update last_abs_pe for this state with the current surprise
            last_abs_pe[s] = abs(pe)

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0

            # WM learning: move toward one-hot on reward; slight suppression on non-reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_eta) * w[s, :] + wm_eta * target
            else:
                w[s, a] = max(w[s, a] * (1 - 0.5 * wm_eta), 0.0)
                # renormalize
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p