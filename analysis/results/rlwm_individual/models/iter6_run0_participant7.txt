def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity-limited interference and reward-driven WM encoding.

    Core ideas
    - RL: standard Q-learning with softmax choice.
    - WM: stores state-action distributions; encodes toward chosen action more strongly after reward.
    - Capacity-limited interference: when the number of states exceeds a capacity (derived from cap_ratio),
      WM representations are blended toward the block-average (overload-dependent interference).
    - This mechanism makes WM helpful under low load (nS=3) and more diffuse under high load (nS=6).

    Parameters
    ----------
    states : array-like of int
        State on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, cap_ratio, overload_interf, wm_alpha_pos)
        - lr: RL learning rate in [0,1]
        - wm_weight: mixture weight between WM and RL in [0,1] (used in p_total line)
        - softmax_beta: RL inverse temperature (>0); internally scaled up
        - cap_ratio: fraction of 6 items that can be held without overload (0..1)
        - overload_interf: strength of interference blending when nS exceeds capacity (>=0)
        - wm_alpha_pos: WM encoding step size toward chosen action after reward (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, cap_ratio, overload_interf, wm_alpha_pos = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity threshold in number of states
        K_states = max(1, int(round(cap_ratio * 6)))  # anchor to max set size = 6
        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM map for this state
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture (template)
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Capacity-limited interference (applied before encoding to simulate diffuse WM under overload)
            if nS > K_states:
                # Interference strength increases with overload
                overload = (nS - K_states) / max(1, nS)
                gamma = np.clip(overload_interf * overload, 0.0, 1.0)
                w_mean = np.mean(w, axis=0)
                w[s, :] = (1.0 - gamma) * w[s, :] + gamma * w_mean

            # Reward-driven WM encoding
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.0:
                # Positive outcome: push WM toward chosen action
                enc = np.clip(wm_alpha_pos, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot
            else:
                # Negative outcome: weak repulsion from chosen action (spread to others)
                enc_neg = np.clip(0.25 * wm_alpha_pos, 0.0, 1.0)
                anti = (1.0 - onehot) / (nA - 1)
                w[s, :] = (1.0 - enc_neg) * w[s, :] + enc_neg * anti

            # Normalize/safeguard
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with error-gated encoding and load-scaled decay.

    Core ideas
    - RL: standard Q-learning with softmax.
    - WM: encodes more strongly when recent prediction errors are large (surprise-sensitive),
      and decays toward uniform with a base rate that increases under higher load.
    - This captures that under high load, WM traces fade faster, but surprising outcomes
      can transiently boost encoding to stabilize learning.

    Parameters
    ----------
    states : array-like of int
        State on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, rpe_sensitivity, wm_decay_base, load_scaler)
        - lr: RL learning rate in [0,1]
        - wm_weight: mixture weight between WM and RL in [0,1] (used in p_total line)
        - softmax_beta: RL inverse temperature (>0); internally scaled up
        - rpe_sensitivity: scales WM encoding strength as a function of unsigned prediction error (>=0)
        - wm_decay_base: baseline WM decay toward uniform per encounter (0..1)
        - load_scaler: multiplicative increase in decay when nS=6 vs 3 (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rpe_sensitivity, wm_decay_base, load_scaler = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track a simple RPE trace per state to gate encoding by recent surprise
        rpe_trace = np.zeros(nS)
        # Precompute decay factor as function of load
        load_mult = 1.0 + max(0.0, load_scaler) * (1 if nS > 3 else 0)
        decay_rate = np.clip(wm_decay_base * load_mult, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax over W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture (template)
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update RPE trace for the visited state (leaky integrator)
            # Higher rpe_sensitivity makes the trace respond more to error magnitude
            rpe_trace[s] = 0.5 * rpe_trace[s] + np.clip(rpe_sensitivity * abs(delta), 0.0, 1.0)

            # WM decay toward uniform (load-scaled)
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            # Surprise-gated encoding: larger recent errors increase encoding
            # After reward: encode toward chosen action; after no reward: distribute to others
            enc_strength = np.clip(0.2 + 0.6 * rpe_trace[s], 0.0, 1.0)  # between 0.2 and 0.8
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.0:
                w[s, :] = (1.0 - enc_strength) * w[s, :] + enc_strength * onehot
            else:
                anti = (1.0 - onehot) / (nA - 1)
                w[s, :] = (1.0 - 0.5 * enc_strength) * w[s, :] + (0.5 * enc_strength) * anti

            # Normalize/safeguard
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with time-based decay, load-dependent lapses, and misbinding (swap) errors.

    Core ideas
    - RL: standard Q-learning with softmax.
    - WM: maintains state-specific distributions but:
        (a) decays with time since last visit (recency-based),
        (b) exhibits load-dependent lapses (mixture with uniform),
        (c) suffers misbinding: retrieving another state's representation with some probability,
            which increases with the number of competing items (set size).
    - These effects reduce WM usefulness especially in the 6-item condition, and more so for
      infrequently visited states.

    Parameters
    ----------
    states : array-like of int
        State on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, time_decay_tau, load_lapse_slope, misbinding_rate)
        - lr: RL learning rate in [0,1]
        - wm_weight: mixture weight between WM and RL in [0,1] (used in p_total line)
        - softmax_beta: RL inverse temperature (>0); internally scaled up
        - time_decay_tau: time constant for recency-based WM decay (>0). Larger means slower decay.
        - load_lapse_slope: additional lapse probability when nS=6 vs 3 (>=0)
        - misbinding_rate: base probability of swapping to another state's WM, scaled by set size (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, time_decay_tau, load_lapse_slope, misbinding_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last visit time to implement recency-based decay
        last_seen = -1 * np.ones(nS, dtype=int)
        t_global = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply recency-based decay to the current state's WM
            if last_seen[s] >= 0:
                dt = max(1, t_global - last_seen[s])
                # Exponential-like decay factor toward uniform as a function of elapsed time
                decay = 1.0 - np.exp(-dt / max(1e-6, time_decay_tau))
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Compute an effective WM readout including misbinding and load-dependent lapses
            # Misbinding: with probability swap, sample another state's WM (average proxy)
            if nS > 1:
                swap = np.clip(misbinding_rate * (nS - 1) / nS, 0.0, 1.0)
                avg_other = np.mean(np.delete(w, s, axis=0), axis=0) if nS > 1 else w[s, :]
                W_eff = (1.0 - swap) * w[s, :] + swap * avg_other
            else:
                W_eff = w[s, :]

            # Load-dependent lapse: mix with uniform more under higher load
            lapse = np.clip((0.0 + load_lapse_slope * (1 if nS > 3 else 0)), 0.0, 1.0)
            W_eff = (1.0 - lapse) * W_eff + lapse * w_0[s, :]

            Q_s = q[s, :]
            # RL policy (given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy on the effective WM readout
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture (template)
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM encoding based on outcome: rewarded -> toward chosen action; unrewarded -> spread
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.0:
                enc = 0.6  # fixed encoding strength; overall effect modulated by decay/lapse above
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot
            else:
                enc_neg = 0.2
                anti = (1.0 - onehot) / (nA - 1)
                w[s, :] = (1.0 - enc_neg) * w[s, :] + enc_neg * anti

            # Normalize/safeguard
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

            # Update last seen and time
            last_seen[s] = t_global
            t_global += 1

        blocks_log_p += log_p

    return -blocks_log_p