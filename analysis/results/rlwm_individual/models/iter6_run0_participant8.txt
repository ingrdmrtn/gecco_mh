def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, decaying WM with entropy-weighted arbitration and choice stickiness.

    Idea
    - RL: standard Q-learning with softmax + choice-stickiness bias.
    - WM: a cached distribution over actions per state; encoding is stronger after reward,
      but decays with set size (more interference when nS=6).
    - Arbitration: WM weight scales with (a) base wm_weight, (b) success of WM encoding,
      (c) set-size penalty, and (d) WM certainty (low entropy).
    - Choice stickiness: adds a bias toward repeating the last action in the block.

    Parameters
    ----------
    model_parameters : (lr, wm_weight, softmax_beta, wm_slot_prob, wm_decay_ss, stickiness)
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_slot_prob : float
            Base probability (0-1) that a rewarded state-action is strongly encoded in WM.
        wm_decay_ss : float
            Set-size interference/decay rate for WM (>=0). Larger values => faster WM decay with larger set sizes.
        stickiness : float
            Choice stickiness parameter (>=0). Adds a bias to repeat previous action within block.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_slot_prob, wm_decay_ss, stickiness = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_action = None

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax with stickiness bias
            bias = np.zeros(nA)
            if last_action is not None:
                bias[last_action] = stickiness
            logits_rl = softmax_beta * Q_s + bias
            logits_rl -= logits_rl[a]
            p_rl = 1.0 / np.sum(np.exp(logits_rl))

            # WM softmax with same stickiness bias (to keep bias consistent across systems)
            logits_wm = softmax_beta_wm * W_s + bias
            logits_wm -= logits_wm[a]
            p_wm_core = 1.0 / np.sum(np.exp(logits_wm))

            # Arbitration: WM certainty via entropy; lower entropy => higher certainty
            ent = -np.sum(np.clip(W_s, 1e-12, 1.0) * np.log(np.clip(W_s, 1e-12, 1.0)))
            ent_max = np.log(nA)
            wm_cert = 1.0 - ent / ent_max  # in [0,1]

            # Set-size penalty (more interference in larger set sizes)
            ss_penalty = 3.0 / max(3.0, float(nS))  # =1 for 3, =0.5 for 6

            # Encoding success proxy: stronger after recent reward in the same state
            # Use the peakiness of W_s toward the chosen action as a proxy (0..1)
            enc_success = W_s[a]

            wm_weight_eff = np.clip(wm_weight * wm_cert * ss_penalty * (0.5 + 0.5 * enc_success), 0.0, 1.0)

            p_total = p_wm_core * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay/interference scales with set size
            forget = 1.0 - np.exp(-wm_decay_ss * (float(nS) / 3.0))
            w[s, :] = (1.0 - forget) * W_s + forget * w_0[s, :]

            # WM encoding: if rewarded, push toward one-hot with probability scaled by wm_slot_prob and set size
            enc_prob = wm_slot_prob * (3.0 / max(3.0, float(nS)))
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Stochastic-like soft update (expected update magnitude depends on enc_prob)
                w[s, :] = (1.0 - enc_prob) * w[s, :] + enc_prob * one_hot
            else:
                # Slight correction toward uniform on errors
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Normalize to keep as a proper distribution
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + WM with interference-scaled precision + accuracy-gated arbitration.

    Idea
    - RL: Q-learning with separate learning rates for positive vs negative outcomes.
    - WM: action distribution per state; precision (inverse temperature) degrades with set size by phi_interf.
    - Arbitration: base WM weight is up- or down-regulated by a running estimate of block accuracy.
      If recent accuracy is high, rely more on WM; if low, rely more on RL.
    - WM update: Hebbian strengthening on rewarded trials, mild unlearning on errors.

    Parameters
    ----------
    model_parameters : (lr_pos, lr_neg, wm_weight0, softmax_beta, phi_interf, acc_sensitivity)
        lr_pos : float
            RL learning rate for positive outcomes.
        lr_neg : float
            RL learning rate for negative outcomes.
        wm_weight0 : float
            Base WM mixture weight.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        phi_interf : float
            WM interference/precision loss per set-size unit (>=0).
        acc_sensitivity : float
            Sensitivity (>0) controlling how strongly recent accuracy modulates WM reliance.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight0, softmax_beta, phi_interf, acc_sensitivity = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # baseline high precision before interference
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Running accuracy estimate (EWMA)
        acc_hat = 0.5
        acc_alpha = 0.2

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            logits_rl = softmax_beta * Q_s
            logits_rl -= logits_rl[a]
            p_rl = 1.0 / np.sum(np.exp(logits_rl))

            # WM precision degrades with interference phi_interf and set size
            wm_beta_eff = softmax_beta_wm / (1.0 + phi_interf * (float(nS) - 3.0) / 3.0)
            logits_wm = wm_beta_eff * W_s
            logits_wm -= logits_wm[a]
            p_wm = 1.0 / np.sum(np.exp(logits_wm))

            # Arbitration: WM weight adapted by recent accuracy (higher acc -> more WM)
            wm_gain = 1.0 / (1.0 + np.exp(-acc_sensitivity * (acc_hat - 0.5)))  # in (0,1)
            wm_weight_eff = np.clip(wm_weight0 * wm_gain, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update with valence asymmetry
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr_use * delta

            # WM update
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Hebbian strengthening proportional to (1 - current prob)
                eta_plus = 0.6
                w[s, :] = (1 - eta_plus) * W_s + eta_plus * one_hot
            else:
                # Mild unlearning toward uniform on errors
                eta_minus = 0.1
                w[s, :] = (1 - eta_minus) * W_s + eta_minus * w_0[s, :]

            # Normalize WM row
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

            # Update running accuracy estimate
            acc_hat = (1 - acc_alpha) * acc_hat + acc_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + WM recency cache + set-size logistic arbitration + lapse.

    Idea
    - RL: Q-learning augmented with an eligibility trace on the chosen state-action to boost recency learning.
    - WM: recency-weighted cache that strongly encodes rewarded actions; decays otherwise.
    - Arbitration: WM weight determined by a logistic function of set size (more WM in small sets).
    - Lapse: with small probability, choices are random.

    Parameters
    ----------
    model_parameters : (lr, softmax_beta, lambda_trace, wm_recency, ss_logit_slope, lapse)
        lr : float
            RL learning rate.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        lambda_trace : float
            Eligibility trace parameter in [0,1]; larger values sustain greater carry-over influence.
        wm_recency : float
            WM recency learning rate in [0,1]; how strongly WM is driven toward the most recent rewarded action.
        ss_logit_slope : float
            Slope (>0) of the logistic mapping from set size to WM weight; larger => sharper WM drop with nS=6.
        lapse : float
            Lapse rate in [0,1]; with this probability, choices are uniform random.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, lambda_trace, wm_recency, ss_logit_slope, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    lapse = np.clip(lapse, 0.0, 1.0)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility trace for RL
        e = np.zeros((nS, nA))

        # Set-size logistic WM weight: higher for small sets
        # Map nS in {3,6} with center at 4.5 to separate the two sizes
        x = (4.5 - float(nS))  # positive for nS=3, negative for nS=6
        wm_weight_eff_block = 1.0 / (1.0 + np.exp(-ss_logit_slope * x))
        wm_weight_eff_block = np.clip(wm_weight_eff_block, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            logits_rl = softmax_beta * Q_s
            logits_rl -= logits_rl[a]
            p_rl = 1.0 / np.sum(np.exp(logits_rl))

            # WM policy (deterministic-ish)
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= logits_wm[a]
            p_wm = 1.0 / np.sum(np.exp(logits_wm))

            # Mixture with block-level WM weight, then add lapse
            p_mix = wm_weight_eff_block * p_wm + (1 - wm_weight_eff_block) * p_rl
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(p_total + 1e-12)

            # RL update with eligibility trace (replace trace on chosen, decay others)
            e *= lambda_trace
            e[s, a] = 1.0
            delta = r - Q_s[a]
            q += lr * delta * e  # broadcast across all SxA via eligibility

            # WM update: recency cache updated on reward toward one-hot, decay otherwise
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - wm_recency) * W_s + wm_recency * one_hot
            else:
                decay = 0.1 * (float(nS) / 3.0)  # more decay under larger set size
                w[s, :] = (1 - decay) * W_s + decay * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p