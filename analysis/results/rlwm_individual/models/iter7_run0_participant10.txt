def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with probabilistic encoding and load-dependent decay.

    Idea:
    - RL learns action values per state using a reward sensitivity rho that scales prediction errors.
    - WM stores the last rewarded action as a sharp distribution if it successfully encodes.
    - WM encoding is probabilistic and decreases with set size via a power-law; WM also decays toward uniform at a rate that increases with set size.
    - Action selection is a mixture of RL and WM policies with a fixed mixture weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight for WM policy (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - rho: Reward sensitivity scaling of RL prediction error (>=0)
    - eta_pow: Exponent controlling set-size impact on WM encoding/decay (>=0); larger -> stronger load effects
    - enc_fail: Baseline WM encoding failure probability (0..1); higher -> less likely to encode even after reward

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, rho, eta_pow, enc_fail = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay and encoding probability
        # Decay rate increases with set size; encoding probability decreases with set size
        # Power-law-shaped: more sensitive when eta_pow is large
        decay = 1.0 - 1.0 / (1.0 + max(0.0, float(nS) - 1.0) ** max(0.0, eta_pow))
        p_encode_base = 1.0 / (1.0 + max(0.0, float(nS) - 1.0) ** max(0.0, eta_pow))
        p_encode = max(0.0, 1.0 - enc_fail) * p_encode_base

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(1e-12, denom_rl)

            # WM policy (deterministic softmax over WM distribution)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update with reward sensitivity
            delta = (rho * r) - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial for current state
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM encoding after reward with probability p_encode
            if r > 0.5:
                if np.random.rand() < p_encode:
                    w[s, :] = 0.0
                    w[s, a] = 1.0

            # Normalize to avoid numerical drift
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with arbitration based on value advantage and load penalty.

    Idea:
    - RL learns standard Q-values per state.
    - WM stores the last rewarded action; WM traces decay toward uniform with load-dependent leak.
    - Arbitration weight for WM is dynamic: a sigmoid of the (WM - RL) "advantage" in the current state,
      biased by a base weight and penalized by set size (load).
    - Thus, when WM is sharper than RL in a state, arbitration favors WM, but high set size reduces reliance on WM.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Baseline WM mixture offset in the sigmoid (0..1 roughly interpretable)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - gain_k: Sensitivity of arbitration to WM-RL advantage (>=0)
    - gain_b: Bias term for arbitration (can be negative/positive)
    - load_penalty: Strength of set-size penalty on WM use (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, gain_k, gain_b, load_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load penalty mapped to a decay rate for WM
        wm_decay = 1.0 - np.exp(-max(0.0, load_penalty) * max(0.0, float(nS) - 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(1e-12, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Dynamic arbitration weight based on advantage and load
            # Advantage = (WM sharpness - RL sharpness) estimated by max-value spread
            wm_spread = np.max(W_s) - np.mean(W_s)
            rl_spread = np.max(Q_s) - np.mean(Q_s)
            advantage = wm_spread - rl_spread

            # Effective bias: baseline wm_weight and gain_b
            x = gain_k * advantage + gain_b + (wm_weight - 0.5) - max(0.0, load_penalty) * (nS - 3.0)
            w_eff = 1.0 / (1.0 + np.exp(-x))
            w_eff = min(1.0, max(0.0, w_eff))

            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and reward-based overwrite
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with stickiness + WM with negative tagging and load-dependent leak.

    Idea:
    - RL learns Q-values and includes a "stickiness" bias that favors repeating the last action taken in a state.
    - WM stores positive evidence (last rewarded action) as a sharp distribution, and also applies negative tagging:
      after non-reward, WM downweights the chosen action.
    - WM reliability decays toward uniform more strongly under higher set sizes via a load_scale parameter.
    - Action selection mixes RL and WM policies with a fixed mixture weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight for WM policy (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - stickiness: Additive bias applied in RL policy to the previously chosen action in that state (can be +/-)
    - wm_anti: Strength of negative WM tagging after non-reward (0..1), higher -> stronger suppression
    - load_scale: Controls how fast WM decays with set size (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, wm_anti, load_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-dependent WM leak
        wm_decay = 1.0 - np.exp(-max(0.0, load_scale) * max(0.0, float(nS) - 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Apply stickiness bias to RL values for previous action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL policy with biased Q_s
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(1e-12, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update uses the unbiased learned Q-table
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM update: reinforce on reward; negatively tag on non-reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Downweight the selected action, renormalize
                suppression = max(0.0, min(1.0, wm_anti))
                w[s, a] = (1.0 - suppression) * w[s, a]
                w[s, :] = np.maximum(w[s, :], 1e-12)
                w[s, :] /= np.sum(w[s, :])

            # Ensure normalization
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p