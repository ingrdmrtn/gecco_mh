def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Delay- and set-size–dependent WM recall with RL+WM mixture.

    Idea
    - RL learns Q-values with a standard delta rule and softmax policy.
    - WM stores state-action associations and is read out via a softmax; however, access to WM
      is probabilistic and depends on:
        (a) delay since the state's last visit (longer delay => weaker WM recall),
        (b) set size (larger set => weaker WM recall).
    - The WM readout probability pr dynamically gates between a (near-deterministic) WM policy
      and a uniform fallback when WM is not recalled.
    - WM traces are updated on each visit with an imprint strength.

    Parameters
    ----------
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - softmax_beta: Base RL inverse temperature; internally scaled by 10.
        - wm_base_recall: Baseline WM recall propensity (logit scale).
        - wm_delay_cost: Cost of delay per trial since last visit (higher -> less recall).
        - wm_size_cost: Additional WM recall cost per +1 state beyond 3 (higher -> less recall).
        - wm_learn: WM imprint strength on visited state (0..1).

    Set-size impact
    - Effective WM recall probability pr decreases as set size increases via wm_size_cost*(nS-3).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_param, wm_base_recall, wm_delay_cost, wm_size_cost, wm_learn = model_parameters

    softmax_beta = softmax_beta_param * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track delay (age) since last visit for each state
        age = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Increment all ages, reset on visited state later
            age += 1.0

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob for taken action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax over current WM weights
            wm_probs = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_probs = wm_probs / np.sum(wm_probs)

            # Delay- and set-size–dependent WM recall probability
            size_penalty = wm_size_cost * (nS - 3.0)
            recall_logit = wm_base_recall - wm_delay_cost * age[s] - size_penalty
            pr = 1.0 / (1.0 + np.exp(-recall_logit))

            # WM policy for chosen action: either use WM or default to uniform if not recalled
            p_wm = pr * wm_probs[a] + (1.0 - pr) * (1.0 / nA)

            # Mixture: arbitration by recall probability against RL
            # Effective mixture: use pr as WM gate, (1-pr) to rely on RL
            p_total = pr * p_wm + (1.0 - pr) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay towards baseline
            w = (1.0 - 0.0) * w  # no global decay aside from imprint; keep template structure
            # WM imprint on visited state toward the chosen action with magnitude proportional to reward
            # Shift row toward one-hot of action a when rewarded; minimal change if r=0
            target = np.copy(w_0[s, :])
            target[a] = 1.0
            w[s, :] = (1.0 - wm_learn * r) * w[s, :] + (wm_learn * r) * target

            # Reset age for visited state
            age[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–modulated lateral inhibition in WM.

    Idea
    - RL learns standard Q-values and generates a softmax policy.
    - WM stores action weights per state. After each visit, WM for that state undergoes
      lateral inhibition (competition) between actions. The inhibition is stronger in larger sets,
      modeling increased interference/competition under higher load.
    - Choices are a fixed mixture of RL and WM policies.

    Parameters
    ----------
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_weight: Fixed mixture weight on WM policy (0..1).
        - inhibition_gain: Base strength of lateral inhibition in WM for the visited state (>=0).
        - size_gain: How much inhibition_gain scales per +1 state beyond 3 (>=0).
        - wm_decay: Per-trial global decay of WM toward uniform baseline (0..1).

    Set-size impact
    - Lateral inhibition strength = inhibition_gain * (1 + size_gain * (nS - 3)),
      i.e., stronger competition among actions under larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_param, wm_weight, inhibition_gain, size_gain, wm_decay = model_parameters

    softmax_beta = softmax_beta_param * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            wm_probs = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_probs = wm_probs / np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Fixed mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM imprint on visited state toward the chosen action, scaled by reward
            # Rewarded choices push the WM row toward one-hot on action a
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            imprint = r * (one_hot - w[s, :])
            w[s, :] += imprint

            # Lateral inhibition within the visited state (competition)
            # Strength increases with set size
            inhib_strength = inhibition_gain * (1.0 + size_gain * (nS - 3.0))
            if inhib_strength > 0.0:
                # Increase chosen action, suppress others
                chosen_boost = inhib_strength * r
                suppress = chosen_boost / max(nA - 1, 1)
                w[s, a] += chosen_boost
                for aj in range(nA):
                    if aj != a:
                        w[s, aj] = max(0.0, w[s, aj] - suppress)

                # Renormalize row to sum to 1 to keep WM interpretable as preferences
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] = w[s, :] / row_sum
                else:
                    w[s, :] = np.copy(w_0[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM heuristic: win-stay/lose-shift with set-size–modulated reliability.

    Idea
    - RL provides a softmax policy from learned Q-values.
    - WM implements a heuristic policy at the state level:
        * If last time this state was visited and rewarded, repeat that action (win-stay).
        * If last time was unrewarded, choose uniformly among the other two actions (lose-shift).
      The reliability of this heuristic is modulated by set size: more reliable in small sets.
    - WM traces decay over time toward 'no information' (uniform), reducing heuristic influence.

    Parameters
    ----------
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_win: Baseline reliability (probability) of win-stay when last outcome was reward (0..1).
        - wm_lose: Baseline reliability (probability) of lose-shift away from last action when last outcome was 0 (0..1).
        - size_bias_ws: Set-size bias applied in logit space; positive => more reliable in small sets.
        - wm_decay: Decay (forgetting) of the last-outcome/action memory per intervening trial (0..1).

    Set-size impact
    - Effective win/lose reliability is adjusted via a logit shift: +size_bias_ws*(3.5 - nS).
      Thus, nS=3 increases heuristic reliability if size_bias_ws > 0, while nS=6 decreases it.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_param, wm_win, wm_lose, size_bias_ws, wm_decay = model_parameters

    softmax_beta = softmax_beta_param * 10.0
    softmax_beta_wm = 50.0  # used indirectly for forming soft decisions if needed
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Store for each state: last action taken and its last outcome strength (decaying)
        last_action = -np.ones(nS, dtype=int)
        last_val = np.zeros(nS, dtype=float)  # +1 for win memory, -1 for lose memory, decays toward 0

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Decay memory for all states
            last_val *= (1.0 - wm_decay)

            # Build WM heuristic policy for the current state
            if last_action[s] == -1 or abs(last_val[s]) < 1e-8:
                # No usable memory: fallback uniform
                wm_probs = np.ones(nA) / nA
            else:
                size_shift = size_bias_ws * (3.5 - nS)
                def logistic(x):
                    return 1.0 / (1.0 + np.exp(-x))

                if last_val[s] > 0:  # win memory
                    pw = np.clip(wm_win, eps, 1 - eps)
                    pw = logistic(np.log(pw / (1 - pw)) + size_shift)
                    wm_probs = np.ones(nA) * ((1.0 - pw) / (nA - 1))
                    wm_probs[last_action[s]] = pw
                else:  # lose memory
                    pl = np.clip(wm_lose, eps, 1 - eps)
                    pl = logistic(np.log(pl / (1 - pl)) + size_shift)
                    wm_probs = np.ones(nA) * (pl / (nA - 1))
                    wm_probs[last_action[s]] = 1.0 - pl

            p_wm = wm_probs[a]

            # Gate WM vs RL by the absolute memory strength (|last_val| in [0,1] approx)
            wm_weight_t = np.clip(abs(last_val[s]), 0.0, 1.0)
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update memory for this state: store last action and signed outcome, overwrite with stronger evidence
            last_action[s] = a
            # New memory value leans toward +1 for reward, -1 for no reward
            last_val[s] = (1.0 - wm_decay) * last_val[s] + wm_decay * (2.0 * r - 1.0)

            # Maintain w to satisfy template variables; align it to the heuristic distribution for interpretability
            w[s, :] = wm_probs  # optional alignment; not used in the heuristic calculation elsewhere

        blocks_log_p += log_p

    return -blocks_log_p