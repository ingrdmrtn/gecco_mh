def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated arbitration and set-size modulation of the WM weight.

    Mechanism
    - RL: tabular Q-learning (single learning rate) with softmax policy (inverse temperature scaled by 10).
    - WM: cached one-hot association per state when rewarded; on non-rewarded trials, WM decays toward uniform at rate wm_forget.
    - Arbitration: WM weight is dynamically determined by a logistic transform of:
        w0 (bias) + w_size*(3 - set_size) - gamma_uncert * H_RL(s),
      where H_RL(s) is the entropy of the RL softmax over actions in the current state (higher entropy -> more uncertainty).
      Thus WM is favored in small set sizes and when RL is uncertain.
    - No explicit lapse; robustness ensured via tiny floor.

    Set-size dependence
    - The arbitration weight depends on set size via the w_size parameter (favoring set size 3 if positive).
    - Performance will drop in larger set sizes because arbitration shifts away from WM as set size increases (and due to higher RL certainty penalty).

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        w0 : float
            Baseline arbitration logit bias toward WM (negative -> prefer RL, positive -> prefer WM).
        w_size : float
            Set-size slope on arbitration logit; positive increases WM weight for small (3) vs large (6) sets.
        gamma_uncert : float
            Sensitivity of arbitration to RL uncertainty (entropy of RL policy) in the current state.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_forget : float
            WM decay toward uniform on non-rewarded trials (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, w0, w_size, gamma_uncert, softmax_beta, wm_forget = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL policy entropy (uncertainty) for arbitration
            Q_shift = Q_s - np.max(Q_s)
            rl_probs = np.exp(softmax_beta * Q_shift)
            rl_probs /= np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.maximum(rl_probs, tiny)))

            # WM policy (sharp softmax over WM action probabilities)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-gated and set-size-modulated arbitration
            wm_logit = w0 + w_size * (3 - nS) - gamma_uncert * H_rl
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: store one-hot on reward; decay toward uniform on no-reward
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Renormalize WM row
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference-limited recall: set size reduces WM contribution via retrieval probability
    and accelerates WM decay (interference), plus a global lapse.

    Mechanism
    - RL: tabular Q-learning with softmax policy (inverse temperature scaled by 10).
    - WM: probabilistic associative table per state; when rewarded, WM learns toward a one-hot code
      with rate wm_learn; otherwise WM is unchanged aside from interference-induced decay toward uniform.
    - Arbitration: final policy mixes WM and RL using a recall probability p_recall that decreases
      exponentially with set size (interference), then mixes in an epsilon lapse to uniform.

    Set-size dependence
    - p_recall = clip(p_recall_base * exp(-interference_slope * (set_size - 3)), 0, 1).
    - Interference-induced decay: for the current state, after each trial,
      w[s,:] = (1 - d)*w[s,:] + d*w0[s,:], where d = min(1, interference_slope * (set_size - 3 + 1)).
      Thus larger sets cause faster WM decay and lower recall.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        p_recall_base : float
            Baseline WM recall probability at set size 3 (0..1).
        interference_slope : float
            Controls how set size reduces WM recall and increases WM decay (>=0).
        wm_learn : float
            WM learning rate toward one-hot on rewarded trials (0..1).
        epsilon : float
            Lapse probability mixing uniform choice into the final policy (0..0.2 typical).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, p_recall_base, interference_slope, wm_learn, epsilon = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent recall weight and decay magnitude
        p_recall = p_recall_base * np.exp(-interference_slope * (nS - 3))
        p_recall = max(0.0, min(1.0, p_recall))
        decay_d = interference_slope * (nS - 3 + 1.0)
        decay_d = max(0.0, min(1.0, decay_d))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration with recall probability and lapse
            p_total = p_recall * p_wm + (1.0 - p_recall) * p_rl
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)

            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM learning toward one-hot on rewarded trials
            if r > 0.5:
                target = tiny * np.ones(nA)
                target[a] = 1.0
                # convex combination toward target
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target

            # Interference-induced decay toward uniform (applied every trial for the current state)
            w[s, :] = (1.0 - decay_d) * w[s, :] + decay_d * w_0[s, :]

            # Renormalize WM row
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM confidence arbitration + response stickiness,
    with WM forgetting increasing with set size.

    Mechanism
    - RL: tabular Q-learning with separate learning rates for positive vs. negative prediction errors;
      softmax policy (inverse temperature scaled by 10).
    - WM: one-hot cache on reward; otherwise decays toward uniform. Forgetting rate grows with set size.
    - Arbitration: WM weight is the current WM confidence in the state (max(W_s)), so arbitration is driven
      by the strength/sharpness of the WM trace. This indirectly depends on set size via forgetting.
    - Bias: response stickiness (kappa_sticky) biases the final policy toward repeating the previous action.

    Set-size dependence
    - WM forgetting rate = wm_forget_base + wm_forget_size * (set_size - 3), clipped to [0,1].
      Larger sets cause faster decay of WM traces, lowering WM confidence and shifting control to RL.

    Parameters
    ----------
    model_parameters : tuple
        lr_pos : float
            RL learning rate for positive prediction errors (0..1).
        lr_neg : float
            RL learning rate for non-positive prediction errors (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_forget_base : float
            Baseline WM forgetting rate at set size 3 (0..1).
        wm_forget_size : float
            Increment to WM forgetting per +3 items in set (>=0).
        kappa_sticky : float
            Strength of response repetition bias; higher favors repeating last action (0..5 typical).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_forget_base, wm_forget_size, kappa_sticky = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent forgetting
        wm_forget = wm_forget_base + wm_forget_size * (nS - 3)
        wm_forget = max(0.0, min(1.0, wm_forget))

        last_action = None

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence-based arbitration weight from WM contents
            wm_weight = np.max(W_s)  # in [~1/3, 1]; higher when WM is sharp/one-hot

            # Mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Apply response stickiness bias multiplicatively
            if last_action is not None:
                if a == last_action:
                    # Probability of chosen action gets boosted by exp(kappa); normalize implicit in chosen-prob extraction trick:
                    # For chosen-action probability under softmax with bias on chosen action only, the factor cancels as:
                    # p' = exp(kappa) * p_mix / [exp(kappa)*p_mix + (1-p_mix)] -> implemented via:
                    denom = np.exp(kappa_sticky) * p_mix + (1.0 - p_mix)
                    p_total = (np.exp(kappa_sticky) * p_mix) / np.maximum(denom, tiny)
                else:
                    # Chosen action is not the repeated one; its probability becomes p_mix / [exp(kappa)*p_rep + (1-p_rep)]
                    # But we don't know p_rep here; approximate by inverse transformation using only chosen-prob:
                    # Equivalent effect implemented by reducing chosen-prob relative to when it is the repeated one:
                    denom = p_mix + (1.0 - p_mix) * np.exp(kappa_sticky)
                    p_total = p_mix / np.maximum(denom, tiny)
            else:
                p_total = p_mix

            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe > 0.0 else lr_neg
            q[s][a] += lr_use * pe

            # WM update: store one-hot on reward; otherwise decay
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Renormalize WM row
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p