def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Confidence- vs-Uncertainty arbitration with capacity-limited WM and decay
    - Arbitration: Mix WM and RL policies using a logistic transform of (WM confidence - RL uncertainty).
      WM confidence = max(W_s) - mean(W_s). RL uncertainty = softmax entropy of Q_s.
      A bias term shifts overall tendency to use WM.
    - RL: Single learning rate (lr) and softmax inverse temperature (softmax_beta).
    - WM: Deterministic softmax but its effective sharpness decreases with set size (capacity_scale).
      WM decays toward uniform after each trial (wm_decay).
    - Set-size effect: Larger set size reduces WM effective temperature via capacity_scale.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_decay: Proportion of decay of WM toward uniform per trial (0..1).
    - capacity_scale: Scales how much WM inverse temperature decreases with set size (>0).
                      Effective beta_wm = 50 / (1 + capacity_scale * max(0, nS-3)).
    - mix_bias: Bias toward using WM vs RL in arbitration; positive favors WM.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_decay, capacity_scale, mix_bias = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic baseline
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Capacity-limited WM: reduce effective inverse temperature with set size
            beta_wm_eff = softmax_beta_wm / (1.0 + capacity_scale * max(0, nS - 3))
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration by confidence (WM) vs. uncertainty (RL entropy)
            # WM confidence: peakedness of W_s
            wm_conf = float(np.max(W_s) - np.mean(W_s))
            # RL uncertainty via entropy of RL softmax
            rl_softmax = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_softmax = rl_softmax / max(np.sum(rl_softmax), 1e-12)
            rl_entropy = -np.sum(rl_softmax * np.log(np.clip(rl_softmax, 1e-12, 1.0)))
            # Normalize entropy to [0, log(nA)] -> 0..1
            rl_uncert = rl_entropy / np.log(nA)

            wm_weight = 1.0 / (1.0 + np.exp(-(wm_conf - rl_uncert + mix_bias)))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Simple Hebbian-like maintenance with decay toward uniform
            # Reward strengthens chosen action; no-reward weakly weakens it via decay only.
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0 if r > 0.0 else 0.0
            learn_strength = 1.0 if r > 0.0 else 0.0
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if learn_strength > 0.0:
                # sharpen toward the rewarded action
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot  # conservative consolidation

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Rehearsal-dependent WM precision and variance-based arbitration
    - Arbitration: Weight WM vs. RL by relative precision. WM precision increases with rehearsal
      (repeated visits) and decreases with set size. RL precision is proportional to its inverse temperature.
      wm_weight = beta_wm_eff / (beta_wm_eff + softmax_beta).
    - RL: Standard delta-rule with learning rate lr and softmax temperature softmax_beta.
    - WM: Encodes toward one-hot on reward using wm_alpha; decays to uniform on no-reward.
      Effective inverse temperature of WM is:
        beta_wm_eff = 1 / (wm_noise_base * (nS/3)^size_sensitivity / (1 + rehearse_gain * visits_s))
      Larger set size (nS) increases noise; repeated visits reduce noise (rehearsal benefit).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_alpha: WM learning rate toward target (0..1).
    - wm_noise_base: Baseline WM noise scale (>0); lower means sharper WM policy.
    - size_sensitivity: Exponent for set-size effect on WM noise (>=0).
    - rehearse_gain: Rehearsal benefit per additional visit to a state (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_alpha, wm_noise_base, size_sensitivity, rehearse_gain = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # baseline; we'll replace by beta_wm_eff
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros(nS)  # rehearsal counter per state

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Rehearsal-dependent and set-size-dependent WM precision
            size_factor = (max(nS, 1) / 3.0) ** max(size_sensitivity, 0.0)
            rehearse_factor = 1.0 + max(rehearse_gain, 0.0) * max(visits[s], 0.0)
            # convert to an effective beta (higher rehearse_factor => higher beta)
            beta_wm_eff = 1.0 / max(wm_noise_base * size_factor / rehearse_factor, 1e-6)
            # cap at the baseline high precision
            beta_wm_eff = min(beta_wm_eff, softmax_beta_wm)

            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Precision-based arbitration
            wm_weight = beta_wm_eff / (beta_wm_eff + max(softmax_beta, 1e-6))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward: update toward one-hot; No-reward: relax toward uniform
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

            # Update rehearsal count after processing this trial
            visits[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Meta-learned RL learning rate from WM confidence, with set-size-dependent WM decay
    - Arbitration: Mixture of WM and RL policies weighted by a sigmoid of WM confidence alone
      (no free arbitration parameters to avoid redundancy). Confidence = max(W_s) - mean(W_s).
    - RL: Base learning rate lr_base adapted on each trial using meta_rate * (WM confidence * PE).
      Thus, when WM is confident and outcomes are surprising, RL learns faster.
      lr_eff_t = clip(lr_base + meta_rate * wm_conf * (r - Q_s[a]), [0,1]).
    - WM: Encodes rewarded action with wm_alpha; decays toward uniform with a rate that increases with set size:
      wm_decay_eff = wm_decay + setsize_decay_slope * max(0, nS - 3).

    Parameters (model_parameters):
    - lr_base: Baseline RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_alpha: WM learning rate toward one-hot on reward (0..1).
    - wm_decay: Baseline WM decay toward uniform (0..1).
    - meta_rate: Sensitivity of RL learning rate to WM confidence times prediction error (can be +/-).
    - setsize_decay_slope: Increase in WM decay per item beyond 3 (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_base, softmax_beta, wm_alpha, wm_decay, meta_rate, setsize_decay_slope = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from current W with high precision
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM confidence-based arbitration (bounded, parameter-free)
            wm_conf = float(np.max(W_s) - np.mean(W_s))
            wm_weight = 1.0 / (1.0 + np.exp(-5.0 * (wm_conf - 0.1)))  # fixed slope/center to avoid extra params

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update with meta-learned effective learning rate
            delta = r - Q_s[a]
            lr_eff = lr_base + meta_rate * wm_conf * delta
            lr_eff = min(max(lr_eff, 0.0), 1.0)
            q[s, a] += lr_eff * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Set-size-dependent decay plus reward-based encoding
            wm_decay_eff = wm_decay + max(setsize_decay_slope, 0.0) * max(0, nS - 3)
            wm_decay_eff = min(max(wm_decay_eff, 0.0), 1.0)

            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            # decay toward uniform regardless, with stronger effect for larger set size
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p