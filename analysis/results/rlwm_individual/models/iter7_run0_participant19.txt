def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with prediction-error-gated encoding and set-size-dependent WM precision and decay.

    Mechanism:
    - RL learns state-action values with softmax choice.
    - WM stores near-deterministic bindings for recently and surprisingly rewarded state-action pairs.
    - WM precision (beta) degrades and WM traces decay more when set size is larger (load effect).
    - Arbitration mixes WM and RL with fixed wm_weight; load acts via WM precision and decay.

    Parameters (6 total):
    - lr: RL learning rate (0..1)
    - wm_weight: mixture weight on WM vs RL in the policy (0..1)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_learn: WM learning rate toward rewarded one-hot for the current state (0..1)
    - wm_decay_base: baseline WM decay toward uniform per trial; scaled up with set size (>=0)
    - pe_gate: prediction-error gating threshold; stronger gating (higher) requires larger |PE| to encode (>=0)

    Set-size impact:
    - WM softmax precision reduces with set size: beta_wm_eff = 50 * exp(-wm_decay_base * max(0, nS-3)).
    - WM decay increases with set size: wm_decay = wm_decay_base * (nS / 3).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_learn, wm_decay_base, pe_gate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Load-dependent precision for WM
            beta_wm_eff = softmax_beta_wm * np.exp(-wm_decay_base * max(0, nS - 3))
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Set-size-dependent decay toward uniform for current state
            wm_decay = wm_decay_base * (nS / 3.0)
            wm_decay = np.clip(wm_decay, 0.0, 1.0)

            # Prediction-error-gated encoding: encode if reward and surprise large enough
            if r == 1 and abs(delta) >= pe_gate:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * target

            # Apply decay toward uniform each trial
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * (np.ones(nA) / nA)

            # Renormalize and clip
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with overload-dependent forgetting and action lapses.

    Mechanism:
    - RL learns values and chooses via softmax.
    - WM acts like a limited slot system: when set size exceeds capacity, WM precision drops
      proportionally and bindings are forgotten toward uniform more quickly.
    - WM encoding strengthens the currently rewarded action; policy includes a small lapse noise.

    Parameters (6 total):
    - lr: RL learning rate (0..1)
    - wm_weight: arbitration weight for WM vs RL mixture (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - capacity_k: effective WM capacity in number of items (>=1)
    - wm_store_bonus: governs both the strength of WM encoding after reward and overload-driven forgetting (>=0)
    - noise_eps: lapse probability added to the WM policy (0..0.5)

    Set-size impact:
    - WM precision scales with min(1, capacity_k / nS).
    - Overload-driven forgetting scales with max(0, nS - capacity_k) / nS.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_k, wm_store_bonus, noise_eps = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            cap = max(1.0, capacity_k)
            precision_scale = min(1.0, cap / max(1.0, nS))
            beta_wm_eff = softmax_beta_wm * precision_scale
            p_soft = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = (1 - noise_eps) * p_soft + noise_eps * (1.0 / nA)
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Encode rewarded action toward one-hot with strength wm_store_bonus
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                enc = np.clip(wm_store_bonus, 0.0, 1.0)
                w[s, :] = (1 - enc) * w[s, :] + enc * target

            # Overload-dependent forgetting toward uniform
            overload = max(0.0, nS - cap) / max(1.0, nS)
            f = np.clip(wm_store_bonus * overload, 0.0, 1.0)
            w[s, :] = (1 - f) * w[s, :] + f * (np.ones(nA) / nA)

            # Normalize and clip
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with global choice-kernel bias and load-driven arbitration within WM.

    Mechanism:
    - RL learns Q-values with softmax.
    - WM contains state-specific action distributions (w) but is contaminated by a global choice kernel
      (w_0) that captures recent action tendencies independent of state (perseveration-like).
    - Under higher set size, the WM policy relies more on the global kernel (interference),
      reducing the influence of the state-specific WM trace.

    Parameters (6 total):
    - lr: RL learning rate (0..1)
    - wm_weight: mixture weight of WM vs RL in the policy (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_learn: WM learning rate toward one-hot after reward (0..1)
    - stickiness: learning rate for the global choice kernel w_0 (0..1)
    - setsize_slope: scales how much WM relies on global kernel as set size increases (>=0)

    Set-size impact:
    - Kernel mixing kappa = clip(setsize_slope * (nS - 3) / 3, 0, 1); larger set size -> more kernel reliance.
    - Also apply mild leak of WM toward uniform proportional to kappa.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_learn, stickiness, setsize_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute global choice kernel (average across states) for current time
            kernel = np.mean(w_0, axis=0)  # action-frequency bias across states
            kappa = np.clip(setsize_slope * max(0.0, nS - 3) / 3.0, 0.0, 1.0)
            wm_mix = (1 - kappa) * W_s + kappa * kernel  # interference-weighted WM
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_mix - wm_mix[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Update state-specific WM on reward
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * target

            # Mild load-proportional leak toward uniform
            leak = 0.5 * kappa  # bounded in [0, 0.5]
            w[s, :] = (1 - leak) * w[s, :] + leak * (np.ones(nA) / nA)

            # Update global choice kernel with recency (stickiness) regardless of outcome
            # Move the current state's kernel row toward the chosen action one-hot
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w_0[s, :] = (1 - stickiness) * w_0[s, :] + stickiness * one_hot

            # Normalize and clip for numerical stability
            w[s, :] = np.clip(w[s, :], 1e-12, None); w[s, :] /= np.sum(w[s, :])
            w_0[s, :] = np.clip(w_0[s, :], 1e-12, None); w_0[s, :] /= np.sum(w_0[s, :])

        blocks_log_p += log_p

    return -blocks_log_p