def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with choice-bonus and set-size–dependent interference.

    Idea:
    - Mixture of RL and WM policies.
    - WM precision is degraded by cross-item interference that increases with set size.
    - Each time an action is chosen, WM adds a small "choice-bonus" that can facilitate short-run perseveration within a state.
      This can be adaptive (if rewarded, it compounds with reward-driven storage) or maladaptive (if unrewarded).
    - WM also decays toward a uniform prior; decay accelerates with set size (more interference).

    Parameters
    ----------
    model_parameters : list/tuple of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight of WM policy in final choice.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10).
        kappa_stick : float >= 0
            Additive choice-bonus added to the WM value of the chosen action on each trial for the current state.
        psi_interf : float in [0, 1]
            Set-size–dependent interference strength. Larger psi_interf means stronger reduction of WM precision
            and faster WM decay as set size grows.

    Set-size impact
    ---------------
    - WM precision: beta_wm_eff = 50 * (1 - psi_interf * (nS-3)/max(1,nS)), i.e., lower at set size 6 when psi_interf>0.
    - WM decay: decay_wm = lr * (1 + psi_interf * (nS-3)/max(1,nS)), i.e., faster leak for larger sets.
    """
    lr, wm_weight, softmax_beta, kappa_stick, psi_interf = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Set-size–dependent WM precision (interference lowers precision)
            interf = psi_interf * max(0.0, (float(nS) - 3.0) / max(1.0, float(nS)))
            beta_wm_eff = max(1e-6, softmax_beta_wm * (1.0 - interf))
            # Softmax WM policy
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Size-dependent decay (more interference at larger set size)
            decay_wm = lr * (1.0 + interf)
            decay_wm = min(max(decay_wm, 0.0), 1.0)
            w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]

            # Reward-sensitive storage: push chosen action up by reward, plus a choice-bonus
            w[s, a] += (0.5 + 0.5 * r) * wm_weight + kappa_stick

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with size-dependent lapse and reward-weighted WM updates.

    Idea:
    - Choices are a convex mixture of RL and WM.
    - WM retrieval suffers a lapse that increases with set size via a logistic transform.
    - WM is maintained as a probability-like vector that is pulled toward the chosen action after feedback,
      with more pull when reward is present.

    Parameters
    ----------
    model_parameters : list/tuple of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight of WM policy in final choice; also scales WM update strength.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10).
        lapse_base : float (real)
            Baseline log-odds of a WM retrieval lapse at set size 3.
        size_slope : float (real)
            Additional log-odds of lapse per extra memory item beyond 3 (i.e., increases at set size 6 if positive).

    Set-size impact
    ---------------
    - WM lapse: epsilon_wm = sigmoid(lapse_base + size_slope * (nS - 3)).
    - WM updates: same form across set sizes, but their behavioral impact is reduced at larger set sizes due to higher lapse.
    """
    lr, wm_weight, softmax_beta, lapse_base, size_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Lapse grows with set size via logistic link
            logit_eps = lapse_base + size_slope * max(0.0, float(nS) - 3.0)
            epsilon_wm = 1.0 / (1.0 + np.exp(-logit_eps))
            # Deterministic WM softmax
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Lapse-augmented WM policy
            p_wm = (1.0 - epsilon_wm) * p_wm_soft + epsilon_wm * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Gentle leak toward prior (scaled by lr for parsimony)
            leak = min(max(lr, 0.0), 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]
            # Reward-weighted pull toward chosen action
            # Interpreting w as a nonnegative preference vector; increase chosen entry, normalize softly
            w[s, a] += wm_weight * (0.25 + 0.75 * r)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Gated WM with size-dependent retrieval and leak.

    Idea:
    - WM is updated strongly when the gate is open; the gate opens more on rewarded trials,
      but overall retrieval success declines with set size.
    - WM retrieval probability (gamma) is controlled by a logistic gate with bias and a penalty for larger set sizes.
    - WM leaks toward uniform with a baseline leak that is amplified by set size.

    Parameters
    ----------
    model_parameters : list/tuple of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight of WM policy in final choice.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10).
        leak_base : float in [0,1]
            Baseline WM leak toward uniform on each visit to a state.
        gate_bias : float (real)
            Baseline log-odds for WM retrieval; higher values increase retrieval success. Larger set sizes decrease it.

    Set-size impact
    ---------------
    - Retrieval success: gamma = sigmoid(gate_bias - (nS-3)), i.e., reduced at set size 6.
    - Leak: leak_eff = leak_base * (1 + (nS-3)/max(1,nS)), i.e., stronger leak at larger set sizes.
    """
    lr, wm_weight, softmax_beta, leak_base, gate_bias = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Retrieval success declines with set size
            size_pen = max(0.0, float(nS) - 3.0)
            gamma = 1.0 / (1.0 + np.exp(-(gate_bias - size_pen)))
            # WM softmax
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Retrieval mixture: with prob gamma use WM, else uniform
            p_wm = gamma * p_wm_soft + (1.0 - gamma) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Size-amplified leak
            leak_eff = leak_base * (1.0 + size_pen / max(1.0, float(nS)))
            leak_eff = min(max(leak_eff, 0.0), 1.0)
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # Gated storage: reward opens the gate more strongly
            gate_open = 1.0 / (1.0 + np.exp(-(gate_bias + 2.0 * (r - 0.5))))
            w[s, a] += gate_open * (0.5 + 0.5 * r) * wm_weight

        blocks_log_p += log_p

    return -blocks_log_p