def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with error-contingent WM gating and set-size attenuation of WM influence.

    Idea:
    - RL: standard Rescorla-Wagner with softmax.
    - WM: a one-shot associative store for the current state->action that updates in expectation:
      on rewarded trials, store the chosen action with probability p_rew; on error trials, store
      the chosen action with probability p_err (to capture error-driven rehearsal/strategy).
      We implement this probabilistically in expectation (no sampling).
    - Arbitration: mixture of RL and WM policies, with WM weight attenuated by set size using a
      divisive factor 1 + gamma * (nS - 3). Larger sets reduce WM contribution.

    Parameters (6 total):
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], base weight of WM policy before set-size attenuation.
    - softmax_beta: >0, RL inverse temperature (internally scaled by 10).
    - p_rew: [0,1], probability that WM stores the chosen action after reward.
    - p_err: [0,1], probability that WM stores the chosen action after no reward.
    - gamma: >=0, set-size attenuation factor; larger gamma reduces WM influence more for nS=6.

    Set size effects:
    - WM mixture weight is divided by 1 + gamma*(nS-3), reducing WM impact for larger sets.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, p_rew, p_err, gamma = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size attenuated WM weight
        wm_mix = wm_weight / (1.0 + max(gamma, 0.0) * max(nS - 3, 0))
        wm_mix = float(np.clip(wm_mix, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Expected update: w[s] moves toward one-hot(a) with probability depending on r
            p_update = p_rew if r > 0.0 else p_err
            p_update = float(np.clip(p_update, 0.0, 1.0))
            # Move expected probability mass:
            w[s, :] = (1.0 - p_update) * w[s, :]
            w[s, a] += p_update

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration, RL uncertainty learning, and set-size reduction
    of WM precision.

    Idea:
    - RL: Rescorla-Wagner values with softmax; we track per-state RL uncertainty U_s (running
      variance proxy of prediction error magnitude) that is updated online.
    - Arbitration: WM weight increases with RL uncertainty: wm_mix = clip(wm_weight0 + u_sens * U_s, 0,1).
    - WM: associative store updated to the last chosen action on rewards (one-shot overwrite).
    - Set size effect on WM: WM precision (inverse temperature) decreases with set size using
      beta_wm = 50 / (1 + gamma_ss*(nS - 3)).

    Parameters (6 total):
    - lr: [0,1], RL learning rate.
    - wm_weight0: [0,1], base WM mixture weight before uncertainty modulation.
    - softmax_beta: >0, RL inverse temperature (internally scaled by 10).
    - kappa_u: [0,1], learning rate for RL uncertainty tracker.
    - u_sens: >=0, sensitivity of WM mixture to RL uncertainty (higher -> more WM when uncertain).
    - gamma_ss: >=0, set-size penalty on WM precision; larger -> less precise WM for nS=6.

    Set size effects:
    - WM precision decreases with set size: beta_wm = 50 / (1 + gamma_ss*(nS-3)).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, kappa_u, u_sens, gamma_ss = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm_base = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state uncertainty tracker (initialized moderately)
        U = 0.5 * np.ones(nS)

        # Set-size dependent WM precision
        beta_wm = softmax_beta_wm_base / (1.0 + max(gamma_ss, 0.0) * max(nS - 3, 0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with set-size-reduced precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Uncertainty-modulated WM mixture
            wm_mix = wm_weight0 + max(u_sens, 0.0) * U[s]
            wm_mix = float(np.clip(wm_mix, 0.0, 1.0))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Uncertainty update (running estimate of absolute PE)
            U[s] = (1.0 - kappa_u) * U[s] + kappa_u * abs(pe)

            # WM update: on reward, store chosen action as a one-shot overwrite
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference-by-ISI decay of WM, state-specific win-stay bias in RL,
    and set-size scaling of WM mixture.

    Idea:
    - RL: Rescorla-Wagner; policy includes a state-specific bias toward the last rewarded action
      in that state (win-stay bias added as a small boost to that action).
    - WM: perfect write on rewarded trials, no overwrite on errors; memory decays as a function
      of time since last seen (ISI) for that state: w[s] <- blend toward uniform by exp(-inter_decay * ISI).
    - Arbitration: WM mixture weight attenuated by set size via 1 + ss_weight*(nS-3).

    Parameters (6 total):
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], base WM mixture weight before set-size attenuation.
    - softmax_beta: >0, RL inverse temperature (internally scaled by 10).
    - inter_decay: >=0, interference/decay rate per trial of ISI; larger -> faster WM decay.
    - ws_bias: >=0, additive win-stay bias strength for the last rewarded action in state.
    - ss_weight: >=0, set-size attenuation for WM mixture; larger reduces WM at nS=6.

    Set size effects:
    - WM mixture weight is divided by 1 + ss_weight*(nS-3), reducing WM impact in larger sets.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, inter_decay, ws_bias, ss_weight = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last-seen time and last rewarded action per state
        last_seen = -1 * np.ones(nS, dtype=int)
        last_rew_action = -1 * np.ones(nS, dtype=int)

        # Set-size attenuated WM weight
        wm_mix_base = wm_weight / (1.0 + max(ss_weight, 0.0) * max(nS - 3, 0))
        wm_mix_base = float(np.clip(wm_mix_base, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Apply WM decay based on ISI for this state before computing policy
            if last_seen[s] < 0:
                isi = 0
            else:
                isi = t - last_seen[s]
            if isi > 0:
                # Exponential decay toward uniform
                decay_factor = np.exp(-max(inter_decay, 0.0) * float(isi))
                w[s, :] = decay_factor * w[s, :] + (1.0 - decay_factor) * w_0[s, :]

            Q_s = q[s, :]

            # Add state-specific win-stay bias to RL Q for the last rewarded action in this state
            if last_rew_action[s] >= 0:
                Q_eff = Q_s.copy()
                Q_eff[last_rew_action[s]] += ws_bias / max(softmax_beta, 1e-8)
            else:
                Q_eff = Q_s

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store perfect mapping on reward; no overwrite on error
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
                last_rew_action[s] = a

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p