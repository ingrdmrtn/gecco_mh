def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, recency-weighted WM with recall-noise.
    - Policy: convex mixture of RL softmax and WM softmax.
    - WM weight is the product of an item-capacity term and a state-wise confidence term.
      Capacity term scales as min(1, C / set_size). Confidence term scales with how peaked
      the WM distribution is for the current state (relative to uniform), and is reduced by wm_noise.
    - WM dynamics: reward-gated recency update toward a one-hot cache; otherwise leak toward uniform.

    Parameters (tuple):
    - lr_rl: learning rate for RL value updates (0..1).
    - beta_rl: inverse temperature for RL softmax; internally scaled x10.
    - C: WM capacity in slots (>=0). Determines set-size effect via min(1, C / nS).
    - wm_noise: recall noise in WM arbitration (0..1). Higher values reduce WM weight.
    - recency: recency/overwrite strength for WM when rewarded (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_rl, beta_rl, C, wm_noise, recency = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        cap_term = min(1.0, float(C) / max(1.0, float(nS)))
        cap_term = np.clip(cap_term, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence term: how peaked WM is relative to uniform
            peak = np.max(W_s)
            uniform = 1.0 / nA
            if peak <= uniform:
                conf = 0.0
            else:
                conf = (peak - uniform) / (1.0 - uniform)
            conf = np.clip(conf, 0.0, 1.0)

            # Arbitration weight: capacity-limited and noisy recall
            wm_weight = cap_term * (1.0 - wm_noise) * conf
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM update: leak toward uniform, then reward-driven overwrite
            # Leak when no reward (or small overwrite regardless)
            if r <= 0.0:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            # Reward: recency overwrite toward one-hot cache
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - recency) * w[s, :] + recency * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM with time-based recall and set-size interference.
    - Policy: convex mixture of RL softmax and WM softmax.
    - Arbitration: WM weight decays with time since last reward for that state
      and with larger set sizes (item-based recall 3/nS). A prior term sets baseline WM usage.
    - WM dynamics: reward-driven storage to one-hot; storing a new association causes
      global interference (other states drift toward uniform) with strength xi_interf.

    Parameters (tuple):
    - alpha_pos: RL learning rate for positive prediction errors (0..1).
    - alpha_neg: RL learning rate for negative prediction errors (0..1).
    - beta_rl: inverse temperature for RL softmax; internally scaled x10.
    - tau_wm: time constant for WM recall retention (in trials; >0). Larger => slower decay.
    - xi_interf: global interference strength applied to other states upon successful storage (>=0).
                 Effective interference per other state is xi_interf / nS.
    - wm_prior: baseline WM arbitration weight (0..1), before time and set-size modulation.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_rl, tau_wm, xi_interf, wm_prior = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last rewarded time for each state to drive retention
        last_reward_time = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Time since last rewarded exposure for this state
            if last_reward_time[s] < 0:
                dt = np.inf
            else:
                dt = max(0, t - last_reward_time[s])

            # Retention term: exp(-dt / tau)
            if np.isinf(dt):
                time_term = 0.0
            else:
                time_term = np.exp(-float(dt) / max(1e-6, float(tau_wm)))
            time_term = np.clip(time_term, 0.0, 1.0)

            # Set-size penalty via item probability 3/nS (bounded by 1)
            ss_term = min(1.0, 3.0 / max(1.0, float(nS)))

            wm_weight = wm_prior * time_term * ss_term
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            rpe = r - Q_s[a]
            alpha = alpha_pos if rpe >= 0.0 else alpha_neg
            q[s, a] += alpha * rpe

            # WM update: on reward, store and induce global interference
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * onehot  # mild consolidation
                # Interference across other states scales with set size
                if nS > 1:
                    eps = float(xi_interf) / float(nS)
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        w[s2, :] = (1.0 - eps) * w[s2, :] + eps * w_0[s2, :]
                last_reward_time[s] = t
            else:
                # Without reward, small passive drift toward uniform
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration, set-size–scaled WM precision, and set-size–scaled leak.
    - Policy: convex mixture of RL softmax and WM softmax.
      The mixture weight is a logistic function of the entropy difference (RL minus WM).
      When WM is more certain (lower entropy), WM weight increases.
    - WM precision increases for small set sizes via wm_beta_scale; WM leak increases with set size.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - beta_rl: inverse temperature for RL softmax; internally scaled x10.
    - wm_beta_scale: scales WM precision as wm_beta = wm_beta_scale * (7 - nS); higher for smaller nS.
    - k_logit: gain of entropy-based arbitration (>=0). Larger values make sharper switching.
    - bias_logit: baseline bias in favor of WM (>0 favors WM; <0 favors RL).
    - wm_leak_ss: base WM leak factor per visit scaled by set size: leak = wm_leak_ss * (nS / 6).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_beta_scale, k_logit, bias_logit, wm_leak_ss = model_parameters
    softmax_beta = beta_rl * 10.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM precision and leak
        wm_beta = max(1e-3, wm_beta_scale * float(7 - nS))  # 7-nS: 4 for nS=3, 1 for nS=6
        softmax_beta_wm = 10.0 * wm_beta  # make WM comparatively sharp but set-size dependent
        wm_decay = np.clip(wm_leak_ss * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Full distributions to compute entropies
            # RL distribution
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl) / np.sum(np.exp(logits_rl))
            # WM distribution
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            probs_wm = np.exp(logits_wm) / np.sum(np.exp(logits_wm))

            # Entropy H = -sum p log p
            H_rl = -np.sum(probs_rl * (np.log(probs_rl + 1e-12)))
            H_wm = -np.sum(probs_wm * (np.log(probs_wm + 1e-12)))

            # Entropy-based arbitration
            x = bias_logit + k_logit * (H_rl - H_wm)
            wm_weight = 1.0 / (1.0 + np.exp(-x))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform every visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-gated strengthening toward chosen action
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Use same lr to consolidate memory for parsimony
                w[s, :] = (1.0 - lr) * w[s, :] + lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p