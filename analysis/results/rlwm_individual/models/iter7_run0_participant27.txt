def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and set-size–scaled WM leak.

    Mechanism
    ---------
    - RL: single learning rate; softmax choice (beta scaled by 10; per template).
    - WM: fast supervised mapping toward one-hot after reward, toward uniform after no reward.
          WM leaks toward uniform on each visit; the leak grows with set size.
    - Arbitration: WM weight is dynamic based on WM confidence (low entropy/high margin implies
      higher weight). Confidence is transformed via a sigmoid with gain, and scaled by set size.

    Set-size effects
    ----------------
    - WM leak increases with set size via leak_eff = leak0 * (nS/3)^nu_size.
    - Confidence input is scaled by (3/nS)^nu_size, making smaller sets grant more WM influence.

    Parameters
    ----------
    model_parameters : tuple
        (lr, w0_bias, softmax_beta, kappa_conf, leak0, nu_size)
        - lr: RL learning rate for Q updates (0..1).
        - w0_bias: baseline bias for WM mixture before confidence (real-valued).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - kappa_conf: gain on WM confidence (difference between best and second-best WM values).
        - leak0: base WM leak per visit (0..1).
        - nu_size: exponent controlling set-size scaling for both leak and confidence (>=0).
    """
    lr, w0_bias, softmax_beta, kappa_conf, leak0, nu_size = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic as per template
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scalings
        leak_eff = max(leak0, 0.0) * (float(nS) / 3.0) ** max(nu_size, 0.0)
        conf_scale = (3.0 / float(nS)) ** max(nu_size, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (kept consistent with template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax on WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence: margin between top-1 and top-2 WM weights
            sorted_W = np.sort(W_s)[::-1]
            margin = sorted_W[0] - sorted_W[1] if nA >= 2 else sorted_W[0]
            # Dynamic mixture via sigmoid(w0 + kappa * margin * conf_scale)
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-(w0_bias + kappa_conf * margin * conf_scale)))

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: supervised mapping with set-size–scaled leak
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            # Move toward target
            w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            # Leak toward uniform
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # Normalize for numerical stability
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–scaled WM learning rate and action stickiness in WM.

    Mechanism
    ---------
    - RL: single learning rate; softmax action selection.
    - WM: error-driven mapping toward one-hot after reward; after no reward, partial
          unlearning toward uniform. WM learning rate scales with set size.
    - Stickiness: WM includes a recency bias favoring the last chosen action in each state.
    - Arbitration: fixed mixture weight (wm_weight).

    Set-size effects
    ----------------
    - WM learning rate alpha_wm = alpha0 * (3/nS)^rho, making WM learn faster in small sets.
      This captures reduced interference in low load.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, alpha0_wm, rho_size, kappa_stick)
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight on WM policy (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - alpha0_wm: base WM learning rate (>0).
        - rho_size: exponent for set-size scaling of WM learning rate (>=0).
        - kappa_stick: strength of action stickiness in WM policy (>=0).
    """
    lr, wm_weight, softmax_beta, alpha0_wm, rho_size, kappa_stick = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # last chosen action per state

        alpha_wm = max(alpha0_wm, 0.0) * (3.0 / float(nS)) ** max(rho_size, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # Add stickiness bias to WM policy for the last action in this state
            if last_action[s] >= 0:
                W_s[last_action[s]] += max(kappa_stick, 0.0)

            # RL policy (template-consistent)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with stickiness-adjusted W
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward -> move toward one-hot; no-reward -> move toward uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target

            # Track last action for stickiness on the next visit
            last_action[s] = a

            # Normalize for numerical stability
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with prediction-error–gated arbitration and set-size–scaled PE sensitivity.

    Mechanism
    ---------
    - RL: standard Q-learning and softmax choice.
    - WM: Hebbian-like fast mapping toward the chosen action, gated by outcome (stronger with reward).
    - Arbitration: trial-by-trial mixture weight updated by a delta-rule toward WM when
      recent unsigned RL prediction error is low and reward is high; otherwise shifts to RL.

    Set-size effects
    ----------------
    - PE sensitivity scales as (3/nS)^xi_size: in small sets, the system trusts WM more readily
      for a given PE reduction; in large sets, arbitration is more conservative.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight_init, softmax_beta, alpha_wm, pe_gain)
        - lr: RL learning rate (0..1).
        - wm_weight_init: initial mixture weight at the start of each block (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - alpha_wm: WM learning rate toward chosen action; gated by reward (0..1).
        - pe_gain: base sensitivity of arbitration to unsigned PE; scaled by set size (>0).
                   Effective gain = pe_gain * (3/nS)^xi_size, where xi_size is set via pe_gain's fraction.
    Notes
    -----
    To keep parameter count ≤5, we encode set-size scaling by splitting pe_gain into two parts:
    pe_gain = g_base + g_size, where g_size in (0,1) is interpreted as xi_size and g_base >= 0 as base gain.
    Specifically, we define:
        g_base = max(pe_gain, 0) / 2
        xi_size = min(max(pe_gain, 0) / 2, 4)
    This ensures both are used and bounded sensibly without extra parameters.
    """
    lr, wm_weight_init, softmax_beta, alpha_wm, pe_gain = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    # Derive base gain and size exponent from pe_gain (shared parameter used in two places)
    g_base = max(pe_gain, 0.0) / 2.0
    xi_size = min(max(pe_gain, 0.0) / 2.0, 4.0)  # cap exponent to avoid extremes

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize dynamic mixture at block start
        wm_weight_dyn = 1.0 / (1.0 + np.exp(-5.0 * (wm_weight_init - 0.5)))  # squash to (0,1) smoothly

        # Set-size scaling of PE sensitivity
        pe_sens = g_base * (3.0 / float(nS)) ** xi_size

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: PE-gated update of wm_weight_dyn
            # Compute unsigned prediction error from RL
            pe = abs(r - Q_s[a])
            # Target mixture favors WM when reward is high and PE is low
            target_wm = r * np.exp(-pe_sens * pe)
            # Update mixture toward target
            wm_weight_dyn = wm_weight_dyn + (1.0 - wm_weight_dyn) * target_wm - wm_weight_dyn * (1.0 - target_wm)
            # Clamp to (0,1)
            wm_weight_dyn = min(max(wm_weight_dyn, 1e-6), 1.0 - 1e-6)

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: Hebbian-like toward chosen action; reward gates the step size
            eta = max(alpha_wm, 0.0) * (0.5 + 0.5 * r)  # half-size on no-reward, full-size on reward
            target = (1.0 - r) * w_0[s, :] + r * np.eye(3)[a] if nA == 3 else ((1.0 - r) * w_0[s, :])
            if r > 0.0:
                tgt = np.zeros(nA)
                tgt[a] = 1.0
            else:
                tgt = w_0[s, :]
            w[s, :] = (1.0 - eta) * w[s, :] + eta * tgt

            # Normalize for numerical stability
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p