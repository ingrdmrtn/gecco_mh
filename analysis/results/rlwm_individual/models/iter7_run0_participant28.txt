def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with dynamic arbitration by prediction error and load-induced WM noise.

    Idea:
    - RL: standard delta rule.
    - WM: stores the last rewarded action per state; retrieval is noisy and gets noisier with set size.
    - Arbitration: the WM mixture weight is adjusted trial-by-trial: larger recent absolute prediction
      errors and larger set sizes reduce reliance on WM.

    Parameters (5):
    - lr: RL learning rate (0..1). Updates Q-values from reward prediction errors.
    - wm_base: Baseline WM mixture weight (0..1) when errors are small and load is low.
    - softmax_beta: Base RL inverse temperature; internally scaled by 10 for expressivity.
    - pe_sensitivity: How strongly absolute prediction error down-weights WM (>=0).
    - wm_load_slope: Load sensitivity. Increases WM retrieval noise and further down-weights WM as set size grows.

    Set-size impacts:
    - WM retrieval noise increases with set size via wm_load_slope, mixing WM softmax with the uniform policy.
    - Arbitration also penalizes WM more as set size increases, compounding the effect of load.
    """
    lr, wm_base, softmax_beta, pe_sensitivity, wm_load_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load terms
        load_term = max(0.0, float(nS) - 3.0)

        log_p = 0.0
        # Initialize arbitration baseline in logit space
        def logit(p):
            p = min(max(p, 1e-6), 1.0 - 1e-6)
            return np.log(p) - np.log(1.0 - p)

        base_logit = logit(wm_base)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-induced noise toward uniform
            W_s = w[s, :].copy()
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Retrieval noise increases with set size via a sigmoid of load
            eta = 1.0 / (1.0 + np.exp(-wm_load_slope * load_term))  # 0 at load=0, increases with load
            p_wm = (1.0 - eta) * p_wm_det + eta * (1.0 / nA)

            # Dynamic arbitration weight: down-weight WM with abs prediction error and load
            # Use most recent Q_s for delta
            delta = r - Q_s[a]
            wm_logit = base_logit - pe_sensitivity * abs(delta) - wm_load_slope * load_term
            wm_weight_t = 1.0 / (1.0 + np.exp(-wm_logit))

            # Mixture policy
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM update: on rewarded trials, store one-shot; otherwise leave memory as-is
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with within-state eligibility shrinkage + WM with load-dependent decay.

    Idea:
    - RL: before updating, shrink all action values in the visited state toward zero baseline,
      then apply a delta update to the chosen action. The shrinkage (trace_lambda) mimics an
      eligibility-like redistribution: it weakly erases old values and increases flexibility.
    - WM: one-shot storage of the last rewarded action per state, but memory decays each trial
      toward uniform, and the decay rate increases with set size.

    Parameters (5):
    - lr: RL learning rate (0..1).
    - trace_lambda: Within-state shrinkage factor per visit (0..1). Larger => more forgetting of all actions
      in the visited state before applying the RL update.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight: Mixture weight for WM policy (0..1).
    - wm_decay_load: Base load-sensitivity of WM decay: effective per-trial decay is
      1 - exp(-wm_decay_load * max(1, nS-2)). Larger set sizes -> faster WM decay.

    Set-size impacts:
    - WM decays faster for larger set sizes, reducing its contribution unless reinforced.
    - RL component is indirectly affected since faster WM decay shifts more weight to RL when WM becomes less informative.
    """
    lr, trace_lambda, softmax_beta, wm_weight, wm_decay_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM decay per trial increases with set size
        load_scale = max(1.0, float(nS) - 2.0)  # =1 for nS=3; >1 for nS>=4
        wm_decay = 1.0 - np.exp(-wm_decay_load * load_scale)
        wm_decay = min(max(wm_decay, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with within-state shrinkage (eligibility-like)
            # Shrink all action values in the visited state toward zero baseline (or uniform baseline).
            q[s, :] = (1.0 - trace_lambda) * q[s, :]
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update and decay
            # Rewarded trials: overwrite with one-hot; regardless, apply decay toward uniform.
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Global decay toward uniform for all states after each trial to reflect ongoing interference
            w = (1.0 - wm_decay) * w + wm_decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with global-action interference and surprise-adjusted RL temperature.

    Idea:
    - RL: standard delta rule, but the effective inverse temperature adapts to recent surprise:
      after surprising outcomes (large absolute PE), the RL policy becomes more exploratory.
      The strength of this adaptation scales with set size (harder blocks -> more adaptation).
    - WM: one-shot storage of last rewarded action per state. At retrieval, there is interference
      from a global "last rewarded action" trace that blends into the state-specific WM content.
      The interference weight grows with set size (more load -> more global confusion across states).

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in the final choice (0..1).
    - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
    - confus_slope: Load sensitivity of WM global interference. Interference weight is
      phi = 1 - exp(-confus_slope * max(0, nS-3)).
    - temp_meta: Strength of surprise-driven reduction of RL temperature; effective beta is
      beta_eff = beta / (1 + temp_meta * surprise_avg * (1 + (nS-3))).

    Set-size impacts:
    - WM: interference from the last globally rewarded action increases with set size, diluting
      state-specific WM.
    - RL: the surprise-driven temperature adaptation is amplified by set size, producing more exploration
      under higher load when outcomes are surprising.
    """
    lr, wm_weight, softmax_beta, confus_slope, temp_meta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global last rewarded action trace (one-hot distribution over actions)
        g = (1.0 / nA) * np.ones(nA)
        # Interference weight increases with set size
        phi = 1.0 - np.exp(-confus_slope * max(0.0, float(nS) - 3.0))
        phi = min(max(phi, 0.0), 1.0)

        # Track running surprise via an exponential average of |delta|
        surprise_avg = 0.0
        surprise_alpha = 0.2  # fixed smoothing for stability

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # RL temperature adaptation by surprise and load
            beta_eff = softmax_beta / (1.0 + temp_meta * surprise_avg * (1.0 + max(0.0, float(nS) - 3.0)))
            beta_eff = max(beta_eff, 1e-6)

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM retrieval with global-action interference
            W_s = w[s, :].copy()
            W_blend = (1.0 - phi) * W_s + phi * g
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_blend - W_blend[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update running surprise
            surprise_avg = (1.0 - surprise_alpha) * surprise_avg + surprise_alpha * abs(delta)

            # WM update: on reward, store one-hot for the state and update global trace
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                g = one_hot  # replace global trace with the most recent rewarded action

        blocks_log_p += log_p

    return -blocks_log_p