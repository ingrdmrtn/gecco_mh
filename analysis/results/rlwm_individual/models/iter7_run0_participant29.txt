def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    PE-gated WM encoding + load-modulated WM arbitration.

    Idea:
    - RL learns standard Q-values with a single learning rate.
    - WM stores the most recently reinforced action for each state, decaying toward uniform.
    - WM encoding is gated by the signed prediction error (PE): large positive PEs trigger
      stronger WM writing, small/negative PEs trigger weaker writing.
    - Arbitration weight on WM is reduced under higher load (set size 6) via a load factor.

    Parameters (model_parameters):
    - lr:                RL learning rate (0..1)
    - wm_weight_base:    Base mixture weight of WM policy before load penalties (0..1)
    - softmax_beta:      Base RL inverse temperature (scaled internally by *10)
    - crowd_k:           Load sensitivity of WM arbitration; effective WM weight is
                         wm_weight_base / (1 + crowd_k*(nS-3)) (nS in {3,6})
    - pe_gate:           PE gating steepness for WM write; write_strength = sigmoid(pe_gate*(r - Q))
    - wm_decay:          Per-visit decay of WM toward uniform (0..1)

    Set-size impact:
    - WM mixture weight is reduced when set size increases via crowd_k.
    """
    lr, wm_weight_base, softmax_beta, crowd_k, pe_gate, wm_decay = model_parameters

    softmax_beta *= 10.0  # higher leverage on RL
    softmax_beta_wm = 50.0  # very deterministic WM
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-modulated WM weight (fixed within block)
        wm_weight = wm_weight_base / (1.0 + max(0.0, crowd_k) * max(0, nS - 3))
        wm_weight = min(max(wm_weight, 0.0), 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM table
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # PE-gated WM write on positive outcomes (stronger with larger positive PE)
            # Write strength in [0,1] via sigmoid of signed PE
            write_strength = 1.0 / (1.0 + np.exp(-pe_gate * delta))
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    State-specific arbitration via learned WM reliability + RL.

    Idea:
    - RL updates Q-values with a single learning rate.
    - WM stores recently rewarded actions per state with decay.
    - A state-specific arbitration variable m_s (WM reliability) is learned online from outcomes:
      m_s is updated toward observed reward and used as the WM mixture weight for that state.
    - Under higher load, the effective WM weight is reduced by a load penalty.

    Parameters (model_parameters):
    - lr:               RL learning rate (0..1)
    - softmax_beta:     Base RL inverse temperature (scaled internally by *10)
    - wm_weight_init:   Initial WM reliability m_s for all states at block start (0..1)
    - wm_reliab_lr:     Learning rate for WM reliability m_s update toward reward (0..1)
    - wm_decay:         Per-visit WM decay toward uniform (0..1)
    - load_slope:       Load penalty factor on m_s: m_eff = m_s / (1 + load_slope*(nS-3))

    Set-size impact:
    - Arbitration weight m_s is down-weighted under higher set size via load_slope.
    """
    lr, softmax_beta, wm_weight_init, wm_reliab_lr, wm_decay, load_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific WM reliability
        m = np.full(nS, float(wm_weight_init))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-penalized arbitration for this state
            wm_weight_eff = m[s] / (1.0 + max(0.0, load_slope) * max(0, nS - 3))
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM write on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong overwrite on reward to favor deterministic WM mapping
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

            # Update WM reliability m_s toward experienced reward
            # Interpreted as "WM worked when outcome is good"; keeps m in [0,1]
            m[s] = (1.0 - wm_reliab_lr) * m[s] + wm_reliab_lr * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Advantage RL with baseline + load-amplified WM interference.

    Idea:
    - RL uses an advantage formulation: Q-values learn the advantage over a running
      baseline b_s for each state (i.e., Q approximates r - b_s). This can stabilize
      learning under variable reward expectations.
    - WM stores last rewarded action for each state with decay toward uniform.
    - WM suffers stronger interference under larger set sizes: the decay parameter
      is amplified as a deterministic function of set size, mimicking interference.

    Parameters (model_parameters):
    - lr:                Advantage RL learning rate (0..1)
    - wm_weight:         Fixed WM mixture weight (0..1)
    - softmax_beta:      Base RL inverse temperature (scaled internally by *10)
    - base_lr:           Learning rate for the state baseline b_s (0..1)
    - interfer_phi:      Load-driven amplification of WM decay:
                         wm_decay_eff = 1 - (1 - wm_decay)^(1 + interfer_phi*(nS-3))
    - wm_decay:          Base WM decay toward uniform (0..1)

    Set-size impact:
    - WM decay increases with set size via interfer_phi, which weakens WM policy in larger sets.
    """
    lr, wm_weight, softmax_beta, base_lr, interfer_phi, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective WM decay under load (deterministic interference)
        load_factor = 1.0 + max(0.0, interfer_phi) * max(0, nS - 3)
        wm_decay_eff = 1.0 - (1.0 - wm_decay) ** load_factor
        wm_decay_eff = min(max(wm_decay_eff, 0.0), 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))  # Advantage values per action
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        b_state = 0.5 * np.ones(nS)  # Baseline initialized to mid-reward

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Advantage Q and baseline
            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (uses advantage values directly)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # Advantage RL update:
            # baseline update
            b_state[s] += base_lr * (r - b_state[s])
            # advantage delta
            adv = (r - b_state[s]) - Q_s[a]
            q[s][a] += lr * adv

            # WM decay with load-amplified interference
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM write on reward (deterministic overwrite toward chosen action)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p