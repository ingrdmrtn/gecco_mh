def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + decaying associative WM; WM weight scales with set size via a power law.

    Mechanism
    - RL: Q-learning with replacing eligibility traces for within-block credit assignment.
    - WM: per-state associative probabilities over actions (w) that get strengthened on rewards and
      decay toward uniform otherwise.
    - Arbitration: fixed base WM weight scaled by set size via a power-law (smaller sets -> stronger WM).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Base mixture weight on WM policy (scaled by set size).
        softmax_beta : float >= 0
            Inverse temperature for RL softmax (scaled internally).
        trace_lambda : float in [0,1]
            Eligibility trace decay (replacing traces).
        wm_eta : float in (0,1)
            WM association update step size.
        ss_exponent : float >= 0
            Power-law exponent for set-size scaling of WM weight:
            wm_weight_eff = wm_weight * (3 / set_size) ** ss_exponent.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, trace_lambda, wm_eta, ss_exponent = model_parameters

    softmax_beta *= 10.0  # RL temperature upscaling
    softmax_beta_wm = 50.0  # near-deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action (softmax trick)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            # near-deterministic softmax over current WM association vector
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: set-size scaled WM weight
            set_size_t = max(3, int(block_set_sizes[t]))
            wm_weight_eff = np.clip(wm_weight * (3.0 / float(set_size_t)) ** max(0.0, ss_exponent), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with replacing eligibility traces
            pe = r - Q_s[a]
            e *= trace_lambda
            e[s, :] = 0.0
            e[s, a] = 1.0
            q += lr * pe * e

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward strengthens chosen action; no-reward decays toward uniform.
            if r >= 0.5:
                # Move w[s] toward a one-hot on action a
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            else:
                # Decay toward uniform baseline
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

            # Normalize for safety
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-based arbitration: WM vs RL weighted by relative confidence and set size.
    WM is a directly queried probabilistic associative store updated with a learning rate.

    Mechanism
    - RL: standard softmax Q-learning.
    - WM: per-state probability distribution over actions (w), updated by wm_lr toward the
      one-hot chosen action on rewards, and pushed away (anti-Hebbian) on non-rewards.
    - Arbitration: WM weight is a sigmoid of a bias plus the difference (WM confidence - RL uncertainty)
      minus a set-size penalty.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_bias : float
            Bias term in arbitration (positive favors WM).
        softmax_beta : float >= 0
            Inverse temperature for RL softmax (scaled internally).
        k_unc : float >= 0
            Gain on (WM confidence - RL uncertainty).
        ss_penalty : float >= 0
            Penalty multiplier per (set_size - 3) reducing WM reliance.
        wm_lr : float in (0,1)
            Learning rate for WM associative updates.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_bias, softmax_beta, k_unc, ss_penalty, wm_lr = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic readout of w row
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute arbitration signal
            # RL uncertainty: softmax entropy (0..log nA) normalized to [0,1]
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs /= np.sum(rl_probs)
            entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            rl_unc = entropy / np.log(3.0)  # normalize by log(nA)

            # WM confidence: peakedness of w (max minus mean), scaled to [0,1]
            wm_conf = (np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA)
            wm_conf = np.clip(wm_conf, 0.0, 1.0)

            set_size_t = int(block_set_sizes[t])
            ss_term = ss_penalty * max(0, set_size_t - 3)

            # Sigmoidal arbitration
            mix_logit = wm_bias + k_unc * (wm_conf - rl_unc) - ss_term
            wm_weight_eff = 1.0 / (1.0 + np.exp(-mix_logit))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update (probabilistic associative)
            if r >= 0.5:
                # Move towards one-hot at action a
                w[s, :] = (1.0 - wm_lr) * w[s, :]
                w[s, a] += wm_lr
            else:
                # Push probability mass away from chosen action a toward uniform w_0
                w[s, a] = (1.0 - wm_lr) * w[s, a] + wm_lr * w_0[s, a]
                others = [aa for aa in range(nA) if aa != a]
                spill = (wm_lr * (1.0 / nA)) / len(others)
                w[s, others] = np.clip(w[s, others] + spill, 0.0, 1.0)

            # Normalize for safety
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RPE-gated WM encoding with lag- and set-size-dependent forgetting; hybrid choice.

    Mechanism
    - RL: standard Q-learning softmax.
    - WM: per-state distribution w over actions. Encoding into WM is gated by a logistic
      function of |RPE| (surprise). WM traces decay toward uniform as a function of
      time since last visit multiplied by set size (interference).
    - Arbitration: constant base WM weight mixed with RL.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate (also used for WM encoding step size).
        wm_weight : float in [0,1]
            Mixture weight on WM policy (arbitration).
        softmax_beta : float >= 0
            Inverse temperature for RL softmax (scaled internally).
        gate_slope : float >= 0
            Slope of logistic gating as a function of |RPE|.
        lag_interference : float >= 0
            Scale factor on decay due to set-size-weighted inter-visit lag.
        wm_forget : float >= 0
            Base forgetting rate per unit of interference.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_slope, lag_interference, wm_forget = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply lag- and set-size-dependent forgetting before using WM
            if last_seen[s] >= 0:
                lag = max(1, t - int(last_seen[s]))
                set_size_t = int(block_set_sizes[t])
                interference = lag_interference * lag * set_size_t
                # Convert to decay strength in [0,1): d = 1 - exp(-wm_forget * interference)
                d = 1.0 - np.exp(-wm_forget * max(0.0, interference))
                w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]
                # Normalize
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Gated encoding based on |RPE| surprise
            gate_prob = 1.0 / (1.0 + np.exp(-gate_slope * np.abs(pe)))
            # Deterministic expected update equivalent: scale the step by gate_prob
            enc_step = lr * gate_prob
            # Move toward one-hot at chosen action proportional to enc_step
            w[s, :] = (1.0 - enc_step) * w[s, :]
            w[s, a] += enc_step
            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p