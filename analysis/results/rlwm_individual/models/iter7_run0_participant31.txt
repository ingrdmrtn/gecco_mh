def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with Pearce–Hall learning-rate modulation + WM with set-size–dependent precision.

    Mechanism
    - RL: tabular Q-learning where the learning rate on each trial increases with the unsigned PE
      (Pearce–Hall). Effective lr_t = clip(lr_base + ph_kappa*|PE|, 0..1).
    - WM store: one-hot cache of last rewarded action per state (no leak). WM policy precision
      depends on set size via an effective inverse temperature:
        beta_wm_eff = wm_beta_base / (1 + ss_beta_scale * (nS - 3)).
      Larger set size lowers WM precision.
    - Arbitration: convex mixture of WM and RL policies with fixed base weight wm_weight.

    Parameters
    - lr_base: base learning rate for RL (0..1).
    - wm_weight: mixture weight for WM vs RL (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>0).
    - ph_kappa: gain on unsigned PE for learning-rate modulation (>=0).
    - wm_beta_base: base WM inverse temperature (>0).
    - ss_beta_scale: set-size scaling of WM precision (>=0); larger values reduce WM precision more in nS=6.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr_base, wm_weight, softmax_beta, ph_kappa, wm_beta_base, ss_beta_scale = parameters
    softmax_beta *= 10.0  # RL inverse temperature scaled up
    softmax_beta_wm = 50.0  # not directly used; WM precision is computed below

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM precision
        beta_wm_eff = wm_beta_base / (1.0 + ss_beta_scale * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax policy
            z_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(z_rl) / np.sum(np.exp(z_rl))
            p_rl = max(pi_rl[a], 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            z_wm = beta_wm_eff * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with Pearce–Hall modulation
            delta = r - q[s, a]
            lr_t = np.clip(lr_base + ph_kappa * abs(delta), 0.0, 1.0)
            q[s, a] += lr_t * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # overwrite cache on reward

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with novelty bonus and state-wise forgetting + standard WM.

    Mechanism
    - RL:
      - Softmax action selection from Q-values augmented by a novelty bonus for less-tried actions
        in the current state. Bonus_a(s) = novelty_eff / sqrt(1 + N_s(a)), where N_s(a) is the
        visit count of action a in state s.
      - State-wise forgetting toward the uniform prior before each update:
        q[s,:] <- (1 - rl_decay)*q[s,:] + rl_decay*(1/nA).
      - Standard delta rule update with learning rate lr.
    - WM store: one-hot cache of last rewarded action per state (no leak).
    - Arbitration: fixed mixture weight wm_weight.
    - Set size effect: novelty weight attenuates in larger set sizes:
        novelty_eff = novelty_weight / (1 + ss_novelty_gain * (nS - 3)).

    Parameters
    - lr: learning rate for RL (0..1).
    - wm_weight: mixture weight for WM vs RL (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 (>0).
    - rl_decay: decay toward prior on each state visit (0..1).
    - novelty_weight: base novelty bonus weight (>0).
    - ss_novelty_gain: how strongly set size reduces novelty weight (>=0).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, rl_decay, novelty_weight, ss_novelty_gain = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Action-visit counts per state for novelty
        N = np.zeros((nS, nA))
        novelty_eff = novelty_weight / (1.0 + ss_novelty_gain * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with novelty bonus and forgetting
            # Apply state-wise forgetting toward uniform prior
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)

            Q_s = q[s, :].copy()
            bonus = novelty_eff / np.sqrt(1.0 + N[s, :])
            Q_eff = Q_s + bonus

            z_rl = softmax_beta * (Q_eff - np.max(Q_eff))
            pi_rl = np.exp(z_rl) / np.sum(np.exp(z_rl))
            p_rl = max(pi_rl[a], 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :].copy()
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update and count update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            N[s, a] += 1.0

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + confidence-weighted arbitration and graded WM encoding.

    Mechanism
    - RL: standard tabular Q-learning with fixed learning rate and softmax choice.
    - WM store: graded update on reward towards a one-hot vector:
        w[s,:] <- (1 - wm_update_eff)*w[s,:] + wm_update_eff*one_hot(a),
      where wm_update_eff = wm_update / (1 + ss_update_scale*(nS - 3)).
      Larger set size reduces WM encoding strength.
    - Arbitration: dynamic mixture based on WM confidence c_wm, defined as the margin between the
      largest and second-largest WM entries in state s. Weight is a logistic transform:
        weight = sigmoid(logit(wm_weight) + conf_slope * c_wm),
      thereby giving WM more influence when its policy is sharp/confident.

    Parameters
    - lr: RL learning rate (0..1).
    - wm_weight: base WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>0).
    - wm_update: base WM encoding strength on reward (0..1).
    - conf_slope: sensitivity of arbitration to WM confidence (can be +/-).
    - ss_update_scale: set-size scaling on WM update strength (>=0).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_update, conf_slope, ss_update_scale = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # high precision when WM is concentrated

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_update_eff = wm_update / (1.0 + ss_update_scale * max(0, nS - 3))
        wm_update_eff = np.clip(wm_update_eff, 0.0, 1.0)

        # Helper to compute sigmoid and logit safely
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        def logit(p):
            p = np.clip(p, 1e-6, 1 - 1e-6)
            return np.log(p) - np.log(1 - p)

        base_logit = logit(wm_weight)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            z_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(z_rl) / np.sum(np.exp(z_rl))
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            # Confidence-weighted arbitration
            sorted_W = np.sort(W_s)[::-1]
            top = sorted_W[0]
            second = sorted_W[1] if nA > 1 else 0.0
            c_wm = max(0.0, top - second)
            wm_weight_eff = sigmoid(base_logit + conf_slope * c_wm)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM graded update on reward
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_update_eff) * w[s, :] + wm_update_eff * one_hot

        blocks_log_p += log_p

    return -blocks_log_p