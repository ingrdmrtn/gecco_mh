def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recall-gated WM policy based on set size (logistic recall), reward-gated WM storage.
    
    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled by 10 internally).
    - WM channel: recalls the correct action with probability p_recall; when recall succeeds,
      choice follows a near-deterministic softmax over WM weights W (beta_wm=50); when recall fails,
      WM contributes a uniform policy. The total WM policy is: p_wm = p_recall * softmax(W) + (1-p_recall) * uniform.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.
    
    WM dynamics:
    - State-specific store-on-reward: if reward=1, store a one-hot vector on the chosen action.
      If reward=0, do not store (no overwrite).
      Baseline W initialized as uniform; no decay here (all variability comes from whether storage occurred).
    
    Set-size effect:
    - WM recall probability depends on set size via a logistic function:
        p_recall = sigmoid(wm_recall_bias + wm_recall_slope * (3 - nS))
      which yields higher recall for smaller sets (nS=3) and lower recall for larger sets (nS=6).
      Parameters wm_recall_bias and wm_recall_slope govern this effect.
    
    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - softmax_beta: Inverse temperature for RL channel (scaled by 10 internally).
    - wm_recall_bias: Bias term for WM recall probability (real).
    - wm_recall_slope: Slope controlling how recall changes with set size (real; typically >0).
    """
    lr, wm_weight, softmax_beta, wm_recall_bias, wm_recall_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute block-level WM recall probability from set size
        lin = wm_recall_bias + wm_recall_slope * (3 - nS)
        p_recall = 1.0 / (1.0 + np.exp(-lin))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm_det = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(denom_wm_det, 1e-12)
            p_wm = p_recall * p_wm_det + (1.0 - p_recall) * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated storage (overwrite on reward, otherwise keep as is)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall learning rate + WM with set-size-scaled mixture weight and decay.
    
    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled by 10 internally).
    - WM channel: softmax over WM weights with high inverse temperature (deterministic tendency).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.
    
    RL dynamics (Pearce-Hall):
    - Learning rate is modulated by unsigned prediction error: alpha_t = clip(alpha0 + kappa * |PE|, 0..1),
      where PE = r - Q_s[a]. This captures attention to surprising outcomes.
    
    WM dynamics:
    - Decay of WM toward uniform with rate wm_decay_eff each trial on the visited state.
    - If reward=1, store a one-hot for the chosen action (overwrite).
    
    Set-size effects:
    - WM mixture weight decays exponentially with set size: wm_weight_eff = wm_weight0 * exp(-wm_size_sensitivity * (nS - 3)).
      Larger sets reduce WM influence.
    - WM decay increases slightly with set size: wm_decay_eff = 1 - (1 - wm_decay0)^(1 + (nS - 3)).
      This makes WM more labile when load is larger.
    
    Parameters (tuple):
    - alpha0: Baseline RL learning rate (0..1).
    - kappa: Gain on unsigned PE for RL learning rate (>=0).
    - wm_weight0: Baseline WM mixture weight at set size 3 (0..1).
    - softmax_beta: Inverse temperature for RL channel (scaled by 10 internally).
    - wm_decay0: Baseline WM decay rate at set size 3 (0..1).
    - wm_size_sensitivity: Strength of set-size penalty on WM mixture weight (>=0).
    """
    alpha0, kappa, wm_weight0, softmax_beta, wm_decay0, wm_size_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_weight0 * np.exp(-wm_size_sensitivity * max(0, nS - 3))
        wm_decay_eff = 1.0 - (1.0 - wm_decay0) ** (1 + max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall learning rate
            pe = r - Q_s[a]
            alpha_t = alpha0 + kappa * abs(pe)
            alpha_t = min(max(alpha_t, 0.0), 1.0)
            q[s, a] += alpha_t * pe

            # WM decay toward uniform, then reward-gated overwrite
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Leaky WM success/failure accumulator with set-size-dependent WM precision.
    
    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled by 10 internally).
    - WM channel: leaky accumulator of action evidence per state, turned into a probability via softmax.
      WM inverse temperature decreases with set size (noisier WM under higher load).
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.
    
    WM dynamics (leaky accumulator):
    - On each visit to state s:
      * All WM weights w[s,:] decay multiplicatively toward uniform via separate leak dynamics for success and failure.
      * If reward=1: increase evidence for chosen action a by (1 - wm_leak_succ); others only decay.
      * If reward=0: reduce evidence for chosen action a by wm_leak_fail (i.e., push it toward uniform).
      This creates success-driven sharpening and failure-driven suppression.
      Implementation ensures w stays in [0,1] and normalized implicitly by the softmax policy.
    
    Set-size effects:
    - WM precision (inverse temperature) scales as beta_wm_eff = 50 / (1 + size_noise * (nS - 3)),
      making WM choices less precise for larger sets. Parameter size_noise controls this drop.
    
    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - softmax_beta: Inverse temperature for RL channel (scaled by 10 internally).
    - wm_leak_succ: Leak applied to WM trace on success (0..1); lower values keep stronger traces.
    - wm_leak_fail: Additional leak applied to chosen action on failure (0..1).
    - size_noise: Strength of set-size penalty on WM precision (>=0).
    """
    lr, wm_weight, softmax_beta, wm_leak_succ, wm_leak_fail, size_noise = model_parameters
    softmax_beta *= 10.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_wm_eff = 50.0 / (1.0 + size_noise * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM leaky accumulator update
            # Global decay toward uniform for the visited state (success leak as baseline)
            w[s, :] = (1.0 - wm_leak_succ) * w[s, :] + wm_leak_succ * w_0[s, :]
            if r > 0.5:
                # Successful outcome: strengthen chosen action by pulling it away from uniform
                w[s, a] = (1.0 - wm_leak_succ) * w[s, a] + (1.0 - (1.0 - wm_leak_succ))  # add (approx) (1 - leak)
                # Clip to [0,1] for numerical stability
                w[s, a] = min(max(w[s, a], 0.0), 1.0)
            else:
                # Failure: additional leak for the chosen action toward uniform
                w[s, a] = (1.0 - wm_leak_fail) * w[s, a] + wm_leak_fail * w_0[s, a]
                w[s, a] = min(max(w[s, a], 0.0), 1.0)

        blocks_log_p += log_p

    return -blocks_log_p