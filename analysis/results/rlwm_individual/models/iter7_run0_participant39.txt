def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + time-based WM retrieval with load-dependent decay.

    Mechanism
    - RL: standard Q-learning over state-action values.
    - WM: trial-wise, state-specific memory trace that stores the last rewarded action as a one-hot vector.
          Retrieval is probabilistic and decays exponentially with time since the state was last seen,
          with a decay rate that increases with set size.
    - Mixture: action probability is a convex combination of RL and WM policies with weight wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_retrieval0: baseline WM retrieval probability at zero delay and minimal load (0..1).
        - decay_rate_base: baseline time-decay rate for WM retrieval; effective decay scales with set size (>=0).

    Set-size effects
    ----------------
    - WM retrieval: p_retrieve = wm_retrieval0 * exp(- decay_rate_base * (nS/3) * time_since_seen).
      Larger set sizes (nS=6) yield faster decay and therefore weaker WM guidance.
    """
    lr, wm_weight, softmax_beta, wm_retrieval0, decay_rate_base = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track time since last seen for each state
        time_since = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Update time-since-seen counters
            time_since += 1
            time_since[s] = 0  # current state is being processed now

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM retrieval probability decays with time and load
            decay_eff = decay_rate_base * (float(nS) / 3.0)
            p_rec = wm_retrieval0 * np.exp(-decay_eff * time_since[s])

            # WM policy: if retrieved, use softmax over W; else fallback is uniform (no info)
            p_wm_soft = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_rec * p_wm_soft + (1 - p_rec) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: store rewarded action as one-hot trace; otherwise no consolidation
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-sensitive exploration + WM partial storage capacity (probabilistic).

    Mechanism
    - RL: Q-learning where the effective inverse temperature decreases with set size,
          capturing greater exploration under higher load.
    - WM: when rewarded, the state-action is encoded with strength wm_alpha, but WM is only
          available with probability proportional to a capacity proportion (wm_slots_prop)
          relative to current set size (a simple capacity-limited storage approximation).
    - Mixture: combine WM and RL policies using wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta_base: base RL inverse temperature at set size 3; scaled by 10 internally.
        - beta_load_slope: slope by which RL inverse temperature decreases with load (>=0).
                           Effective beta: softmax_beta_eff = softmax_beta_base / (1 + beta_load_slope * (nS/3 - 1)).
        - wm_alpha: WM learning rate toward one-hot on rewarded trials (0..1).
        - wm_slots_prop: proportion of set items that can be stored in WM (0..1).
                         Retrieval probability p_store = min(1, wm_slots_prop * (3 / nS)).

    Set-size effects
    ----------------
    - RL exploration: higher nS decreases softmax_beta_eff => more exploration.
    - WM storage: higher nS reduces p_store => less reliable WM.
    """
    lr, wm_weight, softmax_beta_base, beta_load_slope, wm_alpha, wm_slots_prop = model_parameters
    softmax_beta = softmax_beta_base * 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent RL temperature and WM storage probability
        beta_eff = softmax_beta / (1.0 + beta_load_slope * (float(nS) / 3.0 - 1.0))
        beta_eff = max(beta_eff, 1e-6)
        p_store = min(1.0, max(0.0, wm_slots_prop * (3.0 / float(nS))))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with load-sensitive beta
            p_rl = 1 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy: with prob p_store, use WM softmax; otherwise uniform
            p_wm_soft = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_store * p_wm_soft + (1 - p_store) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: encode on rewarded trials with strength wm_alpha
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_alpha) * w[s] + wm_alpha * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated WM with load-dependent forgetting and tunable WM policy temperature.

    Mechanism
    - RL: standard Q-learning.
    - WM: updates are gated by the magnitude of the RL prediction error (PE). Only sufficiently
          surprising outcomes are stored in WM. WM representations passively forget toward uniform,
          with forgetting increasing with set size.
    - Policies: RL and WM produce separate softmax policies, combined by wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - gate_threshold: absolute PE threshold for WM storage (>=0). If |PE| > gate_threshold, store/update WM.
        - wm_forget_base: base WM forgetting rate per trial (0..1); effective forgetting scales with set size.
        - wm_beta: WM inverse temperature used in WM softmax (>=0).

    Set-size effects
    ----------------
    - WM forgetting: forget_eff = wm_forget_base * (nS / 6). Larger sets forget faster.
    """
    lr, wm_weight, softmax_beta, gate_threshold, wm_forget_base, wm_beta = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = max(1e-6, wm_beta)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        forget_eff = np.clip(wm_forget_base * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM forgetting toward uniform
            if forget_eff > 0:
                w = (1 - forget_eff) * w + forget_eff * w_0

            # PE-gated WM update
            if abs(delta) > gate_threshold:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = onehot

        blocks_log_p += log_p

    return -blocks_log_p