def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with RL forgetting and reward-gated WM writes; WM reliance decreases with set size.

    Policy:
    - RL: softmax over Q with softmax_beta.
    - WM: softmax over W with very high inverse temperature (near-deterministic recall).
    - Mixture: wm_weight = sigmoid(psi0 - psi_size * (set_size - 3)).

    Learning:
    - RL: delta-rule with learning rate lr, plus value forgetting toward uniform by q_forget each visit.
    - WM: decay toward uniform (wm_decay) each visit; reward-gated replacement toward chosen action
          (only when r=1).

    Set-size effects:
    - WM reliance explicitly reduced as set size grows (psi_size > 0).
    - Larger set sizes indirectly impair WM because decay plus sparser rewarded writes leave W less precise.

    Parameters (6):
    - lr: RL learning rate for Q updates.
    - softmax_beta: RL softmax inverse temperature; internally scaled by 10.
    - q_forget: RL forgetting rate toward uniform each time the state is visited (0=no forgetting, 1=full reset).
    - wm_decay: WM decay toward uniform each time the state is visited (0=no decay, 1=full reset).
    - psi0: baseline WM mixture bias at set size 3 (mapped via sigmoid).
    - psi_size: slope controlling reduction of WM mixture weight as set size increases.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, q_forget, wm_decay, psi0, psi_size = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture weight for this block as a function of set size
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))
        wm_weight_eff = np.clip(sigmoid(psi0 - psi_size * (nS - 3)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with forgetting toward uniform baseline
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Forgetting for the visited state
            q[s, :] = (1.0 - q_forget) * q[s, :] + q_forget * (1.0 / nA)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-gated replacement: write chosen action only if rewarded
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Replace toward one-hot; strength complements decay to avoid exceeding 1.0 total movement
                replace_rate = 1.0 - wm_decay
                w[s, :] = (1.0 - replace_rate) * w[s, :] + replace_rate * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall volatility-driven learning rate, WM capacity-limited mixture,
    and action stickiness.

    Policy:
    - RL: softmax over Q with softmax_beta; adds stickiness to the previously chosen action.
    - WM: softmax over W with high inverse temperature.
    - Mixture: wm_weight = wm_write_prob * min(1, K / set_size), capturing WM capacity limits.

    Learning:
    - RL (Pearce-Hall): per-state associability alpha_s updated by unsigned prediction error,
      alpha_s <- (1 - kappa_ph)*alpha_s + kappa_ph*|delta|, learning rate = base_lr * alpha_s.
    - WM: each trial decays a bit toward uniform; when a state is visited, with probability
      wm_write_prob it writes the chosen action (recency-based, reward-agnostic).

    Set-size effects:
    - WM contribution scales inversely with set size via K/set_size (capacity K).
    - Larger set sizes reduce WM weight and increase reliance on RL with volatility-driven learning.

    Parameters (6):
    - base_lr: base RL learning rate, scaled by state-specific associability.
    - kappa_ph: Pearce-Hall update rate for associability (0=stable, 1=fast volatility tracking).
    - softmax_beta: RL softmax inverse temperature; internally scaled by 10.
    - K: WM capacity in items; controls wm_weight via K/set_size.
    - wm_write_prob: probability of writing chosen action into WM on each visit.
    - stickiness: action perseveration weight added to the previously chosen action in policy.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    base_lr, kappa_ph, softmax_beta, K, wm_write_prob, stickiness = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Pearce-Hall associability per state
        alpha_s = 0.5 * np.ones(nS)

        # Mixture weight by capacity
        wm_weight_eff = np.clip(wm_write_prob * min(1.0, float(K) / max(1.0, float(nS))), 0.0, 1.0)

        last_action = None  # for stickiness

        log_p = 0.0
        rng = np.random.RandomState(0)  # deterministic write sampling for reproducibility in likelihood calc

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # Add stickiness bias to Q preferences (does not change learning target)
            if last_action is not None:
                for aa in range(nA):
                    if aa == last_action:
                        Q_s[aa] += stickiness

            W_s = w[s, :]

            # RL policy with adjusted preferences
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update (Pearce-Hall)
            # Use the true Q without stickiness for learning
            delta = r - q[s, a]
            alpha_s[s] = (1.0 - kappa_ph) * alpha_s[s] + kappa_ph * abs(delta)
            eff_lr = base_lr * np.clip(alpha_s[s], 0.0, 1.0)
            q[s, a] += eff_lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Simple decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            # Probabilistic write (recency-based)
            if rng.rand() < wm_write_prob:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Bayesian RL (Beta-Bernoulli) + WM cache with size-dependent mixture and lapse.

    Policy:
    - RL: maintains Beta(s,a) posteriors over reward probabilities; Q = mean of Beta.
          Action selection via softmax over Q with softmax_beta.
    - WM: near-deterministic recall distribution W_s; softmax with high inverse temperature.
    - Mixture weight: wm_weight = sigmoid(wm_bias - wm_size_slope*(set_size - 3)).
    - Lapse: with probability lapse, choose uniformly at random.

    Learning:
    - RL: update Beta counts per (s,a): successes += r, failures += (1-r), initialized by prior_count.
    - WM: on rewarded trials, with probability wm_learn_prob, write a one-hot for the chosen action.
          Otherwise W decays gently toward uniform.

    Set-size effects:
    - WM mixture weight decreases with set size (wm_size_slope > 0).
    - Larger set sizes thus shift control toward Bayesian RL.

    Parameters (6):
    - prior_count: symmetric Beta prior pseudo-count for successes and failures (>=0).
    - softmax_beta: RL softmax inverse temperature; internally scaled by 10.
    - wm_bias: baseline WM mixture bias at set size 3 (mapped via sigmoid).
    - wm_size_slope: slope controlling WM mixture reduction as set size increases.
    - wm_learn_prob: probability of WM write on rewarded trials.
    - lapse: lapse probability mixing in uniform random choice.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    prior_count, softmax_beta, wm_bias, wm_size_slope, wm_learn_prob, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Beta-Bernoulli parameters: successes and failures
        succ = prior_count * np.ones((nS, nA))
        fail = prior_count * np.ones((nS, nA))

        q = succ / np.maximum(succ + fail, eps)  # expected value
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        wm_weight_eff = np.clip(sigmoid(wm_bias - wm_size_slope * (nS - 3)), 0.0, 1.0)
        lapse = np.clip(lapse, 0.0, 1.0)

        log_p = 0.0
        rng = np.random.RandomState(0)  # deterministic WM write sampling for likelihood

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update: Beta counts
            succ[s, a] += r
            fail[s, a] += (1 - r)
            q[s, a] = succ[s, a] / max(succ[s, a] + fail[s, a], eps)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Gentle decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            # Reward-contingent WM write
            if (r > 0) and (rng.rand() < wm_learn_prob):
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p