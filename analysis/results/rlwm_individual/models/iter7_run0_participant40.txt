def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + fast-decaying WM with per-state rehearsal cost.

    Mechanism
    - RL: standard delta-rule learning (single learning rate).
    - WM: fast learning trace for the chosen action that decays toward uniform.
      WM decays faster the more often a state is visited (rehearsal cost).
    - Arbitration: WM weight starts high but declines with the number of times a
      state has been seen (effective capacity cost) and with larger set sizes.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Baseline WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled internally by 10).
        wm_fast_lr : float
            WM rapid learning rate for the chosen action (0-1).
        wm_decay : float
            Baseline WM decay toward uniform each encounter (0-1).
        arbitration_hazard : float
            Strength of state-wise decline of WM weight with repeated encounters (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_fast_lr, wm_decay, arbitration_hazard = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        visits = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight declines with set size and with per-state visit count
            size_scale = 3.0 / float(nS)
            eff_wm_weight = wm_weight_base * size_scale
            eff_wm_weight *= np.exp(-arbitration_hazard * visits[s])
            eff_wm_weight = np.clip(eff_wm_weight, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay increases with how many times this state has been rehearsed
            # Effective decay = 1 - (1 - wm_decay)^(visits+1) approximated with linear cap
            extra_forgetting = 1.0 - np.exp(-(visits[s] + 1))
            decay_now = np.clip(wm_decay * (0.5 + 0.5 * extra_forgetting), 0.0, 1.0)
            w[s, :] = (1.0 - decay_now) * w[s, :] + decay_now * w_0[s, :]

            # WM rapid learning toward one-hot on chosen action proportional to reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_fast_lr) * w[s, :] + wm_fast_lr * target

            visits[s] += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recency-based retrieval and PE-gated encoding; RL beta scales with set size.

    Mechanism
    - RL: standard delta-rule learning. Inverse temperature decreases with set size.
    - WM: encoding is gated by the magnitude of the prediction error (stronger after surprises);
      retrieval strength decays exponentially with time since last visit to the state.
    - Arbitration: WM mixture weight is the base weight times the retrieval probability,
      giving stronger WM influence when the state was recently seen.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            Base RL inverse temperature (scaled internally by 10).
        wm_gate_temp : float
            Controls how strongly abs(PE) gates WM encoding (>=0).
        wm_interference_tau : float
            Time constant for decay of WM retrieval with delay (in trials, >0).
        beta_size_gain : float
            Scales how RL beta changes with set size; effective beta = base * (1 + gain*(3/nS - 1)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_gate_temp, wm_interference_tau, beta_size_gain = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        # Size-scaled RL beta
        size_factor = (3.0 / float(nS))  # 1 for nS=3, 0.5 for nS=6
        beta_rl = softmax_beta * (1.0 + beta_size_gain * (size_factor - 1.0))
        beta_rl = max(1e-6, beta_rl)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_visit = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # Retrieval probability decays with time since last visit
            if last_visit[s] >= 0:
                dt = (t - last_visit[s])
            else:
                dt = 1e6  # effectively unseen -> no retrieval
            retrieval_prob = np.exp(-float(dt) / max(1e-6, wm_interference_tau))
            retrieval_prob = np.clip(retrieval_prob, 0.0, 1.0)

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            eff_wm_weight = wm_weight * retrieval_prob
            eff_wm_weight = np.clip(eff_wm_weight, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # PE-gated WM encoding (stronger with surprise magnitude)
            gate_strength = 1.0 / (1.0 + np.exp(-wm_gate_temp * np.abs(delta)))
            target = w[s, :].copy()
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] = (1.0 - gate_strength) * w[s, :] + gate_strength * target

            last_visit[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size-dependent WM forgetting, plus lapses.

    Mechanism
    - RL: standard delta-rule learning.
    - WM: one-shot consolidation on rewards; otherwise drift toward uniform.
      Forgetting is stronger for larger set sizes.
    - Arbitration: WM is weighted by RL uncertainty (entropy of RL policy) and set size
      (more WM influence when nS is small and when RL is uncertain).
    - Lapse: with small probability, choices are random (epsilon).

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled internally by 10).
        uncert_temp : float
            Sensitivity of arbitration to RL policy entropy (>=0).
        wm_forget_size : float
            Base WM forgetting rate scaled by set size (0-1).
        epsilon : float
            Lapse probability of choosing uniformly at random (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, uncert_temp, wm_forget_size, epsilon = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3
    max_entropy = np.log(nA)

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax vector and chosen probability
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            pvec_rl = exp_logits / np.sum(exp_logits)
            p_rl = pvec_rl[a]

            # RL uncertainty as policy entropy
            entropy = -np.sum(pvec_rl * np.log(np.clip(pvec_rl, 1e-12, 1.0)))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight increases with uncertainty and decreases with set size
            size_scale = 3.0 / float(nS)
            uncert_gain = 1.0 / (1.0 + np.exp(-uncert_temp * (entropy - 0.5 * max_entropy)))
            eff_wm_weight = np.clip(wm_weight_base * size_scale * uncert_gain, 0.0, 1.0)

            mix_prob = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            mix_prob = np.clip(mix_prob, 1e-12, 1.0)

            # Lapse mixture
            p_total = (1.0 - epsilon) * mix_prob + epsilon * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting scaled by set size and negative outcomes
            forget = wm_forget_size * (float(nS) / 3.0)
            forget = np.clip(forget, 0.0, 1.0)
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]

            # WM one-shot consolidation on reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target  # consolidate moderately

        blocks_log_p += log_p

    return -blocks_log_p