Below are three distinct cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways to account for set-size effects in the rlwm task. Each function returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported, as requested.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and WM decay/encoding.

    Mechanism:
    - RL: tabular Q-learning with standard delta rule.
    - WM: fast, reward-gated one-shot encoding that decays toward a uniform prior within a block.
    - Arbitration: mixture between RL and WM policies where the effective WM weight is modulated
      by the relative uncertainty (entropy) difference between RL and WM at the current state:
         wm_weight_eff = wm_weight * sigmoid((H_RL - H_WM) / unc_temp)
      High RL uncertainty and low WM uncertainty shifts weight toward WM. Set-size affects both
      through RL/WM policy entropies implicitly.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline mixture weight on WM in [0,1].
    - softmax_beta: Baseline RL inverse temperature (internally scaled by 10).
    - wm_decay: Per-trial decay of WM rows toward uniform in [0,1].
    - unc_temp: Temperature controlling sensitivity of arbitration to entropy differences (>0).
    - wm_learn: Strength of WM encoding on rewarded trials in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, unc_temp, wm_learn = model_parameters
    softmax_beta *= 10.0  # base RL inverse temperature
    softmax_beta_wm = 50.0  # WM is more deterministic

    wm_weight = float(np.clip(wm_weight, 0.0, 1.0))
    lr = float(np.clip(lr, 0.0, 1.0))
    wm_decay = float(np.clip(wm_decay, 0.0, 1.0))
    wm_learn = float(np.clip(wm_learn, 0.0, 1.0))
    unc_temp = max(1e-6, float(unc_temp))  # avoid divide by zero

    def softmax_probs(x, beta):
        x = np.array(x, dtype=float)
        z = x - np.max(x)
        ex = np.exp(beta * z)
        return ex / np.sum(ex)

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        p = p / np.sum(p)
        return -np.sum(p * np.log(p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy (full distribution for entropy and chosen prob)
            Q_s = q[s, :]
            p_rl_vec = softmax_probs(Q_s, softmax_beta)
            p_rl = np.clip(p_rl_vec[a], 1e-12, 1.0)

            # WM policy (full distribution for entropy and chosen prob)
            W_s = w[s, :]
            p_wm_vec = softmax_probs(W_s, softmax_beta_wm)
            p_wm = np.clip(p_wm_vec[a], 1e-12, 1.0)

            # Entropy-based arbitration (higher RL entropy -> more WM)
            H_rl = entropy(p_rl_vec)
            H_wm = entropy(p_wm_vec)
            arb_factor = 1.0 / (1.0 + np.exp(-(H_rl - H_wm) / unc_temp))
            wm_weight_eff = np.clip(wm_weight * arb_factor, 0.0, 1.0)

            # Mixture policy likelihood
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM reward-gated encoding
            if r > 0.0:
                # Move WM row toward one-hot on chosen action by wm_learn
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

                # Renormalize to keep a valid distribution
                w[s, :] = np.clip(w[s, :], 1e-9, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent forgetting + WM precision degraded by set size (power law).

    Mechanism:
    - RL: tabular Q-learning with load-dependent decay (forgetting) toward uniform values each trial:
         q <- (1 - q_decay_eff) * q + q_decay_eff * uniform
      where q_decay_eff = q_decay_load * max(0, nS - 1). Larger set sizes increase forgetting.
    - WM: associative table used as a policy with an effective precision (inverse temperature)
      reduced by set size:
         beta_wm_eff = softmax_beta_wm / (1 + (nS - 1) ** power) * wm_precision_base
    - WM update: on reward, strengthen the chosen action; on no reward, the WM row softly relaxes
      toward uniform (feedback-sensitive rehearsal).

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline mixture weight on WM in [0,1].
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - q_decay_load: Coefficient for load-dependent Q forgetting per trial (>=0).
    - wm_precision_base: Scales WM precision (>0).
    - power: Exponent controlling how sharply WM precision declines with set size (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, q_decay_load, wm_precision_base, power = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    lr = float(np.clip(lr, 0.0, 1.0))
    wm_weight = float(np.clip(wm_weight, 0.0, 1.0))
    q_decay_load = max(0.0, float(q_decay_load))
    wm_precision_base = max(1e-6, float(wm_precision_base))
    power = max(0.0, float(power))

    def softmax_probs(x, beta):
        x = np.array(x, dtype=float)
        z = x - np.max(x)
        ex = np.exp(beta * z)
        return ex / np.sum(ex)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent parameters
        q_decay_eff = q_decay_load * max(0, nS - 1)  # more forgetting at larger set sizes
        beta_wm_eff = softmax_beta_wm * wm_precision_base / (1.0 + (max(0, nS - 1)) ** power)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl_vec = softmax_probs(Q_s, softmax_beta)
            p_rl = np.clip(p_rl_vec[a], 1e-12, 1.0)

            # WM policy with degraded precision under load
            W_s = w[s, :]
            p_wm_vec = softmax_probs(W_s, beta_wm_eff)
            p_wm = np.clip(p_wm_vec[a], 1e-12, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with load-dependent forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            if q_decay_eff > 0.0:
                q = (1.0 - q_decay_eff) * q + q_decay_eff * (1.0 / nA)

            # WM update: reward strengthens chosen; no-reward relaxes toward uniform
            if r > 0.0:
                # Strengthen chosen action by shifting mass toward chosen
                gain = 0.5  # implicit moderate gain; encoded via precision base in policy
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain
            else:
                # On no reward, decay that state's WM toward uniform prior
                relax = 0.25
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

            # Renormalize the updated WM row
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent decision noise + WM recency-gated arbitration.

    Mechanism:
    - RL: standard Q-learning. The RL inverse temperature is reduced by set size:
         beta_rl_eff = softmax_beta / (1 + beta_load_slope * (nS - 1))
      Larger sets induce more decision noise in RL.
    - WM: maintains a per-state "last rewarded action" with confidence c[s] that decays over time.
      Choice policy for WM is formed by mixing a one-hot on the stored action with uniform, weighted
      by c[s]. Effective WM weight in the mixture is further gated by c[s]:
         wm_weight_eff = wm_weight * c[s]
    - WM updates: on reward, store chosen action and set c[s]=1; otherwise confidence decays.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline mixture weight on WM in [0,1].
    - softmax_beta: Baseline RL inverse temperature (scaled by 10 internally).
    - recency: Per-trial decay factor of WM confidence in [0,1] (higher = faster decay).
    - beta_load_slope: Nonnegative slope scaling how much set size reduces RL beta.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, recency, beta_load_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    lr = float(np.clip(lr, 0.0, 1.0))
    wm_weight = float(np.clip(wm_weight, 0.0, 1.0))
    recency = float(np.clip(recency, 0.0, 1.0))
    beta_load_slope = max(0.0, float(beta_load_slope))

    def softmax_probs(x, beta):
        x = np.array(x, dtype=float)
        z = x - np.max(x)
        ex = np.exp(beta * z)
        return ex / np.sum(ex)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will not be used directly; we construct WM policy from c and last_a
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state WM memory: last rewarded action and its confidence
        last_a = -np.ones(nS, dtype=int)
        c = np.zeros(nS, dtype=float)  # confidence in [0,1]

        # Load-dependent RL beta
        beta_rl_eff = softmax_beta / (1.0 + beta_load_slope * max(0, nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with load-reduced beta
            Q_s = q[s, :]
            p_rl_vec = softmax_probs(Q_s, beta_rl_eff)
            p_rl = np.clip(p_rl_vec[a], 1e-12, 1.0)

            # WM policy constructed from last rewarded action and confidence
            if last_a[s] >= 0:
                wm_row = (1.0 - c[s]) * w_0[s, :].copy()
                wm_row[last_a[s]] += c[s]
            else:
                wm_row = w_0[s, :].copy()

            # Convert to distribution
            wm_row = np.clip(wm_row, 1e-12, None)
            wm_row /= np.sum(wm_row)
            p_wm_vec = softmax_probs(wm_row, softmax_beta_wm)
            p_wm = np.clip(p_wm_vec[a], 1e-12, 1.0)

            # Arbitration gated by current WM confidence
            wm_weight_eff = np.clip(wm_weight * c[s], 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM confidence update: decay, then reward-gated strengthening
            c[s] = (1.0 - recency) * c[s]  # decay
            if r > 0.0:
                last_a[s] = a
                c[s] = 1.0  # reset to full confidence on successful encoding

        blocks_log_p += log_p

    return -blocks_log_p