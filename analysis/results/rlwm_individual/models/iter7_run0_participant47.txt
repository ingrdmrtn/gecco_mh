def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Uncertainty-based arbitration with load-dependent WM precision.
    - RL: single-rate Q-learning.
    - WM store: leaky prototype per state that is sharpened by reward.
    - Arbitration: WM weight is higher when WM is more certain than RL within a state.
      WM certainty decreases with set size via a precision slope.

    Parameters:
      lr:                RL learning rate (0..1)
      softmax_beta:      RL inverse temperature (scaled internally by 10)
      wm_precision0:     Baseline WM precision multiplier (>=0; scales WM inverse temperature)
      wm_prec_slope:     How much WM precision degrades per added item beyond 3 (>=0)
      arb_temp:          Arbitration sensitivity to relative uncertainty (>=0)
      wm_decay:          Leak of WM toward uniform on each visit (0..1)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_precision0, wm_prec_slope, softmax_beta, arb_temp, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (used as base; we scale it by precision)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision (larger set -> lower precision)
        load_excess = max(0, nS - 3)
        wm_precision = wm_precision0 / (1.0 + wm_prec_slope * load_excess)
        wm_precision = max(0.0, wm_precision)
        beta_wm_eff = softmax_beta_wm * wm_precision

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (treat W_s as utilities; higher at remembered action)
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (wm_logits - wm_logits[a])))

            # Compute state-specific uncertainty (Shannon entropy) for arbitration
            # RL entropy from softmax over Q_s
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs /= np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1)))

            # WM entropy from softmax over W_s with beta_wm_eff
            wm_probs = np.exp(beta_wm_eff * (W_s - np.max(W_s)))
            wm_probs /= np.sum(wm_probs)
            H_wm = -np.sum(wm_probs * np.log(np.clip(wm_probs, 1e-12, 1)))

            # Arbitration: if WM is more precise (lower entropy), weight increases
            # wm_weight = sigmoid(arb_temp * (H_rl - H_wm))
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_temp * (H_rl - H_wm)))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: leak toward uniform, then reward-based sharpening
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Storage strength increases with precision (coupling memory quality and load)
                store_strength = 1.0 - np.exp(-wm_precision)
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                # Renormalize to a probability distribution
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Set-sizeâ€“specific RL learning rates + utility-boosted WM with load-driven noise.
    - RL: two learning rates depending on set size (small vs large).
    - WM: stores the last rewarded action per state as a utility boost; subject to leak.
    - Policy: WM uses its utilities with a high temperature but suffers load-driven noise.
      Arbitration weight depends on predicted WM reliability from set size.

    Parameters:
      lr_small:       RL learning rate when set size <= 3 (0..1)
      lr_large:       RL learning rate when set size >= 6 (0..1)
      softmax_beta:   RL inverse temperature (scaled internally by 10)
      wm_util_bonus:  Additive boost to the WM utility for last rewarded action (>=0)
      noise_scale:    Scales the degradation of WM reliability with set size (>=0)
      wm_decay:       Leak of WM traces toward uniform on each visit (0..1)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr_small, lr_large, softmax_beta, wm_util_bonus, noise_scale, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Choose RL learning rate by load
        lr = lr_small if nS <= 3 else lr_large

        # Predicted WM reliability decreases with set size (converted to mixture weight)
        # wm_weight_eff in [0,1], larger sets -> lower weight
        wm_weight_eff = 1.0 / (1.0 + noise_scale * max(0, nS - 3))

        # Effective WM temperature also degrades with load-driven noise
        beta_wm_eff = softmax_beta_wm * wm_weight_eff

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM utilities: boost the action that WM currently favors (if any)
            wm_utils = W_s.copy()
            # Convert a "probability-like" trace into utilities by adding a bonus to the current argmax
            a_star = np.argmax(W_s)
            wm_utils[a_star] += wm_util_bonus
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (wm_utils - wm_utils[a])))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: leak toward uniform, then if rewarded, shift probability mass to chosen action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Storage rule: move a fraction of mass to the chosen action
                store_strength = 1.0 - (1.0 - wm_decay) ** 2
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Capacity-limited WM with interference and lapse.
    - RL: single-rate Q-learning.
    - WM: effective availability scales with capacity relative to set size; susceptible to
      between-item interference as set size grows.
    - Policy: mixture of WM and RL plus an overall lapse rate to uniform choice.

    Parameters:
      lr:                RL learning rate (0..1)
      softmax_beta:      RL inverse temperature (scaled internally by 10)
      wm_capacity:       Effective WM capacity in number of states (>=0)
      interference:      Additional reduction of WM influence per item beyond capacity (>=0)
      lapse:             Lapse probability to choose uniformly at random (0..1)
      wm_decay:          Leak of WM traces toward uniform on each visit (0..1)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_capacity, interference, lapse, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM availability based on capacity vs load
        cap_ratio = wm_capacity / max(1.0, nS)
        cap_ratio = np.clip(cap_ratio, 0.0, 1.0)
        # Interference penalty grows with load beyond capacity
        overload = max(0.0, nS - wm_capacity)
        wm_weight_eff = cap_ratio / (1.0 + interference * overload)

        # Effective WM inverse temperature scales with availability
        beta_wm_eff = softmax_beta_wm * wm_weight_eff

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy using load-attenuated temperature
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (wm_logits - wm_logits[a])))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: leak toward uniform, then reward-constrained strengthening
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Storage strength tied to capacity ratio (more capacity -> stronger storage)
                store_strength = cap_ratio
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p