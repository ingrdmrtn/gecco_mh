def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Gated one-shot WM with load-dependent gating and lapse.

    Idea:
    - RL learns state-action values with softmax choice.
    - WM stores a one-shot perfect association upon rewarded trials, but only if a gate opens.
    - The WM gate opens with a probability that decreases with load (set size).
    - WM policy is near-deterministic but subject to a lapse that increases with load.
    - Arbitration mixes WM and RL policies based on whether a strong memory is present and the gate strength.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - wm_gate_bias: real, baseline logit controlling WM gate opening.
      Effective gate prob: g = sigmoid(wm_gate_bias - wm_gate_load*load_scaled).
    - wm_gate_load: float >= 0, how strongly load reduces WM gate opening.
    - wm_lapse: float >= 0, baseline WM lapse; effective lapse increases with load:
      lapse_eff = clip(wm_lapse * (1 + load_scaled), 0, 0.5).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_gate_bias, wm_gate_load, wm_lapse = model_parameters
    softmax_beta *= 10.0  # RL has higher upper bound
    softmax_beta_wm = 50.0  # WM is very deterministic absent lapses
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        load_scaled = max(0.0, (nS - 3) / 3.0)  # 0 for set size 3, 1 for set size 6
        gate_prob = 1.0 / (1.0 + np.exp(-(wm_gate_bias - wm_gate_load * load_scaled)))
        lapse_eff = min(0.5, max(0.0, wm_lapse * (1.0 + load_scaled)))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY
            # Near-deterministic softmax from WM map, with lapse toward uniform that increases with load
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            wm_probs = (1.0 - lapse_eff) * wm_probs + lapse_eff * (1.0 / nA)
            p_wm = wm_probs[a]

            # Arbitration: use WM when a strong memory exists at this state (i.e., high peak), scaled by gate strength
            strong_memory = 1.0 if np.max(W_s) >= 0.99 else 0.0
            wm_weight = gate_prob * strong_memory

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL UPDATE
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WORKING MEMORY UPDATE
            # Passive forgetting toward uniform (small, tied to lapse)
            w[s, :] = (1.0 - lapse_eff) * w[s, :] + lapse_eff * w_0[s, :]

            # On rewarded trials, with probability gate_prob, store a one-shot correct mapping
            # (here we implement expected update by scaling toward one-hot by gate_prob)
            if r > 0.0:
                # Expected effect of a Bernoulli gate: move W_s toward one-hot with weight = gate_prob
                w[s, :] = (1.0 - gate_prob) * w[s, :] + gate_prob * w_0[s, :]
                w[s, a] = (1.0 - gate_prob) * w[s, a] + gate_prob * 1.0

            # Normalize to a proper distribution to avoid drift
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent Q-forgetting + WM as a learned probabilistic map with confidence-based arbitration.

    Idea:
    - RL: standard Q-learning with softmax, plus forgetting of Q-values toward uniform that grows with load.
    - WM: probabilistic map W_s over actions, learned via reward-prediction updates (Hebbian flavor).
    - Arbitration: WM weight increases with WM confidence (max probability in W_s),
      transformed by a slope parameter; effective WM confidence naturally reduces under high load via decay.
    - WM policy is near-deterministic softmax from W_s.
    - RL learning rate is fixed (lr_base); WM has its own learning rate (lr_boost).

    Parameters (model_parameters):
    - lr_base: float in [0,1], RL learning rate.
    - lr_boost: float in [0,1], WM learning rate for updating W (also affects speed of sharpening).
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - wm_slope: real, slope controlling sensitivity of arbitration to WM confidence:
      wm_weight = sigmoid(wm_slope * (conf - 1/nA)), conf = max_a P_WM(a|s).
    - decay_base: float in [0,1], baseline forgetting of both RL Q and WM towards uniform each trial.
    - decay_load: float >= 0, additional forgetting per unit load (0 for nS=3; 1 for nS=6).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_base, lr_boost, softmax_beta, wm_slope, decay_base, decay_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load_scaled = max(0.0, (nS - 3) / 3.0)

        # Effective decay per visit for both Q and W
        decay_eff = min(1.0, max(0.0, decay_base + decay_load * load_scaled))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Arbitration weight from WM confidence (max prob)
            conf = float(np.max(wm_probs))
            wm_weight = 1.0 / (1.0 + np.exp(-wm_slope * (conf - 1.0 / nA)))
            # Soft load penalty implicit via decay_eff lowering conf; keep weight in [0,1]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL UPDATE with forgetting
            delta = r - Q_s[a]
            q[s, a] += lr_base * delta
            # Forgetting of Q towards uniform baseline
            q[s, :] = (1.0 - decay_eff) * q[s, :] + decay_eff * w_0[s, :]

            # WM UPDATE with Hebbian sharpening and forgetting
            # Move chosen action prob toward r (1 if rewarded, 0 if not), then renormalize
            w[s, a] += lr_boost * (r - W_s[a])
            # Push non-chosen actions slightly toward 1 - r to maintain normalization tendency
            if nA > 1:
                others = [aa for aa in range(nA) if aa != a]
                adjust = lr_boost * ((1.0 - r) / (nA - 1))
                w[s, others] += adjust - lr_boost * (W_s[others] - w_0[s, others])
            # Apply forgetting toward uniform
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]
            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent interference between states.

    Idea:
    - RL: standard Q-learning with softmax.
    - WM: each state's mapping can be corrupted at retrieval by interference from other states,
           which increases with load (set size). This blurs WM policy when many items must be held.
    - Arbitration: fixed WM propensity modulated by interference (less WM influence under higher interference).
    - WM temperature also increases with load (i.e., less precise retrieval).

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - wm_strength: real, base logit for WM mixture weight; weight = sigmoid(wm_strength) reduced by interference.
    - interference: float >= 0, scales retrieval interference with load:
      i_eff = clip(interference * load_scaled, 0, 1).
    - wm_temp: float >= 0, adds temperature to WM under load:
      beta_wm_eff = softmax_beta_wm / (1 + wm_temp * load_scaled).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_strength, interference, wm_temp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load_scaled = max(0.0, (nS - 3) / 3.0)

        i_eff = min(1.0, max(0.0, interference * load_scaled))
        base_wm_weight = 1.0 / (1.0 + np.exp(-wm_strength))  # in (0,1)
        # Reduce WM influence proportionally to interference
        wm_weight_global = base_wm_weight * (1.0 - i_eff)
        beta_wm_eff = softmax_beta_wm / (1.0 + wm_temp * load_scaled)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY with interference-based retrieval:
            # Blend the current state's memory with the average of other states.
            if nS > 1:
                other_idx = [ss for ss in range(nS) if ss != s]
                mean_other = np.mean(w[other_idx, :], axis=0)
                W_retrieved = (1.0 - i_eff) * W_s + i_eff * mean_other
            else:
                W_retrieved = W_s

            # Softmax with load-inflated temperature
            wm_logits = beta_wm_eff * (W_retrieved - np.max(W_retrieved))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Arbitration: global weight reduced by interference
            wm_weight = wm_weight_global

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL UPDATE
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM UPDATE
            # Reward-driven sharpening toward one-hot for the chosen action; mild interference-induced decay
            # Sharpen toward 1 for rewarded, toward 0 for non-rewarded choice, then normalize
            eta_wm = lr  # reuse lr as WM plasticity
            w[s, a] += eta_wm * (r - W_s[a])
            # Interference-based erosion toward uniform each trial
            w[s, :] = (1.0 - 0.5 * i_eff) * w[s, :] + 0.5 * i_eff * w_0[s, :]
            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p