def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity scaling and uncertainty arbitration.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: fast associative cache for state->action with near-deterministic policy.
    - Arbitration: WM weight increases when RL is uncertain (high softmax entropy)
      and decreases with higher set size (capacity dilution).
    - WM capacity: mixture weight is scaled down when set size exceeds an
      effective capacity proportional to slots_prop.

    Parameters
    ----------
    model_parameters : (lr, base_mix, softmax_beta, slots_prop, conf_boost)
        lr : float
            RL learning rate (0-1).
        base_mix : float
            Base logit for WM mixture; transformed by sigmoid internally. Center point.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        slots_prop : float
            Proportion of 3-item capacity allocated to WM (>=0). Effective capacity is 3*slots_prop.
            The WM contribution scales by min(1, capacity/nS).
        conf_boost : float
            Sensitivity of WM engagement to RL entropy (>=0). Higher means more WM when RL is uncertain.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, base_mix, softmax_beta, slots_prop, conf_boost = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice probability for chosen action a
            p_wm_core = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL uncertainty via softmax entropy (higher => more uncertain)
            # Compute full RL policy to get entropy
            logits = softmax_beta * Q_s
            logits -= np.max(logits)
            pi = np.exp(logits)
            pi /= np.sum(pi)
            entropy = -np.sum(pi * (np.log(pi + 1e-12)))

            # Capacity scaling: reduce WM impact when nS exceeds capacity
            capacity = 3.0 * max(0.0, slots_prop)
            cap_scale = min(1.0, capacity / max(1.0, float(nS)))

            # Mixture weight from base logit plus uncertainty boost, passed through sigmoid
            mix_logit = base_mix + conf_boost * entropy
            wm_weight_eff = 1.0 / (1.0 + np.exp(-mix_logit))
            wm_weight_eff *= cap_scale
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = p_wm_core * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating:
            # - If rewarded, move W_s toward one-hot on chosen action (store).
            # - If not rewarded, slight decay toward uniform (unlearn noise).
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong caching toward chosen action
                w[s, :] = 0.7 * W_s + 0.3 * one_hot
            else:
                # Mild decay to baseline when error
                w[s, :] = 0.95 * W_s + 0.05 * w_0[s, :]

            # Normalize to a proper distribution
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recency-weighted decay and set-size-dependent RL temperature.

    Idea
    - RL: Q-learning with an inverse temperature that decreases with set size (less precise under load).
    - WM: rapidly formed associative distribution per state that decays with time since last seen.
    - Arbitration: fixed base WM mixture, modulated by recency (more WM when state was seen recently).

    Parameters
    ----------
    model_parameters : (lr, wm_base, beta_base, beta_ss_cost, tau_decay)
        lr : float
            RL learning rate (0-1).
        wm_base : float
            Base WM mixture weight (0-1).
        beta_base : float
            Base RL inverse temperature (scaled by 10 internally).
        beta_ss_cost : float
            Set-size cost on RL precision (>=0). Effective beta = beta_base / (1 + beta_ss_cost * ((nS-3)/3)).
        tau_decay : float
            WM time constant (in trials) for decay and arbitration recency modulation (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_base, beta_base, beta_ss_cost, tau_decay = model_parameters
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Size-dependent RL temperature
        softmax_beta = beta_base * 10.0
        ss_factor = 1.0 + beta_ss_cost * max(0.0, (float(nS) - 3.0) / 3.0)
        softmax_beta /= ss_factor

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply time-since-last-seen decay to WM row before using it
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
            else:
                dt = np.inf
            # Decay factor toward uniform
            if np.isfinite(dt):
                decay = 1.0 - np.exp(-dt / max(1e-6, tau_decay))
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability for chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability for chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Recency-based WM weight: more WM when state is recent
            if np.isfinite(dt):
                recency = np.exp(-dt / max(1e-6, tau_decay))
            else:
                recency = 0.0
            wm_weight_eff = np.clip(wm_base * recency, 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If rewarded, bias strongly toward chosen action.
            # - If not, gentle pull toward uniform (error weakens trace).
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.6 * W_s + 0.4 * one_hot
            else:
                w[s, :] = 0.97 * W_s + 0.03 * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

            # Update last seen time for state s
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with block-progress gating and WM learning.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: policy distribution per state that learns quickly from reward and slowly otherwise.
    - Arbitration: WM influence starts high and decays with within-block progress,
      and is further reduced under high set size (3/nS factor).

    Parameters
    ----------
    model_parameters : (lr, wm_start, softmax_beta, progress_decay, wm_learn)
        lr : float
            RL learning rate (0-1).
        wm_start : float
            Initial WM mixture weight at the start of a block (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        progress_decay : float
            Exponential decay rate of WM reliance across trials within a block (>=0).
        wm_learn : float
            WM learning rate toward the target distribution on each trial (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_start, softmax_beta, progress_decay, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        T_block = len(block_states)
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Within-block progress in [0,1]
            progress = 0.0
            if T_block > 1:
                progress = t / float(T_block - 1)

            # WM mixture decays with progress and with set size
            wm_weight_eff = wm_start * np.exp(-progress_decay * progress) * (3.0 / max(3.0, float(nS)))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update toward a target distribution:
            # - If reward: target is one-hot on chosen action.
            # - If no reward: mild regression to uniform (uncertain mapping).
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]

            w[s, :] = (1.0 - wm_learn) * W_s + wm_learn * target

            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p