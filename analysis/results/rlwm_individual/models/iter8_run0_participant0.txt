def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-gated WM precision and fast Hebbian updates.

    Mechanism:
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: a fast, one-shot-like Hebbian cache per state that stores action preferences.
      WM softmax precision is reduced under higher load (set size), via gate_beta.
    - Policy: mixture of RL and WM with fixed wm_weight (template).
    - WM dynamics: fast learning toward the chosen action on rewarded trials (wm_eta),
      and global decay toward uniform each trial amplified by load (wm_decay).

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_weight: [0,1] mixture weight of WM in the policy (1=all WM, 0=all RL).
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - gate_beta: >=0, reduces WM softmax precision as load increases.
    - wm_eta: [0,1], fast WM update toward the chosen action on each trial,
              weighted by reward (reward-gated Hebbian update).
    - wm_decay: [0,1], base decay of WM toward uniform each trial, amplified by load.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_beta, wm_eta, wm_decay = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic (will be downscaled by load)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Overload factor: 0 when nS=1; increases with set size
        overload = float(nS - 1) / max(1.0, float(nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :].copy()
            # WM precision is reduced under load via gate_beta
            beta_wm_eff = softmax_beta_wm / (1.0 + gate_beta * overload)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global WM decay toward uniform, amplified by load
            d = np.clip(wm_decay * (0.5 + 0.5 * overload), 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # Reward-gated Hebbian WM update toward chosen action
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - wm_eta * r) * w[s, :] + (wm_eta * r) * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with chunking interference across states and load-dependent drift.

    Mechanism:
    - RL: standard delta-rule with softmax (beta scaled by 10).
    - WM store: per-state action distribution w[s,:] approximating best action.
      On reward, the chosen action is consolidated; on non-reward, a mild
      redistribution occurs away from the chosen action.
    - Chunking interference: under higher load, WM for a state s is contaminated
      by the average of other states' WM (simulating imperfect individuation).
      This is controlled by chunking parameter and increases with set size.
    - Drift: global decay toward uniform, stronger under high load.
    - Policy: mixture of RL and WM with fixed wm_weight.

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_weight: [0,1] mixture weight of WM in the policy.
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - chunking: [0,1], degree that WM rows mix with the across-state mean under load.
    - decay: [0,1], base drift of WM toward uniform each trial.
    - noise_wm: >=0, adds temperature to WM policy (lower precision with larger value).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, chunking, decay, noise_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base WM determinism before noise
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load factor in [0,1], 0 for nS=1, approaches 1 as nS grows
        load = float(nS - 1) / max(1.0, float(nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # Apply chunking interference to WM representation before policy
            W_s = w[s, :].copy()
            mean_other = np.mean(w, axis=0) if nS <= 1 else (np.sum(w, axis=0) - W_s) / max(1, nS - 1)
            ch = np.clip(chunking * load, 0.0, 1.0)
            W_eff = (1.0 - ch) * W_s + ch * mean_other

            # Add WM noise via reduced precision
            beta_wm_eff = softmax_beta_wm / (1.0 + noise_wm)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_eff - W_eff[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global decay toward uniform, scaled by load
            d = np.clip(decay * (0.5 + 0.5 * load), 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # WM consolidation with reward, redistribution otherwise
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                # Consolidate chosen action
                w[s, :] = 0.8 * w[s, :] + 0.2 * one_hot
            else:
                # Reduce confidence in chosen action, slight boost to others
                reduce = 0.15
                w[s, a] = (1.0 - reduce) * w[s, a]
                redistribute = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = w[s, aa] + redistribute

            # Renormalize row to be a probability distribution (stability)
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with WM-informed meta-control of RL exploration and WM consolidation.

    Mechanism:
    - RL: delta-rule with an effective inverse temperature scaled by WM confidence.
      When WM is confident and load is low, RL becomes more exploitative.
    - WM: fast store updated with wm_alpha; decays to uniform modestly each trial.
    - Policy: mixture of RL and WM (fixed wm_weight).
    - Meta-control: RL softmax beta is modulated as
        beta_eff = base_beta * (1 + beta_gain * WM_confidence * (1 - overload)),
      where WM_confidence is the gap between the highest and second-highest WM
      weights for the current state.

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_weight: [0,1] mixture weight of WM in the policy.
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - beta_gain: >=0, scales how much WM confidence boosts RL beta (less exploration).
    - wm_success_decay: [0,1], decay of WM toward uniform each trial.
    - wm_alpha: [0,1], WM learning rate toward one-hot of chosen action, reward-gated.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, beta_gain, wm_success_decay, wm_alpha = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        overload = float(nS - 1) / max(1.0, float(nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute WM confidence for current state (gap between top-1 and top-2)
            W_s_full = w[s, :].copy()
            sorted_w = np.sort(W_s_full)[::-1]
            if len(sorted_w) >= 2:
                wm_conf = max(0.0, sorted_w[0] - sorted_w[1])
            else:
                wm_conf = 0.0

            # RL beta modulated by WM confidence and load
            beta_eff = softmax_beta * (1.0 + beta_gain * wm_conf * (1.0 - overload))

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy from current row
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            d = np.clip(wm_success_decay, 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # WM learning toward chosen action, reward-gated
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - wm_alpha * r) * w[s, :] + (wm_alpha * r) * one_hot

        blocks_log_p += log_p

    return -blocks_log_p