def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + WM lapse driven by set size via logistic load.

    Policy:
    - Action probability is a mixture of RL softmax and WM softmax.
    - RL uses separate learning rates for rewarded vs. unrewarded outcomes (lr_pos, lr_neg).
    - WM forms near-deterministic associations when rewarded; on trials, WM is subject to a
      load-driven lapse that increases with set size (logistic with slope wm_binding and threshold load_thresh).

    Parameters:
    - model_parameters[0]: lr_pos (float in [0,1]) RL learning rate for positive outcomes (reward=1).
    - model_parameters[1]: lr_neg (float in [0,1]) RL learning rate for negative outcomes (reward=0).
    - model_parameters[2]: wm_weight_base (float in [0,1]) Baseline mixture weight for WM.
    - model_parameters[3]: softmax_beta (float >= 0) RL inverse temperature (internally scaled by 10).
    - model_parameters[4]: wm_binding (float >= 0) Controls WM binding strength and load-lapse slope.
    - model_parameters[5]: load_thresh (float) Set-size threshold at which WM lapses are 50%.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, wm_binding, load_thresh = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-driven WM lapse increases with set size via logistic
        lapse = 1.0 / (1.0 + np.exp(-wm_binding * (float(nS) - load_thresh)))
        lapse = float(np.clip(lapse, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (chosen-action probability via softmax normalization trick)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-driven lapse to uniform
            W_s = w[s, :]
            p_wm_clean = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse) * p_wm_clean + lapse * (1.0 / nA)

            # Mixture
            wm_weight = float(np.clip(wm_weight_base, 0.0, 1.0))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with valence-asymmetric learning
            pe = r - q[s, a]
            alpha = lr_pos if r > 0.5 else lr_neg
            q[s, a] += alpha * pe

            # WM update:
            # - If rewarded, strongly bind chosen action (peaked distribution).
            # - If not rewarded, gently relax toward uniform baseline.
            if r > 0.5:
                eps = 1e-6
                w[s, :] = eps
                w[s, a] = 1.0 - (nA - 1) * eps
            else:
                # Relaxation rate tied to wm_binding (stronger binding -> faster forgetting when incorrect)
                relax = 1.0 - np.exp(-max(0.0, wm_binding))
                relax = float(np.clip(relax, 0.0, 1.0))
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with entropy-based arbitration + decaying WM store.

    Policy:
    - Action probability is a mixture of RL and WM policies.
    - Arbitration uses the difference in uncertainty (entropy) between RL and WM:
      wm_weight_eff = sigmoid(arb_bias + arb_slope * (H_rl - H_wm)) scaled by wm_strength.
      If WM is more certain (lower entropy), the mixture leans toward WM, and vice versa.
    - WM decays toward uniform when not reinforced; when rewarded, WM learns toward a one-hot.

    Parameters:
    - model_parameters[0]: lr (float in [0,1]) RL learning rate.
    - model_parameters[1]: softmax_beta (float >= 0) RL inverse temperature (internally scaled by 10).
    - model_parameters[2]: wm_strength (float in [0,1]) Scales the arbitration weight and WM learning.
    - model_parameters[3]: wm_decay (float in [0,1]) Trialwise decay of WM toward uniform when not reinforced.
    - model_parameters[4]: arb_slope (float) Slope controlling sensitivity to entropy difference.
    - model_parameters[5]: arb_bias (float) Bias term in the arbitration logistic.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_strength, wm_decay, arb_slope, arb_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice prob for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Full RL policy distribution for entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))

            # WM policy prob and entropy
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]
            H_wm = -np.sum(wm_probs * np.log(np.clip(wm_probs, 1e-12, 1.0)))

            # Entropy-based arbitration, scaled by wm_strength
            arb_arg = arb_bias + arb_slope * (H_rl - H_wm)
            gate = 1.0 / (1.0 + np.exp(-arb_arg))
            wm_weight = float(np.clip(wm_strength * gate, 0.0, 1.0))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: decay toward uniform, reinforce on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # Learn toward one-hot with rate tied to wm_strength
                alpha_wm = 1.0 - np.exp(-max(0.0, wm_strength))
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with power-law load arbitration and WM recency-dependent retrieval noise.

    Policy:
    - Mixture of RL and WM policies.
    - WM mixture weight decreases with set size via a power law:
        wm_weight_eff = wm_weight_base / (1 + max(0, nS-1) ** eta_power).
    - WM retrieval noise increases with time since last visit to the state:
        noise = 1 - exp(-Î”t / recency_tau),
      so recent states are recalled more accurately from WM.
    - WM updates to a sharp association on reward; otherwise gently relaxes.

    Parameters:
    - model_parameters[0]: lr (float in [0,1]) RL learning rate.
    - model_parameters[1]: wm_weight_base (float in [0,1]) Base WM weight at minimal load.
    - model_parameters[2]: softmax_beta (float >= 0) RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: recency_tau (float > 0) Time constant controlling WM retrieval noise vs. recency.
    - model_parameters[4]: eta_power (float >= 0) Exponent controlling how load reduces WM contribution.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, recency_tau, eta_power = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last visit time for each state within the block
        last_time = -np.ones(nS, dtype=int)

        # Power-law load arbitration
        load_factor = (max(0, nS - 1)) ** max(0.0, eta_power)
        wm_weight_load = wm_weight_base / (1.0 + load_factor)
        wm_weight_load = float(np.clip(wm_weight_load, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with recency-dependent retrieval noise
            W_s = w[s, :]
            p_wm_clean = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute recency noise based on time since last visitation of state s
            if last_time[s] < 0:
                delta_t = 1e9  # effectively never seen -> high noise
            else:
                delta_t = max(0, t - last_time[s])
            # Convert recency_tau into rate; protect against nonpositive values
            tau = max(1e-6, float(recency_tau))
            noise = 1.0 - np.exp(-float(delta_t) / tau)
            noise = float(np.clip(noise, 0.0, 1.0))
            p_wm = (1.0 - noise) * p_wm_clean + noise * (1.0 / nA)

            # Mixture with load-adjusted WM weight
            p_total = wm_weight_load * p_wm + (1.0 - wm_weight_load) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update
            if r > 0.5:
                eps = 1e-6
                w[s, :] = eps
                w[s, a] = 1.0 - (nA - 1) * eps
            else:
                relax = 0.05
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

            # Update last visit time
            last_time[s] = t

        blocks_log_p += log_p

    return -blocks_log_p