def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size–dependent interference and recency-based WM imprint.

    Idea
    - Choices arise from a fixed mixture of RL and WM policies.
    - WM traces suffer global interference that increases with set size (harder with 6).
    - WM also emphasizes recent correct associations via a recency-controlled imprint.
    - RL updates via a standard delta rule.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight: Baseline mixture weight for WM policy (0..1).
        - softmax_beta: Base RL inverse temperature (scaled internally by 10).
        - interference_rate: Base global WM interference (0..1 per trial).
        - recency_gain: Strength of WM imprint for the chosen action in the current state (0..1).
        - size_penalty: How much interference grows with set size (>0 implies more interference at size 6).

    Set-size impact
    - WM interference per trial: decay = clip(interference_rate * (1 + size_penalty*(nS-3)), 0, 1).
      So larger nS elicits stronger drift of WM toward uniform.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, interference_rate, recency_gain, size_penalty = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent interference (constant within block)
        decay = interference_rate * (1.0 + size_penalty * (nS - 3))
        decay = np.clip(decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global interference toward uniform
            w = (1.0 - decay) * w + decay * w_0
            # Recency-based imprint in current state (only if rewarded)
            w[s, :] = (1.0 - recency_gain) * w[s, :]
            w[s, a] += recency_gain * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with action-level choice bias and set-size–modulated arbitration.

    Idea
    - Choices are a mixture of RL and WM policies.
    - RL policy is augmented by a dynamic action-bias signal capturing choice persistence/exploration patterns.
    - WM updates via a fast imprint (driven by wm_weight) without extra parameters.
    - Arbitration (wm_weight_t) is reduced in larger set sizes via a size_bias term.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight: Baseline WM weight (0..1), later transformed by size_bias.
        - softmax_beta: Base RL inverse temperature (scaled internally by 10).
        - bias_decay: Per-trial decay of action bias toward zero (0..1).
        - bias_gain: Increment to the chosen action's bias each trial (>0).
        - size_bias: Scales the effect of set size on WM arbitration; positive favors WM at nS=3 over nS=6.

    Set-size impact
    - wm_weight_t = sigmoid(logit(wm_weight) + size_bias * (3.5 - nS)),
      increasing WM reliance at small set size if size_bias > 0.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, bias_decay, bias_gain, size_bias = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global action bias (not state-specific)
        bias = np.zeros(nA)

        # Precompute WM imprint strength as a function of wm_weight_base
        wm_imprint = np.clip(0.5 + 0.5 * wm_weight_base, 0.0, 1.0)

        # Set-size–modulated arbitration weight
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        def logit(p):
            p = np.clip(p, eps, 1.0 - eps)
            return np.log(p / (1.0 - p))

        wm_weight_t = sigmoid(logit(wm_weight_base) + size_bias * (3.5 - nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with action bias (additive on logits)
            # Implemented by offsetting Q_s with bias before using template-style denominator
            Qb = Q_s + (bias / np.maximum(softmax_beta, eps))
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Qb - Qb[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with set-size–modulated arbitration
            p_total = p_wm * wm_weight_t + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Mild drift to uniform to avoid saturation
            w = (1.0 - 0.05) * w + 0.05 * w_0
            # Imprint reward association in WM for current state
            w[s, :] = (1.0 - wm_imprint) * w[s, :]
            w[s, a] += wm_imprint * r

            # Update action bias (global)
            bias = (1.0 - bias_decay) * bias
            bias[a] += bias_gain

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise-based arbitration and size penalty.

    Idea
    - Arbitration between RL and WM depends on surprise (|r - Q_s[a]|) and set size:
      more reliance on WM when outcomes are unsurprising (stable) and set size is small.
    - WM has an explicit learning rate and a surprise-coupled decay (forget more when surprising).
    - RL is standard delta-rule.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Number of unique states in the current block for each trial (3 or 6).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight: Baseline WM weight (0..1) before surprise/size modulation.
        - softmax_beta: Base RL inverse temperature (scaled internally by 10).
        - wm_learn: WM learning/imprint strength (0..1).
        - surprise_gain: Scales both arbitration shift and WM decay with surprise (>=0).
        - size_gain: Scales arbitration shift with set size; positive favors WM more at nS=3.

    Set-size impact
    - Arbitration weight increases/decreases with (3.5 - nS) via size_gain.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_learn, surprise_gain, size_gain = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    # Helper functions
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, eps, 1.0 - eps)
        return np.log(p / (1.0 - p))

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (per template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise and set size driven arbitration
            surprise = np.abs(r - Q_s[a])  # 0..1 for binary rewards with Q in [0,1]
            wm_logit = logit(wm_weight_base) + size_gain * (3.5 - nS) + surprise_gain * (0.5 - surprise)
            wm_weight_t = sigmoid(wm_logit)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Surprise-coupled decay toward uniform (forget more when surprising)
            decay_t = np.clip(surprise_gain * surprise, 0.0, 1.0)
            w = (1.0 - decay_t) * w + decay_t * w_0
            # WM learning for current state/action
            w[s, :] = (1.0 - wm_learn) * w[s, :]
            w[s, a] += wm_learn * r

        blocks_log_p += log_p

    return -blocks_log_p