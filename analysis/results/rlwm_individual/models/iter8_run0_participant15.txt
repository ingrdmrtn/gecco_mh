def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (Pearce-Hall adaptive learning) + WM with set-size-dependent gating and decay.

    Mechanism
    - RL: tabular Q-learning with a state-specific Pearce-Hall learning rate that
      tracks recent unsigned prediction error: alpha_s <- (1-kappa_ph)*alpha_s + kappa_ph*|delta|.
      Q update: Q[s,a] <- Q[s,a] + alpha_s*delta.
    - WM: a probability table over actions per state. When rewarded, a gating mechanism
      moves the WM distribution toward a one-hot code for the chosen action; on non-rewarded
      trials, WM decays toward uniform.
    - Arbitration: fixed mixture between WM and RL policies (convex combination).
    - Set-size dependence: the WM gate is stronger in small sets and weaker in large sets via a logistic
      function of set size: gate = sigmoid(gate_bias + gate_size_slope*(nS-3)).

    Parameters
    ----------
    model_parameters : tuple
        lr0 : float
            Baseline learning-rate anchor for RL (0..1). Scales the adaptive rate.
        kappa_ph : float
            Pearce-Hall tracking rate for unsigned PE (0..1). Higher -> more responsive alpha.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        gate_bias : float
            Baseline logit for WM gating on rewarded trials (can be negative or positive).
        gate_size_slope : float
            Slope controlling how gate changes with set size (positive -> weaker gate in size 6).
        wm_decay : float
            WM decay rate toward uniform on non-rewarded trials (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr0, kappa_ph, softmax_beta, gate_bias, gate_size_slope, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # state-specific Pearce-Hall alphas initialized to lr0
        alpha_s = lr0 * np.ones(nS)

        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size dependent WM gate strength (0..1 via sigmoid)
        gate = 1.0 / (1.0 + np.exp(-(gate_bias + gate_size_slope * (nS - 3))))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Fixed arbitration between WM and RL
            # Here the WM mixture weight is implicitly the gate's effect on the WM table;
            # arbitration keeps a 50-50 blend to avoid over-parameterization.
            wm_weight = 0.5
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall adaptive alpha
            delta = r - Q_s[a]
            alpha_s[s] = (1.0 - kappa_ph) * alpha_s[s] + kappa_ph * abs(delta)
            eff_alpha = np.clip(alpha_s[s], 0.0, 1.0) * lr0
            q[s][a] += eff_alpha * delta

            # WM update
            if r > 0.5:
                # Move W toward one-hot on rewarded trials with strength = gate
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - gate) * w[s, :] + gate * one_hot
            else:
                # Decay toward uniform when not rewarded
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Renormalize and floor
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-modulated inverse temperature and WM decay; fixed arbitration.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM: probability table per state; rewarded trials store a one-hot distribution; non-rewarded decay toward uniform.
    - Arbitration: fixed wm_weight blending WM and RL policies.
    - Set-size dependence:
        - RL inverse temperature is reduced as set size increases:
          beta_eff = softmax_beta * exp(-beta_size_slope * (nS - 3)).
        - WM decay increases with set size via wm_decay_eff = 1 - (1 - wm_decay)^(nS/3).

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Mixture weight on WM policy (0..1).
        softmax_beta : float
            Baseline RL inverse temperature; internally scaled by 10 before size modulation.
        beta_size_slope : float
            Positive values decrease inverse temperature in larger sets (0..+inf typical).
        wm_decay : float
            Baseline WM decay rate on non-rewarded trials (0..1); scaled up with set size.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, beta_size_slope, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size modulations
        beta_eff = softmax_beta * np.exp(-beta_size_slope * (nS - 3))
        wm_decay_eff = 1.0 - (1.0 - wm_decay) ** (nS / 3.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with size-modulated beta
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size penalty on WM reliability.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM: probability table per state; rewarded trials store a one-hot distribution; non-rewarded decay toward uniform.
    - Arbitration: trial-wise WM weight is a logistic function of (negative) RL uncertainty and set size:
        wm_weight_t = sigmoid(wm_base + size_gain*(3 - nS) - u_s),
      where u_s is a running estimate of RL uncertainty (unsigned PE EMA per state).
      Higher uncertainty reduces WM weight; smaller set size increases WM weight.
    - Set-size dependence: enters explicitly through size_gain*(3 - nS).

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_decay : float
            WM decay toward uniform on non-rewarded trials (0..1).
        tau_uncert : float
            Time constant for uncertainty EMA per state (>0). Larger -> slower update.
        size_gain : float
            Sensitivity of WM arbitration to set size. Positive -> favors WM in small sets.
        wm_base : float
            Baseline logit for WM weight (can be negative or positive).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_decay, tau_uncert, size_gain, wm_base = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    # Precompute EMA gain from tau
    tau_uncert = max(tau_uncert, 1e-6)
    ema_gain = 1.0 / tau_uncert
    ema_decay = 1.0 - ema_gain

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Per-state uncertainty (unsigned PE EMA)
        u = np.zeros(nS)

        size_term = size_gain * (3 - nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-based arbitration weight (0..1)
            wm_logit = wm_base + size_term - u[s]
            wm_weight_t = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update uncertainty EMA
            u[s] = ema_decay * u[s] + ema_gain * abs(delta)

            # WM update
            if r > 0.5:
                w[s, :] = tiny
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p