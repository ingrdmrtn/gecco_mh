Below are three standalone cognitive models that follow your template and return the negative log-likelihood of the observed choices. Each model blends tabular RL with a WM component in distinct ways, with explicit mechanisms for how set size (3 vs 6) impacts behavior.

Note: The functions assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise-gated WM encoding and load-amplified WM decay.

    Overview:
    - RL: standard tabular Q-learning with softmax policy (beta scaled by 10).
    - WM policy: a recall mixture between deterministic WM argmax and uniform guessing.
      The recall probability is driven by the current WM "concentration" (how peaked WM is).
    - WM update: decay toward uniform increases with set size, and WM encoding strength
      depends on the magnitude of the reward prediction error (surprise), tempered by load.

    Set-size impact:
    - Larger set size increases WM decay per trial.
    - Larger set size reduces surprise-gated encoding strength.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in policy.
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10.
    - enc_base: float in [0,1], base WM encoding probability per trial.
    - enc_pe_slope: float >= 0, increase of WM encoding with absolute PE (surprise).
    - wm_decay_base: float in [0,1], base WM decay per trial; amplified by set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, enc_base, enc_pe_slope, wm_decay_base = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load factor: 0 at set size 3, 1 at set size 6
        load = max(0.0, (float(nS) - 3.0) / 3.0)

        # Effective WM decay increases with load via compounding
        wm_decay_eff = 1.0 - (1.0 - np.clip(wm_decay_base, 0.0, 1.0)) ** (1.0 + load)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL chosen-action probability
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: recall probability is based on current WM concentration for state s
            W_s = w[s, :]
            max_w = np.max(W_s)
            # Normalize concentration to [0,1], 0 = uniform, 1 = delta
            conc = 0.0 if nA == 1 else np.clip((max_w - 1.0 / nA) / (1.0 - 1.0 / nA), 0.0, 1.0)

            # Deterministic WM softmax chooses argmax with prob ~1
            argmax_w = int(np.argmax(W_s))
            p_wm_recall = 1.0 if argmax_w == a else 0.0
            p_wm_guess = 1.0 / nA

            # Recall probability equals concentration
            p_wm = conc * p_wm_recall + (1.0 - conc) * p_wm_guess

            # Mixture policy
            p_total = p_wm * np.clip(wm_weight, 0.0, 1.0) + (1 - np.clip(wm_weight, 0.0, 1.0)) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Surprise-gated WM encoding tempered by load
            enc_prob = np.clip(enc_base + enc_pe_slope * abs(delta) * (1.0 - load), 0.0, 1.0)
            # Update WM row toward a one-hot on chosen action by enc_prob
            w[s, :] *= (1.0 - enc_prob)
            w[s, a] += enc_prob

            # Normalize row for numerical stability
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent Q decay + WM with interference (intrusions) under load.

    Overview:
    - RL: tabular Q-learning with softmax policy; includes decay of Q toward uniform
      that increases with set size (load).
    - WM policy: recall is based on WM certainty (1 - normalized entropy). Even when recall happens,
      responses can be intruded by other states' WM traces with probability increasing with load.
      Otherwise, a uniform guess is made.
    - WM update: global decay to uniform that increases with load; consolidation on feedback.

    Set-size impact:
    - Larger set size increases decay in RL values (forgetting).
    - Larger set size increases WM decay and increases intrusion from other states.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight of WM in policy.
    - softmax_beta: float >= 0, RL inverse temperature; scaled by 10.
    - q_decay_base: float in [0,1], base RL decay toward uniform per trial; amplified by load.
    - wm_intrusion: float in [0,1], base intrusion strength; effective intrusion scales with load.
    - wm_decay_base: float in [0,1], base WM decay toward uniform; amplified by load.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, q_decay_base, wm_intrusion, wm_decay_base = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        load = max(0.0, (float(nS) - 3.0) / 3.0)

        q_decay_eff = 1.0 - (1.0 - np.clip(q_decay_base, 0.0, 1.0)) ** (1.0 + load)
        wm_decay_eff = 1.0 - (1.0 - np.clip(wm_decay_base, 0.0, 1.0)) ** (1.0 + load)
        intrusion_eff = np.clip(wm_intrusion * load, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM recall probability from certainty (1 - normalized entropy)
            W_s = w[s, :]
            # Normalized entropy in [0,1]
            eps = 1e-12
            H = -np.sum(np.clip(W_s, eps, 1.0) * np.log(np.clip(W_s, eps, 1.0))) / np.log(nA)
            recall_prob = np.clip(1.0 - H, 0.0, 1.0)

            # Intrusion distribution from other states
            if nS > 1:
                other_idx = [i for i in range(nS) if i != s]
                avg_other = np.mean(w[other_idx, :], axis=0)
            else:
                avg_other = w_0[s, :].copy()

            argmax_w = int(np.argmax(W_s))
            p_wm_arg = 1.0 if argmax_w == a else 0.0
            p_wm_intr = avg_other[a]
            p_wm_guess = 1.0 / nA

            # If recall occurs, mixture of correct WM row and intrusions; else guess
            p_wm = recall_prob * ((1.0 - intrusion_eff) * p_wm_arg + intrusion_eff * p_wm_intr) + (1.0 - recall_prob) * p_wm_guess

            p_total = p_wm * np.clip(wm_weight, 0.0, 1.0) + (1.0 - np.clip(wm_weight, 0.0, 1.0)) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL Q-learning update then decay toward uniform
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            q[s, :] = (1.0 - q_decay_eff) * q[s, :] + q_decay_eff * (1.0 / nA)

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM consolidation: reward strengthens chosen action; no-reward weakly suppresses
            alpha = 0.6 * r + 0.2 * (1.0 - r)  # stronger when rewarded
            w[s, :] *= (1.0 - alpha)
            w[s, a] += alpha

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Entropy-based arbitration with load-scaled WM refreshing.

    Overview:
    - RL: tabular Q-learning with softmax policy (beta scaled by 10).
    - WM policy: softmax over WM weights with high precision (deterministic).
    - Arbitration: the WM mixture weight is dynamic, driven by the relative entropies of
      RL vs WM policies via a temperature parameter. WM dominates when WM is more certain
      (lower entropy) than RL; otherwise RL gains weight. The WM weight is upper-bounded by wm_max_weight.
    - WM update: reward-gated refreshing toward the chosen action, plus small decay to uniform.
      Refreshing effectiveness is reduced by load.

    Set-size impact:
    - Load reduces the effective WM refreshing rate.
    - A small normalization decay to uniform is included each trial and is indirectly modulated by the refresh rate.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_max_weight: float in [0,1], maximum WM contribution to the policy.
    - softmax_beta: float >= 0, RL inverse temperature; scaled by 10.
    - entropy_temp: float > 0, arbitration temperature; smaller values make arbitration more sensitive to entropy differences.
    - wm_refresh_base: float in [0,1], base WM refresh rate toward chosen action when rewarded.
    - load_scale: float in [0,2], scales how strongly load reduces WM refreshing.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_max_weight, softmax_beta, entropy_temp, wm_refresh_base, load_scale = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        load = max(0.0, (float(nS) - 3.0) / 3.0)
        # Effective refresh diminishes with load
        wm_refresh_eff = np.clip(wm_refresh_base * (1.0 - load_scale * load), 0.0, 1.0)
        # Small decay tied to refresh
        wm_decay_eff = 0.5 * wm_refresh_eff

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL chosen-action probability and full policy for entropy
            Q_s = q[s, :]
            # Full RL softmax
            z_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = z_rl / np.sum(z_rl)
            p_rl = pi_rl[a]

            # WM policy distribution via high-precision softmax over WM weights
            W_s = w[s, :]
            z_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = z_wm / np.sum(z_wm)
            p_wm = pi_wm[a]

            # Entropies normalized to [0,1]
            eps = 1e-12
            H_rl = -np.sum(np.clip(pi_rl, eps, 1.0) * np.log(np.clip(pi_rl, eps, 1.0))) / np.log(nA)
            H_wm = -np.sum(np.clip(pi_wm, eps, 1.0) * np.log(np.clip(pi_wm, eps, 1.0))) / np.log(nA)

            # Arbitration: WM gets more weight when WM entropy < RL entropy
            # wm_weight_t in [0, wm_max_weight]
            diff = (H_rl - H_wm) / max(entropy_temp, 1e-6)
            wm_weight_t = np.clip(wm_max_weight / (1.0 + np.exp(-diff)), 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform plus reward-gated refresh to chosen action
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Reward-gated refreshing; also allow small refresh on errors
            alpha = wm_refresh_eff * (0.7 * r + 0.2 * (1.0 - r))
            w[s, :] *= (1.0 - alpha)
            w[s, a] += alpha

            # Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p