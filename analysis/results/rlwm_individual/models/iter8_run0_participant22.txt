def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Gated WM with set-size–dependent interference

    Idea
    - Choices are a mixture of RL and WM.
    - WM updates are gated by surprise (absolute RL prediction error) exceeding a threshold.
    - Larger set sizes cause stronger WM interference/decay toward uniform.
    - WM is precise when available, but retrieval can still be noisy via softmax.

    Parameters
    ----------
    parameters : tuple/list of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight placed on WM policy when combining with RL.
        softmax_beta : float >= 0
            Base RL inverse temperature (internally scaled by 10).
        gate_threshold : float >= 0
            Surprise gate threshold; WM updates occur when |r - Q(s,a)| > gate_threshold.
        interference_rate : float in [0,1]
            Baseline WM decay rate toward uniform; amplified as set size grows.

    Set-size impact
    ----------------
    - WM interference scales with set size: effective_decay = interference_rate * (nS/3).
    - WM precision is fixed high (softmax_beta_wm = 50), but final influence on choice is bounded by wm_weight.
    """
    lr, wm_weight, softmax_beta, gate_threshold, interference_rate = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: interference (decay) + gated storage
            effective_decay = interference_rate * max(1.0, float(nS) / 3.0)
            effective_decay = max(0.0, min(1.0, effective_decay))
            w[s, :] = (1.0 - effective_decay) * w[s, :] + effective_decay * w_0[s, :]

            # Gated storage: only update toward chosen action if surprise large enough
            if abs(delta) > gate_threshold:
                # Push prob mass toward chosen action; magnitude scaled by surprise
                store_strength = min(1.0, abs(delta))  # surprise-normalized in [0,1]
                # Move some mass from others to chosen
                bump = store_strength * 0.5  # capped local bump
                w[s, :] = (1 - bump) * w[s, :] + bump * w_0[s, :]
                w[s, a] += bump

            # Renormalize W_s to be a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with asymmetric learning + WM confidence modulated by set size (power law)

    Idea
    - Choices are a mixture of RL and WM.
    - RL uses separate learning rates for positive vs. negative prediction errors.
    - WM precision is scaled by a confidence parameter and diminishes with set size via a power law:
      beta_wm_eff = 50 * wm_confidence / (nS ** size_exponent)
    - WM updates every trial with mild decay toward uniform and a reward-contingent boost.

    Parameters
    ----------
    parameters : tuple/list of length 6
        lr_pos : float in [0,1]
            RL learning rate for positive prediction errors (r - Q > 0).
        lr_neg : float in [0,1]
            RL learning rate for negative prediction errors (r - Q < 0).
        wm_weight : float in [0,1]
            Mixture weight for WM policy.
        softmax_beta : float >= 0
            Base RL inverse temperature (internally scaled by 10).
        wm_confidence : float >= 0
            Scales WM softmax precision; higher means sharper WM policy.
        size_exponent : float >= 0
            Controls how strongly set size reduces WM precision (power-law effect).

    Set-size impact
    ----------------
    - WM precision declines as nS increases: beta_wm_eff = 50 * wm_confidence / (nS ** size_exponent).
    - RL learning rates are not directly set-size dependent; performance differences arise from WM contribution.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_confidence, size_exponent = parameters
    softmax_beta *= 10.0
    base_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Set-size–dependent WM precision via power law
            denom = max(1.0, float(nS))
            beta_wm_eff = base_beta_wm * max(0.0, float(wm_confidence)) / (denom ** max(0.0, float(size_exponent)) + 1e-12)

            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            delta = r - Q_s[a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # WM update: mild decay + reward-contingent boost
            decay = 0.1  # modest constant decay to keep WM stable but flexible
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Reward-contingent boost: push mass toward chosen action more when rewarded
            boost = 0.3 * (0.5 + 0.5 * r)  # 0.15 if r=0, 0.3 if r=1
            w[s, :] = (1.0 - boost) * w[s, :] + boost * w_0[s, :]
            w[s, a] += boost

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Slot-limited WM availability and set-size–dependent inverse temperature modulation

    Idea
    - Choices combine RL and WM.
    - WM is only available for a proportion of states approximated by availability = min(1, K_slots / nS).
      This captures limited-item WM; when WM is not available, behavior defaults toward RL.
    - RL inverse temperature is modulated by set size: beta_rl_eff = (softmax_beta*10) * (beta_scale ** (nS - 3)).
      This captures strategic exploration under higher load.
    - Revisiting a state refreshes its WM representation (wm_refresh) and decays otherwise.

    Parameters
    ----------
    parameters : tuple/list of length 6
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Base weight on WM policy before availability scaling.
        softmax_beta : float >= 0
            Base RL inverse temperature (scaled by 10 internally).
        K_slots : float >= 0
            Effective WM capacity in number of items; availability ~ K_slots / nS.
        wm_refresh : float in [0,1]
            Strength of WM refresh toward the chosen action upon visiting the state.
        beta_scale : float > 0
            Multiplier on RL inverse temperature per extra item beyond 3; <1 reduces beta with larger set size.

    Set-size impact
    ----------------
    - WM availability ~ min(1, K_slots / nS).
    - RL beta scales as beta_rl_eff = (softmax_beta*10) * (beta_scale ** (nS - 3)).
    """
    lr, wm_weight, softmax_beta, K_slots, wm_refresh, beta_scale = parameters
    beta_scale = max(beta_scale, 1e-6)  # keep positive
    blocks_log_p = 0.0
    softmax_beta_wm = 50.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective RL beta for this set size
        softmax_beta_eff = (softmax_beta * 10.0) * (beta_scale ** max(0, nS - 3))

        # WM availability based on slot capacity
        availability = min(1.0, max(0.0, float(K_slots)) / max(1.0, float(nS)))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta_eff * (Q_s - Q_s[a])))

            # WM softmax
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM contribution scaled by availability
            wm_mix = wm_weight * availability
            p_total = wm_mix * p_wm_soft + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward prior and refresh toward chosen action
            decay = 0.1  # modest global decay each visit
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Refresh magnitude scales with wm_refresh and reward
            refresh = max(0.0, min(1.0, wm_refresh)) * (0.5 + 0.5 * r)
            w[s, :] = (1.0 - refresh) * w[s, :] + refresh * w_0[s, :]
            w[s, a] += refresh

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p