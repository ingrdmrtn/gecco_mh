Here are three standalone cognitive models following your template. Each returns the negative log-likelihood of the observed choices and uses up to 6 parameters. I describe the mechanisms and how set size modulates parameters in each docstring.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with probabilistic encoding and state-wise perseveration.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with high determinism (fixed 50).
    - Arbitration: fixed wm_weight blending WM and RL.

    RL mechanisms:
    - Standard delta rule with learning rate lr.
    - State-wise perseveration (stickiness): the previously chosen action in each state
      gets an additive bias stickiness in the RL decision values.

    WM mechanisms:
    - Probabilistic encoding: on rewarded trials, WM stores the chosen action with
      probability p_enc = clip(p_enc0 + p_enc_slope * (nS - 3) / 3, 0..1); otherwise no update.
      WM contents are a simple one-hot-like row for the state when stored (i.e., higher value on the
      stored action), and remain near-uniform otherwise. No explicit decay parameter is used here.

    Parameters:
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (rescaled by 10).
    - wm_weight: Arbitration weight on WM (0..1).
    - p_enc0: Baseline WM encoding probability at set size 3 (0..1).
    - p_enc_slope: Change in encoding probability from set size 3 to 6 (can be negative).
    - stickiness: Additive value bonus to the previously chosen action in the same state.

    Set-size effect:
    - WM encoding probability increases or decreases with set size via p_enc_slope, capturing
      load-dependent WM success or failures.
    """
    lr, softmax_beta, wm_weight, p_enc0, p_enc_slope, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective WM encoding probability for this block
        p_enc = np.clip(float(p_enc0) + float(p_enc_slope) * (nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (high inverse temperature)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # Learning updates
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: probabilistic encoding on rewarded trials
            if r > 0.0:
                if np.random.rand() < p_enc:
                    # Set a strong preference for the chosen action in WM for this state
                    w[s, :] = w_0[s, :].copy()
                    w[s, a] = 1.0

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-dependent learning rate and entropy-based WM arbitration; WM with decay.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with inverse temperature wm_beta.
    - Arbitration: Trial-by-trial WM weight determined by WM certainty:
        wm_weight_t = sigmoid(wm_conf_gain * (max(W_s) - 1/nA))
      Higher max(W_s) increases reliance on WM.

    RL mechanisms:
    - Delta rule with effective learning rate lr_eff that decreases with set size:
        lr_eff = clip(lr_base / (1 + lr_slope * (nS - 3) / 3), 0..1).

    WM mechanisms:
    - Passive decay toward uniform: w <- (1 - decay_eff) * w + decay_eff * w_0,
      with decay_eff = clip(wm_decay_base * (nS / 3), 0..1), increasing with load.
    - Rewarded strengthening for chosen action in WM via a fast update toward 1.

    Parameters:
    - lr_base: Baseline RL learning rate at set size 3 (0..1).
    - lr_slope: Controls how much learning rate shrinks as set size increases (>=0).
    - softmax_beta: RL inverse temperature (rescaled by 10).
    - wm_beta: WM inverse temperature (unscaled).
    - wm_decay_base: Baseline WM decay at set size 3 (0..1), scaled up by set size.
    - wm_conf_gain: Gain for mapping WM confidence (max(W_s)) to arbitration weight.

    Set-size effect:
    - RL learning rate decreases with set size via lr_slope.
    - WM decay increases with set size via wm_decay_base scaling, reducing WM fidelity in 6-item blocks.
    """
    lr_base, lr_slope, softmax_beta, wm_beta, wm_decay_base, wm_conf_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = float(wm_beta)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent parameters
        lr_eff = np.clip(float(lr_base) / (1.0 + float(lr_slope) * (nS - 3) / 3.0), 0.0, 1.0)
        decay_eff = np.clip(float(wm_decay_base) * (nS / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy and dynamic arbitration
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_conf = np.max(W_s) - (1.0 / nA)
            wm_weight_t = 1.0 / (1.0 + np.exp(-float(wm_conf_gain) * wm_conf))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr_eff * pe

            # WM decay toward uniform
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # WM strengthening on rewarded trials
            if r > 0.0:
                w[s, a] += lr_eff * (1.0 - w[s, a])
            else:
                # mild weakening when not rewarded
                w[s, a] -= 0.25 * lr_eff * w[s, a]
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay depending on set size + WM with cross-state interference.

    Policy:
    - RL system: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM system: softmax over W with high determinism (fixed 50).
    - Arbitration: fixed wm_weight_base blending WM and RL.

    RL mechanisms:
    - Delta rule with learning rate lr.
    - Value decay toward uniform Q each trial with set-size-dependent rate:
        q <- (1 - q_decay_eff) * q + q_decay_eff * q_0,
      where q_decay_eff = clip(q_decay_base + q_decay_slope * (nS - 3) / 3, 0..1).

    WM mechanisms:
    - Rewarded one-shot strengthening for the chosen action in the visited state.
    - Cross-state interference in WM: when strengthening W[s,a] after reward, a fraction
      wm_interf of that update spreads to the same action a in other states, evenly divided.
      This captures increased confusability under higher load when multiple items share actions.

    Parameters:
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (rescaled by 10).
    - wm_weight_base: Fixed arbitration weight on WM (0..1).
    - q_decay_base: Baseline RL decay toward uniform at set size 3 (0..1).
    - q_decay_slope: Increase in RL decay from set size 3 to 6 (can be >= -q_decay_base).
    - wm_interf: Fraction of WM strengthening that spreads to other states (0..1).

    Set-size effect:
    - RL decay increases with set size via q_decay_slope, reducing Q stability in 6-item blocks.
    - WM interference impacts performance more when there are more states, because the same
      total interference is distributed across more states but affects more rows overall.
    """
    lr, softmax_beta, wm_weight_base, q_decay_base, q_decay_slope, wm_interf = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    wm_weight = np.clip(wm_weight_base, 0.0, 1.0)
    wm_interf = np.clip(wm_interf, 0.0, 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        q_0 = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        q_decay_eff = np.clip(float(q_decay_base) + float(q_decay_slope) * (nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with decay
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            # Apply global decay toward uniform
            q = (1.0 - q_decay_eff) * q + q_decay_eff * q_0

            # WM update: strengthen chosen action on reward, with cross-state interference
            if r > 0.0:
                gain = lr * (1.0 - w[s, a])
                w[s, a] += gain
                if nS > 1 and wm_interf > 0.0:
                    spread = wm_interf * gain
                    others = (np.arange(nS) != s)
                    w[others, a] += spread / (nS - 1)
                    w[:, a] = np.clip(w[:, a], 0.0, 1.0)
            else:
                # mild weakening when not rewarded
                w[s, a] -= 0.25 * lr * w[s, a]
                w[s, a] = np.clip(w[s, a], 0.0, 1.0)

            # Optional gentle WM normalization toward [0,1] bounds per row
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p