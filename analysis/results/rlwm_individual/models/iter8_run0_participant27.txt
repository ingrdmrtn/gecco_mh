def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with leak, where WM recall probability scales with set size.

    Mechanism
    ---------
    - RL: standard Q-learning with softmax policy.
    - WM: fast associative store updated toward the most recent rewarded action for a state.
    - Recall: WM retrieval succeeds with probability p_recall = min(1, K_slots / nS).
      With probability (1 - p_recall), WM produces a uniform policy (pure guess).
    - Leak: WM traces decay toward uniform each trial by a leak factor.

    Set-size effects
    ----------------
    - Larger set sizes reduce the effective WM recall probability through K_slots / nS,
      making WM less reliable in 6-item blocks than in 3-item blocks.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, K_slots, wm_leak)
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight between WM and RL in choice (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - K_slots: effective WM capacity in number of state-action bindings (>=0).
        - wm_leak: per-trial WM leak toward uniform (0..1).
    Returns
    -------
    Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_slots, wm_leak = model_parameters

    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute recall probability based on capacity and set size
        p_recall = min(1.0, max(0.0, float(K_slots) / float(nS)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax policy
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Effective WM policy with recall failures defaulting to uniform guessing
            p_wm = p_recall * p_wm_soft + (1.0 - p_recall) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak WM toward uniform, then update toward chosen action if rewarded
            # Leak
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            # Reward-gated imprinting of the chosen action (fast update upon reward)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong attraction toward chosen action while keeping row normalized
                eta = 1.0 - (1.0 - wm_leak)  # ties use of wm_leak to update speed
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            # Normalize
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with outcome-gated WM learning and size-dependent lapse in WM.

    Mechanism
    ---------
    - RL: standard Q-learning with softmax policy.
    - WM: associative map updated toward one-hot chosen action when rewarded, decays toward uniform otherwise.
    - WM policy: softmax over WM weights with a size-dependent lapse that increases with set size.

    Set-size effects
    ----------------
    - Lapse increases with set size: eps = eps0 + eps_size * (nS - 3) / 3.
      Larger sets yield more WM lapses (fallback to uniform), weakening WM control.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, alpha_wm, eps0, eps_size)
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight between WM and RL in choice (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - alpha_wm: WM learning rate toward targets (0..1).
        - eps0: baseline WM lapse probability in 3-item sets (0..1).
        - eps_size: additional lapse increment from 3 to 6 items (can be negative or positive).
    Returns
    -------
    Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_wm, eps0, eps_size = model_parameters

    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Size-dependent lapse (clipped to [0,1))
        eps = eps0 + eps_size * (float(nS) - 3.0) / 3.0
        eps = min(max(eps, 0.0), 1.0 - 1e-6)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM softmax with a lapse to uniform that increases with set size
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - eps) * p_wm_soft + eps * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-gated WM learning: move toward chosen action when rewarded,
            # otherwise drift back toward uniform.
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recency-based recall and set-sizeâ€“scaled memory span.

    Mechanism
    ---------
    - RL: standard Q-learning with softmax policy.
    - WM: rapidly encodes rewarded action bindings; decays otherwise.
    - Recall probability depends on recency since the state was last seen:
        p_recall(s,t) = exp(-ISI_s / tau_eff), where ISI_s is trials since last visit.
    - tau_eff scales with set size: tau_eff = recency_tau * (nS / 3)^size_exp.

    Set-size effects
    ----------------
    - Larger set size shortens effective WM span when size_exp > 0, reducing recall probability
      and thus weakening WM influence in 6-item blocks.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, recency_tau, size_exp, wm_alpha)
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight between WM and RL in choice (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - recency_tau: base WM recency time constant (>0).
        - size_exp: exponent controlling how set size scales memory span (can be negative or positive).
        - wm_alpha: WM learning rate toward targets (0..1).
    Returns
    -------
    Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, recency_tau, size_exp, wm_alpha = model_parameters

    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last-visit time per state for recency-based recall
        last_visit = -1 * np.ones(nS, dtype=int)
        tau_eff = max(1e-6, float(recency_tau)) * (float(nS) / 3.0) ** float(size_exp)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute recency-based recall probability
            if last_visit[s] < 0:
                isi = 0.0
            else:
                isi = float(t - last_visit[s])
            p_rec = np.exp(-isi / tau_eff)
            p_rec = min(max(p_rec, 0.0), 1.0)

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax mixed with uniform based on recall probability
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_rec * p_wm_soft + (1.0 - p_rec) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-gated WM learning with decay to uniform otherwise
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update last-visit for the current state
            last_visit[s] = t

        blocks_log_p += log_p

    return -blocks_log_p