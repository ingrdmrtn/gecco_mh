Here are three standalone cognitive models that follow your template, each proposing a different mechanism for how WM and RL interact and how set size modulates behavior. Each function returns the negative log-likelihood of the observed choices.

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reliability-based arbitration and load-sensitive WM precision.

    Idea:
    - RL: standard delta rule with fixed beta (scaled by 10 internally).
    - WM: probabilistic memory over actions per state; WM "precision" (inverse temperature)
      decreases with set size, making WM softer under higher load.
    - Arbitration: weight given to WM increases when WM policy is more certain than RL policy,
      using an entropy-based reliability signal. A base WM bias anchors the arbitration.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_base: Baseline tendency to rely on WM in arbitration (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - ent_slope: Sensitivity of arbitration to (H_rl - H_wm); higher -> more reliability-based shifts.
    - wm_noise_load: Load sensitivity for WM precision; beta_wm_eff = 50/(1 + wm_noise_load*(nS-3)).
    - bias_wm: Bias term added to the arbitration logistic before combining with wm_base.

    Set-size impacts:
    - WM: as set size increases, beta_wm_eff decreases, increasing WM entropy.
    - Arbitration: relies more on the source (WM vs RL) with lower entropy; the entropic gap
      itself is influenced by set size via WM precision.
    """
    lr, wm_base, softmax_beta, ent_slope, wm_noise_load, bias_wm = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM inverse temperature under load
        beta_wm_eff = softmax_beta_wm / (1.0 + max(0.0, wm_noise_load) * (float(nS) - 3.0))
        beta_wm_eff = max(1e-6, beta_wm_eff)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy prob of chosen action
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob of chosen action (softmax over WM trace with load-reduced precision)
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Compute entropies for arbitration
            # RL distribution
            rl_logits = softmax_beta * Q_s
            rl_logits = rl_logits - np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))

            # WM distribution
            wm_logits = beta_wm_eff * W_s
            wm_logits = wm_logits - np.max(wm_logits)
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)
            H_wm = -np.sum(wm_probs * np.log(np.clip(wm_probs, 1e-12, 1.0)))

            # Reliability-based arbitration weight for WM
            reliability_signal = H_rl - H_wm  # positive => WM is sharper than RL
            arb = 1.0 / (1.0 + np.exp(-(bias_wm + ent_slope * reliability_signal)))
            wm_weight = (1.0 - wm_base) * arb + wm_base  # combines baseline and reliability

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then reward-driven sharpening toward chosen action
            # Decay increases under load via the same wm_noise_load factor
            decay = 1.0 - np.exp(-max(0.0, wm_noise_load) * max(0.0, float(nS) - 1.0))  # in [0,1)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM(Win-Stay/Lose-Shift) with load-sensitive heuristic strength.

    Idea:
    - RL: standard delta rule.
    - WM: a WSLS heuristic maintained per state: after reward, stay with last action; after loss,
      shift away from last action. The strength of WS and LS decays with set size (load).
    - Arbitration: fixed mixture weight parameter.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - ws_strength: Logit-scale strength of Win-Stay at set size 3; p_ws = sigmoid(ws_strength).
    - ls_strength: Logit-scale strength of Lose-Shift at set size 3; p_ls = sigmoid(ls_strength).
    - load_sensitivity: Scales the reduction of WS/LS as set size increases; effective p /= (1 + load_sensitivity*(nS-3)).

    Set-size impacts:
    - WM: both win-stay and lose-shift probabilities are reduced as set size increases, making WM
      less deterministic under higher load.
    """
    lr, wm_weight, softmax_beta, ws_strength, ls_strength, load_sensitivity = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # kept but WM uses heuristic, not this beta
    blocks_log_p = 0.0

    # Helper to compute effective WS/LS probabilities under load
    def eff_prob(logit_base, nS, load_sens):
        base = 1.0 / (1.0 + np.exp(-logit_base))
        denom = 1.0 + max(0.0, load_sens) * (float(nS) - 3.0)
        return np.clip(base / max(1e-6, denom), 0.0, 1.0)

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will store current WSLS policy per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Memory for last action and outcome per state
        last_action = -np.ones(nS, dtype=int)  # -1 = unknown
        last_reward = -np.ones(nS, dtype=int)  # -1 = unknown, else 0/1

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = int(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from WSLS heuristic
            if last_action[s] == -1 or last_reward[s] == -1:
                # No memory yet: uniform
                wm_probs = np.ones(nA) / nA
            else:
                la = int(last_action[s])
                if last_reward[s] == 1:
                    p_ws = eff_prob(ws_strength, nS, load_sensitivity)
                    wm_probs = np.ones(nA) * ((1.0 - p_ws) / (nA - 1))
                    wm_probs[la] = p_ws
                else:
                    p_ls = eff_prob(ls_strength, nS, load_sensitivity)
                    wm_probs = np.ones(nA) * (p_ls / (nA - 1))
                    wm_probs[la] = 1.0 - p_ls

            # Store the current WM policy vector in w for transparency
            w[s, :] = wm_probs

            # Probability of chosen action under WM
            p_wm = np.clip(wm_probs[a], 1e-12, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: update memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM(elimination-by-feedback) with load-dependent lapse.

    Idea:
    - RL: standard delta rule.
    - WM: keeps, for each state, a set of feasible actions (candidates).
        * On negative feedback: eliminate the chosen action from the candidate set.
        * On positive feedback: commit the candidate set to the rewarded action (one-hot).
      The WM policy is the normalized candidate distribution, sharpened by a fixed high beta.
    - Lapse in WM retrieval increases with set size, mixing the WM candidate policy with uniform.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in action selection (0..1).
    - softmax_beta: Base RL inverse temperature; multiplied by 10 internally.
    - elim_sharpness: Controls how sharply WM favors the candidate distribution (acts as extra beta on WM).
    - lapse_base: Base WM lapse probability at set size 3 (0..1).
    - lapse_slope: Increase in lapse per unit increase in set size beyond 3 (>=0).

    Set-size impacts:
    - WM: lapse probability increases with set size; higher load -> more uniform WM retrieval.
    """
    lr, wm_weight, softmax_beta, elim_sharpness, lapse_base, lapse_slope = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # baseline WM determinism, further scaled by elim_sharpness
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # w will store the current candidate distribution per state (normalized)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize candidate sets to all actions
        candidates = np.ones((nS, nA), dtype=float)

        # Load-dependent WM lapse
        p_lapse = np.clip(lapse_base + max(0.0, lapse_slope) * (float(nS) - 3.0), 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM candidate policy: normalized candidates, sharpened by elim_sharpness
            cand = candidates[s, :].copy()
            if np.sum(cand) <= 0:
                cand = np.ones(nA)  # safety: reset to all if eliminated all
            cand = cand / np.sum(cand)

            # Apply sharpening via effective WM beta
            beta_eff_wm = softmax_beta_wm * max(1e-6, elim_sharpness)
            logits_wm = beta_eff_wm * cand
            logits_wm = logits_wm - np.max(logits_wm)
            probs_wm_core = np.exp(logits_wm)
            probs_wm_core = probs_wm_core / np.sum(probs_wm_core)

            # Lapse mixture within WM
            wm_probs = (1.0 - p_lapse) * probs_wm_core + p_lapse * (np.ones(nA) / nA)

            # Store in w for transparency
            w[s, :] = wm_probs

            p_wm = np.clip(wm_probs[a], 1e-12, 1.0)

            # Mixture with RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM elimination update
            if r < 0.5:
                candidates[s, a] = 0.0
                # keep at least one candidate
                if np.sum(candidates[s, :]) <= 0.0:
                    candidates[s, :] = 1.0
            else:
                # Commit to the rewarded action
                candidates[s, :] = 0.0
                candidates[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p

Model notes:
- Model 1 introduces reliability-based arbitration via entropy differences between WM and RL, and decreases WM precision with set size.
- Model 2 implements a WM heuristic (WSLS) whose efficacy degrades with set size; RL remains standard.
- Model 3 uses elimination-by-feedback in WM with a load-dependent lapse that increases with set size, offering a distinct mechanism from swap noise or decay.

All parameters are used in the choice probability or value updating steps, and set size impacts are explicitly incorporated.