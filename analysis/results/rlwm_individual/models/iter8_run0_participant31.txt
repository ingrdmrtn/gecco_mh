def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + leaky WM with set-size–dependent interference.

    Mechanism
    - RL: tabular Q-learning with a single learning rate.
    - WM: graded cache of action values per state that is strongly peaked after reward,
      but leaks toward uniform. Leak is stronger for larger set sizes (more interference).
    - Arbitration: fixed mixture weight between WM and RL policies.

    Parameters
    - lr: RL learning rate (0..1).
    - wm_weight: mixture weight for WM vs RL (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - wm_retention: base retention factor for WM update; higher means less leak (0..1).
    - wm_interf_slope: slope that increases WM leak with set size (>=0). Effective
      retention per block is wm_retention^(1 + wm_interf_slope * max(0, nS-3)).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_retention, wm_interf_slope = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent effective retention (more items => more leak)
        interf_factor = 1.0 + wm_interf_slope * max(0, nS - 3)
        wm_retention_eff = np.clip(wm_retention ** interf_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (provided)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over W_s with high inverse temperature
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leaky integration toward one-hot after reward; otherwise decay toward uniform
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = wm_retention_eff * w[s, :] + (1.0 - wm_retention_eff) * one_hot
            else:
                w[s, :] = wm_retention_eff * w[s, :] + (1.0 - wm_retention_eff) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + PE-gated WM encoding with set-size–dependent gate and WM leak.

    Mechanism
    - RL: tabular Q-learning with a single learning rate.
    - WM: cache encodes chosen action only when reward arrives AND the RL prediction
      error magnitude exceeds a gate (novelty/surprise gating). The gate increases
      with set size (harder to encode under load). WM leaks toward uniform otherwise.
    - Arbitration: fixed mixture of WM and RL policies.

    Parameters
    - lr: RL learning rate (0..1).
    - wm_weight: mixture weight for WM vs RL (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - pe_threshold: base PE magnitude threshold for WM encoding (>=0).
    - ss_threshold_slope: additional threshold per extra item beyond 3 (>=0).
    - wm_leak: WM leak toward uniform on any trial without encoding (0..1).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, pe_threshold, ss_threshold_slope, wm_leak = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent encoding threshold
        thr_eff = pe_threshold + ss_threshold_slope * max(0, nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (provided)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Encode only if reward and surprise large enough; otherwise leak toward uniform
            if (r > 0.0) and (abs(delta) >= thr_eff):
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL with set-size–scaled exploration + capacity-limited WM with eviction.

    Mechanism
    - RL: tabular Q-learning with a single learning rate. The effective inverse
      temperature decreases with set size to capture higher exploration under load.
    - WM: can hold up to K states with strong (one-hot) entries; others decay toward
      uniform. When encoding a rewarded action exceeds capacity, the weakest stored
      state is evicted (its WM row reset toward uniform). All stored WM rows leak
      slightly each time they are visited.

    Parameters
    - lr: RL learning rate (0..1).
    - softmax_beta_base: RL inverse temperature baseline (scaled internally by 10).
    - wm_weight_base: mixture weight for WM vs RL (0..1).
    - capacity_K: WM capacity in number of states (0..nS).
    - wm_decay: leak of WM toward uniform on each visit to a state (0..1).
    - ss_beta_slope: scales how much RL beta reduces per extra item beyond 3 (>=0).
      Effective beta = softmax_beta_base / (1 + ss_beta_slope * max(0, nS-3)).

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_base, wm_weight_base, capacity_K, wm_decay, ss_beta_slope = parameters
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track WM strength per state for eviction policy
        strength = np.zeros(nS)  # 0=not stored/weak, 1=strongly stored

        # Set-size–scaled RL temperature and a fixed WM weight
        softmax_beta = (softmax_beta_base * 10.0) / (1.0 + ss_beta_slope * max(0, nS - 3))
        wm_weight = np.clip(wm_weight_base, 0.0, 1.0)
        K = int(np.clip(np.round(capacity_K), 0, nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (provided structure)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            z_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(z_wm) / np.sum(np.exp(z_wm))
            p_wm = max(pi_wm[a], 1e-12)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak on visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            strength[s] = (1.0 - wm_decay) * strength[s]

            # Encode rewarded action with capacity constraint
            if r > 0.0:
                # If capacity is zero, just keep decaying (no storage)
                if K > 0:
                    # If not yet at capacity, encode directly
                    if np.sum(strength > 0.5) < K or strength[s] > 0.5:
                        one_hot = np.zeros(nA)
                        one_hot[a] = 1.0
                        w[s, :] = one_hot
                        strength[s] = 1.0
                    else:
                        # Evict the weakest stored state (excluding current s if it's the weakest)
                        stored_idx = np.where(strength > 0.5)[0]
                        if stored_idx.size > 0:
                            weakest = stored_idx[np.argmin(strength[stored_idx])]
                            # Evict weakest
                            w[weakest, :] = w_0[weakest, :]
                            strength[weakest] = 0.0
                        # Store current
                        one_hot = np.zeros(nA)
                        one_hot[a] = 1.0
                        w[s, :] = one_hot
                        strength[s] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p