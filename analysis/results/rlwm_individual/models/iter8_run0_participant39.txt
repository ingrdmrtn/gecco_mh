def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic WM recall with load-dependent recall and WM leak.

    Mechanism
    - RL: standard Q-learning with inverse-temperature softmax (beta scaled by 10 as per template).
    - WM policy: on each choice, WM is recalled with probability that decreases with set size; otherwise
      the WM route contributes a uniform policy. When recalled, policy is a sharp softmax over WM weights.
    - WM update: reward-gated encoding toward the chosen action and concurrent leak toward uniform.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1); fixed across trials, per template mixture.
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_eta: strength of WM encoding toward chosen action on rewarded trials (0..1).
        - rec_slope: load sensitivity of WM recall; higher values reduce recall as set size increases (>=0).
                     Effective recall p_rec = exp(-rec_slope * (nS-3)/(6-3)).
        - wm_leak: per-trial WM leak toward uniform (0..1).

    Set-size effects
    ----------------
    - WM recall probability p_rec decays with larger set size (nS=6 vs 3), reducing WM's contribution at high load.
    """
    lr, wm_weight, softmax_beta, wm_eta, rec_slope, wm_leak = model_parameters
    softmax_beta *= 10  # RL temperature scaling as specified
    softmax_beta_wm = 50  # deterministic WM when recalled
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent recall probability (3 -> 1.0; 6 -> exp(-rec_slope))
        load_scale = (nS - 3) / (6 - 3)
        p_rec = np.exp(-rec_slope * load_scale)
        p_rec = np.clip(p_rec, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: recalled with prob p_rec (sharp softmax), else uniform
            p_wm_recalled = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_rec * p_wm_recalled + (1 - p_rec) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-gated WM encoding toward chosen action with concurrent leak toward uniform
            if wm_leak > 0:
                w[s] = (1 - wm_leak) * w[s] + wm_leak * w_0[s]
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_eta) * w[s] + wm_eta * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent binding errors (swaps) and error-based WM repulsion.

    Mechanism
    - RL: standard Q-learning with inverse-temperature softmax (beta scaled by 10).
    - WM policy: at retrieval, with probability that increases with set size, the participant
      accesses an averaged WM trace (swap/binding error) instead of the correct state's trace.
    - WM update: on rewarded trials, encode toward chosen action; on non-rewarded trials,
      reduce probability of the chosen action (repulsion) and redistribute mass to other actions.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - swap_base: baseline probability of WM binding error at set size 3 (0..1).
        - load_slope: incremental increase in swap probability from 3 to 6 items (>=0).
                      Effective swap p_swap = clip(swap_base + load_slope * (nS-3)/(6-3), 0..1).
        - anti_err: strength (0..1) of WM repulsion for the chosen action on non-rewarded trials.

    Set-size effects
    ----------------
    - WM binding/swap errors increase with set size via p_swap, degrading WM policy at higher load.
    """
    lr, wm_weight, softmax_beta, swap_base, load_slope, anti_err = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        load_scale = (nS - 3) / (6 - 3)
        p_swap = np.clip(swap_base + load_slope * load_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # Swap mixture: use state-specific WM with prob (1-p_swap), else use average trace (binding error)
            W_specific = w[s, :]
            W_mean = np.mean(w, axis=0)  # proxy for retrieving another state's trace at random
            W_used = (1 - p_swap) * W_specific + p_swap * W_mean

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_used - W_used[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Encode toward the chosen action on reward
                w[s] = 0.5 * w[s] + 0.5 * onehot
            else:
                # Repel the chosen action on error and redistribute to others
                decrease = anti_err * w[s, a]
                w[s, a] = w[s, a] - decrease
                # Add the removed mass uniformly to the other two actions
                others = [i for i in range(nA) if i != a]
                w[s, others] = w[s, others] + decrease / (nA - 1)
            # Keep within simplex numerically
            w[s] = np.maximum(w[s], 1e-12)
            w[s] = w[s] / np.sum(w[s])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Surprise-gated WM encoding and load-scaled global recency interference.

    Mechanism
    - RL: standard Q-learning with inverse-temperature softmax (beta scaled by 10).
    - WM policy: mixture policy as per template; WM route is a sharp softmax over current WM trace.
      Additionally, WM reliability on a trial is reflected by surprise-gated mixing with uniform in the WM route.
    - WM update: encoding toward the chosen action is gated by RL surprise (absolute RPE).
      Each choice also induces global recency interference toward the chosen action across all states,
      scaled by set size (stronger interference at higher load).

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - surprise_gate: sensitivity (>0) of gating to surprise; higher values produce sharper gating.
                         Gate g = 1 / (1 + exp(-surprise_gate * (|RPE| - 0.5))).
        - recency: base strength (0..1) of WM encoding toward chosen action when gate is fully open.
        - load_gain: scales global interference by set size; effective gamma = recency * load_gain * (nS-3)/(6-3).

    Set-size effects
    ----------------
    - Global interference toward the chosen action increases with set size, degrading item-specific WM under load.
    """
    lr, wm_weight, softmax_beta, surprise_gate, recency, load_gain = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        load_scale = (nS - 3) / (6 - 3)
        global_gamma = np.clip(recency * load_gain * load_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Surprise-gated reliability inside WM route: g in [0,1]
            # Compute surprise using current Q before update
            rpe = abs(r - Q_s[a])
            g = 1.0 / (1.0 + np.exp(-surprise_gate * (rpe - 0.5)))
            p_wm_sharp = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = g * p_wm_sharp + (1 - g) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Surprise-gated item-specific encoding
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            w[s] = (1 - recency * g) * w[s] + (recency * g) * onehot
            # Global recency interference toward chosen action across all states, load-scaled
            if global_gamma > 0:
                e_a = np.zeros(nA)
                e_a[a] = 1.0
                w = (1 - global_gamma) * w + global_gamma * np.tile(e_a, (nS, 1))

        blocks_log_p += log_p

    return -blocks_log_p