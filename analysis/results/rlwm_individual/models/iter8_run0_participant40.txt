def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated Working Memory with set-size-dependent decay.

    Mechanism
    - RL: standard delta-rule learning on state-action values.
    - WM: encodes an action in working memory only upon positive feedback (reward).
      WM traces decay faster in larger set sizes (higher load).
    - Arbitration: mixture of WM and RL; WM weight decreases with set size (3/nS).

    Parameters
    ----------
    model_parameters : list or array of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), downscaled by 3/nS.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay: faster decay for larger sets
        # Map nS=3 -> decay=0.1, nS=6 -> decay=0.4 (linear)
        wm_decay = 0.1 + 0.3 * ((max(nS, 3) - 3) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 0.9)  # keep stable
        wm_encode_gain = 0.6  # deterministic WM write on reward

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probability (chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working-memory policy: softmax over W
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight shrinks with set size (capacity limits)
            eff_wm_weight = np.clip(wm_weight * (3.0 / float(nS)), 0.0, 1.0)

            p_total = p_wm * eff_wm_weight + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay every trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # PE-gated WM encoding: only commit to WM upon reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_encode_gain) * w[s, :] + wm_encode_gain * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-weighted WM arbitration and repeat-boosted WM encoding.

    Mechanism
    - RL: standard delta-rule learning.
    - WM: softly encodes the most recent chosen action, with stronger encoding
      when the same state repeats consecutively (a rehearsal benefit).
      WM decays toward uniform; decay is stronger in larger set sizes.
    - Arbitration: WM mixture weight scales with 3/nS (capacity) and with RL
      uncertainty (higher entropy of RL policy -> more WM reliance).

    Parameters
    ----------
    model_parameters : list or array of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), scaled by uncertainty and 3/nS.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    nA = 3
    log_nA = np.log(nA)

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay
        wm_decay = 0.1 + 0.3 * ((max(nS, 3) - 3) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 0.9)

        last_state = None
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen action probability (keep the provided stable formulation)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Also compute full RL policy to assess uncertainty (entropy)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            pvec_rl = exp_logits / np.sum(exp_logits)
            entropy_rl = -np.sum(np.where(pvec_rl > 0, pvec_rl * np.log(pvec_rl), 0.0))
            uncert = entropy_rl / log_nA  # normalized 0..1

            # Working-memory policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: baseline WM capacity factor times RL uncertainty
            size_scale = 3.0 / float(nS)
            eff_wm_weight = wm_weight * size_scale * uncert
            eff_wm_weight = np.clip(eff_wm_weight, 0.0, 1.0)

            p_total = p_wm * eff_wm_weight + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding: always encode the chosen action (last response),
            # but with a repeat boost if the same state appears consecutively.
            base_encode = 0.25
            repeat_boost = 0.45 if (last_state is not None and last_state == s) else 0.0
            wm_encode_gain = np.clip(base_encode + repeat_boost, 0.0, 1.0)

            target = np.zeros(nA)
            target[a] = 1.0
            w[s, :] = (1.0 - wm_encode_gain) * w[s, :] + wm_encode_gain * target

            last_state = s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + NoGo Working Memory (WM tracks avoid-to-choose) with recent-error arbitration boost.

    Mechanism
    - RL: standard delta-rule learning.
    - WM: encodes "avoid this action" tags on non-rewarded trials; WM prefers
      actions that are least tagged (approach the complement of recent errors).
      WM decays faster in larger set sizes.
    - Arbitration: baseline WM weight scales with 3/nS; additionally, after a
      recent error in the same state, arbitration transiently increases WM reliance
      to leverage the NoGo tag.

    Parameters
    ----------
    model_parameters : list or array of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), downscaled by 3/nS; transiently boosted after errors.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM here represents "preference" away from recently incorrect actions.
        # Initialize uniform preference (not yet avoiding any action).
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay
        wm_decay = 0.1 + 0.3 * ((max(nS, 3) - 3) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 0.9)

        # Track whether the last visit to a state resulted in an error
        last_error_action = -np.ones(nS, dtype=int)
        last_error_flag = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: prefer actions with higher W_s (less avoided)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: capacity-limited baseline, with transient boost after recent error
            size_scale = 3.0 / float(nS)
            base_w = wm_weight * size_scale
            # If previous visit to this state was an error, boost WM reliance
            error_boost = 0.5 if (last_error_flag[s] and last_error_action[s] >= 0) else 0.0
            eff_wm_weight = np.clip(base_w + error_boost * (1.0 - base_w), 0.0, 1.0)

            p_total = p_wm * eff_wm_weight + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # NoGo WM updating:
            # If non-rewarded, increase avoidance of the chosen action by shifting
            # probability mass away from 'a' and toward others.
            if r <= 0:
                avoid_gain = 0.6
                target = np.ones(nA) / (nA - 1)
                target[a] = 0.0  # "do not choose a"
                w[s, :] = (1.0 - avoid_gain) * w[s, :] + avoid_gain * target
                last_error_action[s] = a
                last_error_flag[s] = True
            else:
                # On reward, softly normalize back toward a uniform non-avoid stance,
                # and clear recent error flag.
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
                last_error_flag[s] = False

        blocks_log_p += log_p

    return -blocks_log_p