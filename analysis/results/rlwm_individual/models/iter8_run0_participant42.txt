def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-weighted arbitration and WM leak.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: fast encoding toward chosen action on reward; in the absence of reward,
      WM leaks back toward a uniform prior for that state.
    - Arbitration: convex mixture between WM and RL. WM contribution decays exponentially
      with set size: wm_weight_eff = wm_weight0 * exp(-load_penalty * (nS - 3)).
      This captures that working memory is more impactful at low load.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight0: Baseline WM mixture weight in [0,1] before load scaling.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_alpha: WM encoding gain toward the chosen action on reward in [0,1].
    - wm_leak: WM leak toward uniform after non-reward in [0,1].
    - load_penalty: >=0, exponential down-weighting of WM as set size increases.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_alpha, wm_leak, load_penalty = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-scaled WM weight
        wm_weight_eff = wm_weight0 * np.exp(-max(0.0, load_penalty) * max(0, nS - 3))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (sharp softmax over WM weights)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward strengthens mapping; non-reward leaks to uniform
            if r > 0:
                # Move distribution toward one-hot at chosen action
                w[s, :] = (1 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # Leak toward uniform
                w[s, :] = (1 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # Renormalize (numerical safety)
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-based arbitration and WM decay.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: encodes toward chosen action when rewarded; otherwise decays toward uniform at rate wm_decay.
    - Arbitration: WM weight depends on an internal confidence signal derived from WM sharpness
      (max-minus-second-max of W_s). The effective WM weight is:
          wm_weight_eff = wm_weight0 * sigmoid(arb_bias + arb_temp * (conf - log(nS)))
      which reduces WM impact with larger set sizes and low WM confidence.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight0: Baseline WM mixture weight in [0,1].
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay: WM decay rate toward uniform in [0,1].
    - arb_bias: Arbitration bias term (real-valued).
    - arb_temp: Arbitration slope/temperature >=0 mapping confidence to WM weight.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay, arb_bias, arb_temp = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence (sharpness of W_s)
            sorted_vals = np.sort(W_s)[::-1]
            conf = sorted_vals[0] - sorted_vals[1] if len(sorted_vals) >= 2 else 0.0

            # Arbitration weight as a function of WM confidence and set size
            x = arb_bias + max(0.0, arb_temp) * (conf - np.log(max(1, nS)))
            wm_weight_eff = wm_weight0 / (1 + np.exp(-x))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-based encoding, otherwise global decay toward uniform
            if r > 0:
                w[s, :] = 0.0 * w[s, :]
                w[s, a] = 1.0
            else:
                w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Safety renormalization
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with state-specific forgetting + gated WM retrieval sensitive to load.

    Mechanism:
    - RL: tabular Q-learning with state-specific decay toward uniform after each trial
      on the visited state, capturing forgetting under interference:
          q[s] <- (1 - q_decay) * q[s] + q_decay * uniform  (then standard delta update)
    - WM: stores the last rewarded action per state as a sharp distribution; no update on non-reward.
    - Gating: WM retrieval probability is set-size sensitive via a logistic gate:
          gate = sigmoid(gate_bias - load_curvature * (nS - 3))
      The effective WM mixture weight is wm_weight0 * gate if the state has a stored rewarded action,
      otherwise 0 (i.e., RL only when WM has nothing for that state).

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight0: Maximum WM mixture weight in [0,1] when WM is available.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - q_decay: RL decay toward uniform for visited state in [0,1].
    - gate_bias: Bias term controlling baseline WM retrieval (real).
    - load_curvature: >=0, how strongly set size reduces WM retrieval.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, q_decay, gate_bias, load_curvature = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track whether WM has a valid cached mapping for each state
        has_wm = np.zeros(nS, dtype=bool)

        # Precompute gate as a function of load
        gate_arg = gate_bias - max(0.0, load_curvature) * max(0, nS - 3)
        gate = 1.0 / (1.0 + np.exp(-gate_arg))
        gate = np.clip(gate, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL decay toward uniform for the visited state (interference/forgetting)
            q[s, :] = (1 - q_decay) * q[s, :] + q_decay * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight only if we have a cached mapping for this state
            wm_weight_eff = wm_weight0 * gate if has_wm[s] else 0.0
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL delta update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: cache last rewarded action; no update on non-reward
            if r > 0:
                w[s, :] = 0.0 * w[s, :]
                w[s, a] = 1.0
                has_wm[s] = True

            # Safety renormalization
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p