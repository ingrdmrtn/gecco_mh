def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Confidence- and load-gated WM arbitration with reward-contingent WM updates.
    - RL: Q-learning with single learning rate.
    - WM: probabilistic belief per state that decays to uniform; reward strengthens the chosen
      action, no-reward redistributes belief toward uniform.
    - Arbitration: WM weight is dynamically computed from
        wm_weight (base) + gate_k * WM confidence - load_k * load,
      passed through a sigmoid. WM confidence is 1 - entropy(W_s), and load = max(0, nS-3).

    Parameters:
      lr:           RL learning rate (0..1)
      wm_weight:    Base WM mixture weight (pre-sigmoid bias)
      softmax_beta: RL inverse temperature (internally scaled by 10)
      wm_decay:     WM decay toward uniform per state visit (0..1); also controls storage strength
      gate_k:       Gain of confidence-based WM gating (positive increases WM use with confidence)
      load_k:       Penalization of WM use with higher set size (per extra item over 3)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, gate_k, load_k = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from current WM belief (use log-prob as logits for stability)
            eps = 1e-12
            wm_logits = np.log(W_s + eps)
            p_wm0 = 1 / np.sum(np.exp(softmax_beta_wm*(wm_logits-wm_logits[a])))

            # Compute dynamic WM weight via sigmoid(base + gate_k*confidence - load_k*load)
            # WM confidence is 1 - normalized entropy
            H = -np.sum(W_s * np.log(W_s + eps))
            H /= np.log(nA)  # normalize to [0,1]
            confidence = 1.0 - H
            load = max(0.0, nS - 3.0)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(wm_weight + gate_k * confidence - load_k * load)))

            p_wm = p_wm0
            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-contingent storage: strengthen chosen on reward, soften on no-reward
            if r > 0:
                # Move mass toward one-hot on chosen action
                store_strength = wm_decay
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
            else:
                # No reward: redistribute a fraction toward uniform (forget wrong binding)
                suppress = 0.5 * wm_decay
                w[s, :] = (1.0 - suppress) * w[s, :] + suppress * w_0[s, :]

            # Renormalize and clip for stability
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL + Dirichlet-like WM counts with load-adjusted concentration.
    - RL: Q-learning with single learning rate.
    - WM: per-state Dirichlet-like belief over actions that decays to uniform and
      receives outcome-dependent count increment; larger set size increases an
      effective concentration that makes WM distributions flatter/more conservative.
    - Arbitration: mixture with fixed base wm_weight.

    Parameters:
      lr:           RL learning rate (0..1)
      wm_weight:    Mixture weight for WM (0..1)
      softmax_beta: RL inverse temperature (internally scaled by 10)
      wm_decay:     WM decay toward uniform per state visit (0..1)
      conc_base:    Base log-concentration shaping WM storage magnitude
      conc_slope:   Load slope on log-concentration (per extra item in set size)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, conc_base, conc_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-adjusted concentration influences storage magnitude
        load = max(0.0, nS - 1.0)
        log_conc = conc_base + conc_slope * load
        conc = np.exp(np.clip(log_conc, -5.0, 5.0))  # keep in reasonable range
        # Convert to an additive mass for updates relative to decay
        add_mass = conc / (1.0 + conc)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy derived from belief distribution (use log-prob as logits)
            eps = 1e-12
            wm_logits = np.log(W_s + eps)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm*(wm_logits-wm_logits[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Dirichlet-like update: add outcome-weighted mass to chosen action
            # Reward adds more mass; no-reward adds minimal (to reflect weak evidence)
            outcome_mass = add_mass * (0.5 + 0.5 * r)  # in [0, add_mass]
            if outcome_mass > 0:
                w[s, :] = (1.0 - outcome_mass) * w[s, :]
                w[s, a] += outcome_mass

            # Renormalize and clip
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: RL + WM with set-sizeâ€“scaled cross-talk and lapse affecting WM channel.
    - RL: Q-learning with single learning rate.
    - WM: decays toward uniform; reward encodes chosen action, but probability mass
      leaks (cross-talk) from chosen to other actions within the state; leak grows with set size.
    - Arbitration: effective wm_weight shrinks with set size via leak term; WM policy includes a lapse
      that mixes in uniform responding only on the WM pathway.

    Parameters:
      lr:            RL learning rate (0..1)
      wm_weight:     Base WM mixture weight (0..1)
      softmax_beta:  RL inverse temperature (internally scaled by 10)
      wm_decay:      WM decay/encoding strength per visit (0..1)
      wm_leak_cross: Cross-talk factor; increases leak and reduces WM weight with larger set sizes
      lapse:         Lapse probability applied to WM policy (0..1), mixing with uniform
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_leak_cross, lapse = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size scaled effective WM weight (more load -> less reliance on WM)
        wm_weight_eff = wm_weight / (1.0 + max(0.0, nS - 3.0) * max(0.0, wm_leak_cross))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Base WM policy from WM belief
            eps = 1e-12
            wm_logits = np.log(W_s + eps)
            p_wm_base = 1 / np.sum(np.exp(softmax_beta_wm*(wm_logits-wm_logits[a])))
            # Apply WM-specific lapse (mix with uniform through WM channel only)
            p_wm = (1.0 - lapse) * p_wm_base + lapse * (1.0 / nA)

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-driven encoding
            if r > 0:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

            # Cross-talk: bleed some probability mass from chosen to others, scaled by load
            load = max(0.0, nS - 3.0)
            leak = max(0.0, wm_leak_cross) * load
            if leak > 0:
                bleed = leak * w[s, a]
                w[s, a] -= bleed
                w[s, :] += bleed / (nA - 1.0)
                w[s, a] -= bleed / (nA - 1.0)  # remove the portion added back to chosen
                # Numerical fix: above two lines could overshoot due to order; recompute cleaner:
                # Distribute bleed equally to non-chosen
                w[s, :] = np.clip(w[s, :], 0.0, None)
                total_non_chosen = max(1e-12, np.sum(w[s, :]) - w[s, a])
                # Ensure normalization after cross-talk adjustments

            # Renormalize and clip
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p