def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic WM encoding with load-dependent retention.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: caches a near-deterministic action distribution for each state when rewarded.
    - Encoding: On rewarded trials, WM encodes the chosen action with a probability that
      decreases with set size.
    - Retention: WM representations globally decay toward uniform; decay gets stronger
      with larger set size.
    - Arbitration: Fixed base weight on WM, down-weighted as set size increases.

    Parameters
    ----------
    model_parameters : (lr, wm_weight_base, softmax_beta, wm_encode_prob, wm_decay_block)
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Base WM mixture weight in policy (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_encode_prob : float
            Base probability of encoding the rewarded action into WM (0-1).
        wm_decay_block : float
            Global WM decay/interference rate per trial (>=0). Stronger in larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_encode_prob, wm_decay_block = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (softmax trick per template)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY:
            # Near-deterministic readout of cached distribution W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: base WM weight reduced by set size (3/nS)
            ss_scale = 3.0 / max(3.0, float(nS))
            wm_weight = np.clip(wm_weight_base * ss_scale, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WORKING MEMORY UPDATE:
            # 1) Global decay toward uniform, stronger in larger set sizes
            decay = 1.0 - np.exp(-wm_decay_block * (float(nS) / 3.0))
            w = (1.0 - decay) * w + decay * w_0

            # 2) Probabilistic encoding of rewarded action with load scaling
            if r > 0:
                enc_prob = np.clip(wm_encode_prob * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
                # Use expectation of probabilistic encoding (no sampling): convex combination
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - enc_prob) * w[s, :] + enc_prob * one_hot

            # Keep WM rows normalized
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and set-size–dependent WM learning/forgetting.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: incremental distribution updated toward a one-hot target on reward.
    - Entropy-based arbitration: rely more on the lower-entropy (more certain) system.
      The WM mixture weight is modulated by the difference in entropies between WM and RL.
    - Set-size effects:
        * WM learning strength decreases with set size.
        * WM decay toward uniform increases with set size.

    Parameters
    ----------
    model_parameters : (lr, wm_base, softmax_beta, gamma_entropy, phi_wm)
        lr : float
            RL learning rate (0-1).
        wm_base : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        gamma_entropy : float
            Sensitivity to entropy difference (>=0). Higher -> prefer lower-entropy system.
        phi_wm : float
            Base WM update strength toward one-hot on rewarded trials (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, gamma_entropy, phi_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute full RL and WM distributions to get entropies
            # Softmax distributions (stable via subtraction)
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)

            # Entropies
            eps = 1e-12
            H_rl = -np.sum(rl_probs * np.log(rl_probs + eps))
            H_wm = -np.sum(wm_probs * np.log(wm_probs + eps))

            # Arbitration weight: prefer lower-entropy system
            # wm_weight = sigmoid(wm_base_logit + gamma*(H_rl - H_wm))
            # Implement with bounded linear-sigmoid form via tanh for robustness
            ent_diff = H_rl - H_wm
            wm_weight = np.clip(wm_base + 0.5 * np.tanh(gamma_entropy * ent_diff), 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updates:
            # 1) Set-size–modulated decay toward uniform
            decay = 1.0 - np.exp(-(float(nS) / 3.0) * phi_wm / 5.0)
            w = (1.0 - decay) * w + decay * w_0

            # 2) Reward-driven learning toward one-hot with strength decreasing with set size
            if r > 0:
                learn_rate_wm = 1.0 - np.exp(-phi_wm * (3.0 / max(3.0, float(nS))))
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - learn_rate_wm) * w[s, :] + learn_rate_wm * one_hot

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with reward-rate–modulated exploration + WM with repetition-based refresh.

    Idea
    - RL: Q-learning with an inverse temperature that adapts to the recent reward rate
      (more deterministic when reward rate is low, reflecting focused exploration/exploitation).
    - WM: near-deterministic cache; encodes on reward and also "refreshes" when the same state
      repeats (even without reward), capturing rehearsal-like processes.
    - Set-size effects:
        * WM contribution is down-weighted in larger set sizes.
        * WM representations decay more in larger set sizes.

    Parameters
    ----------
    model_parameters : (lr, wm_weight0, softmax_beta, beta_gain_rr, wm_refresh)
        lr : float
            RL learning rate (0-1).
        wm_weight0 : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            Base RL inverse temperature (scaled by 10 internally).
        beta_gain_rr : float
            Gain controlling how much RL beta increases as recent reward rate decreases (>=0).
        wm_refresh : float
            Strength of WM refresh toward the last chosen action on state repetitions (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, beta_gain_rr, wm_refresh = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Running reward rate (exponential moving average)
        rr = 0.5  # start neutral
        rr_alpha = 0.2

        last_state = None
        last_action = None

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Reward-rate–modulated RL temperature:
            # low rr -> increase beta (more deterministic), high rr -> closer to base
            beta_eff = softmax_beta * (1.0 + beta_gain_rr * max(0.0, 1.0 - rr))

            p_rl = 1 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy probability
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM arbitration weight with set-size penalty
            wm_weight = np.clip(wm_weight0 * (3.0 / max(3.0, float(nS))), 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update reward rate
            rr = (1 - rr_alpha) * rr + rr_alpha * r

            # WM updates:
            # 1) Global decay toward uniform with set-size scaling
            decay = 1.0 - np.exp(- (float(nS) / 3.0) * 0.5)
            w = (1.0 - decay) * w + decay * w_0

            # 2) Reward-based encoding to one-hot
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

            # 3) Repetition-based refresh: if the same state repeats, nudge WM toward last chosen action
            if last_state is not None and s == last_state and last_action is not None:
                refresh = 1.0 - np.exp(-wm_refresh * (3.0 / max(3.0, float(nS))))
                refresh = np.clip(refresh, 0.0, 1.0)
                one_hot_last = np.zeros(nA)
                one_hot_last[int(last_action)] = 1.0
                w[s, :] = (1 - refresh) * w[s, :] + refresh * one_hot_last

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

            last_state = s
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p