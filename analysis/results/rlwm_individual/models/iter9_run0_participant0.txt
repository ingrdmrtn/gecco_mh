def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-adaptive arbitration and surprise-gated WM consolidation.

    Mechanism:
    - RL: standard delta-rule with softmax choice (beta scaled by 10).
    - WM store: per-state action distribution w[s,:] that is sharpened toward
      the chosen action when a gate opens; the gate depends on (signed) surprise
      r - Q(s,a) and a sensitivity parameter.
    - Arbitration: the effective WM weight is a logistic transform of a baseline
      plus a load term that reduces WM under higher set sizes.
    - WM policy: softmax over WM row with precision controlled by wm_precision.

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_weight0: real, baseline for WM mixture weight before load adjustment.
                  Effective weight = sigmoid(wm_weight0 + load_slope*(3 - nS)).
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - gate_sensitivity: >=0, controls how strongly (r - Q) opens the WM gate.
                        Larger â†’ reward-consistent choices consolidate more.
    - wm_precision: >=0, scales the WM softmax precision (higher is more precise).
    - load_slope: real, how much smaller set sizes (lower load) boost WM weight.
                  Positive values increase WM reliance at nS=3 vs nS=6.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, gate_sensitivity, wm_precision, load_slope = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic base for WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-adaptive WM mixture (state-independent within block)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-(wm_weight0 + load_slope * (3 - nS))))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: precision scaled by wm_precision
            beta_wm_eff = softmax_beta_wm * max(wm_precision, 1e-6)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: surprise-gated consolidation toward chosen action
            gate = 1.0 / (1.0 + np.exp(-gate_sensitivity * (r - Q_s[a])))
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            # Small drift toward uniform to avoid overcommitment
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            # Consolidate proportionally to gate
            w[s, :] = (1.0 - gate) * w[s, :] + gate * (0.8 * w[s, :] + 0.2 * one_hot)

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + Bayesian WM counts with load-dependent precision.

    Mechanism:
    - RL: delta-rule plus small exponential forgetting toward uniform baseline.
    - WM: per-state Dirichlet-like action counts (c[s,a]); policy uses the
      normalized counts as logits with a precision that decreases with load.
    - WM updating: reward strengthens chosen action's count; non-reward softly
      shifts mass to alternatives. Counts also undergo mild stabilization toward
      a base concentration to prevent unbounded growth.
    - Policy: fixed mixture of RL and WM.

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_mix: [0,1] mixture weight of WM in the policy.
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - wm_conc0: >0, base concentration of WM counts per row (Dirichlet prior strength).
    - load_gamma: >=0, increases load impact; higher reduces WM precision as set size grows.
                  WM precision factor = wm_conc0 / (wm_conc0 + load_gamma * (nS - 1)).
    - rl_forget: [0,1], forgetting rate of RL values toward uniform each trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_mix, softmax_beta, wm_conc0, load_gamma, rl_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and baseline
        q = (1.0 / nA) * np.ones((nS, nA))
        q0 = (1.0 / nA) * np.ones((nS, nA))

        # WM counts initialized with symmetric prior
        counts = (wm_conc0 / nA) * np.ones((nS, nA))
        w = counts / np.sum(counts, axis=1, keepdims=True)  # normalized WM distributions
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent precision scaling for WM softmax
        prec_scale = wm_conc0 / max(wm_conc0 + load_gamma * (nS - 1), 1e-12)
        beta_wm_eff = softmax_beta_wm * prec_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy from counts' normalized distribution with load-adjusted precision
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with forgetting
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q = (1.0 - rl_forget) * q + rl_forget * q0

            # WM counts update: reward strengthens chosen action, otherwise redistribute a bit
            if r > 0.5:
                counts[s, a] += 1.0
            else:
                # move a small fraction from chosen to others
                shift = 0.3
                dec = min(shift, counts[s, a] * 0.5)
                counts[s, a] -= dec
                inc = dec / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        counts[s, aa] += inc

            # Stabilize counts to avoid unbounded growth: tiny pull toward prior mass
            total_prior = wm_conc0
            current_total = np.sum(counts[s, :])
            if current_total > 1e-12:
                blend = 0.02
                counts[s, :] = (1.0 - blend) * counts[s, :] + blend * (total_prior * w_0[s, :])

            # Normalize to get WM distribution
            row_sum = np.sum(counts[s, :])
            if row_sum > 1e-12:
                w[s, :] = counts[s, :] / row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty arbitration and load-dependent WM decay.

    Mechanism:
    - RL: standard delta-rule with softmax choice.
    - WM: distribution over actions per state that is updated error-correctively
      toward the chosen action after reward; small correction away after non-reward.
    - Arbitration: WM weight increases when RL is uncertain (high entropy) and
      decreases with load. Weight is a logistic transform of a baseline plus
      an entropy term (scaled by arb_kappa) minus a load penalty.
    - WM decay: toward uniform with strength increasing with load (decay_load).

    Parameters:
    - lr: [0,1] learning rate for RL values.
    - wm_w0: real, baseline input to the logistic arbitration (prior WM reliance).
    - softmax_beta: base inverse temperature for RL (internally scaled by 10).
    - arb_kappa: >=0, scales the impact of RL entropy on arbitration; higher makes
                 the model rely more on WM when RL is uncertain.
    - wm_update: [0,1], step size for WM corrections toward/away from chosen action.
    - decay_load: >=0, load sensitivity of WM decay toward uniform (stronger with larger nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_w0, softmax_beta, arb_kappa, wm_update, decay_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load-dependent decay per trial
        decay = np.clip(decay_load * max(0, nS - 3) / max(1, 6 - 3), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy distribution (for entropy)
            logits = softmax_beta * Q_s
            logits = logits - np.max(logits)
            p_rl_full = np.exp(logits)
            p_rl_full = p_rl_full / max(np.sum(p_rl_full), 1e-12)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # RL entropy (max at log nA)
            eps = 1e-12
            H_rl = -np.sum(p_rl_full * np.log(np.clip(p_rl_full, eps, 1.0)))
            H_max = np.log(nA)

            # Arbitration: more WM when RL uncertain, less WM under load
            wm_input = wm_w0 + arb_kappa * (H_rl / max(H_max, eps)) - (nS - 3) * 0.5 * decay_load
            wm_weight = 1.0 / (1.0 + np.exp(-wm_input))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform, stronger at higher set sizes
            w = (1.0 - decay) * w + decay * w_0

            # WM corrective update
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                # move WM toward chosen action
                w[s, :] = (1.0 - wm_update) * w[s, :] + wm_update * one_hot
            else:
                # move slightly away from chosen action
                away = wm_update * 0.4
                give = min(away, w[s, a] * 0.8)
                w[s, a] -= give
                w[s, :] += give / (nA - 1) * (1 - one_hot)

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p