def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Win-stay WM plus RL with set-size-biased arbitration.

    Idea
    - RL learns state-action values via a standard delta rule.
    - WM stores the last rewarded action for each state (win-stay), with global decay over time.
    - Action selection is a mixture of RL and WM policies.
    - The WM mixture weight is a static logistic function with a set-size bias (favoring WM in small sets if positive).

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block indices.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - softmax_beta_param: RL inverse temperature base, scaled internally by 10.
        - wm_gate_bias: Intercept for WM mixture weight (logit space).
        - size_arbit: Set-size bias for WM mixture weight; positive increases WM for nS=3, reduces for nS=6.
        - wm_decay: Per-trial global WM decay toward uniform (0..1).
        - ws_imprint: Strength of imprinting the rewarded action into WM (0..1).

    Set-size impact
    - WM weight per trial uses wm_gate_bias + size_arbit*(3.5 - nS), favoring WM more at small set size if size_arbit > 0.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_gate_bias, size_arbit, wm_decay, ws_imprint = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM policy table
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior

        # Compute fixed WM mixture weight for this block based on set size
        size_term = size_arbit * (3.5 - nS)
        wm_weight_block = 1.0 / (1.0 + np.exp(-(wm_gate_bias + size_term)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of the chosen action (softmax trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy derived from W_s via a near-deterministic softmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay of WM toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Win-stay imprint: if rewarded, imprint chosen action at state s
            if r > 0.0:
                w[s, :] = (1.0 - ws_imprint) * w[s, :]
                w[s, a] += ws_imprint

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with set-size-scaled noise.

    Idea
    - RL uses separate learning rates for positive and negative outcomes.
    - WM learns via reward-triggered imprint and forgets toward uniform after non-reward.
    - WM selection noise increases with set size, reducing WM precision for nS=6.
    - Choices are a fixed mixture of RL and WM.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block indices.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    model_parameters : iterable of length 6
        - alpha_pos: RL learning rate for positive outcomes.
        - alpha_neg: RL learning rate for negative outcomes.
        - softmax_beta_param: RL inverse temperature base, scaled internally by 10.
        - wm_weight: Fixed mixture weight for WM (0..1).
        - wm_noise_base: Base WM noise (smaller => sharper WM policy).
        - noise_size_gain: Increase in WM noise per +3 set-size step (>=0 increases noise at nS=6).

    Set-size impact
    - Effective WM inverse temperature decreases as set size increases:
      beta_wm(nS) = 1.0 / (wm_noise_base + noise_size_gain * (nS - 3)), shaping WM policy precision.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight, wm_noise_base, noise_size_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base scale; we will modulate via noise
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute WM precision for this set size
        wm_noise = wm_noise_base + noise_size_gain * (nS - 3)
        wm_noise = max(wm_noise, 1e-3)
        beta_wm = softmax_beta_wm / (1.0 + wm_noise)  # higher noise => lower beta

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy with set-size-dependent precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            eta = alpha_pos if delta >= 0.0 else alpha_neg
            q[s, a] += eta * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-driven imprint and non-reward forgetting toward uniform
            if r > 0.0:
                # move W_s toward one-hot on action a
                w[s, :] = (1.0 - alpha_pos) * w[s, :]
                w[s, a] += alpha_pos
            else:
                # non-reward => forget (flatten) W_s
                w[s, :] = (1.0 - alpha_neg) * w[s, :] + alpha_neg * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Surprise-gated arbitration with visit-dependent WM decay and size-penalized RL temperature.

    Idea
    - RL learns via delta rule; its softmax temperature is penalized by set size (less precise at nS=6).
    - WM stores recent state-action evidence with decay that depends on time since last visit to each state.
    - Arbitration weight (WM vs RL) increases with surprise (unsigned RPE), encouraging WM use when RL is uncertain/learning.
    - WM updates are surprise-proportional imprints for the current state.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block indices.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    model_parameters : iterable of length 6
        - lr: RL learning rate (0..1).
        - wm_weight_base: Base arbitration bias toward WM (logit space).
        - softmax_beta_param: Base RL inverse temperature (scaled internally by 10).
        - surprise_gain: How strongly unsigned RPE increases WM weight (logit slope).
        - visit_decay: Per-trial WM decay rate used to compute time-since-last-visit decay (0..1).
        - size_beta_penalty: Penalizes RL inverse temperature at larger set sizes (>=0).

    Set-size impact
    - RL precision is reduced as set size increases:
      beta_RL(nS) = softmax_beta_param * 10 * exp(-size_beta_penalty * (nS - 3)).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, surprise_gain, visit_decay, size_beta_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    visit_decay = np.clip(visit_decay, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last-visit time for each state to implement gap-dependent decay
        last_visit = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply time-since-last-visit decay to all states before using WM
            for j in range(nS):
                gap = (t - last_visit[j]) if last_visit[j] >= 0 else 1
                gap = max(gap, 1)
                # Effective retention over 'gap' steps
                lam = (1.0 - visit_decay) ** gap
                w[j, :] = lam * w[j, :] + (1.0 - lam) * w_0[j, :]

            last_visit[s] = t  # update visit time for current state

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL inverse temperature with set-size penalty
            beta_rl_t = softmax_beta * np.exp(-size_beta_penalty * (nS - 3))

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_rl_t * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy (deterministic readout of W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute surprise (unsigned RPE) based on current RL estimate
            delta = r - Q_s[a]
            wm_weight_t = 1.0 / (1.0 + np.exp(-(wm_weight_base + surprise_gain * np.abs(delta))))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Surprise-proportional imprint on WM for current state
            alpha_wm = np.clip(np.abs(delta), 0.0, 1.0)
            w[s, :] = (1.0 - alpha_wm) * w[s, :]
            w[s, a] += alpha_wm * r

        blocks_log_p += log_p

    return -blocks_log_p