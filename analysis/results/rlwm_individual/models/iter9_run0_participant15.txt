def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with dynamic arbitration by RL uncertainty and set size; WM decays and learns on reward.

    Mechanism
    - RL: tabular Q-learning with softmax policy (inverse temperature scaled by 10).
    - WM: state-action probability table; when rewarded, WM shifts toward a one-hot memory of the rewarded action;
      when not rewarded, WM decays toward uniform.
    - Arbitration: mixture weight for WM is a sigmoid of a linear combination of (a) set size and (b) RL uncertainty
      (entropy of the RL softmax policy in the current state). Higher RL uncertainty increases WM reliance; larger set
      size decreases WM reliance.

    Set-size dependence
    - The arbitration depends explicitly on set size via theta_size; larger set sizes reduce wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        theta0 : float
            Baseline bias for WM arbitration (positive favors WM).
        theta_size : float
            Linear penalty of set size on WM arbitration (positive reduces WM use as set size increases).
        theta_uncert : float
            Sensitivity to RL uncertainty (entropy); positive increases WM weight when RL is uncertain.
        wm_decay : float
            WM update/decay rate. On reward, WM shifts toward one-hot with this rate; on no reward, it decays
            toward uniform with this rate.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, theta0, theta_size, theta_uncert, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (softmax on WM table)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL uncertainty (entropy of RL softmax over current state)
            logits = softmax_beta * Q_s
            logits -= np.max(logits)
            pi = np.exp(logits)
            pi /= np.sum(pi)
            # entropy normalized to [0,1] by dividing by log(nA)
            entropy = -np.sum(pi * (np.log(pi + tiny))) / np.log(nA)

            # Dynamic WM arbitration weight via sigmoid
            x = theta0 + theta_uncert * entropy + theta_size * (nS - 3)
            wm_weight = 1.0 / (1.0 + np.exp(-x))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-driven strengthening vs decay to uniform
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.5:
                # move toward one-hot memory of rewarded action
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * onehot
            else:
                # decay toward uniform when not rewarded
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # normalize and floor for numerical stability
            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with precision and lapse.

    Mechanism
    - RL: tabular Q-learning with softmax policy (inverse temperature scaled by 10).
    - WM: probability table per state; on reward, moves toward one-hot memory with learning rate wm_learn; on no reward,
      decays toward uniform with the same wm_learn.
    - Arbitration: WM availability is capacity-limited: wm_weight = min(1, k_capacity / set_size).
      WM precision is scaled by tau_wm (multiplying the WM inverse temperature).
    - Lapse: with probability epsilon, choice is random.

    Set-size dependence
    - The arbitration weight scales inversely with set size through k_capacity: larger set sizes reduce wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        k_capacity : float
            Effective number of items that WM can support; WM weight is min(1, k_capacity / set_size).
        tau_wm : float
            Scales WM determinism; effective WM inverse temperature is tau_wm * 50.
        wm_learn : float
            WM update rate toward one-hot on reward and toward uniform on non-reward (0..1).
        epsilon : float
            Lapse probability mixing uniform choice into the final policy.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, k_capacity, tau_wm, wm_learn, epsilon = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base WM determinism
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight = min(1.0, float(k_capacity) / float(nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with adjustable precision
            beta_wm_eff = max(tau_wm, 0.0) * softmax_beta_wm
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Arbitration + lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)

            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * onehot
            else:
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with perseveration + WM confidence arbitration that downscales with set size.

    Mechanism
    - RL: tabular Q-learning with softmax policy (inverse temperature scaled by 10), plus a state-local perseveration
      bias added to the Q-value of the last chosen action in that state.
    - WM: probability table tracking confidence in the best action; on reward, WM moves toward one-hot with rate
      alpha_count; on no reward it decays toward uniform at rate wm_decay.
    - Arbitration: WM weight equals a confidence term (how peaked WM is) divided by a size penalty factor that increases
      with set size by an exponent size_exponent.

    Set-size dependence
    - The arbitration weight is divided by (1 + (set_size - 1)^size_exponent), decreasing WM influence in larger set sizes.

    Parameters
    ----------
    model_parameters : tuple
        lr : float
            RL learning rate (0..1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        persev : float
            Additive bias added to the Q-value of the last chosen action in a state (encourages staying).
        wm_decay : float
            WM decay toward uniform on non-rewarded trials (0..1).
        alpha_count : float
            WM update rate toward one-hot on rewarded trials (0..1).
        size_exponent : float
            Exponent controlling how strongly set size reduces WM weight (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, persev, wm_decay, alpha_count, size_exponent = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    tiny = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Size penalty factor for WM arbitration
        size_penalty = 1.0 + (max(nS - 1, 0)) ** max(size_exponent, 0.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += persev

            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence: 0 when uniform, 1 when perfectly peaked
            max_p = np.max(W_s)
            conf = (max_p - 1.0 / nA) / (1.0 - 1.0 / nA)
            conf = np.clip(conf, 0.0, 1.0)

            wm_weight = conf / size_penalty
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, tiny)
            log_p += np.log(p_total)

            # RL update (without perseveration in learning)
            Q_true = q[s, :]
            delta = r - Q_true[a]
            q[s][a] += lr * delta

            # WM update
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - alpha_count) * w[s, :] + alpha_count * onehot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], tiny)
            w[s, :] /= np.sum(w[s, :])

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p