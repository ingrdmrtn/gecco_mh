def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-gated WM with load-sensitive precision and WM learning

    Idea:
    - RL: standard delta rule with softmax.
    - WM: stores stateâ€“action associations; its policy precision decreases with set size.
    - Gating: mixture weight of WM vs RL is scaled by a load-dependent gate that down-weights WM at larger set sizes.
    - WM update: Hebbian when rewarded, weak anti-Hebbian when not; global decay increases as WM precision drops.

    Parameters:
    - lr: float in [0,1]. RL learning rate for Q-values.
    - beta_rl: float >= 0. Base RL inverse temperature; internally scaled by 10.
    - wm_weight_base: float in [0,1]. Baseline mixture weight on WM vs RL before load gating.
    - wm_precision_base: float in (0,1]. Baseline WM precision scaling (higher -> more deterministic WM).
    - load_gating_slope: float >= 0. How strongly larger set sizes reduce WM precision and WM gating.
    - wm_learn_rate: float in [0,1]. Learning rate for WM trace updates.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, beta_rl, wm_precision_base, load_gating_slope, wm_learn_rate = model_parameters
    softmax_beta = beta_rl
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-sensitive precision and gate
        load_factor = 1.0 + load_gating_slope * max(0, nS - 3)
        wm_precision_eff = np.clip(wm_precision_base / load_factor, 0.0, 1.0)
        gate = np.clip(wm_weight_base / load_factor, 0.0, 1.0)

        # Map precision to decay: lower precision -> stronger decay toward uniform
        wm_decay = np.clip(1.0 - wm_precision_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (kept as in template)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY: softmax with load-reduced precision
            beta_wm_eff = softmax_beta_wm * max(wm_precision_eff, eps)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            wm_weight = gate
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WORKING MEMORY UPDATING:
            # 1) Global decay toward uniform (stronger under load)
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # 2) Outcome-dependent local update
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0:
                # Hebbian strengthening of the chosen action
                w[s, :] = (1.0 - wm_learn_rate) * w[s, :] + wm_learn_rate * one_hot
            else:
                # Mild anti-Hebbian: push away from the chosen action
                anti = (w[s, :] * (1 - one_hot))
                anti = anti / max(anti.sum(), eps)
                w[s, :] = (1.0 - 0.5 * wm_learn_rate) * w[s, :] + 0.5 * wm_learn_rate * anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with choice-trace bias and load-dependent interference

    Idea:
    - RL: standard delta rule with softmax.
    - WM: leaky memory of rewarded actions per state.
    - Choice-trace bias: WM policy gets an additive boost for the most recent action in that state (captures short-term choice inertia within WM).
    - Load-dependent interference: WM decay accelerates with set size via a size_interference parameter.

    Parameters:
    - lr: float in [0,1]. RL learning rate for Q-values.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_weight: float in [0,1]. Mixture weight on WM vs RL policy.
    - rho_choice_trace: float >= 0. Additive bias applied to the last action in a state within WM policy.
    - size_interference: float >= 0. Scales the increase of WM decay with set size.
    - wm_decay: float in [0,1]. Baseline per-trial decay of WM toward uniform at set size 3.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_weight, rho_choice_trace, size_interference, wm_decay = model_parameters
    softmax_beta = beta_rl
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Last action per state for choice-trace bias (-1 = none)
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-dependent decay: increases with set size
        load_factor = 1.0 + size_interference * max(0, nS - 3)
        wm_decay_eff = 1.0 - (1.0 - wm_decay) / load_factor
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # Add choice-trace bias to WM activations (before softmax)
            if last_action[s] >= 0:
                W_s[last_action[s]] += rho_choice_trace

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with bias
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global decay toward uniform (load dependent)
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

            # WM local update: reward strengthens chosen association
            if r > 0:
                oh = np.zeros(nA); oh[a] = 1.0
                w[s, :] = 0.9 * w[s, :] + 0.1 * oh

            # Update last action trace
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + entropy-based meta-control of WM weighting with load penalty

    Idea:
    - RL: standard delta rule with softmax.
    - WM: stores associations; refreshed more after rewards.
    - Meta-control: per-trial WM mixture weight is a sigmoid of the entropy difference between RL and WM policies,
      penalized by set size (larger set sizes reduce reliance on WM).
      When RL is uncertain (high entropy) but WM is confident (low entropy), the controller shifts weight to WM.

    Parameters:
    - lr: float in [0,1]. RL learning rate for Q-values.
    - beta_rl: float >= 0. RL inverse temperature; internally scaled by 10.
    - wm_weight_bias: float (real). Baseline bias of the controller toward WM (positive favors WM).
    - entropy_slope: float >= 0. Sensitivity of the controller to entropy difference (H_rl - H_wm).
    - size_penalty: float >= 0. Linear penalty per extra item above 3 on WM weighting.
    - wm_refresh_gain: float in [0,1]. Magnitude of WM strengthening after rewards.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_weight_bias, entropy_slope, size_penalty, wm_refresh_gain = model_parameters
    softmax_beta = beta_rl
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    def softmax(vec, beta):
        z = vec - np.max(beta * vec)
        exps = np.exp(beta * vec + (-np.max(beta * vec)))
        s = exps / max(exps.sum(), eps)
        return s

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Size penalty term for controller
        size_pen = size_penalty * max(0, nS - 3)

        # Base WM decay
        base_decay = 0.05 + 0.05 * max(0, nS - 3)  # mild increase with set size

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (template)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Full policy distributions to compute entropy
            pi_rl = softmax(Q_s, softmax_beta)
            pi_wm = softmax(W_s, softmax_beta_wm)

            H_rl = entropy(pi_rl)
            H_wm = entropy(pi_wm)

            # Controller computes WM weight via sigmoid of entropy difference minus size penalty
            ctrl_input = wm_weight_bias + entropy_slope * (H_rl - H_wm) - size_pen
            wm_weight = 1.0 / (1.0 + np.exp(-ctrl_input))

            # WM policy probability of chosen action (using standard WM softmax)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating:
            # 1) Global decay toward uniform (slightly stronger under load)
            w = (1.0 - base_decay) * w + base_decay * w_0
            # 2) Reward-contingent refresh/anti-refresh
            one_hot = np.zeros(nA); one_hot[a] = 1.0
            if r > 0:
                w[s, :] = (1.0 - wm_refresh_gain) * w[s, :] + wm_refresh_gain * one_hot
            else:
                # slight suppression of the chosen action to reduce perseveration when unrewarded
                suppress = w[s, :].copy()
                suppress[a] = 0.0
                if suppress.sum() > 0:
                    suppress /= suppress.sum()
                    w[s, :] = 0.95 * w[s, :] + 0.05 * suppress

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects:
- cognitive_model1: set size reduces WM precision (beta_wm_eff) and the WM mixture weight (gate), and increases WM decay.
- cognitive_model2: set size increases WM decay via size_interference, indirectly lowering WM influence at set size 6.
- cognitive_model3: set size enters the meta-controller as a penalty term that shifts weight away from WM as the set grows, and slightly increases global WM decay.