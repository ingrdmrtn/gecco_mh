def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + reward-gated one-shot WM with set-size-dependent encoding and global decay.

    Mechanism:
    - RL: standard delta rule with learning rate lr.
    - WM: a fast associative store that moves toward a one-hot code for the chosen action
      when rewarded, and toward uniform otherwise. The encoding step size shrinks with set size.
      Additionally, WM decays globally toward uniform each trial (capacity/maintenance limits).
    - Mixture: convex combination of WM and RL policies with fixed wm_weight.
    - Set size effects:
        * Encoding step is reduced when nS=6: enc = p_encode_base / (1 + (nS-3)).
        * Noisy maintenance captured by per-trial decay toward uniform.

    Parameters
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], mixture weight on WM policy.
    - softmax_beta: >=0, base inverse temperature for RL (internally scaled by 10).
    - p_encode_base: [0,1], base step size for WM encoding on each trial (scaled down by set size).
    - decay_wm_trials: [0,1], per-trial WM decay rate toward uniform (applies to all states).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, p_encode_base, decay_wm_trials = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM encoding step
        enc = p_encode_base / (1.0 + max(0, nS - 3))

        # Clamp decay to [0,1]
        decay = min(max(decay_wm_trials, 0.0), 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability under softmax
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM preference vector
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated one-shot encoding plus global decay
            # Move toward one-hot on reward, toward uniform otherwise
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = np.ones(nA) / nA  # uniform fallback when outcome is uninformative
            w[s, :] = (1 - enc) * w[s, :] + enc * target

            # Global maintenance decay toward uniform (capacity/maintenance limits)
            w = (1 - decay) * w + decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency-limited WM with age- and set-size-dependent WM temperature.

    Mechanism:
    - RL: standard delta rule with learning rate lr.
    - WM: stores the last rewarded action per state (one-shot replacement). No update on losses.
      Memory reliability decays with the number of trials since the state was last seen (age).
      The WM softmax temperature is divided by a factor that grows with age and set size,
      making WM more random for older items and larger sets.
    - Mixture: convex combination with fixed wm_weight.
    - Set size effects:
        * WM temperature drops more sharply with age when nS=6 (via size_lag_slope).

    Parameters
    - lr: [0,1], RL learning rate.
    - wm_weight: [0,1], mixture weight on WM policy.
    - softmax_beta: >=0, base inverse temperature for RL (internally scaled by 10).
    - age_scale: >=0, scales how quickly WM reliability degrades with age.
    - size_lag_slope: >=0, additional degradation per age unit when nS=6 vs 3.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, age_scale, size_lag_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base deterministic WM; effective value will drop with age and set size

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track age (trials since last presentation) per state
        age = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            # Increment age for all states, reset for current state
            age += 1

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            age[s] = 0  # current state is just seen

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability under softmax
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Age- and size-dependent WM temperature (higher age => lower effective beta)
            degradation = 1.0 + age_scale * age[s] * (1.0 + size_lag_slope * max(0, nS - 3))
            beta_wm_eff = softmax_beta_wm / degradation

            # WM policy with effective temperature
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot replacement only on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            # On loss, keep existing WM trace (no reinforcement)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-weighted arbitration and set-size-dependent WM noise.

    Mechanism:
    - RL: standard delta rule with learning rate lr. RL policy used to compute entropy (uncertainty).
    - WM: associative delta-learning with its own learning rate eta_wm toward one-hot after reward
      and toward uniform after loss. WM softmax becomes noisier as set size increases.
    - Arbitration: dynamic WM weight given by a sigmoid of the difference between WM confidence
      (max entry of w[s]) and RL entropy; bias shifts weight down for larger set sizes.
    - Set size effects:
        * WM softmax temperature is scaled down by exp(-wm_noise * (nS-3)).
        * Arbitration includes a negative bias proportional to (nS-3).

    Parameters
    - lr: [0,1], RL learning rate.
    - softmax_beta: >=0, base inverse temperature for RL (internally scaled by 10).
    - eta_wm: [0,1], WM learning rate for associative updates.
    - kappa: >=0, gain controlling sensitivity of arbitration to confidence-entropy contrast.
    - size_bias: >=0, arbitration bias per extra item (reduces WM weight as set size grows).
    - wm_noise: >=0, scales how much WM temperature drops with set size.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, eta_wm, kappa, size_bias, wm_noise = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base WM inverse temperature before size-dependent noise

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM temperature scaling (more noise for larger sets)
        beta_wm_scale = np.exp(-wm_noise * max(0, nS - 3))
        beta_wm_eff_block = softmax_beta_wm * beta_wm_scale

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability under softmax
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute full RL policy to get entropy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl)
            probs_rl = probs_rl / np.sum(probs_rl)
            rl_entropy = -np.sum(probs_rl * np.log(np.maximum(probs_rl, 1e-12)))

            # WM chosen-action probability with size-dependent noise
            p_wm = 1 / np.sum(np.exp(beta_wm_eff_block * (W_s - W_s[a])))

            # WM confidence: peak of WM distribution for the state
            wm_conf = np.max(W_s)

            # Dynamic arbitration weight via sigmoid of (wm_conf - rl_entropy) with size bias
            arb_input = kappa * (wm_conf - rl_entropy) - size_bias * max(0, nS - 3)
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-arb_input))

            # Mixture
            p_total = wm_weight_dyn * p_wm + (1 - wm_weight_dyn) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM associative update: move toward one-hot on reward, else toward uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1 - eta_wm) * w[s, :] + eta_wm * target

        blocks_log_p += log_p

    return -blocks_log_p