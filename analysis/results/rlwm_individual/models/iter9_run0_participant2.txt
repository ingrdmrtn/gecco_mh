def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + reward-gated WM encoding with load-sensitive mixture.

    Idea:
    - RL learns Q via a standard delta rule and softmax policy.
    - WM stores a sharp associative mapping for a state only when two conditions hold:
        (i) the trial is rewarded, and
        (ii) RL is confident for that state (margin gate > gate_thr).
      This approximates selective gating into WM based on reliability.
    - WM traces decay toward uniform between uses (wm_decay).
    - The WM mixture weight is load-sensitive: higher set size reduces reliance on WM
      by compressing the base wm_weight through a logistic transformation.

    Parameters (model_parameters): 5 params
    - lr: (0,1) RL learning rate.
    - wm_weight: (0,1) Base mixture weight for WM (before load compression).
    - softmax_beta: >0 RL inverse temperature (internally scaled x10).
    - gate_thr: >=0 Confidence margin threshold for committing to WM.
    - wm_decay: (0,1) Per-trial decay of WM traces toward uniform.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, gate_thr, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compress WM reliance by load (more load -> less WM)
        def compress_mix(base_w, load):
            # logistic space adjustment with unit negative slope per extra item
            base_w = np.clip(base_w, 1e-6, 1 - 1e-6)
            logit = np.log(base_w) - np.log(1 - base_w)
            adj = logit - (load - 3.0)  # unit slope w.r.t. items beyond 3
            out = 1.0 / (1.0 + np.exp(-adj))
            return np.clip(out, 0.0, 1.0)

        wm_mix_eff = compress_mix(wm_weight, nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy (full softmax for stability)
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = float(p_rl_vec[a])

            # WORKING MEMORY POLICY: sharp softmax over WM weights
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = float(p_wm_vec[a])

            # Mixture
            p_total = wm_mix_eff * p_wm + (1.0 - wm_mix_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM UPDATE:
            # 1) continuous decay toward uniform (leak)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) reward-gated encoding if RL is confident for this state
            if r > 0.5:
                # margin between top-1 and top-2 Q
                sorted_Q = np.sort(Q_s)[::-1]
                margin = sorted_Q[0] - sorted_Q[1] if len(sorted_Q) > 1 else sorted_Q[0]
                if margin >= gate_thr:
                    # commit a sharp one-hot code into WM
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                else:
                    # mild Hebbian-like nudge toward chosen action when not confident
                    w[s, :] = 0.9 * w[s, :]
                    w[s, a] += 0.1
                    w[s, :] = np.clip(w[s, :], 0.0, None)
                    w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load-dependent WM leak.

    Idea:
    - RL uses softmax with delta-rule learning.
    - WM stores an associative vector per state; rewarded trials push WM toward a one-hot
      code, while ongoing leak pulls WM toward uniform. Leak increases with set size.
    - Arbitration: when RL is precise (low entropy), rely more on RL; when RL is uncertain
      (high entropy), rely more on WM. Base WM reliance is further penalized by load.

    Parameters (model_parameters): 5 params
    - lr: (0,1) RL learning rate.
    - wm_weight: (0,1) Base WM mixture weight before arbitration.
    - softmax_beta: >0 RL inverse temperature (internally scaled x10).
    - entropy_gain: real. Positive values increase WM reliance as RL entropy grows.
    - wm_leak: (0,1) Base WM leak toward uniform; effective leak grows with set size.

    Returns:
    - Negative log-likelihood of choices.
    """
    lr, wm_weight, softmax_beta, entropy_gain, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load-modulated leak factor
        leak_eff = np.clip(wm_leak * (1.0 + 0.5 * (nS - 3.0)), 0.0, 1.0)

        # Helper to bound mix in [0,1]
        def logistic(x):
            return 1.0 / (1.0 + np.exp(-x))

        # Base mix in logit space
        wm_base_logit = np.log(np.clip(wm_weight, 1e-6, 1 - 1e-6)) - np.log(np.clip(1 - wm_weight, 1e-6, 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = float(p_rl_vec[a])

            # RL entropy (base 2 not required since it's monotonic)
            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, eps, 1.0)))
            H_max = np.log(nA)

            # Arbitration: more WM when RL is uncertain, less WM under load
            wm_logit = wm_base_logit + entropy_gain * (H_rl - 0.5 * H_max) - (nS - 3.0)
            wm_mix_eff = logistic(wm_logit)
            wm_mix_eff = np.clip(wm_mix_eff, 0.0, 1.0)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = float(p_wm_vec[a])

            # Mixture
            p_total = wm_mix_eff * p_wm + (1.0 - wm_mix_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM UPDATE
            # 1) leak toward uniform (load-inflated)
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # 2) reward-driven sharpening toward a one-hot code
            if r > 0.5:
                # use leak as a "learning rate" to keep param count compact
                alpha_wm = np.clip(2.0 * wm_leak, 0.0, 1.0)
                w[s, :] = (1.0 - alpha_wm) * w[s, :]
                w[s, a] += alpha_wm
                # renormalize
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent decay + WM with probabilistic encoding and forgetting.

    Idea:
    - RL: delta-rule with softmax action selection, plus small decay of Q toward
      uniform that increases with set size (credit dilution under higher load).
    - WM: on rewarded trials, the correct mapping is encoded with probability
      p_enc = min(1, (3/nS)^phi_enc) (encoding gets harder with larger set size).
      WM traces forget toward uniform with probability increasing with set size.
    - Choice: fixed base mixture weight (wm_weight) between WM and RL policies.

    Parameters (model_parameters): 6 params
    - lr: (0,1) RL learning rate.
    - wm_weight: (0,1) Mixture weight for WM policy.
    - softmax_beta: >0 RL inverse temperature (internally scaled x10).
    - phi_enc: >=0 Load-sensitivity exponent for WM encoding probability.
    - forget_gain: real. Higher -> more forgetting with larger set size.
    - rl_decay_gain: >=0 Scales RL value decay toward uniform as set size grows.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, phi_enc, forget_gain, rl_decay_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    rng = np.random.RandomState(0)  # deterministic; used only to define expectation below

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load terms
        p_enc = min(1.0, (3.0 / float(nS)) ** max(0.0, phi_enc))
        # Map forget_gain*(nS-3) to [0,1) via logistic, then scale
        f_load = 0.5 * (1.0 / (1.0 + np.exp(-forget_gain * (nS - 3.0))))
        # RL decay per update toward uniform Q
        rl_decay = np.clip(rl_decay_gain * max(0.0, nS - 3.0) / max(1.0, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = float(p_rl_vec[a])

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = float(p_wm_vec[a])

            # Mixture
            wm_mix_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_mix_eff * p_wm + (1.0 - wm_mix_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with load-dependent decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # decay all actions toward uniform reference
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (np.mean(q[s, :]))  # mild homogenization

            # WM UPDATE
            # 1) forgetting toward uniform
            w[s, :] = (1.0 - f_load) * w[s, :] + f_load * w_0[s, :]

            # 2) probabilistic encoding on rewarded trials (use expected update to avoid randomness)
            # Instead of sampling, apply expected update: p_enc fraction toward one-hot
            if r > 0.5:
                w[s, :] = (1.0 - p_enc) * w[s, :]
                w[s, a] += p_enc
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p