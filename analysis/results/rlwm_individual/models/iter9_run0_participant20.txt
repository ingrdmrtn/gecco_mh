def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with novelty-gated arbitration and set-size–dependent WM precision.

    Idea:
    - Choices are a mixture of RL (Q-learning) and WM policies.
    - WM precision (inverse temperature) decreases with set size (load).
    - WM mixture weight is boosted for novel states (fewer prior encounters), and shrinks as a state becomes familiar.
    - WM stores the last rewarded action in a near one-shot manner and leaks toward uniform when not rewarded.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight_base: base WM mixture weight in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_precision_base: base WM inverse temperature (higher -> more deterministic).
    - precision_setsize_slope: nonnegative; reduces WM precision as set size increases.
    - novelty_decay: nonnegative; controls how fast the WM weight drops with state familiarity (visit count).

    Set size effects:
    - WM precision is reduced as nS increases via beta_wm = max(1, wm_precision_base / (1 + precision_setsize_slope*(nS-3))).
    """
    lr, wm_weight_base, softmax_beta, wm_precision_base, precision_setsize_slope, novelty_decay = model_parameters
    softmax_beta *= 10.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        visits = np.zeros(nS)  # state-wise encounter counts
        beta_wm = max(1.0, wm_precision_base / (1.0 + precision_setsize_slope * max(0, nS - 3)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # novelty-gated WM mixture weight for this state
            w_novel = np.exp(-novelty_decay * visits[s])
            wm_w_eff = np.clip(wm_weight_base * w_novel, 0.0, 1.0)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: one-shot store on reward; otherwise leak toward uniform
            if r > 0.5:
                w[s, :] = 0.0 * w[s, :] + w_0[s, :]
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            visits[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + fast WM with entropy-based arbitration and set-size–dependent WM decay.

    Idea:
    - Two controllers: model-free RL (Q-learning) and a fast WM value table W.
    - Arbitration weight favors the more reliable controller, where reliability is the inverse entropy
      of its action distribution for the current state. Arbitration is passed through a softmax with temperature.
    - WM decays toward uniform more strongly in larger set sizes.
    - WM learns quickly via its own learning rate toward a supervised target and decays otherwise.

    Parameters (model_parameters):
    - lr_q: RL learning rate in [0,1].
    - lr_wm: WM learning rate in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - decay_setsize_base: base WM decay strength toward uniform (>=0).
    - decay_setsize_slope: additional WM decay per item above 3 (>=0).
    - arb_temp: arbitration inverse temperature controlling how sharply reliability differences influence weights (>=0).

    Set size effects:
    - WM decay increases with set size: decay = decay_setsize_base + decay_setsize_slope * (nS-3)+.
    """
    lr_q, lr_wm, softmax_beta, decay_setsize_base, decay_setsize_slope, arb_temp = model_parameters
    softmax_beta *= 10.0
    arb_temp = max(0.0, float(arb_temp))
    eps = 1e-12

    def softmax_prob(beta, vec, chosen):
        return 1.0 / np.sum(np.exp(beta * (vec - vec[chosen])))

    def entropy_from_logits(beta, vec):
        logits = beta * vec
        logits = logits - np.max(logits)
        probs = np.exp(logits)
        probs = probs / np.sum(probs)
        probs = np.clip(probs, eps, 1.0)
        return -np.sum(probs * np.log(probs))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        extra = max(0, nS - 3)
        wm_decay = max(0.0, decay_setsize_base + decay_setsize_slope * extra)
        beta_wm = 50.0  # WM assumed sharp; reliability will still be entropy-dependent

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = softmax_prob(softmax_beta, Q_s, a)
            p_wm = softmax_prob(beta_wm, W_s, a)

            # reliabilities as inverse entropies
            H_rl = entropy_from_logits(softmax_beta, Q_s)
            H_wm = entropy_from_logits(beta_wm, W_s)
            rel_rl = 1.0 / (H_rl + eps)
            rel_wm = 1.0 / (H_wm + eps)

            # arbitration: softmax over reliabilities
            logits = arb_temp * np.array([rel_rl, rel_wm])
            logits = logits - np.max(logits)
            weights = np.exp(logits)
            weights /= np.sum(weights)
            w_rl, w_wm = float(weights[0]), float(weights[1])

            p_total = w_wm * p_wm + w_rl * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr_q * (r - q[s, a])

            # WM supervised update toward target; then decay toward uniform
            target = w_0[s, :].copy()
            if r > 0.5:
                target[:] = 0.0
                target[a] = 1.0
            w[s, :] = (1.0 - lr_wm) * w[s, :] + lr_wm * target
            # decay toward uniform with set-size–dependent strength
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM with load-driven interference and temporal decay.

    Idea:
    - RL provides gradual learning of Q-values.
    - WM stores the most recent correct action per state, but its effective influence is reduced
      when set size exceeds capacity (K) and by inter-item interference; it also decays with time.
    - Arbitration is a fixed mixture scaled by the availability of WM under load and decay.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: baseline mixture weight for WM in [0,1].
    - capacity_K: WM capacity (continuous >=1), controls availability when nS > K.
    - inter_item_binding: nonnegative; per-item interference penalty when nS exceeds capacity.
    - wm_decay_rate: in [0,1]; temporal decay of WM for a visited state toward uniform on each visit.

    Set size effects:
    - Effective WM availability: avail = min(1, K/nS) * exp(-inter_item_binding * max(0, nS - K)).
      This multiplicatively reduces the baseline WM weight.
    """
    lr, softmax_beta, wm_weight_base, capacity_K, inter_item_binding, wm_decay_rate = model_parameters
    softmax_beta *= 10.0
    wm_decay_rate = np.clip(wm_decay_rate, 0.0, 1.0)
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # availability factor from capacity and interference
        over = max(0.0, nS - float(capacity_K))
        cap_frac = min(1.0, float(capacity_K) / nS)
        avail = cap_frac * np.exp(-inter_item_binding * over)
        avail = np.clip(avail, 0.0, 1.0)
        wm_weight_eff_block = np.clip(wm_weight_base * avail, 0.0, 1.0)

        beta_wm = 50.0  # when WM is available, it is sharp

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: reward-driven store; otherwise partial unstore; both with temporal decay
            if r > 0.5:
                # store the rewarded action strongly
                w[s, :] = (1.0 - wm_decay_rate) * 0.0 + w_0[s, :]
                w[s, :] = (1.0 - 0.0) * w_0[s, :]
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
            else:
                # penalize the chosen action slightly and decay toward uniform
                w[s, a] = (1.0 - wm_decay_rate) * w[s, a] + wm_decay_rate * (1.0 / nA)
                others = [aa for aa in range(nA) if aa != a]
                w[s, others] = (1.0 - wm_decay_rate) * w[s, others] + wm_decay_rate * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p