def cognitive_model1(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + WM with uncertainty-weighted arbitration and size-dependent WM decay.

    Idea
    - Choices are a mixture of RL (Q-learning) and WM (fast table).
    - Arbitration weight for WM is adjusted online by RL uncertainty in the current state:
      - When RL is uncertain (flat Q), WM gets more weight; when RL is peaked, RL gets more weight.
      - Uncertainty is measured as the entropy of the RL softmax policy.
      - The gain (unc_gain) controls how strongly uncertainty shifts the mixture.
    - WM precision is high but decays faster with larger set size.
    - WM stores a reward-modulated bump for the chosen action.

    Parameters
    ----------
    parameters : tuple/list of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_mix : float in [0,1]
            Baseline mixture weight on WM before arbitration.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10).
        unc_gain : float >= 0
            Gain controlling how much RL uncertainty shifts weight toward WM.
        wm_decay : float in [0,1]
            Base decay of WM toward uniform each time the state is visited; amplified by set size.

    Set-size impact
    ---------------
    - WM decay increases with set size: decay_eff = wm_decay * (nS/3).
    - WM precision is fixed high but arbitration increases WM weight more in large sets
      indirectly because RL tends to be more uncertain with more competing states.
    """
    lr, wm_weight, softmax_beta, unc_gain, wm_decay = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Policy for the working memory
            # - Deterministic softmax over WM table
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-weighted arbitration:
            # compute RL policy over actions to estimate entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            entropy = -np.sum(pi * np.log(np.maximum(pi, 1e-12)))
            # Normalize entropy to [0,1] via dividing by log(nA)
            entropy_norm = entropy / np.log(nA)

            wm_weight_adj = wm_weight + unc_gain * (entropy_norm - 0.5)
            wm_weight_adj = np.clip(wm_weight_adj, 0.0, 1.0)

            p_wm = p_wm_soft
            p_total = p_wm * wm_weight_adj + (1 - wm_weight_adj) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Value updating for the working memory
            # - Decay toward uniform with size-dependent amplification
            decay_eff = wm_decay * (float(nS) / 3.0)
            decay_eff = np.clip(decay_eff, 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]
            # - Reward-modulated bump for the chosen action
            w[s, a] += wm_weight * (0.5 + 0.5 * r)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + WM with explicit WM precision and interference scaling by set size.

    Idea
    - Choices combine RL and WM.
    - WM policy has its own precision (beta_wm_base), reduced by interference that grows with set size.
    - Interference_rate scales the drop in WM precision as nS increases.
    - WM update uses Hebbian-like increment on chosen action and lateral inhibition on others.

    Parameters
    ----------
    parameters : tuple/list of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight for WM in the final policy.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10).
        beta_wm_base : float >= 0
            Base WM inverse temperature before interference (higher => sharper WM policy).
        interference_rate : float >= 0
            Amount by which set size reduces WM precision: beta_wm_eff = beta_wm_base / (1 + interference_rate*(nS-3)).

    Set-size impact
    ---------------
    - WM precision decreases with set size: beta_wm_eff = beta_wm_base / (1 + interference_rate*(nS-3)).
    - Larger set sizes therefore shift policy reliance toward RL when WM precision collapses.
    """
    lr, wm_weight, softmax_beta, beta_wm_base, interference_rate = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # not used directly; beta_wm_base drives WM precision here
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM precision after set-size interference
        beta_wm_eff = beta_wm_base / (1.0 + interference_rate * max(0.0, float(nS) - 3.0))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Policy for the working memory
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Value updating for the working memory
            # - Mild decay to prior
            decay = 0.1 * (1.0 + interference_rate * max(0.0, float(nS) - 3.0))
            decay = np.clip(decay, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # - Hebbian-like increment for chosen action, lateral normalization on others
            bump = wm_weight * (0.3 + 0.7 * r)
            w[s, a] += bump
            w[s, :] -= (bump / (nA - 1.0)) * (np.ones(nA) - np.eye(1, nA, a).flatten())

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, parameters):
    """
    RL + Win-Stay-Lose-Shift WM buffer with size-dependent lapses and RL forgetting.

    Idea
    - RL uses Q-learning with an intra-block forgetting toward uniform (alpha_forget).
    - WM implements a WSLS heuristic: when rewarded, strengthen the chosen action; when unrewarded,
      suppress it and boost alternatives; executed as a soft preference table W.
    - WM lapses increase with set size (more items => noisier retrieval).
    - Final choice is a mixture of RL and WM.

    Parameters
    ----------
    parameters : tuple/list of length 5
        lr : float in [0,1]
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight for WM in the final policy.
        softmax_beta : float >= 0
            RL inverse temperature (internally scaled by 10).
        alpha_forget : float in [0,1]
            RL forgetting rate toward uniform each time a state is visited.
        lapse_base : float in [0,1]
            Base WM lapse probability; effective lapse increases with set size.

    Set-size impact
    ---------------
    - WM lapse increases with set size: lapse_eff = 1 - (1 - lapse_base)^(nS/3).
    - Larger set sizes therefore flatten WM policy more often, favoring uniform choices unless RL is confident.
    """
    lr, wm_weight, softmax_beta, alpha_forget, lapse_base = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic when WM does not lapse
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Size-dependent WM lapse
        lapse_eff = 1.0 - (1.0 - np.clip(lapse_base, 0.0, 1.0)) ** (max(1.0, float(nS)) / 3.0)
        lapse_eff = np.clip(lapse_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Policy for the working memory
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse_eff) * p_wm_soft + lapse_eff * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            # RL update with forgetting toward uniform
            q[s][a] += lr * delta
            q[s, :] = (1.0 - alpha_forget) * q[s, :] + alpha_forget * (1.0 / nA)

            # Value updating for the working memory (WSLS-like)
            # - Small decay to prior to keep bounded
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            if r > 0.5:
                # Win-stay: increase chosen action preference
                w[s, a] += wm_weight * 0.5
            else:
                # Lose-shift: decrease chosen action, slightly boost others
                penal = wm_weight * 0.4
                w[s, a] -= penal
                w[s, :] += (penal / (nA - 1.0)) * (np.ones(nA) - np.eye(1, nA, a).flatten())

        blocks_log_p += log_p

    return -blocks_log_p