def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size penalty.

    Mechanism:
    - RL: standard delta rule with inverse-temperature softmax.
    - WM: fast associative table updated toward a one-hot on reward.
    - Arbitration: WM weight increases with RL uncertainty (entropy of RL policy),
      and decreases with set size (log penalty). The base WM weight is mapped
      through a logit link to keep the effective weight in (0,1).

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0-1)
    - wm_weight_base: baseline WM mixture weight before arbitration (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - k_entropy: slope for entropy-based arbitration; higher => more WM use when RL is uncertain (>=0)
    - ns_bias: set-size penalty on WM weight via -ns_bias*log(nS); higher => stronger downweighting with set size (>=0)
    - eta_wm: WM encoding strength toward one-hot when rewarded (0-1)

    Set-size effects:
    - Effective WM weight is downregulated with larger nS via -ns_bias*log(nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, k_entropy, ns_bias, eta_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    # helper: logit transform for baseline weight
    wm_weight_base = np.clip(wm_weight_base, 1e-6, 1 - 1e-6)
    base_logit = np.log(wm_weight_base) - np.log(1 - wm_weight_base)

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL policy distribution to get entropy
            rl_pref = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_pi = rl_pref / np.sum(rl_pref)
            rl_pi = np.clip(rl_pi, 1e-12, 1.0)
            H_rl = -np.sum(rl_pi * np.log(rl_pi + 1e-12))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: logistic of base_logit + k_entropy*H_rl - ns_bias*log(nS)
            x = base_logit + k_entropy * H_rl - ns_bias * np.log(max(nS, 1))
            wm_weight_eff = 1.0 / (1.0 + np.exp(-x))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: encode rewarded choice toward one-hot, otherwise slight relax to prior
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * target
            else:
                # small relaxation toward uniform to prevent overconfidence on errors
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-dependent Q-decay + WM with load-dependent noise.

    Mechanism:
    - RL: delta rule plus set-size-dependent decay toward uniform for the visited state row.
           Decay increases with nS via a saturating function 1-exp(-k*nS).
    - WM: fast table updated with learning rate alpha_wm. WM policy suffers a load-induced lapse that
           increases with set size as epsilon_wm = 1 - exp(-wm_noise*(nS-1)), mixing with uniform.
    - Mixture: fixed wm_weight across trials, but the WM policy itself degrades with set size.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0-1)
    - wm_weight: mixture weight on WM vs RL (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - decay_q_ns: controls how much RL Q decays with set size per visit; effective decay = 1-exp(-decay_q_ns*nS) (>=0)
    - wm_noise: load-dependent WM lapse rate epsilon_wm = 1 - exp(-wm_noise*(nS-1)) (>=0)
    - alpha_wm: WM learning rate toward one-hot on outcome (0-1)

    Set-size effects:
    - Larger nS increases RL forgetting (stronger decay) and increases WM lapse (noisier WM policy).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, decay_q_ns, wm_noise, alpha_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (before lapse)
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute set-size dependent terms for the block
        decay_eff = 1.0 - np.exp(-max(0.0, decay_q_ns) * max(1, nS))
        epsilon_wm = 1.0 - np.exp(-max(0.0, wm_noise) * max(0, nS - 1))
        epsilon_wm = np.clip(epsilon_wm, 0.0, 0.49)  # keep it bounded

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply RL decay toward uniform for the visited state row
            q[s, :] = (1.0 - decay_eff) * q[s, :] + decay_eff * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy before lapse
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse-adjusted WM policy: epsilon mixes with uniform 1/nA
            p_wm = (1.0 - epsilon_wm) * p_wm_det + epsilon_wm * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: move toward one-hot on chosen action by alpha_wm scaled by outcome
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target
            else:
                # On error, slightly downweight chosen action and renormalize
                w[s, a] = 0.9 * w[s, a]
                # renormalize row to sum to 1
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] = w[s, :] / row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with interference and persistence.

    Mechanism:
    - RL: standard delta rule.
    - WM: table that strengthens on reward and decays toward uniform otherwise.
    - Capacity/interference: effective WM weight per trial depends on how many unique states
      have appeared so far (m). WM weight scales as wm_weight * (min(1, C/m))^(1+xi),
      so it diminishes as more distinct items are encountered, with softness controlled by xi.
    - Persistence: WM entries decay at each visit toward prior with factor (1 - kappa).

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: base WM weight before capacity scaling (0-1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - C: WM effective capacity in number of distinct states (>0)
    - xi: interference exponent; higher => sharper drop past capacity (>=0)
    - kappa: per-visit WM persistence (0-1); higher => slower decay of WM toward prior

    Set-size effects:
    - Larger blocks tend to present more distinct states m, thus downscaling WM weight via the C/m rule.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, C, xi, kappa = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        seen = set()

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Track distinct states seen so far
            seen.add(int(s))
            m = max(1, len(seen))
            # Capacity/interference scaling
            ratio = min(1.0, max(1e-6, C) / float(m))
            wm_weight_eff = wm_weight * (ratio ** (1.0 + max(0.0, xi)))

            # Apply WM persistence decay toward prior at each visit
            w[s, :] = kappa * w[s, :] + (1.0 - kappa) * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens the chosen association; no-reward weakens it
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # strong push toward one-hot on reward
                w[s, :] = 0.7 * w[s, :] + 0.3 * target
            else:
                # mild relaxation toward prior on error
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p