def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with recency-boosted recall.
    - Policy: convex mixture of RL softmax and WM softmax (from template).
    - WM recall is imperfect: when memory fails, WM reduces to uniform; recall
      probability increases with recency-strength and decreases with set size via
      a slot-like capacity term.
    - WM dynamics: state-specific memory strength with global leak; rewarded trials
      store the chosen action more strongly; unrewarded trials weakly store but with
      less weight.
    
    Parameters (tuple):
    - lr: RL and WM learning rate (0..1).
    - wm_weight: baseline arbitration weight for WM in the mixture (0..1).
    - softmax_beta: inverse temperature for RL softmax (internally scaled x10).
    - cap_slots: WM slot capacity (>=0). Recall probability scales as min(1, cap_slots / set_size).
    - wm_leak: global memory leak per trial for memory strength and WM table (0..1).
    - recency_boost: gain on memory strength from visiting a state (>=0), which boosts recall.
    
    Set size effect:
    - Larger set sizes reduce WM recall probability via cap_slots / nS, making WM contribute less
      reliable action probabilities.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, cap_slots, wm_leak, recency_boost = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # state-specific memory strength for recall; leaks each trial
        mem_strength = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Slot-limited base recall probability reduced by larger set sizes
            base_recall = np.clip(cap_slots / max(1.0, float(nS)), 0.0, 1.0)
            # Recency-boosted recall via memory strength for this state
            recall_boost = 1.0 - np.exp(-recency_boost * max(0.0, mem_strength[s]))
            recall_prob = np.clip(base_recall * recall_boost, 0.0, 1.0)

            # Deterministic WM policy, with fallback to uniform on recall failure
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = recall_prob * p_wm_soft + (1.0 - recall_prob) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global leak of memory strengths each trial
            mem_strength *= (1.0 - wm_leak)
            # Visit increases memory strength for current state
            mem_strength[s] += recency_boost

            # WM value table decays toward uniform for the visited state
            w[s,:] = (1.0 - wm_leak) * w[s,:] + wm_leak * w_0[s,:]
            # Store the chosen association; stronger when rewarded
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            lr_wm = lr * (0.5 + 0.5 * r)  # more update when r=1 than r=0
            w[s,:] = (1.0 - lr_wm) * w[s,:] + lr_wm * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and set-size dependent WM noise.
    - Policy: convex mixture of RL and WM (from template), but we dynamically
      rewrite wm_weight on each trial based on RL uncertainty (entropy).
    - WM policy: mixture of deterministic WM softmax and uniform noise; WM noise
      increases with set size.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM arbitration weight (0..1), modified dynamically.
    - softmax_beta: inverse temperature for RL softmax (internally scaled x10).
    - ent_gain: gain controlling how RL entropy shifts the arbitration weight (>=0).
                Higher entropy => increase WM weight.
    - wm_noise_base: baseline WM noise (0..1) in WM policy.
    - ss_noise_gain: additional WM noise per extra item above 3 (>=0).
    
    Set size effect:
    - Larger set sizes increase WM noise: noise = wm_noise_base + ss_noise_gain*(nS-3),
      degrading WM reliability and indirectly down-weighting WM's effective influence.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, ent_gain, wm_noise_base, ss_noise_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax probability for chosen action
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size dependent WM noise
            noise = wm_noise_base + ss_noise_gain * max(0, nS - 3)
            noise = np.clip(noise, 0.0, 1.0)
            p_wm = (1.0 - noise) * p_wm_det + noise * (1.0 / nA)

            # Entropy-based arbitration: compute RL choice distribution
            logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(logits)
            rl_probs = rl_probs / np.sum(rl_probs)
            # Normalized entropy in [0,1]
            H = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            H_norm = H / np.log(nA)
            # Adjust wm_weight on-the-fly by RL uncertainty, keeping it in [0,1]
            # Use logit transform for smooth bounded updates
            eps = 1e-8
            logit_w = np.log(np.clip(wm_weight, eps, 1-eps)) - np.log(np.clip(1-wm_weight, eps, 1-eps))
            logit_w = logit_w + ent_gain * (H_norm - 0.5)  # uncertain => increase WM
            wm_weight = 1.0 / (1.0 + np.exp(-logit_w))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Light decay toward uniform for visited state, magnitude coupled to noise
            w[s,:] = (1.0 - noise) * w[s,:] + noise * w_0[s,:]
            # Reward-weighted storing of chosen action
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            lr_wm = lr * (0.25 + 0.75 * r)  # weaker for errors, stronger for reward
            w[s,:] = (1.0 - lr_wm) * w[s,:] + lr_wm * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise-gated arbitration, reward-rate temperature adaptation,
    and set-size interference on WM sharpness.
    - Policy: convex mixture of RL and WM (from template). We dynamically adjust
      wm_weight downward on surprising outcomes (unsigned RPE).
    - RL softmax temperature adapts to running reward rate: more deterministic
      when recent rewards are high, more exploratory when low.
    - WM softmax sharpness is reduced by a power-law interference term with set size.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM arbitration weight (0..1), multiplicatively reduced by surprise.
    - softmax_beta: base inverse temperature for RL (internally scaled x10).
    - surpr_gain: gain (>=0) controlling reduction of wm_weight by unsigned RPE and 
                  increase of RL beta by high reward rate.
    - interf_exp: exponent (>=0) controlling how set size flattens WM (higher => more interference).
    - tau_reward: exponential averaging parameter (0..1) for running reward rate.
    
    Set size effect:
    - Larger set sizes decrease WM precision via beta_wm_eff = 50 / (1 + (nS-1)^interf_exp).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, surpr_gain, interf_exp, tau_reward = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running reward rate for adaptive RL temperature
        rew_avg = 0.5

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # Adapt RL beta with reward rate: higher reward => higher beta
            beta_rl_eff = softmax_beta * (1.0 + surpr_gain * (rew_avg - 0.5))
            beta_rl_eff = max(1e-6, beta_rl_eff)
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Set-size dependent interference reduces WM sharpness
            beta_wm_eff = softmax_beta_wm / (1.0 + (max(1, nS) - 1.0) ** max(0.0, interf_exp))
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Surprise-gated arbitration: reduce wm_weight when surprised (large |RPE|)
            rpe = r - Q_s[a]
            wm_weight = np.clip(wm_weight * (1.0 - surpr_gain * min(1.0, abs(rpe))), 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM decays toward uniform for visited state with interference-dependent rate
            decay = 1.0 - np.exp(- (1.0 + (max(1, nS) - 1.0) ** max(0.0, interf_exp)) * 0.02)
            w[s,:] = (1.0 - decay) * w[s,:] + decay * w_0[s,:]
            # Store current association with reward-weighted update
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            lr_wm = lr * (0.3 + 0.7 * r)
            w[s,:] = (1.0 - lr_wm) * w[s,:] + lr_wm * onehot

            # Update running reward rate
            rew_avg = (1.0 - tau_reward) * rew_avg + tau_reward * r

        blocks_log_p += log_p

    return -blocks_log_p