def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with interference and reward-gated WM learning.

    Mechanism
    ---------
    - RL: standard Q-learning and softmax choice (given by template).
    - WM store: associative table W updated by a reward-gated delta rule.
    - WM policy: a capacity-dependent recall probability multiplies a deterministic WM softmax;
                 when recall fails, WM contributes a uniform policy (guess).
    - Interference: WM traces leak toward uniform more strongly in larger set sizes.

    Set-size effects
    ----------------
    - phi_capacity controls how recall probability scales with set size: p_recall = sigmoid(phi_capacity) * (3/nS).
      Thus, recall drops as nS increases from 3 to 6.
    - rho_interf scales the leak of WM toward uniform as a function of set size (stronger for larger nS).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, phi_capacity, rho_interf, eta_wm)
        - lr: RL learning rate (0..1).
        - wm_weight: fixed arbitration weight on WM (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - phi_capacity: capacity strength; higher yields higher recall, transformed via sigmoid.
        - rho_interf: WM interference/leak strength per trial; effective leak grows with set size.
        - eta_wm: WM learning rate for associative update toward rewarded action (0..1).
    Returns
    -------
    Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, phi_capacity, rho_interf, eta_wm = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent recall probability (bounded in [0,1])
        cap_base = 1.0 / (1.0 + np.exp(-phi_capacity))  # sigmoid
        p_recall_ss = cap_base * (3.0 / float(nS))
        p_recall_ss = min(max(p_recall_ss, 0.0), 1.0)

        # Set-size dependent interference/leak rate per trial
        leak = 1.0 - np.exp(-rho_interf * (float(nS) / 3.0))
        leak = min(max(leak, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: with probability p_recall use deterministic WM softmax; otherwise uniform
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_recall_ss * p_wm_det + (1.0 - p_recall_ss) * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM associative update: reward-gated attraction to chosen action
            # Move W_s toward one-hot(a) if rewarded; otherwise slight relaxation to uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * target
            else:
                # On no-reward, reduce mistaken association by nudging toward uniform
                w[s, :] = (1.0 - 0.5 * eta_wm) * w[s, :] + (0.5 * eta_wm) * w_0[s, :]

            # Interference/leak toward uniform scaled by set size
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + discrete WM mapping with confidence and size-dependent recall; punishment weakens WM.

    Mechanism
    ---------
    - RL: standard Q-learning and softmax.
    - WM: stores a deterministic mapping for a state after reward; on non-reward, mapping confidence is weakened.
    - WM policy: mixes a deterministic readout with a uniform lapse based on a confidence term that
      scales down with set size.

    Set-size effects
    ----------------
    - Confidence multiplier scales as (3/nS)^xi_size, reducing WM reliability as set size increases.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_confidence0, xi_size, omega_punish)
        - lr: RL learning rate (0..1).
        - wm_weight: fixed arbitration weight on WM (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - wm_confidence0: baseline WM confidence controlling lapse within WM (mapped to 0..1 via sigmoid).
        - xi_size: size-scaling exponent for WM confidence (>0 increases sensitivity to set size).
        - omega_punish: degree to which non-reward weakens WM mapping toward uniform (0..1).
    Returns
    -------
    Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_confidence0, xi_size, omega_punish = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    base_conf = 1.0 / (1.0 + np.exp(-wm_confidence0))  # in (0,1)
    xi_size = max(xi_size, 0.0)
    omega_punish = min(max(omega_punish, 0.0), 1.0)

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        size_scale = (3.0 / float(nS)) ** xi_size
        wm_conf = min(max(base_conf * size_scale, 1e-6), 1.0 - 1e-6)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: confidence-weighted deterministic mapping with lapse to uniform
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = wm_conf * p_wm_det + (1.0 - wm_conf) * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - If rewarded, store deterministic mapping (one-hot) for this state.
            # - If not rewarded, weaken mapping toward uniform by omega_punish.
            if r > 0.0:
                new_row = np.zeros(nA)
                new_row[a] = 1.0
                w[s, :] = new_row
            else:
                w[s, :] = (1.0 - omega_punish) * w[s, :] + omega_punish * w_0[s, :]

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian-like WM with certainty that adapts and degrades with set size.

    Mechanism
    ---------
    - RL: standard Q-learning and softmax.
    - WM: associative table W plus a per-state certainty c_s that scales WM inverse temperature.
          Certainty increases after rewards, and decays/noises more strongly in larger set sizes.
    - WM policy: deterministic softmax using an effective beta_wm_eff = softmax_beta_wm * c_s.

    Set-size effects
    ----------------
    - size_noise increases the decay/noise of WM certainty c_s as set size grows (proportional to nS/3).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_cert_init, wm_cert_learn, size_noise)
        - lr: RL learning rate (0..1).
        - wm_weight: fixed arbitration weight on WM (0..1).
        - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
        - wm_cert_init: initial WM certainty per state at block start (>0).
        - wm_cert_learn: increment of certainty after a reward (>=0).
        - size_noise: per-trial certainty decay rate modulated by set size (>=0).
    Returns
    -------
    Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_cert_init, wm_cert_learn, size_noise = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    wm_cert_init = max(wm_cert_init, 1e-6)
    wm_cert_learn = max(wm_cert_learn, 0.0)
    size_noise = max(size_noise, 0.0)

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Per-state certainty for WM readout
        cert = np.ones(nS) * wm_cert_init

        # Set-size dependent decay toward baseline certainty of 1.0 (arbitrary scaling point)
        decay = 1.0 - np.exp(-size_noise * (float(nS) / 3.0))
        decay = min(max(decay, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with certainty-scaled inverse temperature
            beta_wm_eff = softmax_beta_wm * max(cert[s], 1e-6)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM associative update: move toward one-hot(a) on reward; slight relaxation otherwise
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * target
                cert[s] += wm_cert_learn
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Certainty decay/noise with set-size dependence
            cert[s] = (1.0 - decay) * cert[s] + decay * 1.0
            cert[s] = max(cert[s], 1e-6)

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p