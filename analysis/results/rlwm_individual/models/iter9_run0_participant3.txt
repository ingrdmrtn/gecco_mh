def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic WM with set-size-dependent decay.
    - RL: standard delta-rule with softmax.
    - WM: per-state categorical distribution over actions (w). Acts like a
      fast, item-specific memory that is pulled toward a one-hot vector for
      the last rewarded action; otherwise it decays toward uniform. Decay
      increases with set size (3 -> 6), reflecting higher cognitive load.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of int
        Reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) on each trial.
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight on WM policy versus RL policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL (scaled internally by 10).
        wm_lr : float in (0,1)
            WM encoding rate toward a one-hot vector when rewarded.
        decay_base : float in [0,1]
            Base decay of WM toward uniform at set size 3.
        decay_slope : float >= 0
            Additional decay per unit increase in set size beyond 3.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, decay_base, decay_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (not used directly; WM uses direct probs)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: use current WM categorical probability for chosen action,
            # after applying set-size-dependent decay toward uniform.
            set_size_t = int(block_set_sizes[t])
            decay_t = np.clip(decay_base + decay_slope * (set_size_t - 3), 0.0, 1.0)
            # Apply decay to the WM state before choice likelihood (as a continuous-time approximation)
            W_s = (1 - decay_t) * W_s + decay_t * w_0[s,:]
            W_s = np.maximum(W_s, 1e-12)
            W_s = W_s / W_s.sum()
            p_wm = W_s[a]

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # WM updating: after feedback, encode rewarded associations and re-normalize.
            if r >= 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s,:] = (1 - wm_lr) * w[s,:] + wm_lr * one_hot
            # Always apply decay over time to reflect forgetting/interference.
            w[s,:] = (1 - decay_t) * w[s,:] + decay_t * w_0[s,:]
            w[s,:] = np.maximum(w[s,:], 1e-12)
            w[s,:] = w[s,:] / w[s,:].sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM gated by surprise (state-specific RPE).
    - RL: separate learning rates for positive vs. negative prediction errors.
    - WM: fast table that encodes rewarded actions with rate alpha_wm.
      Its influence on choice is modulated by a gating variable proportional
      to the last absolute RPE observed in that state (surprise). When surprise
      is high, WM retrieval is stronger; otherwise WM is closer to uniform.
      Set size indirectly reduces WM influence because visits are sparser,
      but the gate is state-local and depends on stored surprise.
      This yields larger effective WM noise in larger set sizes.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    model_parameters : sequence
        lr_pos : float in (0,1)
            RL learning rate for positive PEs.
        lr_neg : float in (0,1)
            RL learning rate for negative PEs.
        wm_weight : float in [0,1]
            Mixture weight on WM policy versus RL policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL (scaled internally by 10).
        alpha_wm : float in (0,1)
            WM encoding rate toward one-hot upon reward.
        gate_gain : float >= 0
            Sensitivity of WM gate to absolute PE from the last visit of the state.
            Gate = 1 - exp(-gate_gain * |last_PE_state|), which weakens in larger sets
            due to longer inter-visit intervals (implicitly modeled by the state-local PE memory).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, alpha_wm, gate_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (we use WM probs directly with gating)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last absolute PE per state as a proxy for surprise for gating
        last_abs_pe = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: gate between uniform and WM distribution based on last_abs_pe
            g = 1.0 - np.exp(-gate_gain * np.abs(last_abs_pe[s]))  # in [0,1)
            p_wm = g * W_s[a] + (1 - g) * w_0[s, a]

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            eta = lr_pos if delta >= 0 else lr_neg
            q[s][a] += eta*delta

            # WM updating: encode on reward with alpha_wm; small passive drift toward uniform otherwise
            if r >= 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s,:] = (1 - alpha_wm) * w[s,:] + alpha_wm * one_hot
            else:
                # mild drift toward uniform to reflect forgetting/interference (implicit set-size effect)
                drift = 0.05
                w[s,:] = (1 - drift) * w[s,:] + drift * w_0[s,:]
            w[s,:] = np.maximum(w[s,:], 1e-12)
            w[s,:] = w[s,:] / w[s,:].sum()

            # Update gating signal with the absolute PE just experienced
            last_abs_pe[s] = np.abs(delta)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + precision-weighted WM (set-size sensitive).
    - RL: standard delta-rule with softmax.
    - WM: per-state probability vector w over actions, sharpened by a precision
      parameter kappa that declines with set size. The WM choice probability
      is proportional to w^kappa, i.e., more peaked for high precision.
      WM also forgets toward uniform at rate wm_forget.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    model_parameters : sequence
        lr : float in (0,1)
            RL learning rate.
        wm_weight : float in [0,1]
            Mixture weight on WM policy versus RL policy.
        softmax_beta : float >= 0
            Inverse-temperature for RL (scaled internally by 10).
        wm_precision_base : float >= 0
            WM precision (kappa) at set size 3.
        precision_slope : float >= 0
            Linear decrease in precision as set size increases: kappa = max(0, base - slope*(SS-3)).
        wm_forget : float in [0,1]
            Forgetting rate of WM toward uniform after each trial.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_precision_base, precision_slope, wm_forget = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (we implement precision transform explicitly)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: precision-weighted WM probabilities, with kappa set by set size.
            set_size_t = int(block_set_sizes[t])
            kappa = max(0.0, wm_precision_base - precision_slope * (set_size_t - 3))
            # Transform WM distribution by raising to kappa and renormalizing.
            # For kappa=0, this yields uniform; for large kappa, nearly one-hot on the max entry.
            W_eps = np.maximum(W_s, 1e-12)
            if kappa == 0.0:
                W_prec = w_0[s,:]
            else:
                W_prec = W_eps ** kappa
                W_prec = W_prec / W_prec.sum()
            p_wm = W_prec[a]

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # WM updating: encode on reward, then apply forgetting toward uniform.
            if r >= 0.5:
                # move w toward one-hot on chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # use a fast update proportional to (1 - w[s,a]) to increase chosen probability
                enc_rate = 0.5  # implicit encoding strength; interacts with kappa downstream
                w[s,:] = (1 - enc_rate) * w[s,:] + enc_rate * one_hot
            # forgetting/interference toward uniform (stronger effect in larger sets via lower kappa in policy)
            w[s,:] = (1 - wm_forget) * w[s,:] + wm_forget * w_0[s,:]
            w[s,:] = np.maximum(w[s,:], 1e-12)
            w[s,:] = w[s,:] / w[s,:].sum()

        blocks_log_p += log_p

    return -blocks_log_p