Below are three standalone cognitive models that follow your template, each exploring a distinct RL–WM interaction mechanism and a different way set size (3 vs 6) impacts parameters.

Note:
- Each function returns the negative log-likelihood of the observed choices.
- All parameters are used and each model has at most 6 parameters.
- I do not import packages inside the functions; assume numpy as np is available.
- I keep the RL policy as in the provided template and fill in the WM policy and WM updates in the [FILL IN ...] sections.
- I explicitly show how set size modulates WM dynamics or arbitration.

------------------------------------------------------------

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-gated WM (entropy arbitration) with set-size–scaled WM decay.

    Description:
    - RL: tabular Q-learning with softmax policy.
    - WM: probabilistic memory distribution per state (w), updated toward the last rewarded action.
      WM decays toward uniform each trial, with stronger decay for larger set sizes.
    - Arbitration: trial-wise WM weight increases when WM is confident (low entropy) and decreases otherwise.
      This confidence gating is controlled by an entropy_bias parameter.
    - Set size effect: WM decay is scaled up with larger sets (nS=6 vs 3).

    Parameters
    - model_parameters[0] = lr (float): RL learning rate (0..1).
    - model_parameters[1] = wm_phi (float): WM encoding gain (>0); scales how much WM moves toward rewarded action.
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally multiplied by 10.
    - model_parameters[3] = wm_decay_ctx (float): Base WM decay toward uniform (0..1); scaled by set size.
    - model_parameters[4] = entropy_bias (float): Sensitivity of arbitration to WM certainty (entropy gating; can be positive).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_phi, softmax_beta, wm_decay_ctx, entropy_bias = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # WM policy distribution
        w_0 = (1 / nA) * np.ones((nS, nA)) # Uniform prior for WM

        # Effective WM decay increases with set size
        size_scale = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for nS=3, 1 for nS=6
        wm_decay_eff = np.clip(wm_decay_ctx * (1.0 + size_scale), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (provided)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM action probabilities (treated as logits via high precision)
            # To avoid numerical issues, use the distribution directly in softmax-like form
            W_logits = W_s.copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_logits - W_logits[a])))

            # WM entropy (normalized 0..1) to gate arbitration
            # H = -sum p log p / log(nA)
            p = np.clip(W_s, eps, 1.0)
            H = -np.sum(p * np.log(p)) / np.log(nA)  # normalized entropy
            # Higher confidence (1-H) increases WM weight via a sigmoid
            conf = 1.0 - H
            wm_weight = 1.0 / (1.0 + np.exp(-entropy_bias * (conf - 0.5)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)
            # Also scale by wm_phi to cap reliance on WM based on encoding strength
            wm_weight = np.clip(wm_weight * np.tanh(wm_phi), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform each trial (state-specific)
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM update: move toward chosen action if rewarded; slight suppression if not rewarded
            if r > 0:
                # Update toward one-hot of chosen action; strength controlled by wm_phi
                alpha_wm = 1.0 - np.exp(-wm_phi)  # maps wm_phi >0 to (0..1)
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * onehot
            else:
                # Penalize chosen action slightly on error
                penal = 0.1 * (1.0 - np.exp(-wm_phi))
                w[s, a] = np.clip(w[s, a] - penal, 0.0, 1.0)
                # Renormalize to sum to 1
                total = np.sum(w[s, :])
                if total > 0:
                    w[s, :] /= total
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Discrete WM hypothesis with confidence, repetition bias, and set-size–dependent forgetting.

    Description:
    - RL: tabular Q-learning with softmax policy.
    - WM: for each state, maintains a hypothesized best action m[s] with confidence c[s] in [0,1].
      The WM distribution for a state is: w_s = (1-c)/nA + c*onehot(m). Confidence decays with set size.
    - Repetition bias: WM policy is biased toward repeating the last action taken in that state.
    - Arbitration: fixed base mixture 'mix0' scaled down by set size–dependent forgetting.
    - Set size effect: both WM confidence decay and the arbitration weight are reduced with larger sets.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate (0..1).
    - model_parameters[1] = mix0 (float): Base WM mixture weight (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally multiplied by 10.
    - model_parameters[3] = rep_bias (float): Additive WM bias toward repeating last action in a state (>=0).
    - model_parameters[4] = size_forget (float): Increases WM forgetting and reduces WM mixture with set size (>=0).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, mix0, softmax_beta, rep_bias, size_forget = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))

        # WM structures: hypothesis action m[s], confidence c[s] in [0,1]
        m = -1 * np.ones(nS, dtype=int)
        c = np.zeros(nS)  # start uncertain
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size–dependent forgetting/mixture scaling
        size_scale = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for 3, 1 for 6
        # Confidence decay per trial increases with set size
        c_decay = np.clip(size_forget * size_scale, 0.0, 1.0)
        # Arbitration weight reduced with larger set sizes
        wm_weight_eff = np.clip(mix0 * (1.0 - 0.5 * size_scale * size_forget), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # Build WM distribution from hypothesis and confidence
            if m[s] >= 0:
                W_s = (1.0 - c[s]) * w_0[s, :] + c[s] * np.eye(nA)[m[s]]
            else:
                W_s = w_0[s, :]

            # Add repetition bias in WM logits
            W_logits = W_s.copy()
            if last_action[s] >= 0:
                W_logits[last_action[s]] += rep_bias

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_logits - W_logits[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM confidence decay
            c[s] = (1.0 - c_decay) * c[s]

            # WM update rule:
            # - If rewarded, adopt the chosen action as hypothesis and increase confidence
            # - If not rewarded and hypothesis equals chosen, reduce confidence
            if r > 0:
                m[s] = a
                c[s] = np.clip(c[s] + 0.5 * (1.0 - c[s]), 0.0, 1.0)  # move halfway toward 1
            else:
                if m[s] == a:
                    c[s] = np.clip(c[s] - 0.25 * c[s], 0.0, 1.0)  # reduce confidence

            # Recompute w[s,:] from m[s], c[s] for next trial
            if m[s] >= 0:
                w[s, :] = (1.0 - c[s]) * w_0[s, :] + c[s] * np.eye(nA)[m[s]]
            else:
                w[s, :] = w_0[s, :]

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM delta-rule with set-size–dependent WM decay and uncertainty-based arbitration + lapse.

    Description:
    - RL: tabular Q-learning with softmax policy.
    - WM: maintains action preferences per state (w) updated via a delta-rule toward the most recent outcome.
      WM decays toward uniform more strongly in larger set sizes.
    - Arbitration: WM/RL mixture weight depends on RL certainty (action gap) — rely more on RL when its
      action gap is large; otherwise rely more on WM.
    - Lapse: with small probability, choices are random.
    - Set size effect: increases WM decay and also influences arbitration via the sensitivity parameter.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate (0..1).
    - model_parameters[1] = mix_wm (float): Base WM mixture weight (0..1).
    - model_parameters[2] = softmax_beta (float): RL inverse temperature; internally multiplied by 10.
    - model_parameters[3] = wm_learn (float): WM learning rate toward recent outcomes (0..1).
    - model_parameters[4] = ss_entropy_cost (float): Scales set-size impact on WM decay and arbitration sensitivity (>=0).
    - model_parameters[5] = lapse_rate (float): Lapse/choice noise rate (0..1).

    Returns
    - Negative log-likelihood of observed choices under the model.
    """
    lr, mix_wm, softmax_beta, wm_learn, ss_entropy_cost, lapse_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM decay and arbitration sensitivity
        size_scale = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 (nS=3) to 1 (nS=6)
        wm_decay = np.clip(ss_entropy_cost * size_scale, 0.0, 1.0)  # higher decay for larger sets
        arb_k = 2.0 + 3.0 * ss_entropy_cost * size_scale  # higher sensitivity with larger sets

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_logits = W_s.copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_logits - W_logits[a])))

            # Arbitration based on RL certainty: action gap (Q_best - Q_2nd)
            sorted_Q = np.sort(Q_s)
            if nA >= 2:
                gap = sorted_Q[-1] - sorted_Q[-2]
            else:
                gap = 0.0
            # Higher gap -> more RL; use logistic transform
            rl_weight = 1.0 / (1.0 + np.exp(-arb_k * (gap - 0.1)))
            rl_weight = np.clip(rl_weight, 0.0, 1.0)
            wm_weight = np.clip(mix_wm * (1.0 - rl_weight), 0.0, 1.0)

            # Mixture policy with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse_rate) * p_mix + lapse_rate * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM delta-rule update toward one-hot of chosen action, modulated by reward
            target = w[s, :].copy()
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0  # reward strengthens chosen action in WM
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # If unrewarded, slightly reduce chosen action's probability
                w[s, a] = np.clip(w[s, a] * (1.0 - 0.5 * wm_learn), 0.0, 1.0)
                # Renormalize
                total = np.sum(w[s, :])
                if total > 0:
                    w[s, :] /= total
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes on set size:
- Model 1: wm_decay_eff increases with set size, degrading WM faster in larger sets; arbitration favors WM when its entropy is low (confidence high).
- Model 2: WM confidence c[s] decays faster and the global WM mixture is reduced with larger set sizes via size_forget.
- Model 3: WM decay increases with set size and arbitration sensitivity to RL’s gap also increases with set size, shifting control to RL when it is certain; a lapse parameter captures occasional random choices.