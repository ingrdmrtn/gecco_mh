def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated arbitration and set-size–dependent WM decay.
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: associative table per state; rewarded actions are bound; global decay toward uniform.
    - Arbitration: WM weight increases when RL is uncertain (high entropy) and decreases with set size.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight_base: Base logit for WM mixture weight; transformed via sigmoid to [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_learn: Degree to which a rewarded action is bound into WM on each visit (0-1).
    - wm_decay_base: Base decay of WM toward uniform per trial (0-1); scaled up by set size.
    - gate_kappa: Sensitivity of WM gating to RL uncertainty (>=0). Higher => more WM use under uncertainty.

    Set-size effect:
    - WM decay increases linearly with set size: wm_decay = wm_decay_base * (set_size / 3).
    - Arbitration weight is downscaled implicitly because RL entropy is bounded, but we also include
      an entropy-based gate so WM is used more when RL is uncertain; large set sizes accelerate forgetting.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_learn, wm_decay_base, gate_kappa = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute constants for entropy
        H_min = 0.0
        H_max = np.log(nA)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL softmax over actions to compute entropy (uncertainty)
            logits_rl = softmax_beta * Q_s
            logits_rl = logits_rl - np.max(logits_rl)
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            # Entropy of RL policy
            H_rl = -np.sum(np.where(pi_rl > 0, pi_rl * np.log(np.maximum(pi_rl, eps)), 0.0))

            # WM policy probability of the chosen action (softmax over WM weights)
            wm_logits = W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Uncertainty-gated WM mixture weight; base on sigmoid of a logit with entropy drive
            # Higher entropy => higher WM weight. H_rl in [0, log(nA)].
            wm_weight_logit = wm_weight_base + gate_kappa * (H_rl - H_min)
            wm_weight = sigmoid(wm_weight_logit)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL value update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay scales with set size
            decay_eff = np.clip(wm_decay_base * (float(nS) / 3.0), 0.0, 1.0)
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # WM learning: bind rewarded action more strongly (toward a one-hot)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * onehot

            # Normalize WM row to a distribution
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + leaky WM success/failure accumulator with lateral inhibition.
    - RL: delta-rule with softmax (beta scaled by 10).
    - WM: two leaky accumulators per (state,action): successes and failures.
          WM policy uses (success - inhibition * failure) as logits.
    - Set-size effect: leak increases with set size, reducing WM stability in larger sets.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Mixture weight of WM in the policy (0-1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_gain: Gain applied to WM logits before softmax (>=0).
    - leak: Base leak per trial toward zero for both success and failure accumulators (0-1); scaled by set size.
    - inhibition: Weight of failure evidence subtracting from success evidence in WM logits (>=0).

    Set-size effect:
    - Effective leak = leak * (set_size / 3). Larger sets => faster forgetting of WM evidence.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_gain, leak, inhibition = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # not used directly as probabilities; will hold success evidence
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Separate failure accumulator
        w_fail = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # WM logits: success evidence minus inhibited failure evidence
            wm_logits_raw = w[s, :] - inhibition * w_fail[s, :]
            wm_logits = wm_gain * wm_logits_raw

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax prob of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Leaky WM updates with set-size–scaled leak
            leak_eff = np.clip(leak * (float(nS) / 3.0), 0.0, 1.0)
            w[s, :] = (1.0 - leak_eff) * w[s, :]
            w_fail[s, :] = (1.0 - leak_eff) * w_fail[s, :]

            # Increment success/failure evidence for chosen action
            if r > 0.0:
                w[s, a] += 1.0
            else:
                w_fail[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with persistent bindings and retrieval noise that scales with set size.
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: when rewarded, stores a persistent binding for the chosen action in that state.
          Between trials, bindings drift; retrieval has noise leading to occasional lapses.
    - Set-size effect: retrieval noise increases with set size.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Mixture weight of WM in the policy (0-1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - persist: Strength to overwrite WM toward the rewarded action on reward (0-1).
    - retrieval_noise: Base retrieval noise (0-1) that mixes WM with uniform; scaled by set size.
    - ss_scaler: Scales how much retrieval_noise increases with set size (>=0).

    Set-size effect:
    - Effective noise = clip(retrieval_noise * (1 + ss_scaler * (set_size - 3) / 3), [0,1]).
      Larger sets => more retrieval lapses, weakening WM influence.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, persist, retrieval_noise, ss_scaler = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM binding strengths as a distribution per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax prob of chosen action (deterministic over WM distribution)
            wm_logits = W_s
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Retrieval noise scales with set size
            noise_eff = retrieval_noise * (1.0 + ss_scaler * max(0.0, (float(nS) - 3.0)) / 3.0)
            noise_eff = np.clip(noise_eff, 0.0, 1.0)

            # Noisy WM policy: mix with uniform
            p_uniform = 1.0 / nA
            p_wm = (1.0 - noise_eff) * p_wm_det + noise_eff * p_uniform

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: persistent binding on reward; mild drift toward uniform otherwise
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - persist) * w[s, :] + persist * onehot
            else:
                # small drift toward uniform when not rewarded (implicit forgetting)
                drift = 0.1 * persist  # small fraction of persist used as drift scale
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0

            # Normalize WM row
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p