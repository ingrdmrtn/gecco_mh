def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recall probability depending on set size and WM decay-to-uniform.

    Idea:
    - RL: standard delta-rule (fixed across models by template).
    - WM: a fast associative store that:
        - Decays toward uniform each trial by wm_decay.
        - On rewarded trials, stores a near one-hot association for the current state.
        - On unrewarded trials, weakly suppresses the chosen action in the current state's row.
      At decision time, WM retrieval is not always successful; it succeeds with a set-size-dependent
      recall probability (higher in set size 3, lower in set size 6), and otherwise returns a uniform policy.

    Set-size effects:
    - WM recall probability uses recall_p3 for nS=3 and recall_p6 for nS=6 (and linearly interpolates otherwise).
      This directly reduces effective WM contribution for larger set sizes.

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = wm_weight (float in [0,1]): Mixture weight for WM policy (arbitration parameter).
    - model_parameters[2] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_decay (float in [0,1]): Per-trial WM decay toward uniform for all states.
    - model_parameters[4] = recall_p3 (float in [0,1]): WM recall success probability for set size 3.
    - model_parameters[5] = recall_p6 (float in [0,1]): WM recall success probability for set size 6.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, recall_p3, recall_p6 = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Interpolate recall probability for arbitrary nS (handles 3 and 6 exactly)
        if nS <= 3:
            recall_p = recall_p3
        elif nS >= 6:
            recall_p = recall_p6
        else:
            # linear interpolation between (3, recall_p3) and (6, recall_p6)
            alpha = (nS - 3) / 3.0
            recall_p = (1 - alpha) * recall_p3 + alpha * recall_p6

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (fixed by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM produces a near-deterministic policy when recalled; otherwise uniform.
            p_wm_recalled = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = recall_p * p_wm_recalled + (1.0 - recall_p) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update (fixed by template)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward uniform for all states
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # State-specific update
            if r > 0.5:
                # Move current state's WM toward a one-hot on the chosen action
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay  # inject mass on chosen action
            else:
                # Penalize the chosen action slightly and redistribute to others
                dec = min(wm_decay, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Renormalize current state row
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with asymmetric Hebbian/anti-Hebbian update and size-dependent lateral inhibition.

    Idea:
    - RL: standard delta-rule (fixed by template).
    - WM: a fast map updated by:
        - Rewarded trials: Hebbian boost to the chosen action and lateral inhibition to others.
        - Unrewarded trials: anti-Hebbian suppression of the chosen action with weaker redistribution.
      The strength of lateral inhibition increases with set size, capturing greater interference in larger sets.

    Set-size effects:
    - Lateral inhibition coefficient: inhib = inhib0 + inhib_size_slope * ((nS-3)/3), so nS=6 yields strongest inhibition.
      This reduces WM discriminability as the set gets larger.

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = wm_weight (float in [0,1]): Mixture weight for WM policy (arbitration parameter).
    - model_parameters[2] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_pos_gain (float in [0,1]): WM Hebbian gain on rewarded trials toward chosen action.
    - model_parameters[4] = wm_neg_gain (float in [0,1]): WM anti-Hebbian suppression on unrewarded trials of chosen action.
    - model_parameters[5] = inhib_size_slope (float in [0,1]): Increment in lateral inhibition from set size 3 to 6.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_pos_gain, wm_neg_gain, inhib_size_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Size-dependent inhibition strength (0 at nS=3 if slope applies only with size)
        size_factor = max(0.0, (nS - 3) / 3.0)
        inhib = np.clip(size_factor * inhib_size_slope, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (fixed by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM softmax on the current WM row
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update (fixed by template)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # Hebbian boost to chosen action
                add = wm_pos_gain * (1.0 - w[s, a])
                w[s, a] += add
                # Lateral inhibition spread to non-chosen actions
                if nA > 1:
                    take = inhib * add
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] = max(0.0, w[s, aa] - take / (nA - 1))
            else:
                # Anti-Hebbian suppression of chosen action
                dec = wm_neg_gain * w[s, a]
                w[s, a] -= dec
                # Redistribute a fraction of removed mass evenly to others (to keep the row informative)
                if nA > 1:
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] += dec / (nA - 1)

            # Renormalize current state row softly toward a valid distribution
            row = np.maximum(w[s, :], 1e-12)
            w[s, :] = row / np.sum(row)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with persistence-based hypothesis store and set-size-dependent lapse.

    Idea:
    - RL: standard delta-rule (fixed by template).
    - WM: maintains a persistent hypothesis for each state that tends to stick (persistence)
      when rewarded (win-stay), and to shift weight away from the chosen action when unrewarded (lose-shift).
      WM decision policy includes a lapse-to-uniform component that increases with set size.

    Set-size effects:
    - Lapse increases with set size: lapse_eff = wm_lapse * ((nS/3) - 1), so nS=3 -> 0, nS=6 -> wm_lapse.
      This directly reduces WM reliability in larger sets.

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = wm_weight (float in [0,1]): Mixture weight for WM policy (arbitration parameter).
    - model_parameters[2] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_persist (float in [0,1]): Degree to keep WM mass on the previously favored action on rewarded trials.
    - model_parameters[4] = size_penalty (float in [0,1]): Strength of shifting away from chosen action on unrewarded trials.
    - model_parameters[5] = wm_lapse (float in [0,1]): Additional WM lapse magnitude at set size 6 (vs 3).

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_persist, size_penalty, wm_lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent lapse
        lapse_eff = np.clip((nS / 3.0 - 1.0) * wm_lapse, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (fixed by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy is a mixture of deterministic readout and a lapse to uniform, both applied to W_s
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse_eff) * p_wm_det + lapse_eff * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update (fixed by template)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # Win-stay: concentrate more mass on the chosen action with persistence
                w[s, :] = (1.0 - wm_persist) * w[s, :]
                w[s, a] += wm_persist
            else:
                # Lose-shift: shift mass away from chosen action, scaled by set-size penalty
                dec = size_penalty * min(1.0, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Keep row a valid probability vector
            row = np.maximum(w[s, :], 1e-12)
            w[s, :] = row / np.sum(row)

        blocks_log_p += log_p

    return -blocks_log_p