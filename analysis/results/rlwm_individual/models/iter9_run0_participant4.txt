def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with eligibility traces and set-size–scaled WM precision.

    Policy:
    - RL: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM: softmax over W with very high base inverse temperature (=50), scaled down by set size
          via wm_precision_base * (3 / nS), making WM less precise under higher load.
    - Mixture: the WM mixture weight is state- and set-size–dependent:
          wm_weight_eff = wm_weight_base * (3 / nS) * conf_wm
      where conf_wm = max(W_s) - mean(W_s) in [0,1], favoring WM when its policy is peaked.

    Learning:
    - RL: temporal-difference update with eligibility traces across state-actions:
          e[s,a] <- 1 at choice; e <- lambda_et * e; Q += lr * delta * e
    - WM:
        1) Passive decay toward uniform at rate wm_decay each trial for the visited state.
        2) Reward-gated write toward one-hot of chosen action when r=1 with strength
           alpha_wm = wm_precision_base * (3 / nS) (clipped to [0,1]).

    Set-size effects:
    - WM precision and WM mixture weight scale with (3 / nS), reducing WM influence at set size 6.

    Parameters (6):
    - lr: RL learning rate (0..1)
    - wm_weight_base: base mixture weight on WM (0..1)
    - softmax_beta: RL inverse temperature, internally multiplied by 10
    - lambda_et: eligibility trace decay (0..1)
    - wm_decay: decay of WM values toward uniform (0..1)
    - wm_precision_base: base strength of WM write and precision scaling (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, lambda_et, wm_decay, wm_precision_base = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility trace

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:].copy()
            W_s = w[s,:].copy()

            # RL policy probability of chosen action (softmax trick relative to chosen action)
            p_rl = 1.0 / max(np.sum(np.exp(softmax_beta*(Q_s - Q_s[a]))), eps)

            # WM policy with set-size–scaled precision
            wm_beta_eff = softmax_beta_wm * max(0.0, wm_precision_base) * (3.0 / float(nS))
            p_wm = 1.0 / max(np.sum(np.exp(wm_beta_eff*(W_s - W_s[a]))), eps)

            # Confidence-driven and set-size–scaled WM mixture weight
            conf_wm = float(np.max(W_s) - np.mean(W_s))  # in [0,1]
            wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0) * (3.0 / float(nS)) * conf_wm
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL learning with eligibility traces
            delta = r - q[s, a]
            e *= lambda_et
            e[s, a] = 1.0
            q += lr * delta * e

            # WM passive decay on visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM write toward one-hot with set-size–scaled strength
            if r > 0:
                alpha_wm = np.clip(wm_precision_base * (3.0 / float(nS)), 0.0, 1.0)
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size modulation of both WM weight and RL temperature, and WM delta-rule.

    Policy:
    - RL: softmax over Q with inverse temperature scaled by set size:
          beta_rl_eff = (softmax_beta * 10) * (1 + beta_load * (3/nS - 1)),
      so beta is higher (more deterministic) at set size 3 if beta_load > 0.
    - WM: softmax over W with high temperature (=50).
    - Mixture: wm_weight_eff = wm_weight_base * (3 / nS), reducing WM usage at larger set sizes.

    Learning:
    - RL: standard TD update with single learning rate lr.
    - WM: delta-rule toward one-hot of the chosen action based on outcome, with its own learning
      rate wm_lr, plus decay toward uniform at rate wm_decay_base each visit:
          w[s] <- (1-wm_decay_base)*w[s] + wm_decay_base*w_0[s]
          w[s] <- w[s] + wm_lr * (target - w[s]), target = one_hot if r=1 else uniform.

    Set-size effects:
    - RL inverse temperature and WM mixture weight both scale with set size.
      WM learning rate does not directly depend on set size, isolating temperature vs. weight effects.

    Parameters (6):
    - lr: RL learning rate (0..1)
    - wm_weight_base: base mixture weight on WM (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10)
    - beta_load: set-size sensitivity of RL temperature (can be negative or positive)
    - wm_lr: WM learning rate (0..1)
    - wm_decay_base: WM decay toward uniform (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, beta_load, wm_lr, wm_decay_base = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:].copy()
            W_s = w[s,:].copy()

            # RL policy with size-modulated temperature
            beta_rl_eff = softmax_beta * (1.0 + beta_load * (3.0 / float(nS) - 1.0))
            p_rl = 1.0 / max(np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a]))), eps)

            # WM policy (deterministic lookup)
            p_wm = 1.0 / max(np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))), eps)

            # Mixture: set-size–scaled WM weight
            wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0) * (3.0 / float(nS))
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_base) * w[s, :] + wm_decay_base * w_0[s, :]

            # WM supervised-like update toward target (one-hot if rewarded, otherwise uniform)
            target = np.copy(w_0[s, :]) if r <= 0 else np.eye(nA)[a]
            w[s, :] += wm_lr * (target - w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with PE-gated WM usage and set-size–dependent WM interference (cross-talk).

    Policy:
    - RL: softmax over Q with inverse temperature softmax_beta (scaled by 10).
    - WM: softmax over W with high temperature (=50).
    - Mixture: WM weight decreases when recent prediction errors are large:
          wm_weight_eff = wm_weight * sigmoid(-pe_gate * |delta_prev|),
      where delta_prev is the last TD error in the current block and 0 initially.
      This prioritizes RL when outcomes are surprising, and WM when things are predictable.

    Learning:
    - RL: standard TD update with learning rate lr.
    - WM:
        1) Reward-based write toward one-hot of chosen action with strength wm_reliability.
        2) Interference (cross-talk): a fraction of the WM write spills to other states,
           scaled by set size via crosstalk * (nS - 1) / max(1, nS - 1), i.e., stronger with more states.
        3) Each visited state also decays slightly toward uniform due to interference.

    Set-size effects:
    - Cross-talk increases with set size, degrading WM in larger sets.
    - The PE-gated mixture dynamically shifts control depending on recent surprise.

    Parameters (6):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10)
    - crosstalk: strength of WM interference across states (>=0)
    - wm_reliability: strength of WM write toward the rewarded action (0..1)
    - pe_gate: sensitivity of WM weight to absolute prediction error (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, crosstalk, wm_reliability, pe_gate = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        delta_prev = 0.0

        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:].copy()
            W_s = w[s,:].copy()

            # RL and WM policy probabilities for chosen action
            p_rl = 1.0 / max(np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))), eps)
            p_wm = 1.0 / max(np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))), eps)

            # PE-gated WM mixture weight
            wm_gate = 1.0 / (1.0 + np.exp(pe_gate * abs(delta_prev)))  # sigmoid(-pe_gate*|delta_prev|)
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0) * wm_gate
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL TD update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay at visited state
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # small generic decay per visit

            # WM rewarded write
            if r > 0:
                write = np.zeros(nA); write[a] = 1.0
                w[s, :] = (1.0 - wm_reliability) * w[s, :] + wm_reliability * write

                # Cross-talk: spillover to other states, increasing with set size
                if nS > 1 and crosstalk > 0:
                    spill = crosstalk * ((nS - 1) / float(max(1, nS - 1)))
                    avg_other = np.mean(w, axis=0)
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        # Move other states slightly toward the current write pattern and toward uniform
                        w[s_other, :] = (1.0 - spill) * w[s_other, :] + spill * (0.5 * write + 0.5 * avg_other)

            delta_prev = delta

        blocks_log_p += log_p

    return -blocks_log_p