def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM with load-dependent decay and between-state interference.

    Idea:
    - RL learns Q-values via a delta rule and softmax.
    - WM stores state-action preferences and is queried via a near-deterministic softmax.
    - WM forgets faster under higher load (nS=6) and suffers between-state interference:
      when a state is encoded, other states are partially reset toward uniform.
    - Arbitration is a fixed weight that can still rely more on RL when WM becomes noisy due to decay/interference.

    Parameters:
    - lr: RL learning rate in [0,1].
    - wm_weight0: Base mixture weight for WM in [0,1].
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - wm_decay_base: Baseline WM decay rate toward uniform in [0,1].
    - interference_gain: Strength of cross-state WM interference with load in [0,1+].
    - wm_precision: WM inverse temperature scaling (>=0); higher -> more deterministic WM policy.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay_base, interference_gain, wm_precision = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = max(1e-6, wm_precision) * 50  # WM determinism from parameter
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_decay = np.clip(wm_decay_base * (1.0 + load), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            wm_weight_eff = np.clip(wm_weight0, 0.0, 1.0)
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-driven encoding; otherwise decay
            if r >= 0.5:
                # Encode target as one-hot
                target = np.zeros(nA)
                target[a] = 1.0
                # Stronger encoding when reward received
                encode_strength = 1.0 - (0.5 * wm_decay)  # retain some previous info
                encode_strength = np.clip(encode_strength, 0.0, 1.0)
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * target

                # Interference to other states grows with load
                k_int = np.clip(interference_gain * load * encode_strength, 0.0, 1.0)
                if k_int > 0:
                    for s_ in range(nS):
                        if s_ == s:
                            continue
                        w[s_, :] = (1.0 - k_int) * w[s_, :] + k_int * w_0[s_, :]
            else:
                # No reward: decay toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM as a Win-Stay/Lose-Shift policy with load-dependent leak.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM implements WSLS at the state level:
        - After reward: store a one-hot preference for the chosen action (win-stay).
        - After no reward: suppress the chosen action and boost competitors equally (lose-shift).
      This WM representation leaks toward uniform faster under higher load.
    - Arbitration mixes WM and RL with a base weight (reduced by load via the leak).

    Parameters:
    - alpha_pos: RL learning rate for positive PE in [0,1].
    - alpha_neg: RL learning rate for negative PE in [0,1].
    - wm_weight0: Base WM weight in [0,1].
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - wsls_temp: WM inverse temperature (>0); higher => more deterministic WSLS choices.
    - load_wsls_leak: Load-sensitive WM leak rate in [0,1+]; effective leak = load_wsls_leak * load.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, wm_weight0, softmax_beta, wsls_temp, load_wsls_leak = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = max(1e-6, wsls_temp)  # WM determinism
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        leak = np.clip(load_wsls_leak * load, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (WSLS as softmax over W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: base WM weight reduced under load via leak
            wm_weight_eff = np.clip(wm_weight0 * (1.0 - leak), 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            pe = r - Q_s[a]
            lr_eff = alpha_pos if pe >= 0 else alpha_neg
            q[s][a] += lr_eff * pe

            # WM update: WSLS rule with load-dependent leak
            if r >= 0.5:
                # Win: encode stay as one-hot on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - leak) * w[s, :] + leak * target
            else:
                # Lose: penalize chosen and boost others equally
                lose_vec = np.ones(nA) / (nA - 1)
                lose_vec[a] = 0.0
                w[s, :] = (1.0 - leak) * w[s, :] + leak * lose_vec

            # Constant diffusion toward uniform due to capacity limits
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-gated arbitration and RL temperature boost, and thresholded WM encoding.

    Idea:
    - RL learns Q-values with fixed learning rate; its softmax inverse temperature is adapted:
      when WM is confident (sharp policy), RL beta increases (exploitation), especially under low load.
    - WM stores rewarded mappings; encoding occurs only when the PE exceeds a threshold.
      Otherwise WM decays toward uniform. WM decays faster under higher load.
    - Arbitration weight toward WM is proportional to WM confidence and reduced by load.

    Parameters:
    - lr: RL learning rate in [0,1].
    - wm_weight_base: Base scale for WM arbitration in [0,1].
    - beta_base: Baseline RL inverse temperature (scaled internally x10).
    - beta_boost_wm: Additional RL beta when WM is confident (>=0).
    - wm_forget: WM decay rate in [0,1]; stronger decay with higher load.
    - wm_encode_thresh: Threshold on positive prediction error to encode into WM (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, beta_base, beta_boost_wm, wm_forget, wm_encode_thresh = model_parameters
    softmax_beta = beta_base * 10
    softmax_beta_wm = 50  # near-deterministic WM
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_decay = np.clip(wm_forget * (1.0 + load), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # WM confidence: margin between top-1 and top-2 preference
            sorted_W = np.sort(W_s)
            conf = max(0.0, sorted_W[-1] - sorted_W[-2])  # in [0,1]

            # RL beta boosted by WM confidence and attenuated by load
            beta_eff = softmax_beta + (beta_boost_wm * 10.0) * conf * (1.0 - 0.5 * load)

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight scales with confidence and decreases with load
            wm_weight_eff = np.clip(wm_weight_base * conf * (1.0 - 0.5 * load), 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s][a] += lr * pe

            # WM update: thresholded encoding if positive and strong enough PE
            if (r >= 0.5) and (pe > wm_encode_thresh):
                target = np.zeros(nA)
                target[a] = 1.0
                encode_strength = min(1.0, 0.5 + 0.5 * conf)  # more confident WM updates more strongly
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * target
            else:
                # Decay toward uniform (faster under load)
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p