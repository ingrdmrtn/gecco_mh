def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + confidence-gated WM (entropy-based) with WM decay and delta learning.

    Mechanism
    - RL: classic delta-rule with softmax choice.
    - WM: per-state associative values (w) turned into a near-deterministic softmax policy.
      WM values decay toward a uniform prior each time the state is visited and learn from feedback.
    - Mixture: WM weight is adapted by RL uncertainty (policy entropy). 
      Higher RL entropy -> more WM reliance; lower entropy -> more RL reliance.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float in [0,1]
            RL learning rate.
        - wm_weight0: float
            Baseline logit for WM mixture weight. Transformed via sigmoid on each trial.
        - softmax_beta: float
            Base inverse temperature for RL softmax (scaled internally by 10).
        - entropy_slope: float
            Scales the influence of RL entropy on the WM mixture logit (higher -> more WM under uncertainty).
        - wm_lr: float in [0,1]
            WM learning rate for updating the chosen action value toward experienced reward.
        - wm_decay: float in [0,1]
            Decay rate pulling WM values toward a uniform prior on each visit.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, entropy_slope, wm_lr, wm_decay = model_parameters
    softmax_beta *= 10  # RL inverse temperature higher scale
    softmax_beta_wm = 50  # WM policy is near-deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables (state x action)
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL policy entropy as an uncertainty proxy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            probs = np.exp(logits)
            probs = probs / max(np.sum(probs), eps)
            H = -np.sum(probs * np.log(np.clip(probs, eps, 1.0)))
            H_max = np.log(nA)

            # Trial-wise WM mixture weight via sigmoid(logit): higher entropy -> more WM
            wm_logit = wm_weight0 + entropy_slope * (H_max - H)
            wm_w = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_w = np.clip(wm_w, 0.0, 1.0)

            # WM policy from associative values
            prefs_wm = W_s - np.mean(W_s)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_wm, -50, 50))
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward prior then delta update on current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            w[s, a] += wm_lr * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + set-size adaptive WM precision.

    Mechanism
    - RL: delta-rule with separate learning rates for positive vs negative prediction errors.
    - WM: per-state associative values; the WM policy precision (effective inverse temperature) depends on set size:
      higher precision for small set sizes, lower for large (parameters control this).
    - Mixture: fixed WM mixture weight.

    Parameters
    ----------
    model_parameters: tuple
        - lr_pos: float in [0,1]
            RL learning rate for positive prediction errors.
        - lr_neg: float in [0,1]
            RL learning rate for negative prediction errors.
        - wm_weight: float in [0,1]
            Fixed mixture weight of WM policy.
        - softmax_beta: float
            RL inverse temperature (scaled internally by 10).
        - wm_prec_small: float >= 0
            Multiplier of WM precision (over a high base) when set size is small (nS <= 3).
        - wm_prec_large: float >= 0
            Multiplier of WM precision when set size is large (nS > 3).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_prec_small, wm_prec_large = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Choose WM precision multiplier based on set size
        wm_prec = wm_prec_small if nS <= 3 else wm_prec_large
        wm_beta_eff = softmax_beta_wm * max(wm_prec, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with set-size adaptive precision
            prefs_wm = W_s - np.mean(W_s)
            p_wm_vec = np.exp(np.clip(wm_beta_eff * prefs_wm, -50, 50))
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: associative delta toward r; reuse asymmetry to emphasize rewarded bindings
            # Move chosen action toward 1 if rewarded; toward 0 if not, with different step sizes
            if r > 0:
                w[s, a] += lr_pos * (1.0 - w[s, a])
            else:
                w[s, a] += lr_neg * (0.0 - w[s, a])

            # Optional small stabilization toward a valid distribution
            # Keep other actions slightly decayed toward prior to prevent drift
            others = [i for i in range(nA) if i != a]
            for j in others:
                w[s, j] += 0.1 * lr_neg * (w_0[s, j] - w[s, j])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + episodic WM (last-success retrieval) with recency- and set-size-dependent access.

    Mechanism
    - RL: delta-rule with softmax choice.
    - WM: for each state, store the last rewarded action and a strength variable.
      The WM policy predicts that action with a lapse; otherwise uniform. Strength decays over time
      and faster when set size is larger (interference).
    - Mixture: trial-wise WM weight is a sigmoid of a base bias plus a term increasing with recency/strength
      and decreasing with set size.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float in [0,1]
            RL learning rate.
        - mix_bias: float
            Baseline logit for WM-RL mixture (higher -> more WM).
        - softmax_beta: float
            RL inverse temperature (scaled internally by 10).
        - recency_gain: float >= 0
            Gain scaling the contribution of WM strength to the mixture weight.
        - size_interference: float >= 0
            Controls how quickly WM strength decays per trial and with larger set sizes.
        - lapse: float in [0,1]
            Lapse probability within WM policy (stochasticity of WM retrieval).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, mix_bias, softmax_beta, recency_gain, size_interference, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # used to form WM action distribution from w
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Episodic WM: last rewarded action per state and its strength
        mem_action = -1 * np.ones(nS, dtype=int)
        strength = np.zeros(nS)  # [0,1], recency/confidence of WM memory

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Build WM policy for this state from episodic memory into w[s,:]
            if mem_action[s] >= 0:
                w[s, :] = (lapse / nA) * np.ones(nA)
                w[s, mem_action[s]] += (1.0 - lapse)
            else:
                w[s, :] = (1.0 / nA) * np.ones(nA)

            # Convert WM probabilities to a softmax-compatible preference
            prefs_wm = w[s, :] - np.mean(w[s, :])
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_wm, -50, 50))
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture weight depends on WM strength (recency) and set-size interference
            size_penalty = size_interference * max(0, nS - 3)
            wm_logit = mix_bias + recency_gain * strength[s] - size_penalty
            wm_w = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_w = np.clip(wm_w, 0.0, 1.0)

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Episodic WM update
            if r > 0:
                mem_action[s] = a
                strength[s] = 1.0
            else:
                # Weaken state memory on failures
                strength[s] = strength[s] * (1.0 - 0.5 * lapse)
                # If it becomes very weak, forget the stored action
                if strength[s] < 1e-3:
                    mem_action[s] = -1

            # Global decay of WM strength due to time and interference (faster for larger set sizes)
            decay_factor = np.clip(1.0 - (size_interference * (nS / max(1.0, 6.0))), 0.0, 1.0)
            strength = np.clip(strength * decay_factor, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p