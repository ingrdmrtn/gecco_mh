Below are three standalone cognitive models that follow your template structure and constraints. Each returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM (slot-based) with entropy-weighted arbitration.

    Idea:
    - RL: standard delta rule with softmax choice.
    - WM: a capacity-limited store (K slots). For a stored state, WM provides a near-deterministic
      policy focused on the remembered action with precision wm_precision. Memory strength for a
      stored item increases with wm_fill (deterministic strength increment; no randomness).
    - Capacity enforcement: if more than K states have memory strength > 0, prune weakest until
      <= K remain.
    - Arbitration: WM mixture weight down-regulated by RL policy entropy and by load (K/nS).
      When RL is confident (low entropy), WM arbitration weight is higher; if RL is uncertain,
      arbitration weight shrinks.
    - Load effect: when nS > K, fewer states can be reliably in WM, reducing WM contribution.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature, internally scaled by 10 (>0).
    - wm_precision: sharpness of WM policy (>0). Higher -> more deterministic WM softmax.
    - wm_fill: memory strength increment on rewarded trials (0..1).
    - K_slots: WM capacity (number of storable state-action pairs, >=1).
    - entropy_sensitivity: scales how strongly RL entropy suppresses WM weight (>0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_precision, wm_fill, K_slots, entropy_sensitivity = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Slot-based WM bookkeeping
        mem_strength = np.zeros(nS)       # [0,1] strength of memory per state
        mem_action = -1 * np.ones(nS, dtype=int)  # stored preferred action per state (-1 = none)

        def update_w_row_from_memory(s):
            # Convert mem_strength and mem_action into a probability row w[s,:]
            if mem_strength[s] <= 0 or mem_action[s] < 0:
                w[s, :] = w_0[s, :]
            else:
                a_pref = mem_action[s]
                m = np.clip(mem_strength[s], 0.0, 1.0)
                # Start from uniform and add m mass to preferred action
                w[s, :] = (1 - m) * w_0[s, :]
                w[s, a_pref] += m

        def enforce_capacity():
            # Keep at most K_slots states with positive memory strength
            pos_idx = np.where(mem_strength > 0)[0]
            if len(pos_idx) > K_slots:
                # prune weakest until capacity satisfied
                strengths = mem_strength[pos_idx]
                order = np.argsort(strengths)  # weakest first
                to_prune = pos_idx[order[:len(pos_idx) - K_slots]]
                for sp in to_prune:
                    mem_strength[sp] = 0.0
                    mem_action[sp] = -1
                    w[sp, :] = w_0[sp, :]

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Compute RL choice probability for action a
            Q_s = q[s, :]
            # RL entropy to arbitrate (based on softmax over Q_s with RL beta)
            Q_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            Q_probs /= np.sum(Q_probs)
            H_rl = -np.sum(Q_probs * np.log(np.clip(Q_probs, 1e-12, 1.0)))
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from current w row
            W_s = w[s, :]
            # sharpen WM with wm_precision by exponentiating values
            W_logits = W_s  # interpret current row as "utility"; precision via softmax temp
            p_wm = 1 / np.sum(np.exp((softmax_beta_wm * wm_precision) * (W_logits - W_logits[a])))

            # Arbitration weight depends on capacity load and RL entropy
            cap_factor = min(1.0, K_slots / max(1.0, float(nS)))
            wm_weight_eff = cap_factor * np.exp(-entropy_sensitivity * H_rl)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # If rewarded, strengthen memory for (s,a); otherwise leave as is (no negative storage).
            if r > 0:
                # Set preferred action to current choice; increase strength
                mem_action[s] = a
                mem_strength[s] = mem_strength[s] + wm_fill * (1.0 - mem_strength[s])
                # Enforce capacity constraint
                enforce_capacity()
                # Reflect into w
                update_w_row_from_memory(s)
            else:
                # No strengthening; keep w as-is for s
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-adaptive learning rate + WM with load-suppressed weight and error-driven anti-tagging.

    Idea:
    - RL: learning rate increases with unsigned prediction error (surprise), via a logistic transform.
    - WM: contributes a sharp policy; global WM weight is suppressed by higher set size (load).
      After positive feedback, WM row is pulled toward a delta on the chosen action.
      After negative feedback, WM reduces probability of the chosen action (anti-tag), scaled by surprise.
    - Load effect: WM arbitration weight decreases via a logistic function of (nS-3).
    - WM decays toward uniform w_0 with wm_decay each trial.

    Parameters (model_parameters):
    - lr_base: base term for adaptive learning rate (real-valued; passed through logistic to 0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 (>0).
    - pe_gain: sensitivity of both adaptive learning rate and WM anti-tag to unsigned PE (>0).
    - wm_base: baseline WM mixture weight before load suppression (0..1).
    - load_slope: controls how strongly larger set sizes reduce WM weight (>0).
    - wm_decay: WM decay rate toward uniform each trial (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_base, softmax_beta, pe_gain, wm_base, load_slope, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM weight via logistic suppression with set size
        # For nS = 3 -> near wm_base; for larger nS -> down-weight.
        wm_weight_load = wm_base / (1.0 + np.exp(load_slope * (nS - 3.0)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL choice prob
            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Fixed arbitration per block determined by load
            wm_weight_eff = wm_weight_load

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with adaptive learning rate
            pe = r - Q_s[a]
            lr_adapt = 1.0 / (1.0 + np.exp(-(lr_base + pe_gain * np.abs(pe))))  # in (0,1)
            q[s][a] += lr_adapt * pe

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0

            # WM plasticity
            if r > 0:
                # Pull WM row for state s toward a delta on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * target
            else:
                # Anti-tag: reduce weight on chosen action proportionally to surprise
                neg_push = np.tanh(pe_gain * max(-pe, 0.0))  # 0..~1
                if neg_push > 0:
                    u = np.ones(nA)
                    u[a] = 0.0
                    if np.sum(u) > 0:
                        u = u / np.sum(u)  # redistribute to non-chosen actions
                    w[s, :] = (1 - neg_push) * w[s, :] + neg_push * u

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with optimistic baseline and load-dependent exploration + WM with load-dependent recall.

    Idea:
    - RL: standard delta rule but with optimistic Q initialization controlled by q_init (applied as
      an additive offset to the initial Q table). Exploration is implemented as an epsilon mixture
      that increases with load (nS), making behavior noisier under higher set size.
    - WM: near-deterministic policy when association is stored; WM mixture weight decreases with nS
      via a power-law factor. WM also decays faster under higher load.
    - Combined policy: first mix WM and RL by wm_weight_eff, then apply epsilon-greedy mixing with
      uniform according to epsilon_eff.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 (>0).
    - q_init: optimistic offset added to initial Q values (can be 0..1; effective Qs start at 1/nA + q_init).
    - wm_weight_base: base WM arbitration weight at low load (0..1).
    - load_gamma: exponent controlling how both WM weight and epsilon scale with load (>0).
    - epsilon_base: base exploration rate that scales up with load (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, q_init, wm_weight_base, load_gamma, epsilon_base = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # Apply optimistic initialization as an additive offset
        q += q_init

        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent terms
        load_factor = (max(1.0, nS / 3.0)) ** load_gamma
        wm_weight_eff = wm_weight_base / load_factor  # WM down-weighted under higher load
        epsilon_eff = np.clip(epsilon_base * load_factor, 0.0, 1.0)  # more random under higher load
        wm_decay_eff = np.clip((1.0 / max(1.0, nS)) ** (1.0 / max(1.0, load_gamma)), 0.0, 1.0)
        # wm_decay_eff: smaller for larger nS -> slower pull to uniform per trial; but memory update sets rows to delta

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL choice prob
            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix WM and RL, then apply epsilon mixture with uniform
            mix = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = (1 - epsilon_eff) * mix + epsilon_eff * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay and update
            # Global mild decay toward uniform that depends on load
            w = (1 - wm_decay_eff) * w + wm_decay_eff * w_0

            if r > 0:
                # On reward, set state row close to a delta on chosen action (store association)
                target = np.zeros(nA)
                target[a] = 1.0
                # Strong overwrite to encode; blend uses (1 - wm_decay_eff) to keep some prior if decay is small
                w[s, :] = (1 - wm_decay_eff) * w[s, :] + wm_decay_eff * target
            else:
                # On no reward, nudge away from chosen action slightly (discourage repeating)
                away = np.ones(nA) / (nA - 1)
                away[a] = 0.0
                nudge = 0.25 * wm_decay_eff  # small nudge relative to decay magnitude
                w[s, :] = (1 - nudge) * w[s, :] + nudge * away

        blocks_log_p += log_p

    return -blocks_log_p