def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM slots with rehearsal and set-size-scaled decay.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: a capacity-limited cache; an item (state->action) is encoded into WM with probability
      proportional to the number of slots relative to set size. Cached items yield a near-deterministic
      WM policy favoring the encoded action. WM strength is reinforced by rehearsal on revisits and
      decays more under larger set sizes.
    - Arbitration: mixture weight scales with WM strength.

    Parameters
    ----------
    model_parameters : (lr, wm_mix0, softmax_beta, wm_slots, rehearsal_gain, decay_base)
        lr : float
            RL learning rate (0-1).
        wm_mix0 : float
            Base mixture weight for WM (0-1), scaled by WM strength on a given state.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_slots : float
            Absolute WM capacity in number of slots (>=0). Encoding probability is min(1, wm_slots/nS).
        rehearsal_gain : float
            Increment to WM strength on revisits (>=0).
        decay_base : float
            Baseline WM decay rate per trial, scaled up by set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_mix0, softmax_beta, wm_slots, rehearsal_gain, decay_base = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM strength per state (0=none, higher=stronger cached mapping)
        wm_strength = np.zeros(nS)
        # Track whether the state was observed previously (for rehearsal)
        seen_state = np.zeros(nS, dtype=bool)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: precision increases with WM strength
            beta_wm_eff = softmax_beta_wm * np.clip(wm_strength[s], 0.0, 1.0)
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture weight scales with WM strength
            wm_weight_eff = np.clip(wm_mix0 * (1.0 - np.exp(-wm_strength[s] + 1e-12)), 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay scales with set size
            decay = 1.0 - np.exp(-decay_base * (float(nS) / 3.0))
            wm_strength *= (1.0 - decay)
            # revert WM distributions toward uniform by the same decay at the state level
            for s2 in range(nS):
                w[s2, :] = (1.0 - decay) * w[s2, :] + decay * w_0[s2, :]

            # Rehearsal on revisit
            if seen_state[s]:
                wm_strength[s] += rehearsal_gain
            seen_state[s] = True

            # Encoding into WM on reward with capacity probability
            p_encode = np.clip(wm_slots / max(1.0, float(nS)), 0.0, 1.0)
            if r > 0 and np.random.rand() < p_encode:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Update WM distribution toward chosen action
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
                wm_strength[s] = np.clip(wm_strength[s] + 0.5, 0.0, 1.0)

            # Normalize WM distribution
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with dual learning rates + recency-based WM bias and set-size-reduced choice precision.

    Idea
    - RL: Q-learning with asymmetric learning rates for positive/negative prediction errors.
    - WM: a recency buffer per state (exponential memory of recent chosen actions),
      producing a near-deterministic policy favoring the most recent action(s).
    - Arbitration: fixed WM mixture weight, while RL softmax temperature decreases
      with set size (cost on precision under high load).

    Parameters
    ----------
    model_parameters : (lr_pos, lr_neg, softmax_beta0, wm_recency_tau, wm_bias_weight, ss_cost_beta)
        lr_pos : float
            RL learning rate for positive prediction errors (0-1).
        lr_neg : float
            RL learning rate for negative prediction errors (0-1).
        softmax_beta0 : float
            Baseline RL inverse temperature (scaled by 10 internally).
        wm_recency_tau : float
            Time constant (>0) of recency buffer; larger values retain older actions more.
        wm_bias_weight : float
            Mixture weight (0-1) for WM policy.
        ss_cost_beta : float
            Set-size cost on RL precision (>=0); effective beta scales as exp(-ss_cost_beta*(nS-3)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, softmax_beta0, wm_recency_tau, wm_bias_weight, ss_cost_beta = model_parameters
    softmax_beta = softmax_beta0 * 10  # baseline scaling
    softmax_beta_wm = 50  # WM readout is sharp
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM recency buffer: stores recency-weighted counts per state-action
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # decay factor for recency buffer each trial
        rec_decay = np.exp(-1.0 / max(1e-6, wm_recency_tau))
        # RL precision reduced under higher set size
        beta_eff = softmax_beta * np.exp(-ss_cost_beta * max(0.0, float(nS) - 3.0))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with set-size adjusted beta
            p_rl = 1 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy: softmax over recency buffer
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture: fixed WM bias weight
            wm_weight_eff = np.clip(wm_bias_weight, 0.0, 1.0)
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr_use * delta

            # Recency buffer update
            # global decay
            w *= rec_decay
            # add one-hot to current state to reflect most recent choice
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = w[s, :] + one_hot

            # small attraction to uniform to avoid degeneracy, stronger under larger set sizes
            mix = 0.0 + 0.05 * (float(nS) / 6.0)
            w[s, :] = (1.0 - mix) * w[s, :] + mix * w_0[s, :]

            # normalize
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + count-based WM that adapts precision to surprise and forgets with set size.

    Idea
    - RL: Q-learning with eligibility traces, so recent state-action pairs remain modifiable.
    - WM: per-state action counts (Dirichlet-like) form a probability distribution; rewards strengthen
      the chosen action more than non-rewards. WM precision increases with surprise (|PE|),
      and count memory decays more under larger set sizes.
    - Arbitration: fixed base WM mixture weight.

    Parameters
    ----------
    model_parameters : (lr, lambda_trace, softmax_beta, wm_base, err_boost, ss_forget)
        lr : float
            RL learning rate (0-1).
        lambda_trace : float
            Eligibility trace decay (0-1); higher keeps traces longer.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_base : float
            Base WM mixture weight (0-1).
        err_boost : float
            Scales WM precision with absolute prediction error (>=0).
        ss_forget : float
            Set-size-scaled WM forgetting rate (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, lambda_trace, softmax_beta, wm_base, err_boost, ss_forget = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base WM determinism (to be modulated by error)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM counts: start from a uniform prior of 1 per action
        wm_counts = np.ones((nS, nA))
        # store normalized WM probabilities in w for policy readout
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # eligibility traces over state-action
        e = np.zeros((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM precision increases with surprise (|PE|) from RL
            delta_peek = r - Q_s[a]
            beta_wm_eff = softmax_beta_wm * (1.0 + err_boost * abs(delta_peek))
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture with fixed base WM weight
            wm_weight_eff = np.clip(wm_base, 0.0, 1.0)
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL with eligibility traces
            # decay traces
            e *= lambda_trace
            # set current trace to 1 for chosen s,a
            e[s, :] *= 0.0
            e[s, a] = 1.0
            # TD error
            delta = r - Q_s[a]
            q += lr * delta * e

            # WM forgetting scales with set size
            forget = 1.0 - np.exp(-ss_forget * (float(nS) / 3.0))
            # decay counts toward prior of 1
            wm_counts = (1.0 - forget) * wm_counts + forget * 1.0

            # Update counts: reward strengthens chosen action more
            if r > 0:
                wm_counts[s, a] += 1.0
            else:
                wm_counts[s, a] += 0.1  # weak evidence even without reward

            # Normalize to probabilities for policy readout
            w = np.clip(wm_counts, 1e-12, None)
            w = w / np.sum(w, axis=1, keepdims=True)
            # small shrinkage to uniform to avoid numerical issues
            w = 0.995 * w + 0.005 * w_0

        blocks_log_p += log_p

    return -blocks_log_p