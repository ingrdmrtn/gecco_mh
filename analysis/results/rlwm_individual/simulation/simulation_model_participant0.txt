import numpy as np

def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    RL + WM with chunking interference across states and load-dependent drift.

    Parameters:
    - stimulus: sequence of stimuli (state indices) for each trial (array)
    - blocks: block index for each trial (array)
    - set_sizes: block-dependent set sizes on each trial (array)
    - correct_answer: correct action for each trial (array)
    - parameters: [lr, wm_weight, softmax_beta, chunking, decay, noise_wm]

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """
    lr, wm_weight, softmax_beta, chunking, decay, noise_wm = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    # Process each block independently
    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices].astype(int)
        block_correct = correct_answer[block_indices].astype(int)
        block_set_sizes = set_sizes[block_indices].astype(int)

        nA = 3
        # Use provided set size for the block (assumed constant within block)
        nS = int(block_set_sizes[0])

        # Map each state to its correct action (assumes consistency within state)
        correct_actions = np.zeros(nS, dtype=int)
        for s in range(nS):
            idx = np.where(block_states == s)[0]
            if len(idx) > 0:
                correct_actions[s] = block_correct[idx[0]]
            else:
                # If a state does not appear (edge case), default to 0
                correct_actions[s] = 0

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load term as in fitting code
        load = float(nS - 1) / max(1.0, float(nS - 1))

        for t in range(len(block_states)):
            global_t = block_indices[t]
            s = int(block_states[t])

            # Compute RL policy
            Q_s = q[s, :]
            z_rl = softmax_beta * (Q_s - np.max(Q_s))
            p_rl = np.exp(z_rl)
            p_rl /= np.sum(p_rl)

            # Compute WM effective row with chunking interference
            W_s = w[s, :].copy()
            if nS <= 1:
                mean_other = np.mean(w, axis=0)
            else:
                mean_other = (np.sum(w, axis=0) - W_s) / max(1, nS - 1)
            ch = np.clip(chunking * load, 0.0, 1.0)
            W_eff = (1.0 - ch) * W_s + ch * mean_other

            beta_wm_eff = softmax_beta_wm / (1.0 + noise_wm)
            z_wm = beta_wm_eff * (W_eff - np.max(W_eff))
            p_wm = np.exp(z_wm)
            p_wm /= np.sum(p_wm)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            p_total /= np.sum(p_total)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[global_t] = a

            # Generate reward from ground truth
            r = 1 if a == int(correct_actions[s]) else 0
            simulated_rewards[global_t] = r

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global drift toward uniform (load-dependent)
            d = np.clip(decay * (0.5 + 0.5 * load), 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # WM reward-contingent consolidation/redistribution
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                # Consolidate chosen action
                w[s, :] = 0.8 * w[s, :] + 0.2 * one_hot
            else:
                # Penalize chosen action and redistribute mass to others
                reduce = 0.15
                w[s, a] = (1.0 - reduce) * w[s, a]
                redistribute = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = w[s, aa] + redistribute

            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] = w[s, :] / row_sum

            tr += 1

    return simulated_actions, simulated_rewards