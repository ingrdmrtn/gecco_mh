def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Simulate choices and rewards from Model 2:
    Recency-gated WM with set-size modulation and choice stickiness in arbitration.

    Inputs:
    - stimulus: array of state identities per trial (ints 0..nS-1 within each block)
    - blocks: array of block ids per trial
    - set_sizes: array giving the set size for each trial (constant within a block)
    - correct_answer: array of correct action (0..2) per trial
    - parameters: [lr, wm_weight, softmax_beta, recency_sensitivity, size_sensitivity]

    Returns:
    - simulated_actions: array of chosen actions (0..2)
    - simulated_rewards: array of rewards (0/1)
    """
    import numpy as np

    lr, wm_weight, softmax_beta, recency_sensitivity, size_sensitivity = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr_indices = np.arange(n_trials)

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_indices = tr_indices[block_mask]
        block_states = stimulus[block_mask].astype(int)
        block_correct = correct_answer[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]
        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Recency and last action trackers
        last_visit_t = -1 * np.ones(nS, dtype=float)
        last_action = -1 * np.ones(nS, dtype=int)

        # Precompute baseline logit of wm_weight
        wm_weight_clipped = np.clip(wm_weight, 1e-6, 1 - 1e-6)
        logit_base = np.log(wm_weight_clipped) - np.log(1 - wm_weight_clipped)
        size_term = size_sensitivity * max(0, nS - 3)

        for t_local, tr in enumerate(block_indices):
            s = int(block_states[t_local])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM softmax policies
            prl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl /= np.sum(prl)

            pwm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm /= np.sum(pwm)

            # Recency score for current state
            if last_visit_t[s] < 0:
                recency_score = 0.0
            else:
                gap = t_local - last_visit_t[s]
                recency_score = 1.0 / (1.0 + gap)

            # Action-dependent arbitration due to stickiness term
            p_total = np.zeros(nA)
            for a in range(nA):
                sticky = 1.0 if (last_action[s] == a and last_action[s] >= 0) else 0.0
                logit = logit_base + recency_sensitivity * recency_score - size_term + sticky
                wm_weight_eff = 1.0 / (1.0 + np.exp(-logit))
                p_total[a] = wm_weight_eff * pwm[a] + (1.0 - wm_weight_eff) * prl[a]

            # Normalize to guard numerical issues
            p_sum = np.sum(p_total)
            if p_sum <= 1e-12:
                p_total = np.ones(nA) / nA
            else:
                p_total /= p_sum

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[tr] = a

            # Outcome
            r = 1 if a == int(block_correct[t_local]) else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: consolidate on reward, forget toward uniform on no-reward
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_weight) * w[s, :] + wm_weight * one_hot
            else:
                w[s, :] = (1.0 - wm_weight) * w[s, :] + wm_weight * w_0[s, :]

            # Update recency trackers
            last_visit_t[s] = t_local
            last_action[s] = a

    return simulated_actions, simulated_rewards