def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Simulate behavior for:
    RL with uncertainty-adaptive temperature + WM associative counts with set-size misbinding.

    Inputs:
    - stimulus: array of state indices per trial
    - blocks: array of block indices per trial
    - set_sizes: array with the set size (number of states) for each trial's block
    - correct_answer: array of correct action per trial
    - parameters: [lr, wm_weight, softmax_beta0, beta_gain, misbind_rate, wm_lr]

    Returns:
    - simulated_actions: array of chosen actions per trial
    - simulated_rewards: array of rewards (0/1) per trial
    """
    import numpy as np

    lr, wm_weight, softmax_beta0, beta_gain, misbind_rate, wm_lr = parameters
    softmax_beta0 *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx]
        block_correct = correct_answer[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        # Follow fitting code convention: nS from set_sizes entry
        nS = int(block_set_sizes[0])

        # Map each state id to its correct action within this block
        # Assume states are labeled 0..nS-1 within block
        unique_states = np.unique(block_states)
        # In case labels are not 0..nS-1, build a mapping to 0..nS-1
        state_to_local = {s_val: i for i, s_val in enumerate(unique_states)}
        local_to_correct = np.zeros(nS, dtype=int)
        for s_val in unique_states:
            local_s = state_to_local[s_val]
            # take the first occurrence's correct action
            local_to_correct[local_s] = block_correct[block_states == s_val][0]

        # Initialize RL, WM, and uncertainty
        q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.ones((nS, nA))
        uncert = np.ones(nS)

        for t in range(len(block_states)):
            s_val = block_states[t]
            s = state_to_local[s_val]
            correct_a = local_to_correct[s]

            Q_s = q[s, :]

            certainty = 1.0 / (1.0 + uncert[s])
            softmax_beta = softmax_beta0 * (1.0 + beta_gain * certainty)

            # RL policy (stable softmax)
            z_rl = softmax_beta * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(np.clip(z_rl, -50, 50))
            p_rl_vec /= max(np.sum(p_rl_vec), eps)

            # WM policy from associative counts with centering
            wm_probs = counts[s, :] / max(np.sum(counts[s, :]), eps)
            prefs_wm = wm_probs - np.mean(wm_probs)
            z_wm = softmax_beta_wm * prefs_wm
            p_wm_vec = np.exp(np.clip(z_wm, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)

            # Mixture
            p_total = wm_weight * p_wm_vec + (1.0 - wm_weight) * p_rl_vec
            p_total = np.clip(p_total, eps, 1.0)
            p_total /= np.sum(p_total)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[tr] = a

            # Outcome
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Uncertainty update (variance proxy)
            uncert[s] = np.clip(0.9 * uncert[s] + 0.1 * (pe ** 2), 0.0, 10.0)

            # WM update with misbinding
            leak = np.clip(misbind_rate * ((nS - 1) / max(1, nS)), 0.0, 1.0)
            on_target = 1.0 - leak

            incr = np.zeros(nA)
            if r > 0:
                incr[a] = 1.0
            else:
                incr += 0.2

            # On-target update
            counts[s, :] += wm_lr * on_target * incr

            # Leak to other states
            if leak > 0 and nS > 1:
                spread = (wm_lr * leak) * incr
                if np.sum(spread) > 0:
                    per_state = spread / (nS - 1)
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        counts[s2, :] += per_state

            counts = np.clip(counts, 1e-6, 1e6)

            tr += 1

    return simulated_actions, simulated_rewards