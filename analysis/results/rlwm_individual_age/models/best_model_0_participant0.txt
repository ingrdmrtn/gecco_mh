def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with PE-based arbitration and capacity-limited encoding.

    Mechanism
    - RL: Tabular Q-learning with softmax policy.
    - WM: One-shot encoding toward the chosen action with strength proportional
      to an effective capacity term that decreases with set size and for older adults.
    - Arbitration: Mixture weight increases when the absolute RL prediction error
      is small (confidence high), and decreases when the PE is large; age reduces this sensitivity.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1].
    - wm_logit: baseline mixture logit; transformed via sigmoid.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - k_pe: sensitivity of WM mixture to (1 - |PE|); larger k_pe increases reliance on WM when PE small.
    - rho: base WM overwrite strength in [0,1].
    - age_penalty: reduces WM encoding capacity for older adults (>=0).

    Age and set-size effects
    - Effective WM encoding strength per trial: rho_eff = rho * (3/set_size) * (1 - age_group * age_penalty),
      clipped to [0,1].
    - Effective mixture per trial: sigmoid(wm_logit + k_pe * (1 - |PE|)), where PE is computed from RL before update.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_logit, softmax_beta, k_pe, rho, age_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            pe = r - Q_s[a]
            conf = 1.0 - np.minimum(1.0, np.maximum(0.0, abs(pe)))  # conf in [0,1]
            wm_weight = 1.0 / (1.0 + np.exp(-(wm_logit + k_pe * conf)))

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            q[s, a] += lr * pe

            cap = (3.0 / float(nS)) * (1.0 - float(age_group) * max(0.0, age_penalty))
            rho_eff = np.clip(rho * cap, 0.0, 1.0)

            if rho_eff > 0.0:
                target = np.zeros(nA)

                if r > 0.0:
                    target[a] = 1.0
                    w[s, :] = (1.0 - rho_eff) * w[s, :] + rho_eff * target
                else:

                    anti = np.ones(nA) / nA
                    anti[a] = 0.0
                    anti = anti / np.sum(anti)
                    w[s, :] = (1.0 - 0.5 * rho_eff) * w[s, :] + (0.5 * rho_eff) * anti

                w[s, :] = np.maximum(w[s, :], eps)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p