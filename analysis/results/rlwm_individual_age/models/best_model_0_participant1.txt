def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM and lapse.

    Idea:
    - RL uses eligibility traces to propagate credit within a block.
    - WM stores up to K_eff associations; if a state is stored, WM yields a deterministic policy.
    - Effective WM capacity K_eff decreases with set size and is reduced for older adults.
    - Arbitration uses availability: if state is stored, WM is used with weight proportional to K_eff/nS.
    - A small lapse probability mixes a uniform random choice into the final policy.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = lambda_et in [0,1]: eligibility trace decay parameter
    - model_parameters[3] = K_base (>0): baseline WM capacity (in items)
    - model_parameters[4] = K_age_drop (>=0): capacity drop for older adults (subtract from K_base)
    - model_parameters[5] = lapse in [0,0.2]: lapse probability mixing in uniform responding

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: parameter list

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, lambda_et, K_base, K_age_drop, lapse = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    lambda_et = min(max(lambda_et, 0.0), 1.0)
    K_base = max(K_base, 1e-6)
    K_age_drop = max(K_age_drop, 0.0)
    lapse = min(max(lapse, 0.0), 0.2)

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        stored_action = -np.ones(nS, dtype=int)  # -1 indicates not stored
        recency_counter = np.zeros(nS)  # for LRU eviction

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            recency_counter += 1.0
            recency_counter[s] = 0.0


            K_eff_cont = max(K_base - K_age_drop * age_group, 0.0) / (1.0 + max(nS_t - 3, 0))

            wm_availability_w = min(1.0, K_eff_cont / max(nS_t, 1))

            if stored_action[s] >= 0:
                wm_policy = np.zeros(nA)
                wm_policy[stored_action[s]] = 1.0
                p_wm = wm_policy[a]
            else:

                p_wm = 1.0 / nA

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            if stored_action[s] >= 0:
                w_wm = wm_availability_w
            else:
                w_wm = 0.0

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl

            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)


            e *= lambda_et

            e[s, a] = 1.0

            delta = r - Q_s[a]
            q += lr * delta * e

            if r > 0.0:
                if stored_action[s] == -1:


                    current_K = np.sum(stored_action >= 0)

                    K_cap = int(np.floor(K_eff_cont + 1e-9))
                    if K_cap < 0:
                        K_cap = 0
                    if current_K >= K_cap and K_cap >= 0:

                        if K_cap == 0 and current_K > 0:

                            stored_action[:] = -1
                        else:
                            candidates = np.where(stored_action >= 0)[0]
                            if candidates.size > 0:
                                evict_idx = candidates[np.argmax(recency_counter[candidates])]
                                stored_action[evict_idx] = -1

                stored_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p