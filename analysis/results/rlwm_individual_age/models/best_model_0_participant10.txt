def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Capacity-weighted WM mixture with age bias and noisy encoding.

    Mechanism:
    - RL: delta-rule with softmax.
    - WM: state-action weights that, upon reward, encode the chosen action as a
      near one-hot vector corrupted by encoding noise; otherwise, WM leaks toward uniform
      a small amount each trial. WM action selection uses a high-beta softmax.
    - Mixture: WM mixture weight is a sigmoid over a logit composed of:
        base term + age bias (penalizing older group) - set-size penalty.
      Thus, larger set sizes reduce reliance on WM; older age reduces reliance further.

    Parameters (model_parameters):
    - alpha_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; scaled by 10 internally
    - wm_weight0: base logit for WM mixture weight
    - gamma_ss: penalty per additional item beyond 3 on WM mixture weight (>=0)
    - age_bias: additional penalty applied when age_group == 1 (>=0 reduces WM in older)
    - eps_encode: WM encoding/leak noise (0..1); also drives leak scaled by set size

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_rl, beta_rl, wm_weight0, gamma_ss, age_bias, eps_encode = model_parameters
    beta_rl *= 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        ss_penalty = gamma_ss * max(nS - 3, 0)
        age_penalty = age_bias * (1 if age_group == 1 else 0)
        wm_weight_logit = wm_weight0 - ss_penalty - age_penalty
        wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_logit))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            wm_logits = beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = max(p_wm_vec[a], 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha_rl * delta

            leak = np.clip(eps_encode * (nS / 6.0), 0.0, 1.0)
            w[s, :] = (1.0 - leak) * W_s + leak * (1.0 / nA)

            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eps_encode) * one_hot + eps_encode * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p