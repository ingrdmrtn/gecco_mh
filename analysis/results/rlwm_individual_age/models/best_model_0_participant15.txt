def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + outcome-gated WM mixture with lapse and global WM decay.

    Policy:
    - Mixture of RL softmax and WM softmax:
        p_total = (1 - lapse_age) * [wm_weight_trial * p_wm + (1 - wm_weight_trial) * p_rl] + lapse_age * (1/nA)
      where p_rl is a softmax over Q(s,a) and p_wm is a softmax over W(s,a).

    WM dynamics:
    - Global decay toward a uniform prior per trial (wm_decay).
    - If rewarded, reinforce the chosen action in WM (one-shot boost).
    - WM influence on policy is outcome-gated on each trial and reduced by set size (more load â†’ smaller WM influence).
      WM influence is also reduced for the older group.

    RL dynamics:
    - Standard delta rule with a single learning rate.

    Age and set-size dependence:
    - wm_weight reduced for larger set sizes and for older adults.
    - Lapse increases with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_weight_base: base WM mixture weight in [0,1]
    - wm_conf_boost: scales outcome gating of WM weight (>=0)
    - wm_decay: WM decay toward uniform per trial in [0,1]
    - lapse: base lapse probability in [0,1] that increases with set size and age

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_conf_boost, wm_decay, lapse = model_parameters

    softmax_beta *= 10.0  # higher upper bound
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            wm_weight_eff = wm_weight_base * (1.0 - 0.4 * age_group)  # older rely less on WM

            load_scale = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
            outcome_boost = (1.0 + wm_conf_boost * r) / (1.0 + wm_conf_boost)
            wm_weight_trial = np.clip(wm_weight_eff * load_scale * outcome_boost, 0.0, 1.0)

            lapse_age = lapse * (1.0 + 0.2 * (nS == 6) + 0.5 * age_group)
            lapse_age = np.clip(lapse_age, 0.0, 0.5)

            p_mix = wm_weight_trial * p_wm + (1.0 - wm_weight_trial) * p_rl
            p_total = (1.0 - lapse_age) * p_mix + lapse_age * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if r > 0.5:

                alpha_wm = 0.8 * (1.0 + 0.5 * (outcome_boost - 0.5))  # slightly modulated by wm_conf_boost via outcome_boost
                alpha_wm = np.clip(alpha_wm, 0.0, 1.0)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p