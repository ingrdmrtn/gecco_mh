def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with adaptive arbitration by surprise (prediction error) + WM win-stay cache.

    Idea:
    - RL learns Q-values; softmax choice.
    - WM implements a win-stay cache: if a state was rewarded for an action, WM recommends that action deterministically.
    - Arbitration weight toward WM increases when absolute prediction error is large (fresh information to cache)
      but declines with larger set sizes and in older adults.
    
    Parameters (6):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled x10 internally)
    - beta_wm: WM inverse temperature for cached choice vs others
    - wm_gate_base: base WM weight when |PE| is zero (in [0,1])
    - pe_gate_gain: how much WM weight increases with |PE| (>=0)
    - age_gate_penalty: penalty on WM weight for older group (>=0), scaled by set size
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list of parameters above
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, beta_rl, beta_wm, wm_gate_base, pe_gate_gain, age_gate_penalty = model_parameters
    softmax_beta = beta_rl * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = max(1e-6, beta_wm)
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states  = states[blocks  == b]
        block_setsizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_setsizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_cache = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            W_s = w[s, :].copy()
            if wm_cache[s] >= 0:
                cached = wm_cache[s]

                W_s[:] = (1.0 - 0.9) / (nA - 1)
                W_s[cached] = 0.9

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            pe = r - q[s, a]
            abs_pe = np.abs(pe)

            size_penalty = (float(nS) / 3.0 - 1.0)  # 0 at set size 3, 1 at set size 6
            wm_weight_raw = wm_gate_base + pe_gate_gain * abs_pe
            wm_weight_raw -= age_gate_penalty * age_group * (1.0 + size_penalty)
            wm_weight = np.clip(wm_weight_raw, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            q[s, a] += lr * pe

            if r > 0:
                wm_cache[s] = a

            leak = np.clip(0.03 + 0.04 * (float(nS) / 3.0) + 0.03 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p