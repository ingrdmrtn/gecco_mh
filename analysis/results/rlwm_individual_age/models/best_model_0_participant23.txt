def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Capacity-limited WM with load- and age-modulated gating and decay.

    Idea
    - RL: Standard Q-learning with softmax.
    - WM: Probabilistic cache per state, pushed toward the last rewarded action and otherwise
      decays toward uniform. The WM arbitration weight is reduced by higher set size (load)
      and by being in the older group.
    - Both the WM gate (mixture weight) and WM decay are penalized by load and age, but via distinct
      mechanisms controlled by shared penalty parameters (parsimony).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Reward per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; age_group = 0 if <=45 (young), 1 if >45 (older).
    model_parameters : list or array-like
        [lr_raw, beta_raw, wm_conf_raw, decay_raw, age_wm_cost, load_wm_cost]
        - lr_raw: RL learning rate (logistic-bounded to 0..1).
        - beta_raw: RL inverse temperature, scaled by 10.
        - wm_conf_raw: base WM gate confidence (logistic 0..1).
        - decay_raw: base WM decay toward uniform (logistic 0..1).
        - age_wm_cost: penalty to WM gate and decay when older (>=0 increases penalty).
        - load_wm_cost: penalty to WM gate and decay when set size is 6 relative to 3.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_raw, beta_raw, wm_conf_raw, decay_raw, age_wm_cost, load_wm_cost = model_parameters

    lr = 1.0 / (1.0 + np.exp(-lr_raw))
    beta = max(1e-6, beta_raw * 10.0)
    wm_conf_base = 1.0 / (1.0 + np.exp(-wm_conf_raw))  # 0..1
    decay_base = 1.0 / (1.0 + np.exp(-decay_raw))      # 0..1

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        load_level = (nS - 3) / 3.0

        wm_conf_logit = np.log(wm_conf_base + eps) - np.log(1.0 - wm_conf_base + eps)
        wm_gate_eff = 1.0 / (1.0 + np.exp(-(wm_conf_logit - age_wm_cost * age_group - load_wm_cost * load_level)))
        wm_gate_eff = np.clip(wm_gate_eff, 0.0, 1.0)

        decay_penalty = 1.0 + age_wm_cost * age_group + load_wm_cost * load_level
        decay_eff = 1.0 - (1.0 - decay_base) ** max(1e-6, decay_penalty)
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_gate_eff * p_wm + (1.0 - wm_gate_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - q[s, a]
            q[s, a] += lr * delta

            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_gate_eff) * w[s, :] + wm_gate_eff * onehot

            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p