def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with global perseveration bias and load-sensitive WM strength.

    Core ideas:
    - RL learns Q-values with age-specific inverse temperature (beta_y for young, beta_o for old).
    - Global perseveration bias: tendency to repeat the most recent action (state-independent).
    - WM uses incremental strengthening toward the chosen action when rewarded; WM decays toward uniform with load.
    - Arbitration depends on WM confidence (sharpening of W_s), reduced by set size. No extra parameter for explicit mix.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_y: RL inverse temperature for young, scaled by 10 internally
    - beta_o: RL inverse temperature for old, scaled by 10 internally
    - perseveration: global choice perseveration bias added to last chosen action (in RL logits)
    - wm_eta: WM incremental learning rate on rewarded trials (0..1)
    - load_sensitivity: how strongly set size reduces WM influence and increases WM decay (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, beta_y, beta_o, perseveration, wm_eta, load_sensitivity = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = beta_y if age_group == 0 else beta_o
    beta *= 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        last_action_global = -1  # -1 indicates none yet

        setsize_factor = max(nS - 3, 0) / 3.0

        wm_leak = np.clip(load_sensitivity * setsize_factor, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_action_global >= 0:
                bias[last_action_global] += perseveration
            rl_logits = beta * (Q_s + bias - np.max(Q_s + bias))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            wm_conf = np.max(W_s) - (1.0 / nA)
            wm_conf = np.clip(wm_conf / (1.0 - 1.0 / nA), 0.0, 1.0)

            wm_weight = np.clip(0.5 + wm_eta * (wm_conf - load_sensitivity * setsize_factor), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            pe = r - q[s, a]
            q[s, a] += lr * pe

            w = (1.0 - wm_leak) * w + wm_leak * w0

            if r > 0.5:

                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta

                w[s, :] = w[s, :] / np.sum(w[s, :])

            last_action_global = a

    return nll