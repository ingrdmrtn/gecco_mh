def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + WM with PE-sensitive WM gating and interference-driven WM degradation.

    Mechanism:
    - Action probability is a mixture of RL and WM policies for the chosen action.
    - RL uses a softmax with inverse temperature softmax_beta*10 and a single learning rate lr.
    - WM policy is high-precision softmax over w[s,:].
    - WM gating depends on positive prediction errors (PE): recent success increases WM reliance.
      wm_weight_eff = clip(wm_weight * (1 + pe_sens * max(0, PE)) * ss_factor * age_factor, 0, 1)
    - WM interference: larger set sizes cause extra decay of WM on each visit; older age amplifies it.
    - Negative PE triggers partial eviction of WM for the state (pushes w[s,:] toward uniform).

    Parameters (6):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature scale (>0), multiplied by 10 internally
    - wm_weight: baseline WM mixture weight (0..1)
    - pe_sens: sensitivity of WM gating to positive PE (>=0)
    - wm_interf: WM interference strength with set size (>=0)
    - age_penalty: additional interference multiplier for older group (>=0)

    Age use:
    - Older group (age_group=1) increases WM interference and reduces WM gating via age_factor.

    Set size use:
    - Larger set sizes increase WM interference and reduce gating via ss_factor = 3/nS.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, pe_sens, wm_interf, age_penalty = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            pe = r - Q_s[a]
            pos_pe = max(0.0, pe)

            ss_factor = 3.0 / float(max(3, nS))  # 1.0 for nS=3, 0.5 for nS=6
            age_factor = 1.0 - 0.3 * age_group   # modest direct age reduction of gating

            wm_weight_eff = wm_weight * (1.0 + pe_sens * pos_pe) * ss_factor * age_factor
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            q[s, a] += lr * pe


            extra_decay = wm_interf * max(0, nS - 3) / 3.0
            extra_decay += age_penalty * age_group
            extra_decay = float(np.clip(extra_decay, 0.0, 1.0))

            w[s, :] = (1.0 - extra_decay) * w[s, :] + extra_decay * (1.0 / nA)

            if pos_pe > 0.0:
                k_pos = min(1.0, 0.7 + 0.3 * pos_pe)  # stronger push on clear successes
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k_pos) * w[s, :] + k_pos * onehot

            if pe < 0.0:
                evict = min(1.0, 0.4 + 0.4 * (-pe))
                w[s, :] = (1.0 - evict) * w[s, :] + evict * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p