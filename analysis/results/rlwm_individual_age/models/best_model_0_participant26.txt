def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + WM with interference across states.

    Mechanism
    - RL system: TD(0) with eligibility traces over state-action pairs, allowing some spillover credit.
    - WM system: one-shot storage for rewarded actions, but stored traces interfere across states,
      especially for larger set sizes and in older adults.
    - Arbitration: fixed WM mixture weight per block that declines with set size and increases interference.

    Parameters
    - model_parameters: [lr, softmax_beta, wm_weight_base, interference, age_interference_boost, trace_lambda]
        - lr: RL learning rate (0..1)
        - softmax_beta: inverse temperature (scaled by 10 internally)
        - wm_weight_base: baseline WM weight (0..1)
        - interference: proportion of WM trace leaking to other states (0..1)
        - age_interference_boost: multiplicative increase of interference for older group (>=0)
        - trace_lambda: eligibility trace decay (0..1)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, interference, age_interference_boost, trace_lambda = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        e = np.zeros((nS, nA))

        wm_weight_block = wm_weight_base * (3.0 / max(1.0, float(nS)))
        wm_weight_block = min(1.0, max(0.0, wm_weight_block))

        inter_base = interference * (float(nS) / 3.0)
        inter_eff = inter_base * (1.0 + age_interference_boost * age_group)
        inter_eff = min(1.0, max(0.0, inter_eff))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            pe = r - Q_s[a]

            e *= trace_lambda

            e[s, a] += 1.0

            q += lr * pe * e

            w[s, :] =  (1.0 - 0.5 * inter_eff) * w[s, :] + (0.5 * inter_eff) * w_0[s, :]

            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0

                gamma_main = 1.0 - 0.5 * inter_eff
                w[s, :] = (1.0 - gamma_main) * w[s, :] + gamma_main * target

                if nS > 1:
                    spill = inter_eff / max(1, nS - 1)
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        w[s_other, :] = (1.0 - spill) * w[s_other, :] + spill * target

                for s_norm in range(nS):
                    w[s_norm, :] = np.clip(w[s_norm, :], eps, None)
                    w[s_norm, :] /= np.sum(w[s_norm, :])

        blocks_log_p += log_p

    return -blocks_log_p