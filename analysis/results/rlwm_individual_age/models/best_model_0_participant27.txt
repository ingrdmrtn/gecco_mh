def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Logistic arbitration by load and age with dual-temperature RL/WM and reward-triggered WM consolidation.

    Mechanism
    - RL: tabular Q-learning, softmax.
    - WM: probability table updated strongly on rewards and mildly on nonrewards; small decay each trial.
    - Arbitration: a block-constant mixture weight set by a logistic function of set size and age group.
      Larger set sizes shift weight toward RL; older age shifts away from WM.
    - Temperatures: separate inverse temperatures for RL and WM.

    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - beta_rl_base: RL inverse temperature, scaled by 10
    - beta_wm_base: WM inverse temperature, scaled by 10
    - arb_bias: bias term in the arbitration logistic (positive favors WM)
    - arb_load_slope: negative values decrease WM weight with larger set sizes
    - arb_age_shift: added if old, subtracted if young; positive reduces WM weight for older adults

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl_base, beta_wm_base, arb_bias, arb_load_slope, arb_age_shift = model_parameters
    beta_rl = beta_rl_base * 10.0
    beta_wm_base = beta_wm_base * 10.0
    age_group = 0 if age[0] <= 45 else 1
    age_term = (arb_age_shift if age_group == 1 else -arb_age_shift)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        U = (1.0 / nA) * np.ones((nS, nA))

        load_term = max(0, nS - 3)
        z = arb_bias + arb_load_slope * load_term + age_term
        wm_weight_block = 1.0 / (1.0 + np.exp(-z))
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        beta_wm = beta_wm_base / (1.0 + 0.2 * load_term)

        wm_decay = 0.02 + 0.01 * load_term

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec = p_rl_vec / max(np.sum(p_rl_vec), eps)
            p_rl = p_rl_vec[a]

            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(logits_wm)
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            W = (1.0 - wm_decay) * W + wm_decay * U

            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = 0.4 * W[s, :] + 0.6 * target
            else:

                W[s, a] = 0.9 * W[s, a] + 0.1 * (1.0 / nA)

    return -float(total_log_p)