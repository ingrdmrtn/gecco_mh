def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + Surprise-gated WM, with load/age-adjusted gating threshold.

    Mechanism:
    - RL: delta rule with eligibility traces (per-state/action).
      The eligibility for the visited state-action is set to 1; all eligibilities
      decay by lambda each trial.
    - WM: recency-weighted policy that stores rewarded action per state.
    - Arbitration: WM weight is increased on trials with high surprise
      (|prediction error| exceeds a threshold). The threshold increases with load
      and with age, making surprise-driven WM recruitment less frequent.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - lambda_trace: eligibility trace decay (0..1)
    - wm_weight_base: maximum WM weight used when surprise exceeds threshold (0..1)
    - surprise_threshold_base: base threshold for |PE| to recruit WM (>=0)
    - age_size_threshold_gain: increases threshold with age/load (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list/array of the six parameters above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, lambda_trace, wm_weight_base, surprise_threshold_base, age_size_threshold_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        E = np.zeros((nS, nA))

        extra_items = max(0, nS - 3)
        thr = surprise_threshold_base * (1.0 + age_size_threshold_gain * (age_group + extra_items))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            prl_all = np.exp(logits_rl)
            prl_all_sum = max(np.sum(prl_all), eps)
            prl_all /= prl_all_sum
            p_rl = prl_all[a]

            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            pwm_all = np.exp(logits_wm)
            pwm_all_sum = max(np.sum(pwm_all), eps)
            pwm_all /= pwm_all_sum
            p_wm = pwm_all[a]

            delta_pre = r - q[s, a]
            surprise = abs(delta_pre)

            wm_weight_eff = wm_weight_base if surprise > thr else 0.0
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            E *= lambda_trace
            E[s, :] *= 0.0
            E[s, a] = 1.0

            delta = r - q[s, a]
            q += lr * delta * E

            w[s, :] = 0.9 * w[s, :] + 0.1 * (1.0 / nA)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p