def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with choice-kernel and capacity-limited WM slots, with age/load-modulated access and lapses.

    Mechanisms:
    - RL system: Q-learning with softmax.
    - Choice-kernel (perseveration): recency bias toward last chosen action, decaying over trials.
    - WM system: stores rewarded mappings; access probability approximates capacity-limited slots.
      If the state is in WM, the policy is near-deterministic toward the stored action.
    - Lapse: random choice probability increases with load and age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10)
    - kernel_strength: weight added to last action in softmax (>0)
    - wm_cap: effective WM capacity (approx number of storable pairs) in [0,6]
    - age_pen: factor scaling age penalty on WM access and lapses (>=0)
    - slip: base lapse rate in [0,1]

    Age and set-size effects:
    - WM access probability p_access = min(1, wm_cap / nS) * (1 - 0.5*age_pen*age_group), clipped to [0,1].
    - Lapse rate lambda = min(0.5, slip * (nS/3) * (1 + 0.5*age_pen*age_group)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, kernel_strength, wm_cap, age_pen, slip = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        p_access = min(1.0, max(0.0, wm_cap / max(1.0, float(nS))))
        p_access *= (1.0 - 0.5 * age_pen * age_group)
        p_access = min(max(p_access, 0.0), 1.0)

        lapse = min(0.5, slip * (nS / 3.0) * (1.0 + 0.5 * age_pen * age_group))

        last_action = None
        kernel_decay = 0.2 + 0.2 * max(0.0, (nS - 3.0) / 3.0)  # faster decay at higher load
        kernel_bias = np.zeros(nA)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            Q_s += kernel_strength * kernel_bias
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            W_s = w[s, :]
            wm_logits = softmax_beta_wm * (W_s - W_s.max())
            exp_wm = np.exp(wm_logits)
            p_wm_vec = exp_wm / (exp_wm.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            p_mix = p_access * p_wm + (1.0 - p_access) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            kernel_bias *= (1.0 - kernel_decay)
            kernel_bias = np.maximum(kernel_bias, 0.0)
            kernel_bias[a] += 1.0  # increase bias toward the chosen action

            delta = r - q[s, a]
            q[s, a] += lr * delta

            d = 0.05 * (nS / 3.0) * (1.0 + 0.5 * age_pen * age_group)
            d = min(max(d, 0.0), 1.0)
            w = (1.0 - d) * w + d * w_0

            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                eta_wm = 0.8  # strong storage for wins
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p