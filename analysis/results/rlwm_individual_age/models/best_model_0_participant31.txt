def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with PE-modulated exploration and PE-gated WM encoding.

    Description:
    - RL uses a single learning rate, but its inverse temperature is dynamically
      modulated by the magnitude of the reward prediction error (|PE|): larger |PE|
      temporarily increases exploration (reduces beta).
    - WM encodes chosen actions only when PE exceeds a gating threshold (surprise),
      with the threshold modulated by set size and age. WM also decays toward uniform.
    - Arbitration mixes WM and RL with a base WM weight reduced by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_base, beta_pe_gain, wm_weight_base, wm_pe_gate, wm_decay_base]
        - lr: RL learning rate.
        - beta_base: Base inverse temperature (scaled by 10 internally).
        - beta_pe_gain: How strongly |PE| reduces beta (exploration gain).
        - wm_weight_base: Baseline WM mixture weight before set-size/age penalties.
        - wm_pe_gate: Base PE magnitude threshold for WM encoding.
        - wm_decay_base: Baseline WM decay toward uniform per visit.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, beta_pe_gain, wm_weight_base, wm_pe_gate, wm_decay_base = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - 0.25 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        wm_decay_eff = np.clip(wm_decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        wm_pe_gate_eff = wm_pe_gate * (nS / 3.0) * (1.0 + 0.5 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            pe = r - q[s, a]
            abs_pe = abs(pe)

            beta_dyn = beta_base / (1.0 + beta_pe_gain * abs_pe)
            beta_dyn = max(beta_dyn, 1e-3)
            softmax_beta = beta_dyn * 10.0

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            q[s, a] += lr * pe

            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            if abs_pe >= wm_pe_gate_eff and r > 0.0:
                w[s, a] += (1.0 - w[s, a]) * 0.9
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p