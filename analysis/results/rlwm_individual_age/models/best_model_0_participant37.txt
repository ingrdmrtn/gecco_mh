def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with adaptive WM gating and novelty-seeking exploration.

    Mechanism:
    - RL: Q-learning with single learning rate and forgetting toward uniform (rho).
    - Exploration: add a novelty bonus inversely related to visit count for (s,a).
    - WM: win-based gating: after outcomes, WM weight is gated by a sigmoid of the last reward.
    - Mixture: base WM weight scaled by set size and age, modulated per-trial by the reward gate.

    Parameters (model_parameters; all used):
    - lr: RL learning rate in [0,1].
    - beta: RL inverse temperature, scaled internally by 10.
    - wm_base: base WM weight in [0,1] (pre-scaling by set size/age).
    - gate_sensitivity: controls sigmoid gating by reward; higher -> stronger boost after reward.
    - rho: RL forgetting rate toward uniform per visit in [0,1].
    - novelty_bonus: strength of novelty/uncertainty bonus added to RL utilities.

    Age and set-size effects:
    - WM base weight is scaled by 3/nS and reduced by age (older rely less on WM).
    - Novelty bonus is reduced by age and by set size (harder to exploit directed exploration in larger/older).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_base, gate_sensitivity, rho, novelty_bonus = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        N = 1e-6 * np.ones((nS, nA))

        size_scale = 3.0 / float(nS)  # 1 for 3-set, 0.5 for 6-set
        wm_weight_base = np.clip(wm_base * size_scale * (1.0 - 0.3 * age_group), 0.0, 1.0)
        novelty_eff = np.clip(novelty_bonus * size_scale * (1.0 - 0.5 * age_group), 0.0, None)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            gate = 1.0 / (1.0 + np.exp(-gate_sensitivity * (r - 0.5)))
            wm_weight_eff = np.clip(wm_weight_base * gate, 0.0, 1.0)

            bonus = novelty_eff / np.sqrt(N[s, :] + 1.0)
            u_rl = softmax_beta * (q[s, :] + bonus)
            denom_rl = np.sum(np.exp(u_rl - u_rl[a]))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_mix, 1e-12)
            log_p += np.log(p_total)

            pe = r - q[s, a]
            q[s, a] += lr * pe
            q = (1.0 - rho) * q + rho * (1.0 / nA)

            w[s, :] = 0.9 * w[s, :] + 0.1 * w0[s, :]  # mild background decay every visit
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0

            N[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p