def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and Pearce-Hall associability; WM gating by PE vs threshold.

    Mechanisms:
    - RL: Q-learning with separate learning rates for positive vs negative PEs.
    - Associability (alpha_s): state-specific attention scaling the effective learning rate via Pearce-Hall.
      alpha_s increases with |PE| and decays otherwise.
    - WM: stores the last rewarded action per state; retrieval is sharp (beta_wm=50).
    - Arbitration (gating): WM weight increases when recent PE exceeds a threshold; threshold increases with
      set size and for older adults (harder WM gate).
    - Age and set-size effects:
      - Gate threshold increases with nS and age (older adults engage WM less readily).
      - RL temperature is unaffected; learning is modulated via associability dynamics.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PEs (0..1)
    - lr_neg: RL learning rate for negative PEs (0..1)
    - beta: inverse temperature for RL (scaled x10 internally)
    - gate_sensitivity: slope of logistic gating by (PE - threshold) (>=0)
    - age_gate_shift: adds to threshold proportional to set size and age (>=0)
    - attn_decay: associability decay rate per trial (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta, gate_sensitivity, age_gate_shift, attn_decay = model_parameters
    beta_eff = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])
        nA = 3

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        alpha = 0.5 * np.ones(nS)

        base_threshold = 0.3 + age_gate_shift * (nS / 3.0) * (1.0 + 0.5 * age_group)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            beta_wm = 50.0
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            pe = r - Q_s[a]


            wm_weight = 1.0 / (1.0 + np.exp(-gate_sensitivity * (pe - base_threshold)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            lr_eff = (lr_pos if pe >= 0.0 else lr_neg) * alpha[s]
            q[s, a] += lr_eff * pe

            alpha[s] = (1.0 - attn_decay) * alpha[s] + attn_decay * np.abs(pe)
            alpha[s] = np.clip(alpha[s], 0.0, 1.0)

            if r > 0.5:
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p