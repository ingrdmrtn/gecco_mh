def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + choice perseveration kernel + WM with set-size driven interference

    - RL: tabular with separate learning rates for positive and negative prediction errors.
    - Perseveration: a choice kernel biases repeating the last action; added to softmax as a bonus.
    - WM: fast store of last rewarded action per state, decays faster at larger set sizes (interference).
    - Mixture: policy = wm_weight_eff * WM + (1 - wm_weight_eff) * RL_with_perseveration.
    - Set size and age:
        * WM weight is downweighted as set size increases: wm_weight_eff = wm_weight / (1 + setsize_sensitivity_eff*(nS-1)).
        * setsize_sensitivity_eff = setsize_sensitivity * (1 + age_group), making older participants more sensitive.

    Parameters (list; total 6):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10.
    - persev_weight: strength of perseveration bias added to the chosen action in previous trial (>=0).
    - setsize_sensitivity: how much larger set sizes reduce WM influence (>=0). Older group doubles this.

    Inputs/Outputs:
    - See cognitive_model1.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, persev_weight, setsize_sensitivity = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))


        C = np.zeros(nA)
        kernel_decay = 0.9  # fixed decay; strength controlled by persev_weight param

        sens_eff = setsize_sensitivity * (1.0 + age_group)
        wm_weight_eff = wm_weight / (1.0 + sens_eff * max(0, nS - 1))

        base_wm_decay = 0.05
        wm_decay = base_wm_decay + sens_eff * 0.10  # more decay with sensitivity

        log_p = 0.0
        last_action = None

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            C = kernel_decay * C
            if last_action is not None:
                C[last_action] += 1.0  # increment for the previously chosen action

            prefs = softmax_beta * Q_s + persev_weight * C

            denom_rl = np.sum(np.exp(prefs - prefs[a]))
            p_rl = 1.0 / denom_rl

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe


            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.0 * w[s, :] + one_hot

            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] = w[s, :] / np.sum(w[s, :])

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p