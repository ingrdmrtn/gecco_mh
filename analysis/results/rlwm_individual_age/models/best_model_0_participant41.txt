def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and PE-gated WM mixture, with age/load reducing WM gating.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6 for this task).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr_pos, lr_neg, softmax_beta, wm_base, gate_sensitivity, age_size_tradeoff]
        - lr_pos: RL learning rate for positive RPE (r - Q[a] > 0).
        - lr_neg: RL learning rate for negative RPE (r - Q[a] <= 0).
        - softmax_beta: inverse temperature for RL policy; internally scaled by *10.
        - wm_base: baseline log-odds for WM contribution when PE=0.
        - gate_sensitivity: sensitivity of WM weight to |prediction error| (PE gating).
        - age_size_tradeoff: increases the discount of WM weight as age/load increase.
                             WM log-odds -= age_size_tradeoff * (age_group + (nS-3)).
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_base, gate_sens, age_size_tradeoff, softmax_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    
    age_group = 0 if age[0] <= 45 else 1
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            pe = r - Q_s[a]

            wm_logit = wm_base + gate_sens * abs(pe) - age_size_tradeoff * (age_group + (nS - 3))
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            if pe >= 0:
                q[s, a] = Q_s[a] + lr_pos * pe
            else:
                q[s, a] = Q_s[a] + lr_neg * pe



            decay = 0.8  # implicit constant decay; WM magnitude is mainly controlled by gating into policy
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
        
        blocks_log_p += log_p
    
    return -blocks_log_p