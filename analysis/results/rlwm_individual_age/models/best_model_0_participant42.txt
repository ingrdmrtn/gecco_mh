def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(α) with eligibility traces and age-modulated decay + stochastic WM gating under load + action stickiness.

    Mechanisms
    ----------
    - RL: single learning rate with eligibility trace λ; λ decays more for older adults.
    - WM: deterministic slot-like memory after reward, with decay toward baseline.
    - Arbitration (wm_weight): stochastic logistic gate that weakens with set size and age.
    - Stickiness: bias to repeat previous action within a block.

    Parameters
    ----------
    states : array-like of int
        State index for each trial within block (0..set_size-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int
        Reward (0 or 1).
    blocks : array-like of int
        Block index.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like
        Single value repeated; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, softmax_beta, lambda_trace, beta_wm_gate, sigma_gate_age, kappa_stick]
        - alpha: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - lambda_trace: base eligibility trace decay (0..1).
        - beta_wm_gate: sensitivity of WM gate to load (higher => stronger drop with load).
        - sigma_gate_age: extra gate noise for older adults (>=0).
        - kappa_stick: action stickiness bias added to last action within a block.

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha, softmax_beta, lambda_trace, beta_wm_gate, sigma_gate_age, kappa_stick = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        elig = np.zeros((nS, nA))  # eligibility traces
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        lam_eff = np.clip(lambda_trace * (0.8 ** age_group), 0.0, 1.0)

        last_action = None




        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            bias = np.zeros(nA)
            if last_action is not None:
                bias[last_action] += kappa_stick

            Qb = Q_s + bias
            denom_rl = np.sum(np.exp(softmax_beta * (Qb - Qb[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            Wb = W_s + bias
            denom_wm = np.sum(np.exp(softmax_beta_wm * (Wb - Wb[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)


            load = float(nS - 3)  # 0 for set=3, 3 for set=6

            noise_unit = ((t + 1) % 5) / 4.0 - 0.5  # in [-0.5, 0.5]
            noise = sigma_gate_age * age_group * noise_unit
            gate_bias = -(beta_wm_gate * load) + noise
            wm_weight = 1.0 / (1.0 + np.exp(-gate_bias))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]

            elig *= lam_eff
            elig[s, a] += 1.0
            q += alpha * pe * elig

            w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p