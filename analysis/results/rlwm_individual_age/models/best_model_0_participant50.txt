def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM capacity-gated mixture with state-wise choice-kernel perseveration.

    Idea
    - Choices are a mixture of model-free RL and WM policies.
    - WM engagement is gated by an age-dependent capacity K relative to set size nS.
      Older adults have a lower effective K than younger adults.
    - RL policy is augmented by a choice-kernel that captures perseveration within state.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, K_young, K_old, eta_ck, ck_decay]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - K_young: WM capacity for younger group (in items).
        - K_old: WM capacity for older group (in items).
        - eta_ck: weight of choice-kernel in the RL softmax (>=0).
        - ck_decay: decay of choice-kernel per trial (0..1); higher = more persistent perseveration.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, K_young, K_old, eta_ck, ck_decay = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0  # near-deterministic WM

        K = K_old if age_group == 1 else K_young

        wm_weight = float(np.clip(K / max(1.0, nS), 0.0, 1.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        ck = np.zeros((nS, nA))  # centered choice-kernel

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            CK_s = ck[s, :]

            CK_s_centered = CK_s - np.mean(CK_s)
            prefs = softmax_beta * (Q_s - np.max(Q_s)) + eta_ck * CK_s_centered
            pi_rl = np.exp(prefs - np.max(prefs))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            ck[s, :] *= ck_decay
            ck[s, a] += (1.0 - ck_decay)

            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:

                pass

        blocks_log_p += log_p

    return -float(blocks_log_p)