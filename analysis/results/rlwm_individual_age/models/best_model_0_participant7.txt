def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM with confidence-based arbitration (gating).

    Idea:
    - WM and RL produce policies; arbitration favors WM when its confidence is high.
    - WM confidence is the margin between the top two WM action strengths.
    - Gating weight = sigmoid(gamma_gate * (conf_scaled - theta_wm_eff)).
    - Load reduces effective confidence (divide by nS/3). Age increases threshold.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - theta_wm: gating threshold for WM use (can be negative..positive)
    - gamma_gate: slope of the gating sigmoid (>=0)
    - alpha_wm: WM learning rate toward target (0..1)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, theta_wm, gamma_gate, alpha_wm = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        theta_eff = theta_wm + (0.1 if age_group == 1 else 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)

            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            sorted_W = np.sort(W_s)[::-1]
            conf = sorted_W[0] - (sorted_W[1] if nA > 1 else 0.0)

            conf_scaled = conf / (float(nS) / 3.0)

            gate = 1.0 / (1.0 + np.exp(-gamma_gate * (conf_scaled - theta_eff)))
            gate = np.clip(gate, 0.0, 1.0)

            p_total = gate * p_wm + (1.0 - gate) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            rpe = r - Q_s[a]
            q[s, a] += lr * rpe

            target = np.ones(nA) / nA
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] += alpha_wm * (target - W_s)
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p