Below are three standalone cognitive models that combine reinforcement learning (RL) with working memory (WM) to explain choices in the RLWM task. Each model:

- Follows the requested signature: (states, actions, rewards, blocks, set_sizes, age, model_parameters)
- Returns the negative log-likelihood of the observed choices
- Uses all parameters meaningfully (no unused parameters)
- Uses age group (0 = young, 1 = old) and set size to modulate behavior in theoretically motivated ways
- Fills in the WM policy and WM updating, as requested

Note: Assumes numpy is available as np (already imported elsewhere).



def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and decay-limited WM. Age affects WM capacity (K).
    
    Mechanism:
    - RL system: standard delta rule with softmax choice.
    - WM system: maintains state-action weights W_s; policy via a high-beta softmax.
      WM weight is reduced when set size exceeds WM capacity (K), yielding
      wm_weight_eff = wm_weight_base * min(1, K / set_size).
    - WM update: decays toward uniform on non-rewarded trials and becomes more
      concentrated on the chosen action on rewarded trials (gated by wm_decay).
    - Age group modulates WM capacity: different K for young vs old.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: base mixture weight for WM in the policy (0..1)
    - softmax_beta: inverse temperature for RL softmax (scaled by 10 internally)
    - wm_decay: WM update gate/decay (0..1); larger means stronger update/decay
    - K_young: WM capacity when young (e.g., around 3–4)
    - K_old: WM capacity when old (e.g., around 2–3)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, K_young, K_old = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # highly deterministic WM

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1
    K = K_young if age_group == 0 else K_old

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])
        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action a (stable softmax form)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM capacity-adjusted mixture weight
            cap = min(1.0, max(0.0, K / float(nS)))
            wm_weight_eff = wm_weight_base * cap

            # WM policy probability for chosen action a
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # WM update:
            # Move toward one-hot on reward; decay toward uniform on no-reward
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            uniform = (1.0 / nA) * np.ones(nA)
            target = r * one_hot + (1.0 - r) * uniform
            w[s, :] = (1.0 - wm_decay) * W_s + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p



def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM with set-size-dependent lapse.
    
    Mechanism:
    - RL system: separate learning rates for positive and negative prediction errors,
      with softmax choice.
    - WM system: stores last rewarded action per state; on each trial, the WM
      policy is a mixture of (i) a high-beta softmax over WM weights and (ii) a
      uniform 'guess' with probability equal to a lapse/interference term that
      grows with set size.
    - Age group modulates the lapse rate in WM: separate wm_lapse for young vs old.
    
    Parameters (model_parameters):
    - alpha_pos: RL learning rate for rewarded trials (0..1)
    - alpha_neg: RL learning rate for non-rewarded trials (0..1)
    - wm_weight_base: base mixture weight for WM (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_lapse_young: WM lapse base for young (0..1), increases with set size
    - wm_lapse_old: WM lapse base for old (0..1), increases with set size
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, wm_lapse_young, wm_lapse_old = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1
    wm_lapse_base = wm_lapse_young if age_group == 0 else wm_lapse_old

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # Set-size-dependent WM lapse/interference:
            # Scale lapse approximately linearly with set size (3 -> lower lapse; 6 -> higher)
            # Clip to [0, 1]
            lapse_ss = np.clip(wm_lapse_base * (nS / 6.0), 0.0, 1.0)

            # WM policy: mixture of deterministic WM softmax and uniform guessing due to lapse
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / denom_wm
            p_wm = (1.0 - lapse_ss) * p_wm_det + lapse_ss * (1.0 / nA)

            # Mixture with RL
            p_total = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            if delta >= 0:
                q[s, a] = Q_s[a] + alpha_pos * delta
            else:
                q[s, a] = Q_s[a] + alpha_neg * delta

            # WM update:
            # On reward, set the state to the rewarded action (one-hot).
            # On no-reward, keep previous WM trace (no overwrite).
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p



def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value forgetting + slot-like WM with capacity-limited availability
    and global action lapse. Age affects the WM mixture weight.
    
    Mechanism:
    - RL: delta rule with softmax; includes 'q_forget' that pulls Q-values toward
      uniform each time the state is visited (helps capture exploration/forgetting).
    - WM: slot-like availability p_in = min(1, K_eff / set_size). We model this via
      a set-size factor p_in = min(1, 3 / set_size) (approx 1.0 for 3, 0.5 for 6) and
      then combine with an age-weighted wm_weight (wm_weight_young vs wm_weight_old).
      The WM policy is p_wm = p_in * softmax(W) + (1 - p_in) * uniform.
    - A global lapse mixes the final choice probability with uniform.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_young: WM mixture weight for young
    - wm_weight_old: WM mixture weight for old
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - q_forget: RL forgetting rate (0..1) toward uniform for a visited state
    - lapse: global action lapse (0..1) applied to the final policy; also used as
             a mild WM decay toward uniform after non-reward trials
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_young, wm_weight_old, softmax_beta, q_forget, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1
    wm_weight_age = wm_weight_young if age_group == 0 else wm_weight_old

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # Slot-like WM availability as a function of set size (3 -> 1.0; 6 -> 0.5)
            p_in = min(1.0, 3.0 / float(nS))

            # Effective WM mixture weight (age-scaled)
            wm_weight_eff = wm_weight_age

            # WM policy: available with prob p_in, otherwise uniform
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / denom_wm
            p_wm = p_in * p_wm_det + (1.0 - p_in) * (1.0 / nA)

            # RL-WM mixture
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Global lapse on final choice
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL forgetting toward uniform for visited state s (apply before update)
            q[s, :] = (1.0 - q_forget) * q[s, :] + q_forget * (1.0 / nA)

            # RL update
            delta = r - q[s, a]
            q[s, a] = q[s, a] + lr * delta

            # WM update:
            # On reward, set to one-hot; on no reward, decay toward uniform slightly (using lapse as decay)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = (1.0 - lapse) * W_s + lapse * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p