Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) mechanisms with set size and age effects. Each function returns the negative log-likelihood of the observed choices.

Notes:
- Assumes numpy as np is already imported.
- Age group is encoded as 0 for young (<=45) and 1 for old (>45).
- Each model uses all provided parameters meaningfully and keeps the total parameter count â‰¤ 6.
- The WM component uses a high inverse temperature (softmax_beta_wm = 50) to approximate near-deterministic retrieval.
- Set size reduces WM effectiveness; age influences either WM reliability/decay or RL temperature as specified.

Model 1: RL+WM mixture with capacity-limited WM and age-modulated choice temperature
- Parameters: lr, wm_weight, softmax_beta, wm_decay, K, age_temp_effect
- Idea: WM has a capacity K that scales its contribution as min(1, K/set_size). Older age reduces effective choice temperature, while younger age increases it.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and age-modulated inverse temperature.

    Parameters
    - states: array of state indices per trial (0..nS-1 within block)
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (3 or 6; constant within block)
    - age: array with a single repeated age value
    - model_parameters: list/array with
        lr: RL learning rate (0..1)
        wm_weight: base weight of WM policy in mixture (0..1)
        softmax_beta: base inverse temperature for RL policy (scaled up internally)
        wm_decay: WM decay/interference rate toward uniform (0..1)
        K: WM capacity (1..6); effective WM weight scales as min(1, K/set_size)
        age_temp_effect: scales choice temperature by age group:
                         beta_eff = beta * 10 * (1 + age_temp_effect*(1 - 2*age_group))

    Returns
    - Negative log-likelihood of observed actions under the model
    """
    lr, wm_weight, softmax_beta, wm_decay, K, age_temp_effect = model_parameters

    # Age group encoding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Scale RL temperature and age modulation
    softmax_beta = softmax_beta * 10.0
    softmax_beta *= (1.0 + age_temp_effect * (1 - 2 * age_group))  # increase for young, decrease for old
    softmax_beta = max(1e-6, softmax_beta)

    softmax_beta_wm = 50.0  # near-deterministic WM retrieval
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight depends on capacity and set size
        wm_capacity_factor = min(1.0, max(1e-6, K) / max(1, nS))
        wm_weight_eff = np.clip(wm_weight * wm_capacity_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL softmax probability of chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM softmax probability of chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture of WM and RL
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform and update on reward
            # Decay (interference) increases with set size implicitly via wm_capacity_factor (weight), but here we also apply fixed decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, store a strong one-hot trace for the chosen action
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend the one-hot with current WM row to reflect overwriting
                overwrite_strength = 1.0  # deterministic overwrite when rewarded
                w[s, :] = (1 - overwrite_strength) * w[s, :] + overwrite_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: RL with asymmetric learning rates, WM gated by recent reward and stronger decay with age and set size
- Parameters: lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, lapse
- Idea: WM gets updated only on rewarded trials, decays toward uniform. Set size gates WM contribution by 3/set_size. Older participants suffer more WM decay (fixed multiplicative factor). Includes a small lapse rate.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL (asymmetric learning rates) + WM gated by reward, with set-size gating and age-increased WM decay.

    Parameters
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (3 or 6; constant within block)
    - age: array with a single repeated age value
    - model_parameters:
        lr_pos: RL learning rate for positive PE
        lr_neg: RL learning rate for negative PE
        wm_weight: base WM mixture weight
        softmax_beta: base inverse temperature for RL policy (scaled up internally)
        wm_decay: base WM decay rate toward uniform
        lapse: lapse probability mixed with uniform action

    Returns
    - Negative log-likelihood of observed actions under the model
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, lapse = model_parameters

    # Age group encoding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta = softmax_beta * 10.0
    softmax_beta = max(1e-6, softmax_beta)
    softmax_beta_wm = 50.0
    lapse = np.clip(lapse, 0.0, 0.2)  # restrict lapse to reasonable range
    eps = 1e-12

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size gating of WM: more weight when set size is small
        set_gate = 3.0 / max(3.0, float(nS))  # 1 for 3, 0.5 for 6
        wm_weight_eff = np.clip(wm_weight * set_gate, 0.0, 1.0)

        # Age increases WM decay (older participants decay faster)
        wm_decay_eff = wm_decay * (1.0 + 0.5 * age_group)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture and lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(eps, p_total))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_use * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM update only when rewarded: cache the correct action strongly
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # overwrite on success

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: RL with perseveration bias and recency-based WM; WM is noisier for larger set sizes and older age
- Parameters: lr, wm_weight, softmax_beta, perseveration, wm_noise, lapse
- Idea: WM stores the most recent action in each state (regardless of reward). WM noise grows with set size and age. RL policy includes a perseveration bias for repeating the last chosen action in the current state.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with perseveration and recency WM. WM is noisier under higher set size and with older age.

    Parameters
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (3 or 6; constant within block)
    - age: array with a single repeated age value
    - model_parameters:
        lr: RL learning rate
        wm_weight: base WM mixture weight
        softmax_beta: base inverse temperature for RL policy (scaled up internally)
        perseveration: bias added to the last chosen action for that state in RL values
        wm_noise: base WM noise (0..1) that increases entropy of WM; scaled by set size and age
        lapse: lapse probability mixed with uniform

    Returns
    - Negative log-likelihood of observed actions under the model
    """
    lr, wm_weight, softmax_beta, perseveration, wm_noise, lapse = model_parameters

    # Age group encoding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta = softmax_beta * 10.0
    softmax_beta = max(1e-6, softmax_beta)
    softmax_beta_wm = 50.0
    lapse = np.clip(lapse, 0.0, 0.2)
    eps = 1e-12

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: last action per state (initialize as None via -1)
        last_action = -1 * np.ones(nS, dtype=int)

        # WM weight decreases with set size; WM noise increases with set size and age
        wm_weight_eff = np.clip(wm_weight * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
        wm_noise_eff = np.clip(wm_noise * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL with perseveration bias (state-specific)
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy: one-hot on last action with noise toward uniform
            if last_action[s] >= 0:
                wm_pref = np.zeros(nA)
                wm_pref[last_action[s]] = 1.0
            else:
                wm_pref = np.ones(nA) / nA  # no memory yet

            # Additive noise toward uniform mixture
            wm_policy_vec = (1.0 - wm_noise_eff) * wm_pref + wm_noise_eff * (np.ones(nA) / nA)

            # Convert to softmax probability of chosen action using WM "utilities"
            # Turn the distribution into log-utilities by inverse softmax (safe via small floor)
            wm_policy_vec = np.clip(wm_policy_vec, eps, 1.0)
            wm_utils = np.log(wm_policy_vec)  # logits consistent with desired distribution
            denom_wm = np.sum(np.exp(softmax_beta_wm * (wm_utils - wm_utils[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture and lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(eps, p_total))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM recency update: store the last chosen action regardless of reward
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p

How set size and age affect parameters in these models
- Model 1:
  - Set size: WM effectiveness downscaled via min(1, K/set_size).
  - Age: Choice temperature scaled by age_temp_effect; older group gets lower effective beta, increasing randomness.
- Model 2:
  - Set size: WM weight downscaled by 3/set_size.
  - Age: WM decay increased by 50% for older participants.
- Model 3:
  - Set size: WM weight downscaled by 3/set_size; WM noise upscaled by set_size/3.
  - Age: WM noise increased by 50% for older participants; perseveration applies regardless of age but impacts RL choice dynamics.