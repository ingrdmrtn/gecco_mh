def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying Working Memory (WM) mixture model.

    On each trial, choice probability is a mixture of:
    - RL policy: softmax over Q-values
    - WM policy: near-deterministic retrieval of the correct action if it was recently rewarded and retained in WM

    WM availability scales with set size via a capacity parameter, and WM mixture weight differs by age group.
    WM traces decay toward uniform over time.

    Parameters
    - model_parameters: list or array with 6 parameters
        0) lr: RL learning rate (0..1)
        1) wm_weight_young: base WM mixture weight for age_group=0 (young)
        2) wm_weight_old: base WM mixture weight for age_group=1 (old)
        3) softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
        4) wm_decay: WM decay rate toward uniform (0..1)
        5) wm_capacity: WM capacity in number of items, scales availability as min(1, wm_capacity / set_size)
    Returns
    - negative log-likelihood of observed choices
    """
    lr, wm_weight_young, wm_weight_old, softmax_beta, wm_decay, wm_capacity = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL
    softmax_beta_wm = 50.0  # highly deterministic WM policy
    
    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1
    base_wm_weight = wm_weight_young if age_group == 0 else wm_weight_old

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            # Stable softmax probability for the chosen action a (using normalization by Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # - availability scales with capacity and set size
            # - if a reward was recently observed for this state, WM stores a near-deterministic one-hot
            #   distribution; otherwise it is closer to uniform (subject to decay)
            # Availability
            p_avail = min(1.0, float(wm_capacity) / float(nS))
            # Deterministic WM policy from current WM distribution
            W_s = w[s, :]
            p_wm_choice = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix policies; effective WM component modulated by availability
            wm_weight_eff = base_wm_weight * p_avail
            p_total = wm_weight_eff * p_wm_choice + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # WM update on rewarded trials: store one-hot for the rewarded action in that state
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning + WM gating with set-size interference.

    Choice is a mixture of:
    - RL policy with separate learning rates for positive vs. negative outcomes.
    - WM policy that implements win-stay: upon reward, store the action in WM for that state.
      If no reward, WM reverts to uniform for that state.

    WM contribution is gated by set-size interference via an exponential decay in availability
    and by age-dependent WM mixture weights.

    Parameters
    - model_parameters: list or array with 6 parameters
        0) lr_pos: RL learning rate for positive prediction errors (0..1)
        1) lr_neg: RL learning rate for negative prediction errors (0..1)
        2) wm_weight_young: WM mixture weight for young
        3) wm_weight_old: WM mixture weight for old
        4) softmax_beta: inverse temperature for RL (scaled by 10 internally)
        5) kappa_interference: WM availability decay with set size; availability = exp(-kappa * (set_size - 3))
    Returns
    - negative log-likelihood of observed choices
    """
    lr_pos, lr_neg, wm_weight_young, wm_weight_old, softmax_beta, kappa = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    wm_weight = wm_weight_young if age_group == 0 else wm_weight_old

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM availability
        p_avail = float(np.exp(-kappa * max(0, nS - 3)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic from WM distribution w[s]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            wm_eff = wm_weight * p_avail
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: win-stay memory upon reward, else revert to uniform for that state
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + one-shot WM with retrieval noise and set-size/age effects on RL precision.

    Choice is a mixture of:
    - RL policy with inverse temperature reduced in larger set sizes (precision drops as set size increases).
      Age further shifts inverse temperature (older group lower precision if age_beta_shift < 0).
    - WM policy implements one-shot win-stay per state; retrieval noise makes WM soft rather than fully deterministic.

    Parameters
    - model_parameters: list or array with 6 parameters
        0) lr: RL learning rate (0..1)
        1) wm_weight: WM mixture weight (applied to both age groups)
        2) beta_base: base RL inverse temperature before adjustments (scaled by 10 internally)
        3) wm_eta: WM retrieval noise (>0). Effective WM beta = 1 / wm_eta (higher = more deterministic)
        4) size_beta_scale: scales how much set size reduces beta; beta_eff = beta_base / (1 + size_beta_scale*(set_size-3))
        5) age_beta_shift: additive shift applied to beta for older group only (after scaling); multiplied by 10 internally
    Returns
    - negative log-likelihood of observed choices
    """
    lr, wm_weight, beta_base, wm_eta, size_beta_scale, age_beta_shift = model_parameters
    beta_base *= 10.0
    age_beta_shift *= 10.0

    # Age group: 0 young, 1 old; shift beta for old
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size adjusted RL beta
        beta_sz = beta_base / (1.0 + size_beta_scale * max(0, nS - 3))
        softmax_beta = beta_sz + (age_beta_shift if age_group == 1 else 0.0)

        # WM retrieval inverse temperature from noise
        softmax_beta_wm = 1.0 / max(1e-6, wm_eta)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with adjusted beta
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (one-shot win-stay with retrieval noise)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on reward, store one-hot; else leave as is (no decay parameter here)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # keep current WM trace (imperfect retrieval already handled via wm_eta)
                pass

        blocks_log_p += log_p

    return -blocks_log_p