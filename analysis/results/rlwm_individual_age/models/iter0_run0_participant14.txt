Below are three standalone cognitive models that follow the requested template, combine reinforcement learning (RL) with a working-memory (WM) component, and use age group and set size in meaningful ways. Each returns the negative log-likelihood of the observed choices.

Note: These functions assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with decay and set-size-dependent interference.
    
    Description:
    - Mixture policy between model-free RL and a WM store.
    - RL updates via Rescorla-Wagner with a single learning rate.
    - WM stores action probabilities per state and is updated fast with reward,
      but decays toward uniform and is subject to interference that increases with set size.
    - The WM weight is scaled by set size (lower for larger set size) and by age group
      (young participants get a modest boost to WM weight).
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0, 1]
    - wm_weight: base WM mixture weight in [0, 1]
    - softmax_beta: inverse temperature for RL softmax (internally scaled by 10)
    - wm_decay: per-trial decay of WM toward uniform in [0, 1]
    - wm_interference: interference rate per trial, scaled by set size in [0, 1]
    
    Age group usage:
    - Age group is 0 for young (<=45) and 1 for old (>45).
    - Effective WM weight is multiplied by 1.15 for young and 0.85 for old.
    
    Set size usage:
    - WM weight is scaled by (3 / nS), reducing WM influence in larger set sizes.
    - WM interference is scaled by ((nS - 3) / 3), increasing interference in larger set sizes.
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_interference = model_parameters
    softmax_beta *= 10.0  # expand range
    
    # Age group: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1
    # Deterministic WM readout
    softmax_beta_wm = 50.0
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))     # RL Q-values
        w = (1.0 / nA) * np.ones((nS, nA))     # WM weights per state-action
        w_0 = (1.0 / nA) * np.ones((nS, nA))   # Uniform prior for WM

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action using softmax trick
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working memory policy (softmax over WM weights, high beta)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight scales with set size and age
            # - set size scaling: smaller for larger sets (3/nS)
            # - age scaling: boost for young, dampen for old
            age_scale = 1.15 if age_group == 0 else 0.85
            size_scale = 3.0 / float(nS)
            wm_weight_eff = wm_weight * age_scale * size_scale
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)  # numerical floor
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Row-specific update at current state:
            #    - If rewarded, bias memory strongly toward chosen action
            #    - If not rewarded, decay current state's memory toward uniform
            if r > 0.5:
                refresh = 1.0 - wm_decay
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - refresh) * (1.0 / nA) + refresh * one_hot
            else:
                # Decay toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)

            # 2) Global interference: larger set sizes increase decay to uniform
            intf = wm_interference * max(0.0, (nS - 3.0) / 3.0)
            if intf > 0.0:
                w = (1.0 - intf) * w + intf * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and set-size-adaptive learning rate + WM as one-shot last-correct memory.
    
    Description:
    - RL component with a learning rate that increases for larger set sizes and decreases for older age,
      helping compensate for reduced WM efficacy when the task is harder.
    - WM stores the last rewarded action per state with a memory strength that decays over time,
      more so in larger set sizes. WM policy is near-deterministic when memory strength is high.
    - Mixture of WM and RL where WM mixture weight is reduced in larger set sizes and scaled by age.
    
    Parameters (model_parameters):
    - lr_base: base RL learning rate in [0, 1]
    - lr_size_gain: additive gain on learning rate for larger set sizes (can be negative or positive)
    - lr_age_gain: additive gain if young (age_group==0), negative or positive
    - wm_weight: base WM mixture weight in [0, 1]
    - softmax_beta: inverse temperature for RL softmax (internally scaled by 10)
    - wm_decay: decay of WM memory strength per trial in [0, 1]
    
    Age group usage:
    - Learning rate increases by lr_age_gain if young; decreases by 0 if old.
    - WM mixture weight is multiplied by 1.1 if young, 0.9 if old.
    
    Set size usage:
    - Learning rate increases by lr_size_gain * ((nS - 3)/3).
    - WM mixture weight is scaled by 3/nS.
    - WM memory strength decays per trial at rate wm_decay * ((nS)/6).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_base, lr_size_gain, lr_age_gain, wm_weight, softmax_beta, wm_decay = model_parameters
    softmax_beta *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # For WM, store last rewarded action index and a strength per state
        last_rewarded_action = -1 * np.ones(nS, dtype=int)  # -1 means unknown
        mem_strength = np.zeros(nS)  # in [0,1]

        # Compute effective learning rate for this block
        size_factor = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6
        lr = lr_base + lr_size_gain * size_factor + (lr_age_gain if age_group == 0 else 0.0)
        lr = np.clip(lr, 0.0, 1.0)

        # Effective WM mixture weight
        wm_weight_eff_base = wm_weight * (3.0 / float(nS))
        wm_weight_eff_base *= (1.1 if age_group == 0 else 0.9)
        wm_weight_eff_base = np.clip(wm_weight_eff_base, 0.0, 1.0)

        # Per-trial WM decay scaled with set size
        wm_decay_eff = wm_decay * (float(nS) / 6.0)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy construction
            if last_rewarded_action[s] >= 0:
                # Deterministic preference toward last rewarded action with strength
                one_hot = np.zeros(nA)
                one_hot[last_rewarded_action[s]] = 1.0
                # Convert to soft preference distribution via softmax on strength
                W_s = (1.0 - mem_strength[s]) * (1.0 / nA) + mem_strength[s] * one_hot
            else:
                W_s = np.ones(nA) * (1.0 / nA)

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff_base * p_wm + (1.0 - wm_weight_eff_base) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            # Decay strength each trial
            mem_strength[s] = (1.0 - wm_decay_eff) * mem_strength[s]
            # If reward, store/update last rewarded action and reinforce strength
            if r > 0.5:
                last_rewarded_action[s] = a
                # Push strength up toward 1 quickly when rewarded
                mem_strength[s] = np.clip(mem_strength[s] + (1.0 - mem_strength[s]) * 0.8, 0.0, 1.0)
            else:
                # If not rewarded and memory wrongly points to chosen action, slightly penalize strength
                if last_rewarded_action[s] == a:
                    mem_strength[s] = np.clip(mem_strength[s] * 0.7, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM mixture with uncertainty-based gating and tunable WM temperature.
    
    Description:
    - RL and WM both generate policies; the mixture weight is gated by RL policy uncertainty,
      set size, and age. When RL is uncertain (high entropy), the model leans more on WM,
      especially in small set sizes and for younger participants.
    - RL updates with a single learning rate.
    - WM updates quickly: rewards create a peaked WM distribution for that state; non-rewards
      suppress the chosen action in WM. WM maintains a distribution per state, with softmax readout
      controlled by wm_beta.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0, 1]
    - base_wm_weight: base WM weight in [0, 1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_beta: WM inverse temperature (scaled by 10 internally)
    - gate_sensitivity: scales the effect of RL entropy on WM weight (>=0)
    
    Age group usage:
    - Young participants (age_group==0) receive a +15% multiplier on the gated WM weight,
      older participants receive a -15% multiplier.
    
    Set size usage:
    - WM weight is scaled by 3/nS (less WM reliance in larger sets).
    - RL entropy gating is further scaled down in larger sets: effective gating factor is multiplied by (3/nS).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, base_wm_weight, softmax_beta, wm_beta, gate_sensitivity = model_parameters
    softmax_beta *= 10.0
    wm_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM distribution per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy distribution for s
            Q_shift = Q_s - np.max(Q_s)  # for numerical stab.
            pi_rl = np.exp(softmax_beta * Q_shift)
            pi_rl /= np.sum(pi_rl)
            p_rl = pi_rl[a]

            # RL uncertainty: normalized entropy in [0,1]
            entropy = -np.sum(pi_rl * (np.log(pi_rl + 1e-12)))
            max_entropy = np.log(nA)
            H_norm = entropy / max_entropy  # 0=deterministic, 1=uniform

            # WM policy distribution (softmax over WM logits)
            W_shift = W_s - np.max(W_s)
            pi_wm = np.exp(wm_beta * W_shift)
            pi_wm /= np.sum(pi_wm)
            p_wm = pi_wm[a]

            # Gated WM weight:
            size_scale = 3.0 / float(nS)  # smaller in larger sets
            age_scale = 1.15 if age_group == 0 else 0.85
            # Gate increases with RL uncertainty; scaled down in larger sets
            gate = gate_sensitivity * H_norm * size_scale
            wm_weight_eff = base_wm_weight * (1.0 + gate)
            wm_weight_eff *= size_scale
            wm_weight_eff *= age_scale
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # If reward: sharpen distribution toward chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong blend toward one-hot
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot
            else:
                # Penalize the chosen action within WM, slight decay toward uniform otherwise
                w[s, a] = 0.5 * w[s, a]  # suppress chosen action
                # Renormalize softly by mixing with uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Ensure WM row remains a proper distribution
            w_sum = np.sum(w[s, :])
            if w_sum <= 0.0:
                w[s, :] = w_0[s, :]
            else:
                w[s, :] /= w_sum

        blocks_log_p += log_p

    return -blocks_log_p