Below are three standalone cognitive models that instantiate the RL+WM template for the rlwm task. Each returns the negative log-likelihood of the observed choices. All parameters are used and bounded in a meaningful way by set size (3 vs 6) and age group (young = 0, old = 1). I assume numpy is already imported as np as per the instructions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM model with set-size-dependent WM decay and age-weighted WM mixture.
    
    Policy:
    - Mixture of RL softmax and WM softmax:
        p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
      where p_rl is a softmax over Q(s, a) and p_wm is a softmax over the WM table W(s, a).
    - WM has set-size-dependent decay (faster decay for larger set sizes).
    - Age group reduces WM mixture weight (older rely less on WM).
    
    WM update:
    - Per trial, decay W(s) toward a uniform prior w_0 with a decay rate that depends on set size.
    - If reward=1, store the chosen action as a one-shot memory (with small noise to avoid zeros).
    
    RL update:
    - Standard delta rule with a single learning rate.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0, 1]
    - wm_weight_base: base mixture weight of WM in [0, 1]
    - softmax_beta: inverse temperature (scaled by 10 internally)
    - wm_decay_3: WM decay toward prior per trial in set size 3 blocks (in [0,1])
    - wm_decay_6: WM decay toward prior per trial in set size 6 blocks (in [0,1])
    - wm_noise: small noise added to WM one-shot store to avoid deterministic zeros (e.g., in [0, 0.2])
    
    Age dependence:
    - Age group coded as 0 (young) or 1 (old).
    - Effective WM weight: wm_weight_eff = wm_weight_base * (1 - 0.5 * age_group),
      i.e., older participants rely less on WM.
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay_3, wm_decay_6, wm_noise = model_parameters
    # Scale beta to allow a higher dynamic range (as in the template)
    softmax_beta *= 10.0
    
    # Age group
    age_group = 0 if age[0] <= 45 else 1
    wm_weight_eff_global = wm_weight_base * (1 - 0.5 * age_group)
    wm_weight_eff_global = max(0.0, min(1.0, wm_weight_eff_global))
    
    softmax_beta_wm = 50.0  # very deterministic WM softmax as per template
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Set-size dependent WM decay for this block
        wm_decay = wm_decay_3 if nS == 3 else wm_decay_6
        wm_decay = max(0.0, min(1.0, wm_decay))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy: probability of chosen action via softmax trick
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl
            
            # WM policy: probability of chosen action via WM softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / denom_wm
            
            # Mixture
            p_total = wm_weight_eff_global * p_wm_soft + (1.0 - wm_weight_eff_global) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta
            
            # WM decay toward prior
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # WM one-shot update if rewarded
            if r > 0.5:
                # Put mass on the rewarded action with small noise to avoid zeros
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_noise) * one_hot + wm_noise * w_0[s, :]
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM capacity model with separate RL learning rates and probabilistic WM availability.
    
    Key ideas:
    - RL uses asymmetric learning rates for positive vs. negative prediction errors.
    - WM has finite capacity K; WM availability p_avail scales as K_eff / nS (capped at 1).
    - Age group reduces effective capacity: K_eff = max(1, K_base - age_k_drop * age_group).
    - WM policy is a mixture of (available) WM softmax and uniform guessing when WM is unavailable:
        p_wm = p_avail * p_wm_soft + (1 - p_avail) * (1/nA)
    - Overall choice policy:
        p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
    
    WM update:
    - One-shot store on rewarded trials; no decay across trials (capacity limitation handled via availability).
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (rewarded trials), in [0, 1]
    - lr_neg: RL learning rate for negative PE (unrewarded trials), in [0, 1]
    - softmax_beta: inverse temperature (scaled by 10 internally)
    - wm_weight: mixture weight of WM in [0, 1]
    - K_base: baseline WM capacity (recommended range [2, 6])
    - age_k_drop: capacity drop for older group (>=0). K_eff = K_base - age_k_drop * age_group.
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, K_base, age_k_drop = model_parameters
    softmax_beta *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Effective capacity
        K_eff = K_base - age_k_drop * age_group
        K_eff = max(1.0, min(6.0, K_eff))
        p_avail = min(1.0, K_eff / float(nS))
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl
            
            # WM policy with availability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / denom_wm
            p_wm = p_avail * p_wm_soft + (1.0 - p_avail) * (1.0 / nA)
            
            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if r > 0.5 else lr_neg
            q[s, a] = Q_s[a] + lr_use * delta
            
            # WM update: one-shot on rewarded trials, no decay
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM model with RL forgetting and error-gated WM policy; set-size-dependent exploration.
    
    Key ideas:
    - RL has a single learning rate but includes Q-value forgetting (toward uniform) for stability.
    - Inverse temperature increases or decreases in set size 6 via a delta parameter:
        beta_trial = softmax_beta * (1 + 1_{nS=6} * beta_set6_delta), then scaled by 10.
    - WM mixture weight is penalized by age.
    - WM policy is 'error-gated': when absolute RPE is large (especially in set size 6),
      WM policy is diluted toward uniform guessing, reflecting uncertainty/overload:
        g = 1_{nS=6} * |delta| clipped to [0,1]
        p_wm = (1 - g) * p_wm_soft + g * (1/nA)
    - Overall mixture:
        p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
    
    WM update:
    - Per-trial forgetting toward prior with rate wm_forget; one-shot store on rewarded trials.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0, 1]
    - softmax_beta_base: baseline inverse temperature (scaled by 10 internally)
    - beta_set6_delta: multiplicative delta applied in set size 6 to modulate exploration
                       (beta_trial = beta_base * (1 + delta) when nS=6)
    - wm_weight: base WM mixture weight in [0, 1]
    - wm_forget: WM forgetting rate toward uniform prior in [0, 1]
    - age_wm_penalty: reduces WM mixture for older adults in [0, 1]
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_base, beta_set6_delta, wm_weight, wm_forget, age_wm_penalty = model_parameters
    # Base beta scaling
    softmax_beta_base *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    wm_weight_eff_global = wm_weight * (1.0 - age_wm_penalty * age_group)
    wm_weight_eff_global = max(0.0, min(1.0, wm_weight_eff_global))
    
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        is6 = 1 if nS == 6 else 0
        
        # Trial-level beta factor for this block size
        beta_multiplier = 1.0 + is6 * beta_set6_delta
        softmax_beta_block = softmax_beta_base * beta_multiplier
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy with set-size-dependent beta
            denom_rl = np.sum(np.exp(softmax_beta_block * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl
            
            # WM softmax policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / denom_wm
            
            # Compute RPE prior to updates for error-gating
            delta = r - Q_s[a]
            # Error-gating (only affects set size 6)
            g = is6 * abs(delta)
            if g < 0.0:
                g = 0.0
            if g > 1.0:
                g = 1.0
            p_wm = (1.0 - g) * p_wm_soft + g * (1.0 / nA)
            
            # Mixture
            p_total = wm_weight_eff_global * p_wm + (1.0 - wm_weight_eff_global) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update with forgetting toward uniform (stabilizes estimates)
            q[s, :] = (1.0 - wm_forget) * q[s, :] + wm_forget * (1.0 / nA)
            q[s, a] = q[s, a] + lr * delta
            
            # WM forgetting toward prior + one-shot store if rewarded
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Notes on set size and age dependence:
- Model 1:
  - WM decay depends on set size (wm_decay_3 vs wm_decay_6). Larger set size implies stronger decay/interference.
  - Older adults have reduced WM mixture weight via wm_weight_eff = wm_weight_base * (1 - 0.5 * age_group).
- Model 2:
  - WM availability p_avail scales as K_eff / nS; larger nS reduces the probability that memory is available.
  - Older adults have reduced effective capacity K_eff = K_base - age_k_drop * age_group.
  - RL uses asymmetric learning rates for feedback valence.
- Model 3:
  - RL inverse temperature is modulated by set size: beta increased or decreased in set size 6 via beta_set6_delta.
  - WM mixture is reduced for older adults via age_wm_penalty.
  - WM policy is diluted toward uniform in set size 6 when the absolute RPE is large (error-gated WM).