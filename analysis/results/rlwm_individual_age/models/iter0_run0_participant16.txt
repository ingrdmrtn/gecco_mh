def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with set-size- and age-modulated WM gating and decay.

    Idea:
    - Decisions are a weighted mixture of a model-free RL policy and a working-memory (WM) policy.
    - WM stores the most recent rewarded action for each state, but decays toward uniform.
    - The contribution of WM is reduced when set size is larger (load) and for older adults (age effect).

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values.
    - wm_weight_base: base mixture weight for WM before load/age scaling (0..1).
    - softmax_beta: inverse temperature for RL softmax; internally scaled up (multiplied by 10).
    - wm_decay: decay of WM contents toward uniform each trial (0..1).
    - age_wm_bias: reduction in WM mixture for older adults; effective wm_weight is
                   wm_weight = wm_weight_base * load_factor * (1 - age_wm_bias * age_group),
                   clipped to [0,1].
                   age_group = 0 if age<=45, 1 otherwise.
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, age_wm_bias = model_parameters
    softmax_beta *= 10.0

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # deterministic WM readout
    blocks_log_p = 0.0

    # small epsilon to avoid log(0)
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load factor: less WM reliance at larger set size
        # Scales between 1 (nS=3) and 0.5 (nS=6)
        load_factor = 3.0 / nS

        # Age-scaled, load-scaled WM mixture weight
        wm_weight_eff = wm_weight_base * load_factor * (1.0 - age_wm_bias * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax (stable via difference trick)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: softmax over current WM weights
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then if rewarded, store a as the remembered action
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Overwrite toward one-hot if rewarded (write-in strength equals wm_decay for simplicity)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL (asymmetric learning rates) + capacity-limited WM recall with set-size and age effects.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM supports near-deterministic choice when the state is within capacity K; otherwise recall is probabilistic.
    - WM recall probability is min(1, K / set_size), reduced by an age factor.
    - WM policy is a mixture of a deterministic WM readout and uniform noise, governed by recall probability.
    - Final action policy mixes WM and RL.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - K_capacity: WM capacity in number of state-action pairs (>=0).
    - wm_weight_base: base mixture weight for combining WM and RL (0..1).
    - softmax_beta: inverse temperature for RL softmax; internally scaled up (x10).
    - age_mem_drop: multiplicative drop in recall probability for older adults (0..1). Effective recall:
                    recall_prob = min(1, K/nS) * (1 - age_mem_drop*age_group).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, K_capacity, wm_weight_base, softmax_beta, age_mem_drop = model_parameters
    softmax_beta *= 10.0

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # used for WM readout softmax
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture is not directly capacity limited; capacity acts through recall_prob.
        wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)

        # Capacity-driven recall probability (reduced by age if older)
        recall_prob = min(1.0, max(0.0, K_capacity / max(1.0, nS)))
        recall_prob *= (1.0 - age_mem_drop * age_group)
        recall_prob = np.clip(recall_prob, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM softmax readout
            denom_wm_sf = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_sf = 1.0 / denom_wm_sf

            # Add recall lapses: with probability (1-recall_prob) choose uniformly
            p_wm = recall_prob * p_wm_sf + (1.0 - recall_prob) * (1.0 / nA)

            # Mixture of WM and RL
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rate
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: simple overwrite upon reward, mild decay otherwise
            # Use a small, set-size-sensitive decay so larger sets maintain less stable WM
            wm_decay = min(1.0, 3.0 / max(1.0, nS)) * 0.5  # 0.5 at nS=3, 0.25 at nS=6
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # stronger write when reward confirms mapping
                write_gain = 0.75
                w[s, :] = (1.0 - write_gain) * w[s, :] + write_gain * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with perseveration-as-WM, set-size driven lapses, and age-modulated lapses.

    Idea:
    - RL contributes value-driven choice as usual.
    - "WM" here tracks recent action tendencies per state (perseveration) and is read out deterministically.
    - Larger set sizes increase lapse rate; older adults also show higher lapse.
    - Final policy is a mixture of RL and perseveration-based WM, plus a small lapse to uniform choice.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: baseline influence of perseveration-based WM (0..1).
    - softmax_beta: inverse temperature for RL softmax; internally scaled up (x10).
    - pers_decay: decay of perseveration trace toward uniform (0..1).
    - lapse_base: baseline lapse probability to uniform (0..1); increases with set size.
    - age_lapse_boost: additional lapse increment for older adults (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, pers_decay, lapse_base, age_lapse_boost = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # w stores recent choice tendencies (perseveration) for each state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size increases lapses; cap at <1
        lapse = lapse_base * (nS / 3.0)
        lapse += age_lapse_boost * age_group
        lapse = np.clip(lapse, 0.0, 0.5)  # keep reasonable

        # WM mixture weight can also depend mildly on set size (less perseveration at large set)
        wm_weight_eff = wm_weight_base * (3.0 / nS)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: perseveration softmax (deterministic toward recent action)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Combine with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Perseveration WM update: decay toward uniform, then reinforce the chosen action
            w[s, :] = (1.0 - pers_decay) * w[s, :] + pers_decay * w_0[s, :]
            reinforce = 0.8  # strong tendency to repeat recent action
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - reinforce) * w[s, :] + reinforce * one_hot

        blocks_log_p += log_p

    return -blocks_log_p