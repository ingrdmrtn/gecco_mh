Below are three standalone cognitive models that combine reinforcement learning (RL) with a capacity-limited working memory (WM) system. Each model follows the provided template structure, returns the negative log-likelihood of the observed actions, and uses age group (0=young, 1=old) and set size (3 vs 6) in a meaningful way.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with decay: WM weight and decay depend on set size and age.
    
    Model idea:
    - Choices are a mixture of an RL softmax policy and a WM softmax policy.
    - WM stores recently rewarded stimulusâ€“action mappings, decays toward uniform.
    - WM influence is stronger in small set sizes and for younger participants; older group experiences stronger WM decay and reduced WM weight.
    - RL updates via delta rule.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline mixture weight for WM (0..1)
    - softmax_beta: RL inverse temperature (will be scaled by 10 for range)
    - wm_learn: WM learning rate toward one-hot when rewarded (0..1)
    - wm_decay_base: baseline WM decay toward uniform per trial (0..1)
    - lapse: lapse probability mixed with uniform choice (0..0.1 recommended)
    
    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (3 or 6)
    - age: array with a single repeated age value
    - model_parameters: list as above
    
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_learn, wm_decay_base, lapse = model_parameters
    softmax_beta *= 10.0  # expand range
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12
    
    # Age group: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])
            
            # RL policy for chosen action
            Q_s = q[s, :]
            # softmax probability of chosen action using numerically stable expression
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # WM policy for chosen action (softmax over W_s)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # Mixture weight depends on set size and age
            # Smaller set size -> higher WM weight; older age reduces WM contribution
            size_factor = 3.0 / set_size  # 1 for 3, 0.5 for 6
            age_factor = 1.0 - 0.3 * age_group  # young=1.0, old=0.7
            wm_weight_eff = np.clip(wm_weight_base * size_factor * age_factor, 0.0, 1.0)
            
            # Lapse: mix with uniform policy
            p_total = (1.0 - lapse) * (wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl) + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0 - eps)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM updating and decay
            # Age and set size modulate decay: larger set, older => more decay
            decay_factor = wm_decay_base * (set_size / 3.0) * (1.0 + 0.5 * age_group)  # increases with size and age
            decay_factor = np.clip(decay_factor, 0.0, 1.0)
            
            # Move W toward one-hot of rewarded action; otherwise just decay toward uniform
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            # Apply decay toward uniform
            w[s, :] = (1.0 - decay_factor) * w[s, :] + decay_factor * w_0[s, :]
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity gating: WM engages probabilistically depending on capacity K and set size.
    
    Model idea:
    - WM is used if the state is stored in WM; storage probability depends on a capacity K and set size (p_store = min(1, K_eff/set_size)).
    - K is reduced for older adults; WM influence also wanes with repeated exposures to the same state within a block (automatic 1/(1+c_s) down-weight).
    - RL runs in parallel with delta-rule learning.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM weight when engaged (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10)
    - K_cap: WM capacity parameter (e.g., 1..6)
    - wm_learn: WM learning rate toward one-hot on reward (0..1)
    - lapse: lapse probability toward uniform (0..0.1)
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays
    - model_parameters: list as above
    
    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, K_cap, wm_learn, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    
    age_group = 0 if age[0] <= 45 else 1
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Track per-state exposure count within block for down-weighting WM
        exposure_count = np.zeros(nS, dtype=int)
        # Track whether state is currently stored in WM
        stored = np.zeros(nS, dtype=bool)
        
        # Effective capacity lowered by age
        K_eff = max(0.0, K_cap * (1.0 - 0.3 * age_group))  # 30% reduction if older
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])
            
            # Storage probability for this state based on capacity and set size
            p_store = min(1.0, K_eff / float(set_size))
            # If first time seeing state, decide to store (stochastic gating)
            if exposure_count[s] == 0:
                # Use deterministic approximation of stochastic gating: store if p_store >= 0.5
                # (avoids randomness in likelihood; still captures capacity effect)
                stored[s] = (p_store >= 0.5)
            exposure_count[s] += 1
            
            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # WM engagement weight: only if stored. Decrease with exposures: 1/(1+c)
            wm_active = 1.0 if stored[s] else 0.0
            expo_factor = 1.0 / (1.0 + exposure_count[s])  # natural waning as RL takes over
            size_factor = 3.0 / set_size  # smaller sets favor WM
            wm_weight_eff = np.clip(wm_weight_base * wm_active * expo_factor * size_factor, 0.0, 1.0)
            
            p_total = (1.0 - lapse) * (wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl) + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0 - eps)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update: if stored, quickly learn rewarded mapping; otherwise no WM update
            if stored[s]:
                if r > 0:
                    target = np.zeros(nA)
                    target[a] = 1.0
                    w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
                else:
                    # mild decay toward uniform if no reward
                    w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM arbitration via RL uncertainty and perseveration bias.
    
    Model idea:
    - RL and WM policies are combined; WM weight increases when RL is uncertain (higher entropy).
    - WM strength is attenuated in larger set sizes and for older participants.
    - Includes an action perseveration bias added to RL logits to capture tendency to repeat last action.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10)
    - wm_learn: WM learning rate toward one-hot on reward (0..1)
    - persev: perseveration bias added to last-action logit (>=0)
    - lapse: lapse probability toward uniform (0..0.1)
    
    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, wm_learn, persev, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    nA = 3
    
    age_group = 0 if age[0] <= 45 else 1
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Track last action for perseveration (per state)
        last_action = -1 * np.ones(nS, dtype=int)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL logits with perseveration bias
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += persev
            rl_logits = softmax_beta * Q_s + bias
            rl_logits -= rl_logits[a]  # for stable chosen prob
            p_rl = 1.0 / np.sum(np.exp(rl_logits))
            
            # WM policy
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= wm_logits[a]
            p_wm = 1.0 / np.sum(np.exp(wm_logits))
            
            # Arbitration: WM weight scaled by RL uncertainty (entropy in [0, log nA])
            # Normalize entropy to [0,1] to gate WM: high entropy -> more WM.
            # First compute RL policy distribution for current state
            rl_full = np.exp(softmax_beta * Q_s)
            rl_full = rl_full / np.sum(rl_full)
            entropy = -np.sum(rl_full * (np.log(rl_full + eps)))
            entropy_norm = entropy / np.log(nA)
            
            size_factor = 3.0 / set_size  # smaller set -> more WM
            age_factor = 1.0 - 0.3 * age_group  # older -> less WM
            wm_weight_eff = np.clip(wm_weight_base * entropy_norm * size_factor * age_factor, 0.0, 1.0)
            
            p_total = (1.0 - lapse) * (wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl) + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0 - eps)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update: fast one-hot toward rewarded action; mild decay otherwise
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            
            # Update perseveration memory
            last_action[s] = a
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Parameter notes on set size and age:
- Model 1: wm_weight_eff scales as wm_weight_base * (3/set_size) * (1 - 0.3*age_group). WM decay increases with set size and age.
- Model 2: WM capacity K is reduced by 30% for older participants; p_store = min(1, K_eff/set_size). WM weight also declines with exposures to a given state and is stronger in small set sizes.
- Model 3: Arbitration uses RL entropy; WM weight scales with entropy, reduced by larger set sizes and by age (1 - 0.3*age_group). Perseveration bias affects RL logits.