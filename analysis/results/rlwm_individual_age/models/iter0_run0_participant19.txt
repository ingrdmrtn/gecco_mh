Below are three standalone cognitive models (RL+WM variants) tailored to the RLWM task. Each function:

- Follows the provided template structure per block.
- Uses all parameters meaningfully (â‰¤ 6 per model).
- Incorporates set size (3 vs 6) and age group (young=0, old=1) effects.
- Returns the negative log-likelihood of the observed choices.

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited, decaying working memory.
    - RL policy: softmax over Q-values with inverse temperature beta.
    - WM policy: stores rewarded state-action pairs with probability constrained by a capacity K.
      Memory strength decays over time; WM policy is near-deterministic (high beta).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
    - Set-size effect: probability of encoding a state into WM is scaled by K / nS (capped at 1).
    - Age effect: young participants have an additive boost to effective capacity (age_wm_shift > 0).

    Parameters:
    - model_parameters = [lr, wm_weight_base, softmax_beta, wm_capacity_K, wm_decay, age_wm_shift]
      lr: RL learning rate in [0,1]
      wm_weight_base: base weight of WM in mixture in [0,1]
      softmax_beta: base RL inverse temperature (rescaled internally by *10)
      wm_capacity_K: WM capacity (in "slots"), positive real
      wm_decay: per-trial decay of memory strength in [0,1]
      age_wm_shift: capacity shift added for young (=0) and subtracted for old (=1); effective K' = K + (1 - 2*age_group)*age_wm_shift

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_capacity_K, wm_decay, age_wm_shift = model_parameters
    softmax_beta *= 10.0  # higher dynamic range as per template
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    # Age group: 0=young (<=45), 1=old (>45)
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM preference matrix (row is a distribution over actions)
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # baseline/uniform
        # Memory strength per state (0..1), indicates how strong WM is for that state
        m = np.zeros(nS)

        # Age-adjusted effective capacity
        # Young gets +age_wm_shift, old gets -age_wm_shift
        K_eff = max(0.0, wm_capacity_K + (1 - 2 * age_group) * age_wm_shift)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy: probability of chosen action via softmax trick
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # Current WM distribution for state s is mixture of stored one-hot and uniform, weighted by memory strength m[s].
            W_s = m[s] * w[s, :] + (1.0 - m[s]) * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight scales with set size via encoding availability ~ min(1, K_eff/nS)
            enc_availability = min(1.0, K_eff / float(nS)) if nS > 0 else 0.0
            wm_weight_eff = np.clip(wm_weight_base * enc_availability, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Decay memory strength for the current state
            m[s] = (1.0 - wm_decay) * m[s]

            # - Encode if rewarded with probability enc_availability; if not rewarded, do nothing
            #   We incorporate this probabilistically in expectation by setting the target distribution.
            #   When rewarded, set one-hot on the chosen action; otherwise keep current.
            if r > 0:
                # Deterministic update of WM content to the rewarded action; strength set to enc_availability plus the residual after decay
                # m[s] moves toward 1 with step enc_availability
                m[s] = np.clip(m[s] + enc_availability * (1.0 - m[s]), 0.0, 1.0)
                new_pref = np.zeros(nA)
                new_pref[a] = 1.0
                w[s, :] = new_pref

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with perseveration and set-size dependent WM decay.
    - RL softmax with beta, plus a perseveration bias favoring the last action in the block.
    - WM stores rewarded actions; memory strength decays faster in larger set size (nS=6).
    - WM policy near-deterministic; mixture weight is reduced when WM memory is weak (state-specific).
    - Age effect: young participants have higher decision precision (beta increased), old reduced.

    Parameters:
    - model_parameters = [lr, wm_weight_base, softmax_beta, perseveration_bias, wm_decay_size6, age_beta_shift]
      lr: RL learning rate in [0,1]
      wm_weight_base: base mixture weight for WM in [0,1]
      softmax_beta: base RL inverse temperature (rescaled internally by *10)
      perseveration_bias: added value to the last action (>=0)
      wm_decay_size6: WM decay per trial when set size is 6, in [0,1]
      age_beta_shift: scales beta by (1 + age_beta_shift) if young, (1 - age_beta_shift) if old

    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, perseveration_bias, wm_decay_size6, age_beta_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    # Age scales beta multiplicatively: young > precise, old < precise
    beta_age_scale = (1.0 + age_beta_shift) if age_group == 0 else max(0.0, 1.0 - age_beta_shift)
    softmax_beta *= beta_age_scale

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        m = np.zeros(nS)  # memory strength per state

        # Set-size dependent WM decay: faster decay for larger sets
        # Use provided parameter for size 6; for size 3, assume half as fast decay
        wm_decay = wm_decay_size6 if nS == 6 else 0.5 * wm_decay_size6

        last_action = None
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            # Add perseveration bonus to last action (state-independent)
            if last_action is not None:
                Q_s[last_action] += perseveration_bias

            # RL policy prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM distribution (state-specific), include perseveration bias as a shift as well
            W_s = m[s] * w[s, :] + (1.0 - m[s]) * w_0[s, :]
            if last_action is not None:
                W_s_bias = W_s.copy()
                W_s_bias[last_action] += perseveration_bias
            else:
                W_s_bias = W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_bias - W_s_bias[a])))

            # WM weight scaled by local memory strength
            wm_weight_eff = np.clip(wm_weight_base * m[s], 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay then, if rewarded, store deterministic one-hot and boost strength
            m[s] = (1.0 - wm_decay) * m[s]
            if r > 0:
                m[s] = np.clip(m[s] + (1.0 - m[s]), 0.0, 1.0)  # snap toward 1 when rewarded
                new_pref = np.zeros(nA)
                new_pref[a] = 1.0
                w[s, :] = new_pref

            # Update perseveration memory
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM with interference-based recall.
    - RL: separate learning rates for positive and negative prediction errors.
    - WM: caches last rewarded action per state; recall is probabilistic and degrades with set size.
      p_recall = 1 - interference * ((nS - 3) / 3). When recall fails, WM acts uniform.
      WM policy is near-deterministic when recalled.
    - Mixture: p_total = wm_weight * [p_recall*p_wm + (1 - p_recall)*p_uniform] + (1 - wm_weight)*p_rl
      where p_uniform = 1/3 for the chosen action in WM channel.
    - Age effect: young participants learn faster (both lr_pos and lr_neg scaled up), old scaled down.

    Parameters:
    - model_parameters = [lr_pos, lr_neg, wm_weight, softmax_beta, interference, age_lr_shift]
      lr_pos: RL learning rate for positive PE in [0,1]
      lr_neg: RL learning rate for negative PE in [0,1]
      wm_weight: mixture weight for WM in [0,1]
      softmax_beta: RL inverse temperature (rescaled internally by *10)
      interference: recall interference parameter in [0,1]; larger => worse recall, stronger set-size drop
      age_lr_shift: scales both learning rates by (1 + age_lr_shift) if young, (1 - age_lr_shift) if old

    Returns:
    - Negative log-likelihood.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, interference, age_lr_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    # Age scaling of learning rates
    lr_scale = (1.0 + age_lr_shift) if age_group == 0 else max(0.0, 1.0 - age_lr_shift)
    lr_pos_eff = np.clip(lr_pos * lr_scale, 0.0, 1.0)
    lr_neg_eff = np.clip(lr_neg * lr_scale, 0.0, 1.0)

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: for each state, store last rewarded action index or -1 if none
        wm_store = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL choice probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM recall probability degrades with set size
            size_factor = max(0.0, (nS - 3) / 3.0)  # 0 for 3, 1 for 6
            p_recall = np.clip(1.0 - interference * size_factor, 0.0, 1.0)

            # WM policy:
            # If we have a stored action for s, then when recall succeeds WM is near-deterministic toward that action.
            if wm_store[s] >= 0:
                # Deterministic WM prob for chosen action given stored action a*
                a_star = wm_store[s]
                # Softmax probability of chosen action under a near-delta distribution at a_star
                # Equivalent to: W_s one-hot at a_star
                W_s = np.zeros(nA)
                W_s[a_star] = 1.0
                p_wm_recalled = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                # No memory stored; fallback is uniform for WM channel
                p_wm_recalled = 1.0 / nA  # identical to uniform case

            p_wm_uniform = 1.0 / nA
            p_wm = p_recall * p_wm_recalled + (1.0 - p_recall) * p_wm_uniform

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos_eff * pe
            else:
                q[s, a] += lr_neg_eff * pe

            # WM update: store action if rewarded; keep previously stored otherwise
            if r > 0:
                wm_store[s] = a

        blocks_log_p += log_p

    return -blocks_log_p