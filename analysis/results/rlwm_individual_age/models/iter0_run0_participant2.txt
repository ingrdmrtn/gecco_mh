def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and set-size/age-dependent arbitration.

    Idea:
    - RL system learns action values via delta-rule.
    - WM system stores the last rewarded action per state (nearly deterministic retrieval) with decay.
    - Arbitration weight for WM decreases with larger set size and with older age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM arbitration weight (unconstrained real; passed through sigmoid)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_decay: decay rate of WM content toward uniform each trial (0..1)
    - size_beta: slope of set-size effect on WM arbitration (positive -> lower WM weight at larger set size)
    - age_beta: additive bias on WM arbitration for older age group (positive -> lower WM weight in older)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, size_beta, age_beta = model_parameters
    softmax_beta *= 10.0

    # age group: 0 = young, 1 = old
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM contents
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY policy: softmax with high beta over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight depends on set size and age
            # Larger set sizes reduce WM weight; older age reduces WM weight
            nS_current = int(block_set_sizes[t])
            wm_logit = wm_weight_base + size_beta * (3 - nS_current) + age_beta * age_group
            wm_weight_eff = sigmoid(wm_logit)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)  # guard against log(0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WORKING MEMORY update:
            # 1) decay toward uniform each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) if rewarded, store the chosen action deterministically
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + Win-Stay Working Memory heuristic + lapse that grows with set size and age.

    Idea:
    - RL uses separate learning rates for positive and negative outcomes.
    - WM implements a win-stay heuristic: if the last encounter of a state was rewarded for an action,
      WM points to that action (deterministic), otherwise uniform.
    - Mixture of WM and RL, then mixed with a lapse process that favors random choice.
    - Lapse probability increases with set size and with age.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - wm_weight: weight on WM vs RL (0..1 after sigmoid transform to ensure valid mixing)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - lapse_base: baseline lapse logit (unconstrained; passed through sigmoid)
    - lapse_size_age_slope: slope applied to (set_size - 3 + age_group) to grow lapse with load/age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, lapse_base, lapse_size_age_slope = model_parameters
    softmax_beta *= 10.0

    # age group: 0 = young, 1 = old
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic for win-stay representation
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    wm_weight_eff = sigmoid(wm_weight_base)

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize Q-values and a WM store that will encode "win-stay"
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will be set to one-hot on rewarded action, else uniform
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: win-stay representation -> near-deterministic softmax on W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Lapse probability depends on set size and age: epsilon on uniform
            nS_current = int(block_set_sizes[t])
            lapse_logit = lapse_base + lapse_size_age_slope * ((nS_current - 3) + age_group)
            epsilon = sigmoid(lapse_logit)
            p_total = (1.0 - epsilon) * p_total + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update for win-stay: if rewarded, store that action; else revert to uniform
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration (statewise, trial-by-trial) and WM decay; age reduces WM influence.

    Idea:
    - RL learns action values via delta-rule.
    - WM stores last rewarded action with decay.
    - Arbitration weight is dynamic: increases when WM is confident and RL is uncertain.
      WM confidence is the peakiness of its distribution; RL uncertainty is low value spread.
    - Age reduces WM arbitration; set size implicitly reduces WM confidence through decay and less consistent storage.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base bias for WM arbitration (unconstrained; passed through sigmoid)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_decay: decay rate for WM toward uniform (0..1)
    - arb_slope: scales the influence of (WM confidence - RL confidence) on arbitration
    - age_bias: subtractive bias applied for older adults on WM arbitration (>=0 reduces WM weight when old)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, arb_slope, age_bias = model_parameters
    softmax_beta *= 10.0

    # age group: 0 = young, 1 = old
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute dynamic arbitration:
            # WM confidence: deviation from uniform (0..1), here as (max - mean)
            wm_conf = np.max(W_s) - np.mean(W_s)
            # RL confidence proxy: spread of Q relative to its mean (max - mean)
            rl_conf = np.max(Q_s) - np.mean(Q_s)
            conf_diff = wm_conf - rl_conf

            # Base WM bias (sigmoid), adjusted by confidence and age penalty for older group
            wm_logit = wm_weight_base + arb_slope * conf_diff - age_bias * age_group
            wm_weight_eff = sigmoid(wm_logit)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform; if rewarded, store deterministically
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p