def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture model with decay.
    
    The model arbitrates between a model-free RL controller and a fast WM controller.
    WM is capacity-limited and decays toward an uninformative prior; its contribution
    to choice is down-weighted as set size increases, and also down-weighted for the
    older age group.

    Parameters
    ----------
    states : array-like of int
        State identifier on each trial (0-indexed within a block).
    actions : array-like of int
        Chosen action on each trial; actions are 0..2 (three options).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block at each trial.
    age : array-like or scalar
        Participant age (if array, it repeats the same value). Used to code age group.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, wm_capacity]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline weight of WM policy in mixture (0..1)
        - softmax_beta: RL inverse temperature; internally scaled by 10 for range
        - wm_decay: per-trial decay of WM toward uniform (0..1)
        - wm_capacity: effective WM capacity (around 3..6); larger values reduce
                       the set-size penalty on WM
        
    Age and set-size effects
    ------------------------
    - Age group is coded 0 for young (<=45), 1 for older (>45). WM weight is 
      multiplicatively reduced by 30% for older adults.
    - WM weight decreases with larger set size using a smooth capacity function:
      wm_weight = wm_weight_base * 1 / (1 + exp((nS - wm_capacity)))
      so when nS > capacity, WM contribution drops.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available per instructions; if already imported, this is a no-op

    lr, wm_weight_base, softmax_beta, wm_decay, wm_capacity = model_parameters
    softmax_beta = softmax_beta * 10.0  # expand dynamic range

    # Age group (0 = young, 1 = old)
    if np.isscalar(age):
        age_group = 0 if age <= 45 else 1
    else:
        age_group = 0 if age[0] <= 45 else 1

    # Deterministic WM readout (high beta)
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and WM "maps" per state
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Age factor: older adults show less WM contribution
        age_factor = 0.7 if age_group == 1 else 1.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Qs = q[s, :]
            Ws = w[s, :]

            # RL policy probability of chosen action
            logits_rl = softmax_beta * Qs
            logits_rl = logits_rl - np.max(logits_rl)  # stabilize
            prl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM policy probability of chosen action
            logits_wm = softmax_beta_wm * Ws
            logits_wm = logits_wm - np.max(logits_wm)
            pwm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # Set-size dependent WM weight with capacity and age effects
            # sigmoid( -(nS - wm_capacity) ) == 1/(1+exp(nS - wm_capacity))
            size_factor = 1.0 / (1.0 + np.exp(nS - wm_capacity))
            wm_weight = np.clip(wm_weight_base * size_factor * age_factor, 0.0, 1.0)

            p_total = wm_weight * pwm + (1.0 - wm_weight) * prl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Qs[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]

            # WM one-shot write on reward: store the rewarded action deterministically
            if r > 0.5:
                w[s, :] = (1e-12) * np.ones(nA)
                w[s, a] = 1.0
                w[s, :] = w[s, :] / np.sum(w[s, :])

        total_log_p += log_p

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with perseveration and set-size/age-dependent lapses.

    The controller blends:
    - An RL softmax with action stickiness (perseveration) that is state-specific.
    - A WM softmax that rapidly encodes rewarded actions and weakly suppresses unrewarded ones.
    A global lapse epsilon increases with set size and age group, mixing in uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial/block.
    age : array-like or scalar
        Participant age. Age group affects lapses (older -> larger lapses).
    model_parameters : list or array
        [lr, beta_base, wm_weight_base, epsilon_base, persev]
        - lr: learning rate used for RL and WM (symmetric for Â±PE)
        - beta_base: base inverse temperature for RL; internally scaled by 10
        - wm_weight_base: baseline WM mixing weight
        - epsilon_base: base lapse rate; will be scaled by set size and age
        - persev: action stickiness added to the last action taken in the same state

    Age and set-size effects
    ------------------------
    - Lapse epsilon = epsilon_base * (nS / 3) * (1 + 0.5 * age_group).
      Thus, larger set size and older age both increase lapses.
    - WM weight is reduced as set size increases: wm_weight = wm_weight_base * (3 / nS).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available

    lr, beta_base, wm_weight_base, epsilon_base, persev = model_parameters
    beta = beta_base * 10.0

    # Age group
    if np.isscalar(age):
        age_group = 0 if age <= 45 else 1
    else:
        age_group = 0 if age[0] <= 45 else 1

    beta_wm = 50.0
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Qs = q[s, :]
            Ws = w[s, :]

            # RL with state-specific perseveration bias
            stick = np.zeros(nA)
            if last_action[s] >= 0:
                stick[last_action[s]] = persev
            logits_rl = beta * Qs + stick
            logits_rl = logits_rl - np.max(logits_rl)
            prl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM softmax
            logits_wm = beta_wm * Ws
            logits_wm = logits_wm - np.max(logits_wm)
            pwm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # Mixture weight decreases with set size
            wm_weight = np.clip(wm_weight_base * (3.0 / nS), 0.0, 1.0)

            # Lapse grows with set size and age
            eps = epsilon_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
            eps = np.clip(eps, 0.0, 0.99)

            p_mix = wm_weight * pwm + (1.0 - wm_weight) * prl
            p_total = (1.0 - eps) * p_mix + eps * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Qs[a]
            q[s, a] += lr * delta

            # WM update: move toward chosen action if rewarded, away if not
            # First, small leak toward uniform to limit interference
            w[s, :] = 0.95 * w[s, :] + 0.05 * w0[s, :]
            target = np.zeros(nA)
            if r > 0.5:
                target[a] = 1.0
            else:
                # on error, push chosen action down a bit and renormalize
                target[:] = w0[s, :]
            w[s, :] = (1 - lr) * w[s, :] + lr * target
            # ensure proper normalization
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update last action for perseveration
            last_action[s] = a

        total_log_p += log_p

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted arbitration: RL with decay + WM with its own learning rate.
    
    The arbitration weight for WM depends on:
    - An uncertainty/confidence signal derived from WM for the current state
      (spread of WM values).
    - Set size (larger set size reduces WM weight).
    - Age group (older group further reduces WM influence).

    RL includes value decay toward an uninformative prior to capture interference across
    larger sets.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial/block.
    age : array-like or scalar
        Participant age (used to set age group: 0 young, 1 old).
    model_parameters : list or array
        [lr, beta, wm_weight_base, alpha_wm, rl_decay, setsize_slope]
        - lr: RL learning rate
        - beta: RL inverse temperature; internally scaled by 10
        - wm_weight_base: baseline WM arbitration weight
        - alpha_wm: WM learning rate (separate from RL)
        - rl_decay: per-trial decay of RL toward uniform prior (0..1)
        - setsize_slope: slope controlling how strongly set size reduces WM weight
                         via a logistic transform

    Age and set-size effects
    ------------------------
    - WM weight factor = sigmoid(setsize_slope * (4.5 - nS) - 0.5 * age_group).
      Thus, nS=3 increases WM weight relative to nS=6; older group shifts toward RL.
    - Arbitration also scales with WM confidence in the state: conf = max(Ws) - min(Ws).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available

    lr, beta, wm_weight_base, alpha_wm, rl_decay, setsize_slope = model_parameters
    beta = beta * 10.0
    beta_wm = 50.0

    # Age group
    if np.isscalar(age):
        age_group = 0 if age <= 45 else 1
    else:
        age_group = 0 if age[0] <= 45 else 1

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL decay toward uniform prior each trial to capture interference
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA) * np.ones_like(q)

            Qs = q[s, :]
            Ws = w[s, :]

            # RL policy
            logits_rl = beta * Qs
            logits_rl = logits_rl - np.max(logits_rl)
            prl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM policy
            logits_wm = beta_wm * Ws
            logits_wm = logits_wm - np.max(logits_wm)
            pwm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # WM confidence for current state (0..1)
            conf = float(np.max(Ws) - np.min(Ws))
            # Set-size and age factor via logistic transform
            size_age_term = setsize_slope * (4.5 - nS) - 0.5 * age_group
            size_age_factor = 1.0 / (1.0 + np.exp(-size_age_term))

            wm_weight = np.clip(wm_weight_base * size_age_factor * conf, 0.0, 1.0)

            p_total = wm_weight * pwm + (1.0 - wm_weight) * prl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Qs[a]
            q[s, a] += lr * delta

            # WM update (supervised-like)
            # Move WM toward a one-hot for the chosen action on reward; otherwise mild drift to uniform
            target = (1e-12) * np.ones(nA)
            if r > 0.5:
                target[a] = 1.0
            else:
                target = w0[s, :]

            w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        total_log_p += log_p

    return -float(total_log_p)