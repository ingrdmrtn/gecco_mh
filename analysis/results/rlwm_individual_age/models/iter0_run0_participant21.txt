Below are three standalone cognitive models (RL+WM variants) tailored to the rlwm task. Each function follows the requested signature and returns the negative log-likelihood of the observed choices. All parameters are used and limited to six per model. Age group (young=0, old=1) and set size both modulate behavior as specified in each docstring.

Note: These functions assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with WM decay, set-size modulation of WM reliance, and age penalty on WM.
    
    Components:
    - RL: tabular Q-learning with softmax action selection.
    - WM: one-shot storage of rewarded action per state with decay back to uniform.
    - Policy: mixture of WM and RL, with a lapse to uniform.
    
    Parameters (list of 6):
    - model_parameters[0] = lr in [0,1]: RL learning rate (applied to prediction error).
    - model_parameters[1] = wm_w0 (real): baseline WM reliance before transforms.
    - model_parameters[2] = wm_ss_slope (real): set-size slope; larger set size reduces WM reliance (negative slope) or increases (positive slope).
    - model_parameters[3] = beta (real >= 0): RL inverse temperature (scaled by 10 inside).
    - model_parameters[4] = wm_decay in [0,1]: decay of WM toward uniform each time the state is visited.
    - model_parameters[5] = lapse in [0,1]: stimulus-independent lapse to uniform policy.
    
    Age usage:
    - Older adults (age_group=1) have a 30% reduction in WM reliance (multiplicative attenuation).
    
    Set-size usage:
    - WM reliance is transformed by a logistic of wm_w0 + wm_ss_slope*(reference - set_size), with reference=3.
      Thus, larger set sizes decrease WM reliance when wm_ss_slope > 0 (or increase if negative).
    """
    eps = 1e-12
    lr, wm_w0, wm_ss_slope, beta, wm_decay, lapse = model_parameters
    # constrain to valid ranges
    lr = 1 / (1 + np.exp(-lr))
    wm_decay = 1 / (1 + np.exp(-wm_decay))
    lapse = 1 / (1 + np.exp(-lapse))
    softmax_beta = abs(beta) * 10.0  # RL inverse temperature
    softmax_beta_wm = 50.0           # highly deterministic WM policy
    
    # Age group
    age_group = 0 if age[0] <= 45 else 1
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        
        # WM reliance for this block, modulated by set size and age
        # Reference set size = 3
        wm_logit = wm_w0 + wm_ss_slope * (3 - nS)
        wm_weight_block = 1 / (1 + np.exp(-wm_logit))
        if age_group == 1:
            wm_weight_block *= 0.7  # 30% reduction in WM reliance for older adults
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            
            # RL policy probability for chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            
            # WM policy probability for chosen action
            # Current WM row softmax
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)
            
            # Mixture + lapse
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] = q[s, a] + lr * delta
            
            # WM decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            # If rewarded, store the action as a one-hot in WM (overwrite to encode correct mapping)
            if r == 1:
                w[s, :] = (1.0 / nA) * np.zeros(nA)
                w[s, a] = 1.0
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited WM access and perseveration bias in RL.
    
    Components:
    - RL: tabular Q-learning with softmax; includes a perseveration bias that adds kappa to the last action taken in the same state.
    - WM: one-shot storage of the last rewarded action per state; WM is accessed with probability proportional to capacity K and set size nS.
    - Policy: mixture of WM and RL weighted by the WM access probability; plus lapse.
    
    Parameters (list of 6):
    - model_parameters[0] = lr (real): RL learning rate (sigmoid-bounded to [0,1]); reduced for older adults multiplicatively.
    - model_parameters[1] = beta0 (real): base RL inverse temperature (scaled by 10 inside).
    - model_parameters[2] = beta_ss_slope (real): how set size modulates beta; beta_eff = |(beta0 + beta_ss_slope*(3 - nS))|*10.
    - model_parameters[3] = K_cap (real >= 0): WM capacity proxy (effective items); WM access prob = min(1, K_cap/nS), reduced by 20% for older adults.
    - model_parameters[4] = kappa (real): perseveration bias added to the Q-value of the last action chosen in the same state.
    - model_parameters[5] = lapse in [0,1]: lapse probability to uniform.
    
    Age usage:
    - LR attenuated by 30% for older adults.
    - WM access probability reduced by 20% for older adults.
    
    Set-size usage:
    - Inverse temperature decreases as set size increases if beta_ss_slope > 0 (or increases if negative).
    - WM access probability scales as min(1, K_cap/nS).
    """
    eps = 1e-12
    lr, beta0, beta_ss_slope, K_cap, kappa, lapse = model_parameters
    lr = 1 / (1 + np.exp(-lr))
    lapse = 1 / (1 + np.exp(-lapse))
    
    age_group = 0 if age[0] <= 45 else 1
    if age_group == 1:
        lr *= 0.7  # 30% reduction for older adults
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM memory: -1 if unknown, otherwise stores the last rewarded action
        mem_action = -1 * np.ones(nS, dtype=int)
        # Last action per state for perseveration
        last_action_state = -1 * np.ones(nS, dtype=int)
        
        # Effective beta for the block, modulated by set size
        beta_eff = abs(beta0 + beta_ss_slope * (3 - nS)) * 10.0
        
        # WM access probability based on capacity and set size
        wm_access = min(1.0, max(0.0, K_cap) / max(1, nS))
        if age_group == 1:
            wm_access *= 0.8  # reduce WM access by 20% for older adults
        wm_access = np.clip(wm_access, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            
            # RL softmax with perseveration: add kappa to last action for this state (if exists)
            Qb = q[s, :].copy()
            if last_action_state[s] != -1:
                Qb[last_action_state[s]] += kappa
            denom_rl = np.sum(np.exp(beta_eff * (Qb - Qb[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            
            # WM policy: if memory exists for this state, pick stored action deterministically
            if mem_action[s] != -1:
                # Construct a WM row favoring the memorized action
                W = np.zeros(nA)
                W[mem_action[s]] = 1.0
                denom_wm = np.sum(np.exp(50.0 * (W - W[a])))
                p_wm = 1.0 / max(denom_wm, eps)
            else:
                p_wm = 1.0 / nA  # no memory, uniform
            
            # Mixture and lapse
            p_mix = wm_access * p_wm + (1.0 - wm_access) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))
            
            # RL update
            delta = r - q[s, a]
            q[s, a] = q[s, a] + lr * delta
            
            # Update memory on reward: store the action as correct mapping
            if r == 1:
                mem_action[s] = a
            # update last action for perseveration
            last_action_state[s] = a
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and set-size/age-modulated WM reliance.
    
    Components:
    - RL: tabular Q-learning with separate positive and negative learning rates; softmax action selection.
    - WM: decaying memory toward uniform; on positive feedback, WM stores the action deterministically; on negative feedback, WM clears the state (resets to uniform).
    - Policy: mixture of WM and RL with lapse; WM reliance depends on set size and age.
    
    Parameters (list of 6):
    - model_parameters[0] = alpha_pos in [0,1]: RL learning rate for positive PE (rewarded choices).
    - model_parameters[1] = alpha_neg in [0,1]: RL learning rate for negative PE (unrewarded choices); older adults get 20% reduction.
    - model_parameters[2] = beta (real >= 0): RL inverse temperature (scaled by 10 inside).
    - model_parameters[3] = wm_w0 (real): baseline WM reliance (pre-logistic).
    - model_parameters[4] = wm_ss_slope (real): set-size slope on WM reliance; larger set size reduces reliance if slope > 0.
    - model_parameters[5] = wm_decay in [0,1]: WM decay to uniform when the state is visited.
    
    Age usage:
    - Older adults (age_group=1) have reduced alpha_neg (multiplied by 0.8).
    - WM reliance also attenuated multiplicatively by 25% for older adults.
    
    Set-size usage:
    - WM reliance = sigmoid(wm_w0 + wm_ss_slope*(3 - nS)).
    """
    eps = 1e-12
    alpha_pos, alpha_neg, beta, wm_w0, wm_ss_slope, wm_decay = model_parameters
    alpha_pos = 1 / (1 + np.exp(-alpha_pos))
    alpha_neg = 1 / (1 + np.exp(-alpha_neg))
    wm_decay = 1 / (1 + np.exp(-wm_decay))
    softmax_beta = abs(beta) * 10.0
    softmax_beta_wm = 50.0
    
    age_group = 0 if age[0] <= 45 else 1
    if age_group == 1:
        alpha_neg *= 0.8  # reduced negative learning
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        
        # WM reliance with set size and age
        wm_logit = wm_w0 + wm_ss_slope * (3 - nS)
        wm_weight_block = 1 / (1 + np.exp(-wm_logit))
        if age_group == 1:
            wm_weight_block *= 0.75
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            
            # RL policy probability
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            
            # WM policy probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)
            
            # Mixture (no explicit lapse parameter to keep params â‰¤ 6 here)
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))
            
            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] = q[s, a] + alpha_pos * pe
            else:
                q[s, a] = q[s, a] + alpha_neg * pe
            
            # WM update with decay on visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            if r == 1:
                # store correct action deterministically
                w[s, :] = (1.0 / nA) * np.zeros(nA)
                w[s, a] = 1.0
            else:
                # on negative feedback, clear WM for this state (reset to uniform)
                w[s, :] = w0[s, :].copy()
        
        blocks_log_p += log_p
    
    return -blocks_log_p