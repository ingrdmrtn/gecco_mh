Below are three standalone cognitive models that implement RL + working-memory (WM) mechanisms with age- and set-size-dependent modulations. Each function returns the negative log-likelihood of the observed choices.

Notes:
- Assume numpy as np is already imported.
- Age group: 0 = young (<=45), 1 = old (>45).
- All parameters are used meaningfully and do not exceed 6 per model.
- The WM policy uses a high inverse temperature (softmax_beta_wm = 50) to approximate nearly deterministic retrieval when WM is reliable.
- To ensure numerical stability, probabilities are clipped away from 0.

Model 1: RL + WM mixture with age- and set-size-dependent WM gating and WM decay
- Idea: WM contribution decreases with larger set sizes and with older age. WM contents decay toward uniform. RL uses a single learning rate.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and set-size-dependent WM gating and WM decay.

    Parameters
    ----------
    states : array-like, int
        Stimulus/state index per trial, 0..(set_size-1).
    actions : array-like, int
        Chosen action per trial, 0..2 (3 actions).
    rewards : array-like, float
        Observed reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial (states reset across blocks).
    set_sizes : array-like, int
        Set size on each trial for the corresponding block (3 or 6).
    age : array-like, int
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, wm_setsize_cost, wm_age_cost]
        - lr: RL learning rate in [0,1].
        - wm_weight_base: baseline WM weight in [0,1].
        - softmax_beta: inverse temperature for RL choice; internally scaled x10.
        - wm_decay: WM decay toward uniform per trial in [0,1].
        - wm_setsize_cost: penalty on WM weight per extra items beyond 3.
        - wm_age_cost: additional WM penalty for older group (age_group=1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_setsize_cost, wm_age_cost = model_parameters
    softmax_beta *= 10.0  # increase RL inverse temperature scale
    
    # Age group coding
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0  # nearly deterministic WM policy
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])
        
        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform attractor for decay
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # Effective WM weight decreases with larger set sizes and older age
            # logistic transform to keep in [0,1]
            # logit(base) - penalties
            base = np.clip(wm_weight_base, 1e-6, 1-1e-6)
            logit_base = np.log(base) - np.log(1 - base)
            setsize_pen = wm_setsize_cost * max(0, nS - 3)  # extra items beyond 3
            age_pen = wm_age_cost * age_group
            logit_eff = logit_base - setsize_pen - age_pen
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_eff))
            
            # RL policy: probability of chosen action via softmax
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            
            # WM policy: high-beta softmax over WM values
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)
            
            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM decay toward uniform then overwrite chosen action toward the observed outcome
            # First decay the entire row toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Then set chosen action closer to the observed outcome
            # Here we perform a "replacement"-like update (one-shot)
            w[s, a] = r  # deterministic encoding of the most recent outcome for chosen action
            
        blocks_log_p += log_p

    return -float(blocks_log_p)


Model 2: RL + capacity-limited WM with age-dependent capacity and choice stickiness
- Idea: WM contribution is scaled by an effective capacity K that declines with age. When set size > K, WM weight is reduced proportional to K/set_size. Additionally, include a perseveration (stickiness) bias toward repeating the previous action, which impacts the RL policy.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age-dependent capacity and choice stickiness.

    Parameters
    ----------
    states : array-like, int
        Stimulus/state index per trial, 0..(set_size-1).
    actions : array-like, int
        Chosen action per trial, 0..2 (3 actions).
    rewards : array-like, float
        Observed reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial (states reset across blocks).
    set_sizes : array-like, int
        Set size on each trial for the corresponding block (3 or 6).
    age : array-like, int
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, K_base, k_age, stickiness]
        - lr: RL learning rate in [0,1].
        - wm_weight: base WM mixture weight in [0,1].
        - softmax_beta: inverse temperature for RL choice; internally scaled x10.
        - K_base: baseline WM capacity (in items).
        - k_age: capacity decrement for older group (subtract if age_group=1).
        - stickiness: choice perseveration weight added to the last chosen action.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, K_base, k_age, stickiness = model_parameters
    softmax_beta *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Effective capacity considering age
        K_eff = K_base - k_age * age_group
        K_eff = max(0.0, K_eff)  # capacity cannot be negative
        
        prev_action = None  # for stickiness
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # Effective WM weight reduced when set size exceeds capacity
            cap_scale = 1.0
            if nS > 0:
                cap_scale = min(1.0, max(0.0, K_eff) / float(nS))
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0) * cap_scale
            
            # RL policy with stickiness bias
            Q_s = q[s, :].copy()
            if prev_action is not None and 0 <= prev_action < nA:
                Q_s[prev_action] += stickiness  # add perseveration bias toward repeating the last choice
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            
            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            
            # WM update: overwrite chosen action with observed outcome; others unchanged
            w[s, a] = r
            
            prev_action = a
        
        blocks_log_p += log_p
    
    return -float(blocks_log_p)


Model 3: RL with asymmetric learning rates and WM mixture; set-size and age reduce exploration (increase beta)
- Idea: RL uses separate learning rates for positive and negative outcomes. WM mixes in, with weight independent of set size, but decision noise (beta) increases with smaller set sizes and in younger participants (better precision). We model this by scaling beta upward when set size is small (3) and downward when large (6), and further increasing beta for younger participants (age_group=0). This yields more deterministic choices when WM load is low and for younger adults.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates, WM mixture, and beta modulated by set-size and age.

    Parameters
    ----------
    states : array-like, int
        Stimulus/state index per trial, 0..(set_size-1).
    actions : array-like, int
        Chosen action per trial, 0..2 (3 actions).
    rewards : array-like, float
        Observed reward per trial (0 or 1).
    rewards : array-like, float
        Observed reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial (states reset across blocks).
    set_sizes : array-like, int
        Set size on each trial for the corresponding block (3 or 6).
    age : array-like, int
        Participant age repeated per trial.
    model_parameters : list or array
        [lr_pos, lr_neg, wm_weight, softmax_beta, beta_setsize_penalty, beta_age_penalty]
        - lr_pos: RL learning rate for positive outcomes.
        - lr_neg: RL learning rate for negative outcomes.
        - wm_weight: WM mixture weight in [0,1].
        - softmax_beta: baseline inverse temperature; internally scaled x10.
        - beta_setsize_penalty: decrease in inverse temperature per extra items beyond 3.
        - beta_age_penalty: decrease in inverse temperature for older group.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, beta_setsize_penalty, beta_age_penalty = model_parameters
    base_beta = softmax_beta * 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])
        
        # Beta modulation by set size (larger set size -> lower beta) and age (older -> lower beta)
        beta_eff = base_beta - beta_setsize_penalty * max(0, nS - 3) - beta_age_penalty * age_group
        beta_eff = max(1e-3, beta_eff)  # keep positive
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # RL policy with effective beta
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            
            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)
            
            # Mixture
            wm_w = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL asymmetric update
            delta = r - q[s, a]
            lr_use = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr_use * delta
            
            # WM update: one-shot overwrite of chosen action with outcome
            w[s, a] = r
        
        blocks_log_p += log_p
    
    return -float(blocks_log_p)