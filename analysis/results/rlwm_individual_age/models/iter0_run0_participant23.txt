def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM weighting and WM decay.

    The model mixes a model-free RL policy and a transient working-memory (WM) policy
    that stores recently rewarded state-action pairs within a block. The contribution
    of WM is reduced for larger set sizes and for older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action on each trial (0..nA-1).
    rewards : array-like of {0,1}
        Reward feedback on each trial.
    blocks : array-like of int
        Block index per trial. Value changes indicate resets of RL and WM.
    set_sizes : array-like of int
        Set size (number of states) for the block on each trial (constant within a block).
    age : array-like or scalar
        Participant age (single value repeated). Age group is 0 if <=45 else 1.
    model_parameters : list or array-like
        [lr, base_wm_weight, softmax_beta, wm_forget, wm_age_shift, wm_setsize_slope]
        - lr: RL learning rate in [0,1].
        - base_wm_weight: baseline WM weight (for young, set size 3) in (0,1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_forget: WM decay/encoding strength per update in [0,1]; higher = stronger encoding and faster decay to baseline between updates.
        - wm_age_shift: additive shift (logit space) reducing WM weight for older group (age_group=1).
        - wm_setsize_slope: slope (logit space) for effect of larger set sizes on WM weight (applied to nS-3).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, base_wm_weight, softmax_beta, wm_forget, wm_age_shift, wm_setsize_slope = model_parameters
    # enforce effective ranges
    base_wm_weight = 1 / (1 + np.exp(-base_wm_weight))  # map to (0,1)
    wm_forget = 1 / (1 + np.exp(-wm_forget))            # (0,1)
    softmax_beta *= 10                                   # higher upper bound as specified
    softmax_beta_wm = 50                                 # very deterministic WM policy as specified

    # Age group coding: 0=young (<=45), 1=old (>45)
    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute WM mixture weight (trial-invariant within block) using logistic link
        # logit(wm_weight) = logit(base) + age_shift*age_group + setsize_slope*(nS-3)
        def logit(p): 
            p = np.clip(p, 1e-6, 1 - 1e-6)
            return np.log(p) - np.log(1 - p)
        logit_base = logit(base_wm_weight)
        logit_wm = logit_base + wm_age_shift * age_group + wm_setsize_slope * (nS - 3)
        wm_weight_block = 1.0 / (1.0 + np.exp(-logit_wm))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (softmax on w with large beta)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            #  - passive decay toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            #  - encode rewarded action as transient one-hot (use same wm_forget as encoding strength)
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(+stickiness) + WM mixture with separate RL learning rates and a capacity-like WM gate.

    The RL system uses separate learning rates for positive and negative outcomes and includes
    a perseveration bias ("stickiness") toward repeating the previous action within a state.
    WM contribution is gated by an effective capacity that declines with larger set sizes
    and with age.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
        Age group coded via age threshold at 45.
    model_parameters : list or array-like
        [alpha_pos, alpha_neg, softmax_beta, wm_capacity, wm_age_drop, stickiness]
        - alpha_pos: RL learning rate for r=1 in [0,1].
        - alpha_neg: RL learning rate for r=0 in [0,1].
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_capacity: effective WM capacity anchor around set size 3 (in logits; higher => more WM use).
        - wm_age_drop: negative shift (logit space) applied if age_group=1 to reduce WM weight.
        - stickiness: choice perseveration weight added to the chosen vs. non-chosen action in policy.
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_capacity, wm_age_drop, stickiness = model_parameters
    # Map learning rates into (0,1)
    alpha_pos = 1 / (1 + np.exp(-alpha_pos))
    alpha_neg = 1 / (1 + np.exp(-alpha_neg))
    softmax_beta *= 10
    softmax_beta_wm = 50

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Previous action within state for stickiness
        prev_action = -np.ones(nS, dtype=int)

        # WM weight gate: higher when nS <= effective capacity, lower otherwise and for older adults.
        # logit(wm_weight) = k*(wm_capacity - (nS-3)) + wm_age_drop*age_group, with k fixed=2
        k = 2.0
        logit_wm = k * (wm_capacity - (nS - 3)) + wm_age_drop * age_group
        wm_weight_block = 1.0 / (1.0 + np.exp(-logit_wm))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with stickiness bias toward previous action in this state
            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                # add stickiness to the previous action
                bias = np.zeros(nA)
                bias[prev_action[s]] = stickiness
                Q_eff = Q_s + bias
            else:
                Q_eff = Q_s

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            alpha = alpha_pos if r == 1 else alpha_neg
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # Update previous action
            prev_action[s] = a

            # WM update: store last rewarded action only
            # If reward, set WM for this state to a one-hot; if not, softly revert to uniform.
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                # mild leak toward uniform when feedback is negative
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with confidence-based arbitration modulated by age and set size.

    The mixture weight is determined online by comparing confidence of the WM and RL
    policies within the current state. Confidence is proxied by the action preference
    spread (max - second max). Age and set size bias the arbitration toward RL when
    older or under higher load.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [lr, softmax_beta, wm_decay, k_conf, age_shift, setsize_shift]
        - lr: RL learning rate in [0,1].
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_decay: WM decay toward uniform (and encoding strength) in [0,1].
        - k_conf: sensitivity scaling from (WM_conf - RL_conf) to mixture logit.
        - age_shift: additive shift on mixture logit for older group (typically negative).
        - setsize_shift: per-unit set-size additive shift on mixture logit (typically negative).
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, softmax_beta, wm_decay, k_conf, age_shift, setsize_shift = model_parameters
    lr = 1 / (1 + np.exp(-lr))
    wm_decay = 1 / (1 + np.exp(-wm_decay))
    softmax_beta *= 10
    softmax_beta_wm = 50

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute confidence proxies
            def spread(x):
                # max - second max
                idx = np.argsort(x)[::-1]
                return x[idx[0]] - x[idx[1]]
            wm_conf = spread(W_s)
            rl_conf = spread(Q_s)

            # Arbitration mixture weight via logistic of confidence difference with age and set size biases
            mix_logit = k_conf * (wm_conf - rl_conf) + age_shift * age_group + setsize_shift * (nS - 3)
            wm_weight = 1.0 / (1.0 + np.exp(-mix_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with decay and encoding on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * onehot

        blocks_log_p += log_p

    return -blocks_log_p