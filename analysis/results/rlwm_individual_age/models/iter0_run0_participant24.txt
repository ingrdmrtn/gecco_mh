Below are three standalone cognitive models that mix reinforcement learning (RL) and working memory (WM) and incorporate effects of set size (3 vs 6) and age group (young vs old). Each function returns the negative log-likelihood of the observed choices.

Notes common to all models:
- They follow the RL+WM mixture structure from your template.
- RL policy uses a softmax over Q-values; WM policy uses a near-deterministic softmax over WM values.
- Set size reduces WM contribution (or learning) in different ways across models.
- Age group is coded 0 for young and 1 for old and is used meaningfully in each model, but without adding extra parameters solely for age (to keep total parameters â‰¤ 6).

Model 1: RL + WM mixture with decay and one-shot WM storage; set size reduces WM weight; older group further reduces WM and inverse temperature
- Parameters (6): lr, wm_weight0, softmax_beta, wm_store_strength, wm_decay, size_penalty
- Set size effect: wm_weight is reduced as set size increases via size_penalty.
- Age effect: if old, both wm_weight and softmax_beta are reduced multiplicatively (hard-coded factors).

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with WM decay and one-shot storage. Set size reduces WM weight.
    Older group has reduced WM weight and inverse temperature.

    Parameters
    ----------
    states : array-like
        State (stimulus) index for each trial, 0..(set_size-1) within each block.
    actions : array-like
        Chosen action on each trial (0,1,2).
    rewards : array-like
        Feedback on each trial (0 or 1).
    blocks : array-like
        Block index for each trial.
    set_sizes : array-like
        Set size (3 or 6) for the block each trial belongs to.
    age : array-like
        Participant age (same scalar repeated); age[0] <= 45 => young (0), else old (1).
    model_parameters : list or array
        [lr, wm_weight0, softmax_beta, wm_store_strength, wm_decay, size_penalty]
        - lr: RL learning rate (0..1 after logistic transform).
        - wm_weight0: baseline WM mixture weight (0..1 after logistic).
        - softmax_beta: base inverse temperature for RL, scaled by 10.
        - wm_store_strength: strength to store rewarded action in WM (0..1 after logistic).
        - wm_decay: per-trial decay of WM toward uniform (0..1 after logistic).
        - size_penalty: reduces WM weight as set size increases (>=0 after exp transform).
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available; included for clarity only if already imported elsewhere

    # Parameter transforms
    lr = 1.0 / (1.0 + np.exp(-model_parameters[0]))
    wm_weight0 = 1.0 / (1.0 + np.exp(-model_parameters[1]))
    softmax_beta = model_parameters[2] * 10.0
    wm_store_strength = 1.0 / (1.0 + np.exp(-model_parameters[3]))
    wm_decay = 1.0 / (1.0 + np.exp(-model_parameters[4]))
    size_penalty = np.exp(model_parameters[5])  # >= 0

    # Age group
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight: reduced with set size; further reduced if old
        wm_weight_eff = wm_weight0 / (1.0 + size_penalty * max(0, nS - 3))
        if age_group == 1:
            wm_weight_eff *= 0.7  # older: less reliance on WM
            softmax_beta_eff = softmax_beta * 0.8  # older: noisier RL policy
        else:
            softmax_beta_eff = softmax_beta

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Decay WM toward uniform each trial (for current state)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy probability of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store rewarded action one-shot with strength wm_store_strength
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_store_strength) * w[s, :] + wm_store_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: RL+WM with set-size-dependent learning rate and probabilistic WM storage capacity
- Parameters (6): lr_raw, wm_weight0, softmax_beta, wm_store_strength, size_lr_penalty, wm_capacity_raw
- Set size effects:
  - WM weight decreases with set size.
  - RL learning rate decreases as set size increases (size_lr_penalty).
  - WM storage is capacity-limited via p_store = min(1, K/nS) where K is derived from wm_capacity_raw.
- Age effect:
  - Older group has lower WM capacity (0.8x) and lower WM weight (0.7x).

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with set-size-dependent RL learning and capacity-limited WM storage.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size for each trial's block (3 or 6).
    age : array-like
        Participant age (scalar repeated). 0=young, 1=old via age cutoff 45.
    model_parameters : list or array
        [lr_raw, wm_weight0, softmax_beta, wm_store_strength, size_lr_penalty, wm_capacity_raw]
        - lr_raw: base RL learning rate (mapped to 0..1).
        - wm_weight0: baseline WM weight (0..1).
        - softmax_beta: RL inverse temperature, scaled by 10.
        - wm_store_strength: WM store strength (0..1).
        - size_lr_penalty: factor reducing RL learning rate with larger set sizes (>=0).
        - wm_capacity_raw: maps to capacity K in [1,6].
    Returns
    -------
    float
        Negative log-likelihood.
    """
    import numpy as np  # assumed available

    # Parameter transforms
    lr_base = 1.0 / (1.0 + np.exp(-model_parameters[0]))
    wm_weight0 = 1.0 / (1.0 + np.exp(-model_parameters[1]))
    softmax_beta = model_parameters[2] * 10.0
    wm_store_strength = 1.0 / (1.0 + np.exp(-model_parameters[3]))
    size_lr_penalty = np.exp(model_parameters[4])  # >= 0
    # Map wm_capacity_raw to a continuous capacity in [1,6]
    cap01 = 1.0 / (1.0 + np.exp(-model_parameters[5]))  # 0..1
    K_capacity = 1.0 + 5.0 * cap01  # 1..6

    # Age group
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective RL learning rate reduced by set size
        lr = lr_base / (1.0 + size_lr_penalty * max(0, nS - 3))

        # WM weight reduced by set size; and reduced further if old
        wm_weight_eff = wm_weight0 / (1.0 + max(0, nS - 3))
        if age_group == 1:
            wm_weight_eff *= 0.7

        # Effective WM capacity as probability the current state is stored
        K_eff = K_capacity * (0.8 if age_group == 1 else 1.0)
        p_store = min(1.0, K_eff / float(nS))

        # Initialize value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store rewarded action proportional to storage probability
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend scaled by p_store * wm_store_strength
                k_eff = p_store * wm_store_strength
                w[s, :] = (1.0 - k_eff) * w[s, :] + k_eff * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: RL+WM with action perseveration bias; set size reduces WM weight; older group has lower RL inverse temperature and higher perseveration gain
- Parameters (6): lr, wm_weight0, beta_rl, beta_pers, pers_decay, size_penalty
- Policy combines:
  - RL+perseveration via a softmax over beta_rl*Q + beta_pers*pi (state-specific action stickiness).
  - WM policy mixed with the above using wm_weight.
- Set size effect: wm_weight reduced with size_penalty.
- Age effect: older group reduces beta_rl and increases beta_pers to reflect greater stickiness/noise.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with action perseveration. Set size reduces WM weight. Older group is more perseverative and noisier.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block.
    age : array-like
        Participant age (scalar repeated). 0=young, 1=old via age cutoff 45.
    model_parameters : list or array
        [lr, wm_weight0, beta_rl, beta_pers, pers_decay, size_penalty]
        - lr: RL learning rate (0..1 via logistic).
        - wm_weight0: baseline WM weight (0..1 via logistic).
        - beta_rl: RL inverse temperature, scaled by 10.
        - beta_pers: gain on perseveration bias, scaled by 10.
        - pers_decay: stickiness update/decay rate (0..1 via logistic).
        - size_penalty: reduces WM weight with set size (>=0 via exp).
    Returns
    -------
    float
        Negative log-likelihood.
    """
    import numpy as np  # assumed available

    # Parameter transforms
    lr = 1.0 / (1.0 + np.exp(-model_parameters[0]))
    wm_weight0 = 1.0 / (1.0 + np.exp(-model_parameters[1]))
    beta_rl = model_parameters[2] * 10.0
    beta_pers = model_parameters[3] * 10.0
    pers_decay = 1.0 / (1.0 + np.exp(-model_parameters[4]))
    size_penalty = np.exp(model_parameters[5])

    # Age group
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective WM weight reduced by set size; and by age
        wm_weight_eff = wm_weight0 / (1.0 + size_penalty * max(0, nS - 3))
        if age_group == 1:
            wm_weight_eff *= 0.75

        # Age effects on RL inverse temperature and perseveration gain
        if age_group == 1:
            beta_rl_eff = beta_rl * 0.8
            beta_pers_eff = beta_pers * 1.25
        else:
            beta_rl_eff = beta_rl
            beta_pers_eff = beta_pers

        # Initialize value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        # Perseveration traces per state-action
        pi = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            Pi_s = pi[s, :]

            # RL + perseveration composite policy
            logits_rlpers = beta_rl_eff * Q_s + beta_pers_eff * Pi_s
            # Compute choice prob of chosen action via denominator ratio trick
            denom_rlpers = np.sum(np.exp(logits_rlpers - logits_rlpers[a]))
            p_rlpers = 1.0 / max(denom_rlpers, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rlpers
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-based one-shot store without explicit decay
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Use a modest storage strength tied to pers_decay for parsimony
                k_store = pers_decay
                w[s, :] = (1.0 - k_store) * w[s, :] + k_store * one_hot

            # Perseveration update: move towards the last chosen action with decay
            # Normalize so that sum(Pi_s) does not explode
            pi[s, :] = (1.0 - pers_decay) * pi[s, :]
            pi[s, a] += pers_decay

        blocks_log_p += log_p

    return -blocks_log_p

Parameter notes and hypothesized set size and age effects:
- Model 1: WM becomes less influential with larger set sizes (size_penalty), and older adults rely less on WM and have noisier RL (reduced beta). WM stores rewarded actions with strength wm_store_strength and decays with wm_decay.
- Model 2: Larger set sizes reduce both WM contribution and RL learning rate (size_lr_penalty). WM storage is capacity-limited (K in [1,6]); older adults have lower effective capacity and rely less on WM.
- Model 3: Larger set sizes reduce WM weight (size_penalty). Older adults show lower RL inverse temperature (beta_rl) and higher perseveration (beta_pers), consistent with greater choice stickiness/noise.