def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + Capacity-limited Working Memory (WM) mixture
    - RL: action values per state learned via delta rule.
    - WM: one-shot storage of correct action upon reward; decays toward uniform on each trial.
    - Policy: mixture of RL and WM, with WM weight scaled by set size via a capacity factor.
    - Age: young (0) vs old (1) modulates effective WM capacity.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1), shared across set sizes and ages.
    - wm_weight: baseline weight of WM in the mixture (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - wm_capacity: WM capacity in number of items (0..6). Effective WM weight scales as min(1, capacity / set_size).
    - wm_decay: decay toward uniform per trial visit (0..1). Larger means faster forgetting.
    - age_capacity_scale: multiplicative scale on capacity for young vs old (e.g., >1 for young advantage).
      Applied as: eff_capacity = wm_capacity * (1 + age_capacity_scale) if young, (1 - age_capacity_scale) if old.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity, wm_decay, age_capacity_scale = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age-modulated capacity
    if age_group == 0:
        eff_capacity_multiplier = (1.0 + age_capacity_scale)
    else:
        eff_capacity_multiplier = max(0.0, (1.0 - age_capacity_scale))

    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight based on capacity and set size
        eff_capacity = wm_capacity * eff_capacity_multiplier
        p_store = min(1.0, max(0.0, eff_capacity / max(1, nS)))
        wm_mix_weight = np.clip(wm_weight * p_store, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probability for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM table
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Rewarded: one-shot update toward one-hot at chosen action
            # - Always: decay toward uniform (interference/forgetting)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * one_hot + wm_decay * w_0[s, :]
            else:
                # No new storage; just decay toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL + WM with interference-sensitive decay and negative feedback pruning
    - RL: standard delta rule.
    - WM: stores rewarded actions; subject to interference that grows with set size.
      On negative feedback, WM prunes the chosen action (anti-association) and redistributes mass.
    - Policy: mixture of RL and WM. WM weight attenuates with set size via an exponential interference term.
    - Age: older group (1) exhibits stronger WM decay and interference; young (0) weaker.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - base_decay: baseline WM decay toward uniform per trial (0..1).
    - interference: scaling of WM decay/interference per additional item (>=0). Effective decay increases with set size.
    - age_decay_boost: age effect on decay/interference (>=0). Added to interference for old, subtracted for young.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, base_decay, interference, age_decay_boost = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference-dependent decay; age-modulated
        if age_group == 1:
            eff_interf = max(0.0, interference + age_decay_boost)
        else:
            eff_interf = max(0.0, max(0.0, interference - age_decay_boost))

        # Effective decay grows with set size
        eff_decay = np.clip(base_decay + eff_interf * (max(0, nS - 1)), 0.0, 1.0)

        # WM mixture weight attenuated by interference with set size
        wm_mix_weight = wm_weight * np.exp(-eff_interf * max(1, nS))
        wm_mix_weight = np.clip(wm_mix_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0:
                # Store one-hot at chosen action (rewarded)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eff_decay) * one_hot + eff_decay * w_0[s, :]
            else:
                # Negative feedback pruning: depress chosen action, redistribute to others
                anti = w[s, :].copy()
                anti[a] = 0.0
                if anti.sum() > 0:
                    anti = anti / anti.sum()
                else:
                    anti = np.ones(nA) / nA
                w[s, :] = (1.0 - eff_decay) * anti + eff_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with asymmetric learning + WM one-shot + perseveration bias
    - RL: separate learning rates for positive and negative prediction errors.
    - WM: one-shot storage of last rewarded action per state (deterministic retrieval).
           WM weight decreases for large set size; if set size > 3, a size penalty applies.
    - Policy: mixture of RL and WM with an added perseveration bias favoring the last action chosen in the same state.
    - Age: age scales exploration/exploitation by modulating beta.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE.
    - lr_neg: RL learning rate for negative PE.
    - wm_weight: baseline WM mixture weight.
    - softmax_beta: base inverse temperature for RL (scaled internally by 10).
    - perseveration: bias added to the last action taken in the same state (>=0).
    - age_beta_scale: scales beta by age group:
        beta_eff = beta * (1 + age_beta_scale) for young, beta * (1 - age_beta_scale) for old.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, perseveration, age_beta_scale = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age scaling of beta
    if age_group == 0:
        beta_eff = softmax_beta * (1.0 + age_beta_scale)
    else:
        beta_eff = softmax_beta * max(0.0, (1.0 - age_beta_scale))

    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # WM mixture weight penalized for large set size (>3)
        size_penalty = 1.0 if nS <= 3 else 0.5  # reduced WM contribution in larger sets
        wm_mix_weight = np.clip(wm_weight * size_penalty, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # Add perseveration bias to RL values
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            W_s = w[s, :]

            # RL probability under biased Q
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: one-shot store upon reward; otherwise no overwrite
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Mild leak toward uniform to avoid numerical lock-in
                leak = 0.05
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p