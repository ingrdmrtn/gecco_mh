Below are three standalone cognitive models that implement different RL+WM mechanisms and age/set-size effects. Each function follows the required template and returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + capacity-limited WM mixture with reward-gated one-shot WM encoding and decay.
    
    Mechanism:
    - RL: single learning rate, softmax action selection.
    - WM: decays toward uniform; on rewarded trials, encodes the chosen action as a near one-hot (one-shot).
    - Arbitration: mixture of WM and RL policies, with the WM weight reduced by larger set size and by older age.
      Age group (0=young, 1=old) reduces WM reliance additively in logit space.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: base WM mixture weight (real-valued; internally squashed to (0,1))
    - softmax_beta: RL inverse temperature (scaled by *10 internally)
    - wm_decay: WM decay/learning rate toward uniform and toward one-hot on reward in [0,1]
    - k_set: penalty applied per extra item beyond set size 3 (capacity load effect on WM weight)
    - k_age: penalty applied for older age group (adds to logit of WM weight)
    
    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2)
    - rewards: array of rewards (0/1)
    - blocks: array of block indices
    - set_sizes: array of set size per trial (3 or 6)
    - age: array-like with the participant's age (same value repeated)
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, k_set, k_age = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    # Age group coding: 0 for young (<=45), 1 for old (>45)
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    # Convert base wm_weight to (0,1) via logistic
    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = block_set_sizes[0]

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        # Precompute WM weight for this block given set size and age
        base_w = logistic(wm_weight)
        base_logit = logit(base_w)
        # Penalize WM reliance for larger set sizes and older age (in logit space)
        adj_logit = base_logit - k_set * (nS - 3) - k_age * age_group
        wm_weight_eff = logistic(adj_logit)

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action a (softmax over WM values)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) If rewarded, move toward one-hot for chosen action (one-shot encoding)
            if r > 0:
                # Move additional mass to chosen action
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay * nA
            # Normalize for numerical stability
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL with asymmetric learning rates + recency-based WM (reward-agnostic) mixture.
    
    Mechanism:
    - RL: two learning rates for positive and negative outcomes; softmax action selection.
    - WM: maintains a recency-weighted action memory per state; decays to uniform and
          reinforces the most recent chosen action regardless of reward (captures choice recency/short-term memory).
    - Arbitration: WM/RL mixture with WM weight reduced by (set size + age) jointly.
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewards (r=1) in [0,1]
    - lr_neg: RL learning rate for non-rewards (r=0) in [0,1]
    - wm_weight: base WM mixture weight (real-valued; internally squashed to (0,1))
    - softmax_beta: RL inverse temperature (scaled by *10 internally)
    - wm_decay: WM decay/recency parameter in [0,1]
    - k_setage: joint penalty factor applied to (set_size_load + age_group) in the logit of WM weight
    
    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2)
    - rewards: array of rewards (0/1)
    - blocks: array of block indices
    - set_sizes: array of set size per trial (3 or 6)
    - age: array-like with the participant's age (same value repeated)
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, k_setage = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = block_set_sizes[0]

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        # WM weight adjusted by combined (set size load + age)
        base_w = logistic(wm_weight)
        base_logit = logit(base_w)
        load = (nS - 3) + age_group  # 0 for small/young; increases with bigger set and/or older age
        wm_weight_eff = logistic(base_logit - k_setage * load)

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update with asymmetric learning rates
            if r > 0:
                delta = 1.0 - Q_s[a]
                q[s, a] += lr_pos * delta
            else:
                delta = 0.0 - Q_s[a]
                q[s, a] += lr_neg * delta

            # WM update (recency-based, reward-agnostic):
            # Decay toward uniform, then add recency bump on the chosen action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Additional bump for the chosen action (keeps memory of last choice)
            w[s, :] = (1.0 - wm_decay) * w[s, :]
            w[s, a] += wm_decay * nA
            # Normalize
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL + WM with uncertainty-weighted arbitration and error-sensitive forgetting.
    
    Mechanism:
    - RL: single learning rate; softmax action selection.
    - WM: if rewarded, encode chosen action as near one-hot; if not rewarded, stronger forgetting (toward uniform).
    - Arbitration: WM vs RL mixture weight is dynamic:
        wm_weight_eff = sigmoid(logit(wm_weight) - bias_set*(set_size-3) - bias_age*age_group + ent_term),
      where ent_term increases reliance on WM when RL policy is uncertain (higher entropy).
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: base WM mixture weight (real-valued; squashed to (0,1))
    - softmax_beta: RL inverse temperature (scaled by *10 internally)
    - wm_forget: WM forgetting rate (applied every trial); larger values -> faster decay to uniform
    - bias_set: strength of set-size penalty on WM reliance
    - bias_age: strength of age penalty on WM reliance
    
    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2)
    - rewards: array of rewards (0/1)
    - blocks: array of block indices
    - set_sizes: array of set size per trial (3 or 6)
    - age: array-like with the participant's age (same value repeated)
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_forget, bias_set, bias_age = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic WM
    blocks_log_p = 0.0
    eps = 1e-12

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = block_set_sizes[0]

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        base_w = logistic(wm_weight)
        base_logit = logit(base_w)

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL softmax policy vector for entropy computation
            denom = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / np.maximum(denom, eps)
            # Entropy of RL policy (0..ln 3), higher -> more WM reliance
            entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            # Center entropy to roughly [ -1.1 .. 0 ], so it adds positive when high uncertainty
            ent_term = entropy - 1.0  # ~0 when near max entropy ln(3) â‰ˆ 1.10

            # Dynamic WM weight for this trial
            wm_logit = base_logit - bias_set * (nS - 3) - bias_age * age_group + ent_term
            wm_weight_eff = logistic(wm_logit)

            W_s = w[s, :]

            # RL policy probability for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action a
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with error-sensitive forgetting:
            # Always forget toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            if r > 0:
                # On reward, move toward one-hot on chosen action
                w[s, :] = (1.0 - wm_forget) * w[s, :]
                w[s, a] += wm_forget * nA
            else:
                # On error, apply an extra forgetting step (stronger decay)
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Normalize
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size influences:
- Model 1: WM weight is penalized linearly in logit space by set size (k_set) and age group (k_age). Larger set sizes and being older reduce WM reliance.
- Model 2: A single joint penalty parameter (k_setage) reduces WM reliance as a function of (nS-3 + age_group). RL also allows asymmetric learning from reward vs non-reward (lr_pos vs lr_neg), which can capture group differences in sensitivity.
- Model 3: WM reliance is dynamically increased when RL is uncertain (higher entropy), but reduced by larger set size (bias_set) and older age (bias_age). WM forgetting is stronger on errors.