def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM capacity-and-decay mixture model with set-size dependent recall and age-sensitive WM capacity.

    The model mixes:
    - RL system: Q-learning with softmax action selection.
    - WM system: a cached state-action distribution that can perfectly retrieve the correct action
      when the item is in working memory. WM retrieval probability scales with capacity and set size.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: mixture weight for WM versus RL (0-1), further modulated by recall probability
    - softmax_beta: inverse temperature for RL softmax; internally scaled by 10
    - K: WM capacity (in number of items). Effective capacity is reduced for older adults.
         We use a simple age-based scaling: K_eff = K if young, 0.7*K if old.
    - decay: WM decay rate toward uniform on negative feedback (0-1)
    - lapse: lapse probability of choosing uniformly at random (0-1)

    Age group:
    - age_group = 0 for young (<=45), 1 for old (>45)
    - K is reduced for older adults by a fixed factor (0.7), implementing an age effect.

    Set size effect:
    - WM recall probability on each trial: p_recall = min(1, K_eff / nS), where nS is the block set size.
      Larger set sizes reduce WM recall.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K, decay, lapse = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    age_group = 0 if age[0] <= 45 else 1
    # Age effect on capacity: older adults have reduced effective capacity
    K_eff_scale = 1.0 if age_group == 0 else 0.7

    softmax_beta_wm = 50.0  # highly deterministic WM choice
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a (softmax centered on chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM recall probability depends on set size and capacity
            p_recall = min(1.0, (K * K_eff_scale) / float(nS + 1e-12))

            # WM softmax over its current distribution (acts like a strong preference for stored action)
            # First, form a WM-driven action-probability via softmax of WM "preferences"
            wm_pref = W_s  # W_s sums to 1; we can still push it via softmax
            p_wm_vec = np.exp(softmax_beta_wm * (wm_pref - wm_pref.max()))
            p_wm_vec = p_wm_vec / (p_wm_vec.sum() + 1e-12)

            # If WM fails to recall, it reverts to uniform
            p_wm_mixed = p_recall * p_wm_vec + (1.0 - p_recall) * (1.0 / nA)
            p_wm = p_wm_mixed[a]

            # Mixture of WM and RL plus lapse
            p_total = (1.0 - lapse) * (wm_weight * p_wm + (1.0 - wm_weight) * p_rl) + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            # - Positive feedback: store chosen action with strength (1 - decay) toward one-hot
            # - Negative feedback: decay toward uniform
            if r > 0.5:
                # move distribution toward the chosen action
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]  # slight clearing
                # then add mass to chosen action and renormalize
                w[s, a] += (1.0 - w[s, a]) * 1.0  # push toward 1 on the chosen action
                w[s, :] /= (w[s, :].sum() + 1e-12)
            else:
                # decay toward uniform on an error
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning and set-size-scaled WM access, age reduces WM weight.

    The model combines:
    - RL system: softmax policy with separate learning rates for positive and negative outcomes.
    - WM system: one-shot store on reward, no explicit decay; access to WM decreases with set size.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewarded trials (0-1)
    - lr_neg: RL learning rate for unrewarded trials (0-1)
    - wm_weight: baseline WM mixture weight (0-1); effective weight reduced for older adults
    - softmax_beta: inverse temperature for RL softmax; internally scaled by 10
    - eta_store: WM storage strength on rewarded trials (0-1) toward the chosen action
    - sigma_setsize: controls how WM access drops with set size; p_recall = 1 / (1 + exp(sigma*(nS - 3)))

    Age group:
    - age_group = 0 for young (<=45), 1 for old (>45)
    - Effective WM weight: wm_weight_eff = wm_weight * (1 - 0.3*age_group), reducing WM contribution in older adults.

    Set size effect:
    - WM recall probability decreases as set size increases via a logistic function centered at 3.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, eta_store, sigma_setsize = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    wm_weight_eff = wm_weight * (1.0 - 0.3 * age_group)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Set-size dependent WM recall probability via logistic drop
            p_recall = 1.0 / (1.0 + np.exp(sigma_setsize * (nS - 3.0)))

            # WM softmax policy from its current distribution
            wm_pref = W_s
            p_wm_vec = np.exp(softmax_beta_wm * (wm_pref - wm_pref.max()))
            p_wm_vec = p_wm_vec / (p_wm_vec.sum() + 1e-12)

            # If WM not accessible, fallback to uniform
            p_wm = (p_recall * p_wm_vec[a]) + (1.0 - p_recall) * (1.0 / nA)

            # Mixture policy (no explicit lapse here; lapse is effectively captured by low wm_weight_eff and RL softness)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_use * delta

            # WM update: store strongly on reward; otherwise leave as is (no explicit decay)
            if r > 0.5:
                # Move distribution toward one-hot on chosen action
                w[s, :] = (1.0 - eta_store) * w[s, :] + eta_store * w_0[s, :]
                w[s, a] += (1.0 - w[s, a]) * 1.0
                w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    PE-gated RLâ€“WM arbitration model with age and set-size effects on gating.

    The model mixes RL and WM based on a dynamic, trial-wise gate:
    wm_weight_t = sigmoid(g0 + g_pe*|delta| + g_set*(nS-3) + g_age*age_group)
    - Larger |delta| can drive more WM reliance (e.g., after surprising outcomes).
    - Larger set sizes reduce WM reliance (negative g_set).
    - Older age can reduce WM reliance (positive g_age pushing sigmoid lower when added).

    Components:
    - RL: Q-learning with softmax policy.
    - WM: cached one-hot when recently rewarded; decays toward uniform on errors.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - softmax_beta: inverse temperature for RL softmax; internally scaled by 10
    - g0: gating intercept (real)
    - g_pe: gating sensitivity to |prediction error|
    - g_set: gating sensitivity to set size (nS-3)
    - g_age: gating offset for older adults

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, g0, g_pe, g_set, g_age = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Current WM-driven distribution
            wm_pref = W_s
            p_wm_vec = np.exp(softmax_beta_wm * (wm_pref - wm_pref.max()))
            p_wm_vec = p_wm_vec / (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Compute current PE for gating (based on current policy's chosen action value)
            delta = r - Q_s[a]
            # Dynamic gating weight via sigmoid
            gate_input = g0 + g_pe * abs(delta) + g_set * (nS - 3.0) + g_age * age_group
            wm_weight_t = 1.0 / (1.0 + np.exp(-gate_input))

            # Mixture probability
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM update: strengthen on reward, decay on error
            if r > 0.5:
                # move toward one-hot on chosen action
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]  # slight reset before storing
                w[s, a] += (1.0 - w[s, a]) * 1.0
                w[s, :] /= (w[s, :].sum() + 1e-12)
            else:
                # decay toward uniform when error
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p