Below are three standalone cognitive models that implement an RL+WM mixture for this RLWM task. Each model:

- Takes states, actions, rewards, blocks, set_sizes, age, model_parameters
- Uses a block-wise RL+WM arbitration with a fixed WM softmax (very deterministic)
- Returns the negative log-likelihood of the observed choices
- Uses all parameters meaningfully, incorporates set size (3 vs 6) and age group (young=0, old=1) effects
- Follows the provided template structure and fills in the WM policy and WM updating rules

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with capacity/decay modulated by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay_base, wm_lr, age_mod]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM weight in the mixture (0..1)
        - softmax_beta: RL inverse temperature (>0); scaled by 10 internally
        - wm_decay_base: baseline WM decay toward prior (0..1)
        - wm_lr: WM learning rate (0..1)
        - age_mod: modulation of WM capacity/decay by age group (>=0)
          (older age increases decay and reduces WM weight)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay_base, wm_lr, age_mod = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    # Age group: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective WM weight and decay for this block
        # Larger set sizes reduce WM weight and increase WM decay.
        # Older age (age_group=1) further reduces WM weight and increases decay.
        size_factor = 3.0 / float(nS)  # 1.0 for set size=3; 0.5 for set size=6
        wm_weight_eff = wm_weight_base * size_factor * (1.0 - age_mod * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        wm_decay_eff = wm_decay_base + (1.0 - size_factor) * wm_decay_base + age_mod * age_group * (1.0 - wm_decay_base)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action (numerically stable form provided)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM values; very deterministic)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward prior, then WM learning on the experienced (s,a)
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0
            w[s, a] += wm_lr * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM as last-reward memory with interference.
    WM weight is modulated by set size (capacity load) and age group.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [alpha_pos, alpha_neg, softmax_beta, wm_weight0, kappa_size, age_penalty]
        - alpha_pos: RL learning rate for positive outcomes (0..1)
        - alpha_neg: RL learning rate for negative outcomes (0..1)
        - softmax_beta: RL inverse temperature (>0); scaled by 10 internally
        - wm_weight0: baseline WM mixture weight (0..1)
        - kappa_size: scaling of set-size effect on WM (>=0)
        - age_penalty: reduction in WM efficacy for older adults (>=0)

        WM interference/decay and mixture weights are computed from set size and age:
        Higher set size and older age both reduce WM reliability and mixture weight.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight0, kappa_size, age_penalty = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    # Age group: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute WM mixture weight as a logistic transform of baseline adjusted by set size and age
        # More load (nS larger) -> lower weight; older -> lower weight
        size_term = kappa_size * (3.0 / float(nS) - 1.0)  # 0 at nS=3, negative at nS=6
        logit_w = np.log(wm_weight0 + 1e-12) - np.log(1.0 - wm_weight0 + 1e-12) + size_term - age_penalty * age_group
        wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # WM interference/decay strength d in [0,1], increases with set size and age
        # We use a logistic transform to keep it bounded.
        decay_logit = kappa_size * (float(nS) - 3.0) + age_penalty * age_group
        d = 1.0 / (1.0 + np.exp(-decay_logit))
        d = np.clip(d, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL choice prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice prob of chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM interference: decay toward prior, then overwrite current association with the observed outcome
            w = (1.0 - d) * w + d * w_0
            # Overwrite stores most recent observed value for (s,a) with high fidelity
            w[s, a] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with stickiness.
    WM weight depends on WM certainty (entropy), set size (capacity), and age.
    Includes action stickiness in the RL policy.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr, softmax_beta, wm_capacity, age_bias, decay_base, stickiness]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (>0); scaled by 10 internally
        - wm_capacity: scales sensitivity of arbitration to set size/certainty (>=0)
        - age_bias: reduces WM reliance and increases WM decay for older adults (can be >0)
        - decay_base: baseline WM decay toward prior (0..1) via logistic transform
        - stickiness: choice perseveration weight added to the last chosen action in the current state (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_capacity, age_bias, decay_base, stickiness = model_parameters
    softmax_beta *= 10.0

    # Age group: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action in each state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # WM decay parameter for this block (logistic to keep [0,1])
        # Larger set size and older age -> higher decay.
        base_logit = np.log(decay_base + 1e-12) - np.log(1.0 - decay_base + 1e-12)
        decay_logit = base_logit + (float(nS) - 3.0) * wm_capacity + age_bias * age_group
        decay_eff = 1.0 / (1.0 + np.exp(-decay_logit))
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness added to the value of the last action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # Compute RL choice prob of chosen action (provided stable form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy and its entropy (for arbitration)
            W_s = w[s, :]
            # Full WM softmax distribution for entropy
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            exp_wm = np.exp(wm_logits)
            pW = exp_wm / np.sum(exp_wm)
            # Chosen-action prob under WM
            p_wm = pW[a]
            # Entropy of WM distribution (0..log(nA)), higher => more uncertain WM
            H_wm = -np.sum(pW * np.log(np.clip(pW, 1e-12, 1.0)))

            # Arbitration: higher entropy -> down-weight WM; larger set size and older age also reduce WM weight
            # Convert H_wm into a penalty and transform via logistic
            # Max entropy log(nA); normalize by log(nA)
            H_norm = H_wm / np.log(nA)
            # Arbitration logit combines capacity (set size), age, and certainty
            arb_logit = wm_capacity * (3.0 / float(nS)) - age_bias * age_group - H_norm * wm_capacity
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_logit))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update (no asymmetry here)
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: decay toward prior then fast update on (s,a)
            w = (1.0 - decay_eff) * w + decay_eff * w_0
            # One-shot, high-fidelity update of the experienced pair
            w[s, a] = r

            # Update stickiness memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p