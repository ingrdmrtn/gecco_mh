def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with set-size and age modulation of WM use; WM decays when not rewarded.
    
    The model mixes a model-free RL policy with a fast working-memory (WM) policy.
    - WM weight decreases with larger set size and is reduced for older adults.
    - WM stores rewarded action for each state (near-deterministic), and decays toward uniform otherwise.
    
    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size of the current block (3 or 6) repeated over trials within the block.
    age : array-like
        Participant's age (single value repeated). Age group is 0 for young (<=45), 1 for old (>45).
    model_parameters : list or tuple
        [lr, wm_weight, softmax_beta, wm_decay, gamma, age_wm_penalty]
        - lr: RL learning rate (0..1).
        - wm_weight: Base WM weight in arbitration (0..1) before set-size/age modulation.
        - softmax_beta: Inverse temperature for RL policy (scaled by 10 in-function).
        - wm_decay: WM update/forgetting step size (0..1). Larger means faster overwrite/forgetting.
        - gamma: Set-size sensitivity of WM weight; wm weight scales by (3/nS)^gamma.
        - age_wm_penalty: Proportional reduction of WM weight for older adults.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, gamma, age_wm_penalty = model_parameters
    # Higher upper bound for beta
    softmax_beta *= 10.0
    
    # Age group coding: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0  # WM is very deterministic
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior for WM
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy probability of chosen action (stable via chosen-centered shift)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            # WM policy probability of chosen action (near-deterministic over W)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # Set-size and age modulation of WM weight
            size_factor = (3.0 / nS) ** gamma  # decreases with larger set size if gamma>0
            age_factor = (1.0 - age_wm_penalty * age_group)  # reduces WM use for older adults
            wm_weight_eff = wm_weight * size_factor * age_factor
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)
            
            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update:
            # - If rewarded, move WM toward a one-hot on the chosen action (fast encoding).
            # - If not rewarded, WM decays toward uniform (forgetting).
            if r > 0.0:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay * 1.0
                # Renormalize to a proper distribution
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # Forget toward uniform baseline
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size dependent learning rate and WM with lapse; age modulates RL exploration.
    
    The model mixes a model-free RL policy (with learning rate depending on set size)
    and a WM policy that stores the last rewarded action for a state. A lapse parameter
    adds random choice to account for attentional failures. Older adults explore more
    (lower effective beta) and learn more slowly.
    
    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size of the current block (3 or 6) repeated over trials within the block.
    age : array-like
        Participant's age (single value repeated). Age group is 0 for young (<=45), 1 for old (>45).
    model_parameters : list or tuple
        [lr_small, lr_large, softmax_beta, wm_weight, lapse, age_beta_shift]
        - lr_small: RL learning rate used when set size is 3.
        - lr_large: RL learning rate used when set size is 6.
        - softmax_beta: Base inverse temperature for RL (scaled by 10 in-function).
        - wm_weight: Base WM mixture weight (0..1).
        - lapse: Lapse rate for random choice mixture (0..1).
        - age_beta_shift: Proportional reduction in beta for older adults (0..1).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_small, lr_large, softmax_beta, wm_weight, lapse, age_beta_shift = model_parameters
    # Higher upper bound for beta
    softmax_beta *= 10.0
    
    # Age group coding: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0  # WM is very deterministic
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Choose RL learning rate based on set size, then slow down for older adults
        lr_eff = lr_small if nS == 3 else lr_large
        lr_eff = lr_eff * (1.0 - 0.3 * age_group)  # older adults learn slower
        
        # Age-dependent exploration: reduce beta for older adults
        beta_eff = softmax_beta * (1.0 - age_beta_shift * age_group)
        beta_eff = max(beta_eff, 1e-6)
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL choice prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            # WM choice prob of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # WM mixture (set size weakly impacts WM through wm_weight scaled by 3/nS)
            wm_weight_eff = wm_weight * (3.0 / nS)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)
            
            # Combine RL and WM, then add lapse mixture with uniform random choice
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta
            
            # WM update: reward-locked storage, otherwise reset to uniform
            if r > 0.0:
                # Strong overwrite of WM toward chosen action
                w[s, :] = (1.0 - 0.9) * w[s, :]  # mild residual
                w[s, a] += 0.9
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # On errors, WM resets to uniform for this state
                w[s, :] = w_0[s, :]
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration; WM decay depends on set size and age.
    
    The model arbitrates between WM and RL based on:
      - WM fidelity (max probability in WM for the current state).
      - RL uncertainty (entropy of RL softmax for the current state).
    WM decay increases with larger set size and for older adults.
    
    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size of the current block (3 or 6) repeated over trials within the block.
    age : array-like
        Participant's age (single value repeated). Age group is 0 for young (<=45), 1 for old (>45).
    model_parameters : list or tuple
        [lr, base_wm_weight, softmax_beta, wm_decay_base, setsize_decay_slope, age_decay_bonus]
        - lr: RL learning rate.
        - base_wm_weight: Base arbitration weight scaling WM contribution (0..1).
        - softmax_beta: Inverse temperature for RL policy (scaled by 10 in-function).
        - wm_decay_base: Baseline WM decay step (0..1).
        - setsize_decay_slope: Additional decay per (nS-3)/3; larger set size -> more decay.
        - age_decay_bonus: Additional decay added for older adults.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, base_wm_weight, softmax_beta, wm_decay_base, setsize_decay_slope, age_decay_bonus = model_parameters
    # Higher upper bound for beta
    softmax_beta *= 10.0
    
    # Age group coding: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0  # WM is very deterministic
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # WM decay increases with set size and with age, bounded to [0,1]
        wm_decay = wm_decay_base + setsize_decay_slope * ((nS - 3.0) / 3.0) + age_decay_bonus * age_group
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # Full RL policy to compute entropy (uncertainty)
            prl_full = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_full = prl_full / np.sum(prl_full)
            # Entropy normalized to [0,1] by log(nA)
            H_rl = -np.sum(prl_full * np.log(np.clip(prl_full, 1e-12, 1.0))) / np.log(nA)
            
            # WM policy prob of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # WM fidelity: confidence in best action
            wm_fidelity = float(np.max(W_s))
            
            # Uncertainty-based arbitration:
            # higher WM fidelity and lower RL entropy -> more WM reliance
            wm_weight_unc = base_wm_weight * wm_fidelity * (1.0 - H_rl)
            # Also penalize WM in larger set sizes
            wm_weight_unc *= (3.0 / nS)
            # Reduce WM weight for older adults
            wm_weight_unc *= (1.0 - 0.5 * age_group)
            wm_weight_unc = np.clip(wm_weight_unc, 0.0, 1.0)
            
            p_total = wm_weight_unc * p_wm + (1.0 - wm_weight_unc) * p_rl
            log_p += np.log(max(p_total, 1e-12))
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update: continuous decay toward uniform and reward-anchored strengthening
            # First decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Then, if rewarded, shift probability mass toward chosen action
            if r > 0.0:
                boost = 0.5 * (1.0 - wm_decay)  # boost scales with remaining mass
                w[s, a] = np.clip(w[s, a] + boost, 0.0, 1.0)
                # Renormalize
                w[s, :] = w[s, :] / np.sum(w[s, :])
        
        blocks_log_p += log_p
    
    return -blocks_log_p