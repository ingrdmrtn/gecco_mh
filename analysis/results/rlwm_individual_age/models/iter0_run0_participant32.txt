Below are three standalone cognitive models that implement RL–WM mixtures for the rlwm task. Each function:

- Follows the provided template structure and fills the WM policy and WM updates.
- Uses all parameters meaningfully (≤ 6 per model).
- Returns the negative log-likelihood of the observed choices.
- Uses age group (0 young, 1 old) and set size to modulate parameters.

Note: Assumes numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with age-modulated WM capacity.
    
    Parameters
    ----------
    states : array-like of int
        State index at each trial.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Received reward at each trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of int
        Participant's age repeated across trials. Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, wm_decay, wm_capacity, age_wm_penalty]
        - lr: RL learning rate
        - wm_weight: baseline WM arbitration weight (0..1)
        - softmax_beta: base inverse temperature for RL (scaled up internally)
        - wm_decay: WM decay toward uniform per trial (0..1)
        - wm_capacity: WM capacity (in items); arbitration scales as capacity / set_size
        - age_wm_penalty: reduction in effective WM capacity for older group
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity, age_wm_penalty = model_parameters
    softmax_beta *= 10  # higher upper bound

    # Age group
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50  # nearly deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM arbitration weight scales by capacity and set size, age reduces capacity
        eff_capacity = max(0.0, wm_capacity - age_group * age_wm_penalty)
        capacity_scaler = np.clip(eff_capacity / nS, 0.0, 1.0)
        wm_weight_eff_block = np.clip(wm_weight * capacity_scaler, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability (efficient softmax trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W_s with very high beta (nearly argmax)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture policy
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            # Numerical safety
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then overwrite on rewarded trials
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Overwrite toward one-hot on correct/rewarded action
            if r > 0.0:
                # learning strength complements decay so both are used meaningfully
                wm_learn = 1.0 - wm_decay
                # move probability mass toward the chosen action
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
                # renormalize for numerical stability
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-specific learning and set-size–dependent noise, mixed with
    gated WM that is boosted on rewarded trials and degrades more under higher load.
    
    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha_pos, alpha_neg, wm_weight, softmax_beta, beta_set_cost, gate]
        - alpha_pos: RL learning rate for rewards
        - alpha_neg: RL learning rate for non-rewards
        - wm_weight: baseline WM weight
        - softmax_beta: base inverse temperature for RL (scaled internally)
        - beta_set_cost: increases decision noise with larger set size
        - gate: WM gating weight after reward vs non-reward (in [0,1])
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, beta_set_cost, gate = model_parameters
    softmax_beta *= 10

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50

    # Age used to modulate learning rates and decision noise (older -> lower gain)
    if age_group == 1:
        alpha_pos *= 0.8
        alpha_neg *= 0.8
        softmax_beta *= 0.8

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size–dependent RL temperature (more noise with larger sets)
            beta_eff = softmax_beta / (1.0 + beta_set_cost * max(0, nS - 3))

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # WM weight: penalized by set size; gated by current outcome
            base_wm_w = wm_weight / (1.0 + max(0, nS - 3))
            gate_t = gate if r > 0.0 else (1.0 - gate)
            wm_weight_eff = np.clip(base_wm_w * gate_t, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta > 0 else alpha_neg
            q[s, a] += alpha * delta

            # WM update: outcome-dependent consolidation and load-dependent forgetting
            # Forgetting increases with set size
            forget = 0.1 + 0.2 * max(0, nS - 3) / 3.0  # 0.1 at 3, 0.3 at 6
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]
            if r > 0.0:
                # Strengthen chosen action representation more when rewarded
                consolidate = 0.7  # strong one-shot-like write without extra parameter
                w[s, :] = (1.0 - consolidate) * w[s, :]
                w[s, a] += consolidate
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM arbitration by uncertainty with capacity-limited WM write.
    
    Arbitration uses a logistic function over (RL uncertainty - WM certainty),
    with age and set size biases. WM updates are capacity-scaled (K) and decay.
    
    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [lr, wm_weight0, softmax_beta, arbitration_slope, capacity_K, set_bias]
        - lr: RL learning rate
        - wm_weight0: baseline arbitration logit (prior WM tendency)
        - softmax_beta: base RL inverse temperature (scaled internally)
        - arbitration_slope: sensitivity to (RL uncertainty - WM uncertainty)
        - capacity_K: WM capacity in items for effective writes
        - set_bias: bias term for larger set sizes in arbitration logit
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, wm_weight0, softmax_beta, arbitration_slope, capacity_K, set_bias = model_parameters
    softmax_beta *= 10

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age bias in arbitration: older group shifts away from WM
        age_bias = -0.5 * age_group

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen-action prob
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Compute RL and WM action distributions for uncertainty estimation
            # RL distribution
            exp_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_vec = exp_rl / np.sum(exp_rl)
            # WM distribution
            exp_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm_vec = exp_wm / np.sum(exp_wm)

            # Normalized entropies (0..1)
            eps = 1e-12
            H_rl = -np.sum(prl_vec * np.log(prl_vec + eps)) / np.log(nA)
            H_wm = -np.sum(pwm_vec * np.log(pwm_vec + eps)) / np.log(nA)

            # Arbitration logit combines baseline, uncertainty contrast, age and set-size bias
            logit = wm_weight0 + arbitration_slope * (H_rl - (1.0 - H_wm)) \
                    + age_bias + set_bias * max(0, nS - 3)
            wm_weight_eff = sigmoid(logit)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay + capacity-scaled write on rewards
            # Decay slightly each trial
            phi = 0.15
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            if r > 0.0:
                # Capacity scaling: if set > K, writes are diluted
                scale = min(1.0, capacity_K / max(1.0, nS))
                write = 0.8 * scale
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes
- Model 1: WM arbitration weight scales as effective capacity (capacity minus age penalty) divided by set size; WM stores rewarded associations with decay otherwise.
- Model 2: RL has valence-specific learning rates; decision noise increases with set size. WM contribution is higher right after rewards (gated) and declines with larger sets; older group has reduced learning/noise gain.
- Model 3: Arbitration uses uncertainty: greater RL uncertainty and stronger WM certainty favor WM; includes age and set-size biases. WM writes are capacity-limited by K.