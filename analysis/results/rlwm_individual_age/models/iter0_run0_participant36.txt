def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and set-size-modulated WM contribution and WM decay.

    Model summary:
    - Choices arise from a mixture of a model-free RL controller and a fast working-memory (WM) controller.
    - The WM policy is near-deterministic (high inverse temperature), storing rewarded state-action pairs and
      decaying toward uniform when not reinforced.
    - The WM mixture weight is reduced by larger set size and, additionally, reduced for older adults.
    - RL updates use a single learning rate.

    Parameters (list; total 6):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight_base: base mixture weight for WM contribution before modulations.
    - softmax_beta: inverse temperature for RL softmax (internally scaled by 10).
    - wm_decay: WM learning/decay rate: toward one-hot when rewarded; toward uniform when not.
    - gamma_age: multiplicative reduction of WM weight for older adults (age_group=1).
    - gamma_set: exponent controlling how WM weight scales with set size (3 vs 6).
                 Effective weight = wm_weight_base * (1 - gamma_age*age_group) * (3/nS)**gamma_set

    Inputs:
    - states: array of state indices per trial (0..nS-1 within each block).
    - actions: array of chosen action indices per trial (0..2).
    - rewards: array of scalar rewards (0/1) per trial.
    - blocks: array of block indices per trial.
    - set_sizes: array of block set sizes (3 or 6) per trial.
    - age: array with a single repeated value of participant age.
    - model_parameters: list of 6 parameters as defined above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, gamma_age, gamma_set = model_parameters
    softmax_beta *= 10.0  # keep consistent with template scaling

    # Age group coding: 0 = young, 1 = old
    age_group = 1 if age[0] > 45 else 0

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables per block
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective WM weight given set size and age group
        # larger set size and older age reduce WM contribution
        wm_weight_eff = wm_weight_base * (1.0 - gamma_age * age_group) * (3.0 / float(nS))**gamma_set
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy: probability of chosen action a via softmax trick
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WORKING MEMORY policy (softmax over WM weights for this state)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If rewarded: move toward one-hot on chosen action.
            # - If not rewarded: decay toward uniform (forget).
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM gating by set size and age, with WM leak.

    Model summary:
    - RL uses separate learning rates for positive vs negative outcomes.
    - WM controller is a win-stay store: upon reward, the state stores the chosen action deterministically.
      Between trials, WM representation leaks toward uniform; leak increases with set size and for older adults.
    - The WM mixture weight is also reduced by set size and age via a logistic gate.

    Parameters (list; total 6):
    - alpha_pos: RL learning rate for positive prediction errors (reward=1).
    - alpha_neg: RL learning rate for negative prediction errors (reward=0).
    - softmax_beta: inverse temperature for RL (scaled by 10 internally).
    - wm_weight_base: base WM mixture weight (before gating).
    - kappa: sensitivity of WM gating/leak to set size (larger kappa -> more reduction/leak at nS=6).
    - age_effect: additional WM reduction/leak for older adults (age_group=1).

    WM specifics:
    - WM policy uses softmax with high temperature (near-deterministic).
    - WM leak per trial: leak = sigmoid(kappa*(nS-3) + age_effect*age_group) in (0,1),
      applied to decay all states toward uniform. Larger set sizes and older age => more leak.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as described.
    - model_parameters: list of 6 parameters as defined above.

    Returns:
    - Negative log-likelihood.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, kappa, age_effect = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM gating weight
        gate = sigmoid(kappa * (nS - 3.0) + age_effect * age_group)
        wm_weight_eff = wm_weight_base * (1.0 - gate)  # more gate -> less WM
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # WM leak (applied every trial to all states)
        leak = sigmoid(kappa * (nS - 3.0) + age_effect * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy prob
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy prob
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update: win-stay write on reward; leak toward uniform on every trial
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # store deterministically on reward

            # Global leak toward uniform for all states
            w = (1.0 - leak) * w + leak * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited WM and perseveration bias; temperature modulated by age and set size.

    Model summary:
    - RL controller with inverse temperature modulated by set size and age (lower effective beta for larger sets and older adults).
    - WM controller stores rewarded associations but decays faster when set size exceeds capacity C.
    - Action perseveration bias favors repeating the previous action in the current state, capturing habitual/repetition tendencies.
    - Choice probability is a mixture of RL and WM; RL policy includes a perseveration bias in its logits.

    Parameters (list; total 6):
    - lr: RL learning rate.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - wm_weight: base WM mixture weight.
    - C: WM capacity in number of items (in states); larger nS > C increases WM decay.
    - decay_base: base WM decay rate, scaled up when nS > C and for older adults.
    - persev: perseveration bias strength added to the chosen action if it matches the most recent action in state.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as described.
    - model_parameters: list of 6 parameters as defined above.

    Returns:
    - Negative log-likelihood.
    """
    lr, beta_base, wm_weight, C, decay_base, persev = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective RL temperature: reduced by set size and older age
        # softmax_beta_eff = softmax_beta / (1 + 0.5*(nS-3) + 0.5*age_group)
        beta_scale = 1.0 / (1.0 + 0.5 * (nS - 3.0) + 0.5 * age_group)
        softmax_beta_eff = softmax_beta * beta_scale

        # WM effective decay: increases when nS > C and for older adults
        overload = max(0.0, (nS - C) / max(1.0, C))
        wm_decay_eff = decay_base * (1.0 + overload) * (1.0 + 0.5 * age_group)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        # WM weight is modestly reduced when overloaded and for older adults
        wm_weight_eff = wm_weight * (1.0 - 0.5 * overload) * (1.0 - 0.25 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += persev  # bias toward repeating last action in this state

            # Compute probability of chosen action with biased logits:
            # P(a) = exp(beta*(Q_a + bias_a)) / sum_j exp(beta*(Q_j + bias_j))
            # Using the "chosen action trick":
            denom_rl = np.sum(np.exp(softmax_beta_eff * ((Q_s + bias) - (Q_s[a] + bias[a]))))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update:
            # - If rewarded: move toward one-hot at chosen action.
            # - Otherwise: decay toward uniform.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * one_hot
            else:
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p