Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) mechanisms, tailored to the RLWM task and to age and set-size manipulations. Each model returns the negative log-likelihood of the observed choices and uses no more than six parameters. All provided parameters are used meaningfully.

Note: Assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-scaled WM weight, WM decay, and perseveration bias.
    
    Mechanism:
    - RL: single learning rate alpha (lr), softmax with inverse temperature beta.
    - WM: one-shot storage of the last rewarded action for a visited state, with decay to baseline
           and a near-deterministic softmax (beta_wm=50).
    - Mixture: WM weight is scaled down by set size (3 vs 6) and further reduced in the older group.
    - Perseveration: add a bias toward repeating the last action in the current state; stronger in older adults.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_base: base WM mixture weight in [0,1] before capacity scaling.
    - k_age: age penalty on WM weight in [0,1], applied only if age_group==1 (older).
    - wm_decay: WM decay toward baseline per state visit in [0,1].
    - beta: RL inverse temperature (scaled internally by 10).
    - pers: perseveration bias magnitude (added to chosen action preference), >=0.
    
    Inputs:
    - states: array of state indices per trial (int)
    - actions: array of chosen action indices per trial (int; 0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial (int)
    - set_sizes: array of set size for the active block on each trial (3 or 6)
    - age: array with a single value repeated (participant age)
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, k_age, wm_decay, beta, pers = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0  # very deterministic WM policy
    
    # Age group coding: 0=younger, 1=older
    age_group = 1 if age[0] > 45 else 0
    
    nA = 3
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        
        nS = int(block_set_sizes[0])
        
        # Initialize RL and WM stores for this block
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Track last action per state for perseveration
        last_act = -np.ones(nS, dtype=int)
        
        # WM mixture weight scaled by set size and age
        size_scale = 3.0 / float(nS)  # 1.0 for set size 3, 0.5 for set size 6
        age_scale = (1.0 - k_age * age_group)  # reduce WM in older depending on k_age
        wm_weight_eff = np.clip(wm_base * size_scale * age_scale, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # Action biases (perseveration)
            bias = np.zeros(nA)
            if last_act[s] >= 0:
                # Slightly stronger perseveration for older adults via multiplicative factor
                bias[last_act[s]] += pers * (1.0 + 0.3 * age_group)
            
            # RL choice probability for chosen action
            Q_s = q[s, :] + bias
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12
            
            # WM choice probability for chosen action
            W_s = w[s, :] + bias
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            
            # WM decay toward baseline for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            # WM storage: on reward, store a strong memory for the correct action
            if r > 0.5:
                # Overwrite policy: strong one-hot with small residual to others
                w[s, :] = 0.0 * w[s, :]
                w[s, :] += 0.0  # explicitly zero others
                w[s, a] = 1.0
            # On no reward, leave the decayed memory (no punitive write)
            
            # Update perseveration memory
            last_act[s] = a
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + cue-based WM retrieval window, lapse noise.
    
    Mechanism:
    - RL: dual learning rates for positive and negative outcomes (alpha_pos, alpha_neg).
    - WM: stores the most recently rewarded action per state; retrieval decays with "window" L,
          modeled as a success probability to retrieve WM that shrinks with larger set sizes
          and with older age.
    - Lapse: undirected choice noise mixed in; increases with set size and age.
    - Policy: mixture of WM softmax (beta_wm=50) and RL softmax (beta) weighted by WM retrieval probability,
              then mixed with lapse to uniform.
    
    Parameters (model_parameters):
    - alpha_pos: RL learning rate for rewards in [0,1].
    - alpha_neg: RL learning rate for non-rewards in [0,1].
    - wm_base: base WM retrieval mixture weight in [0,1].
    - L: WM retrieval "window" strength in [0,1], larger -> better WM access; reduced by set size and age.
    - beta: RL inverse temperature (scaled internally by 10).
    - lapse: base lapse rate in [0,1], scaled up by set size and age.
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as described.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, wm_base, L, beta, lapse = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0
    
    age_group = 1 if age[0] > 45 else 0
    nA = 3
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Track age- and set-size-dependent WM retrieval weight and lapse
        size_scale = 3.0 / float(nS)  # 1 (set size 3) vs 0.5 (set size 6)
        wm_retrieval = np.clip(wm_base * (L * size_scale) * (1.0 - 0.5 * age_group), 0.0, 1.0)
        lapse_eff = np.clip(lapse * (1.0 + 0.5 * age_group) * (1.0 + 0.5 * (nS - 3) / 3.0), 0.0, 0.99)
        
        # Keep a "last rewarded action" marker per state for stronger WM writes
        last_rewarded = -np.ones(nS, dtype=int)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12
            
            # WM policy: when last rewarded action is known, one-hot strong memory
            # Use stored WM matrix regardless; policy derived by softmax over w row.
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12
            
            # Mixture with WM retrieval and RL
            p_mix = wm_retrieval * p_wm + (1.0 - wm_retrieval) * p_rl
            # Lapse mixture to uniform
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += alpha_pos * pe
            else:
                q[s, a] += alpha_neg * pe
            
            # WM update: decay toward baseline slightly each visit; then strengthen on reward
            # Small passive decay to avoid needing extra parameter
            decay_local = 0.2 * (1.0 - L)  # more decay when L is low (poorer WM)
            w[s, :] = (1.0 - decay_local) * w[s, :] + decay_local * w0[s, :]
            if r > 0.5:
                last_rewarded[s] = a
                # Write a strong one-hot memory for rewarded action
                w[s, :] = 0.0
                w[s, a] = 1.0
            # If not rewarded, leave the decayed memory in place
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay + priority-gated WM updates based on surprise threshold, age/set-size modulated.
    
    Mechanism:
    - RL: single learning rate with value decay (forgetting) each visit; softmax with beta.
    - WM: updates only when absolute prediction error |delta| exceeds a threshold theta that increases
          with set size and age (priority gating). WM also decays toward baseline.
    - Mixture: WM weight scaled by set size and age.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_base: base WM mixture weight in [0,1].
    - theta0: base surprise threshold for WM updates (>=0).
    - theta_age: additive increase of threshold in older adults (>=0).
    - beta: RL inverse temperature (scaled internally by 10).
    - wm_decay: WM decay toward baseline per state visit in [0,1].
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as described.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, theta0, theta_age, beta, wm_decay = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0
    
    age_group = 1 if age[0] > 45 else 0
    nA = 3
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Mixture scaling by set size and age
        size_scale = 3.0 / float(nS)
        wm_weight_eff = np.clip(wm_base * size_scale * (1.0 - 0.4 * age_group), 0.0, 1.0)
        
        # Surprise threshold increases with set size and age
        theta = theta0 + theta_age * age_group + 0.2 * (nS - 3)  # higher threshold in larger set
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12
            
            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update with value decay (forgetting) toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # Apply local forgetting to the visited state's action values
            forget_rate = 0.1 * (nS / 6.0) * (1.0 + 0.5 * age_group)  # more forgetting for larger sets and older
            q[s, :] = (1.0 - forget_rate) * q[s, :] + forget_rate * (1.0 / nA)
            
            # WM decay for visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            # Priority-gated WM update: store only if surprise is high
            if abs(delta) >= theta:
                # Strong one-hot write if surprising
                w[s, :] = 0.0
                w[s, a] = 1.0
            # else: no WM write
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Parameter and manipulation notes
- Model 1: WM weight is explicitly reduced by larger set size (3/nS) and by age (1 - k_age*age_group), WM decays with wm_decay, and older adults show stronger perseveration.
- Model 2: WM retrieval weight depends on L, set size, and age; lapse increases with set size and age; RL uses dual learning rates to capture asymmetries often seen in older adults.
- Model 3: WM updates are priority-gated by a surprise threshold that increases with set size and with age; RL includes forgetting that scales with set size and age, capturing higher interference and poorer maintenance in older adults.