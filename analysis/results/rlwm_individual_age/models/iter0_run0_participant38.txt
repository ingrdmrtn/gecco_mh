Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways. Each function:

- Has the required signature and returns the negative log-likelihood of observed choices.
- Uses up to 6 parameters, all used meaningfully.
- Implements age effects (age_group = 1 for this 79-year-old).
- Implements set size effects (3 vs 6) within the WM policy and/or RL policy.
- Fills in the WM policy and update equations in the template structure.

Note: These functions assume numpy (as np) is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age- and set-size-dependent encoding, plus WM decay.

    Mixture model:
    - RL: standard delta-rule Q-learning with softmax policy.
    - WM: one-shot encoding of rewarded stimulus-action pairs, with retrieval probability
      limited by an effective capacity that depends on set size and age. WM decays toward
      uniform across trials within a block.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base mixture weight for WM (0..1)
    - softmax_beta: base inverse temperature for RL softmax (will be scaled up by x10)
    - K: WM capacity (in number of items; influences p_recall ~ min(1, K_eff / set_size))
    - decay: WM decay toward uniform per trial (0..1)
    - age_bias: fractional reduction of WM in older adults (0..1), applied to both effective K and wm_weight

    Age:
    - age_group = 1 for age > 45, 0 otherwise. For older adults, WM capacity and WM mixture weight
      are both reduced by (1 - age_bias).

    Set size:
    - Retrieval probability p_recall depends on set size via K_eff / set_size.

    Returns:
    - Negative log-likelihood of observed choices under the mixture policy.
    """
    lr, wm_weight, softmax_beta, K, decay, age_bias = model_parameters
    softmax_beta *= 10.0  # higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM readout

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Adjust WM parameters for age
    wm_weight_eff_global = wm_weight * (1.0 - age_group * age_bias)
    K_eff_global = K * (1.0 - age_group * age_bias)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (softmax probability of chosen action)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM retrieval probability determined by effective capacity and set size
            nS_now = float(nS)  # set size is constant within block here
            K_eff = max(0.0, K_eff_global)
            p_recall = min(1.0, K_eff / nS_now)

            # WM policy: mixture of WM readout and uniform (if not recalled)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_readout = 1.0 / denom_wm
            p_wm = p_recall * p_wm_readout + (1.0 - p_recall) * (1.0 / nA)

            # Mixture of WM and RL
            wm_weight_eff = np.clip(wm_weight_eff_global, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial
            w = (1.0 - decay) * w + decay * w_0

            # WM encoding after feedback: one-shot encoding upon reward, scaled by p_recall proxy p_enc
            # Use the same capacity-limited factor to control degree of encoding
            p_enc = p_recall
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with perseveration-like WM trace of recent choices; age reduces decision temperature.

    Mixture model:
    - RL: standard Q-learning with softmax policy; inverse temperature reduced in older adults.
    - WM: a choice-trace that boosts the most recently selected action for a state (regardless of reward).
      WM contribution is stronger for smaller set sizes (3 vs 6) via a set-size scaling.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled x10 internally)
    - perseveration: strength of choice-trace update into WM (0..1)
    - age_temp_bias: fractional reduction of softmax_beta for older adults (0..1)
    - wm_decay: WM decay toward uniform per trial (0..1)

    Age:
    - Older adults have lower effective RL inverse temperature: beta_eff = beta * (1 - age_temp_bias).

    Set size:
    - WM mixture weight is scaled by f_set = 3 / set_size (i.e., stronger for set size 3 than 6).

    Returns:
    - Negative log-likelihood of observed choices under the mixture policy.
    """
    lr, wm_weight, softmax_beta, perseveration, age_temp_bias, wm_decay = model_parameters
    softmax_beta *= 10.0  # base scaling

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age reduces decision temperature
    beta_eff_global = softmax_beta * (1.0 - age_group * age_temp_bias)
    beta_eff_global = max(beta_eff_global, 1e-6)

    softmax_beta_wm = 50.0  # deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling for WM mixture weight
        f_set = 3.0 / float(nS)  # 1.0 for set size 3, 0.5 for set size 6
        wm_weight_eff_global = np.clip(wm_weight * f_set, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with age-reduced beta
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff_global * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM choice-trace policy (perseveration): softmax over W_s
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff_global * p_wm + (1.0 - wm_weight_eff_global) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update as a choice trace (reward-independent perseveration)
            # Move W_s toward a one-hot at the chosen action by 'perseveration'
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - perseveration) * w[s, :] + perseveration * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM with interference (set-size- and age-dependent).

    Mixture model:
    - RL: Q-learning with separate learning rates for positive vs. negative prediction errors.
    - WM: one-shot encoding of rewarded stimulus-action pairs. WM retrieval is impaired by
      interference that scales with set size and is amplified by age.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1)
    - lr_neg: RL learning rate for negative PE (0..1)
    - wm_weight: WM mixture weight (0..1)
    - softmax_beta: base inverse temperature for RL (scaled x10 internally)
    - interference: base WM interference strength with set size (>=0)
    - age_bias: multiplicative age amplification of interference (>=0)

    Age:
    - Older adults have higher effective interference: inter_eff = interference * (1 + age_bias).

    Set size:
    - WM recall degrades with set size via p_recall = exp(- inter_eff * set_size).

    Returns:
    - Negative log-likelihood of observed choices under the mixture policy.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, interference, age_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age-amplified interference
    inter_eff_global = interference * (1.0 + age_group * age_bias)
    inter_eff_global = max(0.0, inter_eff_global)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM recall probability decreases exponentially with set size and age-amplified interference
            nS_now = float(nS)
            p_recall = np.exp(-inter_eff_global * nS_now)
            p_recall = np.clip(p_recall, 0.0, 1.0)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_readout = 1.0 / denom_wm
            p_wm = p_recall * p_wm_readout + (1.0 - p_recall) * (1.0 / nA)

            # Mixture
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0.0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM interference-driven decay toward uniform each trial
            # Stronger interference (via inter_eff_global and set size) implies stronger decay
            gamma = 1.0 - np.exp(-inter_eff_global * nS_now)
            gamma = np.clip(gamma, 0.0, 1.0)
            w = (1.0 - gamma) * w + gamma * w_0

            # WM encoding on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Encoding strength proportional to (1 - interference) proxy = (1 - gamma)
                enc_strength = 1.0 - gamma
                w[s, :] = (1.0 - enc_strength) * w[s, :] + enc_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p