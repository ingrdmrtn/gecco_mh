Below are three standalone cognitive models that implement RL+WM mechanisms tailored to the RLWM task. Each function:

- Accepts: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Returns: negative log-likelihood of observed choices
- Uses age group (0=young, 1=old) and set size (3 vs 6) meaningfully
- Fills in only the WM policy and WM updating parts, while preserving the rest of the template logic
- Uses â‰¤ 6 parameters and uses all of them meaningfully


def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM with load- and age-dependent encoding strength.
    
    Parameters (model_parameters):
    - lr: scalar, RL learning rate (0-1)
    - wm_weight: scalar, mixture weight between WM and RL policies (0-1), kept fixed by template
    - softmax_beta: scalar, RL inverse temperature before upscaling (template multiplies by 10)
    - wm_decay: scalar, per-visit decay of WM toward uniform (0-1)
    - wm_enc_base: scalar, baseline WM encoding strength (logit space)
    - age_wm: scalar, additive age effect on WM encoding (older=+age_wm in logit space)
    
    Model ideas:
    - RL: standard delta rule on Q(s,a).
    - WM: stores a distribution over actions per state w[s,:], decays toward uniform each visit.
    - WM encoding strength increases WM certainty toward the rewarded action when r=1.
    - WM encoding strength depends on set size and age: 
        enc_prob = sigmoid(wm_enc_base + age_wm*age_group + gamma*(3 - nS)), gamma=0.8
      So encoding is stronger in small set size (3) and for younger (if age_wm < 0), or vice versa.
    - WM policy: softmax over w[s,:] with an effective temperature scaled by encoding strength.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_enc_base, age_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound, per template
    
    # Age group: 0=young, 1=old
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic (template constant)
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_current = nS  # per block fixed

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (FILL IN)
            # Encoding strength depends on set size and age in logit space
            gamma = 0.8
            enc_logit = wm_enc_base + age_wm * age_group + gamma * (3 - nS_current)
            enc_prob = 1.0 / (1.0 + np.exp(-enc_logit))
            # Effective WM temperature scales with encoding probability
            beta_wm_eff = softmax_beta_wm * enc_prob
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture and likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (FILL IN)
            # 1) Decay the current state's WM distribution toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) Encode outcome: push mass toward rewarded action if r=1
            eta = enc_prob
            if r == 1:
                # Move w[s,:] toward one-hot on action a
                w[s, :] = (1.0 - eta) * w[s, :]
                w[s, a] += eta
            else:
                # Mild corrective shift away from the chosen action
                leak = eta / 3.0
                reduce = min(leak, w[s, a])
                w[s, a] -= reduce
                # Redistribute evenly to other actions
                inc_each = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += inc_each

            # Normalize to be safe
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM with decay and perseveration bias (age-modulated).
    
    Parameters (model_parameters):
    - lr_pos: scalar, RL learning rate for positive RPE (0-1)
    - lr_neg: scalar, RL learning rate for negative RPE (0-1)
    - wm_weight: scalar, mixture weight between WM and RL policies (0-1), kept fixed by template
    - softmax_beta: scalar, RL inverse temperature before upscaling (template multiplies by 10)
    - wm_decay: scalar, per-visit decay of WM toward uniform (0-1)
    - pers_base: scalar, baseline perseveration strength in WM policy
    
    Model ideas:
    - RL: separate learning rates for positive vs negative prediction errors.
    - WM: decays toward uniform; encodes rewarded action with moderate strength (fixed here).
    - Perseveration bias in WM policy: adds a bonus to the last chosen action in each state.
      The bias is stronger for older adults (multiplicative factor >1).
    - Load effect: when set size > capacity (K=3), WM is less certain (lower effective temperature).
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, pers_base = model_parameters
    softmax_beta *= 10  # per template
    
    # Age group: 0=young, 1=old
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12
    K_capacity = 3  # fixed WM capacity
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # track last chosen action per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_current = nS

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (FILL IN)
            # Base WM certainty reduced when set size exceeds capacity
            load_factor = 0.5 if nS_current > K_capacity else 1.0
            beta_wm_eff = softmax_beta_wm * load_factor

            # Perseveration bias; older participants amplify perseveration
            pers_factor = 1.5 if age_group == 1 else 1.0
            pers = pers_base * pers_factor

            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                W_s[last_action[s]] += pers  # add bias toward repeating last action

            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture and likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM update (FILL IN)
            # Decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Moderate encoding on reward: pull toward the chosen action if r=1
            eta = 0.6 if nS_current <= K_capacity else 0.3  # load-dependent encoding strength
            if r == 1:
                w[s, :] = (1.0 - eta) * w[s, :]
                w[s, a] += eta
            else:
                # Gentle push away from chosen action
                leak = eta / 4.0
                reduce = min(leak, w[s, a])
                w[s, a] -= reduce
                inc_each = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += inc_each

            # Normalize
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + event-based WM with time-dependent forgetting (set size and age modulate forgetting).
    
    Parameters (model_parameters):
    - lr: scalar, RL learning rate (0-1)
    - wm_weight: scalar, mixture weight between WM and RL policies (0-1), kept fixed by template
    - softmax_beta: scalar, RL inverse temperature before upscaling (template multiplies by 10)
    - forget_base: scalar, baseline forgetting logit (higher -> more forgetting)
    - forget_age: scalar, additive forgetting effect for older adults in logit space
    - forget_set: scalar, additive forgetting effect per +3 items (i.e., set size 6 vs 3)
    
    Model ideas:
    - WM stores the last rewarded action per state as a one-hot memory trace with strength 1 - f.
    - Forgetting probability f grows with time since encoding, set size, and age (logistic).
    - WM policy chooses according to the current WM distribution (noisy one-hot); 
      here we use the direct probability p_wm = W_s[a] (no softmax needed).
    - RL learns in parallel; mixture combines WM and RL as in the template.
    """
    lr, wm_weight, softmax_beta, forget_base, forget_age, forget_set = model_parameters
    softmax_beta *= 10  # per template
    
    # Age group: 0=young, 1=old
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # not used directly; WM policy is probability from the memory distribution
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM distribution per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last rewarded action per state and time since last encode
        last_rew_action = -1 * np.ones(nS, dtype=int)
        time_since_encode = np.full(nS, 1, dtype=float)  # start at 1 to avoid log(0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Increment time since encode for all states (time-driven forgetting)
            time_since_encode += 1.0

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (FILL IN)
            # Compute forgetting probability for this state based on time, age, and set size
            set_term = forget_set * (nS - 3)  # 0 for size=3, +forget_set*3 for size=6
            time_term = np.log(1.0 + time_since_encode[s])  # sublinear increase over time
            forget_logit = forget_base + forget_age * age_group + set_term + time_term
            f = 1.0 / (1.0 + np.exp(-forget_logit))  # forgetting probability in [0,1]

            # Build WM distribution: if we have a stored action, assign prob 1-f; else uniform
            if last_rew_action[s] >= 0:
                W_s = w_0[s, :].copy() * f
                W_s[last_rew_action[s]] += (1.0 - f)
            else:
                W_s = w_0[s, :].copy()

            # Probability of the chosen action under WM
            p_wm = max(W_s[a], eps)

            # Mixture likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (FILL IN)
            # If rewarded, encode this action as the last rewarded and reset its timer
            if r == 1:
                last_rew_action[s] = a
                time_since_encode[s] = 0.0

            # Keep w matrix synchronized with the current W_s belief for visited state
            w[s, :] = W_s.copy()
            # Normalize for safety
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p