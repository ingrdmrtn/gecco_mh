def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM reliance and WM decay.

    Idea:
    - Choices are driven by a mixture of incremental RL (Q-learning) and a one-shot Working Memory (WM).
    - WM reliance decreases with set size and with being older; WM contents decay toward uniform between trials.
    - RL uses a single learning rate and softmax choice rule.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight_base: base weight of WM in the policy mixture (will be modulated by set size and age).
    - softmax_beta: inverse temperature for RL softmax; internally scaled up by 10 as in the template.
    - wm_decay: decay parameter in [0,1] controlling how quickly WM decays toward uniform each trial.
    - k_set: non-negative capacity-scaling term; larger values reduce WM weight more strongly for set size 6.
    - age_penalty: non-negative penalty applied to WM weight for older group.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, k_set, age_penalty = model_parameters
    softmax_beta *= 10  # preserve template behavior

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL (q) and WM (w) values and uniform prior (w_0)
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM readout: softmax over WM values (deterministic)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Trial-level WM weight: down-weight with set size and age
            # - Capacity drop with set size via k_set
            # - Age penalty for older adults via age_penalty
            size_factor = 1.0 / (1.0 + k_set * max(nS - 3, 0))
            age_factor = 1.0 - age_penalty * age_group
            wm_weight_t = np.clip(wm_weight_base * size_factor * age_factor, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding: on rewarded trials, store the correct response for this state
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0  # one-shot storage of the rewarded action

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-specific learning rates + probabilistic WM encoding modulated by set size and age.
    Arbitration weight is state- and time-varying based on WM confidence, scaled by wm_weight.

    Idea:
    - RL has separate learning rates for positive vs. negative outcomes.
    - WM stores rewarded associations as a probabilistic update whose strength (p_store) decreases with set size and with being older.
    - WM confidence (deviation from uniform) determines the WM weight each trial; scaled by wm_weight parameter.
    - Final choice probability is a WM/RL mixture.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive reward (r=1).
    - wm_weight: global scale for WM arbitration; multiplies the trial-level WM confidence.
    - softmax_beta: inverse temperature for RL softmax; internally scaled up by 10.
    - lr_neg: RL learning rate for negative reward (r=0).
    - p_store_base: baseline WM encoding strength (0..1) applied on rewarded trials.
    - size_age_penalty: non-negative factor that reduces p_store with larger set size and with older age.

    Notes:
    - Effective p_store = clip(p_store_base - size_age_penalty*((nS-3)/3 + age_group), 0, 1).
    - WM update is a convex combination toward the rewarded action on r=1 trials:
      W_s <- (1 - p_store)*W_s + p_store*onehot(a); otherwise unchanged.
    - WM confidence = (max(W_s) - 1/nA)/(1 - 1/nA) in [0,1].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, wm_weight, softmax_beta, lr_neg, p_store_base, size_age_penalty = model_parameters
    softmax_beta *= 10  # preserve template behavior

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL (q) and WM (w) values and uniform prior (w_0)
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute encoding strength given set size and age
        size_component = max(nS - 3, 0) / 3.0
        p_store = p_store_base - size_age_penalty * (size_component + age_group)
        p_store = float(np.clip(p_store, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM readout prob for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # WM confidence-based arbitration (scaled by wm_weight parameter)
            conf = (np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA)
            conf = float(np.clip(conf, 0.0, 1.0))
            wm_weight_t = float(np.clip(wm_weight * conf, 0.0, 1.0))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update (valence-specific learning rates)
            if r > 0:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            # WM probabilistic encoding on rewarded trials
            if r > 0 and p_store > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - p_store) * w[s, :] + p_store * onehot
                # Ensure numerical stability: keep in [0,1] and normalized
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)
                if w[s, :].sum() > 0:
                    w[s, :] = w[s, :] / w[s, :].sum()
            # No explicit decay in this model

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration, WM decay, and a perseveration bias.
    Age reduces choice precision; set size increases WM noise via arbitration.

    Idea:
    - RL update with single learning rate.
    - WM is a one-shot store that decays toward uniform between visits.
    - Arbitration weight uses WM certainty (1 - normalized entropy) scaled by wm_weight parameter.
    - Perseveration adds a tendency to repeat the most recent action (state-independent).
    - Older age reduces effective inverse temperature (more stochastic choices).

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: scales the influence of WM in arbitration and implicitly WM noise (higher wm_weight -> more WM reliance).
    - softmax_beta: RL inverse temperature baseline; internally scaled up by 10, then reduced by age effect.
    - perseveration_beta: strength of perseveration bias; transformed into a mixing weight.
    - wm_decay: WM decay toward uniform (0..1) applied each time a state is visited.
    - age_beta_delta: non-negative factor reducing choice precision for older adults.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, perseveration_beta, wm_decay, age_beta_delta = model_parameters
    softmax_beta *= 10  # baseline inverse temperature

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age reduces effective beta
    beta_eff_scale = 1.0 / (1.0 + age_beta_delta * age_group)
    softmax_beta *= beta_eff_scale

    softmax_beta_wm = 50  # deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL (q) and WM (w) values and uniform prior (w_0)
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        last_action = None  # track global perseveration (state-independent)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM "noise" induced by set size and age, inversely related to wm_weight
            # Larger set sizes and older age increase effective WM noise
            size_term = max(nS - 3, 0) / 3.0
            wm_noise = np.clip((1.0 - wm_weight) * (size_term + 0.1 * age_group), 0.0, 0.99)

            # Noisy WM readout: mixture with uniform before softmax
            W_read = (1.0 - wm_noise) * W_s + wm_noise * w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_read - W_read[a])))
            p_wm = 1.0 / denom_wm

            # Arbitration weight from WM certainty (1 - normalized entropy), scaled by wm_weight
            eps = 1e-12
            H = -np.sum(np.clip(W_read, eps, 1.0) * np.log(np.clip(W_read, eps, 1.0)))
            H_max = np.log(nA)
            wm_certainty = np.clip(1.0 - H / H_max, 0.0, 1.0)
            wm_weight_t = float(np.clip(wm_weight * wm_certainty, 0.0, 1.0))

            # Base mixture
            p_mix = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl

            # Perseveration distribution: bias to repeat last action
            if last_action is None:
                p_prev = 1.0 / nA  # uniform on first trial
            else:
                # Softmax over a bias vector where last_action gets bonus 1, others 0, scaled by perseveration_beta
                beta_prev = 10.0 * perseveration_beta
                # Probability of chosen action under this perseveration policy:
                if a == last_action:
                    num = np.exp(beta_prev * 1.0)
                    denom = np.exp(beta_prev * 1.0) + (nA - 1) * np.exp(0.0)
                else:
                    num = np.exp(0.0)
                    denom = np.exp(beta_prev * 1.0) + (nA - 1) * np.exp(0.0)
                p_prev = num / denom

            # Mix perseveration with task-driven policy
            k_persev = 1.0 - np.exp(-max(perseveration_beta, 0.0))
            p_total = (1.0 - k_persev) * p_mix + k_persev * p_prev
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - Q_s[a])

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on rewarded trials (one-shot store)
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

            # Update last action for perseveration
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p