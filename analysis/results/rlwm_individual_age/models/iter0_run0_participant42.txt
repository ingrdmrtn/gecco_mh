Below are three standalone cognitive models that implement RL+WM mixtures with age and set-size effects. Each returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported. All parameters are used and do not exceed 6 per model.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age-dependent WM down-weighting and reward-gated WM overwrite.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action at each trial (0,1,2).
    rewards : array-like of int
        Binary reward feedback at each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the current block on each trial (3 or 6).
    age : array-like (single value repeated)
        Participant age; age_group is 0 if <=45, 1 if >45.
    model_parameters : list or array
        [lr, wm_w_small, wm_w_large, softmax_beta, phi, gamma_age]
        - lr: RL learning rate (0..1)
        - wm_w_small: WM mixture weight for set size 3
        - wm_w_large: WM mixture weight for set size 6
        - softmax_beta: base RL inverse temperature; internally scaled up
        - phi: WM maintenance (decay) parameter; higher means more persistent WM (0..1)
        - gamma_age: age penalty (0..1) reducing WM weight and RL inverse temperature for older participants

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_w_small, wm_w_large, softmax_beta, phi, gamma_age = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for beta

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # WM policy is highly deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Age-scaled RL inverse temperature
            beta_eff = softmax_beta * (1.0 - 0.5 * gamma_age * age_group)
            beta_eff = max(1e-6, beta_eff)

            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (softmax over WM weights)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Set-size dependent WM mixture + age penalty
            wm_base = wm_w_small if nS == 3 else wm_w_large
            wm_weight = wm_base * (1.0 - gamma_age * age_group)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward baseline, then reward-gated overwrite
            w[s, :] = (1.0 - phi) * w_0[s, :] + phi * w[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with separate learning rates for positive/negative PEs + WM capacity limit and lapses.
    WM weight scales with an effective capacity K (reduced by age) relative to set size.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like (single value repeated)
    model_parameters : list or array
        [alpha_pos, alpha_neg, softmax_beta, K_base, age_penalty, epsilon_lapse]
        - alpha_pos: RL learning rate for positive prediction errors (0..1)
        - alpha_neg: RL learning rate for negative prediction errors (0..1)
        - softmax_beta: RL inverse temperature (scaled up internally)
        - K_base: baseline WM capacity (1..6)
        - age_penalty: reduction in WM capacity for older participants (0..3)
        - epsilon_lapse: lapse rate mixed with uniform policy, scaled by set size (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, softmax_beta, K_base, age_penalty, epsilon_lapse = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and WM weight
        K_eff = max(1.0, K_base - age_penalty * age_group)
        wm_weight = min(1.0, K_eff / float(nS))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Lapse scaled by set size (more lapses with larger sets)
            epsilon_eff = np.clip(epsilon_lapse * (nS / 6.0), 0.0, 1.0)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon_eff) * p_mix + epsilon_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update: decay; reward-gated strengthening (overwrite to one-hot on reward)
            w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]  # mild decay toward baseline
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with stickiness (choice perseveration), set-sizeâ€“scaled exploration, and WM noise.
    Older age increases perseveration and WM noise, reduces WM maintenance.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like (single value repeated)
    model_parameters : list or array
        [lr, softmax_beta, kappa_stick, wm_phi, wm_noise, age_bias]
        - lr: RL learning rate (0..1)
        - softmax_beta: base RL inverse temperature (scaled up internally)
        - kappa_stick: stickiness bias added to previous action's value
        - wm_phi: WM maintenance (0..1), reduced by age
        - wm_noise: baseline WM retrieval noise (0..1), inflated by set size and age
        - age_bias: scales age effects on multiple components (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, kappa_stick, wm_phi, wm_noise, age_bias = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        prev_action = None
        log_p = 0.0

        # Set-size scaled exploration: lower beta for larger sets; age reduces beta further
        beta_scale_set = 3.0 / float(nS)  # 1.0 for set size 3, 0.5 for set size 6
        beta_scale_age = (1.0 - 0.3 * age_bias * age_group)
        beta_eff = max(1e-6, softmax_beta * beta_scale_set * beta_scale_age)

        # WM mixture weight also reduced with set size and age
        wm_weight = np.clip((3.0 / float(nS)) * (1.0 - 0.5 * age_bias * age_group), 0.0, 1.0)

        # Age-adjusted WM maintenance and noise
        wm_phi_eff = np.clip(wm_phi * (1.0 - 0.5 * age_bias * age_group), 0.0, 1.0)
        wm_noise_eff_base = np.clip(wm_noise * (float(nS) / 3.0) * (1.0 + age_bias * age_group), 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add stickiness bias to RL values
            bias = np.zeros(nA)
            if prev_action is not None:
                kappa_eff = kappa_stick * (1.0 + age_bias * age_group)
                bias[prev_action] += kappa_eff
            Q_s_biased = Q_s + bias

            # RL policy with effective beta
            denom_rl = np.sum(np.exp(beta_eff * (Q_s_biased - Q_s_biased[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: noisy retrieval of the modal WM action
            W_s = w[s, :]
            m = int(np.argmax(W_s))
            # Probability of choosing the modal action vs uniform noise
            noise = wm_noise_eff_base
            p_wm = (1.0 - noise) if a == m else 0.0
            p_wm += noise * (1.0 / nA)
            p_wm = np.clip(p_wm, 1e-12, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay toward baseline, then overwrite on reward
            w[s, :] = (1.0 - wm_phi_eff) * w_0[s, :] + wm_phi_eff * w[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes:
- Model 1: Simple RL+WM mixture; WM weight directly depends on set size (two weights) and is down-weighted by age. WM is persistent (phi) and overwritten on reward.
- Model 2: RL uses asymmetric learning rates; WM mixture derives from a capacity K that is reduced by age and diluted by set size. A lapse rate increases with set size.
- Model 3: Adds choice perseveration (stickiness) to RL values, reduces exploration temperature for larger set sizes, and implements a noisy WM retrieval that worsens with set size and age; WM maintenance decreases with age.