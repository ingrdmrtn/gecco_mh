def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+capacity-limited Working Memory (WM) mixture model with age- and set-size-dependent WM availability.

    Description:
    - Choices result from a mixture of a reinforcement-learner (RL) softmax policy and a capacity-limited WM policy.
    - WM is modeled as a rapidly updated, near-deterministic store for state-action associations.
    - WM availability scales with effective capacity and set size, and is reduced for older adults.
    - RL updates values via a standard delta rule.
    - WM decays toward uniform over trials and is overwritten by rewarded associations.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Base mixture weight for WM versus RL (0-1) before availability scaling.
    - softmax_beta: Inverse temperature for RL softmax; internally scaled up for identifiability.
    - wm_capacity: Effective WM capacity in items (0-6). Controls availability as capacity/set_size.
    - wm_decay: Per-trial decay of WM toward uniform (0-1).
    - age_wm_drop: Proportional WM capacity reduction in the older group (0-1). Effective capacity = wm_capacity*(1 - age_group*age_wm_drop).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen action indices per trial (0..2).
    - rewards: array of rewards (0/1) per trial.
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array with a single repeated value for participant's age.
    - model_parameters: list/array with the 6 parameters described above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    softmax_beta *= 10.0

    # Additional parameters
    wm_capacity, wm_decay, age_wm_drop = model_parameters[3:6]

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0
    eps = 1e-12

    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: probabilistic association table per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # Effective WM availability scales with capacity, set size, and age
            eff_capacity = max(0.0, wm_capacity * (1.0 - age_group * age_wm_drop))
            availability = min(1.0, eff_capacity / max(1, set_size))

            # RL policy probability of chosen action
            Q_s = q[s, :].copy()
            # softmax probability of the chosen action 'a'
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights
            W_s = w[s, :].copy()
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Blend with uniform if item unlikely to be in WM (availability)
            p_wm = availability * p_wm_det + (1.0 - availability) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM write: if rewarded, write strong one-hot; if not, weaker write
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = one_hot
            else:
                # partial, weaker imprint when not rewarded
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with lapse and perseveration, modulated by set size and age.

    Description:
    - Choices derive from a WM-RL mixture, then pass through a lapse process.
    - RL includes a perseveration bias: a state-specific memory of the last chosen action biases Q.
    - Lapse rate increases with set size and with age (older adults show more lapses).
    - WM stores the last rewarded action per state (near-deterministic), with light decay.
    - RL updates with a single learning rate.

    Parameters (model_parameters):
    - lr: RL learning rate (0-1).
    - wm_weight: Base WM mixture weight (0-1).
    - softmax_beta: Base inverse temperature for RL; scaled internally.
    - lapse_base: Baseline lapse probability (0-1), scaled by set size and age.
    - perseveration_strength: Strength of perseveration bias added to Q (>=0).
    - age_temp_drop: Multiplicative reduction of RL inverse temperature in older adults (0-1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial.
    - model_parameters: list/array with the 6 parameters above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    softmax_beta *= 10.0

    lapse_base, perseveration_strength, age_temp_drop = model_parameters[3:6]

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM: store last rewarded action per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Perseveration trace: last chosen action per state (one-hot), with decay
        last_choice = (1.0 / nA) * np.ones((nS, nA))
        persev_decay = 0.1  # fixed light decay to avoid adding extra parameters

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # Adjust RL inverse temperature for age (older: lower temperature)
            beta_eff = softmax_beta * (1.0 - age_group * age_temp_drop)

            # Perseveration-biased Q
            bias = last_choice[s, :] - (1.0 / nA)
            Q_s = q[s, :] + perseveration_strength * bias

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy: last rewarded action per state (deterministic), with slight decay
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture prior to lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Lapse probability scales with set size and age
            size_factor = set_size / 6.0  # 0.5 for set size 3, 1.0 for size 6
            lapse = lapse_base * (0.5 + 0.5 * size_factor) * (1.0 + 0.5 * age_group)
            # Cap lapse to [0, 0.99] to avoid degeneracy
            lapse = min(max(lapse, 0.0), 0.99)

            # Final choice probability with lapse to uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update perseveration trace: decay and set last choice for this state
            last_choice = (1.0 - persev_decay) * last_choice + persev_decay * w_0
            one_hot = np.zeros(nA); one_hot[a] = 1.0
            last_choice[s, :] = one_hot

            # WM decay toward uniform, then write last rewarded action
            w = 0.1 * w_0 + 0.9 * w
            if r > 0.5:
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + gated WM, with age- and set-size-dependent gating.

    Description:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM is written via a gating mechanism that depends on set size: more likely to gate-in for small sets.
    - Older adults have reduced WM gating efficacy.
    - Choice policy is a mixture of gated WM and RL.
    - WM is near-deterministic when gated-in; otherwise contributes little (uniform).

    Parameters (model_parameters):
    - lr: Positive RL learning rate (0-1).
    - wm_weight: Base weight on WM in the mixture (0-1).
    - softmax_beta: RL inverse temperature; internally scaled.
    - lr_neg: Negative RL learning rate (0-1) for negative prediction errors.
    - gate_strength: Controls sensitivity of WM gating to set size (higher = steeper decline with larger set size).
    - age_wm_weight_drop: Proportional reduction of WM gating efficacy in older adults (0-1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial.
    - model_parameters: list/array with the 6 parameters above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    softmax_beta *= 10.0

    lr_neg, gate_strength, age_wm_weight_drop = model_parameters[3:6]

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # RL policy with current Q
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM gating probability as logistic of "capacity margin"
            # margin > 0 for small sets (easier to gate), < 0 for large sets
            # Center around 4 items as a nominal WM span; steeper with gate_strength.
            margin = (4.0 - set_size)
            gate_input = gate_strength * margin
            gate_prob = 1.0 / (1.0 + np.exp(-gate_input))
            # Age reduces gating efficacy
            gate_prob *= (1.0 - age_group * age_wm_weight_drop)
            gate_prob = min(max(gate_prob, 0.0), 1.0)

            # WM policy: if gated, near-deterministic softmax; else uniform
            W_s = w[s, :].copy()
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = gate_prob * p_wm_det + (1.0 - gate_prob) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM maintenance: light decay; write when reward indicates correct mapping
            w = 0.1 * w_0 + 0.9 * w
            if r > 0.5:
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                # Write strength also depends on gate probability (if not gated, write less)
                w[s, :] = gate_prob * one_hot + (1.0 - gate_prob) * w[s, :]

        blocks_log_p += log_p

    return -blocks_log_p