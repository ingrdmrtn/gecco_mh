Below are three standalone cognitive models that implement plausible RL+WM mechanisms for this RLWM task, with explicit age-group and set-size effects. Each function:

- Follows the requested signature: (states, actions, rewards, blocks, set_sizes, age, model_parameters)
- Returns the negative log-likelihood of the observed choices
- Uses all provided parameters meaningfully (≤ 6 parameters per model)
- Includes age-group (0=young, 1=old) and set-size (3 vs 6) modulations
- Implements the missing policy and update equations for the WM component in the provided template spirit, without importing any packages (assumes numpy as np is already available)

Model 1: RL + capacity-limited WM mixture (age- and set-size scaled WM weight)
- Idea: A simple mixture of RL and WM policies. WM is a perfect one-shot store for rewarded associations (no decay), but its contribution to choice is capacity-limited and reduced with larger set size and in older adults.
- Parameters (6):
  - lr: RL learning rate
  - wm_weight_base: base WM weight (mixture weight)
  - softmax_beta: RL softmax inverse temperature (scaled internally by 10)
  - cap_base: base WM capacity parameter (in slots)
  - cap_old_delta: capacity decrement if age_group=1 (old)
  - mix_temp: temperature sharpening for the WM mixture weighting across set sizes (controls how strongly capacity/nS scales the WM weight; higher means closer to hard cap)

Docstring in code explains details.

Model 2: RL with asymmetric learning + leaky WM + age-related lapse
- Idea: WM trace decays toward uniform each trial (leak), and rewarded trials overwrite WM for that state. RL has separate learning rates for positive and negative outcomes. Older adults have additional lapse probability, and WM weight is reduced with larger set size.
- Parameters (6):
  - alpha_pos: RL learning rate for rewards
  - alpha_neg: RL learning rate for non-rewards
  - softmax_beta: RL inverse temperature (scaled internally by 10)
  - wm_weight_base: base WM weight
  - wm_leak: WM decay rate toward uniform (per trial)
  - lapse_old: lapse probability applied only if age_group=1 (else 0)

Model 3: RL + WM with uncertainty-based arbitration and age-related perseveration
- Idea: Arbitration weight for WM vs RL is governed by a logistic function of set size and age (proxy for uncertainty/cognitive load). RL and WM policies both include a stickiness bias to repeat the previous action within the same state; this bias is stronger in older adults. WM has leak and reward-based overwrite.
- Parameters (6):
  - lr: RL learning rate
  - softmax_beta: base inverse temperature for RL (scaled internally by 10)
  - wm_w0: base bias term for arbitration toward WM
  - gamma_set: sensitivity of arbitration to set size (more load → less WM)
  - age_penalty: reduction in WM arbitration for older adults
  - kappa_age: stickiness gain that is applied only for older adults

Code for all three models:

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture (no WM decay; rewarded trials write perfect WM for that state).
    
    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if < 45, 1 otherwise.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, cap_base, cap_old_delta, mix_temp]
        - lr: RL learning rate (0..1)
        - wm_weight_base: base WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - cap_base: baseline WM capacity (in “slots”)
        - cap_old_delta: reduction in capacity if age_group=1
        - mix_temp: temperature for scaling WM weight with capacity/nS
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, cap_base, cap_old_delta, mix_temp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy
    
    # Age group
    age_group = 1 if age[0] > 45 else 0
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Initialize RL Q-values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM policy proxy
        
        # Effective WM weight depends on capacity and set size and age
        capacity = cap_base - age_group * cap_old_delta
        capacity = max(0.0, capacity)
        # Smooth scaling of weight by effective capacity ratio
        # wm_scale in [0,1], sharper approach to cap limit with higher mix_temp
        cap_ratio = capacity / float(nS)
        cap_ratio = max(0.0, min(1.0, cap_ratio))
        wm_scale = 1.0 / (1.0 + np.exp(-mix_temp * (cap_ratio - 0.5)))
        wm_weight_eff = wm_weight_base * wm_scale
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # RL policy
            Q_s = q[s, :]
            Q_s_centered = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Q_s_centered)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]
            
            # WM policy: softmax over WM table (which stores a peaked one-hot after reward)
            W_s = w[s, :]
            W_s_centered = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * W_s_centered)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update: on rewarded trials, perfectly store the chosen action for that state
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # no decay in Model 1
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + leaky WM + age-related lapse.
    
    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if < 45, 1 otherwise.
    model_parameters : list or array
        [alpha_pos, alpha_neg, softmax_beta, wm_weight_base, wm_leak, lapse_old]
        - alpha_pos: RL learning rate for rewarded trials
        - alpha_neg: RL learning rate for non-rewarded trials
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_weight_base: base WM mixture weight
        - wm_leak: WM decay toward uniform at each trial (0..1)
        - lapse_old: lapse probability applied only if age_group=1 (else 0)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, wm_leak, lapse_old = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    
    age_group = 1 if age[0] > 45 else 0
    lapse = lapse_old if age_group == 1 else 0.0
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        
        # Reduce WM weight as set size increases (simple inverse scaling)
        wm_weight_eff = wm_weight_base * (3.0 / float(nS))
        wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # RL policy
            Q_s = q[s, :]
            Q_s_centered = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Q_s_centered)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]
            
            # WM policy
            W_s = w[s, :]
            W_s_centered = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * W_s_centered)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]
            
            # Mixture plus lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update with asymmetric learning rates
            if r > 0.0:
                q[s, a] += alpha_pos * (r - Q_s[a])
            else:
                q[s, a] += alpha_neg * (r - Q_s[a])
            
            # WM decay toward uniform, then overwrite on reward
            # Decay for the entire WM table (state-independent decay)
            w = (1.0 - wm_leak) * w + wm_leak * (1.0 / nA)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty/load-based arbitration and age-related perseveration.
    
    Arbitration is a logistic function of set size and age; stickiness bias (perseveration)
    applies to both RL and WM policies for older adults. WM leaks toward uniform and is
    overwritten on rewarded trials.
    
    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if < 45, 1 otherwise.
    model_parameters : list or array
        [lr, softmax_beta, wm_w0, gamma_set, age_penalty, kappa_age]
        - lr: RL learning rate
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_w0: base arbitration bias toward WM (logit space)
        - gamma_set: arbitration sensitivity to set size (higher nS -> less WM if negative)
        - age_penalty: penalty applied to WM arbitration for older adults
        - kappa_age: stickiness (repeat-last-action-in-same-state) applied only if age_group=1
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_w0, gamma_set, age_penalty, kappa_age = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    
    age_group = 1 if age[0] > 45 else 0
    kappa = kappa_age if age_group == 1 else 0.0
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        
        # Track previous action per state for stickiness
        prev_action = -1 * np.ones(nS, dtype=int)
        
        # Arbitration weight: sigmoid of base + set-size + age penalty
        # Larger set sizes reduce WM influence if gamma_set < 0
        logit_w = wm_w0 + gamma_set * (3.0 - float(nS)) - age_penalty * age_group
        wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))
        wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            # Build stickiness bias vector for this state: 1 for prev action, 0 otherwise
            bias_vec = np.zeros(nA)
            if prev_action[s] >= 0:
                bias_vec[prev_action[s]] = 1.0
            
            # RL policy with stickiness
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s)) + kappa * bias_vec
            exp_rl = np.exp(prefs_rl - np.max(prefs_rl))
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]
            
            # WM policy with stickiness
            W_s = w[s, :]
            prefs_wm = softmax_beta_wm * (W_s - np.max(W_s)) + kappa * bias_vec
            exp_wm = np.exp(prefs_wm - np.max(prefs_wm))
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM leak toward uniform; then reward-based overwrite
            w = 0.95 * w + 0.05 * (1.0 / nA)  # mild global leak (fixed small leak to avoid an extra parameter)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            
            # Update stickiness memory
            prev_action[s] = a
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Notes on age and set-size impacts
- Model 1: WM contribution is scaled by an effective capacity that is reduced for older adults and for larger set sizes.
- Model 2: Older adults have a lapse component; WM contribution decays with set size; WM has leaky maintenance.
- Model 3: Arbitration favoring WM is reduced for larger set sizes and older adults; older adults exhibit stronger perseveration (stickiness).

All three models can be fit to the participant’s data by minimizing the returned negative log-likelihood with respect to the model parameters.