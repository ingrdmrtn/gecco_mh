def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-sensitive WM weighting and age modulation.

    Core idea:
    - Choices are generated by a mixture of model-free RL and a simple working memory (WM) store.
    - WM has strong, near-deterministic policy when it contains a clear association.
    - The weight on WM decreases with set size and increases for older adults (age_group=1) via a dedicated parameter.

    Parameters (model_parameters):
    - lr: scalar, RL learning rate for Q-values.
    - wm_weight: base mixture weight on WM (0..1).
    - softmax_beta: inverse temperature for RL policy (internally scaled up by 10).
    - phi_base: per-trial decay of WM toward uniform (capacity/maintenance cost).
    - wm_setsize_slope: how strongly larger set size (6 vs 3) reduces WM contribution.
    - wm_boost_old: additive boost to WM mixture weight for older adults (age_group=1).

    Age and set-size effects:
    - Age group (0=young, 1=old) increases WM mixture via wm_boost_old.
    - WM weight decreases as set size increases via wm_setsize_slope.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, phi_base, wm_setsize_slope, wm_boost_old = model_parameters
    softmax_beta *= 10.0  # as specified
    softmax_beta_wm = 50.0  # very deterministic WM as specified

    # Age group encoding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight per block (monotonic with set size and age)
        wm_weight_eff = wm_weight + wm_boost_old * age_group - wm_setsize_slope * ((nS - 3.0) / 3.0)
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of the chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WORKING MEMORY POLICY (near-deterministic softmax on WM values)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WORKING MEMORY UPDATE:
            # 1) Decay WM toward uniform (capacity/maintenance load)
            w[s, :] = (1.0 - phi_base) * w[s, :] + phi_base * w_0[s, :]
            # 2) If rewarded, store a one-hot "correct" action in WM
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning and perseveration bias.
    - Two RL learning rates: lr_pos for rewards, lr_neg for no reward.
    - Perseveration bias (stickiness) is added to RL values and is stronger in older adults.
    - WM contribution decays with set size and WM contents decay toward uniform.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive outcomes (reward=1).
    - lr_neg: RL learning rate for negative outcomes (reward=0).
    - wm_weight: base WM mixture weight (scaled by set size).
    - softmax_beta: inverse temperature for RL policy (internally scaled by 10).
    - persev_beta: strength of perseveration bias added to RL values.
    - wm_decay: per-trial decay of WM values toward uniform (also used to lightly decay perseveration trace).

    Age and set-size effects:
    - Perseveration is stronger for older adults: persev_beta_eff = persev_beta * (1 + 0.5*age_group).
    - WM mixture weight scales down with larger set sizes: wm_weight_eff = wm_weight * (3/nS).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, persev_beta, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Perseveration trace: last-choice marker per state
        h = np.zeros((nS, nA))

        # Age-modulated perseveration
        persev_beta_eff = persev_beta * (1.0 + 0.5 * age_group)
        # Set size scaling of WM mixture
        wm_weight_eff = np.clip(wm_weight * (3.0 / nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Decay perseveration trace slightly each trial
            h[s, :] *= (1.0 - 0.5 * wm_decay)

            Q_s = q[s, :] + persev_beta_eff * h[s, :]
            W_s = w[s, :]

            # RL policy with perseveration
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL asymmetric update
            if r > 0.0:
                q[s, a] += lr_pos * (r - (q[s, a] - persev_beta_eff * h[s, a]))  # update toward reward
            else:
                q[s, a] += lr_neg * (r - (q[s, a] - persev_beta_eff * h[s, a]))

            # Update perseveration: mark chosen action for this state
            h[s, :] *= 0.0
            h[s, a] = 1.0

            # WM update: decay toward uniform; if rewarded, store one-hot
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with dynamic, reliability-based gating.
    - WM vs RL weight adjusts on each trial based on prediction error magnitude (PE) from RL
      and on block factors (set size, age).
    - Large RL prediction errors shift weight toward WM (trust WM more when RL is unreliable).
    - WM stores rewarded action one-hot with decay to uniform.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values.
    - wm_weight0: baseline gating bias toward WM (pre-sigmoid).
    - softmax_beta: inverse temperature for RL policy (internally scaled by 10).
    - wm_noise: sensitivity of the gate to RL prediction error magnitude (higher -> more WM when PE is large).
    - setsize_gain: increases WM gate for smaller set sizes (positive values favor WM at nS=3).
    - age_bias: additive bias toward WM for older adults (age_group=1) in the gating variable.

    Age and set-size effects:
    - Age increases WM reliance via age_bias.
    - Smaller set size increases WM reliance via setsize_gain * (3 - nS).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_noise, setsize_gain, age_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track last RL PE magnitude per state for gating
        last_abs_pe = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Dynamic gating: combine baseline, age, set size, and recent RL unreliability (|PE|)
            gate_lin = (
                wm_weight0
                + age_bias * age_group
                + setsize_gain * (3.0 - nS)
                + wm_noise * last_abs_pe[s]
            )
            wm_weight_eff = sigmoid(gate_lin)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and compute PE magnitude for next gating
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            last_abs_pe[s] = abs(pe)

            # WM update: mild decay toward uniform; rewarded actions are cached one-hot
            # Use a soft decay linked to wm_noise (higher wm_noise -> slightly more decay)
            decay = 1.0 / (1.0 + np.exp(-wm_noise))  # in (0.5, 1) for wm_noise>0
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - 0.2 * decay) * w[s, :] + 0.2 * decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p