Below are three standalone cognitive models that implement RL+WM mechanisms with age- and set-size-dependent effects. Each returns the negative log-likelihood of the observed choices.

Note: These functions assume numpy as np is already imported (no imports included by design).

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and age effect on capacity.
    - Working memory weight is downscaled when set size exceeds capacity.
    - Older adults have reduced effective WM capacity.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..nA-1).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size of the block for each trial (e.g., 3 or 6).
    age : array-like of float
        Age of participant (single value repeated). Used to derive age group.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, capacity_K]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM mixture weight (0..1)
        - softmax_beta: inverse temperature for RL policy (scaled up internally)
        - wm_decay: WM update strength toward current target (0..1)
        - capacity_K: WM capacity in number of items (>=1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, capacity_K = model_parameters
    softmax_beta *= 10.0  # higher upper bound scaling
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    age_group = 1 if age[0] > 45 else 0
    # Age reduces effective capacity (older adults)
    capacity_age_factor = 0.8 if age_group == 1 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity and mixture weight for this block
        K_eff = max(1.0, capacity_K * capacity_age_factor)
        capacity_scale = min(1.0, K_eff / float(nS))
        wm_weight_block = np.clip(wm_weight_base * capacity_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy for chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # Move WM distribution toward a "target" based on current feedback:
            # If rewarded, target is one-hot on chosen action; if not, push mass away from chosen action.
            target = w_0[s, :].copy()
            if r > 0.5:
                target[:] = 0.0
                target[a] = 1.0
            else:
                # Penalize chosen action modestly; redistribute to others
                target[:] = 1.0 / (nA - 1)
                target[a] = 0.0

            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + one-shot WM with forgetting.
    - WM stores the last rewarded action per state via replacement and decays toward uniform.
    - WM weight declines with larger set sizes; older adults forget faster.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group.
    model_parameters : list or array
        [alpha_pos, alpha_neg, softmax_beta, wm_weight_base, phi_forget]
        - alpha_pos: RL learning rate for rewards
        - alpha_neg: RL learning rate for non-rewards
        - softmax_beta: inverse temperature for RL (scaled internally)
        - wm_weight_base: baseline WM mixture weight
        - phi_forget: WM forgetting rate toward uniform (0..1); larger = faster forgetting

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, phi_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0
    # Older adults forget faster in WM
    age_forget_factor = 1.3 if age_group == 1 else 1.0
    phi_forget_age = np.clip(phi_forget * age_forget_factor, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM representation: track a point mass on last rewarded action, else uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling for WM mixture
        wm_size_scale = 3.0 / float(nS)  # 1 for set size 3, 0.5 for set size 6
        wm_weight_block = np.clip(wm_weight_base * wm_size_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy prob of chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy prob of chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta > 0 else alpha_neg
            q[s, a] += alpha * delta

            # WM update: one-shot replacement on reward; otherwise decay toward uniform
            # Forgetting toward uniform at each trial for this state
            w[s, :] = (1.0 - phi_forget_age) * w[s, :] + phi_forget_age * w_0[s, :]

            if r > 0.5:
                # If rewarded, store chosen action deterministically
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay and uncertainty-gated WM mixture; age reduces WM influence.
    - WM mixture weight increases when RL is uncertain (high entropy of Q-values).
    - WM influence is downscaled for larger set sizes and for older adults.
    - RL includes value decay (forgetting).

    Parameters
    ----------
    states : array-like of int
        State index each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size for each trial's block.
    age : array-like of float
        Participant age (single repeated value). Used to derive age group.
    model_parameters : list or array
        [lr, softmax_beta, wm_weight_base, wm_lr, rl_decay, gating_strength]
        - lr: RL learning rate
        - softmax_beta: RL inverse temperature (scaled internally)
        - wm_weight_base: baseline WM weight
        - wm_lr: WM learning rate toward current target (0..1)
        - rl_decay: decay rate of Q toward uniform baseline (0..1)
        - gating_strength: scales sensitivity of WM gating to RL entropy

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_lr, rl_decay, gating_strength = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0
    # Age reduces WM influence
    age_wm_factor = 0.7 if age_group == 1 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        q_0 = (1.0 / nA) * np.ones((nS, nA))

        # Base WM weight adjusted by set size and age
        size_scale = 3.0 / float(nS)
        wm_weight_base_blk = np.clip(wm_weight_base * size_scale * age_wm_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax policy prob for chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # RL uncertainty (entropy of softmax over Q)
            # Compute full softmax distribution to get entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            entropy = -np.sum(pi * np.log(np.clip(pi, 1e-12, 1.0)))
            # Max entropy for nA actions
            max_entropy = np.log(nA)
            # Gate WM by normalized entropy, transformed by sigmoid
            norm_H = entropy / max_entropy  # in [0,1]
            wm_gate = 1.0 / (1.0 + np.exp(-gating_strength * (norm_H - 0.5)))
            wm_weight = np.clip(wm_weight_base_blk * wm_gate, 0.0, 1.0)

            # WM policy prob for chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward prior
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply decay on all actions for this state
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * q_0[s, :]

            # WM update: track current outcome with learning rate wm_lr
            target = w_0[s, :].copy()
            if r > 0.5:
                target[:] = 0.0
                target[a] = 1.0
            else:
                # Mild negative update: shift mass away from chosen action
                target[:] = 1.0 / (nA - 1)
                target[a] = 0.0

            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size effects:
- Model 1: WM capacity (K) is reduced for older adults and WM mixture weight scales as min(1, K_eff / set_size).
- Model 2: Older adults have faster WM forgetting (phi_forget scaled up). WM weight scales as 3/set_size.
- Model 3: WM weight is uncertainty-gated, multiplied by 3/set_size and reduced by an age factor. RL includes decay (forgetting), potentially more impactful for larger sets due to higher uncertainty, which drives heavier reliance on WM when available.