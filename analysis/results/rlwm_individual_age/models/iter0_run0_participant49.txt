def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and decay-limited working memory, modulated by set size and age.

    The model mixes a model-free RL policy with a working-memory (WM) policy.
    - RL learns Q-values with a single learning rate.
    - WM stores stimulus-action mappings rapidly but suffers from decay to uniform and limited capacity.
    - WM effective weight is reduced as set size increases and for older adults.

    Parameters
    ----------
    states : array-like
        Stimulus/state index on each trial (0..nS-1 within block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Binary feedback (0/1) on each trial.
    blocks : array-like
        Block index for each trial.
    set_sizes : array-like
        Set size on each trial (3 or 6).
    age : array-like
        Participant's age (same value repeated).
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, k0, k_age, decay]
        - lr: RL learning rate (0..1).
        - wm_weight: base WM mixture weight (0..1).
        - softmax_beta: inverse temperature for RL policy (scaled internally).
        - k0: baseline WM capacity (in items).
        - k_age: reduction in capacity if older (added if age_group==1).
        - decay: WM decay toward uniform each trial (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, k0, k_age, decay = model_parameters
    softmax_beta *= 10.0  # expand range
    
    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # nearly deterministic WM readout
    
    eps = 1e-12
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states  = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working memory effective weight depends on capacity and set size and age
            # K = baseline capacity minus age-related reduction (non-negative)
            K = max(0.0, k0 - k_age * age_group)
            capacity_factor = min(1.0, K / max(1, nS_t))  # 0..1
            wm_weight_eff = np.clip(wm_weight * capacity_factor, 0.0, 1.0)

            # WM policy: softmax over WM values
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay (global) toward uniform
            w = (1.0 - decay) * w + decay * w_0

            # WM update: fast storage of correct mapping, fallback to uniform on errors
            if r >= 0.5:
                # Move WM row toward one-hot on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # If error, move toward uncertainty for this state
                target = w_0[s, :]

            # Apply a strong write toward target for the current state
            phi = 1.0  # strong write strength; decay handles forgetting
            w[s, :] = (1.0 - phi) * w[s, :] + phi * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and set-size-driven WM interference.
    Older age reduces RL inverse temperature (more exploration).

    - RL: separate learning rates for positive/negative feedback.
    - WM: fast store-and-retrieve subject to interference that grows with set size.
    - Age: older group has reduced RL inverse temperature via a scaling parameter.

    Parameters
    ----------
    states : array-like
        Stimulus/state index on each trial.
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr_pos, lr_neg, wm_weight, softmax_beta, interference, beta_age_scale]
        - lr_pos: RL learning rate for rewards (0..1).
        - lr_neg: RL learning rate for non-rewards (0..1).
        - wm_weight: base WM mixture weight (0..1).
        - softmax_beta: base inverse temperature for RL (scaled internally).
        - interference: WM interference growth with set size (>0 reduces WM influence as set size increases).
        - beta_age_scale: scaling of inverse temperature reduction for older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, interference, beta_age_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age reduces RL inverse temperature (more exploration) when old
    beta_eff = softmax_beta / (1.0 + beta_age_scale * age_group)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states  = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM interference reduces WM weight with larger set sizes
            wm_weight_eff = wm_weight * np.exp(-interference * max(0, nS_t - 3))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # WM policy from current WM store
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture probability
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            alpha = lr_pos if r >= 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: overwrite on reward, avoid chosen action on error
            # Slight global decay toward uniform to limit perseveration/noise
            decay = 0.05
            w = (1.0 - decay) * w + decay * w_0

            if r >= 0.5:
                # Store chosen mapping
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # Suppress chosen action, redistribute to others
                target = np.ones(nA)
                target[a] = 0.0
                target = target / np.sum(target)

            phi = 1.0  # strong write
            w[s, :] = (1.0 - phi) * w[s, :] + phi * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with perseveration bias and age-dependent WM decay.

    - RL: standard Q-learning with single learning rate plus a perseveration bonus for repeating
      the last action within a state.
    - WM: capacity-limited store with age-dependent decay (older adults forget faster).
    - Set size scales WM weight via a slot-like capacity.

    Parameters
    ----------
    states : array-like
        Stimulus/state index on each trial.
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, decay_young, decay_old, perseveration]
        - lr: RL learning rate (0..1).
        - wm_weight: base WM mixture weight (0..1).
        - softmax_beta: RL inverse temperature (scaled internally).
        - decay_young: WM decay to uniform per trial if young.
        - decay_old: WM decay to uniform per trial if old.
        - perseveration: additive choice bonus applied to last chosen action within a state.

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr, wm_weight, softmax_beta, decay_young, decay_old, perseveration = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    decay = decay_old if age_group == 1 else decay_young
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states  = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # track last chosen action per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # RL policy with perseveration bonus on the previously chosen action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM weight scales with set size via simple slot model; K fixed at 3 slots
            K = 3.0
            capacity_factor = min(1.0, K / max(1, nS_t))
            wm_weight_eff = np.clip(wm_weight * capacity_factor, 0.0, 1.0)

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM global decay
            w = (1.0 - decay) * w + decay * w_0

            # WM update: store correct mapping strongly on reward; uncertainty otherwise
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]

            phi = 1.0
            w[s, :] = (1.0 - phi) * w[s, :] + phi * target

            # Update last action tracker
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p