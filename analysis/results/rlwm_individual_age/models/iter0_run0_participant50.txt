Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) mechanisms and incorporate effects of set size (3 vs 6) and age group (younger vs older). Each function returns the negative log-likelihood of the observed choices.

Notes:
- All models assume 3 actions (0,1,2) and per-block state spaces that reset.
- Age group: 0 = younger (<=45), 1 = older (>45). The participant here is older (age=80), but the code uses the age input dynamically.
- Each model uses at most 6 parameters and uses all parameters meaningfully.

Model 1: RL + capacity-limited WM with decay; age reduces WM influence
- RL: single learning rate and softmax with inverse temperature beta.
- WM: decaying associative store that is capacity-limited; strong deterministic policy (softmax with high beta).
- Set size impact: WM mixture weight scaled by capacity factor min(1, k_capacity / set_size).
- Age impact: WM weight reduced for older adults by a multiplicative factor.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with capacity limit and decay. Age reduces WM influence.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (number of states) for the current block on each trial.
    age : array-like (single repeated value)
        Age per trial (used to determine age group).
    model_parameters : list or array
        [lr, wm_weight_base, beta_base, wm_decay, k_capacity, age_effect_wm]
        - lr: RL learning rate (0..1)
        - wm_weight_base: base WM mixture weight (0..1)
        - beta_base: base inverse temperature; scaled internally (*10)
        - wm_decay: WM decay toward uniform per trial (0..1)
        - k_capacity: WM capacity in items (0..6), scales WM weight by min(1, k_capacity / set_size)
        - age_effect_wm: reduces WM weight in older adults; wm_weight *= (1 - age_effect_wm*age_group)
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, beta_base, wm_decay, k_capacity, age_effect_wm = model_parameters

    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM
    eps = 1e-12

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM weights
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform baseline for WM decay

        # Compute block-specific WM mixture scaling
        cap_factor = min(1.0, max(0.0, float(k_capacity) / max(1.0, nS)))
        wm_weight_block = wm_weight_base * cap_factor
        # Age reduces WM influence
        wm_weight_block = wm_weight_block * (1.0 - age_effect_wm * age_group)
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL policy
            Q_s = q[s, :]
            # Softmax probability for chosen action (use numerically stable form)
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = pi_rl[a]

            # Compute WM policy (decaying store, strong softmax)
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = pi_wm[a]

            # Mixture policy
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: if rewarded, store one-hot for correct action (fast overwrite)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


Model 2: RL with separate learning rates, WM gate and perseveration bias; age reduces decision precision
- RL: separate learning rates for positive and negative prediction errors; softmax with perseveration bias toward last chosen action in that state.
- WM: stores last rewarded action per state; when WM has an entry, a gate can choose WM policy.
- Set size impact: WM gate probability scaled by 3/nS (1 for set size 3, 0.5 for set size 6).
- Age impact: reduces softmax beta in older adults.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with separate learning rates + WM gating + perseveration. Age reduces beta.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [lr_pos, lr_neg, beta_base, wm_gate_base, perseveration_kappa, age_beta_shift]
        - lr_pos: RL learning rate for positive PE (0..1)
        - lr_neg: RL learning rate for negative PE (0..1)
        - beta_base: base inverse temperature; scaled internally (*10)
        - wm_gate_base: base probability to use WM when available (0..1)
        - perseveration_kappa: bias added to last chosen action's preference in RL (>=0)
        - age_beta_shift: reduces beta for older adults; beta *= (1 - age_beta_shift*age_group)
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr_pos, lr_neg, beta_base, wm_gate_base, perseveration_kappa, age_beta_shift = model_parameters

    beta = beta_base * 10.0
    age_group = 1 if age[0] > 45 else 0
    # Age reduces decision precision
    beta = beta * (1.0 - age_beta_shift * age_group)
    beta = max(beta, 1e-3)

    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: -1 denotes "unknown"; otherwise stores a single action index 0..2
        wm_store = -1 * np.ones(nS, dtype=int)

        # Perseveration memory: last action per state, -1 if none
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM gate
        cap_gate = min(1.0, 3.0 / max(1.0, nS))  # 1.0 for 3, 0.5 for 6
        wm_gate = np.clip(wm_gate_base * cap_gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax with perseveration bias
            prefs = q[s, :].copy()
            if last_action[s] >= 0:
                prefs[last_action[s]] += perseveration_kappa
            pref = beta * (prefs - np.max(prefs))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = pi_rl[a]

            # WM policy (deterministic if known)
            if wm_store[s] >= 0:
                wm_policy = np.zeros(3)
                wm_policy[wm_store[s]] = 1.0
                p_wm = wm_policy[a]
                # Use WM with probability wm_gate, otherwise RL
                p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            else:
                p_total = p_rl

            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with separate learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # Update WM store only when rewarded (binding correct association)
            if r > 0.5:
                wm_store[s] = a

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


Model 3: RL + WM with set-size driven WM interference and undirected lapses; age increases lapses
- RL: single learning rate; beta scaled down with larger set sizes.
- WM: reward-gated overwriting; decay (interference) increases with set size.
- Lapses: undirected epsilon, larger in older adults.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with set-size-driven WM interference and undirected lapses. Age increases lapses.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [lr, beta_base, wm_weight_base, interference, lapse, age_lapse]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature; scaled internally (*10)
        - wm_weight_base: base WM mixture weight (0..1)
        - interference: scales WM decay with set size; decay = clip(interference * (nS-1)/5, 0..1)
        - lapse: base undirected lapse probability (0..0.49)
        - age_lapse: additional lapse added for older adults (>=0), total clipped to <0.5
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, beta_base, wm_weight_base, interference, lapse, age_lapse = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL inverse temperature reduced for larger set sizes
        softmax_beta = (beta_base * 10.0) / (1.0 + 0.5 * max(0, nS - 3))

        # WM interference-driven decay with set size
        wm_decay = np.clip(interference * (max(1, nS) - 1) / 5.0, 0.0, 1.0)

        # Mixture weight: use base; implicitly, WM effectiveness is tempered by decay at larger nS
        wm_weight = np.clip(wm_weight_base, 0.0, 1.0)

        # Lapse (undirected), larger for older adults
        epsilon = np.clip(lapse + age_group * age_lapse, 0.0, 0.49)

        # Initialize value stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        softmax_beta_wm = 50.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = pi_rl[a]

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = pi_wm[a]

            # RL-WM mixture
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Undirected lapse to uniform
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM reward-gated overwrite
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)