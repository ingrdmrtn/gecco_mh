def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with set-size- and age-dependent WM engagement and decay.

    Decision policy:
    - Mixture of RL softmax and a high-precision WM policy.
    - WM weight is modulated by set size (smaller sets -> higher WM reliance) and age group.
    - WM stores the last rewarded action for each state as a one-shot association and decays toward uniform with a rate that grows with set size and age.

    Parameters (list/tuple of length 6):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight_base: baseline WM mixture weight (logit space).
    - wm_setsize_slope: how much WM reliance decreases as set size increases (negative values decrease WM weight for larger sets).
    - softmax_beta: RL inverse temperature; internally scaled by 10 for range.
    - wm_decay_base: baseline WM decay (logit scale); higher -> faster decay to uniform.
    - age_wm_penalty: additive penalty on WM weight and decay for older adults (age_group=1). Used to decrease wm_weight and increase decay.

    Age group coding:
    - age_group = 0 if age <= 45 (younger), else 1 (older). This model lowers WM engagement and increases WM decay with age_group.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, wm_setsize_slope, softmax_beta, wm_decay_base, age_wm_penalty = model_parameters
    softmax_beta *= 10.0  # scale RL beta
    softmax_beta_wm = 50.0  # near-deterministic WM
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size and age-dependent WM weight (sigmoid mapping to 0..1)
            wm_weight_logit = wm_weight_base + wm_setsize_slope * (nS - 3) - np.abs(age_wm_penalty) * age_group
            wm_weight_t = 1.0 / (1.0 + np.exp(-wm_weight_logit))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # WM decay increases with set size and age
            decay_logit = wm_decay_base + 0.5 * (nS - 3) + np.abs(age_wm_penalty) * age_group
            decay_t = 1.0 / (1.0 + np.exp(-decay_logit))  # 0..1
            # Apply decay toward uniform for all states each trial
            w = (1.0 - decay_t) * w + decay_t * w_0

            # RL policy probability of chosen action
            Q_s = q[s, :]
            Q_centered = softmax_beta * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(Q_centered))

            # WM policy probability of chosen action (softmax over WM preference)
            W_s = w[s, :]
            W_centered = softmax_beta_wm * (W_s - W_s[a])
            p_wm = 1.0 / np.sum(np.exp(W_centered))

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store deterministic association on correct feedback
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+Capacity-limited WM mixture with RL forgetting and probabilistic WM hits.

    Decision policy:
    - Mixture of RL softmax and WM.
    - WM works with a probabilistic "hit" that depends on WM capacity relative to set size and age.
      If a WM hit occurs (with probability ~ capacity / set_size adjusted by age), WM yields a near-deterministic policy
      for states with a stored association; otherwise, policy is uniform.
    - RL has a decay/forgetting term that pulls Q toward uniform; forgetting increases effective exploration over time.

    Parameters (list/tuple of length 6):
    - lr: RL learning rate (0..1).
    - wm_weight_base: baseline WM mixture weight (logit space).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_capacity: approximate number of items WM can reliably maintain (1..6); larger -> higher hit probability in larger sets.
    - age_effect: reduces effective WM capacity in older adults; zero in younger, positive penalty in older.
    - rl_decay: per-trial RL forgetting rate toward uniform (0..1), applied to all states.

    Age group:
    - age_group = 0 (<=45), 1 (>45). Effective WM capacity = max(0, wm_capacity - age_effect*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_capacity, age_effect, rl_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eff_capacity = max(0.0, float(wm_capacity) - max(0.0, age_effect) * age_group)
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # stores last rewarded action per state when known
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        has_store = np.zeros(nS, dtype=bool)  # whether WM has a stored association for this state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # WM mixture weight (smaller sets -> slightly higher reliance)
            wm_weight_logit = wm_weight_base + 0.4 * (3 - nS)  # positive when nS=3, negative when nS=6
            wm_weight_t = 1.0 / (1.0 + np.exp(-wm_weight_logit))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # RL forgetting toward uniform (applied each trial)
            q = (1.0 - rl_decay) * q + rl_decay * w_0

            # RL policy
            Q_s = q[s, :]
            Q_centered = softmax_beta * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(Q_centered))

            # WM hit probability depends on effective capacity vs set size
            hit_prob = np.clip(eff_capacity / max(1.0, float(nS)), 0.0, 1.0)

            # Build WM policy for the chosen action:
            if has_store[s]:
                # If stored, near-deterministic on stored action
                W_s = w[s, :]
                W_centered = softmax_beta_wm * (W_s - W_s[a])
                p_wm_det = 1.0 / np.sum(np.exp(W_centered))
            else:
                # No stored info; WM is uniform
                p_wm_det = 1.0 / nA

            # WM final probability for chosen action is a mixture of hit and miss (miss -> uniform)
            p_wm = hit_prob * p_wm_det + (1.0 - hit_prob) * (1.0 / nA)

            # Mixture of systems
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store the mapping only on correct feedback
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
                has_store[s] = True
            # If incorrect, do not overwrite; leave as is.

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and time-since-reward WM leak.

    Decision policy:
    - Mixture of RL softmax and WM softmax.
    - RL uses separate learning rates for positive vs negative prediction errors.
    - WM stores the last rewarded action per state; its memory trace leaks as a function of
      time since the last correct reward for that state, with stronger leak for larger set sizes
      and for older adults.

    Parameters (list/tuple of length 6):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - wm_weight_base: baseline WM mixture weight (logit space).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_leak_per_trial: baseline leak per trial (logit scale); larger -> faster memory loss.
    - age_leak_mult: multiplicative factor on leak for older adults (>=0). In older adults,
      effective leak increases by (1 + age_leak_mult); in younger, multiplier is 1.

    Age and set-size effects:
    - WM leak increases with set size and with age_group.
    - WM weight mildly favors small set size (3) relative to large (6).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, wm_leak_per_trial, age_leak_mult = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last time of correct reward per state for leak computation
        last_reward_time = -1 * np.ones(nS, dtype=int)
        t_global = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Trial-dependent WM weight: favor small set sizes
            wm_weight_logit = wm_weight_base + 0.5 * (3 - nS)
            wm_weight_t = 1.0 / (1.0 + np.exp(-wm_weight_logit))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Compute time since last correct for this state
            if last_reward_time[s] >= 0:
                dt = max(0, t_global - last_reward_time[s])
            else:
                dt = t_global + 1  # no success yet; treat as long gap

            # Leak increases with set size and age
            leak_logit = wm_leak_per_trial + 0.4 * (nS - 3)
            base_leak = 1.0 / (1.0 + np.exp(-leak_logit))  # 0..1 per trial
            age_multiplier = 1.0 + max(0.0, age_leak_mult) * age_group
            effective_leak = np.clip(1.0 - (1.0 - base_leak) ** (dt * age_multiplier), 0.0, 1.0)

            # Apply state-specific leak toward uniform
            w[s, :] = (1.0 - effective_leak) * w[s, :] + effective_leak * w_0[s, :]

            # RL policy
            Q_s = q[s, :]
            Q_centered = softmax_beta * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(Q_centered))

            # WM policy
            W_s = w[s, :]
            W_centered = softmax_beta_wm * (W_s - W_s[a])
            p_wm = 1.0 / np.sum(np.exp(W_centered))

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: on correct feedback, store action deterministically and reset last time
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
                last_reward_time[s] = t_global

            t_global += 1

        blocks_log_p += log_p

    return -blocks_log_p