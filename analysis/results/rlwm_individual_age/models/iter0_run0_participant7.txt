Below are three standalone cognitive models that implement RL–WM mixtures with meaningful effects of set size (3 vs 6) and age group (young vs old). Each function follows the provided template, uses all parameters, and returns the negative log-likelihood of the observed choices.

Note: These functions assume numpy has been imported as np elsewhere.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM model with capacity-limited WM and decay.
    
    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM has a capacity K that limits contribution when set size exceeds K.
    - WM traces decay toward a uniform baseline each trial and are updated rapidly with feedback.
    - Age affects effective capacity (older group has reduced K).
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight: base mixture weight on WM (0..1)
    - softmax_beta: base inverse temperature for RL (scaled by 10 internally)
    - wm_decay: decay rate of WM toward uniform per (state, trial) update (0..1)
    - alpha_wm: WM learning rate (0..1)
    - capacity_K: WM capacity K (positive, in number of items)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, alpha_wm, capacity_K = model_parameters
    softmax_beta *= 10.0  # RL beta scaled up per template
    
    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0  # deterministic WM readout as in template
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # RL Q-values and WM weights initialized to uniform
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Effective WM capacity reduced for older adults
        K_eff = capacity_K * (0.6 if age_group == 1 else 1.0)
        # WM mixture weight scaled by capacity relative to set size
        wm_scaler = min(1.0, K_eff / max(1.0, float(nS)))
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()
            
            # RL policy probability of chosen action (stable softmax)
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)
            
            # WM policy (softmax over WM weights)
            Wc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Wc))
            p_wm = np.exp(softmax_beta_wm * Wc[a]) / max(1e-12, denom_wm)
            
            # Effective mixture weight with capacity scaling
            wm_eff = np.clip(wm_weight * wm_scaler, 0.0, 1.0)
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # WM delta rule on chosen action
            wm_delta = r - W_s[a]
            w[s, a] += alpha_wm * wm_delta
            # Ensure valid probability vector for WM row
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM model with separate RL learning rates, WM mixture scaling by load,
    and perseveration bias in WM policy.
    
    Idea:
    - Choices are a mixture of RL and WM.
    - RL uses separate learning rates for rewards and non-rewards.
    - WM mixture weight scales inversely with set size (3/nS), and is reduced in older adults.
    - RL temperature also scales with load (lower under larger set sizes) and is reduced in older adults.
    - WM policy includes a perseveration bias toward the last chosen action for that state.
    
    Parameters (list):
    - lr_pos: RL learning rate for positive RPE (0..1)
    - lr_neg: RL learning rate for negative RPE (0..1)
    - wm_weight: base WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - pers_bias: additive bias toward last chosen action in WM policy (>=0)
    - alpha_wm: WM learning rate (0..1)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, pers_bias, alpha_wm = model_parameters
    softmax_beta *= 10.0  # base RL beta
    
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        # Track last chosen action per state for perseveration; -1 means none yet.
        last_choice = -1 * np.ones(nS, dtype=int)
        
        # Load scaling: stronger WM and RL precision at small set sizes
        load_scale = 3.0 / float(nS)  # 1.0 for nS=3, 0.5 for nS=6
        # Age effects: older adults have reduced WM weight and RL precision
        age_wm_scale = 1.0 if age_group == 0 else 0.7
        age_beta_scale = 1.0 if age_group == 0 else 0.8
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()
            
            # RL softmax with load and age scaling
            beta_eff = softmax_beta * load_scale * age_beta_scale
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(beta_eff * Qc))
            p_rl = np.exp(beta_eff * Qc[a]) / max(1e-12, denom_rl)
            
            # WM policy with perseveration bias toward last chosen action
            wm_logits = W_s.copy()
            if last_choice[s] != -1:
                wm_logits[last_choice[s]] += pers_bias
            Lc = wm_logits - np.max(wm_logits)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)
            
            # Mixture weight scaled by load and age
            wm_eff = np.clip(wm_weight * load_scale * age_wm_scale, 0.0, 1.0)
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
            
            # RL update with asymmetric learning rates
            rpe = r - Q_s[a]
            q[s, a] += (lr_pos if rpe >= 0 else lr_neg) * rpe
            
            # WM update: delta rule on chosen action (no explicit decay here)
            wm_delta = r - W_s[a]
            w[s, a] += alpha_wm * wm_delta
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])
            
            # Update last choice for perseveration
            last_choice[s] = a
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM model with RPE-gated mixture: surprise shifts weight from WM to RL.
    
    Idea:
    - Mixture weight is adapted on each trial based on the absolute RPE.
      Larger surprise reduces WM reliance and increases RL reliance.
    - WM contribution is also capacity-limited by set size.
    - Age reduces both gating sensitivity and effective capacity.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - alpha_wm: WM learning rate (0..1)
    - gate_sensitivity: controls how strongly abs(RPE) gates the mixture (>=0)
    - capacity_K: WM capacity K (positive)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_wm, gate_sensitivity, capacity_K = model_parameters
    softmax_beta *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Effective capacity: older adults reduced K
        K_eff = capacity_K * (0.7 if age_group == 1 else 1.0)
        cap_scale = min(1.0, K_eff / max(1.0, float(nS)))
        # Age reduces gating sensitivity
        gate_sens_eff = gate_sensitivity * (0.7 if age_group == 1 else 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()
            
            # RL policy
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)
            
            # WM policy from WM weights
            Wc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Wc))
            p_wm = np.exp(softmax_beta_wm * Wc[a]) / max(1e-12, denom_wm)
            
            # Compute RPE (based on current Q) to gate mixture
            rpe = r - Q_s[a]
            # Normalized surprise: abs(RPE) in [0,1] for binary rewards if Q in [0,1]
            surprise = np.abs(rpe)
            # Sigmoid gating: g in (0,1). Higher surprise -> higher g.
            g = 1.0 / (1.0 + np.exp(-gate_sens_eff * (surprise - 0.5)))
            # WM weight decreases with surprise; also scale by capacity
            wm_eff = np.clip(wm_weight * cap_scale * (1.0 - g), 0.0, 1.0)
            
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            q[s, a] += lr * rpe
            
            # WM slight decay toward uniform to prevent saturation
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            # WM update
            wm_delta = r - W_s[a]
            w[s, a] += alpha_wm * wm_delta
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Modeling notes on set size and age:
- Model 1: WM mixture weight scales with capacity_K/nS; older adults have reduced effective capacity (0.6×), reducing WM influence especially for nS=6.
- Model 2: WM mixture weight and RL precision scale with 3/nS (i.e., halved at nS=6); older adults have additional reduction in both WM weight (0.7×) and RL inverse temperature (0.8×). Perseveration bias captures tendency to repeat previous actions in a given state.
- Model 3: Trialwise gating uses RPE; higher surprise shifts weight from WM to RL. WM influence is also limited by capacity_K/nS; older adults have reduced capacity (0.7×) and weaker gating sensitivity (0.7×), yielding slower mixture adaptation.