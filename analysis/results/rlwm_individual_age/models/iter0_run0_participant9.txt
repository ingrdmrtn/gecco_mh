Below are three standalone cognitive models that instantiate RL+WM hybrids tailored to the RLWM task. Each function returns the negative log-likelihood of the observed choices. All parameters are used, set size and age group modulate the policy or learning, and the models adhere to the provided template structure.

Note: These functions assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM hybrid with capacity-limited WM and decay; WM capacity depends on age group.

    Parameters
    - states: array-like, state index per trial (0..nS-1 within each block)
    - actions: array-like, chosen action per trial (0..2)
    - rewards: array-like, binary reward per trial (0/1)
    - blocks: array-like, block index per trial
    - set_sizes: array-like, set size for the block of each trial (3 or 6)
    - age: array-like, participant age per trial (constant repeated)
    - model_parameters: list/tuple of 6 parameters:
        lr: RL learning rate (0..1)
        wm_weight: base WM mixture weight (0..1)
        softmax_beta: RL inverse temperature (scaled by *10 internally)
        phi: WM decay rate toward uniform (0..1)
        K_young: effective WM capacity for younger group (>=0)
        K_old: effective WM capacity for older group (>=0)

    Modeling details
    - RL: standard delta rule with softmax policy for chosen-action probability.
    - WM: stores last rewarded action per state; decays toward uniform at rate phi.
    - Mixture: p_total = wm_eff*p_wm + (1 - wm_eff)*p_rl
      where wm_eff = wm_weight * min(1, K_group / nS) captures set-size load and age via K.
    - Age group: 0 if age <= 45, else 1; used to select K_young vs K_old.
    """
    lr, wm_weight, softmax_beta, phi, K_young, K_old = model_parameters
    softmax_beta *= 10.0

    # Determine age group (0=young, 1=old)
    age_group = 0 if age[0] <= 45 else 1
    K_group = K_young if age_group == 0 else K_old

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight given capacity and set size
        wm_eff = wm_weight * min(1.0, max(0.0, K_group) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: chosen-action probability under softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1.0 / nA

            # WM policy: chosen-action probability under near-deterministic softmax over WM weights
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Mixture of WM and RL
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)  # numerical safety
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM write on reward (store last rewarded action deterministically)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM hybrid with age-specific learning rate and WM decay; WM updates are gated by positive prediction error.

    Parameters
    - states: array-like, state index per trial
    - actions: array-like, chosen action per trial (0..2)
    - rewards: array-like, binary reward per trial
    - blocks: array-like, block index per trial
    - set_sizes: array-like, set size per trial (3 or 6)
    - age: array-like, participant age per trial (constant repeated)
    - model_parameters: list/tuple of 6 parameters:
        lr_y: RL learning rate for young group (0..1)
        lr_o: RL learning rate for old group (0..1)
        wm_weight: base WM mixture weight (0..1)
        softmax_beta: RL inverse temperature (scaled by *10 internally)
        phi_y: WM decay for young (0..1)
        phi_o: WM decay for old (0..1)

    Modeling details
    - RL: delta rule with age-specific learning rate.
    - WM: decays to uniform at age-specific rate phi; updates to one-hot only when PE > 0 (positive outcome surprise).
    - Set-size load: WM mixture weight further scaled by 3/nS (stronger reduction in 6-set).
      wm_eff = wm_weight * (3 / nS).
    - Mixture: p_total = wm_eff*p_wm + (1 - wm_eff)*p_rl.
    - Age group: 0 if age <= 45, else 1.
    """
    lr_y, lr_o, wm_weight, softmax_beta, phi_y, phi_o = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    lr = lr_y if age_group == 0 else lr_o
    phi = phi_y if age_group == 0 else phi_o

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM strength
        wm_eff = wm_weight * (3.0 / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1.0 / nA

            # WM chosen-action probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Mixture
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM gated update: only on positive prediction error
            if pe > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM hybrid with uncertainty-weighted mixture and age-dependent lapse.

    Parameters
    - states: array-like, state index per trial
    - actions: array-like, chosen action per trial (0..2)
    - rewards: array-like, binary reward per trial
    - blocks: array-like, block index per trial
    - set_sizes: array-like, set size per trial (3 or 6)
    - age: array-like, participant age per trial (constant repeated)
    - model_parameters: list/tuple of 6 parameters:
        lr: RL learning rate (0..1)
        softmax_beta: RL inverse temperature (scaled by *10 internally)
        wm_base: base WM mixture weight (0..1)
        phi: WM decay to uniform (0..1)
        tau_size: exponent controlling WM down-weighting with set size (>=0)
        lapse_old: additional lapse probability for older group (>=0)

    Modeling details
    - RL: delta rule. Policy computed as full softmax over Q to allow entropy computation.
    - WM: decays to uniform at rate phi; writes one-hot on reward.
    - Uncertainty gating: WM weight is boosted when RL is uncertain.
        wm_size = wm_base * (3/nS) ** tau_size
        H = entropy of RL softmax (normalized by log(nA))
        wm_eff = wm_size * H
    - Age-dependent lapse: final policy blended with uniform via lapse.
        lapse = 0 (young) or min(0.5, lapse_old) (old)
        p_final = (1 - lapse) * mix + lapse * (1/nA)
    - Mixture: mix = wm_eff*p_wm + (1 - wm_eff)*p_rl
    """
    lr, softmax_beta, wm_base, phi, tau_size, lapse_old = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    lapse = 0.0 if age_group == 0 else min(0.5, max(0.0, lapse_old))

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM base weight
        wm_size = wm_base * (3.0 / float(nS)) ** max(0.0, tau_size)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL full softmax for probabilities and entropy
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ) if np.sum(expQ) > 0 else np.ones(nA) / nA
            p_rl = p_rl_vec[a]

            # WM full softmax over WM weights
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW) if np.sum(expW) > 0 else np.ones(nA) / nA
            p_wm = p_wm_vec[a]

            # Uncertainty (normalized entropy of RL policy)
            eps = 1e-12
            entropy = -np.sum(p_rl_vec * np.log(p_rl_vec + eps))
            H_norm = entropy / np.log(nA)

            wm_eff = wm_size * H_norm
            wm_eff = max(0.0, min(1.0, wm_eff))

            # Mixture and lapse
            mix = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_final = (1.0 - lapse) * mix + lapse * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            log_p += np.log(p_final)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM write on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p