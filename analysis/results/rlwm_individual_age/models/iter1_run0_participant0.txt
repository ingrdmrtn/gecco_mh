Below are three standalone cognitive models that follow your template structure and constraints. Each returns the negative log-likelihood of the observed choices, uses all parameters meaningfully, and incorporates age group and set size effects.

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with decay and perseveration bias.

    Mechanism
    - RL: delta rule with softmax choice.
    - WM: one-shot storage on rewarded trials, but subject to capacity (K slots).
      Encoding probability scales with available capacity: min(1, K/nS).
      WM representations decay toward uniform each trial (phi).
    - Perseveration: WM policy is biased to repeat the last action taken in that state (kappa).
    - Mixing: convex combination of WM and RL policies. WM reliance is modulated by set size and age.
      Young participants (age_group=0) get an additive shift to WM reliance.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1]
    - wm_base: baseline WM reliance before modulation (unbounded, squashed by sigmoid)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - phi: WM decay/leak toward uniform in [0,1]
    - kappa: perseveration bias added to the last chosen action in WM readout
    - age_wm_shift: additive shift to WM reliance for young (age_group=0). Positive increases WM reliance.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, phi, kappa, age_wm_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity K depends on baseline and age. We map wm_base to [0,1], scale by max slots (3).
        # Then effective capacity is in [1,3], clipped to [1, nS].
        base = 1.0 / (1.0 + np.exp(-(wm_base + (age_wm_shift * (1 - age_group)))))
        K = np.clip(1.0 + 2.0 * base, 1.0, float(nS))

        # WM weight is scaled by capacity relative to set size.
        wm_weight_eff = np.clip((K / float(nS)), 0.0, 1.0)

        # Track last chosen action per state for perseveration bias
        last_act = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy with perseveration bias
            W_s = w[s, :].copy()
            if last_act[s] >= 0:
                W_s[last_act[s]] += kappa  # additive bias before softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - phi) * w + phi * w_0

            # WM encoding is probabilistic and capacity-limited
            # Probability of encoding scales with K/nS (more items -> lower probability)
            p_enc = np.clip(K / float(nS), 0.0, 1.0)
            if r > 0 and np.random.rand() < p_enc:
                w[s, :] = eps * np.ones(nA)
                w[s, a] = 1.0 - (nA - 1) * eps
                w[s, :] /= np.sum(w[s, :])

            last_act[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + Dirichlet-like WM with age- and set-size-dependent noise.

    Mechanism
    - RL: softmax policy with separate learning rates for positive and negative prediction errors.
    - WM: count-based memory of successes (Dirichlet mean). Reward increments the chosen action count;
      non-reward softly increments all non-chosen actions (to reflect avoiding that action).
      WM policy is derived by softmax over the WM probabilities with an effective inverse temperature
      that decreases (becomes noisier) with larger set size and for older participants.
    - Mixing: fixed mixing weight for WM vs RL.

    Parameters (model_parameters)
    - alpha_pos: RL learning rate for positive PE in [0,1]
    - alpha_neg: RL learning rate for negative PE in [0,1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_weight: mixture weight of WM policy in [0,1] (squashed internally)
    - tau_wm: base WM noise scaling with set size (higher = noisier WM)
    - age_wm_noise_mult: multiplicative factor on WM noise for old vs young (>=0).
      Effective WM noise is tau_wm * (1 + age_group * age_wm_noise_mult).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight, tau_wm, age_wm_noise_mult = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm_base = 50.0
    age_group = 0 if age[0] <= 45 else 1

    # squash wm_weight into [0,1]
    wm_weight = 1.0 / (1.0 + np.exp(-wm_weight))

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL Q and WM counts
        q = (1.0 / nA) * np.ones((nS, nA))
        # Dirichlet prior alpha0 small >0 to avoid zero
        alpha0 = 0.1
        succ = alpha0 * np.ones((nS, nA))
        fail = alpha0 * np.ones((nS, nA))  # keep a tally to shape probabilities

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM probabilities from Dirichlet-like counts
            # Convert counts to probabilities (mean of Dirichlet)
            counts = succ[s, :] + fail[s, :]
            W_s = succ[s, :] / np.maximum(np.sum(counts), eps)

            # Effective WM temperature decreases with set size and age via noise
            size_noise = tau_wm * (float(nS) / 3.0)
            age_noise = (1.0 + age_group * age_wm_noise_mult)
            softmax_beta_wm_eff = softmax_beta_wm_base / (1.0 + size_noise * age_noise)

            denom_wm = np.sum(np.exp(softmax_beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            if delta >= 0:
                q[s, a] += alpha_pos * delta
            else:
                q[s, a] += alpha_neg * delta

            # WM count updates
            if r > 0:
                succ[s, a] += 1.0
            else:
                # Non-reward: increment non-chosen actions to reflect avoidance of a
                inc = 1.0 / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        succ[s, aa] += inc  # nudge probability mass to alternatives
                fail[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Arbitration-by-surprise model: RL + WM with dynamic mixing based on absolute PE,
    modulated by set size and age.

    Mechanism
    - RL: standard delta rule softmax.
    - WM: graded associative store updated by its own learning rate (wm_lr), yielding a WM value
      map W(s,a). WM policy is computed via near-deterministic softmax.
    - Arbitration: WM weight on each trial is a logistic function of
      bias0 + size_term + age_term + gamma * |prediction error|.
      size_term = log(3/nS): favors WM in smaller sets (positive at nS=3, negative at nS=6).
      age_term favors WM for young if age_bias > 0.
      gamma > 0 increases WM reliance when surprise (|PE|) is high.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_lr: WM learning rate in [0,1] for updating W toward reward signals
    - gamma: arbitration sensitivity to |PE| (>=0)
    - bias0: base bias for WM reliance (unbounded; squashed by sigmoid)
    - age_bias: additive bias for young (age_group=0): positive means more WM reliance for young

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_lr, gamma, bias0, age_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value maps
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM value-like map; starts uniform
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # not used as decay anchor here, but kept for template consistency

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Arbitration weight depends on |PE| from RL, set size, and age
            pe = r - Q_s[a]
            size_term = np.log(3.0 / float(nS))  # >0 for nS=3, <0 for nS=6
            age_term = age_bias * (1 - age_group)  # only benefits young if age_bias>0
            mix_logit = bias0 + size_term + age_term + gamma * np.abs(pe)
            wm_weight_t = 1.0 / (1.0 + np.exp(-mix_logit))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            q[s, a] += lr * pe

            # WM update: move W(s,a) toward r, normalize row to keep it probabilistic
            w[s, a] += wm_lr * (r - w[s, a])
            # small repulsion for other actions to keep contrast
            for aa in range(nA):
                if aa != a:
                    w[s, aa] += wm_lr * ((0.0) - w[s, aa])
            # renormalize to [0,1] simplex to maintain a probability-like interpretation
            row_sum = np.sum(np.maximum(w[s, :], 0.0)) + eps
            w[s, :] = np.maximum(w[s, :], 0.0) / row_sum

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes on set size and age effects:
- Model 1: WM reliance is effectively K/nS where K is a capacity derived from wm_base and shifted for young participants; thus, WM is stronger for set size 3 than 6, and stronger for young if age_wm_shift > 0. Perseveration bias adds a tendency to repeat the last chosen action within the WM policy.
- Model 2: WM temperature becomes noisier as set size increases and for older participants via tau_wm and age_wm_noise_mult, reducing WM effectiveness at larger loads and in the older group; RL uses asymmetric learning rates.
- Model 3: Arbitration dynamically increases WM reliance when surprise is high; size_term biases WM for smaller set sizes; age_term biases WM reliance in the young group if age_bias > 0.