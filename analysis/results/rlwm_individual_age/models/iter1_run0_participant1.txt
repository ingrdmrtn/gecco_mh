def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with retrieval noise and eligibility traces.

    Idea:
    - Choices arise from a mixture of model-free RL and a capacity-limited Working Memory (WM).
    - WM can hold up to K stimulus-action associations; for set size nS, retrieval succeeds with probability ~ min(K/nS, 1).
    - WM retrieval has additional noise; when retrieval fails, WM contributes a uniform policy.
    - RL updates include an eligibility trace to carry credit over time within a block.
    - Age modulates RL exploration by scaling the softmax inverse temperature (older -> lower beta).

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (scaled by 10 internally): base inverse temperature for RL
    - model_parameters[2] = lambda_et in [0,1]: eligibility trace decay
    - model_parameters[3] = wm_capacity_K in [0,6]: effective WM capacity (in number of items)
    - model_parameters[4] = wm_noise in [0,1]: failure probability on WM retrieval
    - model_parameters[5] = age_beta_scale in [0,1]: fractional reduction of beta for older adults (beta *= (1 - age_beta_scale*age_group))

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen action indices (0..2)
    - rewards: array of rewards (0/1)
    - blocks: array of block indices
    - set_sizes: array of set sizes (3 or 6) per trial
    - age: array-like with constant age per trial

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, lambda_et, wm_capacity_K, wm_noise, age_beta_scale = model_parameters

    softmax_beta *= 10.0
    softmax_beta = max(softmax_beta, 1e-6)
    lambda_et = min(max(lambda_et, 0.0), 1.0)
    wm_capacity_K = min(max(wm_capacity_K, 0.0), 6.0)
    wm_noise = min(max(wm_noise, 0.0), 1.0)
    lr = min(max(lr, 0.0), 1.0)
    age_beta_scale = min(max(age_beta_scale, 0.0), 1.0)

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age-scaled exploration: older -> lower beta
    beta_age_factor = (1.0 - age_beta_scale * age_group)
    softmax_beta *= max(beta_age_factor, 1e-3)

    nA = 3
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL components
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # WM components: store last rewarded action per state; strength is implicit via retrieval success prob
        wm_store = -1 * np.ones(nS, dtype=int)  # -1 indicates no stored action

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM retrieval success depends on capacity and noise
            cap_success = min(1.0, wm_capacity_K / max(nS_t, 1.0))
            retrieval_success = max(0.0, cap_success * (1.0 - wm_noise))

            # WM policy: deterministic if an association exists and retrieval succeeds; else uniform
            if wm_store[s] >= 0:
                W_s = np.zeros(nA)
                W_s[wm_store[s]] = 1.0
                denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = 1.0 / max(denom_wm, eps)
            else:
                p_wm = 1.0 / nA  # no stored item -> uniform

            # Mixture: use retrieval_success as WM weight on that trial
            wm_weight_eff = min(max(retrieval_success, 0.0), 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # decay traces, update current trace, and apply update
            e *= lambda_et
            e[s, a] += 1.0
            q += lr * delta * e

            # WM update: store correct association when rewarded
            if r > 0.0:
                wm_store[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with time-dependent WM strength and set-size-dependent decay; arbitration by WM strength.

    Idea:
    - RL provides a value-based policy updated with a single learning rate.
    - WM stores rewarded associations and maintains a graded memory vector per state that decays toward uniform.
    - WM decay increases with set size; arbitration weight equals base WM weight scaled by current WM strength and age.
    - Age reduces reliance on WM (older -> lower WM weight).

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (scaled by 10): inverse temperature for RL
    - model_parameters[2] = wm_base_weight in [0,1]: base reliance on WM
    - model_parameters[3] = wm_time_decay in [0,1]: per-trial WM decay toward uniform
    - model_parameters[4] = setsize_decay_gain >= 0: how much larger set size accelerates WM decay
    - model_parameters[5] = age_wm_bias in [0,1]: multiplicative reduction of WM weight for older adults

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_base_weight, wm_time_decay, setsize_decay_gain, age_wm_bias = model_parameters

    softmax_beta *= 10.0
    lr = min(max(lr, 0.0), 1.0)
    wm_base_weight = min(max(wm_base_weight, 0.0), 1.0)
    wm_time_decay = min(max(wm_time_decay, 0.0), 1.0)
    setsize_decay_gain = max(setsize_decay_gain, 0.0)
    age_wm_bias = min(max(age_wm_bias, 0.0), 1.0)

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    nA = 3
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM memory as probability distributions per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy from current WM distribution
            W_s = w[s, :].copy()
            W_s = W_s / max(np.sum(W_s), eps)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute WM strength (0 = uniform, 1 = one-hot)
            max_prob = np.max(W_s)
            wm_strength = (max_prob - 1.0 / nA) / (1.0 - 1.0 / nA)
            wm_strength = min(max(wm_strength, 0.0), 1.0)

            # Effective WM decay increases with set size
            # decay_eff is the proportion mixed with uniform on this trial
            decay_multiplier = 1.0 + setsize_decay_gain * max(nS_t - 3, 0)
            # Convert base per-trial decay into an effective decay given the multiplier
            decay_eff = 1.0 - (1.0 - wm_time_decay) ** decay_multiplier
            decay_eff = min(max(decay_eff, 0.0), 1.0)

            # Age reduces the base WM weight
            wm_weight_eff = wm_base_weight * (1.0 - age_wm_bias * age_group)
            # Also scale by instantaneous WM strength (stronger memory -> rely more on WM)
            wm_weight_eff *= wm_strength
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            # WM reinforcement: on reward, write a near one-hot distribution
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size- and age-dependent state confusion plus lapse.

    Idea:
    - A single RL system learns Q(s,a).
    - Under higher cognitive load (larger set size) and for older adults, the agent confuses states,
      blending the current state's Q with the average Q from other states before acting.
    - A small lapse mixes the policy with a uniform random choice.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (scaled by 10): inverse temperature
    - model_parameters[2] = conf_base: baseline state-confusion logit (can be negative/positive)
    - model_parameters[3] = conf_setsize_gain >= 0: how much set size increases confusion
    - model_parameters[4] = conf_age_gain >= 0: additional confusion for older adults
    - model_parameters[5] = lapse in [0,1]: probability of random responding

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, conf_base, conf_setsize_gain, conf_age_gain, lapse = model_parameters

    softmax_beta *= 10.0
    lr = min(max(lr, 0.0), 1.0)
    conf_setsize_gain = max(conf_setsize_gain, 0.0)
    conf_age_gain = max(conf_age_gain, 0.0)
    lapse = min(max(lapse, 0.0), 1.0)

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Compute confusion probability via a logistic transform
            # conf_logit increases with set size and with age
            conf_logit = conf_base + conf_setsize_gain * max(nS_t - 3, 0) + conf_age_gain * age_group
            conf_prob = 1.0 / (1.0 + np.exp(-conf_logit))
            conf_prob = min(max(conf_prob, 0.0), 1.0)

            # Blend current state's Q with the average Q over other states (state confusion)
            Q_s_true = q[s, :].copy()
            if nS > 1:
                other_indices = [i for i in range(nS) if i != s]
                Q_other_mean = np.mean(q[other_indices, :], axis=0)
            else:
                Q_other_mean = Q_s_true.copy()

            Q_effective = (1.0 - conf_prob) * Q_s_true + conf_prob * Q_other_mean

            # Softmax over effective Q
            denom = np.sum(np.exp(softmax_beta * (Q_effective - Q_effective[a])))
            p_choice = 1.0 / max(denom, eps)

            # Apply lapse
            p_total = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update on true current state-action
            delta = r - q[s, a]
            q[s, a] += lr * delta

        blocks_log_p += log_p

    return -blocks_log_p