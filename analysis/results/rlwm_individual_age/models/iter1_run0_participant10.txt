Below are three distinct cognitive models that combine reinforcement learning (RL) and working memory (WM) with different mechanisms for how set size and age affect behavior. Each function returns the negative log-likelihood of the observed choices.

Note:
- Assume numpy has already been imported as np.
- Age group coding: 0 for young (<= 45), 1 for old (> 45).
- WM state is reset at the start of each block, consistent with the task.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM capacity-limited gating with set-size scaling.

    Mechanism:
    - RL system: single learning rate and softmax policy.
    - WM system: stores rewarded action per state (one-hot). WM policy is a softmax with its own inverse temperature.
    - Mixture weight for WM is capacity-gated: WM weight decreases when set size exceeds an age-dependent capacity K.
      The same K also induces passive WM decay per trial within a block as interference grows with set size.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - beta_wm: WM inverse temperature (scaled by 10 internally)
    - wm_weight_base: base mixture weight for WM (0..1)
    - K_young: effective WM capacity for young participants (>0)
    - K_old: effective WM capacity for old participants (>0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta_rl, beta_wm, wm_weight_base, K_young, K_old = model_parameters
    beta_rl *= 10.0
    beta_wm *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    K = K_young if age_group == 0 else K_old

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-based WM mixture and decay per trial
        # If nS <= K, WM is near full strength; if nS > K, WM weight and maintenance decline.
        wm_weight_ss = wm_weight_base * np.clip(K / max(nS, 1), 0.0, 1.0)
        # Passive decay strength grows with overload
        wm_decay_strength = np.clip(1.0 - np.clip(K / max(nS, 1), 0.0, 1.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of chosen action using normalization trick
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax with its own inverse temperature
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_ss * p_wm + (1.0 - wm_weight_ss) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha * delta

            # WM passive decay toward uniform each trial (interference increases with overload)
            w = (1.0 - wm_decay_strength) * w + wm_decay_strength * (1.0 / nA)

            # WM update on reward: store item as one-hot if rewarded
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with WM-modulated exploration + WM decay and stochastic storage.

    Mechanism:
    - RL system: age-dependent learning rate (young vs old), base softmax inverse temperature.
      The effective inverse temperature increases with WM "confidence", reducing exploration when WM is confident.
    - WM system: each rewarded action is stored stochastically with probability p_store; WM traces decay each trial.
      WM policy is near-deterministic softmax over WM weights.
    - Mixture: the contribution of WM is proportional to a confidence signal derived from WM sharpness and penalized by set size.

    Parameters (model_parameters):
    - alpha_young: RL learning rate for young (0..1)
    - alpha_old: RL learning rate for old (0..1)
    - beta_base: base RL inverse temperature (scaled by 10 internally)
    - wm_conf_weight: scales how strongly WM confidence increases RL beta and WM mixture (0..1+)
    - p_store: probability to store WM on rewarded trials (0..1)
    - wm_decay: per-trial WM decay rate toward uniform (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_young, alpha_old, beta_base, wm_conf_weight, p_store, wm_decay = model_parameters
    beta_rl_base = beta_base * 10.0
    beta_wm = 50.0  # near-deterministic WM softmax

    age_group = 0 if age[0] <= 45 else 1
    alpha = alpha_young if age_group == 0 else alpha_old

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # WM confidence: how peaked W_s is above uniform
            # conf in [0, 1]; 0 means uniform, 1 means perfectly one-hot
            conf = np.clip((np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA), 0.0, 1.0)
            # Penalize confidence by set size (higher set size -> lower effective confidence)
            conf_ss = conf * (3.0 / max(nS, 1))
            conf_ss = np.clip(conf_ss, 0.0, 1.0)

            # RL inverse temperature boosted by WM confidence (less exploration when WM is reliable)
            beta_rl_eff = beta_rl_base * (1.0 + wm_conf_weight * conf_ss)

            # RL policy
            denom_rl = np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(denom_wm, 1e-12)

            # Mixture weight proportional to confidence
            wm_weight = np.clip(wm_conf_weight * conf_ss, 0.0, 1.0)
            p_total = wm_weight * p_wm_det + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha * delta

            # WM decay (global interference each trial)
            w = (1.0 - wm_decay) * w + wm_decay * (1.0 / nA)

            # WM stochastic storage on reward
            if r > 0.0:
                if np.random.rand() < p_store:
                    w[s, :] = 0.0
                    w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size- and age-dependent forgetting + WM win-stay heuristic with load-dependent recall.

    Mechanism:
    - RL system: single learning rate and softmax with inverse temperature. Includes forgetting toward uniform,
      with forgetting rate depending on age (young vs old).
    - WM system: win-stay heuristic. If the state had a previous rewarded action, WM favors that action strongly;
      otherwise WM is close to uniform. WM recall probability is a logistic function of set size.
    - Mixture: WM contribution scales with recall probability; the rest is RL.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - forget_young: RL forgetting rate toward uniform for young (0..1)
    - forget_old: RL forgetting rate toward uniform for old (0..1)
    - wm_weight_base: base mixture weight for WM (0..1)
    - theta_load: load sensitivity controlling how set size reduces WM recall (>0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta_rl, forget_young, forget_old, wm_weight_base, theta_load = model_parameters
    beta_rl *= 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    f_rl = forget_young if age_group == 0 else forget_old

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM memory traces
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM stores last rewarded action as one-hot; otherwise uniform
        w = (1.0 / nA) * np.ones((nS, nA))

        # WM recall probability: logistic that decreases with set size
        # p_recall = wm_weight_base * sigmoid(-theta_load * (nS - 3))
        x = -theta_load * (nS - 3.0)
        p_recall = wm_weight_base * (1.0 / (1.0 + np.exp(-x)))
        p_recall = np.clip(p_recall, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: deterministic softmax over W_s
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = p_recall * p_wm + (1.0 - p_recall) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL forgetting toward uniform before update
            q = (1.0 - f_rl) * q + f_rl * (1.0 / nA)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = q[s, a] + alpha * delta

            # WM update: win-stay â€“ if reward, store last action as one-hot; if no reward, reset to near-uniform
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On loss, reduce confidence (blend with uniform)
                w[s, :] = 0.5 * w[s, :] + 0.5 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p

Model notes:
- Model 1 uses an explicit WM capacity K that differs by age, which gates the WM mixture and induces interference-driven decay when set size exceeds capacity.
- Model 2 links WM sharpness to RL exploration: when WM is strong, RL becomes more exploitative. Age affects learning rate, and WM stores are stochastic and decay over time.
- Model 3 adds age-dependent RL forgetting and a WM recall probability that declines with set size via a logistic function, with a win-stay-like WM update rule.

All parameters are used meaningfully, age and set size modulate behavior in different ways across models, and the parameter combinations differ from those tried previously.