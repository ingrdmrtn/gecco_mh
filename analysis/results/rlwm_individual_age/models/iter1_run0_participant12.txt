Below are three standalone cognitive models tailored for the RLWM task. Each returns the negative log-likelihood of the observed choices. They use both RL and WM components, modulate working-memory engagement by set size (3 vs 6) and age group (0=young, 1=old), and keep parameters â‰¤ 6 per model.

Note: These functions assume numpy is available as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited WM engagement and WM decay.
    
    Model idea:
    - Choices are a mixture of RL and WM policies.
    - WM engagement is reduced when set size exceeds an internal capacity and is lower for older adults.
    - WM values decay toward uniform baseline each trial; WM learns quickly from reward.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight before modulations (0..1)
    - softmax_beta: RL inverse temperature (>0)
    - wm_decay: WM decay toward baseline each trial (0..1)
    - wm_learn: WM learning rate toward one-hot on rewarded trials (0..1)
    - cap_slope: Slope controlling how WM weight drops when set size > capacity (positive)
    
    Age and set-size modulation:
    - Effective WM weight = wm_weight_base * sigmoid(cap_slope*(capacity - nS)) * age_factor
    - capacity fixed at 4; age_factor = 1 for young, 0.7 for old.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_learn, cap_slope = model_parameters
    softmax_beta = softmax_beta * 10.0  # higher upper bound as in template
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    
    # Age group coding: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1
    age_factor = 1.0 if age_group == 0 else 0.7
    capacity = 4.0
    
    eps = 1e-12
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Compute effective WM mixture weight for this block
        # capacity effect: higher for smaller set sizes
        wm_size_factor = 1.0 / (1.0 + np.exp(-cap_slope * (capacity - nS)))
        wm_weight_eff = np.clip(wm_weight_base * wm_size_factor * age_factor, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            
            # RL policy: probability of chosen action (softmax trick centered on chosen action)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # WM policy: softmax on WM values
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] = q[s, a] + lr * delta
            
            # WM decay toward baseline
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            
            # WM learning: on rewarded trials, write one-hot; on non-reward, a mild suppression
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot
            else:
                # Mild negative update to chosen action to reduce its WM strength
                w[s, a] = (1.0 - 0.5 * wm_learn) * w[s, a] + 0.5 * wm_learn * (1.0 / nA)
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with asymmetric RL learning rates and choice perseveration bias.
    
    Model idea:
    - RL uses separate learning rates for positive vs negative prediction errors.
    - Choices are influenced by a perseveration term (stickiness) that biases repeating the last action.
    - WM contributes as a mixture that weakens with larger set sizes and in older adults.
    - WM decays toward baseline each trial; learns from reward.
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1)
    - lr_neg: RL learning rate for negative PE (0..1)
    - softmax_beta: RL inverse temperature (>0)
    - wm_weight_base: Base WM mixture weight (0..1)
    - persev_tau: Choice perseveration strength (>=0)
    - wm_decay: WM decay toward baseline (0..1)
    
    Age and set-size modulation:
    - Effective WM weight = wm_weight_base * (3.0 / nS) * age_factor
      where age_factor = 1.0 for young, 0.6 for old.
    - Perseveration is stronger in older adults: persev_eff = persev_tau * (1.0 + 0.5*age_group)
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, persev_tau, wm_decay = model_parameters
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    
    age_group = 0 if age[0] <= 45 else 1
    age_factor = 1.0 if age_group == 0 else 0.6
    persev_eff = persev_tau * (1.0 + 0.5 * age_group)
    
    eps = 1e-12
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Effective WM mixture weight for this block
        wm_weight_eff = np.clip(wm_weight_base * (3.0 / float(nS)) * age_factor, 0.0, 1.0)
        
        # Initialize perseveration bias (counts last chosen action per state)
        last_action = -np.ones(nS, dtype=int)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            
            # RL with perseveration: add bias to Q values for last chosen action in that state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] = Q_s[last_action[s]] + persev_eff
            
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] = q[s, a] + alpha * pe
            
            # WM decay toward baseline
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            
            # WM learning: strong write on reward; mild weakening on no-reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot
            else:
                w[s, a] = 0.8 * w[s, a] + 0.2 * (1.0 / nA)
            
            # Update perseveration memory
            last_action[s] = a
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q forgetting + WM gating failures + lapse.
    
    Model idea:
    - RL: standard delta-rule with forgetting toward uniform.
    - WM: on rewarded trials, gate the action into WM (one-shot write); gate can fail with
      probability that increases with set size and age. WM decays toward baseline.
    - Action selection: mixture of WM and RL, followed by an epsilon lapse (uniform random).
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (>0)
    - wm_weight_base: Base WM mixture weight (0..1)
    - wm_gate_slope: Slope controlling WM gate success vs set size/age (positive)
    - lapse_eps: Lapse probability (0..0.2)
    - rl_forget: RL forgetting rate toward uniform each trial (0..1)
    
    Age and set-size modulation:
    - WM gate success probability g = sigmoid(wm_gate_slope*(4 - nS) - 0.8*age_group)
    - Effective WM weight = wm_weight_base * g
    """
    lr, softmax_beta, wm_weight_base, wm_gate_slope, lapse_eps, rl_forget = model_parameters
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    
    age_group = 0 if age[0] <= 45 else 1
    
    eps = 1e-12
    blocks_log_p = 0.0
    
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # WM gate success based on set size and age
        g = 1.0 / (1.0 + np.exp(-(wm_gate_slope * (4.0 - nS) - 0.8 * age_group)))
        wm_weight_eff = np.clip(wm_weight_base * g, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            
            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # Mixture WM/RL
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            # Lapse to uniform
            p_total = (1.0 - lapse_eps) * p_mix + lapse_eps * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL forgetting toward uniform
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)
            # RL update
            pe = r - q[s, a]
            q[s, a] = q[s, a] + lr * pe
            
            # WM decay toward baseline
            w = 0.9 * w + 0.1 * w_0
            
            # WM gating write: if reward, write action with probability g (implemented deterministically via g-weighted blend)
            if r > 0.5:
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                # Deterministic equivalent of expected-gate update:
                w[s, :] = (1.0 - g) * w[s, :] + g * one_hot
            else:
                # No write on non-reward; slight drift handled by decay
                pass
        
        blocks_log_p += log_p
    
    return -blocks_log_p