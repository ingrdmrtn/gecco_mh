def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying WM with per-state perseveration bias.
    Mixture weight is determined by WM capacity relative to set size and age.

    Mechanism
    - RL: tabular Q-learning with inverse temperature beta (scaled by 10 internally).
      Adds a perseveration bonus to the last-chosen action within each state.
    - WM: one-shot encoding on rewards with decay toward uniform each trial.
    - Mixture: WM weight equals a capacity factor min(1, K/nS), where K depends on age.
      Young/old differences implemented via K = slots_base + age_slot_shift*age_group.

    Parameters (6)
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL; scaled by 10 internally
    - wm_decay: decay rate for WM values toward uniform each trial (0..1)
    - slots_base: baseline WM capacity in slots (real-valued, e.g., 1..6)
    - age_slot_shift: additive change in slots for older group (young=0 shift, old adds this)
    - kappa: perseveration strength added to the last chosen action in a state (can be +/-)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_decay, slots_base, age_slot_shift, kappa = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL policy

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic; WM retrieval assumed sharp

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables, initialized uniform
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for perseveration
        last_choice = -1 * np.ones(nS, dtype=int)

        # Capacity-limited mixture weight based on age and set size
        K = slots_base + age_slot_shift * age_group
        wm_weight = np.clip(K / max(1.0, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply WM decay toward uniform across all states each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # RL policy with perseveration bias on last action in this state
            Q_s = q[s, :].copy()
            if last_choice[s] >= 0:
                Q_s[last_choice[s]] += kappa
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from current WM row
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: one-shot encode only when rewarded
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update perseveration memory
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and Q-forgetting + WM mixture reduced in larger sets.

    Mechanism
    - RL: tabular Q-learning with separate learning rates for positive vs negative outcomes.
      Older group has a different negative learning rate than younger group (age-specific).
      Includes Q-forgetting that shrinks the active state's Q-row toward uniform after each update.
    - WM: one-shot encoding on reward; decays toward uniform using the same forgetting rate
      (interpreted as interference/decay).
    - Mixture: fixed WM weight scaled by capacity factor min(1, 3/nS) to capture set-size cost
      without extra parameters.

    Parameters (6)
    - lr_pos: learning rate for rewarded trials (0..1)
    - lr_neg_young: learning rate for unrewarded trials in young group (0..1)
    - lr_neg_old: learning rate for unrewarded trials in old group (0..1)
    - softmax_beta: RL inverse temperature; scaled by 10 internally
    - q_forget: forgetting/interference strength toward uniform (0..1) applied to both Q and WM
    - wm_weight_base: base WM mixture weight before set-size scaling (0..1)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg_young, lr_neg_old, softmax_beta, q_forget, wm_weight_base = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    lr_neg = lr_neg_young if age_group == 0 else lr_neg_old

    softmax_beta_wm = 50.0  # WM retrieval precision

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM availability factor (no extra parameter)
        capacity_factor = min(1.0, 3.0 / max(1.0, nS))
        wm_weight = np.clip(wm_weight_base * capacity_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Policies
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            if r > 0.0:
                q[s, a] += lr_pos * (r - q[s, a])
            else:
                q[s, a] += lr_neg * (r - q[s, a])

            # Q-forgetting toward uniform for the active state row
            q[s, :] = (1.0 - q_forget) * q[s, :] + q_forget * (1.0 / nA)

            # WM decay/interference toward uniform
            w = (1.0 - q_forget) * w + q_forget * w_0

            # WM one-shot encoding on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + confidence-gated WM mixture with set-size effect on WM precision and age effect on WM reliance.

    Mechanism
    - RL: standard Q-learning with inverse temperature beta (scaled by 10).
    - WM: one-shot encoding; WM retrieval precision degrades with larger set sizes by scaling the
      effective discriminability of W_s (implemented by scaling W_s, not changing beta constant).
    - Mixture: WM weight is trial-wise gated by the WM confidence for the current state:
        conf = (max(W_s) - mean(W_s)) / (1 - 1/nA), in [0,1]
      Final weight = clip(wm_weight_base + age_wm_shift*age_group, 0,1) * conf.
      Thus, if WM is uninformative (uniform), conf=0 and policy reverts to RL.
      Age group modulates reliance on WM directly.

    Parameters (6)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled by 10 internally
    - beta_wm_base: base WM discriminability scaling factor (>0); larger => more precise WM
    - wm_weight_base: base WM reliance (0..1)
    - size_wm_scale: scales how much set size reduces WM discriminability:
        tau = beta_wm_base / (1 + size_wm_scale * max(0, nS-3))
    - age_wm_shift: additive shift to WM reliance for older group only (can be +/-)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, beta_wm_base, wm_weight_base, size_wm_scale, age_wm_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # fixed; we modulate WM discriminability by scaling W_s

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # WM discriminability scaling as a function of set size
        tau = beta_wm_base / (1.0 + size_wm_scale * max(0, nS - 3))

        # Age-adjusted base WM reliance
        wm_base_eff = np.clip(wm_weight_base + (age_wm_shift if age_group == 1 else 0.0), 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]

            # Confidence gating from WM row for the current state
            conf_raw = np.max(W_s) - np.mean(W_s)
            conf = conf_raw / (1.0 - 1.0 / nA)  # normalize to [0,1]
            conf = np.clip(conf, 0.0, 1.0)

            wm_weight = wm_base_eff * conf

            # Implement WM precision modulation by scaling W_s (equivalent to scaling beta)
            W_scaled = tau * W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_scaled - W_scaled[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: on reward, store the correct action deterministically
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # no explicit decay here; confidence gating already down-weights uncertain WM

        blocks_log_p += log_p

    return -blocks_log_p