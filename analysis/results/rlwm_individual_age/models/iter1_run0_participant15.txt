def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + outcome-gated WM mixture with lapse and global WM decay.

    Policy:
    - Mixture of RL softmax and WM softmax:
        p_total = (1 - lapse_age) * [wm_weight_trial * p_wm + (1 - wm_weight_trial) * p_rl] + lapse_age * (1/nA)
      where p_rl is a softmax over Q(s,a) and p_wm is a softmax over W(s,a).

    WM dynamics:
    - Global decay toward a uniform prior per trial (wm_decay).
    - If rewarded, reinforce the chosen action in WM (one-shot boost).
    - WM influence on policy is outcome-gated on each trial and reduced by set size (more load â†’ smaller WM influence).
      WM influence is also reduced for the older group.

    RL dynamics:
    - Standard delta rule with a single learning rate.

    Age and set-size dependence:
    - wm_weight reduced for larger set sizes and for older adults.
    - Lapse increases with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_weight_base: base WM mixture weight in [0,1]
    - wm_conf_boost: scales outcome gating of WM weight (>=0)
    - wm_decay: WM decay toward uniform per trial in [0,1]
    - lapse: base lapse probability in [0,1] that increases with set size and age

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_conf_boost, wm_decay, lapse = model_parameters

    softmax_beta *= 10.0  # higher upper bound
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Outcome-gated and load-adjusted WM weight
            wm_weight_eff = wm_weight_base * (1.0 - 0.4 * age_group)  # older rely less on WM
            # Reduce WM weight under higher load (nS=6) and boost on rewarded trials
            load_scale = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
            outcome_boost = (1.0 + wm_conf_boost * r) / (1.0 + wm_conf_boost)
            wm_weight_trial = np.clip(wm_weight_eff * load_scale * outcome_boost, 0.0, 1.0)

            # Lapse increases with load and age
            lapse_age = lapse * (1.0 + 0.2 * (nS == 6) + 0.5 * age_group)
            lapse_age = np.clip(lapse_age, 0.0, 0.5)

            p_mix = wm_weight_trial * p_wm + (1.0 - wm_weight_trial) * p_rl
            p_total = (1.0 - lapse_age) * p_mix + lapse_age * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # WM decay toward prior
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM one-shot reinforcement (stronger if rewarded)
            if r > 0.5:
                # Shift mass toward the chosen action
                alpha_wm = 0.8 * (1.0 + 0.5 * (outcome_boost - 0.5))  # slightly modulated by wm_conf_boost via outcome_boost
                alpha_wm = np.clip(alpha_wm, 0.0, 1.0)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q-decay and choice stickiness + WM with interference (set-size and age dependent).

    Policy:
    - Mixture of RL softmax (with stickiness to last action in the state) and WM softmax:
        p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
      where p_rl is computed on Q(s,a) plus a stickiness bias for the last chosen a in that state.

    RL dynamics:
    - Delta-rule learning with learning rate lr.
    - Q-decay toward a uniform prior each trial:
        q[s,:] = (1 - q_decay_eff) * q[s,:] + q_decay_eff * uniform
      where q_decay_eff increases with set size and age.

    WM dynamics:
    - Interference-based decay toward uniform with rate depending on set size and age:
        w[s,:] = (1 - d_eff) * w[s,:] + d_eff * uniform
    - If rewarded, store a one-shot memory (set W_s to one-hot at chosen action).
    - WM interference d_eff increases with set size and age.

    Age dependence:
    - Interference is higher and Q-decay stronger for older participants.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_weight: mixture weight in [0,1]
    - gamma_interf: base WM interference factor (>=0)
    - q_decay: base Q decay rate toward uniform in [0,1]
    - stickiness: choice stickiness added to last chosen action in the current state (can be >=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight, gamma_interf, q_decay, stickiness = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective parameters
        # Interference grows with set size and age
        gamma_eff = gamma_interf * (1.0 + 0.5 * age_group) * (1.0 + 0.5 * (nS == 6))
        # Convert gamma to a bounded decay rate in [0,1] using 1 - exp(-gamma)
        d_eff = 1.0 - np.exp(-gamma_eff)
        d_eff = np.clip(d_eff, 0.0, 1.0)

        # Q-decay increases with set size and age
        q_decay_eff = q_decay * (1.0 + 0.5 * age_group) * (1.0 + 0.5 * (nS == 6))
        q_decay_eff = np.clip(q_decay_eff, 0.0, 1.0)

        # Age-modulated WM weight (older slightly less WM)
        wm_weight_eff = np.clip(wm_weight * (1.0 - 0.3 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply Q decay toward uniform before policy
            q[s, :] = (1.0 - q_decay_eff) * q[s, :] + q_decay_eff * w_0[s, :]

            Q_s = q[s, :].copy()

            # Add stickiness to last chosen action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] = q[s, a] + lr * delta

            # WM interference decay toward uniform
            w[s, :] = (1.0 - d_eff) * w[s, :] + d_eff * w_0[s, :]

            # WM one-shot store on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM precision gating.

    Policy:
    - Mixture of RL softmax and WM softmax:
        p_total = wm_weight_s * p_wm + (1 - wm_weight_s) * p_rl
      where WM weight is computed dynamically from a Bayesian precision signal.

    WM (Bayesian counts):
    - Maintain Dirichlet-like counts C[s,a], initialized with init_precision/nA for each action.
    - Treat rewards as evidence for the chosen action; increment C[s,a] by r.
    - WM values W_s = normalized counts C[s,:] / sum(C[s,:]).
    - WM precision for state s is the total count sum(C[s,:]).
    - WM weight wm_weight_s = precision / (precision + kappa_setsize * (1 + 0.7*age_group))
      so that higher precision increases WM influence; larger kappa and older age reduce WM influence.

    RL dynamics:
    - Standard delta-rule update with learning rate lr.

    Set-size and age dependence:
    - kappa differs by set size (kappa3 vs kappa6), and is amplified for the older group.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - kappa3: precision offset for set size 3 (>=0)
    - kappa6: precision offset for set size 6 (>=0)
    - beta_wm_scale: scales WM inverse temperature: beta_wm = 10 * beta_wm_scale
    - init_precision: prior total count per state (>=0), spread uniformly across actions

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, kappa3, kappa6, beta_wm_scale, init_precision = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = max(1e-3, 10.0 * beta_wm_scale)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will mirror normalized counts
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize Bayesian counts
        C = (init_precision / nA) * np.ones((nS, nA))

        # Choose kappa by set size and adjust by age
        base_kappa = kappa3 if nS == 3 else kappa6
        kappa_eff = base_kappa * (1.0 + 0.7 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update WM table from counts before policy (reflect current precision)
            total_prec = np.sum(C[s, :])
            if total_prec <= 0:
                W_s = w_0[s, :].copy()
            else:
                W_s = C[s, :] / total_prec
            w[s, :] = W_s

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Precision-gated WM weight
            wm_weight_s = total_prec / (total_prec + kappa_eff + 1e-12)
            wm_weight_s = np.clip(wm_weight_s, 0.0, 1.0)

            p_total = wm_weight_s * p_wm + (1.0 - wm_weight_s) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # Bayesian WM counts update: add reward evidence to chosen action
            # Reward increases confidence in chosen action; no increment on 0 reward.
            C[s, a] += r

        blocks_log_p += log_p

    return -blocks_log_p