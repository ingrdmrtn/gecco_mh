def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with WM decay and set-size-dependent WM gating.

    The model blends a model-free RL softmax policy with a working-memory (WM) softmax policy.
    WM is assumed to be reliable in small set sizes and decays with time; its influence is reduced in larger set sizes.
    Age modulates the determinism of WM retrieval (older -> noisier WM), while young participants maintain more deterministic WM.

    Parameters (list of 6):
    - lr: RL learning rate (0-1)
    - wm_bias: base logit for WM mixture weight (real-valued; mapped via sigmoid to [0,1])
    - softmax_beta: base inverse temperature for RL (scaled x10 internally)
    - wm_decay: WM decay rate per trial for the currently visited state (0-1)
    - wm_beta: base inverse temperature for WM (determinism of WM retrieval)
    - gamma_setsize: slope reducing WM weight as set size increases (real-valued; negative reduces WM at larger set size)

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of binary rewards per trial
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes for the current block per trial (3 or 6)
    - age: array with a single repeated value, participant's age
    - model_parameters: list with the 6 parameters above

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_bias, softmax_beta, wm_decay, wm_beta, gamma_setsize = model_parameters
    softmax_beta *= 10.0

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # WM temperature adjusted by age (older participants have noisier WM)
    # Keep it meaningful without extra parameters
    softmax_beta_wm = max(1e-3, wm_beta) * (1.0 - 0.3 * age_group)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline WM prior (uniform)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM softmax probability of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture weight depends on set size (smaller set -> higher WM) via a logit link
            # wm_weight = sigmoid(wm_bias + gamma_setsize * (3 - nS))
            wm_logit = wm_bias + gamma_setsize * (3 - nS)
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))
            # Total choice probability
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and update:
            # Decay toward uniform prior within the current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # If reward is positive, store the correct mapping strongly
            if r > 0:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
                # Overwrite to emphasize current rewarded action
                w[s, :] = w[s, :] * 0.0 + (1.0 / (nA - 0.999))  # keep numeric
                w[s, :] = w_0[s, :]  # reset baseline
                w[s, a] = 1.0  # strong memory of rewarded action
                # renormalize
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates, Q-value forgetting, and WM gating by set size and age.

    - RL uses separate learning rates for positive and negative prediction errors.
    - Q-values are subject to forgetting toward uniform, stronger when set size is larger and for older adults.
    - WM provides an additional policy that emphasizes recently rewarded actions; its mixture weight decreases with set size
      and is modulated by age.

    Parameters (list of 6):
    - lr_pos: learning rate for positive PE (rewarded trials)
    - lr_neg: learning rate for negative PE (unrewarded trials)
    - wm_weight_base: base logit for WM mixture weight (mapped with sigmoid)
    - softmax_beta: base inverse temperature for RL (scaled x10)
    - q_forget_rate: base forgetting rate for Q-values (0-1)
    - age_mod_wm: additive boost to WM weight for young (age_group=0) relative to old; used in WM weight logit

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial
    - age: array with repeated participant age
    - model_parameters: list with 6 parameters above

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, q_forget_rate, age_mod_wm = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # WM temperature: high but slightly reduced for older adults
    softmax_beta_wm = 40.0 * (1.0 - 0.3 * age_group)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM softmax probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # WM mixture weight: decreases with set size, boosted for young by age_mod_wm
            wm_logit = wm_weight_base + (1 - age_group) * age_mod_wm + (-1.0) * (nS - 3)
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # Q-value forgetting toward uniform; stronger for larger set sizes and older adults
            phi = q_forget_rate * ((nS - 3) / 3.0) * (1.0 + 0.5 * age_group)
            phi = np.clip(phi, 0.0, 1.0)
            q[s, :] = (1.0 - phi) * q[s, :] + phi * (1.0 / nA)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM update: emphasize last rewarded action; slight suppression on negative outcomes
            # Move WM toward uniform slightly (recency decay)
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0:
                # Strongly favor current action
                w[s, :] = 0.1 * w[s, :] + 0.9 * w_0[s, :]
                w[s, a] = 1.0
            else:
                # Slightly downweight the chosen action on negative feedback
                w[s, a] = 0.5 * w[s, a]
            # Renormalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM arbitration based on uncertainty (entropy), with perseveration bias.

    - RL updated with a single learning rate.
    - WM stores recent rewarded actions; its determinism depends on a WM temperature parameter.
    - Arbitration weight increases when set size is small and when RL's uncertainty (higher entropy) is low,
      favoring WM when it is likely to be reliable.
    - Includes a perseveration bias that favors repeating the previous action taken in the same state.
    - Age modulates exploration: older adults have lower inverse temperature (more exploration).

    Parameters (list of 6):
    - lr: RL learning rate (0-1)
    - beta_base: base RL inverse temperature (scaled x10)
    - wm_base: base logit for WM mixture weight
    - wm_temp: WM inverse temperature (determinism of WM policy)
    - inv_temp_age_effect: fractional reduction of inverse temperatures for older adults (0-1)
    - perseveration: strength of action stickiness bias added to chosen action's value when state repeats

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial
    - age: array with repeated participant age
    - model_parameters: list with 6 parameters above

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, beta_base, wm_base, wm_temp, inv_temp_age_effect, perseveration = model_parameters
    # Base RL temperature
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    # Age reduces inverse temperatures (older -> more exploration)
    beta_age_factor = 1.0 - inv_temp_age_effect * age_group
    softmax_beta = max(1e-3, softmax_beta * beta_age_factor)
    softmax_beta_wm = max(1e-3, wm_temp * beta_age_factor)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action taken per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add perseveration bias to RL values for repeating last action in the same state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            # RL softmax probability for chosen action a
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM softmax probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration: depend on set size (smaller -> higher WM) and RL certainty (lower entropy -> higher WM)
            # Compute RL policy over actions to estimate entropy
            exp_vals = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi = exp_vals / np.sum(exp_vals)
            entropy = -np.sum(pi * np.log(np.clip(pi, eps, 1.0)))  # in nats, 0..~log(3)

            # Transform entropy into a confidence signal: higher confidence -> lower entropy
            # WM reliance is stronger when entropy is low (RL is confident) AND set size is small
            # Map to logit: wm_base + k1*(3 - nS) - k2*entropy
            k1 = 1.0  # set size slope
            k2 = 1.0  # entropy slope
            wm_logit = wm_base + k1 * (3 - nS) - k2 * entropy
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: recency-weighted store of rewarded actions
            # Slight drift back to uniform
            w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
            if r > 0:
                # Strongly encode the rewarded action
                w[s, :] = 0.1 * w[s, :] + 0.9 * w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

            # Update perseveration tracker
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p