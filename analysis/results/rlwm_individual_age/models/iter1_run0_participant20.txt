def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dual learning rates + novelty bonus, combined with capacity-limited WM and age penalty.

    Policy
    - Mixture of an RL softmax and a WM softmax (beta_wm very large).
    - WM weight is reduced by set size and further penalized by age group.
    - RL includes an exploration novelty bonus that decays with action-state visit count.

    Learning
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores the most recent rewarded action for each state (one-shot) and forgets toward uniform.
      Forgetting increases with set size and age.

    Parameters
    ----------
    states : array-like of int
        State per trial in [0..nS-1] for the block.
    actions : array-like of int
        Chosen action per trial in [0..2].
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial/block.
    age : array-like or scalar
        Participant age; used to compute age_group (0=young, 1=old).
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, wm_weight_base, eta_novelty, age_wm_penalty]
        - alpha_pos: RL learning rate for positive PE.
        - alpha_neg: RL learning rate for negative PE.
        - beta_base: base inverse temperature for RL (internally scaled by 10).
        - wm_weight_base: base WM mixture weight before set-size/age scaling.
        - eta_novelty: novelty bonus strength added to RL values as eta/sqrt(visit_count+1).
        - age_wm_penalty: scales the WM weight penalty in older/large set size conditions.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available

    alpha_pos, alpha_neg, beta_base, wm_weight_base, eta_novelty, age_wm_penalty = model_parameters
    beta = beta_base * 10.0
    beta_wm = 50.0

    # Age group coding
    if np.isscalar(age):
        age_group = 0 if age <= 45 else 1
    else:
        age_group = 0 if age[0] <= 45 else 1

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx].astype(int)
        block_rewards = np.asarray(rewards)[idx].astype(float)
        block_states = np.asarray(states)[idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q, WM table w, and uniform prior
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for novelty bonus
        visit_counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL with novelty bonus
            Qs = q[s, :].copy()
            # novelty bonus favors less-visited actions in the current state
            bonus = eta_novelty / np.sqrt(visit_counts[s, :] + 1.0)
            logits_rl = beta * (Qs + bonus)
            logits_rl -= np.max(logits_rl)
            prl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM policy (deterministic softmax on w)
            Ws = w[s, :]
            logits_wm = beta_wm * Ws
            logits_wm -= np.max(logits_wm)
            pwm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # Capacity/age-modulated WM weight
            # Base reduction with set size, additional penalty with age_group and set size
            penalty = (nS / 6.0) * (age_group * age_wm_penalty)
            wm_weight = wm_weight_base * (3.0 / nS) * (1.0 - penalty)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * pwm + (1.0 - wm_weight) * prl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning with dual learning rates
            pe = r - Qs[a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM updating with forgetting toward uniform
            # Forgetting increases with set size and age
            f = (nS - 3.0) / 3.0  # 0 for set size 3, 1 for 6
            f = np.clip(f, 0.0, 1.0)
            forget_rate = 0.05 + 0.25 * f + 0.25 * age_group * age_wm_penalty
            forget_rate = np.clip(forget_rate, 0.0, 1.0)
            w[s, :] = (1.0 - forget_rate) * w[s, :] + forget_rate * w0[s, :]

            # Reward-dependent one-shot imprint
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # weak suppression of the chosen action on errors
                target = w[s, :].copy()
                target[a] = max(0.0, target[a] - 0.2)
                # renormalize softly toward uniform
                target = 0.5 * target + 0.5 * w0[s, :]

            # Apply a fast update of WM toward target
            wm_alpha = 0.6
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target
            # normalize
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update novelty counts
            visit_counts[s, a] += 1.0

        total_log_p += log_p

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with age- and set-size dependent lapses.

    Policy
    - RL softmax over Q-values.
    - WM softmax over fast memory weights w.
    - Arbitration weight for WM depends on RL uncertainty (entropy of RL policy) and a
      WM reliability parameter scaled by set size and age.
    - A global lapse epsilon mixes uniform random choice; epsilon increases with set size and age.

    Learning
    - RL with single learning rate.
    - WM: fast strengthening on reward and mild reversion to uniform on no reward,
      using the same wm_reliability as a step-size.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array
        [alpha, beta_base, wm_reliability, k_arbitration, epsilon_base]
        - alpha: RL learning rate.
        - beta_base: base inverse temperature for RL (scaled by 10).
        - wm_reliability: baseline WM fidelity and update step-size.
        - k_arbitration: slope mapping (WM reliability - RL entropy) to WM weight via sigmoid.
        - epsilon_base: base lapse rate (scaled by set size and age).

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    import numpy as np  # assumed available

    alpha, beta_base, wm_reliability, k_arbitration, epsilon_base = model_parameters
    beta = beta_base * 10.0
    beta_wm = 50.0

    if np.isscalar(age):
        age_group = 0 if age <= 45 else 1
    else:
        age_group = 0 if age[0] <= 45 else 1

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx].astype(int)
        block_rewards = np.asarray(rewards)[idx].astype(float)
        block_states = np.asarray(states)[idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Qs = q[s, :]

            # RL policy
            logits_rl = beta * Qs
            logits_rl -= np.max(logits_rl)
            prl_vec = np.exp(logits_rl)
            prl_vec /= np.sum(prl_vec)
            prl = prl_vec[a]

            # RL entropy as uncertainty signal (0..ln nA)
            entropy_rl = -np.sum(prl_vec * (np.log(prl_vec + 1e-12)))

            # WM policy
            Ws = w[s, :]
            logits_wm = beta_wm * Ws
            logits_wm -= np.max(logits_wm)
            pwm_vec = np.exp(logits_wm)
            pwm_vec /= np.sum(pwm_vec)
            pwm = pwm_vec[a]

            # Effective WM reliability decreases with set size and age
            wm_rel_eff = wm_reliability * (3.0 / nS) * (1.0 - 0.3 * age_group)
            wm_rel_eff = np.clip(wm_rel_eff, 0.0, 1.0)

            # Arbitration: higher WM weight when WM is reliable and RL is uncertain
            # Normalize entropy by ln(nA)
            Hmax = np.log(nA)
            x = wm_rel_eff - (entropy_rl / (Hmax + 1e-12))
            wm_weight = 1.0 / (1.0 + np.exp(-k_arbitration * x))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Lapse increases with set size and age
            eps = epsilon_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
            eps = np.clip(eps, 0.0, 0.99)

            p_mix = wm_weight * pwm + (1.0 - wm_weight) * prl
            p_total = (1.0 - eps) * p_mix + eps * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Qs[a]
            q[s, a] += alpha * pe

            # WM update: fast on reward, soften on no reward
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                step = np.clip(wm_rel_eff, 0.0, 1.0)
            else:
                target = 0.7 * w0[s, :] + 0.3 * w[s, :]
                target[a] = 0.5 * target[a]  # slightly suppress chosen after error
                step = 0.3 * np.clip(wm_rel_eff, 0.0, 1.0)

            w[s, :] = (1.0 - step) * w[s, :] + step * target
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        total_log_p += log_p

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility trace and action bias, mixed with a win-stay/lose-shift WM heuristic.

    Policy
    - Mixture between RL softmax and WM softmax.
    - RL includes a constant bias toward action 0; bias increases slightly with age group.
    - WM implements a win-stay/lose-shift heuristic encoded in w.

    Learning
    - RL is updated with an eligibility trace (replacing trace for the chosen state-action).
    - WM sets a strong preference for the last rewarded action and flattens after losses.
    - WM weight decreases with set size.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array
        [alpha, beta_base, wm_weight_base, lambda_trace, bias0]
        - alpha: RL learning rate.
        - beta_base: base inverse temperature for RL (scaled by 10).
        - wm_weight_base: baseline WM weight (capacity affected via set size).
        - lambda_trace: eligibility trace decay (0..1).
        - bias0: bias added to action 0 in RL logits; amplified by age group.

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    import numpy as np  # assumed available

    alpha, beta_base, wm_weight_base, lambda_trace, bias0 = model_parameters
    beta = beta_base * 10.0
    beta_wm = 50.0

    if np.isscalar(age):
        age_group = 0 if age <= 45 else 1
    else:
        age_group = 0 if age[0] <= 45 else 1

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx].astype(int)
        block_rewards = np.asarray(rewards)[idx].astype(float)
        block_states = np.asarray(states)[idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Qs = q[s, :]

            # RL logits with action 0 bias (amplified for older group)
            bias_vec = np.zeros(nA)
            bias_vec[0] = bias0 * (1.0 + 0.3 * age_group)
            logits_rl = beta * Qs + bias_vec
            logits_rl -= np.max(logits_rl)
            prl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM policy (win-stay/lose-shift encoded in w)
            Ws = w[s, :]
            logits_wm = beta_wm * Ws
            logits_wm -= np.max(logits_wm)
            pwm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # WM weight scales down with set size
            wm_weight = np.clip(wm_weight_base * (3.0 / nS), 0.0, 1.0)

            p_total = wm_weight * pwm + (1.0 - wm_weight) * prl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces (replacing trace for chosen pair)
            # Decay all traces, then set chosen trace to 1
            e *= lambda_trace
            e[s, :] *= 0.0  # within-state, only chosen action gets a trace this trial
            e[s, a] = 1.0

            pe = r - Qs[a]
            q += alpha * pe * e  # update all state-actions according to current traces

            # WM update following win-stay/lose-shift heuristic
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                step = 0.7
            else:
                # lose: shift probability mass away from chosen action toward uniform
                target = 0.8 * w0[s, :] + 0.2 * w[s, :]
                target[a] = 0.3 * target[a]
                step = 0.4

            w[s, :] = (1.0 - step) * w[s, :] + step * target
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        total_log_p += log_p

    return -float(total_log_p)