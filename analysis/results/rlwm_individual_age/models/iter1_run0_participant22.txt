def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with WM weight modulated by set size and age via a logistic transform.

    Idea:
    - A single RL learning rate updates Q-values.
    - A working-memory (WM) store provides a near-deterministic policy for items currently held in WM.
    - The contribution (weight) of WM is not fixed; it is a logistic function of set size (3 vs 6) and age group.
      Larger set sizes and older age reduce WM reliance.

    Parameters
    ----------
    states : array-like, int
        Stimulus/state index per trial (0..set_size-1).
    actions : array-like, int
        Observed action per trial (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial (states reset across blocks).
    set_sizes : array-like, int
        Set size on each trial (3 or 6).
    age : array-like, int
        Participant age repeated across trials; age_group=0 if <=45, else 1.
    model_parameters : list or array
        [lr, wm_bias, wm_setsize_slope, wm_age_penalty, softmax_beta]
        - lr: RL learning rate (0..1).
        - wm_bias: baseline log-odds of relying on WM.
        - wm_setsize_slope: decrease in WM log-odds per item beyond 3.
        - wm_age_penalty: decrease in WM log-odds for older participants.
        - softmax_beta: baseline RL inverse temperature; internally scaled x10.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr, wm_bias, wm_setsize_slope, wm_age_penalty, softmax_beta = model_parameters
    softmax_beta = softmax_beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size- and age-modulated WM mixture weight via logistic transform
        # logit(wm_w) = wm_bias - wm_setsize_slope * max(0, nS-3) - wm_age_penalty * age_group
        logit_wm = wm_bias - wm_setsize_slope * max(0, nS - 3) - wm_age_penalty * age_group
        wm_w_eff = 1.0 / (1.0 + np.exp(-logit_wm))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax on WM weights (nearly deterministic)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update (one-shot store of rewarded action; mild decay otherwise)
            if r >= 0.5:
                # Store the correct mapping deterministically (one-hot with small smoothing)
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Gentle relaxation toward uniform baseline when feedback is uninformative
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates, WM mixture reduced by set size, Q-value decay, and age-driven perseveration.

    Idea:
    - Separate RL learning rates for positive/negative prediction errors.
    - Q-values decay toward uniform between trials (captures interference/forgetting), stronger globally.
    - WM contributes deterministically, but its weight scales as 3/nS (lower for set size 6).
    - Older participants exhibit stronger perseveration (stickiness) to last chosen action in a state.

    Parameters
    ----------
    states : array-like, int
        Stimulus/state index per trial (0..set_size-1).
    actions : array-like, int
        Observed action per trial (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size per trial (3 or 6).
    age : array-like, int
        Participant age repeated per trial; age_group=0 if <=45, else 1.
    model_parameters : list or array
        [lr_pos, lr_neg, wm_weight, softmax_beta, decay, kappa_age]
        - lr_pos: learning rate for positive PE.
        - lr_neg: learning rate for negative PE.
        - wm_weight: baseline WM mixture weight (scaled by 3/nS).
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - decay: Q-value decay rate toward uniform (0=no decay, 1=full reset each trial).
        - kappa_age: additional perseveration bonus applied only if age_group==1.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, decay, kappa_age = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action_per_state = -np.ones(nS, dtype=int)

        # WM weight reduced by set size
        wm_w_eff = np.clip(wm_weight * (3.0 / max(1, nS)), 0.0, 1.0)
        kappa_eff = kappa_age * age_group  # only older group perseverates

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply global Q decay toward uniform before acting
            q[s, :] = (1.0 - decay) * q[s, :] + decay * (1.0 / nA)

            # Perseveration bias: add a bonus to the previously chosen action in this state
            Q_s = q[s, :].copy()
            if last_action_per_state[s] >= 0:
                Q_s[last_action_per_state[s]] += kappa_eff

            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy with perseveration applied in the same way
            W_s = w[s, :].copy()
            if last_action_per_state[s] >= 0:
                W_s[last_action_per_state[s]] += kappa_eff
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update (asymmetric)
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: store rewarded action; decay otherwise
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            last_action_per_state[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with age-modulated capacity and a lapse component.

    Idea:
    - RL with a single learning rate drives gradual value learning.
    - WM is near-deterministic but can only support a limited 'capacity' that diminishes with age.
      Effective WM weight scales with capacity over set size: wm_w_eff = wm_cap / (nS^alpha), clipped to [0,1].
    - A small lapse (uniform random choice) is mixed into the final policy to capture attention lapses.

    Parameters
    ----------
    states : array-like, int
        Stimulus/state index per trial (0..set_size-1).
    actions : array-like, int
        Observed action per trial (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size per trial (3 or 6).
    age : array-like, int
        Participant age; age_group=0 if <=45, else 1.
    model_parameters : list or array
        [lr, softmax_beta, cap_base, cap_age_penalty, alpha_setsize, lapse]
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - cap_base: baseline WM capacity scale (roughly how many items can be supported at set size 3).
        - cap_age_penalty: reduction in capacity for older participants.
        - alpha_setsize: exponent controlling how capacity scales with set size (nS^alpha).
        - lapse: probability of uniform random choice (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, cap_base, cap_age_penalty, alpha_setsize, lapse = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age-modulated capacity and set-size scaling
        wm_cap = max(0.0, cap_base - cap_age_penalty * age_group)
        denom = max(1.0, (nS ** max(0.0, alpha_setsize)))
        wm_w_eff = np.clip(wm_cap / denom, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mix WM and RL, then add lapse to uniform
            p_mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: store rewarded action, mild decay otherwise
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)