Below are three alternative cognitive models that combine reinforcement learning (RL) and working memory (WM) mechanisms in different ways. Each model returns the negative log-likelihood of the observed choices, uses all parameters, and incorporates set-size and age effects meaningfully.

Note: Assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with capacity-weighted WM and decay-forgetting.
    
    Idea:
    - RL learns slowly and stochastically via a single learning rate.
    - WM stores the most recently rewarded action for each state and decays toward uniform
      when feedback is not rewarding (interference/forgetting).
    - WM mixture weight decreases with larger set size and with age (older group).
    - A small lapse (epsilon) blends in uniform random choice to capture occasional lapses.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback outcome.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size of the block (3 or 6).
    age : array-like or scalar
        Participant age. Age group coded with threshold at 45.
    model_parameters : list or array-like
        [lr, softmax_beta, wm_bias, wm_setsize_slope, wm_age_penalty, lapse]
        - lr: RL learning rate (0..1 after logistic).
        - softmax_beta: RL inverse temperature (scaled by *10 internally).
        - wm_bias: baseline WM tendency (logit) around set size 3.
        - wm_setsize_slope: how much WM weight decreases per +1 item beyond 3.
        - wm_age_penalty: additional WM decrease applied to older group (0 for young, >0 for old).
        - lapse: lapse probability blending uniform choice (0..1 after logistic).
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_bias, wm_setsize_slope, wm_age_penalty, lapse = model_parameters

    # Constrain parameters
    lr = 1 / (1 + np.exp(-lr))
    softmax_beta *= 10.0
    lapse = 1 / (1 + np.exp(-lapse))

    # Age group
    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM policy table
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform anchor

        # Compute WM mixture weight as a function of set size and age (logit link)
        # Larger set sizes and older age reduce WM weight.
        logit_wm = wm_bias - wm_setsize_slope * (nS - 3) - wm_age_penalty * age_group
        wm_weight_block = 1.0 / (1.0 + np.exp(-logit_wm))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture and lapse
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot store on reward; otherwise decay toward uniform
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                # Decay toward uniform to capture interference
                decay = 0.8  # fixed decay factor consistent across participants
                w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty bonus (count-based) and separate inverse temperatures.
    
    Idea:
    - RL system learns with a single learning rate but receives a novelty/uncertainty bonus
      for low-visit state-action pairs (count-based exploration).
    - WM stores rewarded actions one-shot; its policy uses its own inverse temperature.
    - WM mixture weight is fixed but the exploration bonus is modulated by set size
      and age (older and larger set sizes reduce the bonus).
    
    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [lr, beta_rl, beta_wm, wm_weight, bonus_scale, age_bonus_drop]
        - lr: RL learning rate (0..1 after logistic).
        - beta_rl: RL inverse temperature (scaled by *10 internally).
        - beta_wm: WM inverse temperature (scaled by *20 internally; WM is sharp but flexible).
        - wm_weight: fixed WM mixture weight (0..1 after logistic).
        - bonus_scale: base scale of count-based exploration bonus added to Q.
        - age_bonus_drop: multiplicative reduction of bonus for older group (0..1 after logistic).
          Effective bonus = bonus_scale * setsize_factor * (1 - age_factor), where age_factor in (0,1).
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, beta_rl, beta_wm, wm_weight, bonus_scale, age_bonus_drop = model_parameters

    # Constrain scalars
    lr = 1 / (1 + np.exp(-lr))
    beta_rl *= 10.0
    beta_wm *= 20.0
    wm_weight = 1 / (1 + np.exp(-wm_weight))
    age_bonus_drop = 1 / (1 + np.exp(-age_bonus_drop))  # in (0,1)

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Count table for exploration bonus
        counts = 1e-6 + np.zeros((nS, nA))

        # Set-size and age modulation of bonus
        # Larger set sizes -> smaller bonus (harder to explore); older -> additional drop
        setsize_factor = np.exp(-(nS - 3))  # 1 for 3, ~0.135 for 6
        age_factor = age_bonus_drop * age_group
        bonus_scale_eff = max(0.0, bonus_scale * setsize_factor * (1.0 - age_factor))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Count-based bonus: encourage less-visited actions
            bonus = bonus_scale_eff / np.sqrt(counts[s, :] + 1e-8)
            Q_eff = q[s, :] + bonus

            # RL policy with exploration bonus
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_eff - Q_eff[a])))

            # WM policy (its own inverse temperature)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update counts after observing the choice
            counts[s, a] += 1.0

            # WM update: one-shot on reward, decay otherwise
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                decay = 0.85
                w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + WSLS-like WM policy mixture.
    
    Idea:
    - RL has separate learning rates for positive and negative outcomes and includes
      value forgetting toward the uniform baseline (to capture interference).
    - WM component implements a WSLS-like heuristic:
        * If the last outcome in a state was reward=1, strongly repeat that action.
        * If the last outcome was reward=0, avoid repeating that action.
      This is encoded as a WM table that puts positive mass on the last rewarded action
      and negative mass on the last unrewarded action; a high WM inverse temperature
      makes it near-deterministic.
    - The WSLS mixture weight decreases with larger set size and with age.
    
    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [alpha_pos, alpha_neg, softmax_beta, forgetting, wsls_bias, age_wsls_drop]
        - alpha_pos: RL learning rate for rewards (0..1 after logistic).
        - alpha_neg: RL learning rate for no-reward (0..1 after logistic).
        - softmax_beta: RL inverse temperature (scaled by *10).
        - forgetting: RL forgetting rate toward uniform each trial within a state (0..1 after logistic).
        - wsls_bias: baseline WSLS mixture tendency at set size 3 (logit).
        - age_wsls_drop: additional WSLS drop for older group (logit scale; applied if old).
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos, alpha_neg, softmax_beta, forgetting, wsls_bias, age_wsls_drop = model_parameters

    # Constrain
    alpha_pos = 1 / (1 + np.exp(-alpha_pos))
    alpha_neg = 1 / (1 + np.exp(-alpha_neg))
    softmax_beta *= 10.0
    forgetting = 1 / (1 + np.exp(-forgetting))

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM/WSLS preference table
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform

        # Track last action and last outcome per state for WSLS heuristic
        last_action = -np.ones(nS, dtype=int)
        last_reward = -np.ones(nS, dtype=int)

        # WSLS mixture weight depends on set size and age (logit link)
        logit_wsls = wsls_bias - (nS - 3) - age_wsls_drop * age_group
        wsls_weight_block = 1.0 / (1.0 + np.exp(-logit_wsls))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with forgetting toward uniform baseline
            Q_s = q[s, :]
            # Apply forgetting on values in state s prior to update
            q[s, :] = (1.0 - forgetting) * q[s, :] + forgetting * w_0[s, :]
            Q_s = q[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WSLS-like WM policy: encode preference for last rewarded action or
            # avoidance of last unrewarded action.
            if last_action[s] >= 0 and last_reward[s] >= 0:
                pref = np.zeros(nA)
                if last_reward[s] == 1:
                    pref[last_action[s]] = 1.0   # favor repeating
                else:
                    pref[last_action[s]] = -1.0  # avoid repeating
                # Map to a softmax preference vector by adding to uniform baseline
                W_s = w_0[s, :] + pref
                # Normalize to a distribution-like vector for stable softmax behavior
                # (softmax uses relative differences; absolute scale adjusted by beta)
                # Save this preference in w for consistency
                w[s, :] = W_s
            else:
                W_s = w[s, :]

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wsls_weight_block * p_wm + (1.0 - wsls_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            if r == 1:
                delta = 1.0 - q[s, a]
                q[s, a] += alpha_pos * delta
            else:
                delta = 0.0 - q[s, a]
                q[s, a] += alpha_neg * delta

            # Update WSLS traces
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p

Model summary and how set size and age modulate parameters:
- cognitive_model1: WM mixture weight is reduced by larger set sizes (wm_setsize_slope) and by being older (wm_age_penalty). A lapse parameter captures random responding. RL uses a single learning rate and temperature; WM is near-deterministic with decay toward uniform when unrewarded.
- cognitive_model2: Adds a count-based exploration bonus to RL values. The bonus is reduced in larger set sizes and further reduced in the older group (age_bonus_drop). WM has its own temperature and a fixed mixture weight.
- cognitive_model3: Uses RL with forgetting toward uniform and a WSLS-like WM heuristic. The mixture weight for the WSLS policy decreases with set size and with age (age_wsls_drop). Separate RL learning rates for positive/negative outcomes capture asymmetric learning.