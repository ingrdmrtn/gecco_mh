Below are three standalone cognitive models for the RL-WM task. Each function:

- Follows the requested signature.
- Returns the negative log-likelihood of the observed actions.
- Implements an RL–WM mixture policy within blocks, with working memory policy and update rules filled in.
- Uses age group (0 = young, 1 = old) and set size (3 vs 6) in meaningful ways.
- Uses all parameters (no unused parameters).
- Uses at most 6 parameters.

Note: Assumes numpy as np is already imported by the surrounding environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture model.
    
    Idea:
    - Choices are a convex combination of an RL policy and a WM policy.
    - WM contribution is scaled by a capacity limit (wm_capacity) relative to set size,
      and reduced in older adults (age_group = 1) via age_wm_drop.
    - WM values are leaky (wm_decay) and updated toward the most recent outcome for the chosen action.
    - RL updates via a single learning rate (lr).
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base mixture weight for WM before capacity/age adjustments (0..1)
    - softmax_beta: Inverse temperature for RL softmax. Internally scaled by 10.
    - wm_decay: Leak/decay rate for WM values toward uniform baseline (0..1)
    - wm_capacity: WM capacity (in items). Effective WM weight is scaled by min(1, capacity/set_size).
    - age_wm_drop: Proportional reduction in WM reliance in older adults (0..1). For young (age<=45), no drop.
    
    Inputs:
    - states: array of state indices per trial (0..nS-1 per block)
    - actions: array of chosen action indices per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (3 or 6) – constant within a block
    - age: array with a single repeated value (participant age)
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_capacity, age_wm_drop = model_parameters
    # Scale beta to allow a wide range while keeping parameters bounded sensibly
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # Highly deterministic WM policy
    age_group = 0 if age[0] <= 45 else 1

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight for this block accounts for capacity and age
        capacity_factor = min(1.0, wm_capacity / float(nS))
        age_factor = 1.0 - age_wm_drop * age_group
        wm_weight_block = wm_weight_base * capacity_factor * age_factor
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            exp_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            exp_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: leak toward baseline then write current outcome to chosen action
            # Leak
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            # Outcome-based write: move chosen action toward r
            w[s, a] = (1.0 - wm_decay) * w[s, a] + wm_decay * r

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + reward-gated WM with confidence-based mixture.
    
    Idea:
    - RL uses separate learning rates for positive vs negative prediction errors (lr_pos, lr_neg),
      capturing potential age-related differences in learning asymmetry.
    - WM store is updated (strongly) only when reward is received (gating), then leaks (wm_decay) toward uniform.
    - Mixture weight is trial-by-trial: depends on WM confidence (max - mean) and a set-size penalty (3/nS),
      scaled by a base gate parameter (wm_gate). Older adults may rely less on precise softmax (beta_old).
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (rewarded trials)
    - lr_neg: RL learning rate for negative PE (unrewarded trials)
    - beta_young: RL softmax inverse temperature for young group (scaled by 10 internally)
    - beta_old: RL softmax inverse temperature for old group (scaled by 10 internally)
    - wm_gate: Base scaling for WM reliance (0..1) that multiplies confidence * set-size penalty
    - wm_decay: WM leak toward uniform baseline (0..1)
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_young, beta_old, wm_gate, wm_decay = model_parameters
    age_group = 0 if age[0] <= 45 else 1

    beta = beta_young if age_group == 0 else beta_old
    beta *= 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            exp_rl = np.exp(beta * (Q_s - np.max(Q_s)))
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy and confidence
            W_s = w[s, :]
            exp_wm = np.exp(beta_wm * (W_s - np.max(W_s)))
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # WM confidence (how peaked is W_s?) and set-size penalty
            conf = float(np.max(W_s) - np.mean(W_s))  # in [0,1]
            penalty = 3.0 / float(nS)                 # 1 for set size 3; 0.5 for set size 6
            wm_weight = np.clip(wm_gate * conf * penalty, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update with asymmetry
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: leak always, strong write only if rewarded (gated)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            if r > 0.5:
                # Strong one-hot write toward chosen action
                # First, push all down a bit (already leaked), then set chosen high
                w[s, a] = 1.0  # encourage a sharp WM trace after reward

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM mixture with uncertainty-weighted WM reliance and leaky WM integration.
    
    Idea:
    - Mixture weight increases when RL policy is uncertain (high entropy), but is penalized
      by larger set sizes and reduced in older adults.
    - RL uses a single learning rate; softmax beta is reduced for older adults via kappa_age.
    - WM values are leaky toward baseline with a time constant (tau_wm), and updated toward observed outcomes.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: Base inverse temperature for RL (scaled by 10 internally)
    - wm_beta: Inverse temperature for WM softmax (e.g., 5..50)
    - kappa_age: Proportional reduction of both beta and WM reliance in older adults (0..1)
    - kappa_set: Scaling of uncertainty-to-WM conversion; also interacts with set size penalty (0..1+)
    - tau_wm: WM time constant (>=1). Larger means slower decay. Internally used as decay d=exp(-1/tau_wm).
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_beta, kappa_age, kappa_set, tau_wm = model_parameters
    age_group = 0 if age[0] <= 45 else 1

    # Age-reduced beta
    beta = beta_base * (1.0 - kappa_age * age_group)
    beta = max(beta, 1e-6) * 10.0
    wm_beta = max(wm_beta, 1e-6)

    # WM decay factor from time constant
    tau_wm = max(tau_wm, 1.0)
    d = float(np.exp(-1.0 / tau_wm))  # decay multiplier per trial

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            exp_rl = np.exp(beta * (Q_s - np.max(Q_s)))
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            exp_wm = np.exp(wm_beta * (W_s - np.max(W_s)))
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # RL uncertainty via normalized entropy in [0,1]
            entropy = -np.sum(pi_rl * (np.log(pi_rl + 1e-12)))
            max_entropy = np.log(nA)
            H_norm = float(entropy / max_entropy)

            # Mixture weight: more WM when RL is uncertain, penalized by set size and age
            set_penalty = 3.0 / float(nS)  # 1 for 3, 0.5 for 6
            age_penalty = (1.0 - kappa_age * age_group)
            wm_weight = np.clip(kappa_set * H_norm * set_penalty * age_penalty, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: leaky integration toward baseline, then nudge chosen action toward r
            w[s, :] = d * w[s, :] + (1.0 - d) * w0[s, :]
            w[s, a] = d * w[s, a] + (1.0 - d) * r

    return nll

How set size and age are used:
- Model 1: WM mixture weight is reduced when set size exceeds WM capacity and further reduced in older adults via age_wm_drop.
- Model 2: WM mixture weight depends on trial-wise WM confidence times a set-size penalty (3/nS). RL softmax beta differs by age group.
- Model 3: Mixture weight grows with RL uncertainty (entropy) but is penalized by larger set sizes and reduced by age. RL beta is also reduced by age via kappa_age.

All three models implement an RL–WM mixture policy and return negative log-likelihood.