Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) to explain the participantâ€™s choices. Each function:

- Follows the provided template structure and fills in the missing WM policy and WM updates.
- Returns the negative log-likelihood of the observed choices.
- Uses up to 6 parameters and uses all of them meaningfully.
- Incorporates effects of set size (3 vs 6) and age group (0 = young, 1 = old).

Notes common to all models:
- RL policy uses a softmax with softmax_beta scaled by 10 as in the template.
- WM policy uses a high inverse temperature (softmax_beta_wm = 50).
- Probabilities are stabilized by adding a tiny epsilon when taking logs.
- Age group is derived from age[0] <= 45.

Model 1: RL + WM with capacity gating and WM decay
- Parameters: lr, wm_weight, softmax_beta, wm_capacity_k, wm_decay_phi, age_wm_penalty
- Idea: WM contribution is capacity-gated: effective WM weight scales as min(1, K/nS), where K is a capacity parameter reduced in older adults by age_wm_penalty. WM traces decay toward uniform at rate wm_decay_phi; rewarded trials overwrite WM for that state toward the chosen action.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity gating and WM decay.
    
    Parameters:
    - states: array-like, state index on each trial (0..nS-1 within a block)
    - actions: array-like, chosen action on each trial (0..2)
    - rewards: array-like, reward (0/1) on each trial
    - blocks: array-like, block index for each trial
    - set_sizes: array-like, set size on each trial (3 or 6, constant within block)
    - age: array-like, participant age (single value repeated)
    - model_parameters: list/tuple of 6 floats:
        lr: RL learning rate
        wm_weight: base weight of WM in policy mixing (0..1)
        softmax_beta: RL inverse temperature (will be multiplied by 10)
        wm_capacity_k: WM capacity parameter (in items)
        wm_decay_phi: WM decay to uniform (0..1 per trial)
        age_wm_penalty: fractional reduction in capacity for older group (0..1)
    Returns:
    - negative log-likelihood of observed actions
    """
    lr, wm_weight, softmax_beta, wm_capacity_k, wm_decay_phi, age_wm_penalty = model_parameters
    softmax_beta *= 10.0
    
    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1
    
    softmax_beta_wm = 50.0  # deterministic WM
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Age-dependent capacity
        effective_K = wm_capacity_k * (1.0 - age_wm_penalty * age_group)
        effective_K = max(0.0, effective_K)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            
            # WM policy: softmax on WM map
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            # Capacity-gated WM weight (also respects base wm_weight)
            cap_gate = 0.0 if nS <= 0 else min(1.0, effective_K / nS)
            wm_weight_eff = np.clip(wm_weight * cap_gate, 0.0, 1.0)
            
            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM decay toward uniform each trial
            w[s, :] = (1.0 - wm_decay_phi) * w[s, :] + wm_decay_phi * w_0[s, :]
            # If rewarded, overwrite toward one-hot chosen action with strength tied to capacity gate
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                overwrite = cap_gate  # stronger overwrite when within capacity
                w[s, :] = (1.0 - overwrite) * w[s, :] + overwrite * one_hot
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Model 2: RL with valence-specific learning rates, WM scaled by set size, plus lapse
- Parameters: alpha_plus, alpha_minus, wm_weight, softmax_beta, wm_decay_phi, lapse
- Idea: RL has separate learning rates for positive and negative outcomes. WM weight scales as 3/nS (less reliable under larger set size). Age modulates sensitivity to negative feedback by reducing alpha_minus in older adults. Lapse adds uniform choice noise.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-specific learning rates, WM scaled by set size, and lapse.
    
    Parameters:
    - states: array-like, state index per trial
    - actions: array-like, chosen action per trial
    - rewards: array-like, reward (0/1) per trial
    - blocks: array-like, block index per trial
    - set_sizes: array-like, set size per trial
    - age: array-like, participant age (single value repeated)
    - model_parameters: list/tuple of 6 floats:
        alpha_plus: RL learning rate for rewards
        alpha_minus: RL learning rate for non-rewards
        wm_weight: base WM weight in mixture
        softmax_beta: RL inverse temperature (multiplied by 10)
        wm_decay_phi: WM decay toward uniform (0..1)
        lapse: lapse probability (0..1), mixed with uniform policy
    Returns:
    - negative log-likelihood
    """
    alpha_plus, alpha_minus, wm_weight, softmax_beta, wm_decay_phi, lapse = model_parameters
    softmax_beta *= 10.0
    
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1
    
    # Age effect: older group less sensitive to negative outcomes
    alpha_minus_eff = alpha_minus * (1.0 - 0.5 * age_group)
    alpha_minus_eff = max(0.0, alpha_minus_eff)
    
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Set-size scaling of WM weight: reliable for 3, weaker for 6
        size_gate = min(1.0, 3.0 / max(1.0, nS))
        wm_weight_base = np.clip(wm_weight * size_gate, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            mix_p = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl
            # Lapse to uniform
            p_total = (1.0 - lapse) * mix_p + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))
            
            # RL update with valence-specific learning rates
            if r > 0.5:
                delta = r - Q_s[a]
                q[s, a] += alpha_plus * delta
            else:
                delta = r - Q_s[a]
                q[s, a] += alpha_minus_eff * delta
            
            # WM update: decay toward uniform, reward-locked overwriting
            w[s, :] = (1.0 - wm_decay_phi) * w[s, :] + wm_decay_phi * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite strength follows size_gate (weaker when set size is larger)
                overwrite = size_gate
                w[s, :] = (1.0 - overwrite) * w[s, :] + overwrite * one_hot
        
        blocks_log_p += log_p
    
    return -blocks_log_p

Model 3: RL with forgetting, WM capacity K with age-sensitive temperature
- Parameters: lr, wm_weight, softmax_beta, q_decay, wm_capacity_k, age_temp_shift
- Idea: RL values decay toward uniform (q_decay). WM has capacity K: WM weight is scaled by min(1, K/nS). Age increases stochasticity by reducing softmax_beta via age_temp_shift.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value forgetting, WM capacity gating, and age-sensitive temperature.
    
    Parameters:
    - states: array-like, state index per trial
    - actions: array-like, chosen action per trial
    - rewards: array-like, reward (0/1) per trial
    - blocks: array-like, block index per trial
    - set_sizes: array-like, set size per trial
    - age: array-like, participant age (single value repeated)
    - model_parameters: list/tuple of 6 floats:
        lr: RL learning rate
        wm_weight: base WM weight in mixture
        softmax_beta: base RL inverse temperature (multiplied by 10)
        q_decay: RL forgetting toward uniform (0..1 per update)
        wm_capacity_k: WM capacity (in items)
        age_temp_shift: reduces RL temperature in older group via exp(-age_temp_shift)
    Returns:
    - negative log-likelihood
    """
    lr, wm_weight, softmax_beta, q_decay, wm_capacity_k, age_temp_shift = model_parameters
    # Age-dependent temperature
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1
    
    temp_factor = np.exp(-age_temp_shift * age_group)
    softmax_beta = softmax_beta * 10.0 * temp_factor
    
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Capacity-gated WM weight
        cap_gate = min(1.0, wm_capacity_k / max(1.0, nS))
        wm_weight_eff = np.clip(wm_weight * cap_gate, 0.0, 1.0)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))
            
            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Forgetting toward uniform for current state
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * (1.0 / nA)
            
            # WM update: simple decay and reward-based overwrite
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # mild default decay
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - cap_gate) * w[s, :] + cap_gate * one_hot
        
        blocks_log_p += log_p
    
    return -blocks_log_p

How set size and age group affect parameters in these models:
- Set size:
  - Model 1 and 3: Capacity-gated WM weight scales as min(1, K/nS), so WM dominates when set size is small (3) and weakens when set size is large (6).
  - Model 2: WM weight scales approximately as 3/nS, making WM much less reliable in the 6-item condition.
- Age group:
  - Model 1: Effective WM capacity is reduced in older adults by age_wm_penalty.
  - Model 2: Negative learning rate is reduced in older adults, modeling reduced sensitivity to negative feedback.
  - Model 3: RL temperature is reduced in older adults via age_temp_shift, making choices noisier (lower softmax_beta).