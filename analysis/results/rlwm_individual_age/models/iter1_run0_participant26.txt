def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and capacity-limited WM mixing by set size and age.

    The model combines:
    - Model-free RL with separate positive/negative learning rates.
    - A one-shot, capacity-limited WM module that stores rewarded S-A mappings.
    - Arbitration weight scales with a baseline wm_weight_base times a capacity term K_age/nS.
      Younger and older groups have different WM capacities (K_young vs K_old).

    Parameters
    - model_parameters: [lr_pos, lr_neg, wm_weight_base, softmax_beta, K_young, K_old]
        - lr_pos: RL learning rate for positive PE (0..1)
        - lr_neg: RL learning rate for negative PE (0..1)
        - wm_weight_base: baseline mixture weight of WM (0..1)
        - softmax_beta: RL inverse temperature; internally scaled up
        - K_young: effective WM capacity for younger group (>=1)
        - K_old: effective WM capacity for older group (>=1)

    Returns
    - Negative log-likelihood of observed choices
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, K_young, K_old = model_parameters
    softmax_beta *= 10.0  # higher upper bound for RL inverse temperature

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # near-deterministic for WM
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-dependent WM capacity
        K_age = K_old if age_group == 1 else K_young
        cap_scale = min(1.0, max(0.0, K_age / float(nS)))  # capacity reduces WM influence when set size is large

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy (softmax with high beta; equivalent to argmax with lapses)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration: capacity-limited WM weight
            wm_weight = wm_weight_base * cap_scale
            wm_weight = min(1.0, max(0.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM update:
            # - If rewarded, store a as correct (one-shot, normalized)
            # - If not rewarded, gently revert towards uniform (no extra parameter; keeps params <=6)
            if r > 0.5:
                sharpen = 0.9
                w[s, :] = (1.0 - sharpen) * w[s, :]
                w[s, a] += sharpen
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                # small passive diffusion toward uniform scaled by set size (more diffusion when larger nS)
                lam = 0.1 * (nS / 6.0)  # deterministic scaling without new parameter
                w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with size- and age-dependent choice noise (epsilon-greedy) and WM baseline weighting.

    The model combines:
    - RL softmax policy plus an epsilon-greedy lapse process whose rate increases with set size and age.
    - A WM module that stores rewarded mappings one-shot and drives a near-deterministic policy.
    - Arbitration with baseline wm_weight_base scaled by set size (smaller sets => stronger WM).

    Parameters
    - model_parameters: [lr, wm_weight_base, softmax_beta, epsilon_base, epsilon_size, epsilon_age]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM weight (0..1)
        - softmax_beta: RL inverse temperature; internally scaled
        - epsilon_base: base lapse probability (0..1)
        - epsilon_size: additional lapse per (nS-3)/3 (>=0)
        - epsilon_age: additional lapse for older group (>=0), 0 for younger if age_group=0

    Returns
    - Negative log-likelihood of observed choices
    """
    lr, wm_weight_base, softmax_beta, epsilon_base, epsilon_size, epsilon_age = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # compute lapse epsilon for this block
        size_term = max(0.0, (nS - 3) / 3.0)  # 0 for nS=3, 1 for nS=6
        epsilon = epsilon_base + epsilon_size * size_term + epsilon_age * age_group
        epsilon = min(0.5, max(0.0, epsilon))  # keep in reasonable range

        # WM scaling with set size (smaller sets -> stronger WM)
        size_scale = 3.0 / float(nS)
        wm_weight_block = wm_weight_base * size_scale
        wm_weight_block = min(1.0, max(0.0, wm_weight_block))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl_soft = 1.0 / max(eps, denom_rl)

            # Epsilon-greedy mixture for RL: with epsilon choose uniformly at random
            p_rl = (1.0 - epsilon) * p_rl_soft + epsilon * (1.0 / nA)

            # WM near-deterministic softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: rewarded -> one-shot storage; else slight diffusion
            if r > 0.5:
                sharpen = 0.9
                w[s, :] = (1.0 - sharpen) * w[s, :]
                w[s, a] += sharpen
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                lam = 0.1 * size_term  # more diffusion when set size is larger
                w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM arbitration driven by recent RL prediction errors and choice stickiness.

    Mechanisms:
    - RL with single learning rate and softmax, plus a stickiness bias kappa toward repeating
      the last action taken in the same state.
    - WM module updated one-shot by rewards, near-deterministic softmax.
    - Arbitration weight = wm_weight_base * f(PE), where f decreases with large recent PE magnitude
      (i.e., rely less on RL when RL is volatile/incorrect, hence more on WM), and scaled by set size.
      Age shifts the PE sensitivity (age_shift increases effective PE for older adults).

    Parameters
    - model_parameters: [lr, wm_weight_base, softmax_beta, kappa, pe_sensitivity, age_shift]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature; internally scaled
        - kappa: stickiness bonus added to Q-value of the last action in a state (can be +/-)
        - pe_sensitivity: scales the impact of running PE magnitude on arbitration (>=0)
        - age_shift: additive increase in effective PE magnitude for older adults (>=0)

    Returns
    - Negative log-likelihood of observed choices
    """
    lr, wm_weight_base, softmax_beta, kappa, pe_sensitivity, age_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness and rolling PE magnitude for arbitration
        last_action = -1 * np.ones(nS, dtype=int)
        pe_abs_running = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add stickiness bias to the last chosen action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += kappa

            # RL softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM near-deterministic softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration weight constructed from:
            # - baseline wm_weight_base
            # - set size scaling (smaller sets -> larger WM)
            # - dynamic factor from running absolute PE magnitude (higher PE -> increase WM reliance)
            size_scale = 3.0 / float(nS)
            effective_pe = pe_abs_running[s] + age_shift * age_group
            dyn = 1.0 - np.exp(-pe_sensitivity * effective_pe)  # in [0,1], increases with PE
            wm_weight = wm_weight_base * size_scale * dyn
            wm_weight = min(1.0, max(0.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # Compute PE relative to unbiased (without stickiness) Q for learning
            Q_base = q[s, :]
            pe = r - Q_base[a]
            q[s, a] += lr * pe

            # Update running absolute PE (EMA without extra parameter; mild smoothing)
            pe_abs_running[s] = 0.7 * pe_abs_running[s] + 0.3 * abs(pe)

            # WM update
            if r > 0.5:
                sharpen = 0.9
                w[s, :] = (1.0 - sharpen) * w[s, :]
                w[s, a] += sharpen
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                # gentle diffusion to uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p