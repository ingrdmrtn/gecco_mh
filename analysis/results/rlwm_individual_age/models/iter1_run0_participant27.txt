Below are three distinct cognitive models tailored for the RL-WM task. Each function:

- Implements an RL–WM arbitration with set-size dependence.
- Uses age group (0=young, 1=old) in a meaningful way.
- Returns the negative log-likelihood of the observed choices.
- Uses all parameters (≤ 6) and does not import packages.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with global forgetting + WM with load-dependent decay; age reduces WM reliance.
    Policy is a WM–RL mixture; WM is near-deterministic, RL uses softmax.

    Mechanism:
    - RL: Q-learning with softmax; additionally decays toward uniform each trial (global forgetting),
      scaled up under higher set size (more load -> more forgetting).
    - WM: table of action probabilities that decay toward uniform; upon reward, WM overwrites the
      state’s row to a near-delta on the chosen action. WM mixture weight shrinks with load (3/nS).
    - Age: older group has reduced WM mixture weight (multiplicative factor age_wm_factor < 1),
      young group unaffected.

    Parameters (6):
    - model_parameters:
        0) lr                : RL learning rate (0..1)
        1) beta              : RL inverse temperature (>0); internally scaled by 10
        2) wm_weight_base    : Base mixture weight for WM (0..1)
        3) wm_decay_base     : Baseline WM decay toward uniform per trial (0..1)
        4) rl_forgetting     : RL global forgetting strength toward uniform per trial (0..1)
        5) age_wm_factor     : Factor applied to WM weight if old (0..1). If young, factor=1.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length
    - age: array with single value repeated; <=45 => age_group=0, >45 => age_group=1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_weight_base, wm_decay_base, rl_forgetting, age_wm_factor = model_parameters
    beta = beta * 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_uniform = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent settings
        wm_w_block = wm_weight_base * (3.0 / max(1.0, float(nS)))
        wm_w_block = np.clip(wm_w_block, 0.0, 1.0)
        if age_group == 1:
            wm_w_block *= age_wm_factor
        wm_w_block = np.clip(wm_w_block, 0.0, 1.0)

        wm_decay_block = wm_decay_base * (float(nS) / 3.0)
        wm_decay_block = np.clip(wm_decay_block, 0.0, 1.0)

        rl_forgetting_block = rl_forgetting * (float(nS) / 3.0)
        rl_forgetting_block = np.clip(rl_forgetting_block, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (near-deterministic)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            q = (1.0 - rl_forgetting_block) * q + rl_forgetting_block * (1.0 / nA)

            # WM decay and overwrite on reward
            w = (1.0 - wm_decay_block) * w + wm_decay_block * w_uniform
            if r > 0.5:
                w[s, :] = 1e-6
                w[s, a] = 1.0 - (nA - 1) * 1e-6

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL softmax with epsilon-greedy exploration + capacity-gated WM.
    Age increases undirected exploration (epsilon), especially under load.

    Mechanism:
    - RL: Q-learning with softmax; policy blended with epsilon-greedy noise.
    - WM: updated to near-delta on rewarded trials; effective weight scales with capacity K/nS.
    - Epsilon: base epsilon increases with set size (load) and receives an additive age bonus in older adults.
    - Mixture: WM vs RL is mixed via weight scaled by K/nS; action probability = WM_mix*WM + (1 - WM_mix)*RL_eps.

    Parameters (6):
    - model_parameters:
        0) lr             : RL learning rate (0..1)
        1) beta           : RL inverse temperature (>0); internally scaled by 10
        2) wm_weight_base : Base WM mixture weight (0..1)
        3) K              : WM capacity parameter (>=0), scales weight as min(1, K/nS)
        4) epsilon_base   : Base undirected exploration (0..1)
        5) age_eps_bonus  : Additive epsilon for older adults (>=0); 0 for young

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length
    - age: array with single value repeated; <=45 => age_group=0, >45 => age_group=1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_weight_base, K, epsilon_base, age_eps_bonus = model_parameters
    beta = beta * 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    eps_small = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_uniform = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture weight scales with capacity
        wm_w_block = wm_weight_base * min(1.0, float(K) / max(1.0, float(nS)))
        wm_w_block = np.clip(wm_w_block, 0.0, 1.0)

        # Epsilon increases with load; age adds exploration bonus if old
        eps_block = epsilon_base * np.sqrt(max(1.0, float(nS) / 3.0))
        if age_group == 1:
            eps_block += age_eps_bonus
        eps_block = np.clip(eps_block, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax
            Q_s = q[s, :]
            denom_soft = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_soft = 1.0 / max(denom_soft, eps_small)

            # Epsilon-greedy mixture with softmax
            p_rl = (1.0 - eps_block) * p_soft + eps_block * (1.0 / nA)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps_small)

            # Final mixture
            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            p_total = max(p_total, eps_small)
            total_log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: small decay to uniform + overwrite on reward
            w = 0.98 * w + 0.02 * w_uniform  # light leak not parameterized (kept fixed to avoid >6 params)
            if r > 0.5:
                w[s, :] = 1e-6
                w[s, a] = 1.0 - (nA - 1) * 1e-6

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with load-sensitive WM precision.
    Age biases arbitration away from WM (older rely more on RL).

    Mechanism:
    - RL: Q-learning with softmax.
    - WM: near-deterministic map that decays toward uniform; precision (beta_wm_eff) decreases with set size.
    - Arbitration: compute confidence for each system and apply a logistic gate:
        wm_weight_dyn = sigmoid(arb_slope * (conf_wm - conf_rl) + age_bias + load_bias)
      where:
        conf_wm = max(W_s) - second_max(W_s) (0..1)
        conf_rl = softmax gap of Q: max(p_rl_full) - second_max(p_rl_full) (0..1)
      Age bias shifts weight toward RL in older adults (negative bias to WM).
      Load bias shifts arbitration away from WM as set size increases.
    - Final policy: mixture of WM and RL using wm_weight_dyn.

    Parameters (6):
    - model_parameters:
        0) lr            : RL learning rate (0..1)
        1) beta          : RL inverse temperature (>0); internally scaled by 10
        2) wm_beta_base  : Baseline WM inverse temperature scaling (>0); WM beta_eff = wm_beta_base * 50 / sqrt(nS)
        3) wm_decay_base : WM decay to uniform per trial (0..1); scaled by nS/3
        4) arb_slope     : Slope of logistic arbitration (>0)
        5) age_arb_bias  : Additive bias applied if old (can be negative to prefer RL)

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length
    - age: array with single value repeated; <=45 => age_group=0, >45 => age_group=1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_beta_base, wm_decay_base, arb_slope, age_arb_bias = model_parameters
    beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_uniform = (1.0 / nA) * np.ones((nS, nA))

        # Load-sensitive WM parameters
        beta_wm_eff = (wm_beta_base * 50.0) / np.sqrt(max(1.0, float(nS)))
        wm_decay_block = wm_decay_base * (float(nS) / 3.0)
        wm_decay_block = np.clip(wm_decay_block, 0.0, 1.0)

        # Load bias in arbitration: penalize WM as set size increases
        load_bias = -0.5 * np.log(max(1.0, float(nS) / 3.0))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax distribution over all actions
            Q_s = q[s, :]
            prefs_rl = beta * Q_s
            # compute chosen-action prob efficiently
            denom_rl = np.sum(np.exp(prefs_rl - prefs_rl[a]))
            p_rl_chosen = 1.0 / max(denom_rl, eps)
            # full RL probabilities for confidence computation
            rl_probs = np.exp(prefs_rl - np.max(prefs_rl))
            rl_probs = rl_probs / max(np.sum(rl_probs), eps)

            # WM softmax distribution over all actions
            W_s = w[s, :]
            prefs_wm = beta_wm_eff * W_s
            denom_wm = np.sum(np.exp(prefs_wm - prefs_wm[a]))
            p_wm_chosen = 1.0 / max(denom_wm, eps)
            wm_probs = np.exp(prefs_wm - np.max(prefs_wm))
            wm_probs = wm_probs / max(np.sum(wm_probs), eps)

            # Confidence measures: gap between top-1 and top-2 probabilities
            rl_sorted = np.sort(rl_probs)[::-1]
            wm_sorted = np.sort(wm_probs)[::-1]
            conf_rl = rl_sorted[0] - rl_sorted[1] if nA >= 2 else rl_sorted[0]
            conf_wm = wm_sorted[0] - wm_sorted[1] if nA >= 2 else wm_sorted[0]

            # Age bias: older -> shift away from WM (e.g., negative value)
            age_bias = age_arb_bias if age_group == 1 else 0.0

            # Dynamic WM weight via logistic arbitration
            wm_weight_dyn = sigmoid(arb_slope * (conf_wm - conf_rl) + age_bias + load_bias)
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            # Final choice probability
            p_total = wm_weight_dyn * p_wm_chosen + (1.0 - wm_weight_dyn) * p_rl_chosen
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay + overwrite on reward
            w = (1.0 - wm_decay_block) * w + wm_decay_block * w_uniform
            if r > 0.5:
                w[s, :] = 1e-6
                w[s, a] = 1.0 - (nA - 1) * 1e-6

    return -float(total_log_p)