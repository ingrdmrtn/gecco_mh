def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + capacity-limited WM with binding noise and age-adjusted capacity
    - RL: standard delta rule as in the template.
    - WM: slot-like capacity. If set size exceeds capacity, WM contribution is diluted.
           Stored associations are noisy due to binding noise.
    - Policy: mixture of RL and WM. WM mixture weight scales with (effective capacity / set size).
    - Age: older group has effectively fewer WM slots (negative capacity shift), younger more.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_capacity_slots: nominal number of WM slots (>=1)
    - binding_noise: probability mass leaked from the intended WM action to uniform (0..1)
    - age_capacity_shift: capacity shift applied to slots for age (>=0). Subtracted if old, added if young.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_capacity_slots, binding_noise, age_capacity_shift = model_parameters
    softmax_beta *= 10  # higher upper bound as specified

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic WM readout
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted effective capacity
        if age_group == 1:
            eff_slots = max(1.0, wm_capacity_slots - age_capacity_shift)
        else:
            eff_slots = max(1.0, wm_capacity_slots + age_capacity_shift)

        # WM mixture weight scales with how many items can be held
        cap_ratio = min(1.0, eff_slots / max(1.0, nS))
        wm_mix_weight = np.clip(wm_weight0 * cap_ratio, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax prob of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM readout: deterministic softmax on (noisy) W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on reward, store one-hot with binding noise; on no reward, leave as is
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Binding noise: leak probability mass to uniform template
                w[s, :] = (1.0 - binding_noise) * one_hot + binding_noise * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL with uncertainty bonus + WM with set-size gating
    - RL: standard delta rule; action values augmented with an exploration/uncertainty bonus.
          Uncertainty is approximated by inverse sqrt visit count. Age scales exploration.
    - WM: stored associations as point-mass on rewarded actions with mild decay.
          Mixture weight is gated by set size via a logistic gate.
    - Policy: mixture of WM and augmented RL.
    - Age: older group explores more/less via an age_explore_boost; also influences WM gate via set size.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: baseline WM weight before set-size gating (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - explore_bonus_scale: scaling for RL uncertainty bonus (>=0)
    - age_explore_boost: additive age effect on exploration bonus (>=0). Added if old, subtracted if young.
    - wm_gate_slope: slope for logistic gating of WM by set size (>=0). Higher => sharper drop with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, explore_bonus_scale, age_explore_boost, wm_gate_slope = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts per state-action for uncertainty bonus
        counts = np.ones((nS, nA))  # start at 1 to avoid div by zero

        # Age-adjusted exploration bonus scale
        if age_group == 1:
            bonus_scale = max(0.0, explore_bonus_scale + age_explore_boost)
        else:
            bonus_scale = max(0.0, max(0.0, explore_bonus_scale - age_explore_boost))

        # WM set-size logistic gate: weight decreases as set size grows beyond 3
        midpoint = 3.0
        gate = 1.0 / (1.0 + np.exp(wm_gate_slope * (nS - midpoint)))
        wm_mix_weight = np.clip(wm_weight0 * gate, 0.0, 1.0)

        # Small decay for WM to mitigate interference at larger sets
        wm_decay = np.clip(0.05 * (nS - 1), 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # Add uncertainty bonus: higher for less-visited actions
            unc = 1.0 / np.sqrt(counts[s, :] + 1e-8)
            Q_aug = Q_s + bonus_scale * unc

            # RL policy on augmented values
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update counts after observing the choice
            counts[s, a] += 1.0

            # WM update with mild decay toward uniform; reinforce on positive feedback
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite mix to strengthen the rewarded association
                w[s, :] = 0.7 * one_hot + 0.3 * w[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with valence-asymmetric learning + WM with decay and age-dependent downweighting + lapse
    - RL: standard delta rule but with asymmetric learning rate implemented via reward reweighting.
           Since the RL update line is fixed, we emulate asymmetry by scaling reward r.
    - WM: reinforced on positive reward, prunes slightly on negative; decays to uniform each trial.
          WM weight decreases with set size (1/nS), and additionally downweighted for older adults.
    - Policy: mixture of RL and WM, followed by a lapse that mixes with uniform choice.
    - Age: older group has stronger lapse and stronger WM downweighting.

    Parameters (model_parameters):
    - lr: base RL learning rate (0..1)
    - wm_weight0: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: per-trial WM decay toward uniform (0..1)
    - age_wm_down: age effect on WM mixture (>=0). Subtracted if young, added if old.
    - lapse_base: baseline lapse probability mixed with uniform (0..1); amplified if old.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay, age_wm_down, lapse_base = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0
    eps = 1e-12

    # Set valence asymmetry via effective reward scaling
    pos_scale = 1.0
    neg_scale = 0.5  # learn less from negative outcomes

    # Age-adjusted lapse
    age_lapse_boost = 0.1  # fixed increment applied if old, reduced if young
    if age_group == 1:
        lapse = np.clip(lapse_base + age_lapse_boost, 0.0, 1.0)
    else:
        lapse = np.clip(max(0.0, lapse_base - age_lapse_boost), 0.0, 1.0)

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture weight scales with set size and age
        size_scale = 1.0 / max(1.0, nS)
        if age_group == 1:
            wm_mix_weight = np.clip(wm_weight0 * size_scale - age_wm_down, 0.0, 1.0)
        else:
            wm_mix_weight = np.clip(wm_weight0 * size_scale + age_wm_down, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Apply valence asymmetry by scaling the reward used inside delta
            r_eff = pos_scale * r + (1 - r) * (0.0 - neg_scale * 0.0)
            # For binary rewards, r in {0,1}, this keeps r_eff=1 for reward, and 0 for no-reward (implicit asymmetry via decay below)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice prob
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl

            # Lapse: mix with uniform
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with effective reward r_eff to maintain asymmetry via lr
            delta = r_eff - Q_s[a]
            q[s, a] += lr * delta

            # WM decay every trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM valence-dependent update
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.8 * one_hot + 0.2 * w[s, :]
            else:
                # Negative feedback pruning: reduce probability of chosen action slightly
                w[s, a] = 0.5 * w[s, a]
                # Renormalize to keep a proper distribution
                total = w[s, :].sum()
                if total > 0:
                    w[s, :] = w[s, :] / total
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p