Below are three candidate cognitive models for the RL+WM task. Each function:

- Follows the requested signature and returns the negative log-likelihood of the participant’s choices.
- Uses an RL system mixed with a fast Working Memory (WM) system.
- Incorporates age group (young vs. old) and set size (3 vs. 6) in a meaningful way.
- Uses at most 6 parameters.
- Fills in the WM policy and WM update as per the template spirit, while keeping the rest intact.

Notes on shared implementation details:
- The RL policy p_rl computes the softmax probability of the observed action a in state s, using the provided trick p(a|s) = 1 / sum(exp(beta*(Q_s - Q_s[a]))).
- The WM policy p_wm does the same but on WM weights W_s with a large inverse temperature (softmax_beta_wm=50), making WM near-deterministic.
- WM update: a convex combination of the current WM weights and a “memory imprint” for the current (state, action, reward) with a decay/learning step. The imprint pulls WM towards a one-hot for the chosen action when rewarded and towards the baseline w_0 otherwise:
  m = w_0[s,:] + r * (one_hot(a) - w_0[s,:])
  w[s,:] = (1 - wm_decay)*w[s,:] + wm_decay*m
- To avoid log(0), a small constant (1e-12) is added before taking logs.

Model 1: RL + WM with age-dependent WM capacity and set-size scaling
- Intuition: WM contribution is limited by an effective capacity K that differs by age group. WM contribution scales as min(1, K / set_size). Larger set size reduces the WM mixture weight more strongly, with potentially smaller capacity for older adults.
- Parameters (6): lr, wm_weight_base, softmax_beta, k_y, k_o, wm_decay

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age-dependent WM capacity and set-size scaling.

    Parameters
    ----------
    states : array-like
        State index per trial (0..nS-1 for that block).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Binary reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block of each trial.
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, k_y, k_o, wm_decay]
        - lr: RL learning rate (0..1)
        - wm_weight_base: base WM weight before scaling (0..1)
        - softmax_beta: inverse temperature for RL (scaled by *10 internally)
        - k_y: effective WM capacity for young group
        - k_o: effective WM capacity for old group
        - wm_decay: WM update step/decay (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, k_y, k_o, wm_decay = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
        K = max(1e-6, k_y)
    else:
        age_group = 1
        K = max(1e-6, k_o)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: p(a|s) under softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: p(a|s) under near-deterministic softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Set-size and age-dependent WM weight via capacity scaling
            cap_scale = min(1.0, K / float(nS))
            wm_weight_eff = np.clip(wm_weight_base * cap_scale, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with decay towards imprint
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            imprint = w_0[s, :] + r * (one_hot - w_0[s, :])
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * imprint

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: RL+WM with age-dependent learning rate and set-size-dependent WM decay
- Intuition: Older adults may learn more slowly (different RL learning rates), and WM may decay faster under higher set size. WM mixture weight decreases with larger set sizes.
- Parameters (6): lr_y, lr_o, wm_weight_base, softmax_beta, wm_decay_small, wm_decay_large

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age-dependent RL learning rate and set-size-dependent WM decay.

    Parameters
    ----------
    states : array-like
        State index per trial (0..nS-1 for that block).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Binary reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block of each trial.
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr_y, lr_o, wm_weight_base, softmax_beta, wm_decay_small, wm_decay_large]
        - lr_y: RL learning rate for young group
        - lr_o: RL learning rate for old group
        - wm_weight_base: base WM weight (0..1)
        - softmax_beta: inverse temperature for RL (scaled by *10)
        - wm_decay_small: WM decay/update step for set size 3
        - wm_decay_large: WM decay/update step for set size 6

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_y, lr_o, wm_weight_base, softmax_beta, wm_decay_small, wm_decay_large = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
        lr = lr_y
    else:
        age_group = 1
        lr = lr_o

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size dependent WM decay
        wm_decay = wm_decay_small if nS == 3 else wm_decay_large

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM mixture weight: stronger at 3 than at 6
        size_scale = 3.0 / float(nS)  # 1.0 for set size 3, 0.5 for set size 6
        wm_weight_eff_block = np.clip(wm_weight_base * size_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            imprint = w_0[s, :] + r * (one_hot - w_0[s, :])
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * imprint

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: RL with asymmetric learning rates + WM gating with age and set-size scaling + lapse
- Intuition: RL learning can be asymmetric for positive vs. negative feedback. WM access is “gated” more successfully in small set sizes and for younger adults. A small lapse probability accounts for random responding.
- Parameters (6): alpha_pos, alpha_neg, wm_weight_base, softmax_beta, gate_age, lapse

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM gating scaled by set size and age + lapse.

    Parameters
    ----------
    states : array-like
        State index per trial (0..nS-1 for that block).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Binary reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block of each trial.
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [alpha_pos, alpha_neg, wm_weight_base, softmax_beta, gate_age, lapse]
        - alpha_pos: RL learning rate for rewarded trials
        - alpha_neg: RL learning rate for unrewarded trials
        - wm_weight_base: base WM weight (0..1)
        - softmax_beta: inverse temperature for RL (scaled by *10)
        - gate_age: WM gating reduction factor for old group (0..1); effective WM weight multiplied by (1 - gate_age*age_group)
        - lapse: lapse probability mixed with uniform (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, gate_age, lapse = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM gating weight scales down with set size and for older adults
        size_scale = 3.0 / float(nS)  # 1 at 3, 0.5 at 6
        age_scale = (1.0 - np.clip(gate_age, 0.0, 1.0) * age_group)
        wm_weight_eff_block = np.clip(wm_weight_base * size_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture with lapse
            p_mix = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            alpha = alpha_pos if r > 0.5 else alpha_neg
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update (simple imprint with moderate decay tied to reward/no-reward)
            # Use a reward-modulated decay: larger step when rewarded
            wm_decay = 0.5 * (1.0 + r)  # 0.5 when r=0, 1.0 when r=1
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            imprint = w_0[s, :] + r * (one_hot - w_0[s, :])
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * imprint

        blocks_log_p += log_p

    return -blocks_log_p

How age and set size influence parameters across models:
- Model 1: WM contribution is capacity-limited with K differing by age group (k_y vs k_o). Larger set sizes dilute WM via min(1, K/nS).
- Model 2: RL learning rate differs by age group (lr_y vs lr_o). WM decays faster for larger set sizes (wm_decay_large vs wm_decay_small), and WM weight is downscaled at set size 6.
- Model 3: WM gating is downweighted for older adults via gate_age and for larger sets via size_scale. RL uses asymmetric learning rates for positive vs. negative feedback, and a lapse term captures random responding.