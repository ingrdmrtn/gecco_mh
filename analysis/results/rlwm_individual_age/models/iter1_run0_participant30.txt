Below are three distinct cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways. Each function:

- Is standalone and returns the negative log-likelihood of the observed choices.
- Uses all parameters meaningfully (â‰¤ 6 parameters).
- Incorporates set size (3 vs. 6) and age group (0=young, 1=old) effects.
- Fills in WM policy and WM updating where required.

Note: Assume numpy (as np) and other standard packages are already imported in the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM gating + decay + perseveration.

    Idea
    ----
    Choices are a mixture of:
      - RL policy with asymmetric learning rates (positive vs negative prediction errors).
      - WM policy that stores rewarded mappings with decay toward uniform.
    The WM contribution is gated by set size and age: larger set size and older age reduce WM weight.
    Perseveration adds a bias toward repeating the last action in a state.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, wm_gate, wm_decay, perseveration]
        - alpha_pos: RL learning rate for positive PE (0..1)
        - alpha_neg: RL learning rate for negative PE (0..1)
        - beta_base: RL inverse temperature base (>0), scaled by 10 internally
        - wm_gate: baseline WM gate (0..1), down-weighted by set size and age
        - wm_decay: decay of WM toward uniform each trial (0..1)
        - perseveration: choice stickiness toward previous action in same state (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_base, wm_gate, wm_decay, perseveration = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # high precision WM

    negloglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size and age-dependent WM weight via gating
        # size_factor: 1 for 3, 0.5 for 6
        size_factor = 3.0 / float(nS)
        # Older age reduces WM gate; larger set size reduces WM gate
        wm_weight_eff = wm_gate * size_factor * (1.0 - 0.5 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias (additive to Q-values)
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            # RL choice probability of observed action (softmax trick given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over w row
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            negloglik -= np.log(max(p_total, 1e-12))

            # RL update (asymmetric learning)
            pe = r - q[s, a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += lr * pe

            # WM decay toward uniform each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # If rewarded, store a sharp memory for this state-action
            if r > 0.5:
                # Move w[s] toward a one-hot for action a
                target = np.zeros(nA)
                target[a] = 1.0
                # Fast overwrite-like update
                w[s, :] = 0.5 * w[s, :] + 0.5 * target

            # Update last action for perseveration
            last_action[s] = a

    return negloglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with WM noise modulated by set size and age.

    Idea
    ----
    Arbitration weight is driven by relative uncertainty (entropy) of policies:
      - RL uses a standard softmax; uncertainty approximated via entropy of its action distribution.
      - WM is learned via supervised updates on rewarded trials; its noise increases with set size and age.
    WM precision (beta_wm) is inversely related to WM noise, which increases for larger set size and older age.
    Final policy is a mixture weighted by RL vs WM certainty. Includes a small lapse.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, beta_base, wm_lr, wm_noise_base, size_sensitivity, age_bias]
        - lr: RL learning rate (0..1)
        - beta_base: RL inverse temperature base (>0), scaled by 10 internally
        - wm_lr: WM learning rate toward one-hot on rewarded trials (0..1)
        - wm_noise_base: baseline WM noise (>0)
        - size_sensitivity: scales increase of WM noise with set size (>0)
        - age_bias: additive WM noise increase for older age group (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_lr, wm_noise_base, size_sensitivity, age_bias = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    negloglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM noise increases with set size and with age
        size_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        wm_noise = wm_noise_base * (1.0 + size_sensitivity * size_factor) + age_bias * age_group
        wm_noise = max(wm_noise, 1e-6)
        beta_wm = 1.0 / wm_noise  # precision increases when noise is low
        beta_wm = np.clip(beta_wm, 1.0, 100.0)

        lapse = 0.01  # small lapse for numerical stability and unexplained choices

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL action distribution
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / max(denom_rl, 1e-12)
            p_rl = pi_rl[a]

            # WM action distribution
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - np.max(W_s))))
            pi_wm = np.exp(beta_wm * (W_s - np.max(W_s))) / max(denom_wm, 1e-12)
            p_wm = pi_wm[a]

            # Uncertainty-based arbitration: weight WM by relative certainty
            # Certainty ~ 1 - normalized entropy
            def entropy(p):
                p_safe = np.clip(p, 1e-12, 1.0)
                return -np.sum(p_safe * np.log(p_safe))
            H_rl = entropy(pi_rl)
            H_wm = entropy(pi_wm)
            H_max = np.log(nA)
            C_rl = 1.0 - H_rl / H_max
            C_wm = 1.0 - H_wm / H_max

            # Weight WM proportional to its certainty
            wm_weight = C_wm / max(C_wm + C_rl, 1e-12)
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Lapse mixture with uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            negloglik -= np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: supervised toward one-hot when rewarded, small relax otherwise
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target
            else:
                # gentle relaxation toward prior to avoid stale false memories
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

    return negloglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with exploration bonus + capacity-limited WM store.

    Idea
    ----
    - RL includes a directed exploration bonus based on inverse visit counts, scaled by set size and age.
    - WM stores up to K state-action associations with near-deterministic policy for stored states.
      Capacity K is reduced in larger set sizes and in older age.
    - Arbitration: if state is in WM, mix WM and RL; otherwise mostly RL.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, beta_base, wm_K_base, wm_beta, explore_bonus, age_scale]
        - lr: RL learning rate (0..1)
        - beta_base: RL inverse temperature base (>0), scaled by 10 internally
        - wm_K_base: baseline WM capacity in number of states (>=0)
        - wm_beta: WM inverse temperature when state is stored (high -> deterministic)
        - explore_bonus: magnitude of directed exploration bonus (>0)
        - age_scale: reduction factor for WM capacity and exploration in older age (0..1)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_K_base, wm_beta, explore_bonus, age_scale = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    negloglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts per state-action for directed exploration
        N = np.zeros((nS, nA)) + 1e-6

        # WM capacity in number of states: reduced for larger set sizes and older age
        size_factor = 3.0 / float(nS)  # 1 for 3, 0.5 for 6
        K_eff = wm_K_base * size_factor * (1.0 - age_scale * age_group)
        K_eff = max(0.0, K_eff)
        K_int = int(np.floor(K_eff + 1e-9))

        # Track which states are currently in WM and their recency (for eviction)
        in_wm = np.zeros(nS, dtype=bool)
        wm_order = []  # list of states in WM; oldest at index 0
        wm_beta_eff = max(wm_beta, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with exploration bonus: add bonus inversely proportional to sqrt(N)
            # Bonus is larger for larger set sizes (lower size_factor) and smaller for older age.
            bonus_scale = (2.0 - size_factor) * (1.0 - 0.5 * age_group)
            bonus = explore_bonus * bonus_scale / np.sqrt(N[s, :] + 1.0)
            Q_eff = q[s, :] + bonus

            # RL probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_eff - np.max(Q_eff))))
            pi_rl = np.exp(softmax_beta * (Q_eff - np.max(Q_eff))) / max(denom_rl, 1e-12)
            p_rl = pi_rl[a]

            # WM policy: if state stored, use sharp distribution for the stored best action.
            if in_wm[s]:
                W_s = w[s, :]
                denom_wm = np.sum(np.exp(wm_beta_eff * (W_s - np.max(W_s))))
                pi_wm = np.exp(wm_beta_eff * (W_s - np.max(W_s))) / max(denom_wm, 1e-12)
                p_wm = pi_wm[a]
                # Weight WM more when state is in WM; otherwise fully RL
                wm_weight = 0.7
            else:
                p_wm = 1.0 / nA
                wm_weight = 0.0

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            negloglik -= np.log(max(p_total, 1e-12))

            # RL update and visit count update
            pe = r - q[s, a]
            q[s, a] += lr * pe
            N[s, a] += 1.0

            # WM update:
            # If rewarded, attempt to store state-action in WM, respecting capacity.
            if r > 0.5:
                # Create/overwrite a sharp memory for this state favoring action a
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * target

                if K_int > 0:
                    if not in_wm[s]:
                        # If capacity reached, evict the oldest
                        if len(wm_order) >= K_int:
                            evict = wm_order.pop(0)
                            in_wm[evict] = False
                            w[evict, :] = w_0[evict, :].copy()
                        wm_order.append(s)
                        in_wm[s] = True
                    else:
                        # refresh recency
                        if s in wm_order:
                            wm_order.remove(s)
                            wm_order.append(s)
            else:
                # If unrewarded, do a small relaxation toward uniform for that state
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

    return negloglik