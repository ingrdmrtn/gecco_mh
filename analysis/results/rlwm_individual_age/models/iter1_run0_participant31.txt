def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(+/-) with action stickiness and WM with set-size/age-dependent decay.

    The model blends a model-free RL controller with separate learning rates for
    positive and negative outcomes, and a working-memory (WM) controller that
    stores recent rewarded actions for each state but decays toward uniform.
    WM reliability declines with larger set size and with older age (age group 1).
    A choice perseveration (stickiness) term biases RL toward repeating the last
    chosen action in a state.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr_pos, lr_neg, softmax_beta, wm_weight, wm_decay_base, kappa_stick]
        - lr_pos: RL learning rate for positive reward prediction errors.
        - lr_neg: RL learning rate for negative reward prediction errors.
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - wm_weight: Base WM mixture weight (0..1) before set-size/age modulation.
        - wm_decay_base: Baseline WM decay toward uniform (per visit).
        - kappa_stick: Action stickiness weight added to the chosen action's value.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_decay_base, kappa_stick = model_parameters

    # Scale beta as in the template
    softmax_beta *= 10.0

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM weights
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Last action per state for stickiness; initialize to -1 (none)
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective WM parameters depend on set size and age
        # Larger set size => greater WM decay; older => additional decay
        wm_decay_eff = wm_decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        # WM contribution weight smaller for larger set size and older age
        wm_weight_eff_base = wm_weight * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_weight_eff_base = np.clip(wm_weight_eff_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add action stickiness to RL values within-state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += kappa_stick

            W_s = w[s, :]

            # RL policy probability of chosen action (softmax trick used in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (nearly deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff_base * p_wm + (1.0 - wm_weight_eff_base) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with separate learning rates
            pe = r - q[s, a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: decay toward uniform each visit; if rewarded, reinforce chosen action
            # Decay current state's WM toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0.0:
                # Sharpen distribution toward the chosen action
                # Add mass then renormalize
                w[s, a] += (1.0 - w[s, a]) * 0.9
                w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM arbitration by uncertainty and set size; age boosts WM arbitration for young.

    The model combines RL and WM via a dynamic arbitration weight that depends on
    RL uncertainty (entropy), set size, and age. When RL is uncertain (high entropy)
    and set size is small, the arbiter favors WM. WM uses a softer policy (finite
    temperature) and undergoes mild reward-based sharpening with decay.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, softmax_beta, wm_temp, arb_bias, arb_slope, age_wm_bonus]
        - lr: RL learning rate (single).
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - wm_temp: Inverse temperature for WM policy (finite, not infinite).
        - arb_bias: Baseline bias for WM arbitration (logit scale).
        - arb_slope: Sensitivity of arbitration to RL certainty (negative entropy).
        - age_wm_bonus: Additional arbitration bias toward WM for young (applied when age_group=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_temp, arb_bias, arb_slope, age_wm_bonus = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = float(wm_temp)  # controllable WM determinism
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob of chosen action (finite temperature)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL action distribution (for entropy/certainty)
            # softmax over Q_s
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            pi_rl = exp_logits / np.sum(exp_logits)
            # Entropy of RL (in nats)
            entropy_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            # Use certainty signal = -entropy; higher => more certain
            certainty_rl = -entropy_rl

            # Arbitration logit combines: bias, certainty, set size penalty, and age term
            # Larger set size reduces WM arbitration; young get a positive WM bonus
            setsize_penalty = np.log(nS / 3.0)
            age_bonus = (1 - age_group) * age_wm_bonus
            arb_logit = arb_bias + arb_slope * certainty_rl - setsize_penalty + age_bonus

            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_logit))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: mild decay toward uniform; reward-based sharpening
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.0:
                w[s, a] += 0.5 * (1.0 - w[s, a])
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration bonus and WM with finite-capacity state gating.

    The model blends RL that includes a directed exploration bonus (novelty/uncertainty)
    scaled by visit counts with a WM system that prioritizes up to K states (capacity).
    WM influence is strong for stored states and weak otherwise. Capacity K is reduced
    for older adults. Larger set sizes reduce the effective WM weight. The RL exploration
    bonus increases with age to capture greater exploration in older adults.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, softmax_beta, wm_weight_base, bonus_phi, wm_hold_m, age_bonus_phi]
        - lr: RL learning rate.
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - wm_weight_base: Base WM mixture weight for states within capacity.
        - bonus_phi: Directed exploration bonus scale for RL.
        - wm_hold_m: Number of first encounters per state with strong WM retention.
        - age_bonus_phi: Additional exploration bonus multiplier for older adults.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, bonus_phi, wm_hold_m, age_bonus_phi = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track per-state visit counts and WM capacity set
        state_visits = np.zeros(nS, dtype=int)
        wm_stored = np.zeros(nS, dtype=bool)
        # Effective capacity K: smaller for older adults
        K_eff = int(np.clip(np.round(nS * (0.8 - 0.3 * age_group)), 1, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update visit count for directed exploration
            state_visits[s] += 1
            visit_n = state_visits[s]

            # Determine whether this state is stored in WM (subject to capacity)
            if not wm_stored[s]:
                # Store the state if capacity allows
                if np.sum(wm_stored) < K_eff:
                    wm_stored[s] = True

            # RL with directed exploration bonus (state-visit-based)
            Q_s = q[s, :].copy()
            # Exploration bonus encourages exploration early in a state's learning
            phi_eff = bonus_phi * (1.0 + age_bonus_phi * age_group)
            bonus = phi_eff / np.sqrt(max(visit_n, 1))
            Q_bonus = Q_s + bonus

            W_s = w[s, :]

            # RL policy prob of chosen action with bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_bonus - Q_bonus[a])))

            # WM policy prob of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM mixture weight:
            # - strong if state is stored and early (<= m visits),
            # - scaled down by set size,
            # - older adults get reduced WM weight.
            in_hold = 1.0 if visit_n <= int(np.round(wm_hold_m)) else 0.0
            wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
            wm_weight_eff *= (1.0 if wm_stored[s] else 0.2)  # minimal WM if not stored
            wm_weight_eff *= (0.8 + 0.2 * in_hold)  # slightly higher early on
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: if rewarded, set a strong memory; otherwise decay toward uniform
            if r > 0.0:
                # Move most mass to chosen action
                w[s, :] = 0.1 * w[s, :] + 0.9 * w_0[s, :]
                w[s, a] += 0.9
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # Forget when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p