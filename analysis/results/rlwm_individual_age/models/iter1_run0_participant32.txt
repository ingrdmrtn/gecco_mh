def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM slots and perseveration bias.

    Mechanism
    - RL: Q-learning with eligibility traces (lambda) that propagate credit to recent state-actions.
    - WM: slot-limited, near-deterministic memory. WM mixture weight scales with slots/nS.
    - Perseveration: bias toward repeating last action in a state.
    - Set size: reduces effective WM weight via capacity ratio (slots/nS).
    - Age: older group has fewer effective WM slots and slightly stronger perseveration.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, wm_weight, softmax_beta, wm_slots, perseveration, lam]
        - alpha: RL learning rate
        - wm_weight: baseline arbitration weight on WM
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_slots: number of WM slots available (capacity)
        - perseveration: bias added to last action in a state
        - lam: eligibility trace decay (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, wm_weight, softmax_beta, wm_slots, perseveration, lam = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # Age modulation: reduce WM capacity and increase perseveration for older adults
    if age_group == 1:
        wm_slots = max(0.0, wm_slots * 0.7)
        perseveration *= 1.2

    softmax_beta_wm = 50.0  # near-deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL, WM, and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces
        last_action = -1 * np.ones(nS, dtype=int)  # last action taken per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            # Compute RL choice probability for chosen action
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            denom_rl = np.sum(np.exp(logits_rl))
            p_rl = np.exp(logits_rl[a]) / denom_rl

            # WM policy: near-deterministic over w[s]
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            denom_wm = np.sum(np.exp(logits_wm))
            p_wm = np.exp(logits_wm[a]) / denom_wm

            # Capacity-limited WM contribution
            recall_prob = min(1.0, max(0.0, wm_slots) / float(nS))
            wm_weight_eff = np.clip(wm_weight * recall_prob, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning with eligibility traces
            delta = r - q[s, a]
            e *= lam
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += alpha * delta * e

            # WM dynamics: slight decay toward uniform; one-shot write on reward
            decay = 0.05  # mild decay each encounter
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0  # store the rewarded mapping
            # Renormalize to avoid drift
            w[s, :] = w[s, :] / np.sum(w[s, :])

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor–Critic with set-size–specific temperatures and WM gating via load-dependent decay.

    Mechanism
    - Actor–Critic: state value v[s] as baseline; policy preferences (q) updated by TD error.
    - RL temperature: separate betas for small (3) vs large (6) set size.
    - WM: fast write on reward; forgetting increases with set size via kappa, diminishing WM weight.
    - Arbitration: mixture of WM and RL with base weight, reduced under load.
    - Age: older group has slower value learning and stronger WM decay.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like of int
    model_parameters : list or array
        [alpha_v, alpha_p, beta_small, beta_large, wm_weight, kappa]
        - alpha_v: value learning rate
        - alpha_p: policy (actor) learning rate
        - beta_small: inverse temperature for nS=3 (scaled internally by 10)
        - beta_large: inverse temperature for nS=6 (scaled internally by 10)
        - wm_weight: base WM arbitration weight
        - kappa: WM decay factor increasing with set size

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_v, alpha_p, beta_small, beta_large, wm_weight, kappa = model_parameters
    beta_small *= 10.0
    beta_large *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # Age effects
    if age_group == 1:
        alpha_v *= 0.8
        kappa *= 1.3

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))   # policy preferences for actor
        v = np.zeros(nS)                      # state value for critic
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_eff = beta_small if nS <= 3 else beta_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy softmax over actor preferences
            logits_rl = beta_eff * q[s, :]
            logits_rl -= np.max(logits_rl)
            denom_rl = np.sum(np.exp(logits_rl))
            p_rl = np.exp(logits_rl[a]) / denom_rl

            # WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            logits_wm -= np.max(logits_wm)
            denom_wm = np.sum(np.exp(logits_wm))
            p_wm = np.exp(logits_wm[a]) / denom_wm

            # WM weight reduced under load via kappa
            load_factor = max(0, nS - 3) / 3.0  # 0 at 3, 1 at 6
            wm_weight_eff = np.clip(wm_weight / (1.0 + kappa * load_factor), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # TD error
            delta = r - v[s]
            v[s] += alpha_v * delta

            # Policy update: push chosen action up by delta (actor)
            q[s, a] += alpha_p * delta

            # WM decay grows with load; one-shot write on reward
            forget = np.clip(0.05 + kappa * 0.15 * load_factor, 0.0, 0.95)
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-adaptive RL (metaplasticity) + WM with cross-state interference.

    Mechanism
    - RL: learning rate scales with surprise (|delta|) and set size (reduced under higher load).
    - WM: near-deterministic retrieval; after reward, consolidation plus interference:
      memory of the rewarded action bleeds into other states (confusion).
    - Arbitration: fixed WM weight modulated by load; decision noise fixed by beta.
    - Age: older group suffers stronger WM decay (age_wm_scale > 1), reducing WM reliability.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like of int
    model_parameters : list or array
        [alpha0, softmax_beta, wm_weight, wm_confusion, age_wm_scale, surprise_gain]
        - alpha0: base RL learning rate
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_weight: baseline WM arbitration weight
        - wm_confusion: degree of cross-state WM interference (0..1)
        - age_wm_scale: multiplier on WM decay for older adults (>=1); applied if age_group==1
        - surprise_gain: scales learning rate by |delta|

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha0, softmax_beta, wm_weight, wm_confusion, age_wm_scale, surprise_gain = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = softmax_beta * q[s, :]
            logits_rl -= np.max(logits_rl)
            denom_rl = np.sum(np.exp(logits_rl))
            p_rl = np.exp(logits_rl[a]) / denom_rl

            # WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            logits_wm -= np.max(logits_wm)
            denom_wm = np.sum(np.exp(logits_wm))
            p_wm = np.exp(logits_wm[a]) / denom_wm

            # Arbitration with load penalty on WM
            load_factor = max(0, nS - 3) / 3.0
            wm_weight_eff = np.clip(wm_weight / (1.0 + 0.5 * load_factor), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with surprise-adaptive learning rate and set-size penalty
            delta = r - q[s, a]
            alpha_eff = alpha0 * (1.0 + surprise_gain * abs(delta)) / (1.0 + 0.5 * max(0, nS - 3))
            q[s, a] += alpha_eff * delta

            # WM decay with age modulation
            base_decay = 0.07 + 0.15 * load_factor  # more decay at higher load
            if age_group == 1:
                base_decay = np.clip(base_decay * max(1.0, age_wm_scale), 0.0, 0.95)
            w[s, :] = (1.0 - base_decay) * w[s, :] + base_decay * w_0[s, :]

            # WM consolidation with cross-state interference after reward
            if r > 0.0:
                # Consolidate in the current state
                w[s, :] = 0.0
                w[s, a] = 1.0

                # Spread some probability mass to the same action in other states
                if wm_confusion > 0.0 and nS > 1:
                    bleed = wm_confusion / (nS - 1)
                    for sp in range(nS):
                        if sp == s:
                            continue
                        # Move the WM distribution slightly toward the chosen action
                        w[sp, :] = (1.0 - bleed) * w[sp, :]
                        w[sp, a] += bleed
                        # Renormalize each affected row
                        w[sp, :] /= np.sum(w[sp, :])

            # Ensure normalization for current state
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p