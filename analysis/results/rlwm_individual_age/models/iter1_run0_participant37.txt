def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with eligibility traces and uncertainty-weighted arbitration.

    Mechanism:
    - RL: Q-learning with an eligibility trace that keeps credit on recently chosen actions.
    - WM: one-shot storage of rewarded action with decay; partial negative marking on non-reward.
    - Arbitration: Trial-wise mixing weight depends on (a) capacity scaled by set size, (b) age penalty,
                   and (c) relative confidence in WM vs RL for the current state.
    - Policy: Mixture of RL softmax and near-deterministic WM softmax.

    Parameters (model_parameters; all used):
    - lr: RL learning rate in [0,1].
    - beta: RL inverse temperature, scaled internally by 10.
    - wm_cap: baseline WM capacity/weight (>0). Higher -> stronger WM.
    - wm_decay: WM decay rate toward baseline per visit in [0,1].
    - eta: eligibility trace decay in [0,1]; larger -> longer-lasting trace.
    - age_penalty: reduction of WM capacity for older adults (>=0).

    Inputs:
    - states: array of state indices per trial (int)
    - actions: array of chosen action indices per trial (int; 0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial (int)
    - set_sizes: array of set size for the active block on each trial (3 or 6)
    - age: array with a single value repeated (participant age)

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta, wm_cap, wm_decay, eta, age_penalty = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        # eligibility traces over state-actions
        z = np.zeros((nS, nA))

        # capacity-scaled, age-penalized base WM weight
        size_scale = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
        wm_base = wm_cap * size_scale * (1.0 - age_penalty * age_group)
        wm_base = 1.0 / (1.0 + np.exp(-wm_base))  # squash to (0,1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Confidence-based arbitration: larger spread -> higher confidence
            conf_rl = (np.max(Q_s) - np.min(Q_s)) + 1e-6
            conf_wm = (np.max(W_s) - np.min(W_s)) + 1e-6
            conf_mix = conf_wm / (conf_wm + conf_rl)
            wm_weight_eff = np.clip(wm_base * conf_mix, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            delta = r - q[s, a]
            # decay traces
            z *= eta
            # set current chosen trace to 1
            z[s, a] = 1.0
            # update all Qs proportionally to their trace
            q += lr * delta * z

            # WM update: decay toward baseline, then update based on feedback
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            if r > 0.5:
                # Strongly store rewarded action
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            else:
                # Negative marking: de-emphasize the chosen action
                w[s, a] = max(0.0, w[s, a] - 0.5)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q-forgetting + WM win-stay + adaptive lapse that increases with set size and age.

    Mechanism:
    - RL: Q-learning with learning rate and forgetting toward uniform (rho).
    - WM: stores the last rewarded action per state (win-stay); otherwise decays toward baseline.
    - Mixture: fixed WM base weight scaled down by set size; scaled further by age.
    - Lapse: stimulus-independent random choice with probability increasing with set size and age.

    Parameters (model_parameters; all used):
    - lr: RL learning rate in [0,1].
    - beta: RL inverse temperature, scaled internally by 10.
    - wm_base: base WM mixture weight in [0,1].
    - wm_decay: WM decay rate toward baseline per visit in [0,1].
    - rho: RL forgetting rate toward uniform per visit in [0,1].
    - lapse0: base lapse rate in [0,1] (will be scaled by set size and age).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as described above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_base, wm_decay, rho, lapse0 = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # set-size and age scaling
        size_scale = 3.0 / float(nS)  # 1 for 3-set, 0.5 for 6-set
        wm_weight_eff = np.clip(wm_base * size_scale * (1.0 - 0.3 * age_group), 0.0, 1.0)
        lapse_eff = np.clip(lapse0 * (float(nS) / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.49)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy (win-stay)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            q = (1.0 - rho) * q + rho * (1.0 / nA)

            # WM update: decay; if rewarded, store chosen action deterministically
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            # If not rewarded: no explicit negative marking; rely on decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-threshold gating between WM and RL with age-reduced capacity and beta modulation.

    Mechanism:
    - RL: standard Q-learning; RL inverse temperature is modulated by set size and age.
    - WM: stores rewarded actions; 'refresh' pushes WM toward the executed action on rewards.
    - Gating: WM gate is a sigmoid of (effective capacity - set size), where capacity is reduced by age.
              When set size is below capacity -> WM dominates; otherwise RL dominates.

    Parameters (model_parameters; all used):
    - lr: RL learning rate in [0,1].
    - beta: base RL inverse temperature, scaled internally by 10.
    - C: WM capacity threshold (in items, approx 3..6).
    - age_drop: capacity reduction applied if older (>=0).
    - wm_refresh: strength of WM refreshing toward the executed action on reward [0,1].
    - beta_bonus: bonus to RL inverse temperature in small set sizes (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as described above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, C, age_drop, wm_refresh, beta_bonus = model_parameters
    base_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity with age reduction
        C_eff = C - age_drop * age_group
        # Gating weight: sigmoid with steepness 5
        gate = 1.0 / (1.0 + np.exp(-5.0 * (C_eff - float(nS))))
        gate = np.clip(gate, 0.0, 1.0)  # ~1 when nS << C_eff (WM dominates), ~0 when nS >> C_eff

        # RL temperature modulation: sharper with small sets, flatter with age
        beta_eff = base_beta * (1.0 + beta_bonus * (3.0 / float(nS)) - 0.2 * age_group)
        beta_eff = max(1e-6, beta_eff)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            p_total = gate * p_wm + (1.0 - gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay then reward-based refresh
            w[s, :] = 0.95 * w[s, :] + 0.05 * w0[s, :]  # mild passive decay each visit
            if r > 0.5:
                # Refresh toward executed action
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * onehot

        blocks_log_p += log_p

    return -blocks_log_p