Here are three alternative cognitive models tailored to the RL-WM task, using the requested template and constraints. Each model returns the negative log-likelihood of the observed choices and uses age group and set size in a mechanistically meaningful way.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay + slot-limited WM capacity reduced by age and set size.

    Idea:
    - RL updates with a learning rate and decays toward uniform each trial (captures forgetting).
    - WM has a limited number of state "slots" (K_eff) that can be encoded deterministically upon reward.
      Capacity (slots) is reduced by age and larger set sizes; only up to K_eff states can be stored.
    - Arbitration is a mixture of RL and WM, scaled by wm_weight; WM readout is near-deterministic.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: mixture weight scaling for WM (0..1).
    - softmax_beta: RL inverse temperature; internally multiplied by 10.
    - rl_decay: RL decay toward uniform per trial (0..1).
    - K_base: baseline WM capacity in states (1..3).
    - slot_penalty_age_size: reduces capacity with age and set-size (non-negative).
      K_eff = clip(round(K_base - slot_penalty_age_size*(age_group + (nS-3)/3)), 1, 3).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rl_decay, K_base, slot_penalty_age_size = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity in number of states
        cap_reduction = slot_penalty_age_size * (age_group + max(nS - 3, 0) / 3.0)
        K_eff = int(np.clip(np.round(K_base - cap_reduction), 1, 3))

        # Track which states are currently stored (binary mask) based on confidence
        stored_mask = np.zeros(nS, dtype=bool)

        def is_stored(W_row):
            # Consider stored if confidence above 0.8
            conf = (np.max(W_row) - 1.0 / nA) / (1.0 - 1.0 / nA)
            return conf >= 0.8

        # Recompute stored mask from initial (uniform -> none)
        for s in range(nS):
            stored_mask[s] = is_stored(w[s, :])

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Fixed mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with decay to uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)

            # WM update: encode deterministically on rewarded trials if capacity available
            # If state already stored, refresh to one-hot on reward
            if r > 0:
                # Count how many states currently stored
                current_slots = int(np.sum(stored_mask))
                already_stored = stored_mask[s]
                if already_stored or current_slots < K_eff:
                    onehot = np.zeros(nA)
                    onehot[a] = 1.0
                    w[s, :] = onehot
                    stored_mask[s] = True
                # else: no encoding due to capacity cap
            else:
                # No explicit WM update on non-reward; leave as is
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age/set-size-suppressed inverse temperature + WM with probabilistic encoding and decay.
    Arbitration depends on WM certainty; WM certainty grows with successful encoding and decays with load.

    Idea:
    - RL uses a learning rate and an effective softmax beta reduced by age and larger set sizes.
    - WM encodes rewarded associations with probability mass p_store (implemented as a convex update).
      WM decays toward uniform each trial with a decay rate that grows with load and age.
    - Arbitration weight is wm_weight_base scaled by WM certainty (1 - normalized entropy).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: base arbitration weight (0..1), later scaled by WM certainty.
    - softmax_beta: base RL inverse temperature; internally scaled by 10 then downweighted by age/set size.
    - wm_decay_base: base WM decay rate toward uniform (0..1).
    - p_store: WM encoding strength on rewarded trials (0..1).
    - beta_scale_age_size: non-negative factor reducing RL beta with age and set-size.
      beta_eff = beta * exp(-beta_scale_age_size*(age_group + (nS-3)/3)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay_base, p_store, beta_scale_age_size = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL beta reduced by age and set-size; fixed within block
        load_term = age_group + max(nS - 3, 0) / 3.0
        beta_eff = softmax_beta * np.exp(-beta_scale_age_size * load_term)

        # WM decay increases with load and age
        wm_decay = wm_decay_base * (1.0 + load_term)
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with beta_eff
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # WM certainty via normalized entropy
            eps = 1e-12
            P = np.clip(W_s, eps, 1.0)
            P = P / P.sum()
            entropy = -np.sum(P * np.log(P + eps))
            entropy_max = np.log(nA)
            certainty = float(np.clip(1.0 - entropy / (entropy_max + eps), 0.0, 1.0))

            wm_weight_t = float(np.clip(wm_weight_base * certainty, 0.0, 1.0))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay every trial + encode on reward as convex update
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if r > 0 and p_store > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - p_store) * w[s, :] + p_store * onehot

            # Renormalize to keep a valid distribution
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with choice stickiness + RPE-gated WM updates modulated by age and set size.

    Idea:
    - RL updates values with learning rate; choice policy includes a perseveration (stickiness) bias toward
      last action taken in the same state.
    - WM updates are gated by the magnitude of the RPE: larger surprise -> stronger gating.
      Older age and larger set sizes reduce gating effectiveness.
    - Arbitration weight scales with trial-level gate strength.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: global WM arbitration scale (0..1).
    - softmax_beta: RL inverse temperature; internally multiplied by 10.
    - stickiness: choice perseveration bias added to the last chosen action in this state (>=0).
    - gate_sensitivity: scales how strongly |RPE| increases the WM update gate (>=0).
    - gate_penalty_age_size: subtractive penalty on gating from age and set-size (>=0).
      gate = sigmoid(gate_sensitivity*|RPE| - gate_penalty_age_size*(age_group + (nS-3)/3)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, gate_sensitivity, gate_penalty_age_size = model_parameters
    softmax_beta *= 10

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness (initialize to none)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # Add stickiness bias to the last chosen action in this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += stickiness

            # RL policy with stickiness bias
            U = Q_s + bias
            denom_rl = np.sum(np.exp(softmax_beta * (U - U[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Compute RPE for gating
            delta = r - Q_s[a]
            load_term = age_group + max(nS - 3, 0) / 3.0
            gate_input = gate_sensitivity * np.abs(delta) - gate_penalty_age_size * load_term
            # Stable sigmoid
            gate = 1.0 / (1.0 + np.exp(-gate_input))
            gate = float(np.clip(gate, 0.0, 1.0))

            # Arbitration scales with gate strength
            wm_weight_t = float(np.clip(wm_weight * gate, 0.0, 1.0))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update (use original Q_s[a] for delta already computed)
            q[s, a] += lr * delta

            # WM update: gated by |RPE|; on reward, move toward onehot; on no-reward, move toward uniform
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - gate) * w[s, :] + gate * onehot
            else:
                w[s, :] = (1.0 - gate) * w[s, :] + gate * w_0[s, :]

            # Keep WM row normalized
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size effects across models:
- Model 1: Age and set size reduce WM capacity (fewer storable states), which should hurt performance more in 6-item blocks and for older adults; RL experiences global decay.
- Model 2: Age and set size reduce RL reliability (beta) and increase WM decay; WM encodes with strength p_store; arbitration depends on WM certainty.
- Model 3: Age and set size reduce RPE-based WM gating, yielding weaker WM use under higher load and in older adults; RL includes perseveration to capture potential response biases.