def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility-trace) + WM with set-size/age-sensitive interference and choice stickiness.

    Mechanism
    - RL: Q-learning with eligibility traces and a choice stickiness kernel.
    - WM: one-shot caching of correct response on rewarded trials; decays toward uniform with
      an interference rate that increases with set size and age.
    - Mixture policy: WM and RL mixed with a capacity-based weighting (logistic in set size).
      Older adults exhibit lower effective inverse temperature and stronger WM interference.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 for the block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the current block (constant within block).
    age : array-like (single value repeated)
        Participant age; used to determine age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, beta_base, C_capacity, age_noise, lambda_et, kappa_stick]
        - alpha (0..1): RL learning rate.
        - beta_base (>0): baseline RL inverse temperature (scaled internally).
        - C_capacity (0..6): WM capacity anchor controlling WM mixture weight via logistic(C - set_size).
        - age_noise (>=0): increases WM decay/interference and reduces effective RL beta in older adults.
        - lambda_et (0..1): eligibility-trace decay parameter for RL.
        - kappa_stick (>=0): choice stickiness bias added to the last chosen action.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta_base, C_capacity, age_noise, lambda_et, kappa_stick = model_parameters
    # Scale RL temperature; older adults have effectively lower beta (more noise)
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0 / (1.0 + age_noise * age_group)
    softmax_beta_wm = 50.0  # near-deterministic WM when strong

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL values, WM store, and traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces
        last_action = None

        # WM mixture weight as a logistic capacity function of set size
        # Higher set size -> lower WM weight; higher capacity -> higher WM weight
        wm_weight = 1.0 / (1.0 + np.exp(nS - C_capacity))
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Choice stickiness (state-independent): add bias to last chosen action
            bias = np.zeros(nA)
            if last_action is not None:
                bias[last_action] += kappa_stick

            # RL policy with stickiness bias
            logits_rl = softmax_beta * Q_s + bias
            logits_rl -= np.max(logits_rl)
            prl_vec = np.exp(logits_rl)
            prl_vec /= np.sum(prl_vec)
            p_rl = prl_vec[a]

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            pwm_vec = np.exp(logits_wm)
            pwm_vec /= np.sum(pwm_vec)
            p_wm = pwm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces (bandit-style gamma=1)
            pe = r - q[s, a]
            # update eligibility: decay all, set current SA to 1
            e *= lambda_et
            e[s, a] = 1.0
            q += alpha * pe * e

            # WM decay/interference increases with set size and age
            # Interference rate d in [0,1): more decay for larger nS and older age
            d = np.clip(0.1 + 0.4 * (nS / 6.0) * (1.0 + 0.5 * age_group * age_noise), 0.0, 0.95)
            w = (1.0 - d) * w + d * w_0
            # One-shot store on rewarded trials
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM with decaying Dirichlet-Beta counts, set-size/age-sensitive forgetting,
    and action-side motor bias.

    Mechanism
    - RL: standard Q-learning.
    - Bayesian WM: for each state-action, maintain Beta(success, failure) counts with exponential
      decay toward a symmetric prior; use posterior mean as WM value and softmax with high beta.
    - Mixture weight: increases with posterior precision (success+failure) and decreases with set size.
    - Age: increases WM forgetting; also reduces WM contribution indirectly via lower precision.
    - Motor bias: additive preference for action 0.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [alpha, softmax_beta, wm_forget, age_forget, mix_temp, bias0]
        - alpha (0..1): RL learning rate.
        - softmax_beta (>0): RL inverse temperature (scaled internally by 10).
        - wm_forget (0..1): base WM forgetting rate per trial (toward prior).
        - age_forget (0..1): additional forgetting component applied when older, scaled by set size.
        - mix_temp (>0): sensitivity of WM mixture weight to precision; larger -> more WM when precise.
        - bias0 (any real): additive motor bias favoring action 0 in both RL and WM policies.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, softmax_beta, wm_forget, age_forget, mix_temp, bias0 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM: Beta counts per (s,a), start with symmetric Beta(1,1) prior
        succ = np.ones((nS, nA))
        fail = np.ones((nS, nA))
        prior_succ = np.ones((nS, nA))
        prior_fail = np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Posterior mean for WM values
            wm_mean = succ[s, :] / (succ[s, :] + fail[s, :] + 1e-12)

            # Mixture weight based on precision and set size
            precision = np.sum(succ[s, :] + fail[s, :]) / nA  # average precision for this state
            # Effective precision relative to chance (prior ~2), tempered by set size
            prec_centered = precision - 2.0
            wm_weight = 1.0 / (1.0 + np.exp(-(mix_temp * prec_centered - np.log(1 + nS))))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # RL policy with motor bias on action 0
            logits_rl = softmax_beta * Q_s
            logits_rl[0] += bias0
            logits_rl -= np.max(logits_rl)
            prl_vec = np.exp(logits_rl)
            prl_vec /= np.sum(prl_vec)
            p_rl = prl_vec[a]

            # WM policy with motor bias
            logits_wm = softmax_beta_wm * wm_mean
            logits_wm[0] += bias0
            logits_wm -= np.max(logits_wm)
            pwm_vec = np.exp(logits_wm)
            pwm_vec /= np.sum(pwm_vec)
            p_wm = pwm_vec[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM forgetting toward prior; older and larger set sizes forget more
            f_eff = np.clip(wm_forget + age_group * age_forget * (nS / 6.0), 0.0, 0.95)
            succ = (1.0 - f_eff) * succ + f_eff * prior_succ
            fail = (1.0 - f_eff) * fail + f_eff * prior_fail

            # Update counts based on outcome for chosen action in current state
            if r > 0.5:
                succ[s, a] += 1.0
            else:
                fail[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Arbitration model: WM vs RL selected via confidence-based gating with age bias,
    RL with choice kernel, WM decay.

    Mechanism
    - RL: Q-learning with a choice kernel (perseveration).
    - WM: one-shot caching (on reward) with exponential decay toward uniform.
    - Arbitration: probability of using WM increases with WM distinctiveness
      (max-minus-second-best) and decreases with RL uncertainty (state visit scarcity).
      Older adults are biased toward RL (reduced WM gating) via an age-dependent shift.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [alpha, softmax_beta, theta_gate, gate_bias_age, wm_decay, rho_stick]
        - alpha (0..1): RL learning rate.
        - softmax_beta (>0): RL inverse temperature (scaled internally by 10).
        - theta_gate (>0): sensitivity of arbitration to confidence signals.
        - gate_bias_age (>=0): shift that reduces WM gating in older adults.
        - wm_decay (0..1): base WM decay rate toward uniform per trial.
        - rho_stick (>=0): choice kernel strength added to previously chosen action.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, softmax_beta, theta_gate, gate_bias_age, wm_decay, rho_stick = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visit_counts = np.zeros(nS, dtype=float)
        last_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy with choice kernel (stickiness)
            bias = np.zeros(nA)
            if last_action is not None:
                bias[last_action] += rho_stick
            logits_rl = softmax_beta * Q_s + bias
            logits_rl -= np.max(logits_rl)
            prl_vec = np.exp(logits_rl)
            prl_vec /= np.sum(prl_vec)
            p_rl = prl_vec[a]

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            pwm_vec = np.exp(logits_wm)
            pwm_vec /= np.sum(pwm_vec)
            p_wm = pwm_vec[a]

            # Arbitration signals:
            # WM distinctiveness: gap between best and second-best WM values for this state
            sorted_w = np.sort(W_s)[::-1]
            wm_gap = sorted_w[0] - sorted_w[1] if nA >= 2 else 0.0
            # RL uncertainty proxy: less visited states => higher uncertainty
            rl_uncert = 1.0 / (1.0 + visit_counts[s])

            # Age-biased gate: older adults shifted toward RL (lower WM gate)
            gate_input = theta_gate * (wm_gap - rl_uncert) - gate_bias_age * age_group - np.log(1 + nS)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM decay increases with set size (interference)
            d = np.clip(wm_decay * (1.0 + 0.5 * (nS / 6.0)), 0.0, 0.95)
            w = (1.0 - d) * w + d * w_0
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            visit_counts[s] += 1.0
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p