def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-weighted WM, decaying WM store.

    The model mixes a model-free RL policy with a capacity-limited working-memory (WM) policy.
    WM expressivity is scaled down both by set size (capacity) and by age group. WM traces decay
    toward a uniform prior within a block. RL uses a standard delta-rule.

    Parameters
    ----------
    states : array-like of int
        State index at each trial, zero-based within each block.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Reward feedback (0/1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like (constant value repeated)
        Participant's age. Used to derive age group: 0=young, 1=old.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, wm_decay, wm_capacity, age_wm_penalty]
        - lr: RL learning rate (0..1)
        - wm_weight: base weight of WM in the mixture (0..1)
        - softmax_beta: inverse temperature for RL (scaled by 10 internally)
        - wm_decay: per-trial WM decay toward uniform (0..1)
        - wm_capacity: effective WM capacity in number of items (e.g., 2..6)
        - age_wm_penalty: fractional down-weighting of WM if age_group==1 (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity, age_wm_penalty = model_parameters
    softmax_beta *= 10.0  # as specified in template
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # deterministic WM
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action
            Q_s_center = Q_s - np.max(Q_s)
            p_vec_rl = np.exp(softmax_beta * Q_s_center)
            p_vec_rl /= np.sum(p_vec_rl)
            p_rl = p_vec_rl[a]

            # WM policy: softmax over WM map for this state
            W_s_center = W_s - np.max(W_s)
            p_vec_wm = np.exp(softmax_beta_wm * W_s_center)
            p_vec_wm /= np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            # Effective WM weight scales with set size (capacity) and age
            cap_factor = min(1.0, max(0.0, wm_capacity / float(nS)))  # >1 clipped to 1, <1 shrinks WM
            age_factor = 1.0 - age_wm_penalty * age_group
            wm_w_eff = np.clip(wm_weight * cap_factor * age_factor, 0.0, 1.0)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: global decay toward uniform, then reinforce chosen action proportional to reward
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-based sharpening (one-shot-like if r=1, no push if r=0)
            if r > 0.0:
                # Move probability mass toward chosen action within the state
                gain = r  # in {0,1}
                # Normalize row then add gain to chosen, renormalize
                row = w[s, :].copy()
                row /= np.sum(row)
                row = (1.0 - gain) * row
                row[a] += gain
                # slight smoothing to avoid zeros
                row = 0.999 * row + 0.001 / nA
                row /= np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM gating on reward, with age-modulated RL plasticity.

    The policy mixes RL and WM. RL uses separate learning rates for positive and negative prediction errors
    and is downscaled in older adults. WM is updated in a gated, one-shot manner only on rewarded trials,
    and its influence is reduced in larger set sizes.

    Parameters
    ----------
    states : array-like of int
        State index at each trial, zero-based within each block.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Reward feedback (0/1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like (constant value repeated)
        Participant's age. Used to derive age group: 0=young, 1=old.
    model_parameters : list or array
        [lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, age_rl_penalty]
        - lr_pos: RL learning rate for rewarded outcomes
        - lr_neg: RL learning rate for non-rewarded outcomes
        - wm_weight: base mixture weight for WM
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_decay: per-trial WM decay toward uniform
        - age_rl_penalty: fractional down-weighting of RL learning rates in older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, age_rl_penalty = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    # Age modulation of RL learning
    rl_scale = 1.0 - age_rl_penalty * age_group
    lr_pos_eff = np.clip(lr_pos * rl_scale, 0.0, 1.0)
    lr_neg_eff = np.clip(lr_neg * rl_scale, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM weight (penalize at size 6)
        size_penalty = 1.0 if nS == 3 else 0.5
        wm_w_base = np.clip(wm_weight * size_penalty, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability
            Q_s_center = Q_s - np.max(Q_s)
            p_vec_rl = np.exp(softmax_beta * Q_s_center)
            p_vec_rl /= np.sum(p_vec_rl)
            p_rl = p_vec_rl[a]

            # WM policy
            W_s_center = W_s - np.max(W_s)
            p_vec_wm = np.exp(softmax_beta_wm * W_s_center)
            p_vec_wm /= np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            p_total = wm_w_base * p_wm + (1.0 - wm_w_base) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_eff = lr_pos_eff if pe >= 0 else lr_neg_eff
            q[s, a] += lr_eff * pe

            # WM update: decay each trial; gated one-shot write on reward only
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                # Overwrite to a one-hot like distribution for the correct action
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with perseveration bias and lapse rate modulated by age and set size.

    The action policy mixes RL and WM and includes:
    - perseveration bias to repeat the last action taken in that state,
    - a lapse (choice noise) component that increases with set size and in older adults.
    WM is updated one-shot only on rewards; RL uses a single learning rate.

    Parameters
    ----------
    states : array-like of int
        State index at each trial, zero-based within each block.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Reward feedback (0/1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like (constant value repeated)
        Participant's age. Used to derive age group: 0=young, 1=old.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, perseveration, lapse_base, age_lapse_boost]
        - lr: RL learning rate
        - wm_weight: base weight of WM in the RL/WM mixture
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - perseveration: additive bias favoring repeating the last action in a state (in value units)
        - lapse_base: base lapse probability at set size 3 in young
        - age_lapse_boost: additional lapse added if age_group==1

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, perseveration, lapse_base, age_lapse_boost = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state within the block for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Lapse increases with set size and age
        size_boost = 0.0 if nS == 3 else 0.1  # mild extra lapse in larger sets
        lapse = np.clip(lapse_base + size_boost + age_lapse_boost * age_group, 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add perseveration bias to both RL and WM logits for the last chosen action in this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += perseveration

            # RL policy with bias
            Q_logits = softmax_beta * Q_s + bias
            Q_logits -= np.max(Q_logits)
            p_vec_rl = np.exp(Q_logits)
            p_vec_rl /= np.sum(p_vec_rl)
            p_rl = p_vec_rl[a]

            # WM policy with bias
            W_logits = softmax_beta_wm * W_s + bias
            W_logits -= np.max(W_logits)
            p_vec_wm = np.exp(W_logits)
            p_vec_wm /= np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            wm_w_eff = np.clip(wm_weight, 0.0, 1.0)
            p_mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl

            # Lapse mixture: with probability lapse choose uniformly
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: only on reward (one-shot); otherwise weak decay
            w[s, :] = 0.98 * w[s, :] + 0.02 * w_0[s, :]
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

            # Update perseveration tracker
            last_action[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)