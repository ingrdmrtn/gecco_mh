def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM arbitration with set-size- and age-dependent WM capacity and RL stickiness.

    Key ideas:
    - RL system learns Q-values with a single learning rate.
    - WM system stores the last rewarded action per state (no decay), producing a near-deterministic policy.
    - Arbitration weight depends on an effective WM capacity that scales with set size and is reduced in older adults.
    - RL includes a state-specific perseveration (stickiness) bias to repeat the last chosen action in that state.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if < 45, 1 otherwise.
    model_parameters : list or array
        [lr, softmax_beta, wm_capacity_base, wm_capacity_old_delta, stickiness]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_capacity_base: baseline WM capacity scale
        - wm_capacity_old_delta: capacity reduction (if negative) or increase (if positive) for old group
        - stickiness: tendency to repeat last action within a state in RL policy

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_capacity_base, wm_capacity_old_delta, stickiness = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM
    
    # Age group: 0 = young, 1 = old
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size- and age-dependent effective capacity -> WM mixture weight
        capacity = wm_capacity_base + age_group * wm_capacity_old_delta
        # Map capacity to [0,1] arbitration weight by relative capacity to set size
        wm_weight = capacity / float(nS)
        wm_weight = max(0.0, min(1.0, wm_weight))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness bias on last action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy (near-deterministic softmax on stored WM weights)
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: store rewarded mapping; clear if current memory contradicted by error
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If the stored action matches the chosen incorrect action, clear memory to uniform
                if np.argmax(w[s, :]) == a and np.max(w[s, :]) > (1.0 / nA):
                    w[s, :] = w_0[s, :]

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q-forgetting + WM one-shot noisy store + uncertainty-based arbitration.
    
    Key ideas:
    - RL uses a single learning rate and includes forgetting toward uniform after each trial.
    - WM stores rewarded actions with a noise parameter, forming a soft one-hot distribution.
    - Arbitration weight favors WM more in larger uncertainty (early visits) and in smaller set sizes.
    - Age group modulates exploration/exploitation by adjusting softmax beta.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if < 45, 1 otherwise.
    model_parameters : list or array
        [lr, softmax_beta_base, q_forget, wm_weight_base, wm_noise, beta_old_delta]
        - lr: RL learning rate (0..1)
        - softmax_beta_base: baseline RL inverse temperature (scaled by 10)
        - q_forget: RL forgetting rate toward uniform after each trial (0..1)
        - wm_weight_base: base arbitration weight coefficient
        - wm_noise: WM storage noise; 0 = perfect one-hot, 1 = uniform
        - beta_old_delta: additive change to softmax_beta for old group (can be +/-)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta_base, q_forget, wm_weight_base, wm_noise, beta_old_delta = model_parameters
    age_group = 1 if age[0] > 45 else 0
    softmax_beta = (softmax_beta_base + age_group * beta_old_delta) * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track state visit counts for uncertainty weighting
        visits = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration: more WM when early/uncertain and small set size
            # visits[s] is count prior to current choice
            uncert = 1.0 / np.sqrt(visits[s] + 1.0)
            base_ss = (3.0 / float(nS))
            wm_weight = wm_weight_base * base_ss * uncert
            wm_weight = max(0.0, min(1.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # RL forgetting toward uniform
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA)

            # WM update: store rewarded action with noise; else leave as-is
            if r > 0.0:
                w[s, :] = wm_noise * (1.0 / nA)
                w[s, a] += (1.0 - wm_noise)

            visits[s] += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with error-contingent WM boost and age-penalized WM arbitration.

    Key ideas:
    - RL uses a single learning rate and standard softmax.
    - WM stores rewarded actions (one-shot, deterministic) and resets to uniform on errors.
    - Arbitration weight increases transiently after an error on the same state (to favor WM),
      scales down with larger set sizes, and is penalized in the older group.
    - No asymmetry in learning rates and no explicit lapse; distinct from prior models.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if < 45, 1 otherwise.
    model_parameters : list or array
        [lr, softmax_beta, wm_base, error_boost, age_wm_penalty]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10)
        - wm_base: base WM arbitration weight before adjustments
        - error_boost: additive boost to WM weight after an error on that state
        - age_wm_penalty: multiplicative penalty applied to WM weight in old group (0..1 typical)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_base, error_boost, age_wm_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last outcome per state to trigger error-based WM boost
        last_error = np.zeros(nS, dtype=int)  # 1 if previous trial on that state was error, else 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration: base scaled by set size, boosted after recent error on this state,
            # and penalized for older group
            wm_weight = wm_base * (3.0 / float(nS))
            if last_error[s] == 1:
                wm_weight += error_boost
            if age_group == 1:
                wm_weight *= (1.0 - age_wm_penalty)
            wm_weight = max(0.0, min(1.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: set on reward; reset to uniform on error
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
                last_error[s] = 0
            else:
                w[s, :] = w_0[s, :]
                last_error[s] = 1

        blocks_log_p += log_p

    return -blocks_log_p