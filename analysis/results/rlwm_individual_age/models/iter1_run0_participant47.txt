def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with lapses and perseveration (state-specific), WM leaky memory with time constant.

    Mechanisms
    ----------
    - RL: single learning rate, softmax choice.
    - Perseveration: bias toward last chosen action in the same state.
    - WM: near-deterministic policy for stored associations; updates to one-hot on reward.
           WM leaks toward uniform with a time constant (tau_wm).
    - Lapses: mixture with a uniform random choice, increasing with set size and older age.
    - Mixture: WM and RL are linearly mixed, with WM weight reduced by set size and older age.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like of float
        Participant's age (single repeated value). Used to derive age group.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, lapse_base, perseveration, tau_wm]
        - alpha: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL (scaled internally by 10).
        - wm_weight_base: baseline WM mixture weight (0..1).
        - lapse_base: base lapse probability mixed with uniform policy (0..0.5).
        - perseveration: bias added to the previously chosen action in the same state.
        - tau_wm: WM leak time constant (>0). Larger = slower forgetting.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, lapse_base, perseveration, tau_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific perseveration: last action per state
        last_action_per_state = -1 * np.ones(nS, dtype=int)

        # Set-size and age effects on WM weight and lapses
        wm_weight_block = np.clip(wm_weight_base * (3.0 / float(nS)) * (0.85 if age_group == 1 else 1.0), 0.0, 1.0)
        lapse_block = np.clip(lapse_base * (float(nS) / 3.0) * (1.2 if age_group == 1 else 1.0), 0.0, 0.5)

        # WM leak factor from time constant; older = faster leak
        tau_eff = tau_wm / (1.3 if age_group == 1 else 1.0)
        leak = 1.0 - np.exp(-1.0 / max(tau_eff, 1e-6))
        leak = np.clip(leak, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with state-specific perseveration
            Q_s = q[s, :].copy()
            if last_action_per_state[s] >= 0:
                Q_s[last_action_per_state[s]] += perseveration

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy: deterministic softmax over W
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture with lapses
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse_block) * p_mix + lapse_block * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]
            # Reward-locked WM update: write one-hot on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update last action for perseveration
            last_action_per_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with global stickiness and decay + WM with interference scaling by encountered set size.

    Mechanisms
    ----------
    - RL: single learning rate, softmax choice with global stickiness (tendency to repeat last action).
    - RL decay: values drift toward uniform each trial (rho), stronger in older adults.
    - WM: stores last rewarded action per state (one-shot); near-deterministic policy.
    - WM mixture weight shrinks as more unique states are encountered in a block; reduced further in older adults.
    - WM temperature is tied to RL beta via a scale parameter.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like of float
        Participant's age (single repeated value). Used to derive age group.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, rho_rl_decay, stickiness, beta_wm_scale]
        - alpha: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL (scaled internally by 10).
        - wm_weight_base: baseline WM mixture weight (0..1).
        - rho_rl_decay: RL decay rate toward uniform per trial (0..1).
        - stickiness: global tendency to repeat the previous trial's action.
        - beta_wm_scale: scales WM inverse temperature relative to RL.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, rho_rl_decay, stickiness, beta_wm_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age increases RL decay (more drift/interference)
        rho_eff = np.clip(rho_rl_decay * (1.2 if age_group == 1 else 1.0), 0.0, 1.0)
        beta_wm = min(100.0, max(1.0, softmax_beta * max(0.1, beta_wm_scale)))

        # Track unique states encountered to scale WM weight online
        seen_states = set()

        # Global last action for stickiness
        last_action_global = -1

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update set of seen states and compute WM weight
            seen_states.add(s)
            unique_count = max(1, len(seen_states))
            # WM weight shrinks with unique states and with older age
            wm_weight_t = wm_weight_base * (1.0 / float(unique_count)) * (0.85 if age_group == 1 else 1.0)
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # RL decay toward uniform (global interference) before choice
            q = (1.0 - rho_eff) * q + rho_eff * (1.0 / nA) * np.ones_like(q)

            # RL policy with global stickiness bias
            Q_s = q[s, :].copy()
            if last_action_global >= 0:
                Q_s[last_action_global] += stickiness

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # WM update: write one-hot on reward; otherwise keep (no extra forgetting parameter here)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update stickiness memory
            last_action_global = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus + WM with confidence-weighted mixing and learning-rate-based memory.

    Mechanisms
    ----------
    - RL: single learning rate with exploration bonus (UCB-style) based on action-visit counts.
    - WM: probabilistic memory that learns toward one-hot on reward with learning rate (eta_wm),
           and relaxes toward uniform on non-reward.
    - Mixture weight: WM contribution scales with WM confidence (max - second max),
           penalized by larger set size and older age.
    - Older age reduces WM learning rate and the exploration bonus.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like of float
        Participant's age (single repeated value). Used to derive age group.
    model_parameters : list or array
        [alpha, softmax_beta, wm_base, kappa_bonus, wm_temp_scale, eta_wm]
        - alpha: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL (scaled internally by 10).
        - wm_base: baseline WM mixture weight (0..1).
        - kappa_bonus: scale of exploration bonus (UCB-like).
        - wm_temp_scale: scales WM inverse temperature relative to RL.
        - eta_wm: WM learning rate toward target distribution.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_base, kappa_bonus, wm_temp_scale, eta_wm = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0

    # Age effects
    bonus_age_factor = 0.8 if age_group == 1 else 1.0
    wm_age_factor = 0.85 if age_group == 1 else 1.0
    eta_wm_eff = eta_wm * (0.85 if age_group == 1 else 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for UCB-like exploration
        N = 1e-6 * np.ones((nS, nA))  # tiny prior to avoid div by zero

        beta_wm = min(100.0, max(1.0, softmax_beta * max(0.1, wm_temp_scale)))

        # Size penalty: stronger reduction when nS=6; use squared penalty to differ from linear scaling
        size_penalty = (3.0 / float(nS)) ** 2

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with exploration bonus
            bonus_s = kappa_bonus * bonus_age_factor / np.sqrt(N[s, :])
            pref_s = q[s, :] + bonus_s
            denom_rl = np.sum(np.exp(softmax_beta * (pref_s - pref_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy and confidence-weighted mixing
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Confidence = margin between top two WM probs
            sorted_W = np.sort(W_s)[::-1]
            conf = max(0.0, (sorted_W[0] - sorted_W[1])) if len(sorted_W) >= 2 else 0.0

            wm_weight_t = np.clip(wm_base * wm_age_factor * size_penalty * conf, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and counts
            delta = r - q[s, a]
            q[s, a] += alpha * delta
            N[s, a] += 1.0

            # WM learning: toward one-hot on reward, toward uniform on non-reward
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]
            w[s, :] = (1.0 - eta_wm_eff) * w[s, :] + eta_wm_eff * target

        blocks_log_p += log_p

    return -blocks_log_p