def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with valence-asymmetric learning and interference-prone WM; age increases noise and WM interference.

    Mechanism
    - RL: Q-learning with separate positive/negative learning rates, plus softmax choice.
    - WM: one-shot storage on reward with interference across states; policy mixed with RL via a fixed gate.
    - Set size: higher set size increases interference (more items) and reduces effective decision precision.
    - Age (old=1): increases decision noise (reduces beta) and increases WM interference.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1, per block).
    actions : array-like, int
        Chosen action on each trial (0..2).
    rewards : array-like, int
        Binary feedback on each trial (0 or 1).
    blocks : array-like, int
        Block index for each trial.
    set_sizes : array-like, int
        Set size for each block (3 or 6), repeated in the block.
    age : array-like, int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr_pos, lr_neg, softmax_beta_base, wm_gate_base, wm_interference_base, age_noise_interf]
        - lr_pos: learning rate after reward (0..1)
        - lr_neg: learning rate after no-reward (0..1)
        - softmax_beta_base: baseline RL inverse temperature (scaled internally)
        - wm_gate_base: baseline mixture weight of WM vs RL at set size 3 (0..1)
        - wm_interference_base: baseline WM interference strength (0..1 per trial)
        - age_noise_interf: scalar >=0; increases interference and decreases beta in older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta_base, wm_gate_base, wm_interference_base, age_noise_interf = model_parameters
    softmax_beta = softmax_beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age modulations
        load_scale = 3.0 / nS  # 1.0 at set size 3; 0.5 at set size 6
        beta_eff = softmax_beta * load_scale / (1.0 + age_noise_interf * age_group)
        wm_weight = np.clip(wm_gate_base * load_scale, 0.0, 1.0)
        # Interference increases with set size and age
        wm_interf = wm_interference_base * (1.0 / load_scale) * (1.0 + age_noise_interf * age_group)
        wm_interf = np.clip(wm_interf, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy (deterministic given W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            if r > 0:
                q[s, a] += lr_pos * (r - q[s, a])
            else:
                q[s, a] += lr_neg * (r - q[s, a])

            # WM update with interference:
            # 1) decay toward uniform
            w[s, :] = (1.0 - wm_interf) * w[s, :] + wm_interf * w_0[s, :]
            # 2) store correct mapping on reward (one-shot)
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # 3) cross-state interference: a small fraction of each other state's trace leaks in
            #    proportional to average of other states (normalized)
            if nS > 1:
                mean_other = (np.sum(w, axis=0) - w[s, :]) / (nS - 1)
                w[s, :] = (1.0 - 0.1 * wm_interf) * w[s, :] + 0.1 * wm_interf * mean_other

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration by uncertainty with lapse; age increases arbitration bias toward RL and lapse.

    Mechanism
    - RL: Q-learning with single learning rate; maintain state-wise uncertainty as an EWMA of unsigned PEs.
    - WM: item-specific memory with decay toward uniform; confidence = max(W_s) - second_max(W_s).
    - Arbitration: weight = sigmoid(k*(conf_WM - uncert_RL - load_term - age_shift*age_group)).
    - Decision includes a small lapse epsilon to capture random choices.
    - Set size: larger set size increases load_term, driving arbitration toward RL and increasing lapse.
    - Age (old=1): increases arbitration bias toward RL and increases lapse.

    Parameters
    ----------
    states : array-like, int
    actions : array-like, int
    rewards : array-like, int
    blocks : array-like, int
    set_sizes : array-like, int
    age : array-like, int
    model_parameters : list or array
        [lr, softmax_beta_base, wm_decay, arbitration_k, age_shift, lapse_base]
        - lr: RL learning rate (0..1)
        - softmax_beta_base: baseline RL inverse temperature (scaled internally)
        - wm_decay: WM decay rate toward uniform per visit (0..1)
        - arbitration_k: sensitivity of arbitration to confidence-uncertainty difference
        - age_shift: positive values bias older adults toward RL and increase lapse
        - lapse_base: baseline lapse probability at set size 3 (0..0.2 recommended)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta_base, wm_decay, arbitration_k, age_shift, lapse_base = model_parameters
    softmax_beta = softmax_beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # RL uncertainty (EWMA of unsigned PE) per state-action or per state; use per state
        u = np.zeros(nS) + 1.0  # start uncertain

        # Set-size and age modulations
        load_term = (nS - 3) / 3.0  # 0 at 3, 1 at 6
        beta_eff = softmax_beta / (1.0 + 0.5 * load_term + 0.5 * age_shift * age_group)
        # Lapse grows with load and age
        lapse = np.clip(lapse_base * (1.0 + load_term) * (1.0 + age_shift * age_group), 0.0, 0.4)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # RL softmax policy
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM confidence: margin between best and second-best WM value
            sorted_W = np.sort(W_s)[::-1]
            conf_wm = sorted_W[0] - sorted_W[1] if nA > 1 else 0.0
            # Arbitration weight via sigmoid
            x = conf_wm - u[s] - load_term - age_shift * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-arbitration_k * x))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and uncertainty update
            pe = r - q[s, a]
            q[s, a] += lr * pe
            # Unsigned PE EWMA; faster update when state is visited
            u[s] = 0.8 * u[s] + 0.2 * np.abs(pe)

            # WM decay toward uniform, then one-shot store on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM gating and global stickiness; age reduces WM capacity.

    Mechanism
    - RL: Q-learning with eligibility traces (lambda) per state-action; softmax with stickiness bias to repeat the
      previous action globally (not state-specific).
    - WM: capacity K selects a fraction of states to be maintained in WM; if the current state is in WM and was rewarded,
      WM supplies a near-deterministic policy; else rely on RL. Capacity effectively scales as K/nS and is reduced in older adults.
    - Set size: larger set size lowers the probability that a state is in WM (K/nS) and reduces effective beta via load.
    - Age (old=1): decreases effective WM capacity and increases stickiness impact.

    Parameters
    ----------
    states : array-like, int
    actions : array-like, int
    rewards : array-like, int
    blocks : array-like, int
    set_sizes : array-like, int
    age : array-like, int
    model_parameters : list or array
        [lr, softmax_beta_base, lambda_trace, stickiness_beta, wm_capacity_K, age_load_penalty]
        - lr: RL learning rate (0..1)
        - softmax_beta_base: baseline RL inverse temperature (scaled internally)
        - lambda_trace: eligibility decay (0..1)
        - stickiness_beta: strength of global choice repetition bias
        - wm_capacity_K: WM capacity in items (0..6)
        - age_load_penalty: reduces WM capacity and increases stickiness for older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta_base, lambda_trace, stickiness_beta, wm_capacity_K, age_load_penalty = model_parameters
    softmax_beta = softmax_beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        # Global last action for stickiness
        last_action = None

        # Effective beta lower with load; stickiness stronger in older adults
        load_scale = 3.0 / nS
        beta_eff = softmax_beta * load_scale
        stick_beta_eff = stickiness_beta * (1.0 + age_load_penalty * age_group)

        # Probability that any given state is within WM set: p_in = min(1, K_eff/nS)
        K_eff = max(0.0, wm_capacity_K - age_load_penalty * age_group * 2.0)  # reduce capacity with age
        p_in_wm = np.clip(K_eff / max(1.0, nS), 0.0, 1.0)

        # Track which states are currently in WM set (stochastically on each visit)
        # Implement by Bernoulli draw per visit via expected mixing weight; here we use expectation directly as weight.
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL preference with stickiness
            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_action is not None:
                bias[last_action] = stick_beta_eff
            Q_bias = Q_s + bias
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_bias - Q_bias[a])))

            # WM policy if the state has a stored correct response
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight equals probability state is in WM set
            wm_weight = p_in_wm

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Update eligibility traces: replace traces for chosen, decay others
            e *= lambda_trace
            e[s, a] = 1.0

            # TD error with state-action value
            delta = r - q[s, a]
            # Update all Q with eligibility traces
            q += lr * delta * e

            # WM update: store mapping only on reward; otherwise keep as is
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # small passive decay toward uniform so that WM doesn't become stale
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

            # Update last action for stickiness
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p