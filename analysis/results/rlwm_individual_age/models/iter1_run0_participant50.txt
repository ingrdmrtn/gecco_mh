def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with asymmetric learning and set-size/age-modulated WM engagement.

    Idea
    - Choices combine a model-free RL policy and a working-memory (WM) policy.
    - WM contribution shrinks with larger set size and more for older adults.
    - RL has asymmetric learning from positive vs negative outcomes.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr_pos, lr_neg, beta_base, wm_weight_base, age_shift_wm, decay_base]
        - lr_pos: RL learning rate after reward (0..1).
        - lr_neg: RL learning rate after no reward (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_weight_base: baseline WM mixture weight (0..1).
        - age_shift_wm: reduction of WM weight for older group (>=0 reduces WM when age_group=1).
        - decay_base: baseline WM decay strength per trial; increases with set size.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, beta_base, wm_weight_base, age_shift_wm, decay_base = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL temperature (kept constant across set sizes here; WM absorbs set-size effect)
        softmax_beta = beta_base * 10.0

        # WM mixture weight reduced by set size and further by age (older -> less WM reliance)
        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6
        wm_weight = wm_weight_base - ss_penalty * decay_base - age_group * abs(age_shift_wm)
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        # WM decay increases with set size
        wm_decay = float(np.clip(decay_base * ss_penalty, 0.0, 1.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        softmax_beta_wm = 50.0  # near-deterministic WM

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            lr = lr_pos if r > 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay towards uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # WM one-shot storage on rewarded trials
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM gating + RL with directed exploration and perseveration.

    Idea
    - WM acts like a gated store: mapping is stored on rewarded trials with probability
      proportional to capacity. If a state's mapping is stored, WM determines choice; otherwise RL guides choice.
    - Effective WM capacity declines with older age and is stressed at larger set sizes.
    - RL includes uncertainty-driven exploration bonus and action perseveration.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [lr, beta_base, K_base, age_cap_drop, perseveration, exploration]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature; scaled internally (*10)
        - K_base: baseline WM capacity (in items, e.g., around 3)
        - age_cap_drop: capacity reduction for older adults (>=0)
        - perseveration: bias weight added to the last chosen action (>=0)
        - exploration: weight on uncertainty bonus (>=0)

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, beta_base, K_base, age_cap_drop, perseveration, exploration = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0

        # Effective WM capacity after age effect
        K_eff = np.clip(K_base - age_group * abs(age_cap_drop), 1.0, float(nS))

        # Probability to successfully store/retain a mapping upon reward
        p_store = float(np.clip(K_eff / max(1.0, float(nS)), 0.0, 1.0))

        # RL value table and counts for exploration
        q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))  # visit counts to define uncertainty bonus
        last_action_bias = np.zeros(nA)  # perseveration bias resets per block

        # WM store: w rows either uniform (not stored) or one-hot (stored)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        softmax_beta_wm = 50.0  # near-deterministic for stored states

        log_p = 0.0
        last_action = None
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with directed exploration and perseveration
            Q_s = q[s, :]
            U_s = exploration * (1.0 / np.sqrt(counts[s, :] + 1.0))  # higher when uncertain
            P_s = np.zeros(nA)
            if last_action is not None:
                P_s[last_action] = perseveration
            pref_rl = softmax_beta * (Q_s + U_s + P_s - np.max(Q_s + U_s + P_s))
            pi_rl = np.exp(pref_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy: if stored, it's sharp; if not, it's uniform (softmax still yields uniform)
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture weight depends on whether state is stored: use the entropy of W_s as a gate
            # Low entropy (near one-hot) -> rely on WM; high entropy (uniform) -> rely on RL.
            entropy = -np.sum(W_s * (np.log(W_s + eps)))
            max_entropy = np.log(nA)
            wm_gate = float(np.clip(1.0 - entropy / max_entropy, 0.0, 1.0))

            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            counts[s, a] += 1.0

            # WM update: on reward, store with probability p_store; otherwise no change
            if r > 0.5:
                if np.random.rand() < p_store:
                    w[s, :] = 0.0
                    w[s, a] = 1.0
            # Small passive forgetting towards uniform when set size is large (implicit capacity stress)
            stress = max(0.0, (nS - K_eff) / max(1.0, nS))
            w = (1.0 - 0.1 * stress) * w + 0.1 * stress * w_0

            last_action = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + noisy WM mixture; set-size reduces WM weight, age increases WM noise and RL forgetting.

    Idea
    - RL values decay towards uniform (forgetting), stronger in larger sets and for older adults.
    - WM contributes with retrieval noise; noise increases with set size and more for older adults.
    - WM weight is reduced as set size increases (load effect).

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [lr, beta_base, wm_weight_base, rl_forget, wm_noise_base, age_noise]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature; scaled internally (*10)
        - wm_weight_base: base WM mixture weight (0..1)
        - rl_forget: base RL forgetting rate per trial (0..1)
        - wm_noise_base: base WM noise term; higher -> less deterministic WM
        - age_noise: additional WM noise for older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, beta_base, wm_weight_base, rl_forget, wm_noise_base, age_noise = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL temperature
        softmax_beta = beta_base * 10.0

        # WM weight diminishes with set size (3 -> full weight, 6 -> ~half if base is 1)
        wm_weight = float(np.clip(wm_weight_base * (3.0 / max(3.0, float(nS))), 0.0, 1.0))

        # RL forgetting increases with set size and age
        forget_eff = float(np.clip(rl_forget * (float(nS) / 6.0) * (1.0 + 0.5 * age_group), 0.0, 1.0))

        # WM retrieval noise increases with set size and age; convert to inverse temperature
        wm_noise = float(np.clip(wm_noise_base + age_group * abs(age_noise) + 0.1 * (nS - 3), 0.0, None))
        softmax_beta_wm = 1.0 / max(wm_noise + 1e-6, 1e-6)  # higher noise -> lower beta
        softmax_beta_wm = float(np.clip(softmax_beta_wm, 1.0, 50.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy with noise
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Global forgetting towards uniform
            q = (1.0 - forget_eff) * q + forget_eff * (1.0 / nA) * np.ones_like(q)

            # WM update: mild decay plus one-shot storage when rewarded
            w = (1.0 - 0.05) * w + 0.05 * w_0
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)