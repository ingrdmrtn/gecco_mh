def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility trace) + WM with set-size/age-dependent leak and temperature.

    Decision policy:
    - Mixture of RL softmax and WM softmax.
    - RL uses an eligibility trace to propagate credit beyond the chosen action.
    - WM stores the last rewarded action per state; its memory trace leaks toward uniform at a
      rate that increases with set size and with age; WM temperature also decreases with noise.

    Parameters (list/tuple of length 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10, lowered in older adults.
    - wm_weight_base: baseline WM mixture weight (logit space).
    - trace_lambda: eligibility trace decay parameter (0..1), larger -> longer-lasting trace.
    - wm_noise_small: baseline WM noise/leak factor at set size 3; scales up with larger set sizes.
    - age_temp_offset: increases exploration in older adults by reducing RL beta (>=0).

    Age and set-size effects:
    - WM leak and WM noise increase with set size (6 > 3) and with age_group.
    - RL inverse temperature is reduced in older participants by a factor (1 + age_temp_offset).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, trace_lambda, wm_noise_small, age_temp_offset = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q, WM table, and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-dependent RL temperature adjustment (older -> more exploration)
        beta_rl = softmax_beta / (1.0 + max(0.0, age_temp_offset) * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size and age modulate WM noise/leak
            wm_noise = wm_noise_small * (nS / 3.0) * (1.0 + 0.5 * age_group)
            wm_leak = np.clip(wm_noise, 0.0, 1.0)

            # Leak WM trace toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # RL softmax
            Q_s = q[s, :]
            Q_centered = beta_rl * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(Q_centered))

            # WM softmax with effective lower temperature when noise is high
            W_s = w[s, :]
            beta_wm_eff = softmax_beta_wm / (1.0 + wm_noise)
            W_centered = beta_wm_eff * (W_s - W_s[a])
            p_wm = 1.0 / np.sum(np.exp(W_centered))

            # Mixture weight prefers small set sizes
            wm_weight_logit = wm_weight_base + 0.5 * (3 - nS)
            wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_logit))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with eligibility trace
            pe = r - Q_s[a]
            # Replace trace for current state-action, decay others
            e *= trace_lambda
            e[s, a] = 1.0
            q += lr * pe * e

            # WM update: if rewarded, store one-hot for the chosen action
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay-to-baseline and state-wise perseveration, mixed with WM.

    Decision policy:
    - RL softmax augmented with state-dependent perseveration bias (stickiness to last action in state).
    - RL Q-values decay toward uniform each trial to capture forgetting/interference.
    - WM stores last rewarded action per state; used via mixture.

    Parameters (list/tuple of length 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: baseline WM mixture weight (logit space).
    - rl_decay: per-trial decay of RL values toward uniform (0..1), increases effective forgetting.
    - perseveration_kappa: strength of perseveration bias added to RL logits (>=0).
    - age_wm_bias: age-dependent shift on WM weight (positive values favor WM in younger).

    Age and set-size effects:
    - WM weight is reduced in larger set sizes; age_wm_bias increases WM weighting for younger and
      reduces it for older: wm_weight_logit += age_wm_bias * (1 - 2*age_group).
    - RL decay applies every trial, implicitly more harmful in larger sets due to sparser repeats.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, rl_decay, perseveration_kappa, age_wm_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action_in_state = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL decay toward uniform baseline
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA) * np.ones_like(q)

            # RL policy with perseveration bias in logits
            Q_s = q[s, :]
            logits_rl = softmax_beta * Q_s
            if last_action_in_state[s] >= 0:
                pa = last_action_in_state[s]
                bias_vec = np.zeros(nA)
                bias_vec[pa] = perseveration_kappa
                logits_rl = logits_rl + bias_vec

            # Softmax probability of chosen action without computing full denominator repeatedly
            logits_rl_centered = logits_rl - logits_rl[a]
            p_rl = 1.0 / np.sum(np.exp(-logits_rl_centered))

            # WM leak mild, stronger at large set size and older age
            wm_leak = np.clip(0.05 * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.5)
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # WM policy
            W_s = w[s, :]
            W_centered = softmax_beta_wm * (W_s - W_s[a])
            p_wm = 1.0 / np.sum(np.exp(W_centered))

            # Mixture weight depends on set size and age
            wm_weight_logit = wm_weight_base + 0.5 * (3 - nS) + age_wm_bias * (1 - 2 * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update perseveration memory
            last_action_in_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated RL/WM with probabilistic WM updates and decay.

    Decision policy:
    - Gate determines WM mixture weight from the contrast between WM confidence and RL uncertainty.
    - WM confidence = max probability under WM policy; RL uncertainty = entropy of RL policy.
    - WM updates occur stochastically on rewarded trials; update probability decreases with set size
      and with age. WM traces decay each trial.

    Parameters (list/tuple of length 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_update_prob: baseline probability to write to WM on reward (0..1) at set size 3.
    - wm_decay_base: baseline WM decay (0..1) per visit; increases with set size and age.
    - gate_slope: slope of logistic gate mapping (confidence - uncertainty) to WM weight.
    - age_gate_bias: additive bias on the gate for age (positive favors WM in younger, negative in older).

    Age and set-size effects:
    - Effective WM update prob is scaled by (3/nS) and reduced for older adults.
    - WM decay increases with set size and with age.
    - Gate bias shifts WM use: wm_weight_logit += age_gate_bias * (1 - 2*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_update_prob, wm_decay_base, gate_slope, age_gate_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probabilities for entropy computation
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / np.sum(exp_rl)

            # RL uncertainty: normalized entropy in [0,1]
            entropy_rl = -np.sum(pi_rl * (np.log(pi_rl + eps)))
            max_entropy = np.log(nA)
            rl_uncert = entropy_rl / max_entropy

            # WM decay increases with set size and age
            wm_decay = np.clip(wm_decay_base + 0.1 * (nS - 3) + 0.1 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM policy and confidence
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            wm_conf = np.max(pi_wm)

            # Gate: WM weight from confidence vs uncertainty plus age and set-size biases
            gate_input = gate_slope * (wm_conf - rl_uncert) + 0.4 * (3 - nS) + age_gate_bias * (1 - 2 * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))

            # Choice probability mixture
            p_total = wm_weight * pi_wm[a] + (1.0 - wm_weight) * pi_rl[a]
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Probabilistic WM update on reward; probability reduced by set size and age
            if r > 0.5:
                p_update = np.clip(wm_update_prob * (3.0 / nS) * (1.0 - 0.3 * age_group), 0.0, 1.0)
                # Use deterministic equivalent: expected update strength p_update
                w[s, :] = (1.0 - p_update) * w[s, :]  # retain old
                w[s, a] += p_update * (1.0 - w[s, a])  # push toward one-hot

        blocks_log_p += log_p

    return -blocks_log_p