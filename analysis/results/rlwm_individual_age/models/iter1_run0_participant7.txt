Below are three standalone cognitive models that implement different RLâ€“WM mechanisms and arbitration strategies. Each function:

- Takes inputs: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Uses an RL+WM mixture, with age and set size affecting parameters meaningfully
- Returns the negative log-likelihood of the observed choices
- Uses all provided parameters
- Avoids the exact parameter combinations you tried previously


def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM with decay/interference in WM and load-scaled mixture.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM is a fast store biased by reward, but decays toward uniform and suffers set-size (interference) costs.
    - WM mixture weight scales by set size (3/nS), while WM decay increases with set size and age.
    - RL uses a single learning rate.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight_base: base WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - wm_decay_base: baseline WM decay per trial on a visited state (0..1)
    - interference_k: additional WM decay per extra item beyond 3 (>=0)
    - alpha_store: WM learning rate toward target (0..1)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay_base, interference_k, alpha_store = model_parameters
    softmax_beta *= 10.0  # RL beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # very deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Precompute scales
        load_scale = 3.0 / float(nS)  # down-weights WM under higher load
        # WM decay increases with load and age
        wm_decay_eff = wm_decay_base + interference_k * max(0, nS - 3)
        wm_decay_eff = wm_decay_eff * (1.0 if age_group == 0 else 1.3)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        wm_weight_eff = np.clip(wm_weight_base * load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)

            # WM policy (softmax over WM values)
            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            rpe = r - Q_s[a]
            q[s, a] += lr * rpe

            # WM decay toward uniform on visited state
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * (1.0 / nA)

            # WM update: reward-strengthened one-hot coding
            target = np.ones(nA) / nA  # default target for non-reward: drift to uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] += alpha_store * (target - W_s)
            # Renormalize to a proper distribution (avoid collapse)
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM with confidence-based arbitration (gating).

    Idea:
    - WM and RL produce policies; arbitration favors WM when its confidence is high.
    - WM confidence is the margin between the top two WM action strengths.
    - Gating weight = sigmoid(gamma_gate * (conf_scaled - theta_wm_eff)).
    - Load reduces effective confidence (divide by nS/3). Age increases threshold.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - theta_wm: gating threshold for WM use (can be negative..positive)
    - gamma_gate: slope of the gating sigmoid (>=0)
    - alpha_wm: WM learning rate toward target (0..1)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, theta_wm, gamma_gate, alpha_wm = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Age-modulated threshold: older adults require higher confidence to use WM
        theta_eff = theta_wm + (0.1 if age_group == 1 else 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)

            # WM policy
            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            # WM confidence: margin between top-1 and top-2 WM preferences
            sorted_W = np.sort(W_s)[::-1]
            conf = sorted_W[0] - (sorted_W[1] if nA > 1 else 0.0)

            # Load-reduced confidence
            conf_scaled = conf / (float(nS) / 3.0)

            # Gating weight via sigmoid
            gate = 1.0 / (1.0 + np.exp(-gamma_gate * (conf_scaled - theta_eff)))
            gate = np.clip(gate, 0.0, 1.0)

            p_total = gate * p_wm + (1.0 - gate) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            rpe = r - Q_s[a]
            q[s, a] += lr * rpe

            # WM update: move toward one-hot if rewarded, toward uniform if not
            target = np.ones(nA) / nA
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] += alpha_wm * (target - W_s)
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and choice stickiness, mixed with lightweight WM.

    Idea:
    - RL values decay (forget) toward uniform each time the state is visited.
    - RL policy includes a state-specific stickiness bias to repeat the last action.
    - A lightweight WM stores last rewarded action with strong but load-sensitive decay.
    - Mixture weight for WM scales with load; age increases forgetting and reduces RL temperature.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - wm_weight_base: base WM mixture weight (0..1)
    - forget_rl: RL forgetting rate toward uniform on visited state (0..1)
    - stickiness: added bias for repeating last action in a state (>=0)
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, forget_rl, stickiness = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age reduces RL inverse temperature and increases forgetting
    age_beta_scale = 1.0 if age_group == 0 else 0.85
    age_forget_scale = 1.0 if age_group == 0 else 1.2

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        last_choice = -1 * np.ones(nS, dtype=int)

        # Load-scaled WM mixture
        wm_weight_eff = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)
        # WM decay stronger under higher load
        wm_decay = np.clip(0.2 + 0.2 * max(0, nS - 3), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL logits with stickiness toward last action in this state
            logits_rl = Q_s.copy()
            if last_choice[s] == a:
                # We'll add stickiness to the chosen action's logit by adjusting relative differences
                pass  # handled via softmax trick below

            # Softmax with stickiness: add bias to last chosen action index
            Q_bias = logits_rl.copy()
            if last_choice[s] != -1:
                Q_bias[last_choice[s]] += stickiness

            beta_eff = softmax_beta * age_beta_scale
            Qc = Q_bias - np.max(Q_bias)
            denom_rl = np.sum(np.exp(beta_eff * Qc))
            p_rl = np.exp(beta_eff * Qc[a]) / max(1e-12, denom_rl)

            # WM policy: heavy weight on last rewarded action, decays otherwise
            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL forgetting toward uniform on visited state
            forget_eff = np.clip(forget_rl * age_forget_scale, 0.0, 1.0)
            q[s, :] = (1.0 - forget_eff) * q[s, :] + forget_eff * (1.0 / nA)

            # RL update
            rpe = r - Q_s[a]
            q[s, a] += lr * rpe

            # WM decay toward uniform on visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)

            # WM update: set to one-hot if rewarded
            if r > 0.5:
                w[s, :] = np.zeros(nA)
                w[s, a] = 1.0
            else:
                # No explicit anti-learning beyond decay
                pass

            # Keep WM normalized and bounded
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

            # Update last choice
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set size effects:
- Model 1: WM mixture weight scales by 3/nS; WM decay increases with set size and is further increased for older adults (age used in wm_decay_eff). RL is unaffected by age here, allowing age differences to express mainly via WM stability.
- Model 2: Arbitration uses a confidence gate; the effective WM confidence is divided by nS/3 (reduced under higher load), and older adults have a higher threshold theta to engage WM.
- Model 3: WM mixture weight scales by 3/nS. Age reduces RL inverse temperature and increases RL forgetting, capturing age-related noisier RL and greater memory decay under load. Stickiness captures habitual repetition independent of WM.