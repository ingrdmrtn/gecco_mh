def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with set-size-dependent WM precision, WM decay, and perseveration.
    - RL: single learning rate, softmax decision.
    - WM: one-shot storage upon reward; subject to decay toward uniform each trial.
    - Set size reduces WM precision (inverse temperature), not its weight.
    - Age increases WM decay (older forget faster).
    - Perseveration/stickiness bias toward the previously chosen action (global).

    Parameters
    ----------
    states : array-like
        State index at each trial (0..set_size-1 for the current block).
    actions : array-like
        Chosen actions at each trial (0..2).
    rewards : array-like
        Binary rewards at each trial (0/1).
    blocks : array-like
        Block index per trial. Within a block, set size is constant.
    set_sizes : array-like
        Set size for the block of each trial (3 or 6).
    age : array-like
        Participant age per trial (constant across trials).
    model_parameters : list/tuple
        [lr, wm_weight, softmax_beta, beta_wm_base, wm_decay_base, perseveration]
        - lr: RL learning rate (0..1)
        - wm_weight: baseline WM mixture weight (0..1), combined with RL
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - beta_wm_base: base WM inverse temperature before set-size scaling
        - wm_decay_base: base WM decay toward uniform per trial (0..1)
        - perseveration: bias added to the chosen action from the previous trial

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, beta_wm_base, wm_decay_base, perseveration = model_parameters
    softmax_beta *= 10  # higher upper bound

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global previous action for perseveration
        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Decay WM toward uniform each trial; age increases decay
            decay_eff = np.clip(wm_decay_base + 0.2 * age_group, 0.0, 1.0)
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add perseveration bias to preferences
            bias = np.zeros(nA)
            if prev_action is not None:
                bias[prev_action] += perseveration

            # RL softmax using Luce trick with bias
            pref_rl = Q_s + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (pref_rl - pref_rl[a])))

            # WM precision decreases with set size (3 -> strong, 6 -> weaker)
            beta_wm = np.maximum(1e-6, beta_wm_base * (3.0 / float(nS)))
            pref_wm = W_s + bias
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (pref_wm - pref_wm[a])))

            # Mixture of WM and RL
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot store correct response upon reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with familiarity-based WM gating and age-modulated decision noise.
    - RL: single learning rate; inverse temperature reduced for older group (more noise).
    - WM: graded memory strength per state that increases with reward and decays each trial.
          WM produces a policy by mixing uniform with a one-hot at the memorized action.
    - Familiarity gating: WM weight decreases as a state is visited more often
      (shift from WM to RL with practice), with set-size scaling.

    Parameters
    ----------
    states : array-like
        State index at each trial (0..set_size-1 for the current block).
    actions : array-like
        Chosen actions at each trial (0..2).
    rewards : array-like
        Binary rewards at each trial (0/1).
    blocks : array-like
        Block index per trial. Within a block, set size is constant.
    set_sizes : array-like
        Set size for the block of each trial (3 or 6).
    age : array-like
        Participant age per trial (constant across trials).
    model_parameters : list/tuple
        [lr, wm_weight0, softmax_beta, rho, familiarity_k, age_noise]
        - lr: RL learning rate (0..1)
        - wm_weight0: baseline WM mixture weight before gating (0..1)
        - softmax_beta: base RL inverse temperature (scaled by 10 internally)
        - rho: WM decay rate of memory strength per trial (0..1)
        - familiarity_k: steepness of the logistic gate from WM to RL as visits increase
        - age_noise: fractional reduction of RL inverse temperature for older group (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, rho, familiarity_k, age_noise = model_parameters
    softmax_beta *= 10  # higher upper bound

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Memory strength per state (0..1), and visit counts
        m_strength = np.zeros(nS)
        visits = np.zeros(nS, dtype=int)

        # Age-modulated RL inverse temperature
        beta_rl = softmax_beta * (1.0 - age_noise * age_group)
        beta_rl = max(beta_rl, 1e-6)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Update visit count first for gating computation
            visits[s] += 1

            # WM memory strength decays each trial
            m_strength = (1.0 - rho) * m_strength

            Q_s = q[s, :].copy()

            # Construct WM distribution as mix of uniform and one-hot at the currently best stored action
            # Identify WM preferred action as argmax of row w[s]
            wm_pref = int(np.argmax(w[s, :]))
            W_s = (1.0 - m_strength[s]) * w_0[s, :] + m_strength[s] * np.eye(3)[wm_pref]

            # RL choice probability
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # WM choice probability with high precision (use a fixed high beta)
            beta_wm = 30.0
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Familiarity-based WM gating: more visits -> more RL, scaled by set size
            # Gate in (0,1): starts higher for low visits, decays with visits and more for larger set size
            v = visits[s]
            gate = 1.0 / (1.0 + np.exp(familiarity_k * (v - (nS / 2.0))))
            # Baseline WM weight adjusted by set size and age (older rely less on WM)
            wm_base = wm_weight0 * (3.0 / float(nS)) * (1.0 - 0.3 * age_group)
            wm_weight_eff = np.clip(wm_base * gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: increase memory strength with reward and align w[s] to the chosen action
            if r > 0.5:
                # Update WM store to one-hot for the rewarded action
                w[s, :] = 0.0
                w[s, a] = 1.0
                # Increase memory strength toward 1
                m_strength[s] = m_strength[s] + (1.0 - m_strength[s]) * 1.0  # one-shot strengthening

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with slot-limited WM and interference.
    - RL: single learning rate; standard softmax.
    - WM: when a state is stored, it is near-deterministic, but suffers interference that mixes in uniform noise.
    - Slot capacity K: probability the current state is in WM is min(1, K / set_size).
      We integrate this probability analytically (no sampling).
    - Interference increases with set size and for older adults.

    Parameters
    ----------
    states : array-like
        State index at each trial (0..set_size-1 for the current block).
    actions : array-like
        Chosen actions at each trial (0..2).
    rewards : array-like
        Binary rewards at each trial (0/1).
    blocks : array-like
        Block index per trial. Within a block, set size is constant.
    set_sizes : array-like
        Set size for the block of each trial (3 or 6).
    age : array-like
        Participant age per trial (constant across trials).
    model_parameters : list/tuple
        [lr, wm_weight, softmax_beta, K_base, xi_base, gamma_set]
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight for WM before slot/interference adjustments (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - K_base: baseline WM slot capacity (in items, e.g., around 3â€“4)
        - xi_base: base WM interference (0..1), mixing weight of uniform in WM policy
        - gamma_set: additional interference per extra set-size unit beyond 3

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K_base, xi_base, gamma_set = model_parameters
    softmax_beta *= 10  # higher upper bound

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and interference
        K_eff = max(0.0, K_base - 0.5 * age_group)  # fewer slots for older group
        # Probability that current state is in WM
        p_slot = np.clip(K_eff / float(nS), 0.0, 1.0)

        # Interference increases with set size and age
        xi_eff = xi_base + gamma_set * max(0, nS - 3) + 0.1 * age_group
        xi_eff = np.clip(xi_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s_raw = w[s, :].copy()

            # RL choice probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference: mix one-hot with uniform
            # Identify preferred WM action (argmax in w row)
            wm_pref = int(np.argmax(W_s_raw))
            one_hot = np.eye(nA)[wm_pref]
            W_s = (1.0 - xi_eff) * one_hot + xi_eff * w_0[s, :]

            # Choice probability from WM (high precision)
            beta_wm = 40.0
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Incorporate probability that the current state is actually in WM slots
            # If not in WM, policy defaults to uniform (1/nA)
            p_wm_present = p_wm
            p_wm_absent = 1.0 / nA
            p_wm_integrated = p_slot * p_wm_present + (1.0 - p_slot) * p_wm_absent

            # Final mixture with RL
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm_integrated + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store rewarded action as one-hot
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p