def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited working memory and action perseveration.

    Idea
    - WM acts like a limited number of "slots" that can store perfectly learned
      mappings (written on reward=1). If the current block's set size exceeds
      the slot count, the probability that a given state is in WM decreases
      proportionally (slots/nS). The effective WM influence scales with this
      probability.
    - RL learns via a delta rule.
    - Action perseveration (choice kernel) biases choices toward recently chosen
      actions, independent of state; this competes with RL and WM.
    - Age affects the number of WM slots: younger and older groups have
      different effective slot counts.

    Parameters (6)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - wm_weight_base: baseline weight of WM in the mixture (0..1)
    - slots_y: WM slot capacity for the young group (>=1)
    - slots_o: WM slot capacity for the old group (>=1)
    - perseveration_weight: strength of action perseveration bias (>=0)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, slots_y, slots_o, perseveration_weight = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    slots = max(1.0, slots_y) if age_group == 0 else max(1.0, slots_o)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: start uniform; when rewarded, write one-hot
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Action perseveration kernel (global across states)
        choice_bias = np.zeros(nA)
        bias_decay = 0.8  # fixed decay across trials

        # Probability that a given state is stored in WM given capacity
        p_cap = min(1.0, float(slots) / float(nS))
        wm_eff_base = max(0.0, min(1.0, wm_weight_base)) * p_cap

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with perseveration bias
            logits = softmax_beta * (Q_s - np.max(Q_s)) + perseveration_weight * choice_bias
            expQ = np.exp(logits - np.max(logits))
            p_rl_vec = expQ / np.sum(expQ) if np.sum(expQ) > 0 else np.ones(nA) / nA
            p_rl = p_rl_vec[a]

            # WM policy: near-deterministic readout of W
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW) if np.sum(expW) > 0 else np.ones(nA) / nA
            p_wm = p_wm_vec[a]

            # Mixture: WM influence scaled by capacity-limited availability
            wm_eff = wm_eff_base
            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: store one-shot on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update action perseveration kernel
            choice_bias *= bias_decay
            choice_bias[a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning, WM with set-size interference, and directed exploration.

    Idea
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM is a fast one-shot store on reward, but decays toward uniform via interference
      that grows with set size. WM mixture weight also downscales with set size.
    - Directed exploration bonus favors less-tried actions in a state; this bonus is
      attenuated in the older group.

    Parameters (6)
    - alpha_pos: RL learning rate for positive PE (0..1)
    - alpha_neg: RL learning rate for negative PE (0..1)
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - wm_weight: base WM mixture weight (0..1)
    - rho_interference: WM interference strength with set size (>=0)
    - gamma_explore: strength of directed exploration (>=0); reduced by 50% if old

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight, rho_interference, gamma_explore = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    # Older participants explore less: attenuate exploration bonus
    gamma_eff_age = gamma_explore * (1.0 if age_group == 0 else 0.5)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store and baseline
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for directed exploration (state-action)
        counts = np.zeros((nS, nA)) + 1e-6  # avoid division by zero

        # Set-size dependent WM mixture down-weighting
        wm_eff_base = max(0.0, min(1.0, wm_weight)) * (3.0 / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Directed exploration bonus: prefer low-count actions within the state
            # Use uncertainty bonus ~ 1/sqrt(count)
            bonus = gamma_eff_age / np.sqrt(counts[s, :] + 1e-12)

            # RL policy with exploration bonus
            logits = softmax_beta * (Q_s - np.max(Q_s)) + bonus
            expQ = np.exp(logits - np.max(logits))
            p_rl_vec = expQ / np.sum(expQ) if np.sum(expQ) > 0 else np.ones(nA) / nA
            p_rl = p_rl_vec[a]

            # WM policy: near-deterministic readout
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW) if np.sum(expW) > 0 else np.ones(nA) / nA
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_eff_base * p_wm + (1.0 - wm_eff_base) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM interference: decay toward uniform with set-size-dependent rate
            # Interference grows from 0 at nS=3 to rho_interference at nS=6
            interference = max(0.0, min(1.0, rho_interference)) * ((float(nS) - 3.0) / 3.0)
            decay = max(0.0, min(1.0, 1.0 - interference))
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            # WM write on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update counts after observing action
            counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with meta-control gating via logistic function of set size and age.

    Idea
    - RL: standard delta-rule learning.
    - WM: fast one-shot learning on reward; decays toward uniform at rate phi_wm.
    - The mixture weight of WM vs RL is not fixed; it is determined by a logistic
      gating function that depends on set size and age group:
        wm_eff = sigmoid(k0 + k_size * (3/nS) + k_age * age_sign)
      where age_sign = +1 for young, -1 for old.
      This allows flexible, data-driven meta-control over arbitration.

    Parameters (6)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - k0: intercept for WM gating (real)
    - k_size: effect of set size on WM gating (real)
    - k_age: age effect on WM gating (real; positive increases WM in young, decreases in old)
    - phi_wm: WM decay parameter toward uniform per visit to a state (0..1)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, k0, k_size, k_age, phi_wm = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    age_sign = 1.0 if age_group == 0 else -1.0

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store and baseline
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute block-wise WM gating via logistic
        x = k0 + k_size * (3.0 / float(nS)) + k_age * age_sign
        wm_eff_block = 1.0 / (1.0 + np.exp(-x))
        wm_eff_block = max(0.0, min(1.0, wm_eff_block))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ) if np.sum(expQ) > 0 else np.ones(nA) / nA
            p_rl = p_rl_vec[a]

            # WM policy
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW) if np.sum(expW) > 0 else np.ones(nA) / nA
            p_wm = p_wm_vec[a]

            # Mixture with meta-control gating
            p_total = wm_eff_block * p_wm + (1.0 - wm_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform for the visited state, then overwrite on reward
            w[s, :] = (1.0 - phi_wm) * w[s, :] + phi_wm * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p