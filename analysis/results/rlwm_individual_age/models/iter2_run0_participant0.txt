Below are three alternative cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways. Each model returns the negative log-likelihood of the observed choices and uses age and set-size information meaningfully.

Note: Assume numpy is available as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + slot-like WM with capacity- and decay-limited encoding.

    Mechanism
    - RL: tabular Q-learning with softmax.
    - WM: slot-like associative memory per state with decay toward uniform and reward-driven encoding.
      Encoding strength is limited by an effective capacity that scales with set size and age.
    - Arbitration: fixed WM weight scaled by the encoding probability (higher when capacity >= set size).

    Parameters (model_parameters)
    - alpha: RL learning rate in [0,1]
    - beta: RL inverse temperature (scaled internally by 10)
    - wm_weight_base: base mixture weight of WM policy, passed through a sigmoid to [0,1]
    - wm_capacity: available WM slots (0-6); higher means more reliable WM encoding
    - wm_decay: decay rate per trial toward uniform in [0,1]
    - age_penalty: slot reduction applied for older adults (age_group=1); effective K = max(0, wm_capacity - age_group*age_penalty)

    Age and set size usage
    - Effective encoding probability p_enc = min(1, K_eff / nS), where K_eff reduces with age for older adults.
    - WM mixture weight is wm_weight_base * p_enc, so WM contributes more when set size is small relative to capacity and for younger participants.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_weight_base, wm_capacity, wm_decay, age_penalty = model_parameters
    beta *= 10.0
    nA = 3
    softmax_beta_wm = 50.0
    eps = 1e-12

    # Age group: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1

    # Sigmoid-squash the base WM weight
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    wm_weight_base = sigmoid(wm_weight_base)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and encoding probability
        K_eff = max(0.0, wm_capacity - age_group * age_penalty)
        p_enc = min(1.0, K_eff / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy: probability of chosen action via softmax trick
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Arbitration: WM weight scaled by encoding probability (capacity vs set size and age)
            wm_weight = wm_weight_base * p_enc
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]

            # WM encoding/update
            # If rewarded: move WM toward one-hot of chosen action with strength p_enc
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * one_hot
            else:
                # If not rewarded: reduce weight on chosen action and redistribute to others
                old_a_val = w[s, a]
                decrease = p_enc * old_a_val
                w[s, a] = old_a_val - decrease
                incr_each = decrease / float(nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += incr_each

            # Normalize to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with lapse and WM with interference, no explicit wm_weight parameter (arbitration by WM strength).

    Mechanism
    - RL: tabular Q-learning with softmax.
    - WM: error-driven Hebbian memory per state; reward pulls toward chosen action, otherwise interference
      pulls toward uniform. Interference scales with set size; WM policy becomes noisier with interference.
    - Arbitration: dynamic mixture weight equals the current WM state's confidence (max(W_s)), so stronger WM
      states dominate more.
    - Lapse: a state-, age-, and set-size-dependent lapse epsilon mixing in a uniform policy.

    Parameters (model_parameters)
    - alpha: RL learning rate in [0,1]
    - beta: RL inverse temperature (scaled internally by 10)
    - wm_eta: WM learning rate toward rewarded action in [0,1]
    - interference: base interference rate toward uniform per non-reward, scaled by set size (>=0)
    - base_epsilon: base lapse propensity (real), passed through sigmoid to [0,1]
    - age_epsilon_add: additive lapse increase for older adults (>=0), scaled by set size

    Age and set size usage
    - Interference grows with set size: inter_eff = interference * (nS / 3).
    - Lapse increases with set size and age: epsilon = sigmoid(base_epsilon) + age_group * age_epsilon_add * (nS / 3).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_eta, interference, base_epsilon, age_epsilon_add = model_parameters
    beta *= 10.0
    nA = 3
    eps = 1e-12
    softmax_beta_wm_base = 30.0

    age_group = 0 if age[0] <= 45 else 1

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    base_epsilon = sigmoid(base_epsilon)
    age_epsilon_add = max(0.0, age_epsilon_add)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        inter_eff = interference * (float(nS) / 3.0)
        epsilon = min(1.0, base_epsilon + age_group * age_epsilon_add * (float(nS) / 3.0))
        beta_wm_eff = softmax_beta_wm_base / (1.0 + inter_eff)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy from current memory
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Arbitration by WM confidence (max probability in WM)
            wm_strength = np.max(W_s)  # in [1/nA, 1]
            p_mix = wm_strength * p_wm + (1.0 - wm_strength) * p_rl

            # Lapse to uniform
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: rewarded pulls toward chosen; otherwise interference toward uniform
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * one_hot
            else:
                w[s, :] = (1.0 - inter_eff) * w[s, :] + inter_eff * w0[s, :]

            # Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with stickiness and WM recency store, arbitrated by confidence with age- and set-size-dependent slope.

    Mechanism
    - RL: tabular Q-learning with softmax plus state-dependent stickiness (perseveration) for the last action in that state.
    - WM: recency memory that stores the last rewarded action per state. It leaks toward uniform
      with a rate that increases with set size and age, and produces a sharp policy when confident.
    - Arbitration: mixture weight is a sigmoid of WM confidence (max prob minus chance), with an
      arbitration slope that decreases with set size and age.

    Parameters (model_parameters)
    - alpha: RL learning rate in [0,1]
    - beta: RL inverse temperature (scaled internally by 10)
    - wm_retrieval_gain: increases WM policy sharpness and reduces leak (>=0)
    - arbitration_slope: base slope for converting confidence to mixture weight (real)
    - stickiness: preference weight for repeating the previous action in a state (real)
    - age_effect: scales both WM leak and arbitration slope for older adults (>=0)

    Age and set size usage
    - Leak increases with set size and age: leak = (nS/6) * (1 + age_group * age_effect) / (1 + wm_retrieval_gain).
    - WM softmax beta decreases with set size and age: beta_wm = 50 * wm_retrieval_gain / (1 + nS/3 + age_group*age_effect).
    - Arbitration slope reduced by set size and age: slope_eff = arbitration_slope / (1 + (nS/3 - 1) + age_group*age_effect).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_retrieval_gain, arbitration_slope, stickiness, age_effect = model_parameters
    beta *= 10.0
    nA = 3
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM recency store
        m = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        leak = (float(nS) / 6.0) * (1.0 + age_group * max(0.0, age_effect)) / (1.0 + max(0.0, wm_retrieval_gain))
        beta_wm = 50.0 * max(0.0, wm_retrieval_gain) / (1.0 + float(nS) / 3.0 + age_group * max(0.0, age_effect))
        slope_eff = arbitration_slope / (1.0 + (float(nS) / 3.0 - 1.0) + age_group * max(0.0, age_effect))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with stickiness
            Q_s = q[s, :].copy()
            if 0 <= last_action[s] < nA:
                prefs = beta * Q_s + stickiness * (np.arange(nA) == last_action[s]).astype(float)
            else:
                prefs = beta * Q_s
            denom_rl = np.sum(np.exp(prefs - prefs[a]))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy from recency memory
            M_s = m[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (M_s - M_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Arbitration: WM confidence above chance
            conf = max(0.0, np.max(M_s) - 1.0 / nA)
            wm_weight = sigmoid(slope_eff * conf)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # WM leak toward uniform
            m[s, :] = (1.0 - leak) * m[s, :] + leak * (1.0 / nA)

            # WM recency update: on reward, store chosen action as most recent correct
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong overwrite modulated by retrieval gain (bounded to [0,1] via 1 - 1/(1+gain))
                overwrite = 1.0 - 1.0 / (1.0 + max(0.0, wm_retrieval_gain))
                m[s, :] = (1.0 - overwrite) * m[s, :] + overwrite * one_hot

            # Update last action for stickiness
            last_action[s] = a

            # Normalize to ensure valid distribution
            row_sum = np.sum(m[s, :])
            if row_sum > 0:
                m[s, :] /= row_sum
            else:
                m[s, :][:] = 1.0 / nA

        blocks_log_p += log_p

    return -blocks_log_p