def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + capacity-limited WM with probabilistic storage; arbitration by effective WM capacity.

    Idea:
    - RL system updates Q-values with separate learning rates for positive/negative prediction errors.
    - WM stores one rewarded action per state when encoding succeeds; retrieval is near-deterministic.
    - Effective WM availability is limited by a capacity K that is reduced by set size and by age group.
    - Arbitration weight equals the probability that the current state is within WM capacity.

    Parameters (6):
    - model_parameters[0] = lr_pos in [0,1]: RL learning rate for positive PE
    - model_parameters[1] = lr_neg in [0,1]: RL learning rate for negative PE
    - model_parameters[2] = softmax_beta (scaled by *10): RL inverse temperature
    - model_parameters[3] = wm_encode_prob in [0,1]: probability to encode the chosen action into WM on rewarded trials
    - model_parameters[4] = K_base in [0,6]: baseline WM capacity (number of stateâ€“action pairs)
    - model_parameters[5] = age_K_drop in [0, K_base]: capacity reduction applied if age_group=1 (older adults)

    Age group:
    - age_group = 0 if age <= 45 (younger), otherwise 1 (older).
    - Capacity is reduced by age_K_drop for older adults.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_encode_prob, K_base, age_K_drop = model_parameters

    softmax_beta *= 10.0
    lr_pos = min(max(lr_pos, 0.0), 1.0)
    lr_neg = min(max(lr_neg, 0.0), 1.0)
    wm_encode_prob = min(max(wm_encode_prob, 0.0), 1.0)
    K_base = min(max(K_base, 0.0), 6.0)
    age_K_drop = min(max(age_K_drop, 0.0), 6.0)

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    nA = 3
    softmax_beta_wm = 50.0  # near-deterministic WM retrieval
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM content: per state a one-hot template (or uniform if unknown)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        # Effective capacity (expected probability that a random state is within WM)
        # Capacity reduced by set size pressure and by age for older adults.
        K_eff = max(0.0, K_base - age_group * age_K_drop)
        # Probability that current state is resident in WM given capacity and set size:
        # Approximate as K_eff / nS, capped to [0,1].
        wm_state_prob = min(1.0, K_eff / max(1, nS))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (near-deterministic softmax over WM vector)
            W_s = w[s, :]
            W_s = W_s / max(np.sum(W_s), eps)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration weight equals probability that the state is represented in WM
            wm_weight = wm_state_prob

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM updating: encode only on rewarded trials with probability wm_encode_prob
            # If encoded, store a one-hot vector for chosen action; otherwise keep prior WM.
            if r > 0.0:
                if np.random.rand() < wm_encode_prob:
                    w[s, :] = 0.0
                    w[s, a] = 1.0
            # No explicit decay; capacity limitation is reflected in reduced wm_weight via K_eff.

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + graded WM strength with uncertainty-based arbitration; set size inflates WM noise and age reduces RL temperature.

    Idea:
    - RL updates Q-values with a single learning rate and softmax policy.
    - WM holds a graded distribution over actions per state that decays toward uniform and learns from rewarded trials.
    - WM learning rate decreases with set size (more interference -> noisier WM).
    - Arbitration weight grows with RL uncertainty (entropy) and WM certainty (peakedness).
    - Older age reduces RL inverse temperature (more choice noise).

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (scaled by *10): RL inverse temperature baseline
    - model_parameters[2] = wm_alpha_base in [0,1]: base WM learning rate on rewarded trials
    - model_parameters[3] = wm_decay in [0,1]: per-trial decay of WM toward uniform
    - model_parameters[4] = setsize_noise_gain >= 0: larger set sizes reduce effective wm_alpha via 1/(1+gain*(nS-3))
    - model_parameters[5] = age_beta_drop in [0,1]: proportional drop in beta for older adults

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_alpha_base, wm_decay, setsize_noise_gain, age_beta_drop = model_parameters

    softmax_beta *= 10.0
    lr = min(max(lr, 0.0), 1.0)
    wm_alpha_base = min(max(wm_alpha_base, 0.0), 1.0)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    setsize_noise_gain = max(0.0, setsize_noise_gain)
    age_beta_drop = min(max(age_beta_drop, 0.0), 1.0)

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    nA = 3
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Age reduces effective RL beta
        beta_eff = softmax_beta * (1.0 - age_beta_drop * age_group)
        beta_eff = max(beta_eff, 1e-3)

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM distributions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # RL uncertainty via entropy of softmax policy
            pi_rl = np.exp(beta_eff * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / max(np.sum(pi_rl), eps)
            H_rl = -np.sum(pi_rl * np.log(np.maximum(pi_rl, eps))) / np.log(nA)  # normalized to [0,1]

            # WM policy
            W_s = w[s, :].copy()
            W_s = W_s / max(np.sum(W_s), eps)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM certainty (0 = uniform, 1 = one-hot)
            c_wm = (np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA)
            c_wm = min(max(c_wm, 0.0), 1.0)

            # Arbitration: weight WM more when RL is uncertain and WM is certain
            wm_weight = c_wm * H_rl
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning from rewarded outcomes; reduced by set size
            if r > 0.0:
                scale = 1.0 + setsize_noise_gain * max(nS_t - 3, 0)
                alpha_wm = wm_alpha_base / scale
                alpha_wm = min(max(alpha_wm, 0.0), 1.0)
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with PE-gated arbitration and set-size-dependent perseveration bias.

    Idea:
    - RL updates Q-values; WM stores rewarded actions per state (one-hot) with slight decay.
    - An arbitration weight g_t is updated online by prediction-error-driven gating:
      increases when outcomes are better than expected, decreases otherwise.
      Larger set sizes suppress the gate per trial; older age reduces initial gate.
    - Additionally, a perseveration kernel favors repeating the previous action,
      and its strength increases with set size (more load -> more habit/stickiness).

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (scaled by *10): RL inverse temperature
    - model_parameters[2] = gate_lr in [0,1]: learning rate for arbitration gate updates
    - model_parameters[3] = wm_max_weight in [0,1]: initial/max WM reliance (baseline for gate)
    - model_parameters[4] = age_gate_drop in [0,1]: multiplicative reduction of initial gate for older adults
    - model_parameters[5] = persev_gain >= 0: scales stickiness with set size

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, gate_lr, wm_max_weight, age_gate_drop, persev_gain = model_parameters

    softmax_beta *= 10.0
    lr = min(max(lr, 0.0), 1.0)
    gate_lr = min(max(gate_lr, 0.0), 1.0)
    wm_max_weight = min(max(wm_max_weight, 0.0), 1.0)
    age_gate_drop = min(max(age_gate_drop, 0.0), 1.0)
    persev_gain = max(persev_gain, 0.0)

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    nA = 3
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM one-hot store + slight decay toward uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        wm_decay = 0.05  # small fixed decay per trial

        # Arbitration gate initialization with age penalty
        g = wm_max_weight * (1.0 - age_gate_drop * age_group)
        g = min(max(g, 0.0), 1.0)

        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :].copy()
            W_s = W_s / max(np.sum(W_s), eps)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Base mixture
            p_mix = g * p_wm + (1.0 - g) * p_rl
            p_mix = max(p_mix, eps)

            # Perseveration kernel (probability of repeating previous action)
            if prev_action is None:
                p_total = p_mix
            else:
                k = 1.0 / (1.0 + np.exp(-persev_gain * max(nS_t - 2, 0)))  # increases with set size; ~0 at low set size
                # Build perseveration distribution
                pi_persev = np.ones(nA) / nA
                pi_persev[prev_action] += 1.0  # emphasize previous action
                pi_persev = pi_persev / np.sum(pi_persev)

                # We only need probability of chosen action a under the blended distribution
                # p_final = (1-k)*p_mix + k*pi_persev[a]
                p_final = (1.0 - k) * p_mix + k * pi_persev[a]
                p_total = max(p_final, eps)

            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay and reward-based overwriting
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update arbitration gate by PE-gating, with set-size penalty
            # larger set sizes reduce g via a small subtractive term
            expected = np.max(Q_s)  # proxy for expected value in state
            gate_update = gate_lr * (r - expected)
            size_penalty = gate_lr * 0.1 * max(nS_t - 3, 0)  # suppress g under high load
            g = g + gate_update - size_penalty
            g = min(max(g, 0.0), 1.0)

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p