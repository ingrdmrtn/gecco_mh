def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory with reward-gated storage and eviction.

    Mechanism:
    - Model-free RL (single learning rate) drives a softmax policy.
    - A capacity-limited WM stores near-deterministic action policies for recently rewarded state-action pairs.
    - WM stores a state when it receives reward; if storage exceeds capacity, evict the oldest stored state (FIFO).
    - Arbitration: mixture of WM and RL policies with a fixed WM weight.
    - Capacity decreases with set size and with age (older => lower effective capacity).

    Parameters (list; length 5):
    - lr: RL learning rate in (0,1).
    - wm_weight: mixture weight of WM in (0,1).
    - softmax_beta: base inverse temperature for RL; internally scaled by 10.
    - wm_capacity_base: baseline WM capacity in slots (positive real; effective capacity is bounded by set size).
    - wm_gate_threshold: reward threshold in [0,1] to store into WM (e.g., 0.5 means store only when rewarded).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity_base, wm_gate_threshold = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM readout
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM policy table

        # Capacity effective: bounded by set size; age reduces effective capacity
        cap_eff = max(0.0, wm_capacity_base - 0.75 * age_group)
        cap_eff = min(nS, cap_eff)
        # Track which states are currently in WM and an eviction queue
        in_wm = np.zeros(nS, dtype=bool)
        fifo_queue = []

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if state is in WM, read softmax over WM row; else uniform
            if in_wm[s]:
                wm_prefs = softmax_beta_wm * W_s
                wm_prefs -= np.max(wm_prefs)
                exp_wm = np.exp(wm_prefs)
                p_wm_vec = exp_wm / np.sum(exp_wm)
                p_wm = p_wm_vec[a]
            else:
                p_wm = 1.0 / nA

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated storage with capacity limit
            if r >= wm_gate_threshold:
                # Make WM for this state more deterministic on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target
                if not in_wm[s]:
                    in_wm[s] = True
                    fifo_queue.append(s)
                    # Evict if over capacity
                    while cap_eff < np.sum(in_wm):
                        evict = fifo_queue.pop(0)
                        in_wm[evict] = False
                        w[evict, :] = (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WSLS heuristic as Working Memory.

    Mechanism:
    - Model-free RL with softmax.
    - WM implements a win-stay/lose-shift heuristic on a per-state basis:
      - If last experience with the state was rewarded, repeat that action with probability p_win.
      - If last experience was not rewarded, shift away from the last action with probability p_lose,
        distributing probability evenly among the other actions.
      - If no prior info for the state, WM is uniform.
    - Arbitration: mixture of WM and RL policies.
    - Age reduces the strength of WSLS (older => smaller p_win and p_lose).

    Parameters (list; length 6):
    - lr: RL learning rate in (0,1).
    - wm_weight: mixture weight of WM in (0,1).
    - softmax_beta: base inverse temperature for RL; internally scaled by 10.
    - p_win: base probability to stay after win in (0,1).
    - p_lose: base probability to shift after loss in (0,1).
    - age_wsls_penalty: factor in [0,1] by which age reduces p_win and p_lose (0 = no age effect).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, p_win, p_lose, age_wsls_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # not strictly used here, WM is constructed directly
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # not used directly; WM policy is derived from last outcome

        # Memory of last action and last outcome per state
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Age-modulated WSLS parameters
        p_win_eff = np.clip(p_win * (1.0 - age_group * age_wsls_penalty), 0.0, 1.0)
        p_lose_eff = np.clip(p_lose * (1.0 - age_group * age_wsls_penalty), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL softmax prob for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM: WSLS policy construction for state s
            if last_action[s] == -1:
                # No prior info
                p_wm_vec = np.ones(nA) / nA
            else:
                la = last_action[s]
                if last_reward[s] >= 0.5:
                    # Win-stay
                    p_wm_vec = np.ones(nA) * ((1.0 - p_win_eff) / (nA - 1))
                    p_wm_vec[la] = p_win_eff
                else:
                    # Lose-shift: decrease last action, increase others
                    p_wm_vec = np.ones(nA) * (p_lose_eff / (nA - 1))
                    p_wm_vec[la] = 1.0 - p_lose_eff
            p_wm_vec = np.clip(p_wm_vec, 1e-8, 1.0)
            p_wm_vec /= np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update WM trace (last action and last reward)
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying probabilistic WM with entropy-based arbitration.

    Mechanism:
    - Model-free RL with softmax.
    - WM is a probabilistic policy table that is pushed toward one-hot on rewarded actions and otherwise decays toward uniform.
    - Arbitration weight depends on the relative uncertainty (entropy) of RL vs WM policies:
        wm_weight_eff = sigmoid(arb_gain * (H_rl - H_wm)), so WM dominates when it is more certain (lower entropy).
      Age reduces arbitration gain (older => flatter arbitration). Larger set sizes increase WM decay (worse memory).
    - The provided wm_weight parameter serves as a base bias toward WM in the sigmoid.

    Parameters (list; length 6):
    - lr: RL learning rate in (0,1).
    - wm_weight: base bias in the arbitration (logit-space offset; positive favors WM).
    - softmax_beta: base inverse temperature for RL; internally scaled by 10.
    - wm_decay: base per-trial WM decay in (0,1).
    - wm_precision: magnitude of WM update toward one-hot on reward (>=0).
    - arb_gain: base gain of the entropy-based arbitration (>0); age reduces this gain.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_precision, arb_gain = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # WM readout when used
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age effects on WM decay and arbitration gain
        wm_decay_eff = np.clip(wm_decay * (nS / 3.0), 0.0, 1.0)
        arb_gain_eff = max(1e-6, arb_gain * (1.0 - 0.4 * age_group))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax prob for chosen action a and full policy for entropy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            p_rl_vec = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / denom_rl
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax policy for entropy/choice
            wm_prefs = softmax_beta_wm * W_s
            wm_prefs -= np.max(wm_prefs)
            exp_wm = np.exp(wm_prefs)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Entropies
            H_rl = -np.sum(np.clip(p_rl_vec, 1e-12, 1.0) * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(np.clip(p_wm_vec, 1e-12, 1.0) * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))

            # Arbitration weight: sigmoid of (bias + gain * entropy difference)
            logit_w = wm_weight + arb_gain_eff * (H_rl - H_wm)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: push toward one-hot on reward, and decay toward uniform each trial
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Precision-weighted blend toward target
                w[s, :] = (1.0 - wm_precision) * w[s, :] + wm_precision * target

            # Global decay toward uniform
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * (1.0 / nA)
            # Row normalization
            w /= np.clip(np.sum(w, axis=1, keepdims=True), 1e-12, None)

        blocks_log_p += log_p

    return -blocks_log_p