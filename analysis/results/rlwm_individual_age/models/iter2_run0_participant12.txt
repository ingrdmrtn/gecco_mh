def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration and capacity moderation.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM mixture weight is modulated by:
      - Set size relative to an internal capacity (capacity parameter).
      - Age group (young=0 has higher WM engagement; old=1 down-weighted).
      - RL state uncertainty: higher RL entropy increases reliance on WM.
    - WM stores action-outcome associations quickly upon reward (episodic-like), without an explicit decay parameter.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Baseline WM mixture weight before modulations (0..1)
    - softmax_beta: RL inverse temperature (>0), multiplied by 10 internally
    - wm_temp: WM softmax temperature (inverse beta); lower -> more deterministic WM
    - capacity: Internal WM capacity in number of state-action pairs (e.g., 3..6)
    - gamma: Slope controlling sensitivity of WM engagement to capacity - set size

    Age and set-size modulation:
    - age_group = 0 if age <= 45 else 1
    - age_factor = 1.0 (young) vs 0.6 (old)
    - size_factor = sigmoid(gamma * (capacity - nS))
    - uncertainty_factor_t = normalized entropy of RL policy at the current state
    - Effective WM weight per trial:
        w_eff_t = wm_weight * age_factor * size_factor * uncertainty_factor_t
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_temp, capacity, gamma = model_parameters
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 1.0 / max(1e-6, wm_temp)  # convert temp to inverse temp

    age_group = 0 if age[0] <= 45 else 1
    age_factor = 1.0 if age_group == 0 else 0.6

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age static components
        size_factor = 1.0 / (1.0 + np.exp(-gamma * (capacity - nS)))
        base_wm = np.clip(wm_weight * age_factor * size_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            # Compute normalized RL policy for entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))
            H_max = np.log(nA)
            uncertainty_factor = H_rl / max(H_max, eps)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise arbitration: more WM when RL is uncertain
            wm_weight_eff = np.clip(base_wm * uncertainty_factor, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = q[s, a] + lr * delta

            # WM update: rewarded associations pushed toward one-hot; otherwise mild normalization
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Fast overwrite toward chosen action
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                # Soft renormalization toward uniform when no reward
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with WM slot capacity and RL decay; WM expressed as a simple lookup with interference.

    Idea:
    - RL learns with learning rate and value decay across trials (to model interference/load).
    - WM stores last rewarded action per state (episodic lookup). WM engagement scales with available
      slots relative to set size and is reduced for older adults.
    - When WM has an entry for a state, it yields a peaked policy; otherwise it is near-uniform.
    - WM mixture weight is scaled by min(1, K/nS) with K determined by age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (>0), multiplied by 10 internally
    - rl_decay: RL value decay toward uniform per trial (0..1)
    - k_base: Base number of WM slots (e.g., 4..6)
    - k_drop: Slots lost if older (>=0); effective K = k_base - age_group*k_drop

    Age and set-size modulation:
    - Effective WM mixture weight = wm_weight * (K / nS clipped to [0,1]).
    - Young participants have K = k_base; older have fewer slots, K = k_base - k_drop.
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rl_decay, k_base, k_drop = model_parameters
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    K = max(0.0, k_base - age_group * k_drop)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Determine WM engagement scaling by capacity vs set size
        size_scale = np.clip(K / max(1.0, nS), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight * size_scale, 0.0, 1.0)

        # A simple WM presence mask per state: 1 if we have a confident association
        wm_has_entry = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            if wm_has_entry[s]:
                # Use stored vector for this state
                W_s = w[s, :]
            else:
                # Near-uniform if no memory
                W_s = w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with decay toward uniform baseline
            delta = r - Q_s[a]
            q[s, a] = q[s, a] + lr * delta
            # Apply global decay toward uniform across actions at this state
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)

            # WM update: write on reward, otherwise slight normalization
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                wm_has_entry[s] = True
            else:
                # If incorrect, gently move toward uniform (interference)
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with prediction-error gated WM usage and set-size penalty.

    Idea:
    - RL learns expected value; WM stores recent rewarded actions per state.
    - Arbitration: WM weight is passed through a logistic gate that increases
      with surprise (prediction error magnitude), decreases with set size,
      and includes an age-dependent bias.
      gate = sigmoid(wm_weight + age_bias*age_group - nS_slope*(nS-3) + pe_sens*|PE|)
    - WM policy is based on reward counts for actions within each state.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Baseline gate bias for WM (can be negative or positive)
    - softmax_beta: RL inverse temperature (>0), multiplied by 10 internally
    - pe_sens: Sensitivity of WM gate to |prediction error| (>=0)
    - nS_slope: Penalty slope for larger set sizes (>=0)
    - age_bias: Additional negative bias for older adults (can be <=0)

    Age and set-size modulation:
    - Older adults (age_group=1) get an additive age_bias in the gate (typically negative).
    - Larger set sizes reduce WM engagement via -nS_slope*(nS-3).
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, pe_sens, nS_slope, age_bias = model_parameters
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Count-based WM store per state-action
        wm_counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy derived from counts (if counts all zero -> uniform)
            if np.sum(wm_counts[s, :]) > 0:
                W_s = wm_counts[s, :] / np.sum(wm_counts[s, :])
            else:
                W_s = w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration gate uses PE magnitude, set size, and age
            pe = r - Q_s[a]
            gate_input = wm_weight + (age_bias * age_group) - (nS_slope * (nS - 3.0)) + (pe_sens * np.abs(pe))
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] = q[s, a] + lr * pe

            # WM updates: increment counts on rewarded choices; slight decay otherwise
            if r > 0.5:
                wm_counts[s, a] += 1.0
            else:
                # Gentle decay of all counts in this state to model interference
                wm_counts[s, :] = 0.9 * wm_counts[s, :]

            # Keep a normalized view in w (not strictly needed for policy since we use counts directly)
            if np.sum(wm_counts[s, :]) > 0:
                w[s, :] = wm_counts[s, :] / np.sum(wm_counts[s, :])
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p