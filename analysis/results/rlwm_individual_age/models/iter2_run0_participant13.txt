Below are three standalone cognitive models that combine reinforcement learning (RL) with working memory (WM) mechanisms, tailored to the RLWM task. Each model returns the negative log-likelihood of the observed choices and uses age group and set size meaningfully but in distinct ways.

Note: Assume numpy as np is already imported. No imports are included inside the functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and decaying one-shot WM.
    - RL: Asymmetric learning rates for positive vs negative outcomes.
    - WM: One-shot storage of rewarded action per state, with set-size-dependent decay toward uniform.
    - Mixture: Fixed WM weight modulated by age (young rely more on WM if age_wm_bonus > 0; old less).
    
    Parameters
    - model_parameters: list or array with 6 parameters
        0) lr_pos: RL learning rate for positive prediction errors (0..1)
        1) lr_neg: RL learning rate for negative prediction errors (0..1)
        2) beta_base: base RL inverse temperature (scaled by 10 internally)
        3) wm_weight_base: base WM mixture weight (0..1)
        4) wm_decay_base: base WM decay strength (>0); effective decay grows with set size
        5) age_wm_bonus: age effect on WM weight; wm_weight = clip(base +/- bonus, 0..1)
           (+bonus for young, -bonus for old)
    
    Returns
    - Negative log-likelihood of observed choices
    """
    lr_pos, lr_neg, beta_base, wm_weight_base, wm_decay_base, age_wm_bonus = model_parameters
    softmax_beta = 10.0 * beta_base  # higher upper bound as per template
    softmax_beta_wm = 50.0           # very deterministic WM policy
    
    age_group = 0 if age[0] <= 45 else 1
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay (larger sets -> stronger decay)
        # Map wm_decay_base (>0) to [0,1) using 1 - exp(-k * size_factor)
        size_factor = max(1.0, nS / 3.0)
        wm_decay = 1.0 - np.exp(-wm_decay_base * size_factor)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        # Age-dependent WM weight
        if age_group == 0:
            wm_weight = np.clip(wm_weight_base + age_wm_bonus, 0.0, 1.0)
        else:
            wm_weight = np.clip(wm_weight_base - age_wm_bonus, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax_det with large beta)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update: decay toward uniform; on reward, one-shot store
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + adaptive WM weighting by confidence, with set-size-dependent WM noise and leak.
    - RL: Standard delta rule with inverse temperature modulated by age (older -> scaled down if age_beta_scale > 0).
    - WM: One-shot store on reward; retrieval noise increases with set size. WM decays toward uniform with set-size-dependent leak.
    - Mixture: Trial-wise WM weight equals wm_weight_max times WM confidence (concentration of WM for the current state).
    
    Parameters
    - model_parameters: list or array with 6 parameters
        0) lr: RL learning rate (0..1)
        1) beta_base: base RL inverse temperature (scaled by 10 internally)
        2) wm_weight_max: maximum WM mixture weight (0..1)
        3) wm_noise: base WM retrieval noise (>0). Effective WM beta = 1 / (wm_noise * (1 + size_wm_penalty*(set_size-3)_+))
        4) size_wm_penalty: scales the impact of set size on WM noise and leak (>0)
        5) age_beta_scale: multiplicative reduction of RL beta for older group; beta *= (1 - age_beta_scale) if old
    
    Returns
    - Negative log-likelihood of observed choices
    """
    lr, beta_base, wm_weight_max, wm_noise, size_wm_penalty, age_beta_scale = model_parameters
    beta_base *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age-modulated RL precision
        age_scale = (1.0 - age_beta_scale) if age_group == 1 else 1.0
        softmax_beta = beta_base * age_scale

        # Set-size-dependent WM beta via retrieval noise
        size_pen = max(0.0, nS - 3.0)
        wm_beta = 1.0 / max(1e-6, wm_noise * (1.0 + size_wm_penalty * size_pen))

        # Set-size-dependent WM leak toward uniform
        wm_leak = 1.0 - np.exp(-size_wm_penalty * max(1.0, nS - 2.0))
        wm_leak = np.clip(wm_leak, 0.0, 1.0)

        # Initialize stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with retrieval noise
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))

            # Adaptive WM weight based on concentration/confidence of WM at state s
            # c ranges from 1/nA to 1; map to [0,1] then scale by wm_weight_max
            c = float(np.max(W_s))
            c0 = 1.0 / nA
            conf = (c - c0) / (1.0 - c0) if c > c0 else 0.0
            wm_weight = np.clip(wm_weight_max * conf, 0.0, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: leak toward uniform; one-shot overwrite on reward
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with novelty bonus and Q forgetting + WM with age-shifted weighting.
    - RL: Standard delta rule with directed exploration (novelty bonus ~ 1/sqrt(visit_count)).
           Q-values also forget toward uniform baseline, with stronger forgetting in larger set sizes.
    - WM: One-shot win-stay with small decay on non-reward; deterministic retrieval.
    - Mixture: WM weight is transformed by age (young > old if age_wm_shift > 0) via a logit shift.
    
    Parameters
    - model_parameters: list or array with 6 parameters
        0) lr: RL learning rate (0..1)
        1) beta_base: base RL inverse temperature (scaled by 10 internally)
        2) wm_weight_base: base WM mixture weight (0..1)
        3) age_wm_shift: logit-domain shift applied to WM weight for young (+shift) and old (-shift)
        4) q_forget: RL forgetting rate toward uniform baseline per state (>0); effective forgetting grows with set size
        5) explore_bonus: directed exploration bonus magnitude (>0) added to less-visited actions
    
    Returns
    - Negative log-likelihood of observed choices
    """
    lr, beta_base, wm_weight_base, age_wm_shift, q_forget, explore_bonus = model_parameters
    softmax_beta = 10.0 * beta_base
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    # Helper: stable logit/sigmoid transform to adjust wm_weight by age
    eps = 1e-6
    base = np.clip(wm_weight_base, eps, 1.0 - eps)
    logit_base = np.log(base / (1.0 - base))
    if age_group == 0:
        wm_weight = 1.0 / (1.0 + np.exp(-(logit_base + age_wm_shift)))
    else:
        wm_weight = 1.0 / (1.0 + np.exp(-(logit_base - age_wm_shift)))
    wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL storage and counts for exploration
        q = (1.0 / nA) * np.ones((nS, nA))
        q0 = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA))  # for novelty bonus

        # WM storage
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-size dependent RL forgetting strength
        rho = 1.0 - np.exp(-q_forget * max(1.0, nS / 3.0))
        rho = np.clip(rho, 0.0, 1.0)

        # Non-reward WM decay dependent on set size
        wm_leak = np.clip(q_forget * (nS / 6.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Directed exploration bonus (less visited actions get higher bonus)
            bonus = explore_bonus / np.sqrt(1.0 + visits[s, :])

            # RL policy with exploration bonus
            Q_eff = q[s, :] + bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy (deterministic)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # Update counts for exploration
            visits[s, a] += 1.0

            # RL forgetting toward uniform for current state
            q[s, :] = (1.0 - rho) * q[s, :] + rho * q0[s, :]

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: slight decay on non-reward; overwrite on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes:
- Model 1 emphasizes asymmetric RL updating and WM decay that scales with set size, with age shifting the reliance on WM.
- Model 2 emphasizes adaptive mixture control: WM receives more weight when its memory for the current state is sharp; set size degrades WM by increasing retrieval noise and leak; age reduces RL precision.
- Model 3 introduces directed exploration and RL forgetting modulated by set size, while age shifts WM reliance via a logit transform.