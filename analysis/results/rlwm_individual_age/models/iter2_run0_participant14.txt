def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility trace + perseveration) mixed with capacity-limited WM.

    Description:
    - Policy is a mixture of model-free RL and a WM store.
    - RL uses an eligibility trace to propagate credit within a block and an action perseveration
      term that biases toward the last action chosen in the same state.
    - WM precision is capacity-limited: a capacity parameter K is scaled by set size and age group
      to determine WM precision (and thus WM policy determinism and storage strength).
    - Arbitration: a fixed WM weight is down/up-scaled by WM precision; larger set sizes reduce,
      and being young (<=45) increases the effective precision.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - wm_weight0: baseline WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled internally by 10)
    - lam: eligibility trace decay (lambda) for RL (0..1)
    - stickiness: perseveration bias toward the last action in this state (>=0)
    - capacity_K: nominal WM capacity (in [0, ~nS]); larger values increase WM precision

    Age and set size usage:
    - Effective WM precision = sigmoid((K_eff / nS) * c), where K_eff increases for young and
      decreases for old; larger nS reduces precision. This precision scales WM determinism and
      storage strength.
    - RL does not directly depend on set size or age, but arbitration does via WM precision.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, wm_weight0, softmax_beta, lam, stickiness, capacity_K = model_parameters
    softmax_beta *= 10.0  # expand RL beta range

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm_base = 1.0  # base, will be scaled by precision per trial/state

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL eligibility traces and perseveration memory
        e = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # -1 indicates none yet

        # Age-adjusted effective capacity
        age_cap_scale = 1.2 if age_group == 0 else 0.8
        K_eff = max(0.0, capacity_K) * age_cap_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute WM precision and beta per trial/state
            # Precision in [0,1] via a smooth sigmoid of K_eff/nS
            prec_raw = (K_eff / float(nS))
            # Use a gain to sharpen the mapping to precision
            precision = 1.0 / (1.0 + np.exp(-4.0 * (prec_raw - 0.5)))
            precision = float(np.clip(precision, 0.0, 1.0))
            beta_wm = softmax_beta_wm_base + 100.0 * precision

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            Q_s = Q_s.astype(float)
            # RL softmax probability of chosen action
            expQ = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_vec = expQ / np.sum(expQ)
            p_rl = float(prl_vec[a])

            # WM policy: softmax over w[s] with high beta scaled by precision
            W_s = w[s, :].astype(float)
            expW = np.exp(beta_wm * (W_s - np.max(W_s)))
            pwm_vec = expW / np.sum(expW)
            p_wm = float(pwm_vec[a])

            # Arbitration weight scaled by precision (set size and age enter via precision)
            wm_weight_eff = np.clip(wm_weight0 * precision, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            delta = r - q[s, a]
            # decay traces
            e *= lam
            # increment trace for chosen pair
            e[s, a] += 1.0
            # update all Q with traces
            q += alpha * delta * e

            # WM decay toward baseline (slower when precision high)
            decay = np.clip(1.0 - precision, 0.0, 1.0) * 0.5
            w = (1.0 - decay) * w + decay * w_0

            # WM storage on reward; strength scales with precision
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                store_gain = precision  # stronger, more precise storage with higher precision
                w[s, :] = (1.0 - store_gain) * w[s, :] + store_gain * one_hot
            else:
                # mild interference when no reward: move partially toward uniform
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy-based arbitration and age-/set-size-modulated temperature.

    Description:
    - Policy is a mixture of RL and WM, but the WM weight is determined online by the
      relative uncertainty (entropy) of RL vs WM policies.
    - RL inverse temperature increases for smaller set sizes and for younger participants.
    - WM forgets over time, with more forgetting in larger set sizes and in older participants.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - softmax_beta0: base RL inverse temperature (scaled internally by 10)
    - wm_gate0: baseline WM gating bias (logit space)
    - wm_forget: base WM forgetting rate per trial (0..1)
    - entropy_sensitivity: scales the effect of H_rl - H_wm on WM gating
    - beta_age_size_gain: scales the increase of RL beta for small set size and youth

    Age and set size usage:
    - RL beta_eff = beta0 * exp(beta_age_size_gain * size_term * age_term), with size_term=3/nS,
      age_term larger for young than old (young > old).
    - WM forgetting is scaled up with set size (nS/3) and with age (old > young).
    - WM gating weight is logistic(wm_gate0 + entropy_sensitivity*(H_rl - H_wm)); more WM when
      RL is uncertain (high H_rl) and WM is confident (low H_wm).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta0, wm_gate0, wm_forget, entropy_sensitivity, beta_age_size_gain = model_parameters
    softmax_beta = softmax_beta0 * 10.0  # base scaling

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # WM policy is near-deterministic given its contents

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute age/size modifiers
        size_term = 3.0 / float(nS)
        age_term = 1.0 if age_group == 0 else 0.6  # young boost beta more

        # Forgetting scales
        forget_scale_size = float(nS) / 3.0
        forget_scale_age = 0.8 if age_group == 0 else 1.2

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL temperature with age/size scaling
            beta_eff = softmax_beta * np.exp(beta_age_size_gain * size_term * age_term)

            # RL policy distribution
            Q_s = q[s, :].astype(float)
            expQ = np.exp(beta_eff * (Q_s - np.max(Q_s)))
            prl_vec = expQ / np.sum(expQ)
            p_rl = float(prl_vec[a])

            # WM policy distribution
            W_s = w[s, :].astype(float)
            expW = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm_vec = expW / np.sum(expW)
            p_wm = float(pwm_vec[a])

            # Entropies
            H_rl = -np.sum(prl_vec * np.log(np.clip(prl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(pwm_vec * np.log(np.clip(pwm_vec, 1e-12, 1.0)))

            # WM gating on logit scale, then logistic
            gate_input = wm_gate0 + entropy_sensitivity * (H_rl - H_wm)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM forgetting scaled by set size and age
            decay = np.clip(wm_forget * forget_scale_size * forget_scale_age, 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # WM storage: rewarded action strengthens memory for this state
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Use a conservative store gain that competes with decay
                store_gain = np.clip(1.0 - decay, 0.0, 1.0)
                w[s, :] = (1.0 - store_gain) * w[s, :] + store_gain * one_hot
            else:
                # On errors, slightly flatten the memory for this state
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce–Hall associability and WM with interference-based precision.

    Description:
    - RL learning rate is modulated by a state-specific associability that tracks recent surprise
      (absolute prediction error). High surprise => higher alpha for that state.
    - WM precision is degraded by an interference parameter that scales with set size and age.
      Larger set sizes and older age increase interference, lowering WM determinism and storage.
    - Arbitration favors WM when associability (uncertainty) is low and RL is confident; otherwise,
      it leans more on RL when associability is high (uncertain).

    Parameters (model_parameters):
    - alpha0: baseline RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_weight0: baseline WM mixture weight (0..1)
    - wm_decay: baseline WM decay toward uniform per trial (0..1)
    - assoc_gain: update rate for associability (0..1)
    - interference: baseline WM interference strength (>=0), increases with set size and age

    Age and set size usage:
    - Associability gates arbitration: wm_weight_eff = wm_weight0 * (1 - A[s]) scaled by age/size,
      so young and small set sizes boost WM influence.
    - WM interference = interference * (nS/3) * age_scale, with age_scale > 1 for older.
      Higher interference reduces WM beta and weakens storage strength.
    - RL temperature uses the provided beta uniformly; the key modulation is via associability.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha0, softmax_beta, wm_weight0, wm_decay, assoc_gain, interference = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    age_interf_scale = 0.9 if age_group == 0 else 1.2  # older suffers more interference

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise associability initialized high (uncertain)
        A = np.ones(nS) * 0.5

        # Interference that degrades WM precision
        interf = max(0.0, interference) * (float(nS) / 3.0) * age_interf_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].astype(float)
            expQ = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_vec = expQ / np.sum(expQ)
            p_rl = float(prl_vec[a])

            # WM policy with interference-limited precision
            W_s = w[s, :].astype(float)
            beta_wm = 50.0 / (1.0 + interf)  # more interference => less deterministic WM
            expW = np.exp(beta_wm * (W_s - np.max(W_s)))
            pwm_vec = expW / np.sum(expW)
            p_wm = float(pwm_vec[a])

            # Arbitration: rely more on WM when associability is low
            # Age and set-size further scale WM weight
            size_weight_scale = 3.0 / float(nS)
            age_weight_scale = 1.15 if age_group == 0 else 0.85
            wm_weight_eff = wm_weight0 * (1.0 - A[s]) * size_weight_scale * age_weight_scale
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce–Hall associability
            delta = r - Q_s[a]
            alpha_s = np.clip(alpha0 * A[s], 0.0, 1.0)
            q[s, a] += alpha_s * delta

            # Update associability toward recent surprise
            A[s] = (1.0 - assoc_gain) * A[s] + assoc_gain * abs(delta)
            A[s] = float(np.clip(A[s], 0.0, 1.0))

            # WM decay toward uniform (scaled by wm_decay) and interference
            decay = np.clip(wm_decay, 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # Rewarded storage with interference-weakened strength
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Storage gain decreases with interference
                store_gain = 1.0 / (1.0 + interf)
                w[s, :] = (1.0 - store_gain) * w[s, :] + store_gain * one_hot
            else:
                # On non-reward, additional flattening from interference
                flatten = np.clip(0.3 * (1.0 + interf), 0.0, 1.0)
                w[s, :] = (1.0 - flatten) * w[s, :] + flatten * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p