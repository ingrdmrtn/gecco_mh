def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and load-dependent WM noise and signed RL learning rates.

    Idea:
    - Choices are a mixture of an RL softmax policy and a WM softmax policy.
    - WM acts like a fast look-up table that stores rewarded associations, but is noisy.
    - WM noise increases with set size and age; this effectively lowers the WM inverse temperature.
    - RL uses separate learning rates for positive vs negative prediction errors.
    
    Parameters (list):
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature baseline (scaled by 10 inside)
    - wm_noise_base: baseline WM noise scaling (>=0), increased by set size and age
    
    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (3 or 6)
    - age: array with a single repeated age value
    - model_parameters: list as above
    
    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_noise_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :]
            # RL policy (full softmax to also compute entropy if needed later)
            q_shift = Q_s - np.max(Q_s)
            exp_q = np.exp(softmax_beta * q_shift)
            pi_rl = exp_q / np.sum(exp_q)
            p_rl = max(pi_rl[a], eps)

            # WM policy with load- and age-dependent noise lowering beta
            W_s = w[s, :]
            w_shift = W_s - np.max(W_s)
            noise_scale = wm_noise_base * (set_size / 3.0) * (1.0 + 0.5 * age_group)
            beta_wm_eff = softmax_beta_wm / (1.0 + noise_scale)
            exp_w = np.exp(beta_wm_eff * w_shift)
            pi_wm = exp_w / np.sum(exp_w)
            p_wm = max(pi_wm[a], eps)

            # Mixture weight (reduced by load and age)
            size_factor = 3.0 / set_size
            age_factor = 1.0 - 0.25 * age_group  # older reduces effective WM weight
            wm_weight_eff = np.clip(wm_weight * size_factor * age_factor, 0.0, 1.0)

            p_total = np.clip(wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update with signed learning rate
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr_use * delta

            # WM updating: rewarded trials store a one-hot; otherwise diffuse toward uniform
            # Diffusion increases with set size and age (no extra parameter introduced)
            decay_factor = np.clip(0.2 * noise_scale, 0.0, 1.0)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target
            # apply diffusion/decay each trial
            w[s, :] = (1.0 - decay_factor) * w[s, :] + decay_factor * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with prediction-error-gated WM influence and RL forgetting.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM influence is gated by a function of a PE proxy (how suboptimal the chosen action is under RL);
      when the chosen action looks suboptimal (large PE proxy), the model trusts RL more.
    - WM weight decreases with set size and age.
    - RL values forget toward uniform each trial (global decay), more reliance on recent outcomes.
    - WM stores rewarded associations; when not rewarded, WM decays toward uniform with a rate tied to RL forgetting and load.

    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature baseline (scaled by 10 inside)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - wm_gate_sensitivity: sensitivity of the WM gate to PE proxy (>=0)
    - rl_forget: RL forgetting/decay rate toward uniform (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters: as in the template

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, wm_weight_base, wm_gate_sensitivity, rl_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # RL policy distribution
            Q_s = q[s, :]
            q_shift = Q_s - np.max(Q_s)
            exp_q = np.exp(softmax_beta * q_shift)
            pi_rl = exp_q / np.sum(exp_q)
            p_rl = max(pi_rl[a], eps)

            # WM policy distribution
            W_s = w[s, :]
            w_shift = W_s - np.max(W_s)
            exp_w = np.exp(softmax_beta_wm * w_shift)
            pi_wm = exp_w / np.sum(exp_w)
            p_wm = max(pi_wm[a], eps)

            # Gate WM by a PE proxy: how far is chosen action from the best RL action
            best_val = np.max(Q_s)
            pe_proxy = np.clip(best_val - Q_s[a], 0.0, 1.0)  # 0 if best action selected under RL, larger if worse
            gate = 1.0 / (1.0 + np.exp(-wm_gate_sensitivity * (1.0 - pe_proxy)))  # larger when pe_proxy small

            size_factor = 3.0 / set_size
            age_factor = 1.0 - 0.3 * age_group
            wm_weight_eff = np.clip(wm_weight_base * size_factor * age_factor * gate, 0.0, 1.0)

            p_total = np.clip(wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # global soft forgetting toward uniform Q
            q = (1.0 - rl_forget) * q + (rl_forget) * (1.0 / nA)

            # WM update: reward stores the association; otherwise decay toward uniform
            wm_decay = np.clip(rl_forget * (set_size / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Entropy-based arbitration between RL and WM with age- and load-dependent WM dynamics.

    Idea:
    - Compute full RL and WM action distributions each trial.
    - Arbitration weight is a logistic function of relative certainty (inverse entropy):
      more weight to the process with lower entropy, with a bias term.
    - WM learns quickly toward one-hot with wm_learn and decays toward uniform with wm_decay.
    - Set size and age increase WM decay; age also weakens arbitration in favor of WM.

    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature baseline (scaled by 10 inside)
    - wm_learn: WM learning rate toward one-hot on rewarded trials (0..1)
    - wm_decay: baseline WM decay per trial toward uniform (0..1)
    - arbitration_bias: bias term added to the entropy difference (can be negative/positive)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters: as in the template

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, wm_learn, wm_decay, arbitration_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    def entropy(p):
        p_clip = np.clip(p, eps, 1.0)
        return -np.sum(p_clip * np.log(p_clip))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # RL distribution
            Q_s = q[s, :]
            q_shift = Q_s - np.max(Q_s)
            exp_q = np.exp(softmax_beta * q_shift)
            pi_rl = exp_q / np.sum(exp_q)
            p_rl = max(pi_rl[a], eps)

            # WM distribution
            W_s = w[s, :]
            w_shift = W_s - np.max(W_s)
            exp_w = np.exp(softmax_beta_wm * w_shift)
            pi_wm = exp_w / np.sum(exp_w)
            p_wm = max(pi_wm[a], eps)

            # Arbitration: weight WM by relative certainty (inverse entropy), plus bias
            H_rl = entropy(pi_rl)
            H_wm = entropy(pi_wm)
            # Scale difference by load and age (more weight to RL under high load and age)
            scale = 5.0 * (3.0 / set_size) * (1.0 - 0.25 * age_group)
            arb_input = arbitration_bias + scale * (H_rl - H_wm)  # if WM more certain (H_wm < H_rl), arb_input increases
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = np.clip(wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: learn toward one-hot if rewarded; always decay toward uniform
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target

            # Age and load increase decay strength
            decay_eff = np.clip(wm_decay * (set_size / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p