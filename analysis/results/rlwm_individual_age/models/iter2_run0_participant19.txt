def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + gated WM with set-size and age-dependent WM gating, plus choice stickiness.

    Mechanism:
    - RL: tabular Q-learning with softmax policy.
    - WM: stores the last rewarded action for each state (a single-slot associative memory).
      WM policy is near-deterministic when a remembered action is available; otherwise uniform.
    - Arbitration: mixture weight of WM is determined by a logistic gate that depends on:
        (i) a base gate parameter,
        (ii) set size (reduced WM use at larger sizes),
        (iii) age group (young get a positive gate shift).
    - Choice stickiness: a bias toward repeating the previous action (within block) enters RL policy.

    Parameters:
    - model_parameters = [lr, softmax_beta, wm_gate0, age_gate_shift, setsize_gate_slope, stickiness]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      wm_gate0: base WM gate offset (real)
      age_gate_shift: additive WM gate shift for young (=0) vs old (=1): effective shift = (1-2*age_group)*age_gate_shift
      setsize_gate_slope: how strongly larger set sizes reduce WM gating; effective offset = -setsize_gate_slope*(nS-3)
      stickiness: choice perseveration weight added to the last chosen action in softmax (real)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_gate0, age_gate_shift, setsize_gate_slope, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM memory: -1 means empty; otherwise stores action index last rewarded for that state
        wm_memory = -1 * np.ones(nS, dtype=int)

        # Track previous action for stickiness
        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with stickiness
            Q_s = q[s, :].copy()
            if prev_action is not None:
                Q_s[prev_action] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            if wm_memory[s] >= 0:
                # Construct preference vector: choose stored action
                W_s = np.zeros(nA)
                W_s[wm_memory[s]] = 1.0
            else:
                # No memory -> uniform
                W_s = np.ones(nA) / nA
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Gating for WM mixture weight
            age_shift = (1 - 2 * age_group) * age_gate_shift
            setsize_shift = -setsize_gate_slope * float(nS - 3)
            gate = wm_gate0 + age_shift + setsize_shift
            wm_weight = 1.0 / (1.0 + np.exp(-gate))  # sigmoid

            # Mixture policy likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: store last rewarded action; clear on zero reward for that state
            if r > 0:
                wm_memory[s] = a
            else:
                # If negative/zero feedback, we clear the memory to avoid perseverating on wrong mapping
                wm_memory[s] = -1

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + graded WM traces with learning and decay; WM weight scales with set size and age-tuned RL temperature.

    Mechanism:
    - RL: tabular Q-learning with softmax. Age modulates inverse temperature (young -> sharper policy).
    - WM: for each state, a graded preference distribution w[s,:] that learns quickly from reward
      (wm_learning) and decays over time (wm_decay). WM policy uses high precision softmax.
    - Arbitration: mixture weight of WM scales with 3/nS (less WM influence at set size 6) times base_wm_weight.

    Parameters:
    - model_parameters = [lr, softmax_beta, wm_learning, wm_decay, base_wm_weight, age_beta_mult]
      lr: RL learning rate in [0,1]
      softmax_beta: base RL inverse temperature (rescaled by *10 internally)
      wm_learning: WM learning rate toward the one-hot chosen action on rewarded trials in [0,1]
      wm_decay: per-visit decay toward uniform for the current state in [0,1]
      base_wm_weight: baseline WM mixture weight in [0,1]
      age_beta_mult: multiplicative boost to RL beta for young vs old: beta_eff = beta*(1 + age_beta_mult*(1-2*age_group))

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_learning, wm_decay, base_wm_weight, age_beta_mult = model_parameters
    # Age-modulated RL beta
    age_group = 0 if age[0] <= 45 else 1
    beta_eff = softmax_beta * (1.0 + age_beta_mult * (1 - 2 * age_group))
    beta_eff *= 10.0
    softmax_beta_wm = 50.0

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM preference matrix initialized uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_size = base_wm_weight * (3.0 / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy from graded preferences
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            wm_weight = np.clip(wm_weight_size, 0.0, 1.0)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning: on reward, move the state's preference toward the chosen action
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learning) * w[s, :] + wm_learning * target

            # Renormalize to a valid distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration:
    WM weight increases when RL policy is uncertain (high entropy), but decreases with set size.
    Age modulates sensitivity to uncertainty.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: state-specific associative preferences w[s,:], decaying faster for larger set sizes.
      WM policy uses precision controlled by wm_precision.
    - Arbitration: wm_weight = sigmoid(base_weight - size_capacity*(nS-3) + unc_sens*(H_max - H_RL)),
      where H_RL is the entropy of the RL policy at the current state; young participants have higher
      uncertainty sensitivity than old.

    Parameters:
    - model_parameters = [lr, softmax_beta, wm_precision, base_weight, unc_sensitivity_age, size_capacity]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      wm_precision: scales WM softmax precision in [0,1]; beta_wm = 10 + 40*wm_precision
      base_weight: base offset for WM mixture weight passed through sigmoid
      unc_sensitivity_age: base uncertainty sensitivity; effective = unc_sensitivity_age * (1 - 0.5*age_group)
      size_capacity: controls WM decay with set size and penalty on WM weight for larger sets

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_precision, base_weight, unc_sensitivity_age, size_capacity = model_parameters
    softmax_beta *= 10.0
    beta_wm = 10.0 + 40.0 * wm_precision

    age_group = 0 if age[0] <= 45 else 1
    # Young (0) keep full sensitivity; old (1) reduced by half
    unc_sens_eff = unc_sensitivity_age * (1.0 - 0.5 * age_group)

    nA = 3
    H_max = np.log(nA)
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM preferences per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay factor: larger sets -> stronger decay
        # decay_s between 0 and 1; when nS=3, mild decay; when nS=6, stronger decay.
        decay_s = np.clip(size_capacity * float(nS - 3) / max(1.0, float(nS - 3) + 1.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy probabilities for entropy
            Q_s = q[s, :]
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            p_rl = pi[a]

            # Entropy of RL policy at state s
            H = -np.sum(pi * np.log(np.clip(pi, eps, 1.0)))

            # WM policy from current preferences
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Uncertainty-based arbitration with set-size penalty
            gate = base_weight - size_capacity * float(nS - 3) + unc_sens_eff * (H_max - H)
            wm_weight = 1.0 / (1.0 + np.exp(-gate))

            # Mixture likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform for current state driven by set size
            w[s, :] = (1.0 - decay_s) * w[s, :] + decay_s * w_0[s, :]

            # WM update: reward-driven sharpening toward chosen action
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use wm_precision implicitly as learning intensity via beta_wm scale
                # Here we move a fixed fraction proportional to wm_precision
                alpha_wm = np.clip(wm_precision, 0.0, 1.0)
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p