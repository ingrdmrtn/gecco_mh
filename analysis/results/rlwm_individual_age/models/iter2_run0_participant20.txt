def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + capacity-limited decaying WM.
    
    Policy
    - Mixture of RL softmax and WM softmax.
    - WM weight decreases with larger set size; slightly higher for younger adults.
    
    Learning
    - RL: standard TD learning with global forgetting toward uniform.
    - WM: item-specific distribution that decays toward uniform; after reward it moves toward a one-hot code of the chosen action.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..set_size-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6), constant within a block.
    age : array-like or scalar
        Participant age; age_group=0 if age<=45 else 1.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, rl_forget]
        - lr: RL learning rate (0..1).
        - wm_weight_base: base weight for WM in the mixture (0..1).
        - softmax_beta: inverse temperature for RL (scaled by 10 internally).
        - wm_decay: decay of WM representations toward uniform per trial (0..1).
        - rl_forget: global RL forgetting toward uniform per trial (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, rl_forget = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    # Age group coding
    age_group = 0 if (age if isinstance(age, (int, float)) else age[0]) <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = np.asarray(actions)[block_mask].astype(int)
        block_rewards = np.asarray(rewards)[block_mask].astype(float)
        block_states = np.asarray(states)[block_mask].astype(int)
        block_set_sizes = np.asarray(set_sizes)[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))    # RL values
        w = (1.0 / nA) * np.ones((nS, nA))    # WM policy-like weights
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior for WM

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action (stable form)
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WORKING MEMORY POLICY:
            # WM is treated as a categorical distribution over actions held in w.
            # We compute a sharp softmax over W_s (acts like argmax with some smoothing).
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            p_wm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # Mixture weight: capacity-limited (3/nS) and slightly higher for younger (age_group=0)
            wm_weight = np.clip(wm_weight_base * (3.0 / nS) * (1.0 + 0.15 * (1 - age_group)), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with TD error + global forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Global forgetting (applied each trial to entire Q-table)
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            # WORKING MEMORY UPDATE:
            # 1) Decay WM toward uniform for the current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) If rewarded, encode chosen action by moving w[s] toward a one-hot vector
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                enc = 1.0 - wm_decay  # effective encoding strength
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
                # Renormalize for numerical stability
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with choice stickiness and uncertainty-based WM/RL arbitration.

    Policy
    - Mixture of RL softmax and WM softmax.
    - Arbitration weight increases when WM is confident (low entropy) and RL is uncertain (small value gap).
    - Younger adults rely more on WM (age_penalty reduces WM reliance for older adults).

    Learning
    - RL: TD learning; adds a "choice stickiness" kernel in policy (per-state last-choice memory).
    - WM: moves toward one-hot on reward; slight decay toward uniform on no-reward.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, k_uncertainty, stickiness, age_penalty]
        - lr: RL learning rate (0..1).
        - wm_weight_base: baseline WM weight (0..1).
        - softmax_beta: RL inverse temperature (scaled by 10).
        - k_uncertainty: sensitivity of arbitration to (WM_conf - RL_conf).
        - stickiness: strength of choice kernel added to RL logits.
        - age_penalty: subtraction applied to WM reliance for older adults.

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, k_uncertainty, stickiness, age_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if (age if isinstance(age, (int, float)) else age[0]) <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = np.asarray(actions)[block_mask].astype(int)
        block_rewards = np.asarray(rewards)[block_mask].astype(float)
        block_states = np.asarray(states)[block_mask].astype(int)
        block_set_sizes = np.asarray(set_sizes)[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state choice kernel for stickiness (last-choice trace)
        c = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            C_s = c[s, :]

            # RL logits include choice stickiness kernel
            logits_rl = softmax_beta * Q_s + stickiness * C_s
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            p_wm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # Build uncertainties/confidences:
            # WM confidence: 1 - normalized entropy
            eps = 1e-12
            P = np.clip(W_s / (np.sum(W_s) + eps), eps, 1.0)
            H = -np.sum(P * np.log(P))
            H_max = np.log(nA)
            wm_conf = 1.0 - (H / (H_max + eps))

            # RL confidence: value margin between best and second best (normalized)
            sorted_Q = np.sort(Q_s)
            best = sorted_Q[-1]
            second = sorted_Q[-2]
            rl_conf_raw = max(best - second, 0.0)
            # normalize by theoretical range [0,1] approximately
            rl_conf = np.clip(rl_conf_raw, 0.0, 1.0)

            # Arbitration weight (sigmoid), penalize WM for older adults, capacity effect (3/nS)
            base = wm_weight_base + k_uncertainty * (wm_conf - rl_conf) - age_penalty * age_group + 0.3 * (3.0 / nS - 0.5)
            wm_weight = 1.0 / (1.0 + np.exp(-base))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL TD update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * target
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Update choice stickiness kernel (one-hot of last choice in this state)
            c[s, :] *= 0.0
            c[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM with selective WM encoding driven by surprise and a lapse component.

    Policy
    - Mixture of RL and WM softmax, with an additional lapse (uniform) probability.
    - WM reliance decreases with set size; lapse increases with set size and age.
    - WM and RL use separate temperature controls (WM temperature declines with age).

    Learning
    - RL: standard TD learning.
    - WM: encodes the chosen action more strongly when unsigned prediction error (surprise) is high
      and when set size is small; otherwise WM decays toward uniform.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array
        [lr, wm_weight_base, beta_rl_base, beta_wm_base, lapse_base, pe_encode_slope]
        - lr: RL learning rate (0..1).
        - wm_weight_base: base WM weight in mixture.
        - beta_rl_base: base RL inverse temperature (scaled by 10 internally).
        - beta_wm_base: base WM inverse temperature (scaled by 10 internally).
        - lapse_base: baseline lapse rate; increases with set size and age.
        - pe_encode_slope: slope controlling how unsigned PE drives WM encoding.

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, wm_weight_base, beta_rl_base, beta_wm_base, lapse_base, pe_encode_slope = model_parameters
    softmax_beta = beta_rl_base * 10.0  # RL inverse temperature
    softmax_beta_wm = max(1.0, beta_wm_base * 10.0)  # WM inverse temperature

    age_group = 0 if (age if isinstance(age, (int, float)) else age[0]) <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = np.asarray(actions)[block_mask].astype(int)
        block_rewards = np.asarray(rewards)[block_mask].astype(float)
        block_states = np.asarray(states)[block_mask].astype(int)
        block_set_sizes = np.asarray(set_sizes)[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl[a]) / np.sum(np.exp(logits_rl))

            # WM softmax (WM temperature slightly lower for older adults)
            beta_wm_eff = softmax_beta_wm * (1.0 - 0.2 * age_group)
            logits_wm = beta_wm_eff * W_s
            logits_wm -= np.max(logits_wm)
            p_wm = np.exp(logits_wm[a]) / np.sum(np.exp(logits_wm))

            # Mixture weight: reduced with set size
            wm_weight = np.clip(wm_weight_base * (3.0 / nS), 0.0, 1.0)

            # Lapse increases with set size and age
            lapse = np.clip(lapse_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.5)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM selective encoding based on unsigned PE and set size
            # Compute encoding probability via logistic of surprise and capacity term
            surprise = abs(pe)
            enc_drive = pe_encode_slope * (surprise - 0.5) + np.log(3.0 / nS + 1e-12)
            p_encode = 1.0 / (1.0 + np.exp(-enc_drive))
            p_encode = np.clip(p_encode, 0.0, 1.0)

            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - p_encode) * w[s, :] + p_encode * target
            else:
                # If not encoding, decay toward uniform slightly
                decay = 0.1 + 0.1 * (nS / 6.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Renormalize WM
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -float(blocks_log_p)