def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + eligibility traces with capacity-limited WM mixture (age- and set-size sensitive).

    Idea:
    - Model-based WM store is capacity-limited: an effective capacity cap = cap_base - cap_age_penalty*age_group.
      The effective WM contribution scales with cap / set_size (capped at 1).
    - RL updates use eligibility traces (lambda) within a block to propagate credit to recent actions in the same state.
    - WM policy is near-deterministic; WM updates do win-store/lose-decay with strength proportional to the effective capacity ratio.

    Parameters
    ----------
    states : array-like, int
        State index per trial within a block (0..set_size-1).
    actions : array-like, int
        Chosen action per trial (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size on each trial (3 or 6).
    age : array-like, int
        Participant age repeated per trial. age_group = 0 if <=45, else 1.
    model_parameters : list or array
        [lr, lam, wm_weight, softmax_beta, cap_base, cap_age_penalty]
        - lr: RL learning rate (0..1).
        - lam: eligibility trace parameter lambda (0..1). Higher -> longer-lasting traces.
        - wm_weight: baseline WM mixture weight (0..1), further scaled by effective capacity ratio.
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - cap_base: baseline WM capacity in number of items (~3-6).
        - cap_age_penalty: capacity decrement applied if age_group==1.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, lam, wm_weight, softmax_beta, cap_base, cap_age_penalty = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        # Effective WM contribution scales with capacity ratio
        cap_eff = max(0.0, cap_base - cap_age_penalty * age_group)
        cap_ratio = min(1.0, cap_eff / max(1, nS))
        wm_w_eff = np.clip(wm_weight * cap_ratio, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for chosen action probability (softmax trick in template)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: near-deterministic softmax of WM table
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Update traces: decay all, set current (s,a) to 1
            e *= lam
            e[s, :] *= 0.0
            e[s, a] = 1.0
            pe = r - q[s, a]
            q += lr * pe * e

            # WM update: win-store / lose-decay with strength proportional to cap_ratio
            eta = cap_ratio  # stronger storage/decay when capacity ratio is higher
            if r >= 0.5:
                # Store action deterministically with strength eta
                w[s, :] = (1.0 - eta) * w[s, :]
                w[s, a] += eta
            else:
                # Decay toward uniform with strength eta
                w[s, :] = (1.0 - eta) * w[s, :] + eta * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus, WM with decay scaling by set size, and age-driven WM noise.

    Idea:
    - RL includes an exploration bonus inversely proportional to sqrt of visit counts (uncertainty-driven exploration).
    - WM contribution decays toward uniform at the chosen state with a rate that increases for larger set sizes.
    - Older adults (age_group=1) have noisier WM policy (lower effective WM inverse temperature).
    - WM mixture weight scales by 3/nS (lower for larger set sizes).

    Parameters
    ----------
    states : array-like, int
    actions : array-like, int
    rewards : array-like, float
    blocks : array-like, int
    set_sizes : array-like, int
    age : array-like, int
        age_group = 0 if <=45, else 1.
    model_parameters : list or array
        [lr, softmax_beta, wm_weight, bonus, wm_decay, age_wm_noise]
        - lr: RL learning rate.
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - wm_weight: baseline WM mixture weight, scaled by 3/nS.
        - bonus: magnitude of exploration bonus added to Q based on uncertainty (1/sqrt(N)).
        - wm_decay: baseline WM decay rate per chosen state; amplified by set size.
        - age_wm_noise: scales additional WM noise for age_group==1 (reduces WM beta).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, bonus, wm_decay, age_wm_noise = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm_base = 50.0
    # Older group has noisier WM (lower beta); young unaffected
    softmax_beta_wm = softmax_beta_wm_base / (1.0 + age_wm_noise * age_group)

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # counts for uncertainty bonus

        # WM mixture weight reduced in larger set sizes
        wm_w_eff = np.clip(wm_weight * (3.0 / max(1, nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with uncertainty bonus
            bonus_s = bonus / np.sqrt(N[s, :] + 1.0)
            Q_s_aug = q[s, :] + bonus_s
            denom_rl = np.sum(np.exp(beta * (Q_s_aug - Q_s_aug[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Increment counts after observing (s,a)
            N[s, a] += 1.0

            # WM update with set-size-scaled decay
            # Larger set sizes -> stronger decay
            decay_eff = np.clip(wm_decay * (nS / 3.0), 0.0, 1.0)
            if r >= 0.5:
                # Store correct action strongly
                w[s, :] = (1.0 - (1.0 - decay_eff)) * w[s, :]  # ensure we use decay_eff meaningfully
                w[s, :] = decay_eff * w[s, :]  # mild shrink to emphasize next assignment
                w[s, :] = (1.0 - decay_eff) * w[s, :]
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]  # gentle regularization
                # Finalize as near one-hot leaning toward chosen action
                w[s, :] = (1.0 - decay_eff) * w[s, :]
                w[s, a] += decay_eff
                # Re-normalize to avoid drift (implicit via softmax use; keep bounded)
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)
                if w[s, :].sum() > 0:
                    w[s, :] /= w[s, :].sum()
            else:
                # Lose: decay toward uniform at chosen state
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and a meta-controller that sets WM weight from set size and age.
    WM uses win-store / size-dependent decay.

    Idea:
    - Use separate learning rates for positive and negative prediction errors.
    - The arbitration (WM mixture weight) is determined by a logistic function of set size and age:
      wm_w_eff = sigmoid(wm_bias0 + wm_age_shift*age_group + wm_slope*(3/nS)).
      Thus, WM is favored at small set size and for younger adults (if wm_age_shift<0).
    - WM updates: reward -> store one-hot; no reward -> decay toward uniform with rate 1 - 3/nS.

    Parameters
    ----------
    states : array-like, int
    actions : array-like, int
    rewards : array-like, float
    blocks : array-like, int
    set_sizes : array-like, int
    age : array-like, int
        age_group = 0 if <=45, else 1.
    model_parameters : list or array
        [lr_pos, lr_neg, softmax_beta, wm_bias0, wm_slope, wm_age_shift]
        - lr_pos: learning rate for positive PE.
        - lr_neg: learning rate for negative PE.
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - wm_bias0: baseline bias (logit space) toward WM mixture.
        - wm_slope: sensitivity of WM mixture to set size factor (3/nS).
        - wm_age_shift: additional logit shift for age_group==1 (older adults).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_bias0, wm_slope, wm_age_shift = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration: WM weight from set size and age via logistic
        ss_factor = 3.0 / max(1, nS)  # 1.0 at nS=3, 0.5 at nS=6
        wm_logit = wm_bias0 + wm_age_shift * age_group + wm_slope * ss_factor
        wm_w_eff = np.clip(sigmoid(wm_logit), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: win-store / size-dependent decay to uniform
            decay_wm = np.clip(1.0 - (3.0 / max(1, nS)), 0.0, 1.0)  # 0 at nS=3; 0.5 at nS=6
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)