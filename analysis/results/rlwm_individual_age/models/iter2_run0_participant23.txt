Below are three alternative cognitive models tailored for the RL–WM task. Each is a standalone function that returns the negative log-likelihood of the observed choices, uses age group and set size meaningfully, and uses no more than six parameters. All parameters are used.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(α) with eligibility traces + WM with capacity-weighted mixture.
    
    Idea:
    - RL learns Q-values with a single learning rate and an eligibility trace that
      keeps credit on recently chosen state-action pairs.
    - WM stores the most recently rewarded action per state (one-shot) but decays
      toward uniform when not reinforced.
    - The mixture weight of WM is reduced by larger set size and by being older.
      It is also modulated by WM “confidence” (how peaked the WM distribution is).
    
    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of {3,6}
    age : array-like or scalar
        Single numeric age or repeated array.
    model_parameters : list or array-like
        [alpha, beta, wm_base, wm_decay, trace_lambda, age_wm_drop]
        - alpha: RL learning rate (logistic mapped to (0,1)).
        - beta: inverse temperature for RL (scaled by *10 internally).
        - wm_base: baseline WM mixture coefficient (logistic to (0,1)).
        - wm_decay: WM decay rate toward uniform when not rewarded (logistic to (0,1)).
        - trace_lambda: eligibility trace parameter (logistic to (0,1)).
        - age_wm_drop: reduction of WM contribution for older group (logistic to (0,1)).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_base, wm_decay, trace_lambda, age_wm_drop = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = beta * 10.0
    wm_base = 1.0 / (1.0 + np.exp(-wm_base))
    wm_decay = 1.0 / (1.0 + np.exp(-wm_decay))
    trace_lambda = 1.0 / (1.0 + np.exp(-trace_lambda))
    age_wm_drop = 1.0 / (1.0 + np.exp(-age_wm_drop))

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    nA = 3
    softmax_beta_wm = 50.0  # deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL, WM, eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # Capacity factor decreases with larger set size, reduced further in older group
        setsize_capacity = np.exp(-(nS - 3))  # 1 for 3, ~0.135 for 6
        age_factor = age_wm_drop * age_group
        base_wm_weight = wm_base * setsize_capacity * (1.0 - age_factor)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # WM policy with confidence gating
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence = how peaked WM is (0 for uniform, 1-1/3=2/3 for one-hot)
            confidence = max(0.0, np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA + 1e-12)
            wm_weight_t = base_wm_weight * confidence
            wm_weight_t = min(max(wm_weight_t, 0.0), 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            delta = r - q[s, a]
            # Decay traces
            e *= trace_lambda
            # Replacing trace for current (s, a)
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += alpha * delta * e

            # WM update
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                # Decay toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-adjusted temperature + WM confidence-gated mixture + lapse.
    
    Idea:
    - RL learns Q with a single learning rate and age-adjusted inverse temperature
      (older group shows lower β).
    - WM stores rewarded actions one-shot and decays otherwise.
    - The gating between WM and RL depends on WM confidence (max(W) - 1/3)
      scaled by a sensitivity parameter and reduced by set size.
    - A lapse parameter mixes in a uniform random choice component.
    
    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of {3,6}
    age : array-like or scalar
    model_parameters : list or array-like
        [alpha, beta_rl, kappa_conf, lapse, wm_decay, age_beta_drop]
        - alpha: RL learning rate (logistic to (0,1)).
        - beta_rl: RL inverse temperature base (scaled by *10).
        - kappa_conf: sensitivity of WM gating to WM confidence (softplus > 0).
        - lapse: lapse rate mixing uniform choice (logistic to (0,1)).
        - wm_decay: WM decay toward uniform when not rewarded (logistic to (0,1)).
        - age_beta_drop: fractional drop in RL β for older group (logistic to (0,1)).
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta_rl, kappa_conf, lapse, wm_decay, age_beta_drop = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta_rl = beta_rl * 10.0
    # softplus for kappa (positive)
    kappa_conf = np.log1p(np.exp(kappa_conf))
    lapse = 1.0 / (1.0 + np.exp(-lapse))
    wm_decay = 1.0 / (1.0 + np.exp(-wm_decay))
    age_beta_drop = 1.0 / (1.0 + np.exp(-age_beta_drop))

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    nA = 3
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted RL temperature
        beta_rl_eff = beta_rl * (1.0 - age_beta_drop * age_group)
        beta_rl_eff = max(beta_rl_eff, 1e-3)

        # Set size factor reduces the effect of WM confidence on gating
        setsize_reduce = np.exp(-(nS - 3))  # 1 for 3, ~0.135 for 6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM policy and confidence
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            confidence = max(0.0, np.max(W_s) - 1.0 / nA)  # in [0, 2/3]
            # Gating by sigmoid of scaled confidence, reduced by set size
            gate_input = kappa_conf * setsize_reduce * confidence
            wm_gate = 1.0 / (1.0 + np.exp(-gate_input))
            wm_gate = min(max(wm_gate, 0.0), 1.0)

            # Lapse mixture with uniform
            p_mix = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # WM update
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + Bayesian-like WM counts, capacity/age modulate
    both temperature and WM precision.
    
    Idea:
    - RL has separate learning rates for positive and negative prediction errors.
    - WM maintains Dirichlet-like counts over actions per state (posterior predictive),
      updated mainly on reward and leaking toward a flat prior.
    - Mixture weight depends on capacity adjusted by set size; both RL and WM temperatures
      are reduced by an age-related noise increase parameter (older -> lower β).
    
    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of {3,6}
    age : array-like or scalar
    model_parameters : list or array-like
        [alpha_pos, alpha_neg, beta_base, beta_wm, capacity_scale, age_noise_increase]
        - alpha_pos: RL learning rate for positive PE (logistic to (0,1)).
        - alpha_neg: RL learning rate for negative PE (logistic to (0,1)).
        - beta_base: RL inverse temperature base (scaled by *10).
        - beta_wm: WM inverse temperature base (scaled by *20).
        - capacity_scale: scales WM mixture with set size (logistic to (0,1)).
        - age_noise_increase: reduces both RL and WM β in older group (logistic to (0,1)).
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos, alpha_neg, beta_base, beta_wm, capacity_scale, age_noise_increase = model_parameters

    alpha_pos = 1.0 / (1.0 + np.exp(-alpha_pos))
    alpha_neg = 1.0 / (1.0 + np.exp(-alpha_neg))
    beta_base = beta_base * 10.0
    beta_wm = beta_wm * 20.0
    capacity_scale = 1.0 / (1.0 + np.exp(-capacity_scale))
    age_noise_increase = 1.0 / (1.0 + np.exp(-age_noise_increase))

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL Q values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM counts (Dirichlet-like), start from flat prior of 1
        counts = np.ones((nS, nA), dtype=float)

        # Effective β reduced by age noise
        beta_rl_eff = beta_base * (1.0 - age_noise_increase * age_group)
        beta_rl_eff = max(beta_rl_eff, 1e-3)
        beta_wm_eff = beta_wm * (1.0 - age_noise_increase * age_group)
        beta_wm_eff = max(beta_wm_eff, 1e-3)

        # WM capacity-based mixture: higher capacity and smaller set sizes -> higher weight
        capacity_factor = capacity_scale * (3.0 / nS)
        capacity_factor = min(max(capacity_factor, 0.0), 1.0)

        # Leak toward prior (ones) increases with larger set size; scaled by 1 - capacity_factor
        # so higher capacity reduces leak.
        base_leak = 0.15 + 0.35 * (nS - 3) / 3.0  # 0.15 for 3, 0.5 for 6
        leak = base_leak * (1.0 - capacity_factor)
        leak = min(max(leak, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM posterior predictive (normalized counts)
            W_s = counts[s, :] / np.sum(counts[s, :])
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture weight fixed per block by capacity
            wm_weight = capacity_factor

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += lr * pe

            # WM counts update: reward-driven strengthening, with leak toward flat prior
            if r == 1:
                counts[s, a] += 1.0
            # Leak toward prior of ones (prevents unbounded certainty and models forgetting)
            counts[s, :] = (1.0 - leak) * counts[s, :] + leak * np.ones(nA)

        blocks_log_p += log_p

    return -blocks_log_p