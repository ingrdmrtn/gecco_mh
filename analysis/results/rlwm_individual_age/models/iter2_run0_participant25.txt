Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) with age- and set-size-dependent effects. Each function returns the negative log-likelihood of the observed choices for the provided participant. They follow the template’s structure, include separate blocks, and use the age group meaningfully. No imports are included; assume numpy as np is already available.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with reward-gated WM storage and decay.
    
    Idea:
    - Choices are governed by a mixture of RL softmax and WM softmax.
    - WM access weight scales with an effective capacity that depends on set size
      and an age penalty (older -> less WM influence).
    - WM stores the last rewarded action per state (when reward occurs), and otherwise decays to uniform.
    
    Parameters (6):
    - lr: RL learning rate for Q-values (0..1)
    - wm_weight_base: baseline mixture weight of WM vs RL (0..1)
    - softmax_beta: inverse temperature for RL softmax (>0)
    - wm_decay: decay rate of WM weights toward uniform per trial (0..1)
    - k_capacity: WM capacity (in items, >0)
    - age_penalty: multiplicative penalty applied when older (>=0)
    
    Age use:
    - age_group = 0 if young (<=45), 1 if old (>45). WM mixture weight is reduced by (1 - age_penalty*age_group).
    
    Set-size use:
    - Effective WM availability scales with min(1, k_capacity / set_size).
    """
    lr, wm_weight_base, softmax_beta, wm_decay, k_capacity, age_penalty = model_parameters
    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_rl = softmax_beta * 10.0  # higher dynamic range for RL
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # RL policy for chosen action a
            Q_s = q[s, :]
            Q_center = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta_rl * Q_center)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = p_rl_vec[a]

            # WM policy for chosen action a
            W_s = w[s, :]
            W_center = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_center)
            p_wm_vec = expW / np.sum(expW)
            p_wm = p_wm_vec[a]

            # Mixture weight: capacity- and age-modulated
            p_recall = min(1.0, max(0.0, k_capacity / max(1, set_size_t)))
            wm_weight_eff = wm_weight_base * p_recall * (1.0 - age_penalty * age_group)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then if rewarded, store the chosen action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM recency store; WM weight logistic-modulated by set-size and age.
    
    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores the most recent action taken for each state regardless of reward (episodic/recency-based),
      producing a strong but noisy policy (softmax with high inverse temperature).
    - Arbitration weight between WM and RL is a logistic of (bias + size term + age term):
        wm_weight = sigmoid(wm_base + gamma_size*(3 - set_size) - age_shift*age_group)
      Larger set sizes reduce WM weight (since 3 - set_size is negative for set_size=6).
      Older age increases the subtraction term and thus reduces WM weight.
    
    Parameters (6):
    - alpha_pos: RL learning rate for positive PEs (0..1)
    - alpha_neg: RL learning rate for negative PEs (0..1)
    - softmax_beta: RL softmax inverse temperature (>0)
    - wm_base: baseline WM weight in the logistic
    - gamma_size: weight of set-size modulation in the logistic
    - age_shift: penalty applied to WM when age_group=1 (older)
    
    Age use:
    - wm_weight reduced by age_shift when age_group == 1.
    
    Set-size use:
    - WM weight modulated by gamma_size*(3 - set_size).
    """
    alpha_pos, alpha_neg, softmax_beta, wm_base, gamma_size, age_shift = model_parameters
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_rl = softmax_beta * 10.0
    softmax_beta_wm = 40.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM recency store: initialize uniform; then overwrite with one-hot of last chosen action
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            Q_center = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(softmax_beta_rl * Q_center)
            p_rl_vec = p_rl_vec / np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM policy (recency-based)
            W_s = w[s, :]
            W_center = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * W_center)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            # Arbitration weight via logistic, with set-size and age terms
            z = wm_base + gamma_size * (3 - set_size_t) - age_shift * age_group
            wm_weight = sigmoid(z)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM recency update: store last chosen action (irrespective of reward)
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            w[s, :] = onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-wise uncertainty arbitration and count-based WM.
    
    Idea:
    - RL component uses a single learning rate and softmax.
    - WM component tracks counts of past rewards per action for each state and uses a high-beta softmax over those counts.
    - Arbitration weight depends on current uncertainty (entropy) of RL policy, set size, and age:
        wm_weight = sigmoid(theta0 - theta_unc * H(Q_s) - theta_size * (set_size - 3) - age_group)
      where H(Q_s) is the entropy of the RL policy at the current state.
      Thus, higher uncertainty increases WM reliance; larger set size reduces WM reliance; older age reduces WM reliance.
    
    Parameters (6):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL softmax inverse temperature (>0)
    - wm_beta: WM policy inverse temperature (>0)
    - theta0: baseline arbitration bias
    - theta_unc: weight on RL policy entropy (uncertainty term)
    - theta_size: weight on set-size penalty
    
    Age use:
    - Direct subtraction of age_group in the arbitration logit reduces WM reliance in older adults.
    
    Set-size use:
    - Direct penalty proportional to (set_size - 3) in the arbitration logit.
    """
    lr, softmax_beta, wm_beta, theta0, theta_unc, theta_size = model_parameters
    age_group = 0 if age[0] <= 45 else 1

    beta_rl = softmax_beta * 10.0
    beta_wm = wm_beta * 10.0
    eps = 1e-12

    def softmax(vec, beta):
        v = vec - np.max(vec)
        e = np.exp(beta * v)
        return e / np.sum(e)

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM counts of past rewards per action and state
        c = np.ones((nS, nA)) * 1e-3  # small prior to avoid 0/0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # RL policy and entropy (uncertainty)
            p_rl_vec = softmax(q[s, :], beta_rl)
            p_rl = p_rl_vec[a]
            H = entropy(p_rl_vec)

            # WM policy from reward counts
            wm_pref = c[s, :]
            p_wm_vec = softmax(wm_pref, beta_wm)
            p_wm = p_wm_vec[a]

            # Arbitration based on uncertainty, set size, and age
            z = theta0 - theta_unc * H - theta_size * (set_size_t - 3) - age_group
            wm_weight = sigmoid(z)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM count update (reward-sensitive accumulation)
            if r > 0.5:
                c[s, a] += 1.0
            else:
                # small leak toward uniform to prevent overcommitment on persistent errors
                c[s, :] = 0.99 * c[s, :] + 0.01 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p

Notes on parameter-age-set size interactions:
- Model 1: WM’s contribution scales with capacity (k_capacity) relative to set size and is reduced in older adults via age_penalty. WM stores only rewarded actions, with decay otherwise.
- Model 2: WM-RL mixture weight is a logistic function of set size and age; RL uses asymmetric learning rates to capture sensitivity to positive/negative feedback; WM is pure recency.
- Model 3: Arbitration depends dynamically on RL uncertainty (entropy), penalizes larger set sizes, and reduces WM in older adults; WM maintains reward counts, offering a complementary fast-learning channel.