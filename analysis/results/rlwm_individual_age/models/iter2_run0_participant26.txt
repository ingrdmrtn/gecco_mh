def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with PE-gated WM arbitration and forgetting; age and set size reduce WM reliance.

    The model mixes RL and WM policies. The WM contribution on each trial is gated by the magnitude
    of the most recent prediction error in that state (higher |PE| => more WM recruitment) and is
    further reduced by larger set size and by age group. WM traces decay toward uniform with a
    forgetting parameter. RL uses a single learning rate and a standard softmax policy.

    Parameters
    - model_parameters: [lr, wm_weight_base, softmax_beta, gate_sensitivity, wm_forgetting, age_wm_cost]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline mixture weight on WM (0..1)
        - softmax_beta: RL inverse temperature; internally scaled up by x10
        - gate_sensitivity: scales the impact of |PE| on WM gating (>=0)
        - wm_forgetting: decay of WM toward uniform on each state visit (0..1)
        - age_wm_cost: fractional reduction of WM influence for older adults (0..1)
    
    Returns
    - Negative log-likelihood of observed choices
    """
    lr, wm_weight_base, softmax_beta, gate_sensitivity, wm_forgetting, age_wm_cost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last PE per state to gate WM; initialize high to allow early WM use
        last_pe_abs = np.ones(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy (deterministic softmax over WM distribution)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # WM gate from previous |PE| in this state, reduced by set size and age
            pe_gate = 1.0 - np.exp(-gate_sensitivity * last_pe_abs[s])
            size_factor = 3.0 / nS
            age_factor = (1.0 - age_wm_cost * age_group)
            wm_weight = wm_weight_base * pe_gate * size_factor * age_factor
            wm_weight = min(1.0, max(0.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            last_pe_abs[s] = abs(pe)

            # WM decay toward uniform each visit
            w[s, :] = (1.0 - wm_forgetting) * w[s, :] + wm_forgetting * w_0[s, :]
            # On reward, write to WM (one-shot strengthening of the chosen action)
            if r > 0.5:
                write_strength = 0.9
                w[s, :] = (1.0 - write_strength) * w[s, :]
                w[s, a] += write_strength
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with win-stay bonus in WM and age/set-size dependent RL temperature.

    The model mixes RL and WM with a fixed mixture weight. RL softmax temperature decreases
    with larger set size and for older adults (more noise). WM includes a win-stay mechanism:
    if the last visit to a state was rewarded, the last rewarded action receives an additive
    bonus in the WM policy, biasing toward repetition. WM traces decay over visits.

    Parameters
    - model_parameters: [lr, softmax_beta_base, wm_weight, wm_decay, win_stay_bonus, age_beta_penalty]
        - lr: RL learning rate (0..1)
        - softmax_beta_base: base RL inverse temperature; internally scaled up by x10
        - wm_weight: mixture weight on WM (0..1)
        - wm_decay: decay of WM toward uniform on each visit (0..1)
        - win_stay_bonus: additive bonus to WM logit for last rewarded action (>=0)
        - age_beta_penalty: fractional reduction of RL beta for older adults (0..1)
    
    Returns
    - Negative log-likelihood of observed choices
    """
    lr, softmax_beta_base, wm_weight, wm_decay, win_stay_bonus, age_beta_penalty = model_parameters
    softmax_beta_base *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last rewarded action per state; -1 if none yet
        last_rew_action = -1 * np.ones(nS, dtype=int)
        last_rew_flag = np.zeros(nS)  # 1 if last visit rewarded, else 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL temperature adjusted by set size and age
            size_factor = 3.0 / nS  # larger sets => lower beta
            age_factor = (1.0 - age_beta_penalty * age_group)
            beta_eff = max(eps, softmax_beta_base * size_factor * age_factor)

            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy with win-stay bonus applied to logits
            W_s = w[s, :].copy()
            W_logits = np.log(np.clip(W_s, eps, 1.0))
            if last_rew_flag[s] > 0.5 and last_rew_action[s] >= 0:
                W_logits[last_rew_action[s]] += win_stay_bonus
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_logits - W_logits[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            wmix = min(1.0, max(0.0, wm_weight))
            p_total = wmix * p_wm + (1.0 - wmix) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay and write on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                write_strength = 0.9
                w[s, :] = (1.0 - write_strength) * w[s, :]
                w[s, a] += write_strength
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

            # Update win-stay memory traces
            last_rew_flag[s] = 1.0 if r > 0.5 else 0.0
            last_rew_action[s] = a if r > 0.5 else last_rew_action[s]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-driven exploration bonus and set-size arbitration; age increases exploration.

    The model mixes RL and WM. RL values include an exploration bonus inversely proportional to the
    square root of action visit counts within a state (uncertainty bonus). Older adults have a larger
    exploration bonus. WM contribution is reduced for larger set sizes and decays over visits.

    Parameters
    - model_parameters: [lr, softmax_beta, wm_weight_base, wm_decay, explore_bonus, age_explore_scale]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature; internally scaled up by x10
        - wm_weight_base: baseline mixture weight on WM (0..1)
        - wm_decay: decay of WM toward uniform on each visit (0..1)
        - explore_bonus: base magnitude of exploration bonus added to RL values (>=0)
        - age_explore_scale: multiplicative increase of bonus for older adults (>=0)
    
    Returns
    - Negative log-likelihood of observed choices
    """
    lr, softmax_beta, wm_weight_base, wm_decay, explore_bonus, age_explore_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        N = np.ones((nS, nA))  # action visit counts per state-action (start at 1 to avoid div by zero)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add uncertainty-driven exploration bonus to RL values
            bonus_scale = explore_bonus * (1.0 + age_explore_scale * age_group)
            bonus_s = bonus_scale / np.sqrt(np.maximum(1.0, N[s, :]))
            Q_aug = Q_s + bonus_s

            denom_rl = np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # WM weight reduced by set size
            size_factor = 3.0 / nS
            wm_weight = wm_weight_base * size_factor
            wm_weight = min(1.0, max(0.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (with unaugmented values for learning)
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Update visit counts
            N[s, a] += 1.0

            # WM decay and write on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                write_strength = 0.9
                w[s, :] = (1.0 - write_strength) * w[s, :]
                w[s, a] += write_strength
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p