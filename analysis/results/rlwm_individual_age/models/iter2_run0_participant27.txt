Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) mechanisms tailored to the rlwm task. Each function returns the negative log-likelihood of the observed choices and uses age group and set size meaningfully but differently across models.

Model 1: Capacity-limited, gated WM with LRU; RL uses Pearce–Hall adaptive learning rate
- Idea: WM stores a limited number of state–action mappings with near-deterministic policy if present; otherwise RL drives choice. WM capacity depends on age (older → reduced capacity). Storage into WM is gated by prediction error magnitude (higher surprise → more likely to store), with a higher gate for larger set sizes. RL learning rate adapts via a Pearce–Hall mechanism (higher unsigned PE → larger learning rate).
- Parameters (6):
  0) alpha0: base RL learning rate (0..1)
  1) kappa: attention persistence for Pearce–Hall (0..1) controlling how fast the adaptive LR follows unsigned PE
  2) beta_rl: RL inverse temperature (>0), internally scaled by 10
  3) wm_capacity_young: WM capacity when young (0..6)
  4) wm_gate_thr: base storage gate threshold on unsigned PE (0..1); effective threshold increases with load
  5) age_capacity_drop: multiplicative drop of capacity if old (0..1)

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited, gated WM (LRU replacement) + RL with Pearce–Hall adaptive learning rate.
    Policy: If a state is in WM, choose via WM (near-deterministic); else use RL softmax.
    WM capacity depends on age, and WM storage is gated by PE with a higher gate under larger set sizes.

    Parameters
    - model_parameters (len=6):
        0) alpha0             : base RL learning rate (0..1)
        1) kappa              : PE-attention persistence (0..1); higher -> smoother adaptive LR
        2) beta_rl            : RL inverse temperature (>0), internally scaled by 10
        3) wm_capacity_young  : WM capacity if young (0..6)
        4) wm_gate_thr        : base gate threshold on unsigned PE (0..1); effective gate rises with set size
        5) age_capacity_drop  : multiplicative factor (<1) applied to capacity if old

    Inputs
    - states, actions, rewards, blocks, set_sizes: arrays of equal length
    - age: array with single value; age<=45 -> young(0), >45 -> old(1)

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha0, kappa, beta_rl, wm_capacity_young, wm_gate_thr, age_capacity_drop = model_parameters
    beta_rl = beta_rl * 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        bsz = int(set_sizes[mask][0])

        nA = 3
        nS = bsz

        # RL values and adaptive learning-rate trace per state-action
        Q = (1.0 / nA) * np.ones((nS, nA))
        att = np.zeros((nS, nA))  # Pearce–Hall attention proxy (tracks unsigned PE)

        # WM store: mapping from state -> stored action; with LRU order
        stored_action = -1 * np.ones(nS, dtype=int)  # -1 means not stored
        last_used_rank = np.zeros(nS)  # for LRU ordering, larger means more recent
        tstamp = 0.0

        # WM capacity depends on age
        cap = wm_capacity_young if age_group == 0 else max(0.0, wm_capacity_young * age_capacity_drop)
        cap = int(np.floor(min(max(cap, 0.0), float(nS))))

        for t in range(len(bs)):
            s = int(bs[t])
            a = int(ba[t])
            r = float(br[t])
            tstamp += 1.0

            # Determine whether state is currently in WM
            in_wm = (stored_action[s] >= 0)
            if in_wm:
                # WM softmax probability of chosen action (near-deterministic on stored action)
                W_s = np.full(nA, 0.0)
                W_s[stored_action[s]] = 1.0
                denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
                p_wm = 1.0 / max(denom_wm, eps)
                p_total = max(p_wm, eps)
                total_log_p += np.log(p_total)
            else:
                # RL softmax when WM does not contain this state
                Q_s = Q[s, :]
                denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
                p_rl = 1.0 / max(denom_rl, eps)
                p_total = max(p_rl, eps)
                total_log_p += np.log(p_total)

            # RL update with adaptive learning-rate (Pearce–Hall)
            pe = r - Q[s, a]
            att[s, a] = (1.0 - kappa) * att[s, a] + kappa * abs(pe)
            alpha_adapt = np.clip(alpha0 * (0.5 + att[s, a]), 0.0, 1.0)  # attention boosts LR
            Q[s, a] += alpha_adapt * pe

            # WM storage gating: store when unsigned PE exceeds a load-adjusted threshold.
            # Larger set size increases the effective threshold (harder to store under load).
            gate_thr_eff = np.clip(wm_gate_thr + 0.15 * max(0, nS - 3), 0.0, 1.0)
            if abs(pe) >= gate_thr_eff:
                # Store into WM: put the chosen action for this state.
                if cap > 0:
                    if stored_action[s] < 0:
                        # Need space: evict least-recently used if full
                        currently_stored = np.sum(stored_action >= 0)
                        if currently_stored >= cap:
                            # find LRU across currently stored states
                            candidates = np.where(stored_action >= 0)[0]
                            lru_state = candidates[np.argmin(last_used_rank[candidates])]
                            stored_action[lru_state] = -1
                            last_used_rank[lru_state] = 0.0
                    stored_action[s] = a
                    last_used_rank[s] = tstamp

            # If the state is used (by WM or not), refresh its recency if it is stored
            if stored_action[s] >= 0:
                last_used_rank[s] = tstamp

    return -float(total_log_p)


Model 2: WM–RL mixture with decaying WM and intrinsic exploration bonus; age modulates exploration
- Idea: RL softmax is augmented by an intrinsic exploration bonus based on novelty (1/sqrt(visit count)). WM holds a probabilistic table that decays toward uniform, with a near-deterministic policy; the mixture weight is constant across load, but WM decays faster under higher load (no new parameter). Age reduces the intrinsic exploration drive.
- Parameters (6):
  0) lr_rl: RL learning rate (0..1)
  1) beta_rl_base: RL inverse temperature base (>0), scaled by 10
  2) wm_weight_base: mixture weight for WM (0..1)
  3) beta_wm_scale: scales WM inverse temperature (>0)
  4) explore_bonus: intrinsic bonus magnitude for novelty (>=0)
  5) age_explore_factor: multiplicative factor on exploration for older group (0..1). If young, factor=1.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    WM–RL mixture with load-sensitive WM decay and an intrinsic exploration bonus for RL.
    Age reduces intrinsic exploration; WM is near-deterministic but decays faster under higher load.

    Parameters
    - model_parameters (len=6):
        0) lr_rl             : RL learning rate (0..1)
        1) beta_rl_base      : RL inverse temperature base (>0), scaled by 10
        2) wm_weight_base    : WM mixture weight (0..1), constant across loads
        3) beta_wm_scale     : scales WM inverse temperature (>0); WM beta = 10*beta_wm_scale
        4) explore_bonus     : novelty bonus magnitude added to Q (>=0)
        5) age_explore_factor: factor (<1) multiplying exploration if old; if young -> 1

    Mechanisms
    - RL: Q-learning with softmax; adds novelty bonus b/sqrt(N_s,a) to action values.
    - WM: probability table w decays toward uniform; when rewarded, overwrite to near-delta on chosen action.
      Decay speed increases with set size (interference).
    - Policy: mixture p = w * p_wm + (1-w) * p_rl.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_rl, beta_rl_base, wm_weight_base, beta_wm_scale, explore_bonus, age_explore_factor = model_parameters
    beta_rl = beta_rl_base * 10.0
    beta_wm = max(1e-6, beta_wm_scale) * 10.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    # Age-modulated exploration strength
    age_factor = 1.0 if age_group == 0 else np.clip(age_explore_factor, 0.0, 1.0)

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        bsz = int(set_sizes[mask][0])

        nA = 3
        nS = bsz

        Q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_uniform = (1.0 / nA) * np.ones((nS, nA))

        # State-action visit counts for novelty bonus
        N = np.ones((nS, nA))  # start at 1 to avoid div by zero

        # WM decay increases with load (no extra parameter)
        wm_decay = np.clip(0.05 + 0.15 * max(0, nS - 3), 0.0, 1.0)
        wm_w_block = np.clip(wm_weight_base, 0.0, 1.0)

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL with novelty bonus
            novelty = (explore_bonus * age_factor) / np.sqrt(max(1.0, N[s, a]))
            Q_s = Q[s, :].copy()
            # Add novelty bonuses per action for policy, not to stored Q-table
            bonuses = (explore_bonus * age_factor) / np.sqrt(N[s, :])
            Q_eff = Q_s + bonuses

            denom_rl = np.sum(np.exp(beta_rl * (Q_eff - Q_eff[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # Update RL Q and counts
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe
            N[s, a] += 1.0

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_uniform
            # On reward, overwrite row to near-delta
            if r > 0.5:
                w[s, :] = 1e-6
                w[s, a] = 1.0 - (nA - 1) * 1e-6

    return -float(total_log_p)


Model 3: Reliability-based arbitration between RL and WM
- Idea: Arbitration weight is computed dynamically from reliability traces of WM and RL. Reliability tracks recent predictive success via exponential moving averages. Set size penalizes WM reliability, and age biases arbitration toward or away from WM depending on parameter. WM is probabilistic with decay; RL is standard Q-learning.
- Parameters (6):
  0) lr_rl: RL learning rate (0..1)
  1) beta_rl: RL inverse temperature (>0), scaled by 10
  2) wm_learn: probability/intensity for WM overwrite on reward (0..1)
  3) wm_decay: WM decay toward uniform (0..1)
  4) arb_beta: arbitration inverse temperature mapping reliabilities to weights (>0)
  5) age_wm_bias: additive bias to WM reliability if young; reversed sign if old

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Reliability-based arbitration between RL and WM with load and age effects.
    WM: probability table decays; rewarded trials push WM toward chosen action (scaled by wm_learn).
    RL: Q-learning with softmax.
    Arbitration: mixture weight computed from running reliabilities of WM vs RL. WM reliability is penalized
    by set size; age adds a bias favoring WM if young or disfavors if old.

    Parameters
    - model_parameters (len=6):
        0) lr_rl       : RL learning rate (0..1)
        1) beta_rl     : RL inverse temperature (>0), internally scaled by 10
        2) wm_learn    : WM overwrite strength on reward (0..1)
        3) wm_decay    : WM decay toward uniform per trial (0..1)
        4) arb_beta    : inverse temperature for arbitration (>0)
        5) age_wm_bias : bias added to WM reliability if young; subtracted if old

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_rl, beta_rl, wm_learn, wm_decay, arb_beta, age_wm_bias = model_parameters
    beta_rl = beta_rl * 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    # Age bias term (positive for young, negative for old)
    age_bias_term = age_wm_bias if age_group == 0 else -abs(age_wm_bias)

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        bsz = int(set_sizes[mask][0])

        nA = 3
        nS = bsz

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        W_uniform = (1.0 / nA) * np.ones((nS, nA))

        # Running reliabilities (exponentially smoothed prediction success)
        rel_wm = 0.5
        rel_rl = 0.5
        rel_alpha = 0.2  # internal smoothing constant for reliabilities

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # Compute action likelihoods under RL and WM for arbitration
            Q_s = Q[s, :]
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = W[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration weight from reliabilities
            # Penalize WM reliability by load: larger sets reduce WM effective reliability
            load_penalty = 0.2 * max(0, nS - 3)  # 0 for size=3, 0.6 for size=6
            wm_rel_eff = rel_wm - load_penalty + age_bias_term
            rl_rel_eff = rel_rl

            # Softmax over reliabilities to get mixture weight
            m_wm = np.exp(arb_beta * wm_rel_eff)
            m_rl = np.exp(arb_beta * rl_rel_eff)
            wm_weight = m_wm / max(m_wm + m_rl, eps)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # Update RL
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe

            # Update WM: decay toward uniform; on reward, move toward a delta on chosen action
            W = (1.0 - wm_decay) * W + wm_decay * W_uniform
            if r > 0.5:
                # Blend current row toward a near-delta on chosen action with strength wm_learn
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = (1.0 - wm_learn) * W[s, :] + wm_learn * target

            # Update reliabilities based on predictive success of each system
            # Success signals: likelihood accorded by each system to the observed action times outcome
            # Use log-likelihood transformed to bounded success via sigmoid-like mapping
            succ_rl = p_rl if r > 0.5 else (1.0 - p_rl)
            succ_wm = p_wm if r > 0.5 else (1.0 - p_wm)
            rel_rl = (1.0 - rel_alpha) * rel_rl + rel_alpha * succ_rl
            rel_wm = (1.0 - rel_alpha) * rel_wm + rel_alpha * succ_wm

    return -float(total_log_p)