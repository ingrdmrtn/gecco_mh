def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + WM with RPE-gated WM write-in and set-size/age-sensitive gating
    - RL: delta rule with single learning rate.
    - WM: fast supervised memory that stores rewarded action with a confidence weight.
      Negative feedback causes decay toward uniform.
    - Gating: the WM mixture weight is dynamically gated by the unsigned RPE on each trial,
      attenuated by set size and age. Larger set sizes and older age reduce WM reliance.
    - Policy: mixture of RL and WM policies.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - base_wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - gate_k: sensitivity of WM gating to unsigned RPE (>=0). Larger => more WM when surprised.
    - wm_confidence: WM write-in strength toward one-hot on positive reward (0..1).
    - age_gate_shift: age-dependent increase in set-size penalty for WM gating (>=0). Added if old, subtracted if young.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, base_wm_weight, softmax_beta, gate_k, wm_confidence, age_gate_shift = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age- and set-size-sensitive penalty on WM gating
        ss_penalty = (nS - 1) / max(1, nS - 1)  # 0 for nS=1, ~1 for nS>=2
        age_penalty = age_gate_shift if age_group == 1 else -age_gate_shift
        ss_age_penalty = np.clip(1.0 + ss_penalty * (0.5 + age_penalty), 0.1, 10.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RPE-gated WM mixture (unsigned RPE, scaled and penalized by set size and age)
            rpe = abs(r - Q_s[a])
            gate = 1.0 - np.exp(-gate_k * rpe / ss_age_penalty)
            wm_mix_weight = np.clip(base_wm_weight * gate, 0.0, 1.0)

            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If rewarded: move toward one-hot with confidence.
            # - If not rewarded: decay toward uniform (erasing the hypothesis).
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_confidence) * w[s, :] + wm_confidence * one_hot
            else:
                decay = wm_confidence  # reuse as decay strength on errors
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL + WM with entropy-based arbitration and age/set-size dependent temperatures
    - RL: delta rule with inverse temperature beta_rl.
    - WM: fast supervised learner with learning rate wm_alpha; deterministic policy with beta_wm that degrades with set size.
    - Arbitration: compute RL and WM choice entropies; higher certainty gets more weight.
      WM certainty degrades with set size penalty. Age increases RL temperature (noisier) and reduces WM effective beta.
    - Policy: mixture of RL and WM based on normalized inverse entropies.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - beta_rl: baseline RL inverse temperature (scaled internally by 10).
    - beta_wm_base: baseline WM inverse temperature before set-size/age effects.
    - wm_alpha: WM learning rate toward one-hot if rewarded; anti-learning on errors (0..1).
    - age_rl_temp_boost: increases RL temperature for old, decreases for young (>=0).
    - setsize_wm_penalty: penalizes WM determinism per additional state (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, beta_wm_base, wm_alpha, age_rl_temp_boost, setsize_wm_penalty = model_parameters
    beta_rl *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted temperatures
        if age_group == 1:
            beta_rl_eff = max(1e-3, beta_rl / (1.0 + age_rl_temp_boost))  # older -> noisier RL
            beta_wm_eff = max(1e-3, beta_wm_base / (1.0 + setsize_wm_penalty * max(0, nS - 1) * (1.0 + age_rl_temp_boost)))
        else:
            beta_rl_eff = beta_rl * (1.0 + age_rl_temp_boost)  # younger -> sharper RL if boost>0
            beta_wm_eff = max(1e-3, beta_wm_base / (1.0 + setsize_wm_penalty * max(0, nS - 1) * max(1e-3, 1.0 - age_rl_temp_boost)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Full distributions for entropy calculation
            prl_vec = np.exp(beta_rl_eff * (Q_s - np.max(Q_s)))
            prl_vec = prl_vec / np.sum(prl_vec)
            pwm_vec = np.exp(beta_wm_eff * (W_s - np.max(W_s)))
            pwm_vec = pwm_vec / np.sum(pwm_vec)

            # Entropies
            H_rl = -np.sum(np.clip(prl_vec, eps, 1.0) * np.log(np.clip(prl_vec, eps, 1.0)))
            H_wm = -np.sum(np.clip(pwm_vec, eps, 1.0) * np.log(np.clip(pwm_vec, eps, 1.0)))

            # Arbitration weight proportional to inverse entropy
            invH_rl = 1.0 / max(eps, H_rl)
            invH_wm = 1.0 / max(eps, H_wm)
            wm_mix_weight = invH_wm / (invH_wm + invH_rl)
            wm_mix_weight = np.clip(wm_mix_weight, 0.0, 1.0)

            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: supervised with anti-learning on errors
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                anti = np.ones(nA) / (nA - 1)
                anti[a] = 0.0
                # Renormalize anti vector
                anti = anti / np.sum(anti)
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with asymmetric learning + capacity-limited WM with surprise-based gating
    - RL: delta rule with separate learning rates for positive and negative outcomes.
    - WM: limited number of state "slots" (wm_slots) can be actively maintained; others default to uniform.
      Slots are assigned to states that recently produced high surprise (|RPE|) on reward.
      WM contents decay (forget) each trial toward uniform.
    - Arbitration: WM mixture proportional to fraction of available slots over set size.
      Age reduces effective capacity and increases forgetting.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive rewards (0..1).
    - lr_neg: RL learning rate for zero rewards (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_slots: nominal number of WM slots available (>=0).
    - wm_forget: per-trial WM forgetting rate toward uniform (0..1).
    - age_capacity_penalty: reduces effective slots and increases forgetting in older adults (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_slots, wm_forget, age_capacity_penalty = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and forgetting with age
        if age_group == 1:
            eff_slots = max(0.0, wm_slots - age_capacity_penalty)
            eff_forget = np.clip(wm_forget + 0.5 * age_capacity_penalty, 0.0, 1.0)
        else:
            eff_slots = max(0.0, wm_slots + age_capacity_penalty)
            eff_forget = np.clip(wm_forget - 0.5 * age_capacity_penalty, 0.0, 1.0)

        # Track which states occupy WM slots
        in_slots = np.zeros(nS, dtype=bool)
        # Priority values for slot assignment (higher = more priority)
        priorities = np.zeros(nS)

        # Mixture weight based on fraction of states covered by WM
        def current_wm_weight():
            covered = np.sum(in_slots)
            return np.clip(covered / max(1, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_mix_weight = current_wm_weight()
            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta > 0 else lr_neg
            q[s, a] += lr_use * delta

            # Global WM forgetting toward uniform each trial
            w = (1.0 - eff_forget) * w + eff_forget * w_0

            # Surprise-based WM slot assignment on rewarded trials
            if r > 0:
                surprise = abs(delta)
                priorities[s] = surprise  # latest surprise score
                # Assign/maintain slot if high priority
                # If not enough slots, take top-k by priority
                k = int(min(nS, np.floor(eff_slots)))
                if k > 0:
                    top_idx = np.argsort(-priorities)[:k]
                    in_slots[:] = False
                    in_slots[top_idx] = True
                else:
                    in_slots[:] = False

                # If this state is in WM, write strong one-hot
                if in_slots[s]:
                    one_hot = np.zeros(nA)
                    one_hot[a] = 1.0
                    w[s, :] = one_hot
            else:
                # On errors, do not assign slots; just let forgetting erode incorrect associations
                pass

        blocks_log_p += log_p

    return -blocks_log_p