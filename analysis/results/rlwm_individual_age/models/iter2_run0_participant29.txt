def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM contribution.

    Idea:
    - Choices arise from a mixture of a model-free RL system and a working-memory (WM) system.
    - WM has a stronger impact in low set size and for younger participants.
    - WM traces decay toward uniform each trial; rewarded actions refresh WM toward a one-hot code.
    - RL updates via a standard delta rule.
    - Action selection is a probability mixture of RL and WM softmax policies.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base weight of WM contribution in the mixture (0..1)
    - softmax_beta: inverse temperature for RL softmax (scaled by 10 internally)
    - wm_decay: per-trial decay of WM toward uniform (0..1)
    - capacity_kappa: WM capacity anchor that modulates how WM_weight scales with set size
    - age_wm_delta: age modulation of WM scale; increases WM if young, decreases if old

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: block set size per trial
    - age: scalar repeated array; age_group=0 if <=45, else 1
    Returns:
    - Negative log-likelihood of the observed choices under the model
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity_kappa, age_wm_delta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy
    eps = 1e-12

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM table

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective WM weight scaled by set size (capacity) and age
            # Larger capacity_kappa -> less drop in WM with larger set size
            # WM scaling function: sigmoid of (capacity_kappa - nS)
            wm_scale_set = 1.0 / (1.0 + np.exp(nS - capacity_kappa))
            # Age modulation: young -> increase by age_wm_delta, old -> decrease by age_wm_delta
            wm_scale_age = (1.0 + age_wm_delta) if age_group == 0 else (1.0 - age_wm_delta)
            wm_weight_eff = np.clip(wm_weight * wm_scale_set * wm_scale_age, 0.0, 1.0)

            # RL policy: probability of chosen action via softmax
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over WM weights
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)
            # 2) if rewarded, refresh toward a one-hot
            if r > 0.5:
                # Move current state's WM row toward a one-hot on chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Use (1 - wm_decay) here to re-use the same param as "refresh strength"
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with state-specific perseveration and set-size-scaled WM contribution.
    Age modulates WM decay (older -> faster decay).

    Idea:
    - Choices are a mixture of RL and WM.
    - WM decays toward uniform and is refreshed by reward.
    - A state-specific perseveration bias increases the value of repeating the last action in that state.
    - WM contribution is penalized in larger set sizes by a power-law (3/nS)^alpha.
    - Older participants have faster WM decay (less stable WM).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM weight in the mixture (0..1)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_decay_base: base WM decay (0..1)
    - perseveration_bias: additive bias applied to the last chosen action in a state (in value units)
    - size_exponent: exponent alpha for WM size penalty: wm_weight_eff *= (3/nS)^alpha

    Inputs/Outputs as in cognitive_model1.
    Returns negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, perseveration_bias, size_exponent = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size dependent WM scaling (3/nS)^alpha
        wm_scale_size = (3.0 / float(nS)) ** size_exponent
        wm_weight_eff = np.clip(wm_weight * wm_scale_size, 0.0, 1.0)

        # Age-modulated WM decay: older -> faster decay (1 + 0.5*age_group) factor
        wm_decay = np.clip(wm_decay_base * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply perseveration bias by adding to preference vectors before softmax
            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()
            la = int(last_action[s])
            if la >= 0:
                Q_s[la] += perseveration_bias
                W_s[la] += perseveration_bias

            # Policies
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # Updates
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)
            # Rewarded refresh toward one-hot
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + one-trial WM store, with age- and set-size-modulated RL learning rate.

    Idea:
    - RL has both learning and decay (forgetting) toward a baseline (uniform).
    - WM stores the last rewarded action per state (one-trial memory), decaying otherwise.
    - WM has a fixed mixture weight, but RL learning rate adapts to set size and age:
      larger set sizes and older age reduce the effective RL learning rate.

    Parameters (model_parameters):
    - lr_base: base RL learning rate (0..1)
    - wm_weight: weight of WM in choice mixture (0..1)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - rl_forget: RL forgetting rate per visit (0..1) toward uniform
    - size_lr_slope: how much set size (nS-3) reduces lr (can be positive)
    - age_lr_delta: multiplicative lr reduction for older group (0..1); applied if age_group==1

    Inputs/Outputs as in cognitive_model1.
    Returns negative log-likelihood.
    """
    lr_base, wm_weight, softmax_beta, rl_forget, size_lr_slope, age_lr_delta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL learning rate reduced by set size and age
        lr_eff = lr_base / (1.0 + size_lr_slope * max(0, nS - 3))
        if age_group == 1:
            lr_eff *= (1.0 - age_lr_delta)
        lr_eff = np.clip(lr_eff, 0.0, 1.0)

        # WM weight is kept constant across conditions here
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Policies
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr_eff * delta
            # Forgetting on entire state's Q toward uniform baseline
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # WM update: one-trial store of rewarded action with decay otherwise
            # Decay WM row toward uniform slightly each visit (couple to rl_forget to keep parameters <=6)
            wm_decay = np.clip(rl_forget, 0.0, 1.0)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p