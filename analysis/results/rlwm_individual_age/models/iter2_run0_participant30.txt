def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with visit-based gating arbitration modulated by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_gate_thresh, softmax_beta, wm_lr, age_gate_bias, size_gate_bias]
        - lr: RL learning rate (0..1)
        - wm_gate_thresh: threshold controlling reliance on WM as a function of state visit count
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10
        - wm_lr: WM learning rate (0..1)
        - age_gate_bias: positive values reduce WM reliance for older adults
        - size_gate_bias: positive values reduce WM reliance for larger set sizes

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_gate_thresh, softmax_beta, wm_lr, age_gate_bias, size_gate_bias = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Visit-based gating: rely more on WM early, less as state is visited,
            # and reduce WM reliance for larger set sizes and for older age group.
            size_term = size_gate_bias * max(0, nS - 3)
            gate_input = wm_gate_thresh - visits[s] - size_term - age_gate_bias * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))  # sigmoid

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: local, fast associative update toward observed outcome,
            # with slight competitive normalization across actions
            pred = W_s[a]
            w[s, a] += wm_lr * (r - pred)
            # push non-chosen actions slightly away when reward is 1 (sharpening)
            if r > 0.0:
                others = [x for x in range(nA) if x != a]
                w[s, others] -= wm_lr * (r - pred) / (len(others) + 1e-12)
                # keep probabilities bounded within [0,1] before implicit softmax use
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

            visits[s] += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM slots with FIFO replacement and lapse, modulated by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, softmax_beta, wm_K_base, wm_beta, age_slot_penalty, lapse]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10
        - wm_K_base: baseline WM slot capacity (0..6)
        - wm_beta: WM inverse temperature for stored items (>0); high is near-deterministic
        - age_slot_penalty: capacity reduction applied if age_group=1
        - lapse: stimulus-independent random choice probability (0..0.2)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_K_base, wm_beta, age_slot_penalty, lapse = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = wm_beta  # use parameterized WM precision

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity depends on set size and age
        K_eff = wm_K_base - age_slot_penalty * age_group - max(0, nS - 3)
        K_eff = int(max(0, min(nS, np.round(K_eff))))

        # Track which states are in WM and maintain FIFO order
        in_wm = np.zeros(nS, dtype=bool)
        fifo = []

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if state is stored, use high-precision WM; else near-uniform
            if in_wm[s]:
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                # if not stored, WM offers no guidance: uniform-like
                p_wm = 1.0 / nA

            # Mixture weight: deterministic gating by storage (1 if stored else 0)
            wm_weight = 1.0 if in_wm[s] else 0.0

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update and storage policy:
            # When a rewarded association is observed, store (or refresh) the state-action mapping.
            if r > 0.5:
                # Create a peaked pattern for the chosen action
                w[s, :] = w_0[s, :]  # reset row to neutral before setting a strong trace
                w[s, a] = 1.0

                if not in_wm[s]:
                    # Add to WM; if full, evict oldest
                    if K_eff > 0:
                        if len(fifo) >= K_eff:
                            s_out = fifo.pop(0)
                            in_wm[s_out] = False
                            w[s_out, :] = w_0[s_out, :]
                        fifo.append(s)
                        in_wm[s] = True
                else:
                    # Refresh by moving to the end of the queue
                    if s in fifo:
                        fifo.remove(s)
                        fifo.append(s)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and WM decay; age and set size modulate arbitration.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, arb_sensitivity, age_temp_noise]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline tendency to rely on WM (0..1)
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10
        - wm_decay: decay of WM toward prior each trial (0..1)
        - arb_sensitivity: sensitivity of arbitration to confidence/uncertainty difference (>0)
        - age_temp_noise: increases RL stochasticity (reduces effective beta) when age_group=1

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, arb_sensitivity, age_temp_noise = model_parameters
    softmax_beta *= 10.0  # base scaling

    age_group = 0 if age[0] <= 45 else 1
    # Age increases effective decision noise for RL
    beta_eff = max(1e-6, softmax_beta / (1.0 + age_temp_noise * age_group))

    softmax_beta_wm = 50.0  # very deterministic WM policy
    # Convert baseline WM weight to logit for use as an arbitration bias term
    eps = 1e-6
    wm_bias = np.log((np.clip(wm_weight_base, eps, 1 - eps)) / (1 - np.clip(wm_weight_base, eps, 1 - eps)))

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy and uncertainty (entropy of RL softmax)
            Q_s = q[s, :]
            probs_rl = np.exp(beta_eff * (Q_s - np.max(Q_s)))
            probs_rl = probs_rl / np.sum(probs_rl)
            p_rl = probs_rl[a]
            rl_entropy = -np.sum(probs_rl * np.log(np.clip(probs_rl, 1e-12, 1.0)))

            # WM policy and confidence (margin between top two WM values)
            W_s = w[s, :]
            probs_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            probs_wm = probs_wm / np.sum(probs_wm)
            p_wm = probs_wm[a]
            sorted_w = np.sort(W_s)
            wm_conf = (sorted_w[-1] - sorted_w[-2]) if nA > 1 else 0.0

            # Arbitration: compare WM confidence vs RL uncertainty,
            # penalize WM under larger set size and for older age.
            size_penalty = (nS - 3)  # 0 for size=3, 3 for size=6
            arb_input = wm_bias + arb_sensitivity * (wm_conf - rl_entropy - size_penalty - 0.0 * rl_entropy) - 0.0
            # Age reduces effective WM reliance by shifting the arbitration input
            arb_input -= arb_sensitivity * age_group

            wm_weight = 1.0 / (1.0 + np.exp(-arb_input))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward neutral, then local update toward observed outcome
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            w[s, a] += (r - w[s, a])  # fast overwrite-like local update

        blocks_log_p += log_p

    return -blocks_log_p