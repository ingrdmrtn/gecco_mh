def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and capacity-limited WM; arbitration by RL uncertainty.

    Mechanisms
    - RL: model-free Q-learning with eligibility traces.
    - WM: capacity-limited mapping that stores rewarded state-action pairs and decays toward uniform.
    - Arbitration: WM weight increases with RL uncertainty (entropy), and decreases with set size and age.
    
    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, softmax_beta, wm_capacity, trace_lambda, arb_temp, age_wm_shift]
        - lr: RL learning rate for TD error.
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - wm_capacity: Effective WM item slots (0..6), scales as wm_capacity/nS.
        - trace_lambda: Eligibility trace decay (also used to set WM decay magnitude).
        - arb_temp: Sensitivity of WM arbitration to RL entropy (higher -> more WM when RL uncertain).
        - age_wm_shift: Additive shift for WM contribution by age; negative penalizes older group more.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_capacity, trace_lambda, arb_temp, age_wm_shift = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # WM distributions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute constants per block
        # WM capacity-based baseline weight (scaled by set size and age)
        cap_ratio = np.clip(wm_capacity / max(nS, 1.0), 0.0, 1.0)
        wm_base_weight = np.clip(cap_ratio * (1.0 - 0.35 * age_group + age_wm_shift), 0.0, 1.0)

        # WM decay grows with set size and age, tied to (1 - lambda)
        wm_decay = np.clip((1.0 - trace_lambda) * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL action probabilities
            Q_s = q[s, :]
            # Compute RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL policy entropy (base of e; use e=exp(1) for normalization)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pol = np.exp(logits) / np.sum(np.exp(logits))
            eps = 1e-12
            H_rl = -np.sum(pol * np.log(np.clip(pol, eps, 1.0))) / np.log(np.e)  # in nats, 0..~1.1

            # WM policy from current w
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: weight increases with RL uncertainty
            wm_weight_eff = wm_base_weight * (1.0 / (1.0 + np.exp(-arb_temp * (H_rl - 0.5))))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with eligibility traces
            pe = r - q[s, a]
            # Decay traces and set chosen trace to 1 for current (s,a)
            e *= trace_lambda
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += lr * pe * e

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM strengthening on reward; step size tied to capacity
            if r > 0.0:
                eta_wm = cap_ratio
                # Move mass toward chosen action and renormalize
                w[s, :] = (1.0 - eta_wm) * w[s, :]
                w[s, a] += eta_wm
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian RL (Beta-Bernoulli) with decaying WM; fixed arbitration by set size and age.

    Mechanisms
    - RL: Each state-action has a Beta(alpha, beta) success model; Q = alpha / (alpha + beta).
    - WM: A fast, decay-prone store of recently rewarded actions per state, decaying to uniform.
    - Arbitration: Fixed mixture weight modulated by set size and age.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [prior_alpha, prior_beta, softmax_beta, wm_weight_base, decay_per_trial, age_decay_mult]
        - prior_alpha: Prior successes for Beta RL.
        - prior_beta: Prior failures for Beta RL.
        - softmax_beta: RL inverse temperature (scaled by 10 internally).
        - wm_weight_base: Baseline WM mixture weight before set-size/age modulation.
        - decay_per_trial: Baseline WM decay toward uniform per visit (0..1).
        - age_decay_mult: Multiplier for decay when age_group=1 (older).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    prior_alpha, prior_beta, softmax_beta, wm_weight_base, decay_per_trial, age_decay_mult = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Beta parameters
        alpha_mat = prior_alpha * np.ones((nS, nA))
        beta_mat = prior_beta * np.ones((nS, nA))

        # WM distributions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age-dependent weights/decay
        wm_weight_eff = wm_weight_base * (3.0 / float(nS)) * (1.0 - 0.4 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)
        wm_decay_eff = np.clip(decay_per_trial * (nS / 3.0) * (1.0 + age_decay_mult * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL Q-values from Beta posterior means
            al = alpha_mat[s, :]
            be = beta_mat[s, :]
            Q_s = al / np.maximum(al + be, 1e-12)

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL Beta updates
            alpha_mat[s, a] += r
            beta_mat[s, a] += (1.0 - r)

            # WM decay and reinforcement on reward
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0.0:
                # Drive toward chosen action
                w[s, :] = 0.1 * w[s, :]  # shrink others
                w[s, a] += 0.9
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with exploration bonus (UCB-like) and PE-gated WM arbitration.

    Mechanisms
    - RL: Q-learning plus an uncertainty bonus that diminishes with action visit count.
    - WM: Reward-contingent mapping per state that decays slowly to uniform.
    - Arbitration: WM weight increases when recent prediction error magnitude is small (confident),
      decreases when large (learning phase). Set size reduces WM contribution; older age reduces RL beta.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, softmax_beta, wm_weight_base, bonus_ucb, pe_sensitivity, age_beta_penalty]
        - lr: RL learning rate.
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - wm_weight_base: Baseline WM mixture weight before set-size scaling.
        - bonus_ucb: Magnitude of exploration bonus applied to less-visited actions.
        - pe_sensitivity: Sensitivity shaping WM weight by |PE| (higher -> less WM when PE large).
        - age_beta_penalty: Factor reducing RL beta for older adults.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, bonus_ucb, pe_sensitivity, age_beta_penalty = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    # Age reduces RL inverse temperature
    softmax_beta_eff_global = softmax_beta / (1.0 + age_beta_penalty * age_group)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))  # visit counts for UCB bonus

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling for WM base weight and exploration bonus
        wm_weight_scale = (3.0 / float(nS))
        wm_weight_block = np.clip(wm_weight_base * wm_weight_scale * (1.0 - 0.3 * age_group), 0.0, 1.0)
        bonus_scale = (nS / 3.0)

        # Modest WM decay
        wm_decay = 0.05 * (nS / 3.0) * (1.0 + 0.3 * age_group)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0.0
        prev_pe = 0.0  # initialize PE for PE-gated arbitration
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with UCB-like exploration bonus
            Q_s = q[s, :].copy()
            bonus = bonus_ucb * bonus_scale / np.sqrt(counts[s, :] + 1.0)
            Qb_s = Q_s + bonus

            p_rl = 1.0 / np.sum(np.exp(softmax_beta_eff_global * (Qb_s - Qb_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # PE-gated WM arbitration: more WM when |PE| small
            gate = 1.0 / (1.0 + np.exp(pe_sensitivity * (abs(prev_pe) - 0.5)))
            wm_weight_eff = np.clip(wm_weight_block * gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update and count
            pe = r - q[s, a]
            q[s, a] += lr * pe
            counts[s, a] += 1.0
            prev_pe = pe

            # WM decay and reward-contingent update
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.15 * w[s, :]
                w[s, a] += 0.85
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p