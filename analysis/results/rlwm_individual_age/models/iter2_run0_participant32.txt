def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility-trace) + capacity-limited WM with interference and lapses.
    
    Idea
    - RL: Q-learning with replacing eligibility traces that propagate recent
      prediction errors across recent state-action visits (captures rapid within-block
      learning bursts).
    - WM: One-shot write on rewarded trials; decays and suffers interference that grows
      with set size. WM contributes a deterministic policy, mixed with RL.
    - Decision: Softmax for RL; WM is near-deterministic; a small lapse probability
      increases with set size and (more) with older age.
    
    Parameters
    ----------
    model_parameters : [alpha, beta, wm_weight, lambda_tr, rho_interf, lapse_base]
        alpha : base learning rate for RL (0..1)
        beta : base inverse temperature for RL (rescaled internally)
        wm_weight : baseline weight of WM mixture (0..1)
        lambda_tr : eligibility trace decay (0..1)
        rho_interf : WM interference strength (0..1), grows with set size
        lapse_base : base lapse probability (0..0.2)
    
    Age and Set-Size Effects
    - Age group: 0 if <=45 else 1. Older age increases lapses and interference,
      and slightly reduces beta.
    - Set size: increases interference and lapse probability, reduces effective WM weight.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_weight, lambda_tr, rho_interf, lapse_base = model_parameters
    beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    # Age modulation
    beta *= (1.0 - 0.1 * age_group)             # older slightly noisier in RL choice
    lapse_age = lapse_base * (1.0 + 0.5 * age_group)
    rho_age = rho_interf * (1.0 + 0.3 * age_group)

    softmax_beta_wm = 50.0  # near-deterministic WM choice

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size effects on mixture and lapse
            size_factor = max(0, nS - 3) / 3.0  # 0 for 3, 1 for 6
            lapse_t = np.clip(lapse_age * (1.0 + 0.5 * size_factor), 0.0, 0.3)
            rho_t = np.clip(rho_age * (1.0 + size_factor), 0.0, 1.0)
            wm_weight_eff = np.clip(wm_weight / (1.0 + size_factor), 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (near-deterministic softmax)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with lapses to uniform
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse_t) * p_mix + lapse_t * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces (gamma=1 within block)
            delta = r - Q_s[a]
            e *= lambda_tr
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += alpha * delta * e

            # WM dynamics: decay to baseline + interference; update on reward
            # Interference pulls WM towards uniform (loss of distinctiveness at higher set size)
            decay = 0.1 + 0.2 * size_factor  # 0.1 at 3, 0.3 at 6
            w = (1.0 - decay) * w + decay * w_0
            w = (1.0 - rho_t) * w + rho_t * (1.0 / nA)  # interference

            if r > 0.0:
                # One-shot reinforcement of chosen action
                write = 0.6
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy-adaptive meta-control and gated WM with decay.
    
    Idea
    - RL: standard Q-learning.
    - Meta-control: when RL policy is uncertain (high entropy), shift weight toward WM;
      when confident, rely more on RL. Adaptation strength is reduced for older adults.
    - WM: stores rewarded actions in a state-specific map; decays each trial and more
      under higher set size. WM contributes a near-deterministic policy.
    
    Parameters
    ----------
    model_parameters : [alpha, beta_base, wm_base, adapt_gain, wm_decay, age_meta]
        alpha : RL learning rate
        beta_base : base inverse temperature for RL (scaled internally)
        wm_base : baseline WM weight (0..1)
        adapt_gain : strength of entropy-based adaptation (0..2)
        wm_decay : baseline WM decay toward uniform per trial (0..1)
        age_meta : age effect on meta-control and decay (>0 increases cost for older)
    
    Age and Set-Size Effects
    - Age group: 0 if <=45 else 1. For older, adaptation is weaker and decay is stronger.
    - Set size: reduces effective WM by increasing decay and adding noise to beta.
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_base, wm_base, adapt_gain, wm_decay, age_meta = model_parameters
    beta_base *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0

    # Age adjustments
    adapt_gain_eff = adapt_gain * (1.0 - 0.4 * age_group)          # less adaptive if older
    wm_decay_age = wm_decay * (1.0 + 0.5 * age_group)              # more decay if older
    beta_age = beta_base * (1.0 - 0.1 * age_group)                 # slightly noisier RL if older

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            size_factor = max(0, nS - 3) / 3.0
            beta_eff = beta_age / (1.0 + 0.4 * size_factor)  # noisier RL when set size is large

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy and entropy of policy
            logits = beta_eff * (Q_s - np.max(Q_s))
            pi = np.exp(logits) / np.sum(np.exp(logits))
            p_rl = max(pi[a], 1e-12)
            entropy = -np.sum(pi * np.log(np.clip(pi, 1e-12, 1.0)))  # 0..log(nA)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Entropy-adaptive WM weight (bounded in [0,1])
            # Normalize entropy by log(nA) to get [0,1]
            ent_norm = entropy / np.log(nA)
            wm_weight_eff = wm_base * (1.0 - 0.5 * size_factor)  # load reduces baseline WM
            wm_weight_eff = np.clip(wm_weight_eff + adapt_gain_eff * (ent_norm - 0.5), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay and gated write
            decay_t = np.clip(wm_decay_age * (1.0 + 0.5 * size_factor), 0.0, 0.9)
            w[s, :] = (1.0 - decay_t) * w[s, :] + decay_t * w_0[s, :]
            if r > 0.0:
                # Gated consolidation; more when RL was uncertain (high entropy)
                write = 0.5 + 0.4 * ent_norm
                write = np.clip(write, 0.0, 1.0)
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Kalman-like RL with volatility and WM recall mechanism with limited recency,
    plus set-size– and age–dependent stickiness.
    
    Idea
    - RL: maintain learning-rate-like gain that increases with recent volatility
      (approximate Kalman filter). Base step size is eta0, increased by vol*surprise.
    - WM: recall probability p_recall depends on set size and age; if recalled, WM
      strongly favors the last rewarded action for that state. Implemented as
      a near-deterministic WM map with recency updates and decay.
    - Choice: mixture of WM and RL softmax; add action stickiness bias that grows
      with set size and with age.
    
    Parameters
    ----------
    model_parameters : [eta0, beta_inv, p_recall_base, vol, stickiness, K]
        eta0 : base RL step size (0..1)
        beta_inv : RL inverse temperature (scaled internally)
        p_recall_base : base WM recall probability (0..1)
        vol : volatility/surprise gain (>=0) scaling delta magnitude into higher learning
        stickiness : strength of action perseveration bias
        K : WM recency window (positive integer; treated as float then cast)
    
    Age and Set-Size Effects
    - Age group: 0 if <=45 else 1. Older: lower p_recall and higher stickiness.
    - Set size: lower p_recall and higher stickiness at larger set size.
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    eta0, beta_inv, p_recall_base, vol, stickiness, K = model_parameters
    beta_inv *= 10.0
    age_group = 0 if age[0] <= 45 else 1
    K = max(1, int(round(K)))

    # Age effects
    p_recall_age = np.clip(p_recall_base * (1.0 - 0.3 * age_group), 0.0, 1.0)
    stick_age = stickiness * (1.0 + 0.5 * age_group)

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Maintain per-state recency buffers of last rewarded action
        last_rewarded = -np.ones(nS, dtype=int)
        recency_counts = np.zeros(nS, dtype=int)

        last_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            size_factor = max(0, nS - 3) / 3.0
            # Set-size reduces recall probability
            p_recall_t = np.clip(p_recall_age / (1.0 + size_factor), 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with stickiness bias
            logits_rl = beta_inv * (Q_s - np.max(Q_s))
            if last_action is not None:
                stick_vec = np.zeros(nA)
                stick_vec[last_action] = 1.0
                stick_eff = stick_age * (1.0 + 0.5 * size_factor)
                logits_rl += stick_eff * (stick_vec - 1.0 / nA)
            pi_rl = np.exp(logits_rl) / np.sum(np.exp(logits_rl))
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy: if recall succeeds, W_s is concentrated on last rewarded action
            # We implement as a near-deterministic softmax over w[s,:].
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture: recall probability scales WM contribution
            wm_weight_eff = p_recall_t
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with volatility-modulated step size
            delta = r - Q_s[a]
            eta_t = np.clip(eta0 + vol * abs(delta), 0.0, 1.0)
            q[s, a] += eta_t * delta

            # WM decay towards uniform and recency update upon reward
            decay = 0.05 + 0.2 * size_factor  # modest decay; more under load
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                # Update recency buffer
                last_rewarded[s] = a
                recency_counts[s] = min(K, recency_counts[s] + 1)

                # Write recalled action strongly
                write = 0.7
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # If no reward, allow slight diffusion
                w[s, :] = 0.98 * w[s, :] + 0.02 * (1.0 / nA)

            # Maintain WM recall structure: if we have a remembered rewarded action, bias W_s
            for s2 in range(nS):
                if last_rewarded[s2] >= 0 and recency_counts[s2] > 0:
                    # Create a peaked distribution around last rewarded action
                    peak = 0.85
                    temp = (1.0 - peak) / (nA - 1)
                    w[s2, :] = temp
                    w[s2, last_rewarded[s2]] = peak

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p