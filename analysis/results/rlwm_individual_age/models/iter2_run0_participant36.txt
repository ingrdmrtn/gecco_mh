Below are three standalone cognitive models tailored to the RL-WM task. Each function returns the negative log-likelihood of the participant’s observed choices, uses all parameters meaningfully (≤6 each), and incorporates age-group and set-size effects in distinct ways.

Note: Assume numpy as np is already imported in the environment.


def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated hybrid: Asymmetric RL + WM with age- and load-modulated gating and WM noise.

    Summary:
    - RL updates with asymmetric learning rates for positive vs negative prediction errors.
    - WM policy is near-deterministic but becomes noisier in older adults.
    - Arbitration weight depends on RL uncertainty (entropy) relative to a threshold that increases with set size and age.
    - WM is refreshed on rewards; on non-reward it drifts toward uniform (forgetting).

    Parameters (list of 6):
    - lr_pos: RL learning rate for positive prediction errors.
    - lr_neg: RL learning rate for negative prediction errors.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - wm_gate_temp: softness (temperature) of the uncertainty-based WM gate (higher = softer).
    - wm_refresh: WM refresh strength on rewarded trials (how strongly one-hot is written).
    - age_wm_noise: scales WM noise in older adults (reduces WM inverse temperature).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: array with a single repeated value; age_group = 0 (<=45), 1 (>45).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_gate_temp, wm_refresh, age_wm_noise = model_parameters
    softmax_beta = beta_base * 10.0
    age_group = 1 if age[0] > 45 else 0

    softmax_beta_wm_base = 50.0
    eps = 1e-12
    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age- and load-adjusted WM inverse temperature (older adults have noisier WM)
        softmax_beta_wm = softmax_beta_wm_base / (1.0 + age_wm_noise * age_group)

        # Uncertainty threshold for gating WM; higher for older adults and larger sets
        H_thresh = 1.0 + 0.3 * age_group + 0.2 * max(0.0, nS - 3.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            logits_rl = softmax_beta * Q_s
            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            denom_wm = np.sum(np.exp(logits_wm - logits_wm[a]))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Compute RL uncertainty via entropy of the RL softmax for the state
            probs_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs_rl = probs_rl / max(np.sum(probs_rl), 1e-300)
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))

            # WM strength: distinctiveness of best action
            sorted_W = np.sort(W_s)[::-1]
            wm_strength = max(0.0, sorted_W[0] - sorted_W[1])

            # Gating: more WM weight when RL is uncertain and WM is strong
            gate_input = (H_rl - H_thresh) / max(wm_gate_temp, 1e-6)
            wm_gate = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_gate * wm_strength, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (asymmetric)
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update:
            # - On reward: refresh towards one-hot of chosen action with strength depending on set size and age.
            # - On no reward: drift toward uniform.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Slightly reduced refresh under higher load and in older adults
                refresh_eff = np.clip(wm_refresh * (1.0 - 0.15 * age_group) * (1.0 - 0.1 * max(0.0, nS - 3.0)), 0.0, 1.0)
                w[s, :] = (1.0 - refresh_eff) * w[s, :] + refresh_eff * one_hot
            else:
                decay = np.clip(0.2 + 0.1 * age_group + 0.1 * max(0.0, nS - 3.0), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Confidence-arbitrated RL+WM: arbitration by relative confidence; probabilistic WM storage.

    Summary:
    - RL controller with standard learning; WM stores rewarded associations probabilistically.
    - WM storage probability decreases with set size and age; WM decays toward uniform otherwise.
    - Arbitration weight is a sigmoid of (WM confidence - RL confidence), with age bias and load penalty.
    - Distinct from fixed retrieval: here the controller uses relative confidence as the gate.

    Parameters (list of 6):
    - lr: RL learning rate.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - wm_store_prob: base probability to store/overwrite WM on rewarded trials (0-1 after logistic transform).
    - wm_decay: baseline WM decay rate toward uniform on non-rewarded trials (0-1).
    - arb_slope: slope of confidence-based arbitration sigmoid (sensitivity).
    - age_bias: age-dependent arbitration bias (positive shifts weight to RL; negative to WM in older adults).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_store_prob, wm_decay, arb_slope, age_bias = model_parameters
    softmax_beta = beta_base * 10.0
    age_group = 1 if age[0] > 45 else 0

    softmax_beta_wm = 50.0
    eps = 1e-12
    nA = 3
    blocks_log_p = 0.0

    # Logistic transform for parameters meant to be probabilities
    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    base_store = logistic(wm_store_prob)
    decay_base = np.clip(logistic(wm_decay), 0.0, 1.0)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load and age impact on WM storage and arbitration
        load_penalty = 0.3 * max(0.0, nS - 3.0)  # reduces WM store and arbitration toward WM
        store_prob_eff = np.clip(base_store * (1.0 - 0.25 * age_group) * (1.0 - load_penalty), 0.0, 1.0)
        decay_eff = np.clip(decay_base * (1.0 + 0.5 * age_group + 0.3 * max(0.0, nS - 3.0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probabilities
            logits_rl = softmax_beta * Q_s
            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy probabilities
            logits_wm = softmax_beta_wm * W_s
            denom_wm = np.sum(np.exp(logits_wm - logits_wm[a]))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Confidence measures (margin between best and second-best)
            Q_sorted = np.sort(Q_s)[::-1]
            W_sorted = np.sort(W_s)[::-1]
            conf_rl = max(0.0, Q_sorted[0] - Q_sorted[1])
            conf_wm = max(0.0, W_sorted[0] - W_sorted[1])

            # Arbitration weight: sigmoid of relative confidence, with age bias and load penalty
            arb_input = arb_slope * (conf_wm - conf_rl) + ( -0.3 * age_group + age_bias ) - load_penalty
            wm_weight = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update using expected update rule to avoid stochasticity:
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend current WM toward one-hot by store_prob_eff
                w[s, :] = (1.0 - store_prob_eff) * w[s, :] + store_prob_eff * one_hot
            else:
                # Decay to uniform
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + state-dependent choice kernel + recency-weighted WM and lapse.

    Summary:
    - RL controller with inverse temperature.
    - State-dependent choice kernel adds perseverative bias within each state.
    - WM contribution is recency-weighted: stronger soon after a rewarded association, decays with time,
      more rapidly for older adults and larger set sizes.
    - A lapse probability mixes uniform choice that increases with age and set size.

    Parameters (list of 6):
    - lr: RL learning rate.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - kappa_state: learning rate for state-dependent choice kernel (perseveration).
    - lambda_rec: base decay rate for WM recency (per trial since last reward in that state).
    - wm_weight_base: baseline WM weight when the association is fresh.
    - lapse0: base lapse log-odds (converted via logistic), modulated by age and set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, kappa_state, lambda_rec, wm_weight_base, lapse0 = model_parameters
    softmax_beta = beta_base * 10.0
    age_group = 1 if age[0] > 45 else 0

    softmax_beta_wm = 50.0
    eps = 1e-12
    nA = 3
    blocks_log_p = 0.0

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will hold last rewarded action per state (one-hot when known)
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-dependent choice kernel
        K = np.zeros((nS, nA))

        # Track recency since last reward for each state; initialize to large so WM is weak at start
        recency = np.full(nS, 100, dtype=float)

        # Lapse probability increases with age and set size
        lapse = logistic(lapse0 + 0.5 * age_group + 0.4 * max(0.0, nS - 3.0))
        lapse = np.clip(lapse, 0.0, 0.5)  # cap lapse

        # Effective recency decay parameter increases with age and load
        lambda_eff = np.clip(lambda_rec * (1.0 + 0.5 * age_group + 0.4 * max(0.0, nS - 3.0)), 0.0, 10.0)

        # WM base weight reduced by age and set size
        wm_w_base_eff = np.clip(wm_weight_base * (1.0 - 0.25 * age_group) * (1.0 - 0.15 * max(0.0, nS - 3.0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()
            K_s = K[s, :].copy()

            # RL + choice kernel policy
            logits_rl = softmax_beta * Q_s + K_s
            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy (deterministic when one-hot, otherwise near-uniform)
            logits_wm = softmax_beta_wm * W_s
            denom_wm = np.sum(np.exp(logits_wm - logits_wm[a]))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Recency-based WM weight for this state
            wm_weight_s = wm_w_base_eff * np.exp(-lambda_eff * max(0.0, recency[s]))
            wm_weight_s = np.clip(wm_weight_s, 0.0, 1.0)

            # Hybrid without lapse
            p_hybrid = wm_weight_s * p_wm + (1.0 - wm_weight_s) * p_rl

            # Apply lapse: mix with uniform
            p_total = (1.0 - lapse) * p_hybrid + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Choice kernel update (state-dependent perseveration)
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            K[s, :] = (1.0 - kappa_state) * K[s, :] + kappa_state * one_hot

            # WM update and recency tracking
            if r > 0.0:
                w[s, :] = one_hot  # encode last rewarded action
                recency[s] = 0.0
            else:
                # Decay W toward uniform slowly when not rewarded
                decay = np.clip(0.05 + 0.05 * age_group + 0.05 * max(0.0, nS - 3.0), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                recency[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p

How age and set size enter the models:
- Model 1: Older age reduces WM precision (age_wm_noise) and raises the uncertainty threshold H_thresh; larger set size also raises H_thresh and reduces WM refresh.
- Model 2: Older age and larger set size reduce WM storage probability and increase decay; arbitration weight shifts away from WM via age_bias and load_penalty.
- Model 3: Older age and larger set size increase lapse probability and WM recency decay, and reduce baseline WM weight; choice kernel is state-dependent and unaffected by age directly, letting us dissociate perseveration from WM/RL effects.

These three models explore different mechanisms and parameterizations than the previously tried combinations.