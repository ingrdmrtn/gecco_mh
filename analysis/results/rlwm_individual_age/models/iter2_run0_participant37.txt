def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM gating + state-specific perseveration (age- and set-size sensitive).

    Mechanism:
    - RL: Q-learning with asymmetric learning rates (positive vs. negative prediction errors).
      The negative learning rate is scaled by alpha_neg_factor.
      In larger set sizes, effective learning is slightly reduced.
    - WM: item-based storage with limited slots. If the state is within WM capacity, WM carries
      high-fidelity policy; otherwise WM contributes little. Capacity is smaller in the older group.
      WM content decays slightly after non-rewarded trials toward uniform.
    - Perseveration: state-specific tendency to repeat the last action taken at that state,
      mixed with the RL+WM policy via a mixture weight.
    - Mixture: WM and RL are combined with a state-specific WM weight determined by
      the probability that the current state is within capacity.

    Parameters (model_parameters; all used):
    - alpha_base: base RL learning rate for positive prediction errors in [0,1].
    - alpha_neg_factor: multiplicative factor for negative PE learning rate in [0,1].
    - beta: RL inverse temperature, scaled internally by 10.
    - wm_weight_base: base mixture weight for WM in [0,1].
    - k_slots_young: WM slot capacity for the young group (older group uses k_slots_young - 1, min 1).
    - persev: mixture weight for perseveration policy in [0,1].

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial.
      age_group = 0 (<=45, young) or 1 (>45, old), used to reduce capacity.
    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_base, alpha_neg_factor, beta, wm_weight_base, k_slots_young, persev = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Compute effective WM capacity and weight
        k_slots_old = max(1.0, k_slots_young - 1.0)  # older group reduced by 1 slot (min 1)
        k_eff = k_slots_young if age_group == 0 else k_slots_old
        # Probability that a given state is within WM capacity
        p_in_capacity = min(1.0, k_eff / float(nS))
        wm_weight_eff = np.clip(wm_weight_base * p_in_capacity, 0.0, 1.0)

        # Slight RL learning-rate reduction for larger set sizes
        size_lr_scale = 3.0 / float(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy (deterministic if item stored)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Perseveration policy: repeat last action in this state
            if last_action[s] >= 0:
                p_pers = 1.0 if a == last_action[s] else 0.0
                # Smooth slightly to avoid zeros
                p_pers = 0.999 if p_pers == 1.0 else 0.001
            else:
                p_pers = 1.0 / nA  # no history yet

            # Combine WM and RL
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            # Add perseveration mixture
            p_total = (1.0 - persev) * p_mix + persev * p_pers
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha_pos = np.clip(alpha_base * size_lr_scale, 0.0, 1.0)
            alpha_neg = np.clip(alpha_base * alpha_neg_factor * size_lr_scale, 0.0, 1.0)
            alpha_eff = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha_eff * pe

            # WM update: if rewarded, store action with high fidelity; else decay toward uniform
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            else:
                # gentle decay toward uniform after non-reward
                decay = 0.2
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w0[s, :]

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with choice-kernel bias; age reduces decision precision; set size reduces WM weight.

    Mechanism:
    - RL: standard Q-learning.
    - WM: fast memory vector updated with reward-contingent "win" imprint and small decay otherwise.
    - Choice kernel: a recency-driven tendency to repeat chosen actions in a state (independent of reward).
      It decays with lambda_tr and has strength kappa_choice.
    - Mixture: RL and WM combined with wm_weight0 scaled by set size; an additional mixture with
      the choice-kernel-derived policy is controlled by lambda_tr (more recency -> more weight).
    - Age effect: reduces the effective RL inverse temperature.

    Parameters (model_parameters; all used):
    - lr: RL learning rate in [0,1].
    - beta: base RL inverse temperature, scaled internally by 10.
    - wm_weight0: base WM mixture weight in [0,1], downscaled by set size (3/nS).
    - lambda_tr: decay of choice kernel (higher -> slower decay) and governs its mixture weight.
    - kappa_choice: strength of choice-kernel softmax (mapped to an inverse temperature).
    - age_slope: scales the age-related drop in decision precision; beta_eff = beta*(1 - age_slope*age_group).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_weight0, lambda_tr, kappa_choice, age_slope = model_parameters
    # Age-adjusted RL temperature
    age_group = 1 if age[0] > 45 else 0
    softmax_beta = max(1e-6, beta * (1.0 - age_slope * age_group)) * 10.0
    softmax_beta_wm = 50.0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        # Choice kernel per state-action
        choice_pref = np.zeros((nS, nA))

        # WM weight scales down with set size
        wm_weight_eff = np.clip(wm_weight0 * (3.0 / float(nS)), 0.0, 1.0)
        # Choice kernel mixture weight derived from lambda_tr
        choice_mix = np.clip(1.0 - np.exp(-lambda_tr), 0.0, 0.49)
        beta_choice = max(1e-6, kappa_choice) * 10.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Choice-kernel policy (softmax over choice_pref)
            CK_s = choice_pref[s, :]
            denom_ck = np.sum(np.exp(beta_choice * (CK_s - CK_s[a])))
            p_ck = 1.0 / denom_ck if denom_ck > 0 else 1e-12

            # Combine WM and RL
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            # Then mix in the choice kernel
            p_total = (1.0 - choice_mix) * p_mix + choice_mix * p_ck
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: reward increases the stored action; otherwise slight decay
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            else:
                decay = 0.1
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w0[s, :]

            # Choice kernel update: decay and increment chosen action
            decay_ck = np.exp(-lambda_tr)
            choice_pref[s, :] *= decay_ck
            choice_pref[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-adaptive learning rate + WM with interference + age- and size-dependent lapse.

    Mechanism:
    - RL: Q-learning with trial-wise learning rate adapted to surprise |PE|.
      lr_eff = clip(lr0 + lr_gain * |PE|, 0, 1). Higher surprise increases learning.
    - WM: value table that refreshes on reward and decays otherwise; its mixture weight
      is reduced by interference proportional to set size and further reduced by age.
    - Lapse: stimulus-independent random choice that increases with set size and with age.

    Parameters (model_parameters; all used):
    - lr0: base RL learning rate in [0,1].
    - beta: RL inverse temperature, scaled internally by 10.
    - wm_weight0: base WM mixture weight in [0,1].
    - wm_interference: scales the reduction of WM weight with set size (higher -> stronger reduction).
    - lr_gain: gain on surprise for adaptive learning rate (>=0).
    - lapse_age_scale: additional lapse component applied when age_group == 1.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr0, beta, wm_weight0, wm_interference, lr_gain, lapse_age_scale = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight reduced by set-size interference and by age
        size_factor = float(nS) / 3.0  # 1 for 3-set, 2 for 6-set
        wm_weight_eff = wm_weight0 * np.exp(-wm_interference * (size_factor - 1.0))
        wm_weight_eff *= (1.0 - 0.3 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # Lapse increases with set size and with age
        lapse = np.clip(0.02 * size_factor + lapse_age_scale * age_group, 0.0, 0.49)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture and lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Surprise-adaptive RL update
            pe = r - q[s, a]
            lr_eff = np.clip(lr0 + lr_gain * abs(pe), 0.0, 1.0)
            q[s, a] += lr_eff * pe

            # WM update: reward refreshes, otherwise decay toward uniform
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            else:
                decay = 0.15 + 0.1 * (size_factor - 1.0) + 0.05 * age_group  # more decay with load and age
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p