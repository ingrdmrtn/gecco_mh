def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age-dependent capacity and decay.

    Mechanisms:
    - RL: standard Q-learning.
    - WM: stores stimulus-action associations when rewarded; retrieved via softmax.
    - Capacity limit: WM contribution scales with capacity relative to set size.
    - Age: older adults have lower effective WM capacity and higher WM decay.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - beta: inverse temperature for RL (scaled by x10 internally).
    - wm_strength: base mixture weight for WM (0..1).
    - capacity: WM capacity in items (approx. 1..6).
    - wm_decay: base decay rate of WM per trial (0..1); higher = faster decay.
    - age_cap_drop: capacity decrement for older adults (>=0), applied if age_group=1.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_strength, capacity, wm_decay, age_cap_drop = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm_base = 50.0  # sharp WM retrieval

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and WM decay depend on age and set size
        cap_eff = max(0.0, capacity - age_cap_drop * age_group)
        wm_weight_cap = np.clip(wm_strength * min(1.0, cap_eff / max(1.0, nS)), 0.0, 1.0)
        # Older adults and larger set sizes -> stronger decay toward uniform
        decay_eff = np.clip(wm_decay * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
        beta_wm_eff = softmax_beta_wm_base  # keep WM sharp; capacity already modulates mixture

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy: capacity-limited WM contribution
            p_total = wm_weight_cap * p_wm + (1.0 - wm_weight_cap) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform + encoding of rewarded association
            w = (1.0 - decay_eff) * w + decay_eff * w0
            if r > 0.5:
                # Encode strong association for the rewarded action at state s
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite toward one-hot while keeping some residual
                w[s, :] = 0.3 * w[s, :] + 0.7 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-control of exploration and error-gated WM use.

    Mechanisms:
    - RL: Q-learning; inverse temperature is adapted via meta-control to recent outcomes.
      After errors (low reward), exploration increases (lower beta), especially in large set sizes.
    - WM: one-shot encoding of rewarded pairs; WM influence increases after errors (error-gated).
    - Set size: larger sets attenuate WM gating and amplify exploration.
    - Age: older adults show stronger error-gated reliance on WM and weaker meta-control gains.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - beta0: baseline inverse temperature for RL (scaled by x10 internally).
    - meta_gain: how strongly recent outcomes modulate beta (>=0).
    - wm_gate: base sensitivity of WM mixture to recent errors (>=0).
    - wm_temp: WM inverse temperature scale (>=0) controlling WM noisiness.
    - age_gate_bias: additive boost to WM gating for older adults (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta0, meta_gain, wm_gate, wm_temp, age_gate_bias = model_parameters
    base_beta = beta0 * 10.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Running error/surprise trace (higher after errors)
        err_trace = 0.0
        # Effective WM temperature: noisier WM for larger set sizes and older adults if wm_temp is small
        beta_wm_eff = 10.0 + 40.0 * (wm_temp / (1.0 + 0.5 * age_group * (nS / 3.0)))
        beta_wm_eff = max(5.0, beta_wm_eff)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Adapt beta via meta-control: recent errors -> lower beta (more exploration)
            # err_trace in [0,1] approximately; larger sets amplify exploration effect
            beta_mod = 1.0 / (1.0 + meta_gain * err_trace * (nS / 3.0) * (1.0 + 0.3 * age_group))
            beta_eff = base_beta * beta_mod

            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Error-gated WM mixture: more WM after recent errors; attenuated by set size
            wm_gate_eff = wm_gate * (1.0 / (1.0 + (nS - 3) * 0.7))
            wm_weight = 1.0 / (1.0 + np.exp(- (wm_gate_eff * err_trace + age_gate_bias * age_group)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay slightly toward uniform; encode rewards
            w = 0.98 * w + 0.02 * w0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.4 * w[s, :] + 0.6 * one_hot

            # Update error/surprise trace (higher after errors)
            # Use absolute prediction error bounded to [0,1]
            err_mag = min(1.0, abs(pe))
            # Forgetting factor depends on set size and age (older/larger -> slower decay, more persistent gating)
            lam = 0.7 + 0.2 * (nS / 6.0) + 0.1 * age_group
            lam = np.clip(lam, 0.0, 0.98)
            err_trace = lam * err_trace + (1.0 - lam) * err_mag

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with interference-limited WM and endogenous WM gating.

    Mechanisms:
    - RL: separate learning rates for positive and negative outcomes.
    - WM: recency-based memory of rewarded actions; decays toward uniform.
    - WM interference: larger set sizes and older age reduce WM precision (lower WM beta).
    - Endogenous WM mixture: WM contribution scales with its confidence (distribution peakedness).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewarded trials (0..1).
    - lr_neg: RL learning rate for unrewarded trials (0..1).
    - beta: inverse temperature for RL (scaled by x10 internally).
    - wm_recency: controls WM decay speed (>=0); higher = faster decay.
    - interference_base: base WM interference scaling with set size (>=0).
    - age_interaction: scales interference increase for older adults (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta, wm_recency, interference_base, age_interaction = model_parameters
    softmax_beta = beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # WM precision decreases with set size and age
        wm_interference = interference_base * (nS / 3.0) * (1.0 + age_interaction * age_group)
        beta_wm_eff = 50.0 / (1.0 + wm_interference)
        beta_wm_eff = max(5.0, beta_wm_eff)

        # WM decay rate from recency parameter
        decay_rate = 1.0 - np.exp(-max(1e-6, wm_recency) * (nS / 3.0) * (1.0 + 0.5 * age_group))
        decay_rate = np.clip(decay_rate, 0.0, 1.0)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Endogenous WM mixture: based on confidence = peakiness of W_s
            # Use (max - mean) scaled to [0,1]
            conf = (np.max(W_s) - np.mean(W_s)) / max(1e-12, (1.0 - 1.0 / nA))
            conf = np.clip(conf, 0.0, 1.0)
            wm_weight = conf

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL asymmetric update
            pe = r - Q_s[a]
            lr_eff = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_eff * pe

            # WM decay + reward-based encoding (recency memory)
            w = (1.0 - decay_rate) * w + decay_rate * w0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p