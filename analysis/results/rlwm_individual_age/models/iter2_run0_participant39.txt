Below are three alternative cognitive models that combine reinforcement learning (RL) with working memory (WM), with age and set-size effects incorporated differently. Each function returns the negative log-likelihood of the observed choices.

Model 1: Capacity-limited WM with age-dependent slots and decay
- Idea: WM can actively maintain up to K state-action associations (slots). Older participants have fewer effective slots. With larger set sizes, WM coverage shrinks, so RL dominates more often. WM contents decay over time.
- Parameters:
  - lr: RL learning rate (0-1)
  - wm_weight_base: base mixture between WM and RL (0-1)
  - softmax_beta: RL inverse temperature, scaled by 10 inside
  - K_young: WM capacity for younger participants (e.g., 3-6)
  - K_old: WM capacity for older participants (e.g., 1-4)
  - wm_decay: decay rate of WM strength per trial (0-1)

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM (age- and load-modulated) with decay.

    Parameters:
    - lr: float in [0,1], RL learning rate.
    - wm_weight_base: float in [0,1], base weight on WM in the choice mixture.
    - softmax_beta: float >=0, RL inverse temperature before upscaling (multiplied by 10).
    - K_young: float >=1, effective WM capacity (slots) for younger group.
    - K_old: float >=1, effective WM capacity (slots) for older group.
    - wm_decay: float in [0,1], per-trial decay of WM strength toward 0 (forgetting).

    Mechanism:
    - RL: softmax over Q(s,a).
    - WM: stores last rewarded action for each state with a strength in [0,1] that decays.
      If a state is covered by WM capacity (probability ~ K/nS) and memory exists, WM policy is deterministic to the stored action; otherwise uniform.
    - Mixture: p_total = (wm_weight_eff) * p_wm + (1 - wm_weight_eff) * p_rl,
      with wm_weight_eff = wm_weight_base * availability (K/nS) * memory_strength.
    - Age: determines K (K_young vs K_old).
    - Set size: availability scales with K/nS.
    """
    lr, wm_weight_base, softmax_beta, K_young, K_old, wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    K = K_young if age_group == 0 else K_old

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM memory: store last rewarded action and a strength in [0,1]
        mem_act = -1 * np.ones(nS, dtype=int)
        mem_strength = np.zeros(nS)

        # For template consistency (not critical to logic)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM availability scales with capacity versus set size
            availability = min(1.0, max(K, 0.0) / max(nS, 1.0))
            if mem_act[s] >= 0:
                # Deterministic WM on the stored action if available; otherwise uniform
                p_wm_det = 1.0 if a == mem_act[s] else 0.0
                p_wm = availability * (mem_strength[s] * p_wm_det + (1.0 - mem_strength[s]) * (1.0 / nA)) + (1.0 - availability) * (1.0 / nA)
            else:
                p_wm = 1.0 / nA

            wm_weight_eff = np.clip(wm_weight_base * availability * (mem_strength[s] if mem_act[s] >= 0 else 0.0), 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: reward-locked encoding with decay
            # Decay all strengths each trial
            mem_strength *= (1.0 - np.clip(wm_decay, 0.0, 1.0))
            if r == 1:
                mem_act[s] = a
                mem_strength[s] = 1.0  # reset to strong memory on reward

            # keep w normalized (template placeholder dynamics)
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: RL with age-modulated asymmetric learning, uncertainty bonus, and fast WM Q-table
- Idea: RL learns with separate positive/negative learning rates that vary by age. Directed exploration is implemented via a count-based uncertainty bonus, stronger under higher load. WM is a fast-learning Q_wm that sharpens toward rewarded actions and contributes via a high-beta softmax.
- Parameters:
  - lr_pos_base: base positive RL learning rate (0-1)
  - lr_neg_base: base negative RL learning rate (0-1)
  - wm_weight: mixture weight (0-1)
  - softmax_beta: RL inverse temperature, scaled by 10 inside
  - bonus_base: exploration bonus scale for uncertainty (>=0)
  - wm_learn: WM learning rate for Q_wm (0-1)

def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-modulated asymmetric learning and count-based exploration + fast WM Q.

    Parameters:
    - lr_pos_base: float in [0,1], base RL learning rate for positive outcomes.
    - lr_neg_base: float in [0,1], base RL learning rate for negative outcomes.
    - wm_weight: float in [0,1], mixture weight on WM policy.
    - softmax_beta: float >=0, RL inverse temperature before upscaling (multiplied by 10).
    - bonus_base: float >=0, scale of directed exploration via uncertainty bonus.
    - wm_learn: float in [0,1], WM learning rate for its Q_wm table.

    Mechanism:
    - RL: Q-learning with lr_pos/lr_neg, both modulated by age (older: reduced lr_pos, slightly increased lr_neg).
    - Exploration: add count-based bonus bonus_base / sqrt(N_sa+1), scaled up by set size (higher load -> more bonus).
    - WM: separate Q_wm(s,a) updated quickly toward rewarded action; policy is softmax with high beta.
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl, where wm_weight_eff decreases with set size.
    """
    lr_pos_base, lr_neg_base, wm_weight, softmax_beta, bonus_base, wm_learn = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # Age modulation: older learn less from rewards, a bit more from non-rewards
    lr_pos = np.clip(lr_pos_base * (1.0 - 0.3 * age_group), 0.0, 1.0)
    lr_neg = np.clip(lr_neg_base * (1.0 + 0.2 * age_group), 0.0, 1.0)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values and counts for bonuses
        q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))

        # WM Q-values
        q_wm = (1.0 / nA) * np.ones((nS, nA))

        # For template consistency
        w = q_wm.copy()
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent mixture discounting for WM
        wm_weight_load = wm_weight * (3.0 / max(nS, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with directed exploration bonus
            bonus = bonus_base * (nS / 3.0) / np.sqrt(counts[s, :] + 1.0)  # more bonus at larger set size
            Q_aug = q[s, :] + bonus
            denom_rl = np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over q_wm with high beta
            Qwm_s = q_wm[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (Qwm_s - Qwm_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_load * p_wm + (1.0 - wm_weight_load) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # Update counts after observing choice
            counts[s, a] += 1.0

            # WM update: fast attraction to rewarded action; mild repulsion otherwise
            if r == 1:
                # Pull chosen action up, push others down to keep normalized shape
                delta = wm_learn * (1.0 - q_wm[s, a])
                q_wm[s, a] += delta
                others = [aa for aa in range(nA) if aa != a]
                if len(others) > 0:
                    q_wm[s, others] *= (1.0 - wm_learn)
            else:
                # If no reward, slightly flatten toward uniform using wm_learn
                q_wm[s, :] = (1.0 - wm_learn) * q_wm[s, :] + wm_learn * (1.0 / nA)

            # Keep WM rows normalized
            q_wm[s, :] = np.maximum(q_wm[s, :], eps)
            q_wm[s, :] /= np.sum(q_wm[s, :])

            # Maintain w in template form
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: Surprise-gated WM encoding with age/load-modulated gate and WM forgetting
- Idea: WM encoding is triggered by surprise (|prediction error|). A logistic gate determines encoding probability, with age and set size shifting the gate bias and strength. WM forgets toward uniform between trials.
- Parameters:
  - lr: RL learning rate (0-1)
  - wm_weight_base: base mixture weight (0-1)
  - softmax_beta: RL inverse temperature, scaled by 10 inside
  - gate_slope: slope of the sigmoid gating on |PE| (>=0)
  - gate_bias: bias of the sigmoid (can be negative/positive)
  - wm_forget: forgetting rate of WM distribution toward uniform per trial (0-1)

def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + surprise-gated WM with age- and load-modulated gating and WM forgetting.

    Parameters:
    - lr: float in [0,1], RL learning rate.
    - wm_weight_base: float in [0,1], base mixture weight on WM policy.
    - softmax_beta: float >=0, RL inverse temperature before upscaling (multiplied by 10).
    - gate_slope: float >=0, slope of logistic gate on |prediction error|.
    - gate_bias: float (real), bias of logistic gate.
    - wm_forget: float in [0,1], per-trial forgetting of WM toward uniform.

    Mechanism:
    - RL: softmax over Q(s,a).
    - WM: a categorical distribution w[s, :] over actions.
      On each trial, WM is updated toward the chosen action with probability determined by a logistic
      function of |PE|. Gating is weakened under higher load and in older adults.
      WM forgets toward uniform each trial (rate wm_forget).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl,
      with wm_weight_eff decreasing with set size and age.
    """
    lr, wm_weight_base, softmax_beta, gate_slope, gate_bias, wm_forget = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM policy distribution per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load- and age-dependent weighting of WM in the mixture
        wm_weight_eff_base = wm_weight_base * (3.0 / max(nS, 1.0)) * (1.0 - 0.3 * age_group)
        wm_weight_eff_base = np.clip(wm_weight_eff_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over w[s, :]
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff_base * p_wm + (1.0 - wm_weight_eff_base) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Surprise-gated WM encoding probability
            # Gate weakens with larger set size and older age
            load_scale = 3.0 / max(nS, 1.0)              # 1.0 at nS=3, 0.5 at nS=6
            age_scale = (1.0 - 0.3 * age_group)          # reduce gate for older adults
            gate_input = gate_slope * (abs(pe)) * load_scale * age_scale + gate_bias
            p_encode = 1.0 / (1.0 + np.exp(-gate_input))

            # WM forgetting toward uniform on every trial
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Encode toward chosen action with probability p_encode
            # Implement probabilistically in expectation by a convex update
            # Move mass toward one-hot at 'a'
            target = np.zeros(3)
            target[a] = 1.0
            w[s, :] = (1.0 - p_encode) * w[s, :] + p_encode * target

            # Renormalize for safety
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p

Notes on distinctiveness from prior attempts:
- Model 1 introduces explicit capacity K with age-dependent slots and multiplicative availability K/nS, plus WM strength decay (wm_decay), not tried before.
- Model 2 combines age-modulated asymmetric RL learning with a load-scaled count-based exploration bonus and a fast WM Q-table (wm_learn), differing from earlier stickiness or lapse mechanisms.
- Model 3 uses a surprise-gated WM encoding probability with logistic parameters (gate_slope, gate_bias) and WM forgetting (wm_forget), a different gating/forgetting mechanism than previous models.