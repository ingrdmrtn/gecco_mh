def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility traces) + capacity-limited WM mixture

    Model summary:
    - RL: tabular Q-learning with eligibility traces (lambda) across actions for the chosen state-action pair.
    - WM: deterministic storage of the last rewarded action per state (one-hot), queried with a sharp softmax.
    - Arbitration: mixture of WM and RL policies. WM effective weight scales with capacity K relative to set size,
      and K is modulated by age group.
    - Age effect: older group has reduced effective WM capacity via a multiplicative factor.

    Parameters (list; total 6):
    - lr: base learning rate for RL (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - lambda_elig: eligibility trace decay parameter (0..1).
    - wm_capacity_K: nominal number of states that can be held in WM (>=0).
    - wm_base_weight: baseline mixture weight for WM (0..1).
    - age_k_mult: multiplicative factor applied to WM capacity for older group (0..1). Young group uses 1.0.

    Inputs:
    - states: array of state indices per trial (ints).
    - actions: array of chosen actions per trial (ints in [0,2]).
    - rewards: array of rewards per trial (0/1).
    - blocks: array with block index per trial.
    - set_sizes: array with set size per trial (3 or 6; constant within block).
    - age: array with a single repeated value; age[0] used to define age group (0=young, 1=old).

    Output:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, lambda_elig, wm_capacity_K, wm_base_weight, age_k_mult = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # near-deterministic WM reads
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility trace for RL
        e = np.zeros((nS, nA))

        # Age-adjusted WM capacity
        K_eff = wm_capacity_K if age_group == 0 else wm_capacity_K * max(0.0, age_k_mult)
        # Probability that a given state is covered by WM, approximated by K_eff / nS, clipped to [0,1]
        coverage = min(1.0, max(0.0, K_eff / max(1, nS)))
        wm_weight_eff = wm_base_weight * coverage

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for observed action probability
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy for observed action probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            pe = r - Q_s[a]
            # Set trace for chosen state-action to 1, decay all traces by lambda
            e *= lambda_elig
            e[s, a] = 1.0
            q += lr * pe * e

            # WM update: store rewarded pair as one-hot; otherwise keep as is
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # No explicit decay here; for states not in WM, w remains diffuse
                # Keep a weak pull toward baseline to avoid numerical issues
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with volatility-sensitive arbitration + WM with interference-driven decay

    Model summary:
    - RL: standard tabular Q-learning.
    - WM: last rewarded action per state; decays toward uniform with a decay rate that increases with set size
      and with age group.
    - Arbitration: WM weight is downregulated when WM is uncertain (high entropy) and when recent RL volatility is high.
      This captures a rational shift to RL under noisy/large-set conditions.
    - Age effect: increases WM decay and increases downregulation via volatility.

    Parameters (list; total 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_weight0: baseline WM mixture weight (0..1).
    - wm_decay_base: base decay for WM values per trial (0..1).
    - vol_sensitivity: strength of arbitration downweighting by volatility/entropy (>=0).
    - age_wm_penalty: additive penalty to WM decay and arbitration for older group (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: see cognitive_model1.

    Output:
    - Negative log-likelihood.
    """
    lr, softmax_beta, wm_weight0, wm_decay_base, vol_sensitivity, age_wm_penalty = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM decay grows with set size and age
        wm_decay_eff = wm_decay_base + 0.1 * max(0, nS - 3) + age_wm_penalty * age_group
        wm_decay_eff = min(max(wm_decay_eff, 0.0), 1.0)

        # Running volatility estimate from RL prediction errors (EWMA of |PE|)
        vol = 0.0
        vol_alpha = 0.2

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Entropy of WM distribution for current state (0..log nA)
            pW = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pW = pW / np.sum(pW)
            entropy_wm = -np.sum(pW * np.log(np.clip(pW, 1e-12, 1.0)))

            # Arbitration: downweight WM when (i) WM entropy high, (ii) volatility high, (iii) larger set size, (iv) age
            down_weight = 1.0 + vol_sensitivity * (entropy_wm + vol) + 0.2 * vol_sensitivity * max(0, nS - 3) + age_wm_penalty * age_group
            wm_weight_eff = wm_weight0 / down_weight
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and volatility update
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            vol = (1.0 - vol_alpha) * vol + vol_alpha * abs(pe)

            # WM decay toward baseline
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            # If rewarded, overwrite with one-hot memory
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Noisy-gating WM + asymmetric RL + lapse

    Model summary:
    - RL: tabular with separate learning rates for positive and negative prediction errors.
    - WM: stores rewarded action per state as one-hot. Access to WM is governed by a noisy gate that depends on
      recent reward for the state, set size, and age. When the gate is open, WM has strong influence.
    - Arbitration: gate probability directly modulates WM weight on each trial.
    - Lapse: with small probability, choices are random (uniform), capturing occasional attentional lapses.

    Parameters (list; total 6):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_gate_gain: slope of the logistic gate controlling WM access (>=0).
    - lapse_rate: probability of a random choice on each trial (0..0.2 typically).
    - setsize_age_penalty: additive penalty on the gate input per extra item and for older group (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: see cognitive_model1.

    Output:
    - Negative log-likelihood.
    """
    lr_pos, lr_neg, softmax_beta, wm_gate_gain, lapse_rate, setsize_age_penalty = model_parameters
    softmax_beta *= 10.0
    lapse_rate = min(max(lapse_rate, 0.0), 0.5)  # guard

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track recent reward for each state (EWMA) to drive the gate
        recent_r = np.zeros(nS)
        rr_alpha = 0.5

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Noisy WM gate: higher when recent reward for this state is high; penalized by set size and age
            gate_input = recent_r[s] - setsize_age_penalty * (max(0, nS - 3) + age_group)
            gate_prob = 1.0 / (1.0 + np.exp(-wm_gate_gain * gate_input))
            gate_prob = min(max(gate_prob, 0.0), 1.0)

            # Mixture with lapse
            p_mix = gate_prob * p_wm + (1.0 - gate_prob) * p_rl
            p_total = (1.0 - lapse_rate) * p_mix + lapse_rate * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM maintenance and update
            # Slight pull toward baseline each trial to avoid stale certainty
            w[s, :] = 0.98 * w[s, :] + 0.02 * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Update recent reward trace for gating
            recent_r[s] = (1.0 - rr_alpha) * recent_r[s] + rr_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p