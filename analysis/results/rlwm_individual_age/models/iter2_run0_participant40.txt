Here are three distinct cognitive models, each as a standalone Python function. They all return the negative log-likelihood of the observed choices, use all parameters meaningfully, and incorporate age group and set-size effects. They follow the “mixture-of-policies” arbitration form (WM vs RL) and include explicit WM policies and WM updates.

Note: Assume numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + capacity-limited WM slots with load/age-dependent availability.

    Idea:
    - RL: separate positive/negative learning rates (lr_pos, lr_neg), both suppressed by age and larger set sizes.
    - WM: slot-like capacity (capacity) reduced for older adults; probability the current state is in WM is ~ capacity/nS.
           If rewarded, encode deterministically toward selected action with strength (encode_strength).
           WM decays toward uniform at a rate that grows with set size and age (decay_base).
    - Arbitration: wm_weight_base scaled by state-specific WM availability p_inWM(s) (= min(1, cap_eff/nS)).

    Parameters (model_parameters; all used):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - wm_weight_base: base arbitration weight for WM (0..1).
    - capacity: WM capacity in “items”; effective capacity reduced by age (e.g., -0.5 if older) (>=0).
    - encode_strength: strength of WM encoding on rewarded trials (0..1).
    - decay_base: base WM decay rate toward uniform (0..1), scaled by load and age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight_base, capacity, encode_strength, decay_base = model_parameters
    softmax_beta = 1.0  # Not used directly; mixture uses WM with high beta; RL uses derived form below.
    softmax_beta_wm = 50.0  # very deterministic WM policy

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age and load factors
        load = max(nS - 3, 0) / 3.0
        # Suppress RL learning with load and age
        rl_suppress = 1.0 / (1.0 + 0.5 * age_group + 0.5 * load)
        lr_pos_eff = float(np.clip(lr_pos * rl_suppress, 0.0, 1.0))
        lr_neg_eff = float(np.clip(lr_neg * rl_suppress, 0.0, 1.0))

        # WM decay increases with set size and age
        wm_decay = float(np.clip(decay_base * (1.0 + load) * (1.0 + 0.5 * age_group), 0.0, 1.0))

        # Effective WM capacity reduced for older adults
        cap_eff = max(0.0, capacity - 0.5 * age_group)
        p_inWM = float(np.clip(cap_eff / max(nS, 1), 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax probability of chosen action via difference trick with an effective beta)
            # We use a modest fixed RL beta; learning rate asymmetry does most of the work here.
            beta_rl = 10.0 * rl_suppress
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: softmax over WM row with high beta (near-deterministic if encoded)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Arbitration weight scaled by availability of WM (capacity/nS)
            wm_weight_t = float(np.clip(wm_weight_base * p_inWM, 0.0, 1.0))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            if delta >= 0:
                q[s, a] += lr_pos_eff * delta
            else:
                q[s, a] += lr_neg_eff * delta

            # WM decay toward uniform for this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on rewarded trials toward a one-hot at chosen action
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * onehot

            # Normalize WM row to be a valid distribution
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated WM encoding + RL with stickiness and load/age-tuned arbitration.

    Idea:
    - RL: single learning rate (lr) and an effective inverse temperature reduced by age and larger set sizes.
           A recency-based choice stickiness bias (stickiness) favors repeating the last action within a block.
    - WM: encodes more strongly when unsigned PE is large (surprise gating via pe_sensitivity).
           WM decays toward uniform at a rate growing with load and age (decay_base).
    - Arbitration: wm_weight_base scaled by gated WM signal and reduced by load (age + set-size).

    Parameters (model_parameters; all used):
    - lr: RL learning rate (0..1).
    - beta: base RL inverse temperature; internally scaled by 10 and reduced by age/set-size (positive).
    - wm_weight_base: base WM arbitration weight (0..1).
    - pe_sensitivity: sensitivity of WM gating to unsigned PE (>=0); higher -> stronger WM use on surprises.
    - decay_base: base WM decay rate (0..1), scaled by load and age.
    - stickiness: choice repetition strength (>=0); adds bias toward the most recent action.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_weight_base, pe_sensitivity, decay_base, stickiness = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Recency bias vector over actions (block-level)
        bias = np.zeros(nA)
        last_action = None

        load = age_group + max(nS - 3, 0) / 3.0
        # Reduce RL beta with age and load
        beta_eff = softmax_beta / (1.0 + load)

        # WM decay grows with age and set size
        wm_decay = float(np.clip(decay_base * (1.0 + load), 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with stickiness bias
            pref = beta_eff * Q_s + stickiness * bias
            denom_rl = np.sum(np.exp(pref - pref[a]))
            p_rl = 1.0 / denom_rl

            # WM policy (near-deterministic softmax over WM table)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Surprise-gated WM arbitration weight
            delta = r - Q_s[a]
            pe = abs(delta)
            gate = 1.0 / (1.0 + np.exp(-pe_sensitivity * (pe - 0.5)))  # centered gate
            wm_weight_t = float(np.clip((wm_weight_base * gate) / (1.0 + load), 0.0, 1.0))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding strength tied to surprise gate and reward
            if r > 0 and gate > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                enc = float(np.clip(gate, 0.0, 1.0))
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot

            # Normalize WM row
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

            # Update choice bias (recency stickiness)
            bias *= 0.9  # mild decay of stickiness over trials
            bias[a] += 1.0
            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Load/age-weighted WM arbitration + WM error-correcting updates; RL standard.

    Idea:
    - RL: standard delta rule with learning rate lr; effective inverse temperature reduced by age and set size
           using separate penalty strengths (gamma_age, gamma_size).
    - WM: error-correcting updates:
          - If rewarded: move toward one-hot for chosen action by wm_eta.
          - If not rewarded: erase toward uniform by wm_eta (error-correction).
          No separate decay parameter; wm_eta controls both consolidation and forgetting.
    - Arbitration: wm_weight_base adapted to load:
          wm_weight_t = wm_weight_base / (1 + gamma_age*age_group + gamma_size*load_size).
      Where load_size = max(nS-3,0)/3, so WM has less influence under higher load and in older adults.

    Parameters (model_parameters; all used):
    - lr: RL learning rate (0..1).
    - wm_weight_base: base WM arbitration weight (0..1).
    - beta_base: base RL inverse temperature; internally scaled by 10 (positive).
    - gamma_age: penalty weight for age on both beta and WM arbitration (>=0).
    - gamma_size: penalty weight for set-size on both beta and WM arbitration (>=0).
    - wm_eta: WM learning/forgetting rate (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, beta_base, gamma_age, gamma_size, wm_eta = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load_size = max(nS - 3, 0) / 3.0

        # Effective RL beta reduced separately by age and set-size penalties
        beta_eff = softmax_beta / (1.0 + gamma_age * age_group + gamma_size * load_size)

        # WM arbitration weight also reduced by age and set-size penalties
        wm_weight_factor = 1.0 / (1.0 + gamma_age * age_group + gamma_size * load_size)
        wm_weight_eff = float(np.clip(wm_weight_base * wm_weight_factor, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM error-correcting update
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * onehot
            else:
                # On error, erase toward uniform (forget wrong mapping)
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

            # Normalize WM row
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size influences:
- Model 1: Older age reduces effective WM capacity (capacity -> capacity - 0.5 if old), and both age and set-size suppress RL learning rates and increase WM decay. This predicts stronger load costs and age-related deficits in WM-driven performance.
- Model 2: Age and load jointly reduce RL beta and increase WM decay; WM influence is dynamically gated by surprise (unsigned prediction error), with stickiness capturing habitual repetition. Older participants under high load will rely less on WM unless surprised.
- Model 3: Uses separate penalties for age and set-size that simultaneously reduce RL beta and WM arbitration weight; WM learns via error-correcting updates controlled by wm_eta. This cleanly dissociates how age vs load attenuate both temperature and WM reliance.