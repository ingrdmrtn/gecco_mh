def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(α) with eligibility traces and age-modulated decay + stochastic WM gating under load + action stickiness.

    Mechanisms
    ----------
    - RL: single learning rate with eligibility trace λ; λ decays more for older adults.
    - WM: deterministic slot-like memory after reward, with decay toward baseline.
    - Arbitration (wm_weight): stochastic logistic gate that weakens with set size and age.
    - Stickiness: bias to repeat previous action within a block.

    Parameters
    ----------
    states : array-like of int
        State index for each trial within block (0..set_size-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int
        Reward (0 or 1).
    blocks : array-like of int
        Block index.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like
        Single value repeated; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, softmax_beta, lambda_trace, beta_wm_gate, sigma_gate_age, kappa_stick]
        - alpha: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - lambda_trace: base eligibility trace decay (0..1).
        - beta_wm_gate: sensitivity of WM gate to load (higher => stronger drop with load).
        - sigma_gate_age: extra gate noise for older adults (>=0).
        - kappa_stick: action stickiness bias added to last action within a block.

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha, softmax_beta, lambda_trace, beta_wm_gate, sigma_gate_age, kappa_stick = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        elig = np.zeros((nS, nA))  # eligibility traces
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # age-modulated decay of eligibility traces: older => faster decay
        lam_eff = np.clip(lambda_trace * (0.8 ** age_group), 0.0, 1.0)

        # stickiness memory (last action in the block)
        last_action = None

        # base WM gate as function of load; add age-dependent noise term
        # wm_weight_t = sigmoid( gate_bias ), where gate_bias decreases with set size and age
        # Implement by computing per-trial logistic of -(beta_wm_gate * (nS-3)) plus age noise
        # draw a deterministic pseudo-noise from a bounded transform of trial index for reproducibility
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Stickiness bias vector (applied to both systems to reflect motor perseveration)
            bias = np.zeros(nA)
            if last_action is not None:
                bias[last_action] += kappa_stick

            # RL chosen-action probability with stickiness
            Qb = Q_s + bias
            denom_rl = np.sum(np.exp(softmax_beta * (Qb - Qb[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM chosen-action probability with stickiness
            Wb = W_s + bias
            denom_wm = np.sum(np.exp(softmax_beta_wm * (Wb - Wb[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM gate: logistic of negative load with extra dispersion in older adults
            # Larger sets -> smaller wm_weight; age adds noise scale
            load = float(nS - 3)  # 0 for set=3, 3 for set=6
            # pseudo-random but deterministic "noise" based on t to avoid external RNG
            noise_unit = ((t + 1) % 5) / 4.0 - 0.5  # in [-0.5, 0.5]
            noise = sigma_gate_age * age_group * noise_unit
            gate_bias = -(beta_wm_gate * load) + noise
            wm_weight = 1.0 / (1.0 + np.exp(-gate_bias))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            pe = r - Q_s[a]
            # decay eligibilities, then increment current (s,a)
            elig *= lam_eff
            elig[s, a] += 1.0
            q += alpha * pe * elig

            # WM decay and update: decay towards uniform; write if rewarded
            w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dynamic learning rate via Pearce-Hall attention + load/age-scaled WM capacity sharing.

    Mechanisms
    ----------
    - RL: learning rate is adapted by an attention variable A_t (larger after surprising outcomes).
    - WM: capacity-like weight decreases with set size following a power law and decays over time.
    - Age: older adults show stronger WM decay and weaker adaptive attention.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [alpha0, kappa_attn, softmax_beta, phi_wm, eta_load, decay_wm_age]
        - alpha0: baseline RL learning rate (0..1).
        - kappa_attn: coupling from unsigned PE to attention (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - phi_wm: baseline WM weight coefficient at set size 1 (0..1).
        - eta_load: exponent controlling how WM weight scales with set size (>0).
        - decay_wm_age: WM decay multiplier per trial; amplified by age group.

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha0, kappa_attn, softmax_beta, phi_wm, eta_load, decay_wm_age = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight for this block from capacity sharing rule
        wm_weight_block = phi_wm / (float(nS) ** max(1e-6, eta_load))
        # Keep within [0,1]
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        # Age-modulated WM decay factor (older -> stronger decay toward baseline per trial)
        decay_wm = np.clip(decay_wm_age * (1.2 ** age_group), 0.0, 1.0)

        # Initialize attention for Pearce-Hall
        A = 0.5

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Age diminishes attention gain (older -> smaller kappa effective)
            kappa_eff = kappa_attn * (0.8 ** age_group)
            wm_weight = wm_weight_block

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL with dynamic learning rate
            pe = r - Q_s[a]
            alpha_t = np.clip(alpha0 * A, 0.0, 1.0)
            q[s, a] += alpha_t * pe
            # Update attention by unsigned PE
            A = (1.0 - kappa_eff) * A + kappa_eff * abs(pe)

            # WM decay and update
            w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with age-modulated arbitration temperature.
    
    Mechanisms
    ----------
    - RL: standard delta rule.
    - WM: probabilistic memory map updated by reward with forgetting.
    - Arbitration: weight depends on relative confidence (WM max probability vs RL dispersion).
      Age increases arbitration temperature -> noisier arbitration (less selective).

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like
    model_parameters : list or array
        [alpha, softmax_beta, tau_arbit_base, p_wm_learn, p_wm_forget, age_temp_scale]
        - alpha: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - tau_arbit_base: base temperature for arbitration (>=0).
        - p_wm_learn: probability to commit rewarded action to WM (0..1).
        - p_wm_forget: per-trial forgetting probability pulling WM toward uniform (0..1).
        - age_temp_scale: multiplicative increase in arbitration temperature for older adults (>=1).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha, softmax_beta, tau_arbit_base, p_wm_learn, p_wm_forget, age_temp_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration temperature increases with age
        tau = tau_arbit_base * (age_temp_scale ** age_group)
        tau = max(1e-6, tau)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Confidence measures:
            # WM confidence: concentration around argmax
            conf_wm = np.max(W_s) - 1.0 / nA  # 0 if uniform, up to ~2/3 if one-hot
            conf_wm = np.clip(conf_wm, 0.0, 1.0)

            # RL confidence: inverse entropy proxy via Q dispersion
            q_centered = Q_s - np.mean(Q_s)
            disp = np.sqrt(np.mean(q_centered ** 2))  # RMS dispersion
            conf_rl = 1.0 / (1.0 + np.exp(-10.0 * disp))  # squash to (0,1)

            # Arbitration weight favoring the more confident system
            # wm_weight = sigmoid((conf_wm - conf_rl)/tau)
            wm_weight = 1.0 / (1.0 + np.exp(-(conf_wm - conf_rl) / tau))
            # Load naturally reduces WM confidence early; no explicit parameter added here.

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # WM forgetting toward uniform with probability p_wm_forget
            # Implement deterministically as convex combination each trial
            w[s, :] = (1.0 - p_wm_forget) * w[s, :] + p_wm_forget * w_0[s, :]

            # Probabilistic WM commit on reward
            # Use deterministic approximation: blend toward one-hot by p_wm_learn when rewarded
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_wm_learn) * w[s, :] + p_wm_learn * one_hot

        blocks_log_p += log_p

    return -blocks_log_p