def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL (dual learning rates) + decaying WM with age- and set-size-dependent arbitration and lapse.

    Mechanism:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores action preferences per state with a decay-to-uniform mechanism.
    - Arbitration weight depends on set size via a learned sensitivity and is reduced for older adults.
    - A small lapse is added that increases in the older group.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if <= 45, 1 otherwise.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, wm_decay, wm_weight_base, age_lapse_delta]
        - alpha_pos: RL learning rate when PE > 0.
        - alpha_neg: RL learning rate when PE <= 0.
        - beta_base: RL inverse temperature (multiplied by 10 internally).
        - wm_decay: WM decay-to-uniform step size (0..1) per state update.
        - wm_weight_base: baseline WM reliance before set-size and age adjustments (logit scale).
        - age_lapse_delta: additive increase in lapse for older group (non-negative suggested).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_base, wm_decay, wm_weight_base, age_lapse_delta = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM readout

    age_group = 1 if age[0] > 45 else 0

    # Lapse settings: small baseline + age-related increment
    lapse_base = 1e-6
    lapse = lapse_base + age_group * max(0.0, age_lapse_delta)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration: logistic transform with set-size penalty (stronger penalty for larger sets)
        # We use wm_weight_base on logit scale and subtract a set-size penalty (0 for 3, positive for 6)
        setsize_penalty = 1.0 if nS > 3 else 0.0
        wm_weight_logit = wm_weight_base - setsize_penalty - 0.5 * age_group
        wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_logit))
        wm_weight = min(1.0, max(0.0, wm_weight))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with dual learning rates
            pe = r - q[s, a]
            lr = alpha_pos if pe > 0.0 else alpha_neg
            q[s, a] += lr * pe

            # WM update with decay-to-uniform and reward-based sharpening
            # First decay towards uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Then, incorporate outcome: reward sharpens chosen action; no reward slightly down-weights it
            if r > 0.0:
                # Add a bump to the chosen action proportional to wm_decay and renormalize
                w[s, a] += wm_decay
            else:
                w[s, a] = max(0.0, w[s, a] - 0.5 * wm_decay)
            # Renormalize WM row to a proper probability distribution
            row_sum = np.sum(w[s, :])
            if row_sum <= 0.0:
                w[s, :] = w_0[s, :]
            else:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with attention-modulated learning + WM with confidence-dependent noise and set-size gating.

    Mechanism:
    - RL uses a Pearce-Hall-like attention term (updated from unsigned PE) to scale the learning rate per state.
    - WM stores a peak for last rewarded action; non-reward diffuses memory.
    - WM policy precision is reduced when the memory is uncertain (low margin between top two WM actions).
    - Arbitration weight scales with effective capacity (3/nS) and is biased down in older adults.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if <= 45, 1 otherwise.
    model_parameters : list or array
        [lr_base, lr_gain, beta_base, wm_conf_noise, wm_weight_base, age_bias]
        - lr_base: baseline RL learning rate floor.
        - lr_gain: scaling of state attention (0..1) on the learning rate.
        - beta_base: RL inverse temperature (multiplied by 10 internally).
        - wm_conf_noise: scales how much WM precision drops when WM is uncertain (0..1).
        - wm_weight_base: baseline arbitration weight before capacity and age effects (0..1).
        - age_bias: subtractive bias on WM weight for older adults.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    lr_base, lr_gain, beta_base, wm_conf_noise, wm_weight_base, age_bias = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm_max = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise attention (unsigned PE trace), initialize modestly
        attn = 0.5 * np.ones(nS)

        # Arbitration baseline: capacity factor (3/nS) with age penalty
        capacity_factor = 3.0 / float(nS)
        wm_weight = wm_weight_base * capacity_factor - age_group * max(0.0, age_bias)
        wm_weight = min(1.0, max(0.0, wm_weight))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM confidence-dependent precision: reduce beta_wm when margin is small
            W_s = w[s, :]
            sorted_w = np.sort(W_s)[::-1]
            margin = sorted_w[0] - sorted_w[1] if nA > 1 else sorted_w[0]
            beta_wm = softmax_beta_wm_max * (1.0 - wm_conf_noise * (1.0 - margin))
            beta_wm = max(1.0, beta_wm)

            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with attention-modulated learning rate
            pe = r - q[s, a]
            attn[s] = 0.7 * attn[s] + 0.3 * abs(pe)  # update attention trace toward unsigned PE
            lr = lr_base + lr_gain * attn[s]
            lr = min(1.0, max(0.0, lr))
            q[s, a] += lr * pe

            # WM update: reward -> sharpen; no reward -> diffuse
            if r > 0.0:
                w[s, :] *= 0.0
                w[s, a] = 1.0
            else:
                # diffuse toward uniform, keep weak preference for chosen action
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]
                w[s, a] = 0.7 * w[s, a] + 0.3 * (1.0 / nA)
            # Normalize defensively
            row_sum = np.sum(w[s, :])
            if row_sum <= 0.0:
                w[s, :] = w_0[s, :]
            else:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with novelty bonus + slot-limited WM precision + age-dependent arbitration.

    Mechanism:
    - RL adds a novelty bonus to less-visited state-action pairs to encourage exploration.
      The novelty drive is reduced in older adults.
    - WM acts like a limited-capacity store: precision (beta_wm) scales with effective slots K/nS.
    - Arbitration weight scales with the same effective capacity (K/nS) and is age-penalized.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is coded as 0 if <= 45, 1 otherwise.
    model_parameters : list or array
        [lr, beta_base, novelty_eta, wm_slots, age_penalty, beta_wm0]
        - lr: RL learning rate (0..1).
        - beta_base: RL inverse temperature (multiplied by 10 internally).
        - novelty_eta: strength of novelty bonus (added to Q via pseudo-reward from low visit counts).
        - wm_slots: effective WM slots K (0..6), controls precision and arbitration vs set size.
        - age_penalty: subtractive penalty on both novelty and WM arbitration for older adults.
        - beta_wm0: base WM inverse temperature before capacity scaling.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, novelty_eta, wm_slots, age_penalty, beta_wm0 = model_parameters
    softmax_beta = beta_base * 10.0
    age_group = 1 if age[0] > 45 else 0

    # Age-adjust novelty and arbitration (older adults reduced novelty and WM influence)
    novelty_eta_eff = max(0.0, novelty_eta - age_group * max(0.0, age_penalty))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for novelty (start small to avoid div-by-zero)
        visits = 0.5 * np.ones((nS, nA))

        # Effective capacity factor from slots
        K = max(0.0, wm_slots)
        cap_factor = min(1.0, K / float(nS)) if nS > 0 else 0.0

        # Arbitration weight with age penalty
        wm_weight = max(0.0, min(1.0, cap_factor - age_group * max(0.0, age_penalty)))

        # WM precision scales with capacity
        softmax_beta_wm = max(1.0, beta_wm0 * (cap_factor if cap_factor > 0 else 0.1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with novelty bonus
            Q_s = q[s, :].copy()
            novelty_bonus = novelty_eta_eff / np.sqrt(visits[s, :] + 1e-8)
            Q_s_bonus = Q_s + novelty_bonus

            Qc = Q_s_bonus - np.max(Q_s_bonus)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy (slot-limited precision)
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update visits
            visits[s, a] += 1.0

            # WM update: win-store, lose-partial-reset
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Partial reset toward uniform with slight penalty to the chosen action
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
                w[s, a] = 0.8 * w[s, a] + 0.2 * (1.0 / nA)

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum <= 0.0:
                w[s, :] = w_0[s, :]
            else:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p