def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with binding noise and interference (set-size and age sensitive).

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores the most recent rewarded action per state, but is corrupted by binding noise
      (wrong action bound to the state) and subject to interference that grows with set size.
    - WM weight decreases with larger set size. Older adults show higher binding noise and interference.

    Parameters
    ----------
    states : array-like of int
        State index per trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size on each trial (constant within a block; 3 or 6).
    age : array-like of float
        Participant's age (constant repeated). >45 coded as older.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, binding_noise, interference]
        - alpha: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature before scaling.
        - wm_weight_base: baseline mixture weight for WM (0..1).
        - binding_noise: probability mass diverted from the intended WM one-hot to uniform (0..1).
        - interference: rate of WM decay toward uniform, scales with set size (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, binding_noise, interference = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL policy
    softmax_beta_wm = 50.0  # WM assumed near-deterministic

    age_group = 1 if age[0] > 45 else 0

    # Age modulations
    binding_noise_age = np.clip(binding_noise * (1.2 if age_group == 1 else 1.0), 0.0, 1.0)
    interference_age_scale = (1.2 if age_group == 1 else 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight diminishes with larger set size
        wm_weight_block = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)
        # Interference grows with set size and age
        interference_eff = np.clip(interference * (float(nS) / 3.0) * interference_age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM interference (forget toward uniform on the visited state)
            w[s, :] = (1.0 - interference_eff) * w[s, :] + interference_eff * w_0[s, :]

            # WM binding on rewarded trials: one-hot with binding noise to uniform
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - binding_noise_age) * onehot + binding_noise_age * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with epsilon exploration + recency-gated WM (set-size and age sensitive).

    Idea:
    - RL learns Q-values with a single learning rate; choices include epsilon-random lapses
      that increase with set size and age.
    - WM stores the last rewarded action per state. Its influence is gated by recency: if a state
      hasn't been seen recently, WM contribution decays exponentially with the time since last visit.
    - WM weight decreases with larger set size; older adults have higher random exploration and
      shorter effective recency span.

    Parameters
    ----------
    states : array-like of int
        State index per trial within a block.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size on each trial (constant within block).
    age : array-like of float
        Participant's age (constant repeated). >45 coded as older.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, epsilon, recency_tau]
        - alpha: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature before scaling.
        - wm_weight_base: baseline WM mixture weight (0..1).
        - epsilon: baseline random choice rate; increases with set size and age (0..1).
        - recency_tau: time constant (in trials) controlling WM recency gating.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, epsilon, recency_tau = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    # Age effects
    epsilon_age = np.clip(epsilon * (1.3 if age_group == 1 else 1.0), 0.0, 1.0)
    recency_tau_eff_age = max(1e-6, recency_tau * (0.8 if age_group == 1 else 1.0))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store and last seen indices
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_seen = -1 * np.ones(nS, dtype=int)  # -1 means never seen

        # Set-size modulation
        wm_weight_base_block = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)
        epsilon_block = np.clip(epsilon_age * (float(nS) / 3.0), 0.0, 0.49)  # cap to keep mixture stable

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy from stored map
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Recency gating of WM weight for current state
            dt = t - last_seen[s] if last_seen[s] >= 0 else 1e9
            recency_factor = np.exp(-float(dt) / recency_tau_eff_age) if last_seen[s] >= 0 else 0.0
            # Confidence in WM mapping for this state (peakedness)
            conf = max(0.0, (np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA))
            wm_weight_s = np.clip(wm_weight_base_block * recency_factor * conf, 0.0, 1.0)

            # Combine WM with RL, then mix with epsilon-random
            p_mix = wm_weight_s * p_wm + (1.0 - wm_weight_s) * p_rl
            p_total = (1.0 - epsilon_block) * p_mix + epsilon_block * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: on reward, store one-hot; on no reward, leave as is
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # Update last seen index
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated WM + RL with volatility-modulated learning and Q-decay.

    Idea:
    - RL learning rate adapts with surprise (Pearce-Hall): alpha_t = alpha_base + kappa_vol * |delta|.
      Q-values decay toward uniform (leaky memory), stronger with larger set size and in older adults.
    - WM weight increases when RL is uncertain (high entropy) and decreases with larger set size and age.
    - WM uses a tunable temperature (beta_wm) instead of being fully deterministic.

    Parameters
    ----------
    states : array-like of int
        State index per trial within a block.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size on each trial (constant within block).
    age : array-like of float
        Participant's age (constant repeated). >45 coded as older.
    model_parameters : list or array
        [alpha_base, softmax_beta, wm_weight_base, kappa_vol, q_decay, beta_wm_scale]
        - alpha_base: baseline RL learning rate (0..1).
        - softmax_beta: RL inverse temperature before scaling.
        - wm_weight_base: baseline WM mixture weight (0..1).
        - kappa_vol: sensitivity of learning rate to surprise |delta| (0..1).
        - q_decay: leak of Q toward uniform per visit; scales with set size and age (0..1).
        - beta_wm_scale: scales WM inverse temperature between moderate and high values (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, softmax_beta, wm_weight_base, kappa_vol, q_decay, beta_wm_scale = model_parameters
    softmax_beta *= 10.0
    # Map beta_wm_scale in [0,1] to [10, 100]
    softmax_beta_wm = 10.0 + 90.0 * np.clip(beta_wm_scale, 0.0, 1.0)

    age_group = 1 if age[0] > 45 else 0

    # Age effects
    wm_age_factor = 0.8 if age_group == 1 else 1.0            # older rely less on WM
    kappa_vol_age = kappa_vol * (0.8 if age_group == 1 else 1.0)  # older attenuated volatility use
    q_decay_age = q_decay * (1.2 if age_group == 1 else 1.0)      # older stronger Q decay

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects
        wm_weight_block_base = np.clip(wm_weight_base * (3.0 / float(nS)) * wm_age_factor, 0.0, 1.0)
        q_decay_eff = np.clip(q_decay_age * (float(nS) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Uncertainty gating of WM by RL entropy
            # Convert Q_s into a softmax policy to compute entropy proxy
            qsoft = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            qsoft = qsoft / np.sum(qsoft) if np.sum(qsoft) > 0 else np.ones_like(qsoft) / nA
            entropy = -np.sum(qsoft * np.log(np.clip(qsoft, 1e-12, 1.0)))
            entropy_norm = entropy / np.log(nA)  # 0..1

            wm_weight_s = np.clip(wm_weight_block_base * entropy_norm, 0.0, 1.0)

            p_total = wm_weight_s * p_wm + (1.0 - wm_weight_s) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with volatility-modulated learning rate
            delta = r - Q_s[a]
            alpha_t = np.clip(alpha_base + kappa_vol_age * abs(delta), 0.0, 1.0)
            q[s, a] += alpha_t * delta

            # Q decay toward uniform on visited state
            q[s, :] = (1.0 - q_decay_eff) * q[s, :] + q_decay_eff * (1.0 / nA)

            # WM update: reward strengthens mapping; otherwise slight leak toward uniform
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # gentle leak mirrors Q decay scale
                w[s, :] = (1.0 - 0.5 * q_decay_eff) * w[s, :] + (0.5 * q_decay_eff) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p