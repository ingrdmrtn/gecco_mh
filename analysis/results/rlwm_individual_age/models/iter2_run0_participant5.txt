def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: Priority-gated WM encoding with PE-threshold and age/set-size modulated maintenance

    Description:
    - RL: Q-learning with softmax action selection.
    - WM: A graded memory trace per state that decays toward uniform. Encoding into WM is prioritized:
      trials with large surprise (|prediction error|) or positive reward are preferentially encoded.
    - Mixture: Convex combination of RL and WM policies. WM weight is modulated by set size (larger set size
      reduces WM reliance) and age group (older adults reduce WM reliance).
    - Set size effect: Increases effective WM decay (more interference) and reduces WM mixture weight.
    - Age effect: Older group has stronger decay and reduced WM weight; here participant is young (age group=0).

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base mixture weight for WM (0..1), modulated by set size and age
    - softmax_beta: Inverse temperature for RL (scaled by 10 internally)
    - wm_pe_thresh: Threshold on absolute prediction error for WM encoding (>=0); larger -> stricter encoding
    - wm_decay_ss: Base WM decay rate toward uniform that's further scaled by set size and age (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_pe_thresh, wm_decay_ss = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age modulations
        # Larger sets -> less WM reliance, more decay; Older -> more reduction and more decay
        ss_scale = 3.0 / nS
        wm_weight_eff_base = wm_weight_base * ss_scale
        wm_weight_age = wm_weight_eff_base * (1.0 - 0.35 * age_group)
        wm_weight_age = float(np.clip(wm_weight_age, 0.0, 1.0))

        # WM decay increases with set size and age
        wm_decay = wm_decay_ss * (nS / 3.0) * (1.0 + 0.5 * age_group)
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy via nearly deterministic softmax over W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_age * p_wm + (1.0 - wm_weight_age) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform every trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Priority-gated WM encoding:
            # - Always encode after reward
            # - If no reward, encode only if |PE| exceeds threshold
            if (r > 0.0) or (abs(pe) >= wm_pe_thresh):
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Encoding strength proportional to surprise, capped to [0,1]
                enc_strength = float(np.clip(abs(pe), 0.0, 1.0))
                # If rewarded and PE small, still encode with minimal strength to lock in correct response
                if r > 0.0:
                    enc_strength = max(enc_strength, 0.5)
                w[s, :] = (1.0 - enc_strength) * w[s, :] + enc_strength * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: Uncertainty-weighted RL/WM arbitration with set-size and age effects

    Description:
    - RL: Q-learning with softmax; maintains a running uncertainty signal from squared prediction errors.
    - WM: Reward-driven associative memory that decays toward uniform; readout via a moderate-precision softmax.
    - Arbitration: Trial-wise mixture weight favors WM when RL uncertainty is high and RL when uncertainty is low.
      Base WM weight is also reduced by larger set sizes and by older age.
    - Set size effect: Reduces WM reliability (precision) and mixture contribution through a 1/nS factor.
    - Age effect: Older group penalizes WM precision and weight; here participant is young (age=19).

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - alpha_u: Learning rate for RL uncertainty (0..1)
    - wm_beta_base: Base inverse temperature for WM readout (>=0)
    - age_wm_penalty: Magnitude of WM penalty applied to older group (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, alpha_u, wm_beta_base, age_wm_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL uncertainty per state initialized moderately
        u = 0.25 * np.ones(nS)

        # Set-size and age modulations
        # WM weight base reduced by set size and age
        ss_factor = 3.0 / nS
        wm_weight_eff_base = wm_weight_base * ss_factor
        wm_weight_eff = wm_weight_eff_base * (1.0 - age_wm_penalty * age_group)
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

        # WM policy precision reduced with larger set sizes and older age
        wm_beta_eff = wm_beta_base * ss_factor * (1.0 - 0.3 * age_group)
        wm_beta_eff = max(1e-3, wm_beta_eff)  # keep positive

        # WM decay mild but present
        wm_decay = 0.15 * (nS / 3.0) * (1.0 + 0.3 * age_group)
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with moderate precision
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Arbitration via uncertainty: favor WM when RL is uncertain
            # Normalize uncertainty to [0,1] with a sigmoid-like mapping
            u_s = float(u[s])
            wm_arbi = u_s / (u_s + 0.5)  # increases with uncertainty
            mix = float(np.clip(wm_weight_eff * wm_arbi, 0.0, 1.0))

            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Update RL uncertainty (EWMA of squared PE)
            u[s] = (1.0 - alpha_u) * u[s] + alpha_u * (pe * pe)

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-driven WM encoding: on reward, move toward one-hot; on no reward, weak update
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * onehot
            else:
                # mild reinforcement of the chosen action even without reward to reflect hypothesis testing
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.9 * w[s, :] + 0.1 * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: Capacity-limited slot WM with access noise and age-modulated capacity

    Description:
    - RL: Standard Q-learning with softmax.
    - WM: Discrete slots store up to K state-action associations per block.
      If a state's association is stored, WM policy is near-deterministic for that action; otherwise uniform.
      Access noise increases with set size (harder retrieval) and age (less reliable access).
    - Mixture: Fixed base WM weight modulated by whether the state is stored and by access reliability.
    - Set size effect: Reduces access reliability (more interference) and increases eviction pressure when nS > K.
    - Age effect: Reduces effective K (capacity) and access reliability; here participant is young (no penalty).

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - K_slots_base: Base number of WM slots (>=1)
    - wm_refresh: Probability to refresh the currently accessed memory trace (0..1), reducing forgetting
    - age_K_penalty: Fractional reduction in K for older group (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, K_slots_base, wm_refresh, age_K_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # used for probabilistic readout from stored items
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and access reliability
        K_eff = int(max(1, np.floor(K_slots_base * (1.0 - age_K_penalty * age_group))))
        access_rel = (K_eff / nS)  # less reliable when set > capacity
        access_rel *= (1.0 - 0.25 * age_group)  # older less reliable
        access_rel = float(np.clip(access_rel, 0.0, 1.0))

        # Storage: dictionary of stored mappings and recency for LRU eviction
        stored = {}           # state -> action
        recency = {}          # state -> last access time

        # Forgetting probability increases with set size and age if not refreshed
        forget_base = 0.05 * (nS / 3.0) * (1.0 + 0.5 * age_group)
        forget_base = float(np.clip(forget_base, 0.0, 0.5))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Determine if s is in WM and retrieve with access noise
            in_mem = s in stored
            retrieved = in_mem and (np.random.rand() < access_rel)

            if retrieved:
                stored_action = stored[s]
                # Deterministic WM policy favoring stored action
                onehot = np.zeros(nA)
                onehot[stored_action] = 1.0
                W_s = 0.95 * onehot + 0.05 * (1.0 / nA)  # small noise
            else:
                # No reliable WM info
                W_s = (1.0 / nA) * np.ones(nA)

            # Form WM and RL choice probabilities
            # For WM softmax precision, use a high fixed beta if retrieved, else low precision
            beta_wm = 40.0 if retrieved else 1.0
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Mixture weight: scale base WM weight by access reliability and whether retrieved
            wm_weight = wm_weight_base * (access_rel if in_mem else 0.0)
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM maintenance: refresh on access, otherwise possible forgetting
            if in_mem:
                recency[s] = t
                if np.random.rand() < wm_refresh:
                    # Refresh strengthens w toward stored one-hot
                    onehot = np.zeros(nA)
                    onehot[stored[s]] = 1.0
                    w[s, :] = 0.8 * w[s, :] + 0.2 * onehot
                else:
                    # Potential forgetting
                    if np.random.rand() < forget_base:
                        del stored[s]
                        del recency[s]
                        w[s, :] = w_0[s, :].copy()

            # WM encoding on reward: store association; if full, evict least recently used
            if r > 0.0:
                if s not in stored:
                    if len(stored) >= K_eff:
                        # Evict least recently used state
                        lru_state = min(recency, key=recency.get) if len(recency) > 0 else None
                        if lru_state is not None:
                            del stored[lru_state]
                            del recency[lru_state]
                            w[lru_state, :] = w_0[lru_state, :].copy()
                    stored[s] = a
                recency[s] = t
                # Update w to reflect stored association
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * onehot
            else:
                # On no reward, weak move toward chosen action to allow tentative storage signal
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.9 * w[s, :] + 0.1 * onehot

        blocks_log_p += log_p

    return -blocks_log_p