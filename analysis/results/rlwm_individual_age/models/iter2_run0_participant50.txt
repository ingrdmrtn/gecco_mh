def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM gating with eligibility traces and age/set-size dependent WM access.

    Idea
    - A model-free RL system learns action values with eligibility traces.
    - A working-memory (WM) store holds recently reinforced state-action pairs.
    - On each trial, WM is accessed with a probability p_gate that decreases with set size
      and more so for older adults; otherwise RL policy is used.
    - If WM has a stored action for the current state, it yields a near-deterministic policy.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial (blocks reset memory).
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_gate_base, gate_age_penalty, gate_ss_penalty, trace_lambda]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_gate_base: baseline probability to access WM (mapped via sigmoid).
        - gate_age_penalty: penalty subtracted when age_group=1 (older); reduces WM access.
        - gate_ss_penalty: penalty per step-up in set size (0 for 3, 1 for 6); reduces WM access.
        - trace_lambda: eligibility trace decay within a block (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, beta_base, wm_gate_base, gate_age_penalty, gate_ss_penalty, trace_lambda = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6
        gate_input = wm_gate_base - gate_age_penalty * age_group - gate_ss_penalty * ss_penalty
        p_gate = float(np.clip(sigmoid(gate_input), 0.0, 1.0))

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy: if memory is sharp for this state, it will be near-deterministic
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixed arbitration via probabilistic WM access
            p_total = p_gate * p_wm + (1.0 - p_gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # Set current eligibility and decay others
            e *= trace_lambda
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += lr * delta * e

            # WM decay slightly toward uniform (natural forgetting)
            w = 0.99 * w + 0.01 * w_0

            # If rewarded, store this mapping strongly in WM (one-hot)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with WM decay and age-amplified perseveration bias.

    Idea
    - Choices follow a mixture of RL and WM policies.
    - WM decays faster at larger set sizes, harming retrieval.
    - RL policy includes a choice perseveration bias that is stronger in older adults,
      capturing age-related stickiness/response inertia.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_weight_base, wm_decay_base, pers_base, age_pers_boost]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature; internally scaled (*10).
        - wm_weight_base: baseline WM mixture weight (0..1) applied equally across trials.
        - wm_decay_base: baseline WM decay factor that scales up with set size.
        - pers_base: perseveration strength added to the last chosen action.
        - age_pers_boost: additional perseveration added for older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, beta_base, wm_weight_base, wm_decay_base, pers_base, age_pers_boost = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 (size=3) or 1 (size=6)

        wm_weight = float(np.clip(wm_weight_base, 0.0, 1.0))
        wm_decay = float(np.clip(wm_decay_base * (1.0 + ss_penalty), 0.0, 1.0))

        pers = pers_base + age_pers_boost * age_group

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = None  # global perseveration (can reflect response inertia)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            if last_action is not None:
                pref[last_action] += pers  # add stickiness to the previously chosen action
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy from current state's WM record
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (faster at larger set sizes)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Rewarded outcomes refresh WM for the state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update perseveration memory
            last_action = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration: RL with forgetting + WM, arbitration depends on RL uncertainty,
    with capacity drops for larger set sizes and older adults.

    Idea
    - RL values forget toward uniform baseline and learn via a standard delta rule.
    - RL uncertainty (per-state visit count) drives the arbitration: more WM when RL is uncertain.
    - WM capacity is reduced with larger set sizes and further reduced in older adults.
    - Final choice is a mixture of WM and RL policies with dynamic weight w_t per trial.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_strength_base, capacity_drop, age_drop, rl_forget]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_strength_base: baseline WM arbitration strength (logit space).
        - capacity_drop: reduction in WM weight for larger set sizes (logit scale).
        - age_drop: reduction in WM weight for older adults (logit scale).
        - rl_forget: RL forgetting rate toward uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, beta_base, wm_strength_base, capacity_drop, age_drop, rl_forget = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state visit counts to approximate RL uncertainty
        n_visits = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # RL uncertainty (higher when fewer visits)
            uncertainty = 1.0 / np.sqrt(n_visits[s] + 1.0)

            # Arbitration: logit-weight combining base WM strength with uncertainty drive,
            # reduced by set size and age.
            logit_w = (
                wm_strength_base
                - capacity_drop * ss_penalty
                - age_drop * age_group
                + 5.0 * uncertainty
            )
            wm_weight = float(np.clip(sigmoid(logit_w), 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL forgetting toward uniform values plus delta learning
            baseline = (1.0 / nA) * np.ones(nA)
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * baseline
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive decay toward uniform
            w = 0.995 * w + 0.005 * w_0

            # Reward strengthens WM for the state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update uncertainty tracker
            n_visits[s] += 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)