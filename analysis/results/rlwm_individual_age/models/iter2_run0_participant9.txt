def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-weighted arbitration, RL forgetting, and age- and set-size-modulated WM reliance.

    Idea
    - Model-free RL with learning rate and value forgetting toward uniform.
    - WM stores rewarded state-action pairs one-shot and decays toward uniform.
    - Arbitration weight for WM increases when RL is uncertain (high entropy) and
      decreases with set size. Sensitivity to uncertainty is reduced in the older group.

    Parameters (6)
    - alpha_rl: RL learning rate (0..1)
    - beta: RL inverse temperature; scaled by 10 internally
    - k_forget: RL forgetting rate toward uniform (0..1 per trial)
    - wm_decay: WM decay toward uniform per trial (0..1); higher = faster decay
    - wm_w0: Base WM weight (0..1) before uncertainty and set-size modulation
    - eta_uncert: Strength of uncertainty-weighted arbitration (>=0); halved in older group

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta, k_forget, wm_decay, wm_w0, eta_uncert = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    eta_eff = eta_uncert * (1.0 if age_group == 0 else 0.5)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute uniform vector and its entropy
        p_unif = np.ones(nA) / nA
        H_unif = np.log(nA)

        # Base WM weight scaled by set size (fewer items => more WM)
        wm_size_scale = 3.0 / float(nS)
        wm_w_base = np.clip(wm_w0, 0.0, 1.0) * wm_size_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl) if np.isfinite(exp_rl).all() else p_unif
            p_rl = float(p_rl_vec[a])

            # WM policy
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm) if np.isfinite(exp_wm).all() else p_unif
            p_wm = float(p_wm_vec[a])

            # RL uncertainty via entropy
            entropy_rl = -np.sum(p_rl_vec * np.log(np.maximum(p_rl_vec, 1e-12)))
            # Arbitration: WM weight increases with RL uncertainty
            wm_weight = wm_w_base + eta_eff * (entropy_rl / H_unif) * (1.0 - wm_w_base)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe
            q[s, :] = (1.0 - k_forget) * q[s, :] + k_forget * p_unif

            # WM decay toward uniform
            decay = np.clip(wm_decay, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # One-shot store on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning and a competing Win-Stay/Lose-Shift (WSLS) controller,
    plus set-size- and age-modulated lapse.

    Idea
    - RL uses separate learning rates for gains and losses.
    - A WSLS policy acts as a heuristic controller operating per state.
    - Final choice is a mixture of RL and WSLS, wrapped with a lapse to uniform
      that grows with set size and is higher in the older group.

    Parameters (6)
    - alpha_pos: RL learning rate for positive PE (0..1)
    - alpha_neg: RL learning rate for negative PE (0..1)
    - beta: RL inverse temperature; scaled by 10 internally
    - ws_weight: Baseline weight on WSLS vs RL (0..1)
    - ws_lose_shift: Tendency to shift away from last action after loss (0..1)
    - lapse_base: Baseline lapse to uniform (0..1), scaled by set size and age

    Age effect
    - Lapse is increased by 50% in older group.

    Set-size effect
    - Lapse increases linearly with set size: lapse = lapse_base * (nS/3).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, ws_weight, ws_lose_shift, lapse_base = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # not used here; placeholder to respect template idea
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL initialization
        q = (1.0 / nA) * np.ones((nS, nA))

        # State-wise memory for WSLS
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl) if np.isfinite(exp_rl).all() else (np.ones(nA) / nA)
            p_rl = float(p_rl_vec[a])

            # WSLS policy per state s
            p_wsls_vec = np.ones(nA) / nA
            if last_action[s] >= 0:
                if last_reward[s] > 0.0:
                    # Win-Stay: choose last action with high prob
                    p_wsls_vec = np.ones(nA) * (1.0 - 1e-6) / (nA - 1)
                    p_wsls_vec[last_action[s]] = 1.0 - (nA - 1) * (1.0 - 1e-6) / (nA - 1)
                else:
                    # Lose-Shift: shift away with probability ws_lose_shift, uniformly among others
                    p_wsls_vec = np.ones(nA) * (ws_lose_shift / (nA - 1))
                    p_wsls_vec[last_action[s]] = 1.0 - ws_lose_shift
            p_wsls = float(p_wsls_vec[a])

            # Mixture RL + WSLS
            mix_w = np.clip(ws_weight, 0.0, 1.0)
            p_mix = mix_w * p_wsls + (1.0 - mix_w) * p_rl

            # Lapse to uniform increases with set size and more for older
            lapse = np.clip(lapse_base, 0.0, 1.0) * (float(nS) / 3.0) * (1.5 if age_group == 1 else 1.0)
            lapse = np.clip(lapse, 0.0, 0.5)  # guard
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning (valence-asymmetric)
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated WM encoding, surprise-adaptive RL learning rate, and age-dependent stickiness.

    Idea
    - RL learning rate increases with unsigned prediction error (surprise).
    - WM encodes rewarded pairs probabilistically as a function of surprise, then decays.
    - Final policy mixes RL and WM based on current WM strength; includes action stickiness
      that is stronger in the older group.

    Parameters (6)
    - alpha_base: Baseline RL learning rate (0..1)
    - beta: RL inverse temperature; scaled by 10 internally
    - k_surprise: Scales surprise-adaptive learning rate (>=0), alpha_eff = alpha_base + k*|PE|, clipped to 0..1
    - wm_gate: Scales probability of WM encoding given surprise (>=0)
    - stickiness: Perseveration strength added to last-chosen action in a state (>=0)
    - size_penalty: Penalizes WM influence and encoding with larger set size (>=0)

    Age effect
    - Stickiness is 30% stronger in the older group.
    - WM influence is reduced by size_penalty and more so in the older group (extra 20%).

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_base, beta, k_surprise, wm_gate, stickiness, size_penalty = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    rng = np.random  # assume external control if needed; stochasticity only affects encoding expectation via probability weight

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size and age modulation factors
        wm_size_factor = 3.0 / float(nS)  # fewer items => larger WM impact
        wm_age_factor = 1.0 - 0.2 * (1 if age_group == 1 else 0)  # 20% reduction if older
        stick_eff = stickiness * (1.3 if age_group == 1 else 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with stickiness bias
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = stick_eff
            logits_rl = softmax_beta * (Q_s - np.max(Q_s)) + bias
            exp_rl = np.exp(logits_rl - np.max(logits_rl))
            p_rl_vec = exp_rl / np.sum(exp_rl) if np.isfinite(exp_rl).all() else (np.ones(nA) / nA)
            p_rl = float(p_rl_vec[a])

            # WM policy
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm) if np.isfinite(exp_wm).all() else (np.ones(nA) / nA)
            p_wm = float(p_wm_vec[a])

            # WM arbitration weight based on current WM strength in this state
            wm_strength = max(0.0, float(np.max(W_s) - (1.0 / nA))) / (1.0 - (1.0 / nA) + 1e-12)
            wm_weight = wm_strength * wm_size_factor * wm_age_factor
            wm_weight = wm_weight / (1.0 + size_penalty * (float(nS) - 3.0) / 3.0)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning with surprise-adaptive rate
            pe = r - Q_s[a]
            alpha_eff = np.clip(alpha_base + k_surprise * abs(pe), 0.0, 1.0)
            q[s, a] += alpha_eff * pe

            # WM decay toward uniform
            w[s, :] = 0.85 * w[s, :] + 0.15 * w_0[s, :]  # mild decay each trial
            # Surprise-gated WM encoding when rewarded
            if r > 0.0:
                p_enc = 1.0 / (1.0 + np.exp(-wm_gate * abs(pe)))  # sigmoid of surprise
                # Downscale encoding by set size and age
                p_enc *= wm_size_factor * wm_age_factor
                p_enc = np.clip(p_enc, 0.0, 1.0)
                # Use expected update (no randomness): move W toward a one-hot on action a
                w_target = np.zeros(nA)
                w_target[a] = 1.0
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * w_target

            # Update stickiness memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p