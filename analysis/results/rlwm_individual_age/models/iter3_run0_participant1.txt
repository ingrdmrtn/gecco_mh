def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size-dependent WM temperature.

    Idea:
    - RL learns state-action values with a single learning rate and softmax policy.
    - WM stores rewarded associations and produces a softmax policy over its memory trace.
    - WM becomes noisier as set size increases (higher effective temperature).
    - Arbitration weight is driven by the relative uncertainty (entropy) of the two policies.
      Age biases arbitration away from WM for older adults.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): inverse temperature for RL softmax
    - model_parameters[2] = wm_beta_base (>0): base inverse temperature for WM softmax
    - model_parameters[3] = arb_kappa (>0): sensitivity that maps entropy difference to WM weight
    - model_parameters[4] = setsize_wm_temp_gain (>=0): increases WM noise as set size grows
    - model_parameters[5] = age_arb_bias (>=0): shifts arbitration away from WM for older adults

    Inputs:
    - states: array of state indices per trial (ints)
    - actions: array of chosen actions per trial (ints in {0,1,2})
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block index per trial
    - set_sizes: array of set size per trial (3 or 6)
    - age: array with a single repeated value (years)
    - model_parameters: list/array of parameters described above

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_beta_base, arb_kappa, setsize_wm_temp_gain, age_arb_bias = model_parameters

    # Parameter transforms / bounds
    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_beta_base = max(wm_beta_base, 1e-6)
    arb_kappa = max(arb_kappa, 0.0)
    setsize_wm_temp_gain = max(setsize_wm_temp_gain, 0.0)
    age_arb_bias = max(age_arb_bias, 0.0)

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    def softmax_prob(beta, prefs, a_idx):
        denom = np.sum(np.exp(beta * (prefs - prefs[a_idx])))
        return 1.0 / max(denom, eps)

    def entropy_from_p(pvec):
        p = np.clip(pvec, eps, 1.0)
        p = p / np.sum(p)
        return -np.sum(p * np.log(p))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM distribution per state

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = softmax_prob(softmax_beta, Q_s, a)

            # WM policy with set-size-dependent temperature (noisy WM when set size increases)
            wm_beta_eff = wm_beta_base / (1.0 + setsize_wm_temp_gain * max(nS_t - 3, 0))
            W_s = w[s, :] / max(np.sum(w[s, :]), eps)
            # Action probability under WM
            denom_wm = np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute full distributions to estimate entropies
            # RL distribution
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(rl_logits)
            p_rl_vec = p_rl_vec / np.sum(p_rl_vec)
            # WM distribution
            wm_logits = wm_beta_eff * (W_s - np.max(W_s))
            p_wm_vec = np.exp(wm_logits)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)

            H_rl = entropy_from_p(p_rl_vec)
            H_wm = entropy_from_p(p_wm_vec)

            # Uncertainty-based arbitration: higher weight to the lower-entropy system.
            # Positive arb_kappa magnifies effect. Age shifts weight away from WM.
            # Map difference into [0,1] via logistic.
            # w_wm_raw increases as H_rl - H_wm grows (WM less uncertain than RL).
            w_wm_raw = 1.0 / (1.0 + np.exp(-arb_kappa * (H_rl - H_wm)))
            # Age penalty for older adults (reduce WM arbitration weight).
            w_wm = w_wm_raw * (1.0 - age_arb_bias * age_group)
            w_wm = min(max(w_wm, 0.0), 1.0)

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store rewarded association as a peaked memory (one-hot)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # If not rewarded, keep current WM trace (no decay term here; WM noise handled via wm_beta)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM gated by prediction error magnitude and availability costs.

    Idea:
    - RL uses separate learning rates for positive/negative prediction errors and softmax policy.
    - WM stores rewarded associations deterministically (one item per state).
    - Arbitration weight increases with absolute prediction error (strong learning signals favor WM use).
    - WM reliance is reduced by a combined availability cost of set size and age.

    Parameters (6):
    - model_parameters[0] = lr_pos in [0,1]: RL learning rate for positive prediction errors
    - model_parameters[1] = lr_neg in [0,1]: RL learning rate for negative prediction errors
    - model_parameters[2] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[3] = wm_weight_base in [0,1]: base reliance on WM
    - model_parameters[4] = gate_sensitivity (>0): maps |delta| to additional WM weight via logistic
    - model_parameters[5] = wm_availability_cost (>=0): exponential penalty of WM weight with set size and age

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: parameter list

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, gate_sensitivity, wm_availability_cost = model_parameters

    lr_pos = min(max(lr_pos, 0.0), 1.0)
    lr_neg = min(max(lr_neg, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_weight_base = min(max(wm_weight_base, 0.0), 1.0)
    gate_sensitivity = max(gate_sensitivity, 1e-6)
    wm_availability_cost = max(wm_availability_cost, 0.0)

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM memory as deterministic one-hot when encoded; otherwise uniform
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (deterministic if memory is peaked; else near-uniform)
            W_s = w[s, :] / max(np.sum(w[s, :]), eps)
            softmax_beta_wm = 50.0  # highly deterministic when a one-hot is stored
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # RL update components
            delta = r - Q_s[a]

            # Gating by prediction error magnitude: higher |delta| -> more WM reliance
            # Map |delta| in [0,1] through a logistic
            wm_gate = 1.0 / (1.0 + np.exp(-gate_sensitivity * (abs(delta))))
            wm_gate = min(max(wm_gate, 0.0), 1.0)

            # Availability cost: exponential reduction with set size and age
            availability_factor = np.exp(-wm_availability_cost * (max(nS_t - 3, 0) + age_group))
            wm_weight_eff = wm_weight_base * availability_factor
            wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

            # Final arbitration weight
            w_wm = wm_weight_eff * wm_gate
            w_wm = min(max(w_wm, 0.0), 1.0)

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            if delta >= 0.0:
                q[s, a] += lr_pos * delta
            else:
                q[s, a] += lr_neg * delta

            # WM update: encode rewarded association deterministically
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # No explicit decay; reliance is regulated by availability_factor and gating

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM and lapse.

    Idea:
    - RL uses eligibility traces to propagate credit within a block.
    - WM stores up to K_eff associations; if a state is stored, WM yields a deterministic policy.
    - Effective WM capacity K_eff decreases with set size and is reduced for older adults.
    - Arbitration uses availability: if state is stored, WM is used with weight proportional to K_eff/nS.
    - A small lapse probability mixes a uniform random choice into the final policy.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = lambda_et in [0,1]: eligibility trace decay parameter
    - model_parameters[3] = K_base (>0): baseline WM capacity (in items)
    - model_parameters[4] = K_age_drop (>=0): capacity drop for older adults (subtract from K_base)
    - model_parameters[5] = lapse in [0,0.2]: lapse probability mixing in uniform responding

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: parameter list

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, lambda_et, K_base, K_age_drop, lapse = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    lambda_et = min(max(lambda_et, 0.0), 1.0)
    K_base = max(K_base, 1e-6)
    K_age_drop = max(K_age_drop, 0.0)
    lapse = min(max(lapse, 0.0), 0.2)

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL init
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # WM storage: track which states are stored and their action
        stored_action = -np.ones(nS, dtype=int)  # -1 indicates not stored
        recency_counter = np.zeros(nS)  # for LRU eviction

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Update recency
            recency_counter += 1.0
            recency_counter[s] = 0.0

            # Effective capacity given set size and age
            # Reduce capacity with set size (divide by set size factor) and subtract age-related drop
            K_eff_cont = max(K_base - K_age_drop * age_group, 0.0) / (1.0 + max(nS_t - 3, 0))
            # Translate to an availability weight bounded by 0..1
            wm_availability_w = min(1.0, K_eff_cont / max(nS_t, 1))

            # Determine WM policy for this state
            if stored_action[s] >= 0:
                wm_policy = np.zeros(nA)
                wm_policy[stored_action[s]] = 1.0
                p_wm = wm_policy[a]
            else:
                # If not stored, WM contributes a uniform policy
                p_wm = 1.0 / nA

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # Arbitration: if stored, mix WM with RL using availability weight; else RL dominates
            if stored_action[s] >= 0:
                w_wm = wm_availability_w
            else:
                w_wm = 0.0

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            # Lapse
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility traces (replacing traces for chosen SA)
            # Decay traces
            e *= lambda_et
            # Increment chosen state-action trace
            e[s, a] = 1.0

            delta = r - Q_s[a]
            q += lr * delta * e

            # WM update on reward: store association; enforce capacity with LRU eviction
            if r > 0.0:
                if stored_action[s] == -1:
                    # need to store; check capacity
                    # Current number of stored states approximated by count of stored_action != -1
                    current_K = np.sum(stored_action >= 0)
                    # Allow up to floor(K_eff_cont) items
                    K_cap = int(np.floor(K_eff_cont + 1e-9))
                    if K_cap < 0:
                        K_cap = 0
                    if current_K >= K_cap and K_cap >= 0:
                        # Evict the least recently used stored state (largest recency)
                        if K_cap == 0 and current_K > 0:
                            # Evict all if capacity zero
                            stored_action[:] = -1
                        else:
                            candidates = np.where(stored_action >= 0)[0]
                            if candidates.size > 0:
                                evict_idx = candidates[np.argmax(recency_counter[candidates])]
                                stored_action[evict_idx] = -1
                # Store/update the rewarded action
                stored_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p