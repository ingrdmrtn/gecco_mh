def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and age-modulated arbitration sensitivity.

    Mechanism:
    - RL system: standard delta-rule with softmax choice.
    - WM system: fast, high-precision lookup updated by reward and decays toward uniform.
    - Arbitration: trial-by-trial mixture weight is a sigmoid of the entropy difference
      between RL and WM policies; when WM is more certain (lower entropy), arbitration favors WM.
      Arbitration sensitivity is age-dependent; WM decay scales with set size.

    Parameters (model_parameters):
    - alpha_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; internally scaled by 10 for a wide range
    - beta_wm: WM inverse temperature; higher values make WM more deterministic
    - wm_decay: baseline WM decay toward uniform each trial (0..1), scaled by set size
    - arbit_sens_young: arbitration sensitivity to entropy difference for young
    - arbit_sens_old: arbitration sensitivity to entropy difference for old

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index for each trial
    - set_sizes: set size for the block on each trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_rl, beta_rl, beta_wm, wm_decay, arbit_sens_young, arbit_sens_old = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    arbit_sens = arbit_sens_young if age_group == 0 else arbit_sens_old

    nA = 3
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax policy vector and chosen-action probability
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM softmax policy vector and chosen-action probability
            wm_logits = beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Entropy-based arbitration: weight favors lower-entropy system
            # H = -sum p log p
            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))
            # Positive when RL more uncertain than WM -> favor WM
            delta_H = H_rl - H_wm
            wm_weight = 1.0 / (1.0 + np.exp(-arbit_sens * delta_H))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha_rl * delta

            # WM decay scales with set size (more items -> more decay)
            decay_eff = np.clip(wm_decay * (nS / 6.0), 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * W_s + decay_eff * (1.0 / nA)

            # WM reinforcement: if rewarded, imprint the chosen action strongly
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Win-Stay/Lose-Shift heuristic (WSLS) with age- and set-size-dependent strength.

    Mechanism:
    - RL system: delta-rule with softmax.
    - Heuristic WM system: per-state WSLS:
        - If previous encounter of the state was rewarded, repeat last action (win-stay).
        - If not rewarded, avoid last action and randomize among the others (lose-shift).
      A 'switch temperature' controls the softness of lose-shift randomization.
      A lapse parameter mixes the heuristic with uniform noise.
    - Arbitration: fixed mixture where WSLS weight depends on age group and is scaled
      down by set size (stronger reliance on WSLS for smaller sets).

    Parameters (model_parameters):
    - alpha_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; scaled by 10 internally
    - ws_strength_young: base WSLS mixture weight for young (0..1)
    - ws_strength_old: base WSLS mixture weight for old (0..1)
    - ws_switch_temp: softness of lose-shift randomization (>=0); 0 -> uniform over other actions
    - lapse: lapse/noise mixing with uniform for the WSLS policy (0..1)

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index for each trial
    - set_sizes: set size for the block on each trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_rl, beta_rl, ws_strength_young, ws_strength_old, ws_switch_temp, lapse = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    ws_strength_base = ws_strength_young if age_group == 0 else ws_strength_old
    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))

        # Track per-state last action and last reward for WSLS
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WSLS policy vector
            p_ws = np.ones(nA) / nA
            if last_action[s] >= 0:
                la = int(last_action[s])
                if last_reward[s] > 0.0:
                    # Win-stay
                    p_ws = np.zeros(nA)
                    p_ws[la] = 1.0
                else:
                    # Lose-shift: avoid last action; distribute among others
                    p_ws = np.zeros(nA)
                    others = [aa for aa in range(nA) if aa != la]
                    # Optional softness across the two alternatives using a tiny bias from Q
                    if ws_switch_temp <= 0.0:
                        for aa in others:
                            p_ws[aa] = 1.0 / (nA - 1)
                    else:
                        # Prefer the better of the other actions according to RL values, softly
                        other_vals = np.array([Q_s[aa] for aa in others])
                        logits = ws_switch_temp * (other_vals - np.max(other_vals))
                        expv = np.exp(logits)
                        expv = expv / np.sum(expv)
                        for k, aa in enumerate(others):
                            p_ws[aa] = expv[k]
            # Lapse in the heuristic
            p_wm_vec = (1.0 - lapse) * p_ws + lapse * (1.0 / nA)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture weight: age base scaled by set size (smaller sets => stronger WSLS)
            ws_weight = np.clip(ws_strength_base * (3.0 / nS), 0.0, 1.0)

            p_total = ws_weight * p_wm + (1.0 - ws_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha_rl * delta

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Capacity-weighted WM mixture with age bias and noisy encoding.

    Mechanism:
    - RL: delta-rule with softmax.
    - WM: state-action weights that, upon reward, encode the chosen action as a
      near one-hot vector corrupted by encoding noise; otherwise, WM leaks toward uniform
      a small amount each trial. WM action selection uses a high-beta softmax.
    - Mixture: WM mixture weight is a sigmoid over a logit composed of:
        base term + age bias (penalizing older group) - set-size penalty.
      Thus, larger set sizes reduce reliance on WM; older age reduces reliance further.

    Parameters (model_parameters):
    - alpha_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; scaled by 10 internally
    - wm_weight0: base logit for WM mixture weight
    - gamma_ss: penalty per additional item beyond 3 on WM mixture weight (>=0)
    - age_bias: additional penalty applied when age_group == 1 (>=0 reduces WM in older)
    - eps_encode: WM encoding/leak noise (0..1); also drives leak scaled by set size

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_rl, beta_rl, wm_weight0, gamma_ss, age_bias, eps_encode = model_parameters
    beta_rl *= 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Compute static mixture weight for the block from age and set size
        ss_penalty = gamma_ss * max(nS - 3, 0)
        age_penalty = age_bias * (1 if age_group == 1 else 0)
        wm_weight_logit = wm_weight0 - ss_penalty - age_penalty
        wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_logit))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy vector
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy vector
            wm_logits = beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = max(p_wm_vec[a], 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha_rl * delta

            # WM leak toward uniform scales with set size via eps_encode
            leak = np.clip(eps_encode * (nS / 6.0), 0.0, 1.0)
            w[s, :] = (1.0 - leak) * W_s + leak * (1.0 / nA)

            # WM encoding on reward with noise
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eps_encode) * one_hot + eps_encode * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p