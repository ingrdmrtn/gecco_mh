Below are three alternative cognitive models, each as a standalone Python function. They all implement an RLâ€“WM mixture policy, return negative log-likelihood, use all parameters, and incorporate set-size and age effects in distinct ways. Each function follows the requested interface and fills in the missing WM policy and WM updating components.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + leaky WM with age-dependent WM weight and set-size-dependent WM decay.

    Mechanism
    - RL: Asymmetric learning rates for positive vs negative prediction errors; softmax choice.
    - WM: Row-wise memory of rewarded mappings that decays toward uniform after each trial.
           Effective WM decay increases in larger set sizes (more interference).
    - Mixture: Choice probability is a weighted mixture of WM and RL policies.
    - Age effect: WM mixture weight depends on age group (separate weights for younger vs older).

    Parameters (6)
    - model_parameters:
        0) lr_pos: RL learning rate for positive PE (0..1)
        1) lr_neg: RL learning rate for negative PE (0..1)
        2) beta_base: Base RL inverse temperature; scaled by 10 internally
        3) wm_weight_young: WM mixture weight for young participants (0..1)
        4) wm_weight_old: WM mixture weight for old participants (0..1)
        5) size_wm_penalty: Scales how much increasing set size (from 3 to 6) accelerates WM decay (>0)
    Returns
    - Negative log-likelihood of observed choices
    """
    lr_pos, lr_neg, beta_base, wm_weight_young, wm_weight_old, size_wm_penalty = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    wm_weight = wm_weight_young if age_group == 0 else wm_weight_old

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay: more decay when set size is larger
        # Base decay chosen small; scaled by size_wm_penalty*(nS-3)
        base_decay = 0.05
        wm_decay = base_decay * (1.0 + size_wm_penalty * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (soft via softmax with high beta)
            softmax_beta_wm = 50.0
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rate
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update: on reward, store one-shot; otherwise, decay toward uniform
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + capacity-limited WM slots (with age-dependent capacity) + state-wise stickiness.

    Mechanism
    - RL: Single learning rate with value forgetting toward 1/nA each trial; softmax choice.
    - Stickiness: Adds a per-state bias toward repeating the last action taken in that state.
    - WM: Capacity-limited store of rewarded mappings (FIFO replacement). If a state's mapping is in WM,
          WM policy is precise; otherwise it is uniform. WM 'precision' is governed by a dedicated WM beta.
    - Mixture: WM and RL policies are mixed by a single weight.
    - Set-size effect: Capacity is bounded by set size, and forgetting hurts more as set size grows (via normalization).
    - Age effect: WM capacity is shifted for older group.

    Parameters (6)
    - model_parameters:
        0) lr: RL learning rate (0..1)
        1) beta_base: RL inverse temperature; scaled by 10 internally
        2) stickiness: Bias added to the last-chosen action in a state (in Q-units)
        3) wm_beta: WM inverse temperature for states stored in WM (>0)
        4) K_base: Base WM capacity (number of states that can be held)
        5) age_K_shift: Additive capacity shift for older group (can be negative)
    Returns
    - Negative log-likelihood of observed choices
    """
    lr, beta_base, stickiness, wm_beta, K_base, age_K_shift = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective capacity with age effect; bounded to [0, nS]
        K_eff = int(np.clip(np.round(K_base + (age_K_shift if age_group == 1 else 0.0)), 0, nS))

        # RL init and forgetting baseline
        q = (1.0 / nA) * np.ones((nS, nA))
        q_baseline = (1.0 / nA) * np.ones((nS, nA))

        # WM storage
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        stored_states = []  # FIFO list of states currently in WM

        # Track last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness: add bias to last action if it matches
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if state is stored, use precise WM; else uniform
            if s in stored_states and np.max(w[s, :]) > (1.0 / nA):
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))
            else:
                # Unstored behaves like uniform (equivalently, very low beta)
                p_wm = 1.0 / nA

            # Mixture weight (fixed here; can be implicitly modulated by capacity via p_wm quality)
            wm_weight = 0.5  # implicit; mixing governed more by whether s is in WM
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward baseline after each trial
            pe = r - q[s, a]
            q[s, a] += lr * pe
            # Mild global forgetting toward baseline; stronger interference at larger set sizes via normalization factor
            forget_rate = 0.02 * (nS / 3.0)
            q = (1.0 - forget_rate) * q + forget_rate * q_baseline

            # WM update: on reward, insert/update mapping and manage capacity
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
                if s not in stored_states:
                    stored_states.append(s)
                    # enforce capacity
                    if K_eff < len(stored_states):
                        evict = stored_states.pop(0)
                        w[evict, :] = w_0[evict, :]
            else:
                # No change if not rewarded (leave as is)
                pass

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus (directed exploration) + WM win-stay and set-size-scaled learning rate.

    Mechanism
    - RL: Single learning rate scaled down in larger set sizes; softmax choice.
    - Directed exploration: Adds an uncertainty bonus to Q via a UCB-like term derived from visit counts.
    - WM: Win-stay memory per state with its own softmax precision (wm_beta).
    - Mixture: Fixed WM weight mixed with RL policy.
    - Set-size effect: Learning rate shrinks as set size increases (harder credit assignment).
    - Age effect: Younger participants rely relatively more on WM (slightly higher fixed weight).

    Parameters (6)
    - model_parameters:
        0) lr_base: Base RL learning rate (0..1)
        1) beta_base: Base RL inverse temperature; scaled by 10 internally
        2) wm_weight: Base WM mixture weight (used for older group; younger receive a small boost internally)
        3) wm_beta: WM inverse temperature (>0)
        4) size_lr_penalty: Scales how much set size reduces learning rate (>0)
        5) ucb_bonus: Weight on the uncertainty bonus (>0)
    Returns
    - Negative log-likelihood of observed choices
    """
    lr_base, beta_base, wm_weight_base, wm_beta, size_lr_penalty, ucb_bonus = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size effect on learning rate
        lr = lr_base / (1.0 + size_lr_penalty * max(0, nS - 3))

        # WM mixture weight with simple age effect (younger rely a bit more on WM)
        wm_weight = wm_weight_base if age_group == 1 else np.clip(wm_weight_base + 0.1, 0.0, 1.0)

        # Initialize Q and visitation counts for UCB-style uncertainty
        q = (1.0 / nA) * np.ones((nS, nA))
        N = 1e-6 * np.ones((nS, nA))  # to avoid div-by-zero

        # WM tables
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with uncertainty bonus (UCB-like)
            # Bonus ~ ucb_bonus / sqrt(N), larger for less tried actions
            bonus = ucb_bonus / np.sqrt(N[s, :])
            Q_aug = q[s, :] + bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy: win-stay distribution with its own beta
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update and counts update
            pe = r - q[s, a]
            q[s, a] += lr * pe
            N[s, a] += 1.0

            # WM update: win-stay (store one-hot on reward), no change on loss
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                pass

        blocks_log_p += log_p

    return -blocks_log_p