def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay and interference-susceptible WM; age- and set-size–modulated arbitration.

    Description:
    - Policy is a mixture of model-free RL and a WM store.
    - RL updates via a single learning rate with value decay toward uniform.
    - WM stores a belief distribution for each state; after reward, WM moves toward a one-hot
      code for the chosen action; between trials WM suffers interference that grows with set size
      and is worse for older age.
    - Arbitration weight (wm_weight) is scaled by set size (lower weight in size=6) and by age
      (young > old). WM policy precision (beta_wm) is also modulated by age and set size.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1); downscaled by set size and age
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - q_decay: RL value decay toward uniform per trial (0..1)
    - wm_interference: interference/forgetting strength per trial (0..1), scaled by set size and age
    - wm_beta_gain: gain that modulates WM precision with set size and age

    Age group usage:
    - Young (<=45) receives higher WM weight and lower WM interference; WM precision is increased.
    - Old (>45) receives lower WM weight and higher WM interference; WM precision is reduced.

    Set size usage:
    - WM weight scales with 3/nS (smaller in larger sets).
    - WM interference scales with nS/3 (more interference in larger sets).
    - WM precision scales with 3/nS (less precise in larger sets).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, wm_weight, softmax_beta, q_decay, wm_interference, wm_beta_gain = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute block-level age and set-size scalers
        weight_age_scale = 1.15 if age_group == 0 else 0.85
        interf_age_scale = 0.85 if age_group == 0 else 1.15
        size_weight_scale = 3.0 / float(nS)
        size_interf_scale = float(nS) / 3.0
        # WM precision base and modulation
        base_wm_beta = 25.0
        wm_beta_block = base_wm_beta * (1.0 + wm_beta_gain * (size_weight_scale * (1.15 if age_group == 0 else 0.85) - 1.0))
        wm_beta_block = max(1.0, wm_beta_block)

        # Effective mixture weight for this block
        wm_weight_eff_block = np.clip(wm_weight * weight_age_scale * size_weight_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (age- and set-size–modulated precision)
            softmax_beta_wm = wm_beta_block
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight_eff_block + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += alpha * delta
            # Decay all Q-values slightly toward uniform baseline
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA)

            # WM update: interference (state-general) + reward-driven storage (state-specific)
            # Apply interference as attraction toward the grand mean across states plus uniform
            interf = np.clip(wm_interference * interf_age_scale * size_interf_scale, 0.0, 1.0)
            grand_mean = np.mean(w, axis=0)
            w = (1.0 - interf) * w + interf * (0.5 * w_0 + 0.5 * grand_mean)

            # Reward-driven sharpening toward one-hot for the chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Use (1 - interference) as a storage gain so high interference reduces storage
                store_gain = np.clip(1.0 - interf, 0.0, 1.0)
                w[s, :] = (1.0 - store_gain) * w[s, :] + store_gain * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and Bayesian-like WM (Dirichlet counts); age/size modulate temperature.

    Description:
    - Policy is a mixture of model-free RL and a WM store.
    - RL uses an eligibility trace to propagate credit, with trace decay lambda.
    - WM maintains action-counts (Dirichlet-like) for each state; after a rewarded choice, the
      chosen action's count increases more than others; counts leak over time.
    - WM policy is the state-conditional probability of the chosen action from normalized counts
      (no softmax), capturing a categorical memory.
    - RL temperature increases with better conditions (young, small set size).

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature base; scaled internally by 10
    - lam_trace: eligibility trace decay (0..1)
    - wm_conc: base WM prior concentration per action (>=0), controls sharpness of WM
    - beta_age_size_gain: gain for age- and set-size modulation of RL temperature

    Age group usage:
    - Young (<=45): higher RL temperature; slower WM leak.
    - Old (>45): lower RL temperature; faster WM leak.

    Set size usage:
    - RL temperature increases for smaller sets; decreases for larger sets.
    - WM leak increases with larger set size; prior concentration has more impact for small sets.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, wm_weight, softmax_beta, lam_trace, wm_conc, beta_age_size_gain = model_parameters
    base_beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = np.ones((nS, nA)) * (wm_conc)  # Dirichlet-like counts start from prior conc
        w_0 = np.ones((nS, nA)) * (wm_conc)

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        # Age and set-size scaling
        size_scale = 3.0 / float(nS)
        age_scale_beta = 1.15 if age_group == 0 else 0.85
        # RL temperature modulation
        softmax_beta = base_beta * (1.0 + beta_age_size_gain * (age_scale_beta * size_scale - 1.0))
        softmax_beta = max(1e-3, softmax_beta)

        # WM leak (greater with size and for older age)
        wm_leak = (float(nS) / 3.0) * (1.0 if age_group == 0 else 1.2)
        wm_leak = np.clip(0.05 * wm_leak, 0.0, 0.9)  # bound leak per trial

        # Effective mixture weight modulated by set size and age
        wm_weight_eff_block = np.clip(wm_weight * size_scale * (1.1 if age_group == 0 else 0.9), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from normalized counts (categorical memory)
            counts_s = w[s, :]
            probs_s = counts_s / np.sum(counts_s)
            p_wm = float(probs_s[a])

            # Mixture
            p_total = p_wm * wm_weight_eff_block + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces (γ=1)
            delta = r - Q_s[a]
            # Increment trace for current state-action and decay others
            e *= lam_trace
            e[s, a] += 1.0
            q += alpha * delta * e

            # WM leak toward prior + reward-driven increment of chosen action
            # Leak all states
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            if r > 0.5:
                # Rewarded: strongly increment chosen action count in this state
                w[s, a] += 1.0 + size_scale  # slightly stronger for smaller set size
            else:
                # Unrewarded: weak symmetric increment to keep total mass from vanishing
                w[s, :] += 0.05

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with count-based exploration and uncertainty-gated WM; per-trial arbitration adapts by age/size.

    Description:
    - Policy is a mixture of model-free RL and a WM store.
    - RL uses a softmax over Q-values augmented with a count-based exploration bonus
      (encourages less-tried actions).
    - WM stores rewarded mappings as a sharp distribution with forgetting.
    - Arbitration weight is adapted each trial using a sigmoid of RL uncertainty; the slope of this
      sigmoid depends on age and set size (young/small-set rely more on WM when RL is uncertain).

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - eta_bonus: magnitude of RL exploration bonus (>=0)
    - wm_forget: WM forgetting per trial (0..1), scaled by age and set size
    - gate_slope: slope for uncertainty-to-WM gating; scaled by age and set size

    Age group usage:
    - Young (<=45): stronger gating slope (more WM when RL is uncertain), less WM forgetting.
    - Old (>45): weaker gating slope, more WM forgetting.

    Set size usage:
    - Gating slope increases for small set size; decreases for large set size.
    - WM forgetting increases for large set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, wm_weight, softmax_beta, eta_bonus, wm_forget, gate_slope = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Visit counts for exploration bonus
        N = np.zeros((nS, nA))

        # Age/size scaling
        size_scale = 3.0 / float(nS)
        forget_scale = (float(nS) / 3.0) * (0.85 if age_group == 0 else 1.15)
        gating_scale = size_scale * (1.2 if age_group == 0 else 0.9)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL with exploration bonus
            bonus_s = eta_bonus / np.sqrt(N[s, :] + 1.0)
            logits_rl = softmax_beta * (Q_s + bonus_s)
            p_rl = 1.0 / np.sum(np.exp(logits_rl - logits_rl[a]))

            # WM policy (deterministic-ish softmax)
            softmax_beta_wm = 50.0
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL uncertainty (entropy of RL policy) to gate WM
            # Derive RL policy distribution explicitly for entropy
            denom = np.sum(np.exp(logits_rl - np.max(logits_rl)))
            pi = np.exp(logits_rl - np.max(logits_rl)) / denom
            entropy = -np.sum(pi * np.log(np.clip(pi, 1e-12, 1.0)))
            # Normalize entropy to [0, log(nA)] then to [0,1]
            entropy_norm = entropy / np.log(nA)

            # Per-trial effective wm_weight via sigmoid of uncertainty
            slope = gate_slope * gating_scale
            sig = 1.0 / (1.0 + np.exp(-slope * (entropy_norm - 0.5)))
            wm_weight_trial = np.clip(wm_weight * sig, 0.0, 1.0)

            # Mixture
            p_total = p_wm * wm_weight_trial + (1.0 - wm_weight_trial) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta
            # Update counts after observing the choice
            N[s, a] += 1.0

            # WM forgetting and reward-based storage
            decay = np.clip(wm_forget * forget_scale, 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Store with strength inversely related to decay (more forgetting -> weaker store)
                store_gain = np.clip(1.0 - decay, 0.0, 1.0)
                w[s, :] = (1.0 - store_gain) * w[s, :] + store_gain * one_hot

        blocks_log_p += log_p

    return -blocks_log_p