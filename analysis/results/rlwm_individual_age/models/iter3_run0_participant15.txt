def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with arbitration by uncertainty, interference, and perseveration.

    Mechanisms:
    - RL: single learning rate with softmax choice; includes action stickiness (perseveration) bias.
    - WM: one-shot encoding on reward; subject to load-dependent interference (more interference at set size 6).
    - Arbitration: mixture weight between RL and WM is larger when WM is confident (low entropy) and
      smaller under high entropy, scaled by an arbitration slope. Older adults rely less on WM
      (age scaling), but here age_group=0 (young).
    - Age and set size:
        * WM influence reduced with larger set sizes via interference and via a direct load factor (3/nS).
        * Older adults have reduced WM reliance (multiplicative factor).

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - wm_weight0: baseline WM mixture weight in [0,1]
    - wm_interf: WM interference rate toward uniform per trial in [0,1]
    - stickiness: perseveration weight added to last chosen action in softmax (>=0)
    - arb_slope: arbitration slope mapping WM confidence (1 - entropy) to mixture weight (>=0)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, beta_rl, wm_weight0, wm_interf, stickiness, arb_slope = model_parameters

    beta_rl *= 10.0
    age_group = 0 if age[0] <= 45 else 1
    beta_wm = 40.0  # deterministic WM policy

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = None  # for stickiness

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Stickiness feature
            stick_vec = np.zeros(nA)
            if last_action is not None:
                stick_vec[last_action] = 1.0

            # RL policy
            Q_s = q[s, :] + stickiness * stick_vec
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration based on WM confidence (1 - normalized entropy)
            eps = 1e-12
            p_wm_vec = np.exp(beta_wm * (W_s - np.max(W_s)))
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            entropy = -np.sum(p_wm_vec * np.log(p_wm_vec + eps)) / np.log(nA)
            wm_conf = 1.0 - entropy  # in [0,1]

            load_scale = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
            age_scale = 1.0 - 0.3 * age_group  # older rely less on WM
            wm_weight = np.clip(wm_weight0 * age_scale * load_scale * (1.0 + arb_slope * (wm_conf - 0.5)), 0.0, 1.0)

            p_choice = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = max(p_choice, 1e-12)
            nll -= np.log(p_choice)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM interference (toward uniform); stronger at larger set size
            interf_rate = np.clip(wm_interf * (6.0 / float(nS)), 0.0, 1.0)
            w = (1.0 - interf_rate) * w + interf_rate * w0

            # WM one-shot write on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # write more strongly when load is low
                write_alpha = np.clip(0.7 * load_scale * (1.0 - 0.2 * age_group), 0.0, 1.0)
                w[s, :] = (1.0 - write_alpha) * w[s, :] + write_alpha * one_hot

            last_action = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-learning-rate RL + capacity-limited WM with lapse.

    Mechanisms:
    - RL: separate learning rates for positive and negative prediction errors; standard softmax.
    - WM: limited slots K that determine effective WM mixture weight as min(1, K/nS).
           WM stores the most recent rewarded action for each state and produces a near-deterministic policy.
    - Lapse: probability of choosing uniformly, increases with set size and age.
    - Age and set size:
        * WM capacity K reduced in older adults (age factor).
        * Lapse increases with larger set size and age.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE in [0,1]
    - lr_neg: RL learning rate for negative PE in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - K_young: baseline WM capacity (slots) for young adults (>=0)
    - beta_wm: WM inverse temperature (>=0)
    - lapse_base: baseline lapse probability in [0,1]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_rl, K_young, beta_wm, lapse_base = model_parameters

    beta_rl *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM table: initialize uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity (older adults reduced)
        K_eff = max(0.0, K_young * (1.0 - 0.4 * age_group))
        wm_weight = np.clip(min(1.0, K_eff / float(nS)), 0.0, 1.0)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Lapse increases with load and age
            lapse = lapse_base * (1.0 + 0.3 * (nS == 6) + 0.5 * age_group)
            lapse = np.clip(lapse, 0.0, 0.5)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            nll -= np.log(p_choice)

            # RL update with dual learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: store rewarded action as one-hot; otherwise slight decay toward uniform
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w0[s, :]

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + meta-learned WM gate by surprise.

    Mechanisms:
    - RL: TD(0) with eligibility trace on chosen action within state (simplified per-state trace),
           learning rate lr and trace decay lambda_et; softmax temperature scales with set size.
    - WM: table with its own softmax beta_wm; one-shot encoding on reward and slow decay otherwise.
    - Gate meta-learning: a latent gate g in [0,1] updated by surprise (absolute PE) with rate eta_gate.
      Higher surprise decreases reliance on WM (since WM likely incorrect); lower surprise increases it.
      Age reduces the asymptotic gate (older rely less on WM).
    - Set size: softmax beta_rl is reduced at larger set sizes by factor (3/nS)^temp_exp.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: base RL inverse temperature (scaled by 10 internally)
    - eta_gate: learning rate for WM gate update (>=0)
    - lambda_et: eligibility trace decay in [0,1]
    - beta_wm: WM inverse temperature (>=0)
    - temp_exp: exponent controlling set-size scaling of beta_rl (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, eta_gate, lambda_et, beta_wm, temp_exp = model_parameters

    beta_rl *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        # Initialize WM gate
        g = 0.5 * (1.0 - 0.3 * age_group)  # older start with lower WM reliance

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Set-size-scaled beta_rl
            beta_scale = (3.0 / float(nS)) ** max(0.0, temp_exp)
            beta_eff = beta_rl * beta_scale

            # Policies
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Effective WM weight from gate g (clipped)
            wm_weight = np.clip(g, 0.0, 1.0)
            p_choice = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = max(p_choice, 1e-12)
            nll -= np.log(p_choice)

            # RL TD update with eligibility trace for chosen action only (per-state)
            e *= lambda_et  # decay traces
            e[s, :] *= 0.0
            e[s, a] = 1.0
            pe = r - q[s, a]
            q += lr * pe * e

            # WM updates: decay toward uniform; encode on reward
            w = 0.98 * w + 0.02 * w0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot

            # Gate meta-learning by surprise |pe|; older asymptote lower
            surprise = abs(pe)
            target = (1.0 - surprise) * (1.0 - 0.3 * age_group)  # low surprise -> higher gate
            g = g + eta_gate * (target - g)
            g = np.clip(g, 0.0, 1.0)

    return nll