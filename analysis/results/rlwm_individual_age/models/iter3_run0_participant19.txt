def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + one-shot rewarded WM with interference and decay; age modulates WM mixture weight.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: for each state, a "one-shot" cache of the most recently rewarded action (deterministic when available).
      WM traces decay toward uniform each visit and are corrupted by set-size-dependent interference.
    - Arbitration: convex mixture of RL and WM policies; the WM weight is modulated by age.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_weight_base, wm_decay, interference, age_wm_mult]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      wm_weight_base: baseline mixture weight for WM in [0,1]
      wm_decay: per-visit decay of WM traces toward uniform in [0,1]
      interference: additional uniform interference added when set size is 6 (0=no interference, 1=full uniform)
      age_wm_mult: multiplicative modulation of WM weight by age group:
                   wm_weight_eff = wm_weight_base * (1 + age_wm_mult*(1 - 2*age_group));
                   young (age_group=0) -> boost; old (age_group=1) -> reduction

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_decay, interference, age_wm_mult = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM traces: start uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted WM weight
        wm_weight_age = wm_weight_base * (1.0 + age_wm_mult * (1 - 2 * age_group))
        wm_weight_age = float(np.clip(wm_weight_age, 0.0, 1.0))
        # Interference engages more at set size 6
        interf = float(interference) if nS == 6 else 0.0
        interf = float(np.clip(interf, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy: apply interference toward uniform before policy calculation
            W_s = w[s, :]
            W_eff = (1.0 - interf) * W_s + interf * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight_age * p_wm + (1.0 - wm_weight_age) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # One-shot rewarded write: if rewarded, set to one-hot for chosen action
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + slot-limited WM store with age-dependent capacity and size-dependent encoding.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: capacity-limited store that holds deterministic action memories for up to K states.
      If a state is in the store, WM proposes its last rewarded action; otherwise WM is uniform.
      Encoding of a rewarded state into WM occurs with a probability that decreases with set size.
      When WM is full, the least-recently-updated state is evicted (LRU-like).
    - Arbitration: fixed mixture weight for WM vs RL.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_weight_base, slots_young, slots_old, encode_gain]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      wm_weight_base: mixture weight for WM in [0,1]
      slots_young: WM capacity (number of storable states) for young participants (>=0)
      slots_old: WM capacity for older participants (>=0)
      encode_gain: scales encoding probability as p_enc = clip(encode_gain * (3/nS), 0, 1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight_base, slots_young, slots_old, encode_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    K = int(round(slots_young if age_group == 0 else slots_old))
    K = max(0, K)

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    rng = np.random  # assumed deterministic outside of fitting (probabilities used in likelihood only)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        in_store = np.zeros(nS, dtype=bool)
        last_time = -1 * np.ones(nS, dtype=int)  # LRU timestamp

        t_global = 0
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy: deterministic if in store, else uniform
            if in_store[s]:
                W_s = w[s, :]
            else:
                W_s = w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            wm_weight = float(np.clip(wm_weight_base, 0.0, 1.0))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Encoding probability declines with set size
            p_enc = np.clip(encode_gain * (3.0 / float(nS)), 0.0, 1.0)

            # If rewarded, encode into WM with probability p_enc
            if r > 0:
                # Prepare deterministic one-hot for WM
                w[s, :] = 0.0
                w[s, a] = 1.0

                # Probabilistic insertion decision using its probability in likelihood only:
                # For fitting, we treat expected insertion by weighing WM policy via p_enc implicitly
                # But to keep a fully specified generative story without sampling, we insert by expectation:
                # update store deterministically if p_enc >= 0.5, else do not insert.
                will_insert = p_enc >= 0.5

                if will_insert:
                    if not in_store[s]:
                        # Evict if full
                        if in_store.sum() >= K and K > 0:
                            # LRU eviction
                            idxs = np.where(in_store)[0]
                            lru_idx = idxs[np.argmin(last_time[idxs])]
                            in_store[lru_idx] = False
                        if K > 0:
                            in_store[s] = True
                    if in_store[s]:
                        last_time[s] = t_global

            # Update timestamps
            if in_store[s]:
                last_time[s] = t_global
            t_global += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM (Dirichlet counts) with set-size and age-dependent forgetting.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: for each state, maintains Dirichlet counts over actions (cached successes).
      The WM policy is the expected action preference under the Dirichlet (normalized counts),
      passed through a high-precision softmax. Counts forget toward zero each visit.
    - Forgetting increases with set size and is stronger for older adults.
    - Arbitration: mixture of WM and RL with a base WM weight; WM weight mildly scales up at small set size.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, alpha0, wm_forget_base, wm_weight_base, age_forget_mult]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      alpha0: symmetric Dirichlet prior pseudocount per action (>0)
      wm_forget_base: base forgetting rate per visit in [0,1]
      wm_weight_base: baseline mixture weight for WM in [0,1]
      age_forget_mult: multiplicative boost to forgetting for older adults (>=0);
                       forget_eff = wm_forget_base * (nS/3) * (1 + age_forget_mult*age_group)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, alpha0, wm_forget_base, wm_weight_base, age_forget_mult = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM Dirichlet counts
        counts = np.zeros((nS, nA))
        prior = alpha0 * np.ones((nS, nA))

        # WM weight: slightly larger for smaller set size (capped at 1)
        wm_weight = float(np.clip(wm_weight_base * (6.0 / float(nS)), 0.0, 1.0))

        # Forgetting factor depends on set size and age
        forget_eff = wm_forget_base * (float(nS) / 3.0) * (1.0 + age_forget_mult * age_group)
        forget_eff = float(np.clip(forget_eff, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy from Dirichlet expectation
            post = counts[s, :] + prior[s, :]
            W_s = post / np.sum(post)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting toward zero counts (prior remains as baseline)
            counts[s, :] *= (1.0 - forget_eff)

            # Rewarded update: add one success to chosen action
            if r > 0:
                counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p