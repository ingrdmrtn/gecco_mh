def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility-trace) + Capacity-limited WM mixture.

    Mechanism:
    - Model-free RL with an eligibility trace allows credit assignment to persist for recently active state-action pairs.
    - A capacity-limited working memory (WM) policy is mixed with RL. WM has high precision but its weight scales down
      when set size exceeds WM capacity. Younger adults are assumed to have slightly higher WM precision.

    Parameters (6):
    - lr: RL learning rate (0-1).
    - beta_rl: RL inverse temperature; internally scaled by x10 for a wide range.
    - lambda_elig: Eligibility-trace decay (0-1); higher means longer memory of recent choices.
    - wm_capacity_raw: Real-valued; mapped to an effective capacity C in [1,6]; slightly reduced by older age.
    - wm_decay: WM decay/leak toward uniform (0-1) each visit to a state.
    - wm_mix_bias: Base logit controlling the overall WM mixture weight (real-valued; sigmoid to [0,1]).

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes for each trial (3 or 6)
    - age: array with a single repeated value (participant's age)
    - model_parameters: list with the 6 parameters above

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta_rl, lambda_elig, wm_capacity_raw, wm_decay, wm_mix_bias = model_parameters
    beta_rl *= 10.0

    # Age group: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1

    # WM precision slightly reduced for older adults
    beta_wm = 40.0 * (1.0 - 0.2 * age_group)

    # Map capacity raw to [1,6], with a small age penalty
    cap_base = 1.0 + 5.0 / (1.0 + np.exp(-(wm_capacity_raw - 0.5 * age_group)))  # in [1,6]

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility trace over state-action
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (softmax trick)
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy of chosen action
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Capacity-limited WM mixture weight
            C_eff = cap_base  # effective capacity
            cap_scale = min(1.0, C_eff / float(nS))  # down-weight WM when set size > capacity
            wm_base = 1.0 / (1.0 + np.exp(-wm_mix_bias))
            wm_weight = np.clip(wm_base * cap_scale, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            delta = r - Q_s[a]
            # Decay traces and set current
            e *= lambda_elig
            e[s, :] *= 0.0
            e[s, a] = 1.0
            # Update all Q by eligibility
            q += lr * delta * e

            # WM decay toward baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, strengthen memory of chosen action in WM
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
                # Normalize to avoid drift
                w[s, :] = w[s, :] / max(eps, np.sum(w[s, :]))

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL + WM recall with epsilon-greedy exploration.

    Mechanism:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM contributes via a recall probability; if recalled, the WM softmax is used, otherwise RL softmax.
    - An additional epsilon-greedy exploration mixes the chosen mixture policy with uniform choice.
    - Larger set sizes and older age increase exploration (epsilon). Younger age yields more precise WM.

    Parameters (6):
    - lr_pos: RL learning rate for positive PE (0-1).
    - lr_neg: RL learning rate for negative PE (0-1).
    - beta_rl: RL inverse temperature; internally scaled by x10.
    - eps_bias: Base logit of epsilon (real; sigmoid to [0,1]).
    - eps_setsize_slope: Slope modifying epsilon as set size increases (real; >0 means more exploration in larger sets).
    - recall_bias: Base logit for WM recall probability (real; sigmoid to [0,1]).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as described above.
    - model_parameters: list of the 6 parameters above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, beta_rl, eps_bias, eps_setsize_slope, recall_bias = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # WM precision slightly noisier with age
    beta_wm = 50.0 * (1.0 - 0.3 * age_group)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability for chosen action
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM softmax probability for chosen action
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # WM recall probability (less reliable in larger sets implicitly via exploration, kept as base here)
            p_recall = 1.0 / (1.0 + np.exp(-recall_bias))

            # Epsilon increases with set size and age
            eps_logit = eps_bias + eps_setsize_slope * (nS - 3) + 0.5 * age_group
            epsilon = 1.0 / (1.0 + np.exp(-eps_logit))
            epsilon = np.clip(epsilon, 0.0, 1.0)

            # Mixture between recalled WM and RL
            p_mix = p_recall * p_wm + (1.0 - p_recall) * p_rl

            # Final epsilon-greedy mixture with uniform
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM update: decay toward uniform; strengthen only on reward
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot
                w[s, :] = w[s, :] / max(eps, np.sum(w[s, :]))

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and set-size/age modulation.

    Mechanism:
    - RL and WM each define a softmax policy.
    - Arbitration weight for WM depends on relative uncertainty: when WM is more certain (lower entropy) than RL,
      WM receives higher weight. The arbitration is implemented via a logistic function of entropy difference,
      plus biases for set size and age.
    - WM is a leaky memory of rewarded state-action associations.

    Parameters (6):
    - lr: RL learning rate (0-1).
    - beta_rl: RL inverse temperature; internally scaled by x10.
    - wm_leak: Leak toward current evidence on reward and toward uniform on no-reward (0-1).
    - arb_bias: Base arbitration bias toward WM (logit).
    - arb_age_slope: Added bias term for young vs old (multiplied by 1-age_group; positive favors WM in young).
    - arb_set_slope: Bias slope for set size (multiplied by 3 - nS; positive favors WM in small sets).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: as described above.
    - model_parameters: list of the 6 parameters above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta_rl, wm_leak, arb_bias, arb_age_slope, arb_set_slope = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # WM precision with mild age penalty
    beta_wm = 40.0 * (1.0 - 0.2 * age_group)

    eps = 1e-12
    blocks_log_p = 0.0

    def softmax_probs(values, beta):
        v = values - np.max(values)
        exps = np.exp(beta * v)
        probs = exps / max(eps, np.sum(exps))
        return probs

    def entropy(probs):
        p = np.clip(probs, eps, 1.0)
        return -np.sum(p * np.log(p))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl_full = softmax_probs(Q_s, beta_rl)
            p_wm_full = softmax_probs(W_s, beta_wm)

            # Probability of the chosen action under each policy
            p_rl = np.clip(p_rl_full[a], eps, 1.0)
            p_wm = np.clip(p_wm_full[a], eps, 1.0)

            # Entropy-based arbitration: if WM is more certain (lower entropy), increase WM weight
            H_rl = entropy(p_rl_full)
            H_wm = entropy(p_wm_full)
            entropy_diff = H_rl - H_wm  # positive when WM is more certain

            # Arbitration logit combines entropy, set size, and age effects
            arb_logit = (
                arb_bias
                + 5.0 * entropy_diff
                + arb_set_slope * (3 - nS)       # favors WM in smaller set sizes if positive
                + arb_age_slope * (1 - age_group)  # favors WM more for young if positive
            )
            wm_weight = 1.0 / (1.0 + np.exp(-arb_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture probability of chosen action
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: leaky integration; reward strengthens chosen action, no-reward leaks to uniform
            if r > 0:
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * one_hot
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            # Normalize
            w[s, :] = w[s, :] / max(eps, np.sum(w[s, :]))

        blocks_log_p += log_p

    return -blocks_log_p