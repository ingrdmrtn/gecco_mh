def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-gated WM reliance, probabilistic WM storage, and decay.

    Mechanisms:
    - RL: tabular Q-learning with a single learning rate (alpha).
    - WM: when rewarded, store the chosen action with probability (wm_store); WM traces decay toward uniform at rate (wm_decay).
    - Policy mixture: WM vs RL weighted by an entropy-gated reliance. Higher RL entropy increases WM reliance.
    - Set-size effect: WM reliance scales with 3/nS (lower WM impact under larger set size).
    - Age effect: older adults store less in WM and forget faster (reduced wm_store, increased wm_decay).
    - Lapse: mixture with uniform choice.

    Parameters (list of 6):
    - model_parameters[0] = alpha_raw: RL learning rate (logistic mapped to [0,1]).
    - model_parameters[1] = beta_raw: RL inverse temperature (abs(beta_raw)*10 used).
    - model_parameters[2] = wm_store_raw: probability to store in WM upon reward (logistic).
    - model_parameters[3] = wm_decay_raw: per-visit WM decay rate toward uniform (logistic).
    - model_parameters[4] = meta_gain_raw: nonnegative gain on entropy-gating of WM reliance (abs).
    - model_parameters[5] = lapse_raw: lapse rate mixing with uniform (logistic).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    eps = 1e-12

    alpha = 1.0 / (1.0 + np.exp(-model_parameters[0]))
    softmax_beta = abs(model_parameters[1]) * 10.0
    wm_store = 1.0 / (1.0 + np.exp(-model_parameters[2]))
    wm_decay = 1.0 / (1.0 + np.exp(-model_parameters[3]))
    meta_gain = abs(model_parameters[4])
    lapse = 1.0 / (1.0 + np.exp(-model_parameters[5]))

    # Age group: 0 for young (<=45), 1 for old (>45)
    age_group = 0 if age[0] <= 45 else 1
    # Older adults: reduced WM storage and increased decay
    if age_group == 1:
        wm_store *= 0.8
        wm_decay = 1.0 - 0.8 * (1.0 - wm_decay)  # push decay upward

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy probabilities (compute full softmax for entropy)
            Q_s = q[s, :]
            Q_center = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * Q_center)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            # RL entropy normalized by log(nA) to [0,1]
            H = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0))) / np.log(nA)

            # WM policy from current WM table
            W_s = w[s, :]
            W_center = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * W_center)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = pi_wm[a]

            # Set-size scaling for WM reliance
            ss_scale = 3.0 / float(nS)

            # Entropy-gated WM weight: higher RL uncertainty -> more WM
            wm_weight = np.clip(ss_scale * wm_store * (1.0 + meta_gain * H), 0.0, 1.0)

            # Total policy with lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM update: store on reward with probability wm_store, else decay toward uniform
            if r == 1:
                # Stochastic storage approximated deterministically via weighting
                # Move WM toward one-hot for chosen action proportional to wm_store
                target = ((1.0 - 1.0) / (nA - 1)) * np.ones(nA)  # zeros
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_store) * w[s, :] + wm_store * target
            # Per-visit decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM + WSLS heuristic with interference-limited WM retrieval.

    Mechanisms:
    - RL: tabular Q-learning with single learning rate.
    - WM: one-shot storage of rewarded action; retrieval reliability decreases with set-size via an interference factor exp(-gamma*(nS-1)).
    - Heuristic: Win-Stay Lose-Shift (WSLS) policy component based on last outcome in the same state.
    - Policy: convex combination of RL and WM, further mixed with WSLS.
    - Set-size effect: WM reliance reduced through the interference factor.
    - Age effect: older adults have stronger interference (larger gamma) and rely more on WSLS.

    Parameters (list of 6):
    - model_parameters[0] = alpha_raw: RL learning rate (logistic).
    - model_parameters[1] = beta_raw: RL inverse temperature (abs*10).
    - model_parameters[2] = wm_w0_raw: baseline WM mixture weight (logistic).
    - model_parameters[3] = gamma_interf_raw: WM interference rate >= 0 (abs).
    - model_parameters[4] = wsls_raw: WSLS mixture weight (logistic).
    - model_parameters[5] = lapse_raw: lapse rate (logistic).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    eps = 1e-12

    alpha = 1.0 / (1.0 + np.exp(-model_parameters[0]))
    softmax_beta = abs(model_parameters[1]) * 10.0
    wm_w0 = 1.0 / (1.0 + np.exp(-model_parameters[2]))
    gamma = abs(model_parameters[3])
    wsls_w0 = 1.0 / (1.0 + np.exp(-model_parameters[4]))
    lapse = 1.0 / (1.0 + np.exp(-model_parameters[5]))

    age_group = 0 if age[0] <= 45 else 1
    if age_group == 1:
        # Stronger interference and slightly higher WSLS reliance with age
        gamma *= 1.25
        wsls_w0 = np.clip(wsls_w0 * 1.2, 0.0, 1.0)

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Track previous action and reward per state for WSLS
        prev_a = -1 * np.ones(nS, dtype=int)
        prev_r = np.zeros(nS)

        # Compute WM retrieval reliability due to interference
        recall = np.exp(-gamma * max(0, nS - 1))

        # Block-level WM weight with set-size interference
        wm_weight_block = np.clip(wm_w0 * recall, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL chosen-action probability
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM chosen-action probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # RL+WM mixture
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl

            # WSLS policy for this state if we have a previous action
            if prev_a[s] >= 0:
                H = np.ones(nA) * (1.0 / (nA - 1))  # default for lose-shift
                if prev_r[s] > 0:
                    H = np.zeros(nA)
                    H[prev_a[s]] = 1.0  # win-stay
                else:
                    H[prev_a[s]] = 0.0  # lose-shift: avoid previous action
                p_wsls = H[a]
                wsls_weight = wsls_w0
            else:
                p_wsls = 1.0 / nA
                wsls_weight = 0.0

            # Combine with WSLS and lapse
            p_total = (1.0 - wsls_weight) * p_mix + wsls_weight * p_wsls
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM update: store one-hot on reward, reset to uniform on non-reward
            if r == 1:
                w[s, :] = np.zeros(nA)
                w[s, a] = 1.0
            else:
                w[s, :] = w0[s, :].copy()

            # Update WSLS memory
            prev_a[s] = a
            prev_r[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Error-gated WM with decay and state-specific WM strength.

    Mechanisms:
    - RL: tabular Q-learning with single learning rate.
    - WM: updated only when |prediction error| exceeds a threshold tau; WM strength m[s] decays otherwise.
    - Policy: WM vs RL mixture where WM weight equals wm_w0 * m[s], scaled by set size (3/nS).
    - Set-size effect: WM baseline reliance scaled by 3/nS.
    - Age effect: older adults have higher error threshold (less WM updating) and faster WM decay.
    - Lapse: mixture with uniform.

    Parameters (list of 6):
    - model_parameters[0] = alpha_raw: RL learning rate (logistic).
    - model_parameters[1] = beta_raw: RL inverse temperature (abs*10).
    - model_parameters[2] = wm_w0_raw: baseline WM reliance (logistic).
    - model_parameters[3] = tau_raw: PE threshold for WM gating (abs).
    - model_parameters[4] = wm_decay_raw: decay rate of WM strength m[s] (logistic).
    - model_parameters[5] = lapse_raw: lapse rate (logistic).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    eps = 1e-12

    alpha = 1.0 / (1.0 + np.exp(-model_parameters[0]))
    softmax_beta = abs(model_parameters[1]) * 10.0
    wm_w0 = 1.0 / (1.0 + np.exp(-model_parameters[2]))
    tau = abs(model_parameters[3])
    wm_decay = 1.0 / (1.0 + np.exp(-model_parameters[4]))
    lapse = 1.0 / (1.0 + np.exp(-model_parameters[5]))

    age_group = 0 if age[0] <= 45 else 1
    if age_group == 1:
        tau *= 1.25           # older: need larger PE to gate WM
        wm_decay = 1.0 - 0.7 * (1.0 - wm_decay)  # increase decay toward 1

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific WM strength m[s] in [0,1]
        m = np.zeros(nS)

        # Set-size scaling
        ss_scale = 3.0 / float(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL chosen-action probability via denominator trick
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM chosen-action probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM weight uses current state-specific strength m[s]
            wm_weight = np.clip(ss_scale * wm_w0 * m[s], 0.0, 1.0)

            # Mixture with lapse
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM gating by PE magnitude
            if abs(pe) >= tau:
                # Update WM to a one-hot at the chosen action; set strength to 1
                w[s, :] = np.zeros(nA)
                w[s, a] = 1.0
                m[s] = 1.0
            else:
                # No update; decay WM strength and values toward uniform
                m[s] = (1.0 - wm_decay) * m[s]
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p