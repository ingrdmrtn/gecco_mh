def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with age- and set-size–dependent WM capacity, plus action lapses.

    Idea:
    - RL learns action values per state via a single learning rate and softmax choice.
    - WM stores the last rewarded action per state in a near-deterministic code.
    - The mixture weight of WM depends on an effective capacity K_eff divided by the set size nS.
      Younger participants get higher K_eff (age benefit).
    - WM policy has additional retrieval noise (wm_noise). A small lapse allows random choices.

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..set_size-1 within block).
    actions : array-like, int
        Observed action (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size per trial (3 or 6).
    age : array-like, int
        Participant age repeated per trial; age_group=0 if <=45, else 1.
    model_parameters : list or array
        [lr, softmax_beta, K_base, K_age_delta, lapse, wm_noise]
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature; scaled internally by x10.
        - K_base: baseline WM capacity (in “slots”).
        - K_age_delta: additional capacity given to younger participants (age_group==0).
        - lapse: lapse probability mixed with uniform choice (0..0.2 typical).
        - wm_noise: WM retrieval noise (probability mixed with uniform within WM policy).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, K_base, K_age_delta, lapse, wm_noise = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity and mixture weight: higher for younger, drops with larger set size
        K_eff = max(0.0, K_base + K_age_delta * (1 - age_group))  # benefit for young
        wm_w_eff = np.clip(K_eff / max(1, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy with retrieval noise
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(denom_wm, eps)
            p_wm = (1.0 - wm_noise) * p_wm_det + wm_noise * (1.0 / nA)

            # Mixture and lapse
            p_mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: on reward, store deterministically; on no-reward, slight decay to prior
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and WM with age- and set-size–dependent decay.

    Idea:
    - RL uses an eligibility trace to propagate credit within a state: e decays by lambda each trial and is set to 1 for the chosen (s,a).
    - WM stores last rewarded action; WM decays toward uniform each trial with a decay rate that increases with set size and with age.
    - Mixture weight of WM is fixed but its effectiveness is reduced by decay under load/age.
    - Standard softmax for RL; near-deterministic softmax for WM.

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..set_size-1 within block).
    actions : array-like, int
        Observed action (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size per trial (3 or 6).
    age : array-like, int
        Participant age repeated per trial; age_group=0 if <=45, else 1.
    model_parameters : list or array
        [lr, softmax_beta, wm_weight, lambda_trace, wm_decay_base, wm_decay_age_delta]
        - lr: RL learning rate.
        - softmax_beta: RL inverse temperature; scaled internally by x10.
        - wm_weight: baseline WM mixture weight (0..1).
        - lambda_trace: eligibility trace decay (0..1); higher means longer trace.
        - wm_decay_base: base WM decay per trial (0..1).
        - wm_decay_age_delta: additional WM decay added if age_group==1.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, lambda_trace, wm_decay_base, wm_decay_age_delta = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        # WM decay increases with set size and age
        wm_decay_eff = np.clip(wm_decay_base + (nS - 3) * (wm_decay_base / 3.0) + wm_decay_age_delta * age_group, 0.0, 1.0)
        wm_w_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            e *= lambda_trace
            e[s, a] = 1.0
            pe = r - q[s, a]
            q += lr * pe * e

            # WM update: reward strengthens one-hot, otherwise decay toward uniform
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # Global WM decay each trial captures interference under higher load and in older adults
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learned learning rate from surprise and age-modulated directed exploration,
    mixed with simple WM.

    Idea:
    - A running volatility/surprise signal v_t (EWMA of |PE|) scales the effective learning rate.
      Larger set sizes increase sensitivity to volatility.
    - Directed exploration bonus favors actions with lower visit counts within a state; younger
      participants explore more. Set size reduces the exploration bonus.
    - WM stores rewarded actions deterministically; mixture weight is fixed.

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..set_size-1 within block).
    actions : array-like, int
        Observed action (0..2).
    rewards : array-like, float
        Reward per trial (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size per trial (3 or 6).
    age : array-like, int
        Participant age repeated per trial; age_group=0 if <=45, else 1.
    model_parameters : list or array
        [lr0, meta_rate, softmax_beta, wm_weight, explore_bonus, age_explore_delta]
        - lr0: baseline RL learning rate.
        - meta_rate: how strongly surprise increases learning rate (0..1).
        - softmax_beta: RL inverse temperature; scaled internally by x10.
        - wm_weight: WM mixture weight (0..1).
        - explore_bonus: base directed exploration bonus magnitude.
        - age_explore_delta: additional exploration bonus for younger participants.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr0, meta_rate, softmax_beta, wm_weight, explore_bonus, age_explore_delta = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for directed exploration
        N = np.zeros((nS, nA))
        # Surprise/volatility tracker per state
        v = np.zeros(nS)

        wm_w_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Directed exploration bonus (state-specific), reduced under higher set size, boosted for youth
            age_bonus = age_explore_delta * (1 - age_group)  # only young get extra
            explore_eff = (explore_bonus + age_bonus) * (3.0 / max(1, nS))
            bonus_sa = explore_eff / np.sqrt(N[s, :] + 1.0)

            # RL policy with bonus added to Q
            Q_s = q[s, :] + bonus_sa
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # Update counts
            N[s, a] += 1.0

            # Meta-learned learning rate from surprise
            pe = r - q[s, a]
            v[s] = (1.0 - meta_rate) * v[s] + meta_rate * np.abs(pe)
            lr_eff = np.clip(lr0 + v[s] * meta_rate * (nS / 6.0), 0.0, 1.0)

            # RL update
            q[s, a] += lr_eff * pe

            # WM update: store rewarded, else gentle decay
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)