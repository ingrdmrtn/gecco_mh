def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay + Item-based Working Memory (WM) mixture gated by set size and age.

    Idea
    - RL: tabular Q-learning with a forgetting/decay-to-uniform term applied to the active state.
    - WM: one-shot storage of rewarded action; when not rewarded, WM decays toward uniform.
    - Arbitration: a fixed-but-contextual WM mixture weight computed from set size (higher for 3 than 6)
      and age group (reduced in older), passed through a sigmoid. No trialwise PE modulation.

    Parameters
    ----------
    states : array-like of int
        State index (0..nS-1) per trial.
    actions : array-like of int
        Chosen action (0..2) per trial.
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Block set size (3 or 6) per trial (constant within block).
    age : array-like or scalar
        Participant age; used to assign age group (0 young, 1 old).
    model_parameters : list or array-like
        [lr, beta, wm_base, wm_cap, age_wm_pen, decay]
        - lr: RL learning rate (0..1 via logistic).
        - beta: RL inverse temperature, scaled by *10 internally.
        - wm_base: baseline WM mixture weight before context (logit space).
        - wm_cap: scaling for set-size effect on WM weight (positive means higher WM at smaller sets).
        - age_wm_pen: reduction of WM weight for older group (applied only if age_group == 1).
        - decay: forgetting/decay parameter (0..1 via logistic) used for both RL and WM decay to uniform.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, wm_base, wm_cap, age_wm_pen, decay = model_parameters

    lr = 1.0 / (1.0 + np.exp(-lr))
    beta *= 10.0
    decay = 1.0 / (1.0 + np.exp(-decay))
    # age group
    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Contextual WM mixture weight (constant within block)
        # Higher for small set size (3) than large (6); reduced in older group.
        size_term = wm_cap * (6 - nS) / 3.0  # = +wm_cap for nS=3; = 0 for nS=6
        age_term = -age_wm_pen * age_group
        wm_weight_block = 1.0 / (1.0 + np.exp(-(wm_base + size_term + age_term)))

        softmax_beta_wm = 50.0  # deterministic WM
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform on the visited state
            delta = r - q[s, a]
            q[s, a] += lr * delta
            q[s, :] = (1.0 - decay) * q[s, :] + decay * w_0[s, :]

            # WM update: reward-locked one-shot; otherwise decay toward uniform
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-gated arbitration and age-sensitive WM persistence.

    Idea
    - RL: tabular Q-learning (no forgetting).
    - WM: one-shot when rewarded; decays when not rewarded with its own persistence parameter.
    - Arbitration: trial-wise WM weight is a sigmoid of:
        wm_bias + capacity_effect - age_penalty*age_group - ent_slope*H(Q_s)
      where H(Q_s) is the entropy of the RL softmax over actions at the current state.
      Thus, when RL is uncertain (high entropy), the system leans less on WM (if ent_slope > 0).
      Capacity_effect = log(3/nS) favors WM in small set sizes without adding extra parameters.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [lr, beta, wm_bias, ent_slope, age_penalty, wm_persist]
        - lr: RL learning rate (0..1 via logistic).
        - beta: RL inverse temperature scaled by *10 internally.
        - wm_bias: baseline arbitration bias toward WM (logit space).
        - ent_slope: weight on RL entropy in arbitration (positive reduces WM when RL uncertain).
        - age_penalty: reduction of WM reliance for older group.
        - wm_persist: WM persistence (0..1 via logistic) controlling decay toward uniform after errors.

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, beta, wm_bias, ent_slope, age_penalty, wm_persist = model_parameters

    lr = 1.0 / (1.0 + np.exp(-lr))
    beta *= 10.0
    wm_persist = 1.0 / (1.0 + np.exp(-wm_persist))
    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        softmax_beta_wm = 50.0
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # RL action distribution for entropy
            logits = beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            rl_probs = exp_logits / np.sum(exp_logits)
            rl_probs = np.clip(rl_probs, 1e-12, 1.0)
            H = -np.sum(rl_probs * np.log(rl_probs))  # entropy in nats

            # Capacity effect: favors WM in small set sizes
            capacity_effect = np.log(3.0 / nS)

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise arbitration weight
            logit_w = wm_bias + capacity_effect - ent_slope * H - age_penalty * age_group
            wm_weight_t = 1.0 / (1.0 + np.exp(-logit_w))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: reward one-shot; else decay with wm_persist
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                # decay toward uniform: higher wm_persist -> slower decay (so use (1 - persist) toward uniform)
                decay = 1.0 - wm_persist
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size- and age-modulated inverse temperature + WM capacity-by-exposure arbitration.

    Idea
    - RL: standard Q-learning; exploration/exploitation governed by an effective beta that decreases
      with larger set size and in the older group.
    - WM: one-shot storage on reward; decays when not rewarded.
    - Arbitration: WM weight depends on how many unique states have been encountered so far in the block
      relative to an implicit capacity slope. As more unique items accumulate beyond capacity, WM weight drops.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [lr, beta_base, beta_setsize_drop, age_beta_pen, wm_base, cap_slope]
        - lr: RL learning rate (0..1 via logistic).
        - beta_base: base inverse temperature (scaled by *10 internally).
        - beta_setsize_drop: scales reduction in beta when set size increases from 3 to 6.
        - age_beta_pen: multiplicative drop in beta for older group (applied if age_group==1).
        - wm_base: baseline WM weight (logit space).
        - cap_slope: sensitivity of WM weight to number of unique states seen (negative slope reduces WM as exposure grows).

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, beta_base, beta_setsize_drop, age_beta_pen, wm_base, cap_slope = model_parameters

    lr = 1.0 / (1.0 + np.exp(-lr))
    beta_base *= 10.0
    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective beta: reduced by set size and by age group
        setsize_factor = np.exp(-beta_setsize_drop * (nS - 3) / 3.0)  # =1 for 3, e^{-beta_setsize_drop} for 6
        age_factor = np.exp(-age_beta_pen * age_group)
        beta_eff = max(1e-6, beta_base * setsize_factor * age_factor)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        seen_states = set()

        softmax_beta_wm = 50.0
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            seen_states.add(int(s))
            n_seen = len(seen_states)

            # WM weight decreases as more unique states are seen (capacity pressure),
            # controlled by cap_slope; baseline given by wm_base (logit).
            logit_wm = wm_base + cap_slope * (3 - n_seen)  # positive if n_seen < 3 for positive cap_slope
            wm_weight_t = 1.0 / (1.0 + np.exp(-logit_wm))

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: one-shot on reward, mild decay on non-reward
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                decay = 0.15
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p