def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with set-size-dependent availability and age-penalized capacity.

    Key ideas:
    - RL system (Rescorla-Wagner) with softmax action selection.
    - WM system stores most recent rewarded action per state as a one-hot trace and decays toward uniform.
    - WM policy is near-deterministic, but its availability decreases with set size due to a limited WM capacity.
    - Age group reduces effective WM capacity (older group suffers a penalty; here age_group=0 so no penalty).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base reliance on WM policy in the mixture (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - K_base: baseline WM capacity (in number of items/slots)
    - age_capacity_penalty: capacity reduction applied if age_group==1 (>=45)
    - wm_decay: per-trial decay of WM traces toward uniform within a block (0..1)

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen action indices per trial
    - rewards: array of rewards (0/1) per trial
    - blocks: array of block indices per trial
    - set_sizes: array with the block's set size repeated per trial
    - age: array with a single value repeated; age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K_base, age_capacity_penalty, wm_decay = model_parameters

    softmax_beta *= 10.0  # RL beta scaling
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM availability from capacity limits (probability that a state is in WM)
        K_eff = max(0.0, K_base - age_capacity_penalty * age_group)
        p_available = np.clip(K_eff / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM row
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Combine RL and WM with base reliance and availability
            wm_mix = np.clip(wm_weight, 0.0, 1.0) * p_available
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM overwrite on reward: store the rewarded association as one-hot
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with per-state stickiness and load-driven RL forgetting + WM with age-modulated sharpness.

    Key ideas:
    - RL system with a per-state action stickiness bias (perseveration) and set-size dependent Q forgetting.
    - WM system learned by incremental updates; its softmax sharpness (beta_wm) is reduced for older adults.
    - Mixture policy between RL and WM with fixed wm_weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: reliance on WM policy in mixture (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - stickiness: bias added to the last chosen action in the same state (in Q space)
    - q_forget_base: base RL forgetting toward uniform that scales with set size (0..1)
    - wm_beta_age_effect: fractional reduction of WM sharpness if age_group==1 (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
      age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, stickiness, q_forget_base, wm_beta_age_effect = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    base_wm_beta = 50.0
    # Older adults: reduced WM sharpness
    softmax_beta_wm = base_wm_beta * (1.0 - wm_beta_age_effect * age_group)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise last action for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # RL forgetting increases with set size (0 at nS=3; q_forget_base at nS=6)
        load = max(nS - 3, 0) / 3.0
        q_forget = np.clip(q_forget_base * load, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy via near-deterministic softmax over WM table (age-modulated beta)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            mix = np.clip(wm_weight, 0.0, 1.0)
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA)

            # WM incremental update: move toward one-hot of chosen action proportionally to reward
            # Reward strengthens the chosen action trace; no-reward mildly weakens it toward uniform
            alpha_w = 0.8  # strong WM update speed (fixed internal constant)
            if r > 0.5:
                target = w_0[s, :].copy()
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_w) * w[s, :] + alpha_w * target
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM arbitration via capacity-limited WM availability and load-dependent WM lapse.

    Key ideas:
    - RL system with standard softmax.
    - WM stores rewarded associations as one-hot but suffers lapses that increase with set size.
    - WM availability is the probability that an item is in WM: min(1, K_eff / set_size), where K_eff depends on age group.
    - Final WM policy is mixed with a uniform lapse; then mixed with RL via wm_weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base reliance on WM policy (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_capacity_y: effective WM capacity (slots) for young group
    - wm_capacity_o: effective WM capacity (slots) for old group
    - wm_lapse_base: base WM lapse rate scaled by load (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age; age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, wm_capacity_y, wm_capacity_o, wm_lapse_base = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-specific WM capacity
        K_eff = wm_capacity_y if age_group == 0 else wm_capacity_o
        p_available = np.clip(K_eff / float(nS), 0.0, 1.0)

        # WM lapse increases with load
        load = max(nS - 3, 0) / 3.0
        wm_lapse = np.clip(wm_lapse_base * load, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with lapse: mix sharp WM with uniform according to wm_lapse
            W_s = w[s, :]
            p_wm_sharp = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_uniform = 1.0 / nA
            p_wm = (1.0 - wm_lapse) * p_wm_sharp + wm_lapse * p_uniform

            # Arbitration: WM availability scales the wm_weight
            mix = np.clip(wm_weight, 0.0, 1.0) * p_available
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: overwrite on reward; mild regression to uniform otherwise
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
            else:
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p