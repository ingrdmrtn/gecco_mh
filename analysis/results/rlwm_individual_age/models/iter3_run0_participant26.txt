Here are three distinct RL+WM cognitive models tailored to the rlwm task. Each model is a standalone function that returns the negative log-likelihood of the observed choices. They all use age group and set size meaningfully, and they stay within the 6-parameter limit while exploring mechanisms not covered by the previously tried combinations.

Note: Assume numpy as np is already imported in the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with surprise-gated WM encoding and age-modulated RL learning rate.

    Mechanism
    - RL system: standard delta-rule Q-learning with softmax policy.
      Learning rate is reduced for older adults.
    - WM system: probabilistic (soft) one-shot storage when reward is received,
      with encoding strength gated by surprise (|PE|) via a logistic transform.
      WM traces decay toward uniform on each visit.
    - Arbitration: fixed mixture weight scaled by set size (3/nS). WM weight does not directly depend on age,
      but RL learning rate is reduced for older adults, shifting reliance toward WM implicitly.

    Parameters
    - model_parameters: [lr_base, wm_weight_base, softmax_beta, wm_decay, surprise_slope, age_lr_penalty]
        - lr_base: baseline RL learning rate (0..1)
        - wm_weight_base: baseline WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_decay: decay of WM toward uniform per state visit (0..1)
        - surprise_slope: slope of logistic gating function over |PE| (>=0)
        - age_lr_penalty: fractional reduction of RL learning rate in older adults (0..1)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_base, wm_weight_base, softmax_beta, wm_decay, surprise_slope, age_lr_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    lr_eff_scale = 1.0 - age_lr_penalty * age_group
    lr_eff_scale = max(0.0, lr_eff_scale)

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_block = wm_weight_base * (3.0 / nS)
        wm_weight_block = min(1.0, max(0.0, wm_weight_block))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            q[s, a] += (lr_base * lr_eff_scale) * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Surprise-gated WM encoding on rewarded trials
            if r > 0.5:
                # Logistic gate in [0,1] increasing with |PE|
                gate = 1.0 / (1.0 + np.exp(-surprise_slope * np.abs(pe)))
                target = np.zeros(nA)
                target[a] = 1.0
                # Move WM distribution toward the one-hot target by gate amount
                w[s, :] = (1.0 - gate) * w[s, :] + gate * target
                # Normalize just in case of numerical drift
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM + global habit learner (state-independent action values).
    
    Mechanism
    - RL system: standard delta-rule per state with softmax.
    - Habit system: state-independent action values h[a] learned from rewards (model-free habit).
      These biases are added to the RL values before the softmax, scaled by habit_bias_scale.
    - WM system: stores rewarded actions one-shot with strength tied to wm_weight_base;
      WM traces also decay each visit. WM weight is reduced in older adults and when set size is large.
    - Arbitration: mixture of WM policy and RL+Habit policy using set size and age scaled WM weight.
    
    Parameters
    - model_parameters: [lr, wm_weight_base, softmax_beta, habit_lr, habit_bias_scale, age_wm_penalty]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - habit_lr: learning rate for global habit values h[a] (0..1)
        - habit_bias_scale: strength of adding habit values to RL Q (can be +/-)
        - age_wm_penalty: fractional reduction of WM weight for older adults (0..1)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, habit_lr, habit_bias_scale, age_wm_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        h = np.zeros(nA)  # global habit values

        # WM weight scaled by set size and age
        wm_weight_block = wm_weight_base * (3.0 / nS) * (1.0 - age_wm_penalty * age_group)
        wm_weight_block = min(1.0, max(0.0, wm_weight_block))

        # Tie a small WM decay to habit_lr to ensure both are used meaningfully (bounded to [0,1])
        wm_decay = max(0.0, min(1.0, 0.5 * habit_lr))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL + Habit effective values
            Q_eff = Q_s + habit_bias_scale * h

            denom_rl = np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Habit update (global across states)
            h[a] += habit_lr * (r - h[a])

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM one-shot storage on reward with strength tied to wm_weight_base
            if r > 0.5:
                gamma = wm_weight_base  # storage strength
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - gamma) * w[s, :] + gamma * target
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-wise associability (Pearce–Hall) + WM with capacity-limited mixture.

    Mechanism
    - RL system: learning rate is modulated by a state-specific associability α_s that tracks unsigned PE.
      α_s is updated via an exponential moving average; older adults have reduced effective associability.
    - WM system: one-shot storage on reward; WM decays toward uniform. The arbitration weight increases
      with an effective capacity function over set size: 1 - exp(-k_capacity * (3/nS)).
    - Arbitration: mixture of WM policy and RL policy using the capacity-derived WM weight.

    Parameters
    - model_parameters: [lr_base, wm_weight_base, softmax_beta, k_capacity, alpha_decay, age_assoc_penalty]
        - lr_base: baseline RL learning rate (0..1)
        - wm_weight_base: baseline WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - k_capacity: controls how strongly small set sizes increase WM weight (>=0)
        - alpha_decay: associability update rate (0..1); also used as WM decay for parsimony
        - age_assoc_penalty: fractional reduction of associability for older adults (0..1)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_base, wm_weight_base, softmax_beta, k_capacity, alpha_decay, age_assoc_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        alpha = np.ones(nS)  # state-wise associability

        # Capacity-derived WM weight
        cap = 1.0 - np.exp(-k_capacity * (3.0 / nS))
        wm_weight_block = wm_weight_base * cap
        wm_weight_block = min(1.0, max(0.0, wm_weight_block))

        # Effective associability scaling by age
        age_scale = 1.0 - age_assoc_penalty * age_group
        age_scale = max(0.0, age_scale)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            lr_eff = lr_base * alpha[s] * age_scale
            q[s, a] += lr_eff * pe

            # Update associability α_s with unsigned PE EMA
            alpha[s] = (1.0 - alpha_decay) * alpha[s] + alpha_decay * np.abs(pe)
            alpha[s] = min(1.0, max(0.0, alpha[s]))

            # WM decay toward uniform using alpha_decay
            w[s, :] = (1.0 - alpha_decay) * w[s, :] + alpha_decay * w_0[s, :]

            # WM one-shot storage on reward with strength tied to current associability
            if r > 0.5:
                gamma = alpha[s]
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - gamma) * w[s, :] + gamma * target
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p