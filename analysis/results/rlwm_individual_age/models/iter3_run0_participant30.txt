def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM arbitration via uncertainty (entropy) with age and set-size modulation.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_lr, wm_decay_base, arb_beta]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM prior weight (0..1); reduced in older adults
        - softmax_beta: RL inverse temperature (>0); scaled by 10 internally
        - wm_lr: WM learning rate (0..1)
        - wm_decay_base: baseline WM decay toward prior (0..1)
        - arb_beta: arbitration sensitivity to WM vs RL confidence (>=0)

    Notes
    -----
    - WM policy is near-deterministic (high beta).
    - Arbitration weight is a sigmoid of (WM confidence - RL confidence),
      then blended with an age- and set-size-modulated prior weight.
    - Set-size reduces WM contribution; older age further reduces it.
    Returns negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_lr, wm_decay_base, arb_beta = model_parameters
    softmax_beta *= 10.0  # higher upper bound as per template

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age and set-size priors on WM use
        size_factor = 3.0 / float(nS)  # 1.0 for set size=3; 0.5 for set size=6
        wm_prior = wm_weight_base * size_factor * (1.0 - 0.4 * age_group)
        wm_prior = np.clip(wm_prior, 0.0, 1.0)

        # Effective WM decay increases with set size and in older age
        wm_decay_eff = wm_decay_base + (1.0 - size_factor) * wm_decay_base + 0.3 * age_group * (1.0 - wm_decay_base)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            # WM policy probability of chosen action (near-deterministic softmax)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute confidences via negative entropy (higher is more confident)
            def neg_entropy(pvec):
                pvec = np.exp(softmax_beta * (pvec - np.max(pvec)))
                pvec = pvec / np.sum(pvec)
                return -np.sum(pvec * np.log(np.clip(pvec, 1e-12, 1.0)))

            conf_rl = -neg_entropy(Q_s)
            # For WM, use its own temperature to derive distribution
            pvec_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pvec_wm = pvec_wm / np.sum(pvec_wm)
            conf_wm = -np.sum(pvec_wm * np.log(np.clip(pvec_wm, 1e-12, 1.0)))

            # Arbitration weight based on relative confidence
            arb_signal = conf_wm - conf_rl
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-arb_beta * arb_signal))
            # Blend with prior modulated by age and set size
            wm_weight_eff = np.clip(0.5 * wm_prior + 0.5 * wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay to prior and update
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0
            w[s, a] += wm_lr * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL + capacity-limited WM with lapse. WM encodes up to K states.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [alpha_pos, alpha_neg, softmax_beta, wm_K_base, lapse, age_penalty]
        - alpha_pos: RL learning rate for positive PE (0..1) [mapped from 'lr']
        - alpha_neg: RL learning rate for negative PE (0..1) [mapped from 'wm_weight']
        - softmax_beta: RL inverse temperature (>0); scaled by 10 internally
        - wm_K_base: baseline WM capacity in number of states (0..6)
        - lapse: stimulus-independent lapse probability (0..0.2)
        - age_penalty: capacity penalty per age group (>=0), reduces K if older

    Notes
    -----
    - WM stores a deterministic mapping for up to K states (one-shot when rewarded).
    - K decreases with set size (proportional scaling) and with older age.
    - WM policy is near-deterministic if the state is stored; otherwise uniform.
    - Final choice probability includes a lapse that chooses uniformly at random.
    Returns negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_K_base, lapse, age_penalty = model_parameters
    lr = alpha_pos  # to respect template naming
    wm_weight = alpha_neg  # used as alpha_neg
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic when stored
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity K depends on set size and age
        size_scale = 3.0 / float(nS)  # 1 for 3, 0.5 for 6
        K_eff = wm_K_base * size_scale - age_penalty * age_group
        K_eff = int(np.clip(np.round(K_eff), 0, nS))

        # Track which states are stored in WM (up to K)
        stored = np.zeros(nS, dtype=bool)
        n_stored = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic if state is stored, else uniform
            if stored[s]:
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                # Not stored: WM provides no info; use uniform
                p_wm = 1.0 / nA

            # Mixture: capacity-limited WM dominates when stored
            wm_weight_eff = 1.0 if stored[s] else 0.0
            p_mixture = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Add lapse
            p_total = (1.0 - lapse) * p_mixture + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr if pe >= 0 else wm_weight  # alpha_pos vs alpha_neg
            q[s, a] += alpha * pe

            # WM storage/update: one-shot store only if rewarded
            if r > 0.5:
                # Allocate slot if not stored and capacity remains
                if (not stored[s]) and (n_stored < K_eff):
                    stored[s] = True
                    n_stored += 1
                if stored[s]:
                    # Store a near-deterministic one-hot preference for chosen action
                    w[s, :] = (1.0 / nA)
                    w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM mixture with perseveration bias and age/set-size modulated temperature.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, kappa, age_temp_scale]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM weight (0..1), reduced with larger set size and older age
        - softmax_beta: base RL inverse temperature (>0); scaled by 10 internally
        - wm_decay: WM decay toward prior each trial (0..1)
        - kappa: perseveration bias to repeat last action (>=0)
        - age_temp_scale: scales temperature reduction for older adults (>=0)

    Notes
    -----
    - RL softmax includes a stickiness term favoring previous action, independent of state.
    - Temperature decreases with set size and with older age (more noise).
    - WM updates via decay to prior and instant pull toward recent outcome on chosen action.
    Returns negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, kappa, age_temp_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Temperature modulation by set size and age
        size_temp_scale = 3.0 / float(nS)  # 1 for 3, 0.5 for 6
        beta_eff = softmax_beta * size_temp_scale / (1.0 + age_temp_scale * age_group)

        # WM weight decreases with set size and age
        wm_weight_eff = wm_weight_base * size_temp_scale * (1.0 - 0.3 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        last_action = None  # for perseveration within block
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bias to RL values
            if last_action is not None:
                stick = np.zeros_like(Q_s)
                stick[last_action] = kappa
                Q_s = Q_s + stick

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay to prior and instant pull toward recent outcome
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Update chosen action only; reward strengthens, non-reward weakens
            w[s, a] += (r - w[s, a])

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p