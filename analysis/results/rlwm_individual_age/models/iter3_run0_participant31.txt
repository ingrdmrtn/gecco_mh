def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with set-size-specific WM arbitration and age-specific WM decay.

    The model blends a model-free RL controller with a working-memory (WM) controller.
    - RL: standard Rescorla-Wagner with a single learning rate.
    - WM: a cached probability distribution over actions for each state that decays toward
      uniform at an age-dependent rate; on rewarded trials, WM writes a near one-hot trace.
    - Arbitration: the WM weight is set-size specific (one parameter for nS=3, another
      for nS=6), capturing load effects on WM reliance.

    Age modulation:
    - Young (age_group=0) vs Old (age_group=1). WM decay is higher for older participants
      via separate decay parameters for young vs old.

    Parameters
    ----------
    model_parameters : list or tuple
        [lr, softmax_beta, wm_weight_3, wm_weight_6, wm_decay_young, wm_decay_old]
        - lr: RL learning rate (0..1).
        - softmax_beta: base RL inverse temperature (scaled by 10 internally).
        - wm_weight_3: mixture weight for WM when set size is 3.
        - wm_weight_6: mixture weight for WM when set size is 6.
        - wm_decay_young: per-visit WM decay toward uniform for young participants.
        - wm_decay_old: per-visit WM decay toward uniform for older participants.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_3, wm_weight_6, wm_decay_young, wm_decay_old = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM choice
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL Q-values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM cached policy
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform prior for WM

        # Set-size dependent WM weight
        wm_weight = wm_weight_3 if nS == 3 else wm_weight_6
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        # Age-dependent WM decay
        wm_decay = wm_decay_young if age_group == 0 else wm_decay_old
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of the chosen action under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax on WM distribution
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform every visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM write: push toward a one-hot on rewarded trials
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong write with fast normalization
                w[s, :] = 0.1 * w[s, :] + 0.9 * one_hot
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q-forgetting and capacity-limited WM with load/age-driven WM lapse.

    - RL: Rescorla-Wagner with a single learning rate and value forgetting toward
      uniform (q_forget), modeling volatility/limited retention.
    - WM: stores the last rewarded action per state (near one-hot). A retrieval lapse
      increases superlinearly when set size exceeds a capacity (cap_slots), and is
      worse for older age. WM choice is a mixture of its deterministic policy and
      uniform with lapse probability.
    - Arbitration: standard mixture of WM and RL policies.

    Age modulation:
    - WM lapse increases with age via exponent factor on load ratio.

    Parameters
    ----------
    model_parameters : list or tuple
        [lr, softmax_beta, pe_gain, q_forget, cap_slots, wm_lapse_base]
        - lr: RL learning rate (0..1).
        - softmax_beta: base RL inverse temperature (scaled by 10 internally).
        - pe_gain: nonlinear scaling of prediction error (applies tanh(pe_gain * PE)).
        - q_forget: per-visit forgetting toward uniform for Q-values (0..1).
        - cap_slots: effective WM capacity in number of states (e.g., ~3-4).
        - wm_lapse_base: baseline WM retrieval lapse at capacity; scales with load and age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, pe_gain, q_forget, cap_slots, wm_lapse_base = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load- and age-dependent WM lapse
        load_ratio = max(nS / max(cap_slots, 1e-6), 1e-6)
        # Exponent increases with age_group, making lapse worse for older
        lapse_exp = 1.0 + 0.5 * age_group
        wm_lapse = wm_lapse_base * (load_ratio ** lapse_exp)
        wm_lapse = float(np.clip(wm_lapse, 0.0, 0.99))  # cap to avoid degeneracy

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with internal lapse (mixture with uniform at WM stage)
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Probability of action under uniform
            p_uniform_a = 1.0 / nA
            p_wm = (1.0 - wm_lapse) * p_wm_det + wm_lapse * p_uniform_a

            # Arbitration: equal weighting parameter not included; we implicitly
            # allow WM/RL blending via these probabilities with equal mixing strength.
            # To keep mixture flexible under load, scale WM contribution mildly by load_ratio.
            wm_weight = float(np.clip(1.0 / (1.0 + load_ratio), 0.0, 1.0))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL forgetting toward uniform
            q[s, :] = (1.0 - q_forget) * q[s, :] + q_forget * (1.0 / nA)

            # RL update with nonlinear PE compression
            pe_raw = r - q[s, a]
            pe = np.tanh(pe_gain * pe_raw)
            q[s, a] += lr * pe

            # WM update
            # Small leakage toward uniform proportional to lapse (poorer retention at high load/age)
            w[s, :] = (1.0 - 0.5 * wm_lapse) * w[s, :] + (0.5 * wm_lapse) * w_0[s, :]
            if r > 0.0:
                # On reward, write near one-hot trace
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM arbitration based on RL uncertainty, with age-dependent temperature and lapse.

    - RL: standard Rescorla-Wagner with a single learning rate.
    - WM: recency-weighted memory trace updated toward the chosen action on reward,
      with fixed learning rate and mild decay.
    - Arbitration: WM weight increases when RL is uncertain (high choice entropy).
      Uncertainty sensitivity controlled by uncert_temp. Set size increases RL
      stochasticity (lower effective beta), and older age further reduces the effective
      beta while increasing a global lapse.

    Parameters
    ----------
    model_parameters : list or tuple
        [lr, beta_base, wm_bias, lapse_base, uncert_temp, age_noise_mult]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature (scaled by 10 internally).
        - wm_bias: baseline WM mixture bias (0..1) before uncertainty modulation.
        - lapse_base: baseline lapse mixed into the final policy (0..1).
        - uncert_temp: sensitivity of WM arbitration to RL uncertainty (0..+).
        - age_noise_mult: multiplicative increase of lapse and reduction of beta with age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_bias, lapse_base, uncert_temp, age_noise_mult = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age modulate RL temperature and global lapse
        beta_eff = softmax_beta / (1.0 + 0.5 * (nS - 3))  # lower beta for larger set size
        beta_eff /= (1.0 + 0.5 * age_group * age_noise_mult)  # additional reduction for older
        beta_eff = max(beta_eff, 1e-3)

        lapse_eff = lapse_base * (1.0 + 0.5 * (nS - 3)) * (1.0 + age_group * age_noise_mult)
        lapse_eff = float(np.clip(lapse_eff, 0.0, 0.3))  # bounded to keep identifiable

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax with adjusted temperature
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL uncertainty via softmax distribution entropy
            logits = beta_eff * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            entropy = -np.sum(p_vec * np.log(np.clip(p_vec, 1e-12, 1.0))) / np.log(nA)  # normalized 0..1

            # Arbitration: WM weight increases with uncertainty
            wm_weight = np.clip(wm_bias + uncert_temp * entropy, 0.0, 1.0)

            # Combine controllers
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Global lapse to uniform after arbitration
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)

            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: mild decay plus reward-driven write
            # Fixed WM learning rate and decay to limit parameters
            eta_w = 0.6  # write strength on reward
            decay_w = 0.1  # per-visit decay toward uniform
            w[s, :] = (1.0 - decay_w) * w[s, :] + decay_w * w_0[s, :]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_w) * w[s, :] + eta_w * one_hot
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p