def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility traces) + WM with load-driven overwriting and age-biased arbitration.

    Mechanism
    - RL: Q-learning with eligibility traces across state-action pairs to allow credit to
      persist within a block. Decision inverse temperature decreases with larger set size.
    - WM: fast, near-deterministic cache for the most recent rewarded mappings, subject to
      load-dependent overwriting.
    - Arbitration: mixture of WM and RL, with an age-biased baseline WM weight.

    Parameters
    ----------
    states : array-like of int
        State per trial (0..nS-1 in each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, beta_base, wm_weight_base, lambda_et, interference_slope, age_wm_bias]
        - alpha: RL learning rate for Q
        - beta_base: base inverse temperature for RL policy (scaled internally by *10)
        - wm_weight_base: baseline WM weight in the mixture
        - lambda_et: eligibility trace decay (0..1)
        - interference_slope: how much WM overwriting increases with set size
        - age_wm_bias: how much age shifts WM arbitration (negative reduces WM if older)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.

    Notes on set size and age usage
    - Set size decreases RL inverse temperature (more noise) and increases WM overwriting via interference_slope.
    - Age group adds an additive shift to the WM weight via age_wm_bias (older -> lower WM weight if age_wm_bias<0).
    """
    alpha, beta_base, wm_weight_base, lambda_et, interference_slope, age_wm_bias = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # WM store (probability over actions per state)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Load-dependent RL noise
            load = max(0, nS - 3)
            beta_eff = softmax_beta / (1.0 + 0.4 * load)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM choice probability for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Age-shifted WM arbitration weight
            wm_w = wm_weight_base + age_wm_bias * age_group
            # Reduce WM influence under higher load via interference slope
            wm_w = wm_w / (1.0 + interference_slope * load)
            wm_w = np.clip(wm_w, 0.0, 1.0)

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # Update traces: decay all, then set chosen trace to 1
            e *= lambda_et
            e[s, a] = 1.0
            # Q update
            q += alpha * delta * e

            # WM updating with load-dependent overwriting
            # Interference/forgetting increases with load
            overwrite = 0.05 + 0.25 * (load / 3.0)  # ~0.05 at nS=3, ~0.30 at nS=6
            w[s, :] = (1.0 - overwrite) * w[s, :] + overwrite * w_0[s, :]

            # On rewarded trials, strengthen the chosen action in WM
            if r > 0.0:
                write = 0.6  # consolidate towards one-hot
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted arbitration: RL with uncertainty tracking + WM capacity limit,
    with a set-sizeâ€“dependent lapse.

    Mechanism
    - RL: Q-learning; track per-state uncertainty as running absolute PE to modulate arbitration.
    - WM: near-deterministic store subject to an effective capacity; when set size exceeds capacity,
      WM reliability is reduced.
    - Arbitration: weight favors WM when WM is reliable and RL uncertainty is high; otherwise favors RL.
    - Lapse: add a uniform-choice lapse that increases with set size and age.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, beta_base, wm_weight0, capacity, lapse_base, age_scale]
        - alpha: RL learning rate
        - beta_base: base inverse temperature for RL (scaled internally by *10)
        - wm_weight0: baseline arbitration weight for WM
        - capacity: effective WM capacity in number of items
        - lapse_base: base lapse probability at set size 3
        - age_scale: multiplicative increase of lapse and decrease of capacity with age

    Returns
    -------
    float
        Negative log-likelihood.

    Notes on set size and age usage
    - Set size reduces WM reliability when nS > capacity and increases lapse from lapse_base.
    - Age group reduces effective capacity and increases lapse via age_scale.
    """
    alpha, beta_base, wm_weight0, capacity, lapse_base, age_scale = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age affects capacity and lapse
    cap_eff = max(1.0, capacity * (1.0 - 0.3 * age_group * age_scale))
    lapse3 = np.clip(lapse_base * (1.0 + 0.5 * age_group * age_scale), 0.0, 0.3)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # Track RL uncertainty: running mean absolute PE per state
        u = np.ones(nS) * 0.5

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            load = max(0, nS - 3)
            beta_eff = softmax_beta / (1.0 + 0.3 * load)

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM reliability decreases if set size exceeds effective capacity
            overload = max(0.0, (nS - cap_eff) / max(1.0, cap_eff))
            wm_rel = np.clip(np.exp(-2.0 * overload), 0.0, 1.0)

            # Arbitration weight: prefer WM when RL is uncertain and WM is reliable
            # Normalize uncertainty to [0,1]
            rl_unc = np.clip(u[s], 0.0, 1.0)
            wm_w = wm_weight0 * (0.5 + 0.5 * rl_unc) * wm_rel
            wm_w = np.clip(wm_w, 0.0, 1.0)

            p_mix = wm_w * p_wm + (1.0 - wm_w) * p_rl

            # Lapse increases with load and age
            lapse = np.clip(lapse3 * (1.0 + 0.5 * load) * (1.0 + 0.5 * age_group * age_scale), 0.0, 0.6)
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # Update RL uncertainty (running absolute PE)
            u[s] = 0.8 * u[s] + 0.2 * abs(delta)

            # WM decay and write
            forget = 0.08 + 0.22 * (load / 3.0)  # ~0.08 at 3, ~0.30 at 6
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]
            if r > 0.0:
                write = 0.65 * (1.0 - overload)  # less effective write when overloaded
                write = np.clip(write, 0.0, 0.65)
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay and entropy-bonus exploration + WM gated by unsigned prediction error.

    Mechanism
    - RL: Q-learning with value decay to counteract interference across visits within block.
      Decision policy includes an entropy bonus derived from WM distribution to promote
      exploration when WM is uncertain.
    - WM: updated strongly on surprising rewards (unsigned PE); decays with load.
    - Arbitration: convex mixture of WM and RL; WM weight is reduced under higher load and
      slightly increased for younger participants.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, beta_base, wm_weight, phi_ent, rl_decay, age_temp_gain]
        - alpha: RL learning rate
        - beta_base: base inverse temperature for RL (scaled internally by *10)
        - wm_weight: baseline WM mixture weight
        - phi_ent: weight of WM entropy bonus added to RL policy values
        - rl_decay: per-visit decay of Q towards uniform
        - age_temp_gain: increases RL inverse temperature for younger participants

    Returns
    -------
    float
        Negative log-likelihood.

    Notes on set size and age usage
    - Set size reduces RL inverse temperature and increases WM decay.
    - Age modulates RL temperature (age_temp_gain boosts beta for young) and WM weight (slightly higher for young).
    """
    alpha, beta_base, wm_weight, phi_ent, rl_decay, age_temp_gain = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age effect on RL temperature (young -> hotter inverse temp)
    beta_age_mult = 1.0 + age_temp_gain * (1.0 - age_group)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        q0 = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            load = max(0, nS - 3)

            # RL policy values augmented with entropy bonus from WM uncertainty
            W_s = w[s, :]
            entropy_w = -np.sum(W_s * np.log(np.clip(W_s, 1e-12, 1.0))) / np.log(nA)  # normalized [0,1]
            Q_s = q[s, :]

            # Effective beta decreases with load, increases for young
            beta_eff = (softmax_beta * beta_age_mult) / (1.0 + 0.35 * load)

            # Add entropy bonus equally to all actions to encourage exploration where WM is uncertain.
            # Implement as softmax temperature reduction: add phi_ent*entropy to Q prior to softmax is equivalent
            # to subtracting from denominator for chosen action uniformly. We implement by actual augmentation:
            Q_aug = Q_s + phi_ent * entropy_w

            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_aug - Q_aug[a])))

            # WM choice probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: WM weight reduced by load; young get slight boost
            wm_w_eff = wm_weight / (1.0 + 0.5 * load)
            wm_w_eff *= (1.0 + 0.1 * (1.0 - age_group))
            wm_w_eff = np.clip(wm_w_eff, 0.0, 1.0)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay towards uniform
            delta = r - Q_s[a]
            q[s, a] += alpha * delta
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * q0[s, :]

            # WM updating: decay increases with load; write strength scales with unsigned PE
            forget = 0.06 + 0.24 * (load / 3.0)  # ~0.06 at 3, ~0.30 at 6
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]
            pe_mag = abs(delta)
            if r > 0.0:
                write = 0.3 + 0.5 * pe_mag  # stronger for surprising rewards
                write = np.clip(write, 0.0, 0.8)
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p