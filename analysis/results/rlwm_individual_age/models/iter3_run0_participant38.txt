def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with capacity-limited memory and exploration bonus.

    Mechanisms:
    - RL: Q-learning with an uncertainty-driven exploration bonus using visit counts.
      The exploration bonus increases with set size and decreases for older adults.
    - WM: one-shot encoding of rewarded stimulus–action mappings with global capacity
      limits across states. WM strength per state decays over trials, more so with
      higher set size and in older adults.
    - Arbitration: fixed gate scaled by set size and age (more reliance on WM for
      small sets and younger adults).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta: base inverse temperature for RL softmax (scaled x10 internally)
    - wm_gate: base WM mixture weight (0..1) before set-size/age scaling
    - cap_base: base WM capacity in number of states (e.g., 1..3)
    - age_cap_drop: additional decay/interference with age (>=0)
    - bonus: exploration bonus magnitude for rarely tried state-actions (>=0)

    Age and set-size effects:
    - Effective WM mixture weight: wm_gate_eff = wm_gate * (3/nS) * (1 - 0.5*age_group).
    - Effective capacity: cap_eff = min(nS, max(1, round(cap_base - age_cap_drop*age_group*(nS/3)))).
    - WM decay increases with set size and age: gamma_wm = 1 - exp(-age_cap_drop*(nS/3)*(1+0.5*age_group)).
    - RL exploration bonus scales with set size and age: bonus_eff = bonus*(nS/3)*(1 - 0.3*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_gate, cap_base, age_cap_drop, bonus = model_parameters
    beta_eff = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state WM strength for capacity control
        m_strength = np.zeros(nS)

        # Visit counts for exploration bonus
        visits = np.ones((nS, nA)) * 0.5

        # Effective parameters with set size and age
        wm_gate_eff = np.clip(wm_gate, 0.0, 1.0) * (3.0 / max(1.0, nS)) * (1.0 - 0.5 * age_group)
        wm_gate_eff = np.clip(wm_gate_eff, 0.0, 1.0)
        cap_eff = int(min(nS, max(1, int(np.round(cap_base - age_cap_drop * age_group * (nS / 3.0))))))
        bonus_eff = max(0.0, bonus * (nS / 3.0) * (1.0 - 0.3 * age_group))
        # WM decay/interference per trial
        gamma_wm = 1.0 - np.exp(-max(0.0, age_cap_drop) * (nS / 3.0) * (1.0 + 0.5 * age_group))
        gamma_wm = np.clip(gamma_wm, 0.0, 1.0)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with exploration bonus
            Q_s = q[s, :].copy()
            bonus_vec = bonus_eff / np.sqrt(np.maximum(1.0, visits[s, :]))
            Q_bonus = Q_s + bonus_vec
            denom_rl = np.sum(np.exp(beta_eff * (Q_bonus - Q_bonus[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (deterministic-ish)
            W_s = w[s, :]
            beta_wm = 50.0
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration
            p_total = wm_gate_eff * p_wm + (1.0 - wm_gate_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            # Increment visit count
            visits[s, a] += 1.0

            # WM global decay toward uniform to implement interference
            w = (1.0 - gamma_wm) * w + gamma_wm * w0
            m_strength *= (1.0 - gamma_wm)

            # Rewarded trials encode into WM
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strengthen this state's memory trace and overwrite row partially
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
                m_strength[s] = min(1.0, m_strength[s] + 0.5)

            # Enforce capacity across states: only strongest 'cap_eff' states retain sharp WM
            if nS > 0 and cap_eff < nS:
                # find indices sorted by strength
                idx = np.argsort(-m_strength)
                keep = set(idx[:cap_eff])
                for ss in range(nS):
                    if ss not in keep:
                        # push toward uniform
                        w[ss, :] = 0.7 * w[ss, :] + 0.3 * w0[ss, :]
                        # also reduce its strength
                        m_strength[ss] *= 0.7

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and choice kernel + noisy WM retrieval.

    Mechanisms:
    - RL: Q-learning with TD(λ) eligibility traces across state-action pairs to
      propagate credit. Eligibility decays over trials.
    - Choice kernel: habitual bias to repeat chosen actions within a state; kernel
      is learned with its own learning rate and biases RL softmax.
    - WM: one-shot storage of rewarded mappings; retrieval precision decreases with
      set size and for older adults (noisier WM). WM content decays slightly per trial.
    - Arbitration: fixed WM weight scaled by set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta: base inverse temperature for RL softmax (scaled x10 internally)
    - trace: eligibility trace parameter λ (0..1)
    - kernel_lr: learning rate for choice kernel (>=0)
    - wm_weight_base: base WM mixture weight (0..1)
    - age_wm_noise: scales WM temperature degradation with age and set size (>=0)

    Age and set-size effects:
    - WM temperature: beta_wm = 50 / (1 + age_wm_noise * (nS/3) * (1 + age_group))
      -> larger nS and older age reduce WM precision.
    - WM mixture: wm_weight = wm_weight_base * (3/nS) * (1 - 0.25*age_group).
    - Eligibility traces and kernel operate identically across set sizes but their
      influence on policy depends indirectly via softmax.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, trace, kernel_lr, wm_weight_base, age_wm_noise = model_parameters
    beta_eff = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces and choice kernel
        e = np.zeros((nS, nA))
        k = np.zeros((nS, nA))

        # Effective parameters
        wm_weight = np.clip(wm_weight_base, 0.0, 1.0) * (3.0 / max(1.0, nS)) * (1.0 - 0.25 * age_group)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)
        beta_wm = 50.0 / (1.0 + max(0.0, age_wm_noise) * (nS / 3.0) * (1.0 + 1.0 * age_group))
        beta_wm = max(1.0, beta_wm)

        # WM decay per trial (small)
        wm_decay = np.clip(0.05 * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.5)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with choice kernel bias
            Q_s = q[s, :].copy()
            bias_s = k[s, :].copy()
            Q_bias = Q_s + bias_s
            denom_rl = np.sum(np.exp(beta_eff * (Q_bias - Q_bias[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # TD(λ) update
            pe = r - Q_s[a]
            # Decay eligibilities
            e *= trace
            # Accumulate current eligibility
            e[s, a] += 1.0
            # Update all Q with eligibility
            q += lr * pe * e

            # Choice kernel update (state-local habit)
            k[s, :] *= (1.0 - kernel_lr)
            k[s, a] += kernel_lr

            # WM decay and encode if rewarded
            w = (1.0 - wm_decay) * w + wm_decay * w0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strengthen current state mapping
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Adaptive meta-control: WM gating by recent reward rate + age- and set-size-adaptive RL.

    Mechanisms:
    - RL: Q-learning with inverse temperature and learning rate that adapt to set size
      and age (reduced beta and lr for larger sets and in older adults).
    - WM: one-shot encoding of rewarded mappings with small forgetting; forgetting
      increases with set size and age.
    - Meta-control: WM mixture weight is a logistic function of recent block-wise reward
      rate; scaled to rely more on WM when reward rate is high and set size is small.

    Parameters (model_parameters):
    - lr_base: base RL learning rate (0..1)
    - beta_base: base inverse temperature (scaled x10 internally)
    - wm_gate_base: base gain for mapping reward rate to WM weight
    - reward_sensitivity: inverse time constant for running reward rate (>=0)
    - age_beta_drop: additional softening of beta with age and set size (>=0)
    - wm_forget: base WM forgetting rate (>=0)

    Age and set-size effects:
    - lr_eff = lr_base / (1 + 0.5*(nS-3) + 0.5*age_group)
    - beta_eff = (beta_base*10) / (1 + age_beta_drop*age_group*(nS/3))
    - WM forgetting: gamma_wm = 1 - exp(-wm_forget*(nS/3)*(1 + 0.5*age_group))
    - WM weight: wm_weight = sigmoid(wm_gate_base*(rew_rate - 0.5)) * (3/nS)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, beta_base, wm_gate_base, reward_sensitivity, age_beta_drop, wm_forget = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL params
        lr_eff = lr_base / (1.0 + 0.5 * max(0, nS - 3) + 0.5 * age_group)
        beta_eff = (beta_base * 10.0) / (1.0 + max(0.0, age_beta_drop) * age_group * (nS / 3.0))
        beta_eff = max(0.1, beta_eff)

        # WM forgetting rate
        gamma_wm = 1.0 - np.exp(-max(0.0, wm_forget) * (nS / 3.0) * (1.0 + 0.5 * age_group))
        gamma_wm = np.clip(gamma_wm, 0.0, 1.0)

        # Running reward rate for meta-control
        rew_rate = 0.5
        alpha_rr = 1.0 - np.exp(-max(0.0, reward_sensitivity))
        alpha_rr = np.clip(alpha_rr, 0.0, 1.0)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            beta_wm = 50.0
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Meta-control: WM weight from recent reward rate and set size
            # logistic gating; scaled by (3/nS)
            gate_input = wm_gate_base * (rew_rate - 0.5)
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_dyn *= (3.0 / max(1.0, nS))
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr_eff * pe

            # WM forgetting and encode when rewarded
            w = (1.0 - gamma_wm) * w + gamma_wm * w0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

            # Update running reward rate
            rew_rate = (1.0 - alpha_rr) * rew_rate + alpha_rr * r

        blocks_log_p += log_p

    return -blocks_log_p