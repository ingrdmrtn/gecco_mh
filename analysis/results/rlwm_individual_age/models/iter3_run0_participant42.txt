def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited WM with age- and load-dependent interference and arbitration.

    Mechanisms
    ----------
    - RL: Q-learning with separate learning rates for positive/negative prediction errors.
    - WM: slot-like one-shot storage after rewarded trials; decays toward uniform with interference
          that increases with set size and with age.
    - Arbitration: WM weight derived from effective capacity coverage relative to set size,
                   passed through a sigmoid with a free bias.
    - Temperature: RL inverse temperature scaled up internally (beta*10). WM temperature fixed high.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..set_size-1 for the current block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Feedback per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like
        Single value repeated; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_rl, C_wm, xi_interf_age, gate_bias0]
        - alpha_pos: RL learning rate for positive PE.
        - alpha_neg: RL learning rate for negative PE.
        - beta_rl: RL inverse temperature (scaled by 10 internally).
        - C_wm: baseline WM capacity (in items).
        - xi_interf_age: controls how WM decay/interference scales with load and age.
        - gate_bias0: baseline arbitration bias toward WM (sigmoid bias).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_rl, C_wm, xi_interf_age, gate_bias0 = model_parameters
    softmax_beta = beta_rl * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity reduced in older adults
        C_eff = max(0.0, C_wm - 0.5 * age_group)
        # Interference/decay factor increases with load and age
        load = float(max(0, nS - 3))  # 0 for 3, 3 for 6
        base_forget = np.clip(0.05 + 0.05 * load + xi_interf_age * (0.2 * age_group + 0.1 * load), 0.0, 0.9)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration based on capacity coverage (more coverage => rely more on WM)
            coverage = np.clip(C_eff / max(1.0, nS), 0.0, 1.0)
            wm_weight = 1.0 / (1.0 + np.exp(-(gate_bias0 + 5.0 * (coverage - 0.5))))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            pe = r - Q_s[a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += lr * pe

            # WM decay and update:
            # Decay toward uniform with interference determined by load/age
            w[s, :] = (1.0 - base_forget) * w[s, :] + base_forget * w_0[s, :]
            # After rewarded trial, store a sharp memory for the correct action
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded, slightly downweight the chosen action in WM trace
                w[s, a] = 0.5 * w[s, a] + 0.5 * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-weighted WM recency buffer and age/load-dependent lapses.

    Mechanisms
    ----------
    - RL: single learning rate Q-learning.
    - Uncertainty: per-state visit counts define uncertainty (fewer visits => more uncertain).
      Arbitration increases WM weight under uncertainty but is weaker for older adults.
    - WM: recency-limited buffer of size K_recency; after rewarded trial, state is stored as
      a one-hot action memory; unrewarded trials allow decay. WM decays toward uniform each trial.
    - Lapse: epsilon-greedy lapses that increase with set size and age.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of float/int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like (age_group derived: 0=young, 1=old)
    model_parameters : list or array
        [alpha, beta_rl, kappa_unc, K_recency, epsilon_base, age_lapse_scale]
        - alpha: RL learning rate (0..1).
        - beta_rl: RL inverse temperature (scaled by 10 internally).
        - kappa_unc: sensitivity of WM gating to uncertainty.
        - K_recency: WM recency buffer size (items).
        - epsilon_base: base lapse probability.
        - age_lapse_scale: additional lapse probability for older adults.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_rl, kappa_unc, K_recency, epsilon_base, age_lapse_scale = model_parameters
    softmax_beta = beta_rl * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Recency buffer: track which states are currently in WM
        in_buffer = np.zeros(nS, dtype=int)
        buffer_order = []  # list of states from oldest to newest

        # Visit counts for uncertainty
        visits_state = np.zeros(nS, dtype=float)

        # Lapse parameter modulated by load and age
        load = float(max(0, nS - 3))
        epsilon = np.clip(epsilon_base + age_lapse_scale * age_group + 0.05 * load, 0.0, 0.4)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Uncertainty-based arbitration:
            # Uncertainty ~ high when few visits to this state
            U_s = 1.0 / (1.0 + visits_state[s])  # in (0,1]
            gate_drive = kappa_unc * (U_s - 0.5) - 0.5 * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-gate_drive))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # WM decay toward uniform each time the state is visited
            forget = 0.1 + 0.1 * load + 0.1 * age_group
            forget = np.clip(forget, 0.0, 0.9)
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]

            # WM recency buffer management and update
            if r >= 0.5:
                # Put state into buffer (recency-limited)
                if in_buffer[s] == 0:
                    buffer_order.append(s)
                    in_buffer[s] = 1
                    # Evict oldest if capacity exceeded
                    while len(buffer_order) > int(max(0, K_recency)):
                        evict = buffer_order.pop(0)
                        in_buffer[evict] = 0
                        # On eviction, reset its WM row toward uniform
                        w[evict, :] = w_0[evict, :].copy()
                else:
                    # Move to most recent
                    if s in buffer_order:
                        buffer_order.remove(s)
                        buffer_order.append(s)
                # Store sharp memory for rewarded action
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded, weaker update: slight suppression of chosen action
                w[s, a] = 0.6 * w[s, a] + 0.4 * (1.0 / nA)

            # Update visit counts
            visits_state[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with surprise-driven meta-control to WM and novelty bonus; age reduces meta-control.

    Mechanisms
    ----------
    - RL: Actor-Critic.
      Critic V(s) updated with alpha_c; Actor preferences P(s,a) updated with alpha_a using REINFORCE-like rule.
      RL policy temperature beta_rl scaled up internally.
    - Novelty bonus: adds exploration bonus inversely proportional to action visit count.
    - WM: one-shot storage after reward with leaky forgetting toward uniform.
    - Meta-control: WM weight increases with absolute reward prediction error (surprise).
      Older adults have reduced meta-control gain (more reliance on RL under surprise).

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of float/int
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like (age_group derived: 0=young, 1=old)
    model_parameters : list or array
        [alpha_c, alpha_a, beta_rl, meta_gain, age_meta_penalty, bonus_novel]
        - alpha_c: critic learning rate for V(s).
        - alpha_a: actor learning rate for preferences P(s,a).
        - beta_rl: RL inverse temperature for actor softmax (scaled by 10 internally).
        - meta_gain: gain controlling how much surprise boosts WM weight.
        - age_meta_penalty: reduction in meta-control for older adults.
        - bonus_novel: strength of novelty bonus for under-explored actions.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_c, alpha_a, beta_rl, meta_gain, age_meta_penalty, bonus_novel = model_parameters
    softmax_beta = beta_rl * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Actor preferences and Critic values
        P = np.zeros((nS, nA))               # preferences
        V = 0.5 * np.ones(nS)                # state values

        # Novelty counts per state-action
        N_sa = np.zeros((nS, nA), dtype=float)

        # WM traces
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy logits are preferences plus novelty bonus
            novelty = bonus_novel / np.sqrt(1.0 + N_sa[s, :])
            logits = P[s, :] + novelty
            # Build RL probability of chosen action via softmax denominator trick
            denom_rl = np.sum(np.exp(softmax_beta * (logits - logits[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy from w
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Surprise (absolute TD-error) drives WM arbitration
            delta = r - V[s]
            surprise = abs(delta)
            gain_eff = max(0.0, meta_gain - age_meta_penalty * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-gain_eff * (surprise - 0.2)))  # threshold 0.2
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Critic update
            V[s] += alpha_c * delta

            # Actor update (policy-gradient style around softmax of preferences)
            # Compute current RL softmax probabilities for all actions
            logits_ref = P[s, :] + novelty
            exps = np.exp(softmax_beta * (logits_ref - np.max(logits_ref)))
            pi = exps / np.sum(exps)
            # Update preferences: increase for chosen proportional to (1 - pi[a]), decrease others by pi
            for aa in range(nA):
                if aa == a:
                    P[s, aa] += alpha_a * delta * (1.0 - pi[aa])
                else:
                    P[s, aa] -= alpha_a * delta * (pi[aa])

            # WM decay and update
            load = float(max(0, nS - 3))
            forget = np.clip(0.08 + 0.06 * load + 0.06 * age_group, 0.0, 0.9)
            w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Small corrective nudge toward uniform for the chosen action if incorrect
                w[s, a] = 0.7 * w[s, a] + 0.3 * (1.0 / nA)

            # Update novelty counts
            N_sa[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p