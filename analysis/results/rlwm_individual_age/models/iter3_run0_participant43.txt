def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + slot-based WM with age- and set-sizeâ€“dependent storage probability.

    Mechanism
    - Policy is a mixture of RL softmax and WM softmax, weighted by a storage probability that depends on
      effective WM slots and set size.
    - RL: standard delta rule.
    - WM: if a rewarded association is encountered, it is encoded as a near one-hot row (toward the chosen action).
      WM leaks toward uniform each trial.
    - Age impact: older adults have fewer effective WM slots (reduced storage probability).
    - Set-size impact: larger set size dilutes storage probability because slots are spread across more items.

    Parameters (model_parameters)
    - lr: RL learning rate (0..1)
    - wm_weight: base mixture weight (0..1) for WM when an item is stored
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_slots: effective number of WM slots (0..3)
    - wm_leak: WM leak/update rate toward the encoded item and toward uniform (0..1)
    - age_slot_penalty: reduction factor of effective slots in older group (>=0). Effective slots = wm_slots*(1 - age_slot_penalty*age_group), floored at 0.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_slots, wm_leak, age_slot_penalty = model_parameters
    softmax_beta *= 10.0  # RL temperature scaling

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # high precision WM policy
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective storage probability per state scales with slots and set size, reduced in older adults.
        eff_slots = max(0.0, wm_slots * (1.0 - age_slot_penalty * age_group))
        p_store_base = np.clip(eff_slots / max(1.0, float(nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if stored, use sharp WM softmax; if not stored, uniform.
            # We approximate "stored" probability using p_store_base.
            W_center = W_s - np.max(W_s)
            p_vec_wm = np.exp(softmax_beta_wm * W_center)
            p_vec_wm /= np.sum(p_vec_wm)
            p_wm_stored = p_vec_wm[a]
            p_wm_not_stored = 1.0 / nA
            p_wm = p_store_base * p_wm_stored + (1.0 - p_store_base) * p_wm_not_stored

            # Mixture: when stored, WM contributes with weight wm_weight; otherwise RL dominates implicitly
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform each trial
            w = (1.0 - wm_leak) * w + wm_leak * w_0

            # WM encoding: on rewarded trials, move the row toward a one-hot at the chosen action
            if r > 0.0:
                one_hot = np.full(nA, eps)
                one_hot[a] = 1.0
                one_hot /= np.sum(one_hot)
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * one_hot

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-weighted arbitration to WM; WM decays, and arbitration is dynamic.

    Mechanism
    - Policy is a convex combination of RL and WM, but the WM weight is computed online from state-wise
      RL uncertainty (via visit counts) and current WM confidence (peakiness of WM row).
    - RL: standard delta rule.
    - WM: decays toward uniform each trial; on rewarded trials, it encodes the chosen action as near one-hot.
    - Age impact: in older adults, a bias shifts arbitration away from WM (reduces WM weight).
    - Set-size impact: larger set sizes reduce WM confidence (through decay interacting with less frequent revisits),
      and the arbitration uses state-wise confidence directly.

    Parameters (model_parameters)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - arb_bias: baseline arbitration bias toward WM (positive) or RL (negative)
    - arb_slope: how strongly WM confidence minus RL uncertainty modulates WM weight
    - wm_decay: WM decay per trial toward uniform (0..1)
    - age_arb_shift: additional negative shift applied to arbitration for older adults (>=0)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, arb_bias, arb_slope, wm_decay, age_arb_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visit_counts = np.zeros(nS)  # for RL uncertainty proxy

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_center = W_s - np.max(W_s)
            p_vec_wm = np.exp(softmax_beta_wm * W_center)
            p_vec_wm /= np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            # Dynamic arbitration:
            # RL uncertainty (higher when fewer visits): u = 1/(1+count)
            u_rl = 1.0 / (1.0 + max(0.0, visit_counts[s]))
            # WM confidence: peakiness minus uniform baseline
            wm_conf = max(W_s) - 1.0 / nA
            # Age shift pulls away from WM in older adults
            bias = arb_bias - age_arb_shift * age_group
            wm_w = 1.0 / (1.0 + np.exp(-(bias + arb_slope * (wm_conf - u_rl))))
            wm_w = np.clip(wm_w, 0.0, 1.0)

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            log_p += np.log(max(p_total, eps))

            # Update counts after choice observation
            visit_counts[s] += 1.0

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding on rewarded trials
            if r > 0.0:
                one_hot = np.full(nA, eps)
                one_hot[a] = 1.0
                one_hot /= np.sum(one_hot)
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated temperature + leaky WM.

    Mechanism
    - Policy mixes RL and WM with a fixed wm_weight, but the RL temperature is reduced (more noise) under
      higher set size and further reduced in older adults.
    - RL: standard delta rule with effective inverse temperature scaled by set size and age.
    - WM: leaky integrator that decays toward uniform; on rewarded trials, updates toward a one-hot for the chosen action.
    - Age impact: lowers RL precision specifically (older adults show more choice noise).
    - Set-size impact: lowers RL precision as load increases; also WM forgets via leak irrespective of set size.

    Parameters (model_parameters)
    - lr: RL learning rate (0..1)
    - wm_weight: mixture weight for WM in policy (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - wm_forget_rate: WM decay toward uniform per trial (0..1)
    - temp_load_slope: scales how much set size (6 vs 3) reduces RL precision (>=0)
    - age_temp_boost: additional precision reduction in older adults (>=0)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_forget_rate, temp_load_slope, age_temp_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load- and age-modulated RL temperature for this block
        load_factor = 1.0 + temp_load_slope * max(0, nS - 3)
        age_factor = 1.0 + age_temp_boost * age_group
        softmax_beta_eff = softmax_beta / (load_factor * age_factor)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with effective temperature
            p_rl = 1.0 / np.sum(np.exp(softmax_beta_eff * (Q_s - Q_s[a])))

            # WM policy
            W_center = W_s - np.max(W_s)
            p_vec_wm = np.exp(softmax_beta_wm * W_center)
            p_vec_wm /= np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_forget_rate) * w + wm_forget_rate * w_0

            # WM learning on rewarded trials: move row toward one-hot; learning rate complements forget rate
            if r > 0.0:
                one_hot = np.full(nA, eps)
                one_hot[a] = 1.0
                one_hot /= np.sum(one_hot)
                alpha_wm = 1.0 - wm_forget_rate
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -float(blocks_log_p)