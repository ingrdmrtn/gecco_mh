def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-specific forgetting and epsilon-greedy lapses, mixed with WM (win-stay / lose-forget).
    Older age and larger set sizes increase forgetting and lapses, and reduce WM precision.

    Parameters (model_parameters):
    - lr: Q-learning rate for RL (0..1)
    - wm_weight: base WM mixture weight (0..1), scaled down by set size and age
    - softmax_beta: base inverse temperature for RL (internally scaled by 10); reduced by lapses
    - phi_base: base RL forgetting rate toward uniform (0..1), grows with set size and age
    - epsilon_base: base lapse (epsilon-greedy) rate (0..1), grows with set size
    - age_epsilon_boost: additional lapse multiplicative boost for older adults (>=0)

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi_base, epsilon_base, age_epsilon_boost = model_parameters
    softmax_beta *= 10.0  # keep as specified
    
    # Age group: 0 = young (<=45), 1 = older (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # high precision WM; we will modulate it by age and set size

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective parameters that depend on set size and age
        # Forgetting increases with set size and age
        phi_eff = np.clip(phi_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
        # Lapse increases with set size and further for older adults
        epsilon_eff = np.clip(epsilon_base * (nS / 3.0) * (1.0 + age_group * age_epsilon_boost), 0.0, 0.5)

        # WM weight reduced by set size and age
        wm_weight_eff = wm_weight / (1.0 + (nS - 1))  # stronger drop at nS=6
        wm_weight_eff = wm_weight_eff / (1.0 + 0.5 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # WM precision is reduced with set size and age
        beta_wm_eff = softmax_beta_wm / (1.0 + (nS - 3) * 0.4 + 0.6 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture + epsilon-greedy lapses to uniform
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - epsilon_eff) * p_mix + epsilon_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting to uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply forgetting toward uniform baseline
            q[s, :] = (1.0 - phi_eff) * q[s, :] + phi_eff * (1.0 / nA)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Win-stay: on reward, store one-hot association.
            # Lose-forget: on no reward, decay the WM row slightly toward uniform (dependent on phi_eff).
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # gentle decay toward uniform when not rewarded
                w[s, :] = (1.0 - 0.5 * phi_eff) * w[s, :] + (0.5 * phi_eff) * w_0[s, :]

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + dynamically gated WM: WM weight is meta-learned from recent outcomes.
    Older adults show reduced WM precision and weaker WM gating; larger set sizes also reduce WM.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_gate0: initial WM gate/logit (can be negative/positive); transformed via sigmoid to weight
    - softmax_beta: base inverse temperature for RL (scaled by 10)
    - meta_gain: learning rate for WM gate from recent reward prediction success (>=0)
    - beta_wm_base: base WM inverse temperature (starts high), reduced by set size and age
    - age_wm_supp: additional suppression of WM weight for older adults (>=0)

    Mechanism:
    - Maintain an exponential moving average (EMA) of prediction success (1 if chosen had highest Q, else 0).
    - Update WM gate: gate <- gate + meta_gain*(success - 0.5), with extra penalty for large set size and for age.
    - Effective wm_weight = sigmoid(gate - size_penalty - age_penalty).
    - WM policy: softmax over WM store; precision reduced with set size and age.
    - RL: standard delta rule.

    Returns negative log-likelihood.
    """
    lr, wm_gate0, softmax_beta, meta_gain, beta_wm_base, age_wm_supp = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # baseline, modulated below

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Meta-gate state (logit domain)
        gate = wm_gate0
        # EMA of RL "confidence/success"
        ema_success = 0.5

        # WM precision scales with set size and age
        beta_wm_eff = (beta_wm_base * softmax_beta_wm) / (1.0 + 0.5 * (nS - 3) + 0.8 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Compute dynamic WM weight from gate with size and age suppression
            size_penalty = np.log1p(nS - 3) if nS > 3 else 0.0
            age_penalty = age_group * age_wm_supp
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(gate - size_penalty - age_penalty)))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-driven storage; on errors, partial reset toward uniform to allow relearning.
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                w[s, :] = 0.75 * w[s, :] + 0.25 * w_0[s, :]

            # Meta-learning of the WM gate from RL "success"
            # Define success as whether chosen action had maximal Q in that state
            success = 1.0 if Q_s[a] >= np.max(Q_s) - 1e-12 else 0.0
            ema_success = 0.8 * ema_success + 0.2 * success
            gate += meta_gain * (ema_success - 0.5)

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration (novelty bonus) + WM with inhibitory update on errors.
    Older adults get weaker exploration bonus and noisier WM; larger set sizes reduce WM contribution.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10)
    - bonus_strength: directed exploration bonus weight added as bonus/√N(s,a)
    - gamma_size: exponent controlling how strongly set size reduces WM weight (>0)
    - age_bonus_penalty: scales down exploration bonus for older adults (>=0)

    Mechanism:
    - RL: Q-values receive an additive exploration bonus b/√N(s,a); N counts chosen actions per state-action.
    - WM: on reward, store one-hot; on error, inhibit the chosen action in WM by shifting probability to alternatives.
    - Policy mixture: p_total = wm_weight_eff * p_wm + (1-wm_weight_eff) * p_rl, with
      wm_weight_eff = wm_weight * (3/nS)^gamma_size reduced further by age.

    Returns negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, bonus_strength, gamma_size, age_bonus_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for exploration bonus
        N = 1.0 * np.ones((nS, nA))  # start at 1 to avoid div by zero

        # Effective WM weight reduced by set size (capacity-like) and age
        size_factor = (3.0 / float(nS)) ** max(gamma_size, 0.0)
        wm_weight_eff_base = np.clip(wm_weight * size_factor, 0.0, 1.0)
        wm_weight_age = wm_weight_eff_base / (1.0 + 0.5 * age_group)

        # Exploration bonus adjusted by age
        bonus_eff = bonus_strength / (1.0 + age_group * max(age_bonus_penalty, 0.0))

        # WM precision reduced with age and size
        beta_wm_eff = softmax_beta_wm / (1.0 + 0.4 * (nS - 3) + 0.6 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with directed exploration bonus
            bonus = bonus_eff / np.sqrt(N[s, :])
            Q_aug = q[s, :] + bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight_age * p_wm + (1.0 - wm_weight_age) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # update counts after observing action
            N[s, a] += 1.0

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                # Store certainty on the rewarded action
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Inhibitory WM update: penalize the chosen action and redistribute to others
                inhibit = 0.5  # fixed strength; implicitly interacts with beta_wm_eff and wm_weight_age
                w_row = w[s, :].copy()
                # take from chosen and give to others proportionally to their current mass
                take = inhibit * w_row[a]
                w_row[a] -= take
                redistribute = take / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w_row[aa] += redistribute
                w[s, :] = w_row / np.sum(w_row)

        total_log_p += log_p

    return -total_log_p