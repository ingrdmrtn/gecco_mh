def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted RL–WM arbitration with capacity slots and WM leak.

    Idea:
    - RL learns action values (Q) with a single learning rate and softmax choice.
    - WM stores a near-deterministic action template per state, but decays (leaks) toward uniform.
    - Arbitration weight for WM is dynamic: a logistic function of (WM capacity per state – RL entropy).
      WM capacity depends on set size via a slot model and is reduced/boosted by age.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is 0 if <= 45, else 1.
    model_parameters : list/array
        [lr, softmax_beta, kappa, wm_slots_base, slots_old_delta, wm_leak]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - kappa: steepness of arbitration sigmoid (higher -> sharper gating)
        - wm_slots_base: baseline WM slot capacity (approximate number of items)
        - slots_old_delta: change in capacity slots for older adults (often negative)
        - wm_leak: per-trial WM decay toward uniform (0..1), applied to the current state

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, kappa, wm_slots_base, slots_old_delta, wm_leak = model_parameters
    softmax_beta *= 10.0  # higher dynamic range
    softmax_beta_wm = 50.0  # nearly deterministic WM
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity (slots) adjusted by age, then normalized by set size
        slots = wm_slots_base + age_group * slots_old_delta
        slots = max(0.0, slots)
        wm_capacity_per_state = np.clip(slots / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL softmax probability of chosen action (using relative form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Also compute full RL distribution to estimate entropy
            Qc = Q_s - np.max(Q_s)
            rl_exp = np.exp(softmax_beta * Qc)
            p_rl_vec = rl_exp / np.sum(rl_exp)

            # WM policy via near-deterministic softmax over w
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            wm_exp = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = p_wm_vec[a]

            # Arbitration weight: logistic of (WM capacity - RL entropy)
            H_rl = -np.sum(p_rl_vec * (np.log(p_rl_vec + 1e-12))) / np.log(nA)  # normalized [0,1]
            wm_weight = 1.0 / (1.0 + np.exp(-kappa * (wm_capacity_per_state - H_rl)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform on the visited state, then overwrite on reward
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM retrieval failure affected by set size and age.

    Idea:
    - RL uses separate learning rates for positive vs. negative outcomes.
    - WM stores the last rewarded action per state (1-hot). Choice from WM can fail (retrieval failure),
      producing a uniform policy instead. Failure increases with set size and more so for older adults.
    - Arbitration uses a fixed WM weight, while WM reliability changes via retrieval failure.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is 0 if <= 45, else 1.
    model_parameters : list/array
        [alpha_pos, alpha_neg, softmax_beta, wm_weight_base, fail_base, fail_old_boost]
        - alpha_pos: RL learning rate for reward=1
        - alpha_neg: RL learning rate for reward=0
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_weight_base: fixed arbitration weight of WM (0..1)
        - fail_base: baseline WM retrieval failure probability at set size 3
        - fail_old_boost: multiplicative boost of failure for older adults (>=0)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, fail_base, fail_old_boost = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Failure probability increases with set size and is higher for older adults
        size_factor = nS / 3.0  # 1 for set size 3, 2 for set size 6
        fail = fail_base * size_factor * (1.0 + age_group * fail_old_boost)
        fail = np.clip(fail, 0.0, 1.0)

        wm_weight = np.clip(wm_weight_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Full RL vector (not strictly needed here, but kept for completeness/debug)
            Qc = Q_s - np.max(Q_s)
            rl_exp = np.exp(softmax_beta * Qc)
            p_rl_vec = rl_exp / np.sum(rl_exp)

            # WM: one-hot if a rewarded action is stored; otherwise uniform.
            # Retrieval can fail with probability 'fail', yielding uniform choice from WM.
            stored_a = int(np.argmax(w[s, :]))
            has_memory = (np.max(w[s, :]) > (1.0 / nA))
            if has_memory:
                onehot = np.zeros(nA)
                onehot[stored_a] = 1.0
            else:
                onehot = (1.0 / nA) * np.ones(nA)

            p_wm_vec = (1.0 - fail) * onehot + fail * (1.0 / nA) * np.ones(nA)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            alpha = alpha_pos if r > 0.0 else alpha_neg
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: store last rewarded action; on error, clear if the stored mapping was wrong
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If the currently stored action produced 0, clear it (back to uniform)
                if has_memory and stored_a == a:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size- and age-dependent Q-value decay + Bayesian WM (Dirichlet) and precision-weighted arbitration.

    Idea:
    - RL learns with a single learning rate but Q-values decay toward uniform each trial, more strongly
      for larger set sizes and in older adults.
    - WM is a Bayesian associative store: per state, maintain Dirichlet counts over actions updated by rewards.
      WM policy is the Dirichlet mean; precision grows with evidence.
    - Arbitration weight is proportional to WM precision relative to an RL "complexity" term derived from decay.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age. Age group is 0 if <= 45, else 1.
    model_parameters : list/array
        [lr, softmax_beta, q_decay_base, decay_old_mult, wm_concentration0]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - q_decay_base: base Q-value decay rate at set size 3
        - decay_old_mult: multiplicative factor (>0) scaling decay in older adults
        - wm_concentration0: prior Dirichlet concentration per action (>=0)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, q_decay_base, decay_old_mult, wm_concentration0 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will not be used as a 1-hot; placeholder for template structure
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM Dirichlet counts per state-action
        alpha_counts = np.full((nS, nA), wm_concentration0, dtype=float)

        # Q-value decay increases with set size and is larger in older adults
        size_factor = nS / 3.0
        decay = q_decay_base * size_factor * (1.0 + age_group * decay_old_mult)
        decay = np.clip(decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice probability (relative form)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: Dirichlet mean
            alpha_s = alpha_counts[s, :]
            wm_mean = alpha_s / np.sum(alpha_s)
            # For numerical symmetry with template, map to softmax with high beta
            Wc = wm_mean - np.max(wm_mean)
            wm_exp = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = p_wm_vec[a]

            # Precision-weighted arbitration
            wm_precision = max(0.0, np.sum(alpha_s) - nA * wm_concentration0)  # evidence above prior
            rl_complexity = 1.0 + decay * size_factor  # larger when decay and set size are higher
            wm_weight = wm_precision / (wm_precision + rl_complexity)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply decay on the visited state
            q[s, :] = (1.0 - decay) * q[s, :] + decay * (1.0 / nA) * np.ones(nA)

            # WM Dirichlet update: reward provides evidence for the chosen action
            if r > 0.0:
                alpha_counts[s, a] += 1.0
            # No penalty for errors; evidence accumulates only for successes

        blocks_log_p += log_p

    return -blocks_log_p