def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Win-Stay/Lose-Shift (WSLS) working memory with age- and set-size dependent mixture.
    - RL learns Q-values with a single learning rate.
    - WM implements a simple WSLS heuristic per state:
        - After reward: deterministically stores the rewarded action (win-stay).
        - After non-reward: suppresses the last chosen action by a factor (lose-shift), approaching uniform otherwise.
    - Mixture weight for WM decreases with larger set size and for older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (block constant).
    age : array-like of float
        Participant's age (same value repeated). Used to set age group (0 younger, 1 older).
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, phi_stay, lose_suppress]
        - alpha: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled internally by 10)
        - wm_weight_base: baseline WM/RL mixture weight (0..1)
        - phi_stay: strength of win-stay memory (0..1). 1.0 yields one-hot storage after reward.
        - lose_suppress: degree to down-weight last action after a loss (0..1). 1.0 = fully suppress.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, phi_stay, lose_suppress = model_parameters
    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0  # per template
    
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture scaling: lower WM weight for larger set and older age
        size_scale = 3.0 / float(nS)  # 1.0 for 3, 0.5 for 6
        age_scale = 0.8 if age_group == 1 else 1.0
        wm_weight_block = np.clip(wm_weight_base * size_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update (WSLS)
            if r > 0.5:
                # Win-stay: move toward one-hot on chosen action with strength phi_stay
                w[s, :] = (1.0 - phi_stay) * w_0[s, :]
                w[s, a] += phi_stay  # ensures sum near 1 and peaks at a
            else:
                # Lose-shift: suppress chosen action and renormalize toward uniform
                suppress = np.clip(lose_suppress, 0.0, 1.0)
                # Move toward uniform first (prevents degenerate zeros), then down-weight last action
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]
                # Apply suppression
                total_other = np.sum(w[s, :]) - w[s, a]
                w[s, a] *= (1.0 - suppress)
                # Renormalize to keep probabilities well-formed
                norm = np.sum(w[s, :])
                if norm > 0:
                    w[s, :] /= norm
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Count-based WM with entropy-gated arbitration.
    - RL: single learning rate and softmax policy.
    - WM: per-state action counts emphasize rewarded actions; counts decay with lambda_wm.
      The WM policy is derived via softmax over normalized counts.
    - Arbitration: trial-wise WM weight is a logistic function of the entropy difference
      between RL and WM policies (higher WM weight when WM is sharper than RL).
      Older adults and larger set sizes bias arbitration toward RL (lower WM bias).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (block constant).
    age : array-like of float
        Participant's age (same value repeated). Used to set age group (0 younger, 1 older).
    model_parameters : list or array
        [alpha, softmax_beta, wm_bias, kappa_arbitration, lambda_wm]
        - alpha: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled internally by 10)
        - wm_bias: baseline bias toward WM in arbitration (logit space)
        - kappa_arbitration: sensitivity to (H_rl - H_wm)
        - lambda_wm: WM decay rate for counts toward uniform prior (0..1); higher = more decay

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_bias, kappa_arbitration, lambda_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM as counts with a weak symmetric prior (1 per action)
        wm_counts = np.ones((nS, nA))  # Dirichlet-like counts
        w = (wm_counts / np.sum(wm_counts, axis=1, keepdims=True)).copy()
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age and set-size modulate arbitration bias (toward RL when older or larger set)
        size_penalty = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        bias_age = -0.8 if age_group == 1 else 0.0  # shift logit toward RL for older adults
        bias_size = -0.6 * size_penalty
        wm_bias_block = wm_bias + bias_age + bias_size

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl_full = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            p_rl_full = p_rl_full / np.sum(p_rl_full)
            p_rl = p_rl_full[a]  # probability of chosen action a

            # WM policy from counts
            W_s_base = wm_counts[s, :] / np.sum(wm_counts[s, :])
            # map to 'w' slot for compatibility with template policy computation
            w[s, :] = W_s_base
            W_s = w[s, :]
            p_wm_full = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_wm_full = p_wm_full / np.sum(p_wm_full)
            p_wm = p_wm_full[a]

            # Entropy-gated arbitration
            # Entropy in nats
            H_rl = -np.sum(p_rl_full * np.log(np.clip(p_rl_full, 1e-12, 1.0)))
            H_wm = -np.sum(p_wm_full * np.log(np.clip(p_wm_full, 1e-12, 1.0)))
            # Larger (H_rl - H_wm) -> more WM weight
            logit_w = wm_bias_block + kappa_arbitration * (H_rl - H_wm)
            wm_weight_t = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM counts decay and update
            # Decay counts toward symmetric prior (1 per action)
            wm_counts[s, :] = (1.0 - lambda_wm) * wm_counts[s, :] + lambda_wm * np.ones(nA)
            # Rewarded actions incremented more
            if r > 0.5:
                wm_counts[s, a] += 1.0
            else:
                # slight penalty to the chosen action on loss to encourage lose-shift in WM
                wm_counts[s, a] = max(1.0, wm_counts[s, a] - 0.25)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-modulated learning + episodic recall WM.
    - RL: base learning rate scaled by unsigned prediction error (surprise).
    - WM: episodic cache that stores the last rewarded action for each state (deterministic mapping).
           Recall occurs with a logistic probability that depends on set size and age group.
           WM entries forget toward uniform with probability f_forget per trial (passive decay).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size on each trial (block constant).
    age : array-like of float
        Participant's age (same value repeated). Used to set age group (0 younger, 1 older).
    model_parameters : list or array
        [alpha_base, softmax_beta, wm_theta0, wm_theta_size, wm_theta_age, f_forget]
        - alpha_base: base RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (scaled internally by 10)
        - wm_theta0: intercept for WM recall probability (logit scale)
        - wm_theta_size: sensitivity of WM recall to set size factor (logit scale; negative -> less recall in larger sets)
        - wm_theta_age: additional logit penalty for older adults (negative reduces recall)
        - f_forget: WM forgetting probability per trial toward uniform (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, softmax_beta, wm_theta0, wm_theta_size, wm_theta_age, f_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM cache: start uniform; when a rewarded mapping is learned, set to one-hot
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size factor for recall probability: 1 for 3, 0.5 for 6
        size_factor = 3.0 / float(nS)
        # Logit for recall probability includes age penalty
        recall_logit = wm_theta0 + wm_theta_size * size_factor + wm_theta_age * age_group

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM recall probability (trial-constant within block)
            p_recall = 1.0 / (1.0 + np.exp(-recall_logit))
            p_recall = np.clip(p_recall, 0.0, 1.0)

            # WM policy from cache
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture: rely on WM only on recall; otherwise RL
            p_total = p_recall * p_wm + (1.0 - p_recall) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with surprise-modulated learning rate
            delta = r - Q_s[a]
            alpha_eff = np.clip(alpha_base * np.abs(delta), 0.0, 1.0)
            q[s, a] += alpha_eff * delta

            # WM forgetting: probabilistic decay toward uniform
            if np.random.rand() < f_forget:
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

            # WM update: store deterministic mapping when rewarded
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p