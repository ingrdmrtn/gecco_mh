def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with interference and probabilistic gating.
    
    Core idea:
    - RL learns Q-values with a softmax decision rule.
    - WM stores one-hot correct action after rewarded trials.
    - WM contribution is limited by a capacity parameter K, scaled by set size (3 vs 6).
    - Interference increases with set size when nS > K, decaying WM entries toward uniform.
    - Older adults have reduced WM gating probability (age penalty).

    Parameters
    ----------
    states : array-like of int
        State index on each trial within its block.
    actions : array-like of int
        Chosen action (0..2) per trial.
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6); constant within a block.
    age : array-like of int
        Participant age (same value repeated). Age group=0 if <=45 else 1.
    model_parameters : list or array
        [lr, beta_rl, K_capacity, gate_bias, interference_rate, age_gate_penalty]
        - lr: RL learning rate (0..1).
        - beta_rl: RL inverse temperature (scaled by 10 internally).
        - K_capacity: WM capacity in items (0..6).
        - gate_bias: baseline probability (logit) to use WM (before capacity scaling).
        - interference_rate: per-trial decay of WM toward uniform when nS > K (0..1).
        - age_gate_penalty: reduction of WM gating in older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl, K_capacity, gate_bias, interference_rate, age_gate_penalty = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture weight: logistic gating scaled by capacity relative to set size
        # Gating is reduced by age, and upper bounded by capacity ratio.
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))

        cap_ratio = np.clip(K_capacity / max(1.0, nS), 0.0, 1.0)
        base_gate = sigmoid(gate_bias - age_gate_penalty * age_group)
        wm_weight_block = np.clip(base_gate * cap_ratio, 0.0, 1.0)

        # Interference toward uniform grows when nS > K
        overload = max(0.0, (nS - K_capacity) / max(1.0, nS))
        decay_rate = np.clip(interference_rate * overload, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Qs = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Qs - Qs[a])))

            Ws = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (Ws - Ws[a])))

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM interference decay toward uniform
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            # WM storage (gate via reward success)
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with confidence-weighted arbitration via unsigned prediction error (PE).

    Core idea:
    - RL learns Q-values and provides a softmax policy.
    - WM stores rewarded associations and decays toward uniform at a fixed rate.
    - Arbitration favors WM when RL is confident (low unsigned PE), and reduces WM reliance
      with higher set size and in older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within its block.
    actions : array-like of int
        Chosen action (0..2) per trial.
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6); constant within a block.
    age : array-like of int
        Participant age (same value repeated). Age group=0 if <=45 else 1.
    model_parameters : list or array
        [lr, beta_rl, wm_base_logit, pe_sensitivity, wm_decay, age_pe_shift]
        - lr: RL learning rate (0..1).
        - beta_rl: RL inverse temperature (scaled by 10 internally).
        - wm_base_logit: baseline WM weight (in logit units).
        - pe_sensitivity: scales influence of (1 - |PE|) on WM arbitration (>=0).
        - wm_decay: per-trial decay of WM toward uniform (0..1).
        - age_pe_shift: reduces PE sensitivity in older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_base_logit, pe_sensitivity, wm_decay, age_pe_shift = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load penalization term reduces WM weight at larger set sizes
        load_penalty = np.log(max(1.0, nS) / 3.0)  # 0 for nS=3, ln(2) for nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Qs = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Qs - Qs[a])))

            Ws = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (Ws - Ws[a])))

            # Unsigned PE from RL
            pe = abs(r - Qs[a])
            # Confidence = 1 - |PE|, higher confidence => more WM reliance
            pe_eff = (pe_sensitivity - age_pe_shift * age_group) * (1.0 - pe)
            wm_logit = wm_base_logit + pe_eff - load_penalty
            wm_weight = np.clip(sigmoid(wm_logit), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM decay and storage on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and load-dependent lapses.

    Core idea:
    - RL softmax policy mixed with WM policy.
    - WM strength decreases with set size and more so in older adults.
    - A separate lapse process chooses uniformly at random with probability epsilon,
      which increases with cognitive load and in older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within its block.
    actions : array-like of int
        Chosen action (0..2) per trial.
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6); constant within a block.
    age : array-like of int
        Participant age (same value repeated). Age group=0 if <=45 else 1.
    model_parameters : list or array
        [lr, beta_rl, wm_weight_base, wm_decay, lapse_base, load_age_gain]
        - lr: RL learning rate (0..1).
        - beta_rl: RL inverse temperature (scaled by 10 internally).
        - wm_weight_base: baseline WM mixture weight at set size 3 for young adults (0..1).
        - wm_decay: per-trial decay of WM toward uniform (0..1).
        - lapse_base: baseline logit of lapse probability.
        - load_age_gain: additive gain on lapse logit per unit load and per age group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_weight_base, wm_decay, lapse_base, load_age_gain = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight declines with load and age
        load_scale = 3.0 / max(1.0, nS)
        wm_weight_block = np.clip(wm_weight_base * load_scale * (1.0 - 0.5 * age_group), 0.0, 1.0)

        # Lapse increases with load and in older age
        load_level = (nS - 3.0) / 3.0  # 0 for 3, 1 for 6
        lapse_logit = lapse_base + load_age_gain * (load_level + age_group)
        epsilon = np.clip(sigmoid(lapse_logit), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Qs = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Qs - Qs[a])))

            Ws = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (Ws - Ws[a])))

            mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - epsilon) * mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM decay and storage
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        total_log_p += log_p

    return -total_log_p

Notes on parameter effects by set size and age group:
- Model 1: WM weight is scaled by capacity relative to set size (K/nS). Older adults have reduced gating (age_gate_penalty), and interference grows when nS > K.
- Model 2: WM arbitration increases when RL is confident (low unsigned PE). Larger set sizes penalize WM via load_penalty, and older adults have reduced sensitivity to PE (age_pe_shift).
- Model 3: WM weight decreases with load and in older adults; lapse probability increases with load and age (load_age_gain).