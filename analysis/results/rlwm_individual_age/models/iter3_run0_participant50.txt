def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-gated WM engagement and precision modulated by set size and age.

    Idea
    - Choices mix a model-free RL policy and a WM policy.
    - WM precision (and thus its policy determinism) declines with larger set sizes and with age.
    - On each trial, the mixture weight is dynamically gated by RL uncertainty (higher entropy -> more WM reliance).
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_prec_base, ss_prec_drop, age_prec_drop, uncertainty_gate]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_prec_base: baseline WM precision controlling both WM determinism and engagement.
        - ss_prec_drop: reduction of WM precision as set size increases (>=0).
        - age_prec_drop: additional precision reduction for older group (>=0).
        - uncertainty_gate: weight of RL uncertainty in increasing WM reliance (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_prec_base, ss_prec_drop, age_prec_drop, uncertainty_gate = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0

        # WM precision baseline adjusted by set size and age
        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6
        wm_precision = wm_prec_base - ss_prec_drop * ss_penalty - age_group * abs(age_prec_drop)

        # Map precision to WM inverse temperature and decay
        # Use a sigmoid mapping to keep values in a reasonable range
        sig = 1.0 / (1.0 + np.exp(-wm_precision))
        softmax_beta_wm = 1.0 + 99.0 * sig  # between ~1 and 100
        wm_decay = np.clip(1.0 - sig, 0.0, 1.0)  # higher precision -> less decay

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # RL uncertainty via normalized entropy (0..1)
            H = -np.sum(pi_rl * np.log(pi_rl + eps))
            H_norm = H / np.log(nA)

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Dynamic mixture weight: more WM when RL is uncertain
            wm_weight_t = np.clip(sig + uncertainty_gate * H_norm, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay towards uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM one-shot storage on correct feedback
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with confidence-bias toward last rewarded action, modulated by set size and age.

    Idea
    - Choices mix a model-free RL policy with a WM policy.
    - A state-specific confidence bias increases the propensity to repeat the last rewarded action in that state.
    - The bias grows with larger set sizes and is stronger in older adults (possible strategic simplification).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr_pos, beta_base, wm_weight_base, conf_bias_base, ss_bias_boost, age_bias_boost]
        - lr_pos: RL learning rate after reward (0..1); after no-reward, learning rate is lr_pos/2 (conservative).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_weight_base: baseline WM mixture weight (0..1).
        - conf_bias_base: baseline additive bias toward last rewarded action.
        - ss_bias_boost: additional bias with larger set size (>=0).
        - age_bias_boost: additional bias for older group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, beta_base, wm_weight_base, conf_bias_base, ss_bias_boost, age_bias_boost = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0

        ss_factor = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6
        bias_strength = conf_bias_base + ss_bias_boost * ss_factor + age_group * abs(age_bias_boost)
        bias_strength = float(max(0.0, bias_strength))
        wm_weight = float(np.clip(wm_weight_base, 0.0, 1.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last rewarded action per state; -1 means none stored
        last_rew_action = -1 * np.ones(nS, dtype=int)

        softmax_beta_wm = 50.0  # deterministic WM

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with confidence bias toward last rewarded action in this state
            Q_s = q[s, :]
            pref_rl = softmax_beta * (Q_s - np.max(Q_s))
            if last_rew_action[s] >= 0:
                bias_vec = np.zeros(nA)
                bias_vec[last_rew_action[s]] = bias_strength
                pref_rl = pref_rl + bias_vec
            pi_rl = np.exp(pref_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy (no bias inside WM; WM itself stores last rewarded action)
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (asymmetric: smaller update on negative outcome)
            lr = lr_pos if r > 0.5 else 0.5 * lr_pos
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM store on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
                last_rew_action[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age-reduced reward sensitivity and set-size/age-modulated WM noise.

    Idea
    - RL updates use an effective reward scaled down in older adults (reduced reward sensitivity).
    - WM policy becomes noisier with larger set sizes and even more so in older adults.
    - Mixture weight is fixed but RL and WM relative reliability change via these mechanisms.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_weight_base, reward_drop_age, ss_wm_noise, age_wm_noise]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_weight_base: baseline WM mixture weight (0..1).
        - reward_drop_age: multiplicative reduction of reward in older group (0..1).
        - ss_wm_noise: increase in WM noise when set size is 6 vs 3 (>=0).
        - age_wm_noise: additional WM noise for older group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_weight_base, reward_drop_age, ss_wm_noise, age_wm_noise = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        wm_weight = float(np.clip(wm_weight_base, 0.0, 1.0))

        # WM beta reduced (more noise) with larger set size and for older age
        ss_factor = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6
        noise_penalty = ss_wm_noise * ss_factor + age_group * abs(age_wm_noise)
        softmax_beta_wm = 50.0 * np.exp(-max(0.0, noise_penalty))

        # Effective reward scaling for older adults
        reward_scale = 1.0 - age_group * np.clip(reward_drop_age, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with reduced reward sensitivity in older adults
            r_eff = r * reward_scale
            delta = r_eff - Q_s[a]
            q[s, a] += lr * delta

            # WM: no decay term here; one-shot store on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)