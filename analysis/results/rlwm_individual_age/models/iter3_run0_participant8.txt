def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration and age- and set-size-dependent WM capacity.

    Mechanism
    - RL: tabular Q-learning with softmax choice.
    - WM: per-state cache of last rewarded action with a memory strength m[s] that depends on
          an age-dependent capacity relative to set size and decays over time.
    - Arbitration: trial-by-trial mixture weight derived from relative confidence:
          weight_wm = sigmoid(arb_slope * (conf_wm - conf_rl)),
      where conf_wm = m[s] and conf_rl = max(Q[s]) - mean(Q[s]).

    Parameters
    ----------
    states : array-like
        State indices for each trial (0..set_size-1 within block).
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6, constant within a block).
    age : array-like
        Participant age repeated across trials. Used to derive age group.
    model_parameters : list/tuple
        [lr, beta0, arb_slope, wm_cap_y, wm_cap_o, pe_sens]
        - lr: RL learning rate (0..1)
        - beta0: base inverse temperature for RL (scaled by 10 internally)
        - arb_slope: slope of arbitration sigmoid (>=0)
        - wm_cap_y: WM capacity (in items) for young group
        - wm_cap_o: WM capacity (in items) for old group
        - pe_sens: scaling of reward prediction error into WM strengthening (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta0, arb_slope, wm_cap_y, wm_cap_o, pe_sens = model_parameters
    softmax_beta = beta0 * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # deterministic WM baseline

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))     # RL values
        w = (1.0 / nA) * np.ones((nS, nA))     # WM policy cache
        w_0 = (1.0 / nA) * np.ones((nS, nA))   # Uniform prior for WM
        m_strength = np.zeros(nS)              # WM memory strength per state

        # Capacity-limited decay factor per trial (more decay when set size > capacity)
        wm_cap = wm_cap_y if age_group == 0 else wm_cap_o
        cap_ratio = min(1.0, wm_cap / max(1.0, float(nS)))
        # Translate capacity match into decay rate: higher mismatch -> higher decay
        base_decay = 1.0 - 0.7 * cap_ratio  # in [0.3, 1.0) depending on capacity
        base_decay = np.clip(base_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Passive memory decay each trial for all states
            m_strength = (1.0 - base_decay) * m_strength

            Q_s = q[s, :].copy()

            # RL policy probability of observed action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: one-hot toward cached action with strength m_strength[s]
            cached_a = int(np.argmax(w[s, :]))
            W_s_vec = (1.0 - m_strength[s]) * w_0[s, :] + m_strength[s] * np.eye(nA)[cached_a]
            beta_wm_eff = softmax_beta_wm  # already sharp; strength is in W_s_vec
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s_vec - W_s_vec[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration weight from relative confidence
            conf_rl = float(np.max(Q_s) - np.mean(Q_s))
            conf_wm = float(np.clip(m_strength[s], 0.0, 1.0))
            wm_weight = 1.0 / (1.0 + np.exp(-arb_slope * (conf_wm - conf_rl)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-driven caching and strengthening
            if r > 0.5:
                # Cache the rewarded action with strength boosted by PE magnitude
                w[s, :] = 0.0
                w[s, a] = 1.0
                m_strength[s] = np.clip(m_strength[s] + pe_sens * abs(delta), 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decaying choice bias and WM recency cache; WM weight equals memory strength.
    
    Mechanism
    - RL: tabular Q-learning with softmax temperature modulated by age.
    - Choice bias: a decaying additive bias toward the previously selected action (stickiness),
      with separate gain and decay parameters.
    - WM: per-state last-rewarded action cache with strength s_m(s) = exp(-interf * (nS-1) * time_since_last_reward[s]).
      Arbitration uses s_m(s) directly as the WM mixture weight.

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards.
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials.
    model_parameters : list/tuple
        [lr, beta_base, bias_gain, bias_decay, wm_interf, beta_age_shift]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - bias_gain: strength of choice bias added to the last chosen action (>=0)
        - bias_decay: per-trial decay (0..1) of the choice bias trace
        - wm_interf: WM interference rate per extra item over 1 (>=0)
        - beta_age_shift: fractional reduction of beta for older group (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, bias_gain, bias_decay, wm_interf, beta_age_shift = model_parameters
    age_group = 0 if age[0] <= 45 else 1

    # Age reduces effective beta (more exploration in older)
    softmax_beta = beta_base * 10.0 * (1.0 - beta_age_shift * age_group)
    softmax_beta = max(softmax_beta, 1e-3)  # keep positive
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Choice bias trace per state and action
        bias = np.zeros((nS, nA))

        # WM bookkeeping: time since last reward per state and cached action
        time_since = np.full(nS, 1e6)  # large means "unknown"
        cached = np.zeros(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Increment time since last reward for all states (global decay)
            time_since += 1.0

            # RL values plus decaying choice bias
            Q_s = q[s, :].copy() + bias[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM strength from interference and recency
            s_m = np.exp(-wm_interf * max(0.0, float(nS) - 1.0) * (time_since[s] / max(1.0, float(nS))))
            s_m = float(np.clip(s_m, 0.0, 1.0))

            # WM policy over cached action
            # Maintain a cached action vector in w
            cached_a = int(np.argmax(w[s, :]))
            W_s_vec = (1.0 - s_m) * w_0[s, :] + s_m * np.eye(nA)[cached_a]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_vec - W_s_vec[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: use s_m directly as WM weight
            wm_weight = s_m
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update choice bias: decay then add gain to chosen action
            bias *= (1.0 - bias_decay)
            bias[s, a] += bias_gain

            # WM update: if rewarded, cache and reset recency
            if r > 0.5:
                cached[s] = a
                w[s, :] = 0.0
                w[s, a] = 1.0
                time_since[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learned learning rate (by set size and age) + Dirichlet-like WM with forgetting.

    Mechanism
    - RL: learning rate is dynamically computed from a logistic transform influenced by set size and age:
        lr_eff = sigmoid(lr0 + lr_set*(3/nS - 0.5) + lr_age*age_group)
      so smaller set sizes increase lr, and older age can reduce/increase depending on lr_age.
      Softmax with inverse temperature beta (scaled by 10).
    - WM: for each state, maintain action counts that increment on reward and decay each trial with
      an age- and set-size-dependent forgetting rate. WM policy is the posterior mean of a Dirichlet
      with concentration alpha_wm (symmetric prior). Arbitration weight is the reliability
      w = N / (N + alpha_wm), where N is the sum of decayed counts for the state.

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards.
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated across trials (age_group derived inside).
    model_parameters : list/tuple
        [lr0, lr_set, lr_age, beta, alpha_wm, forget0]
        - lr0: base learning-rate logit
        - lr_set: set-size modulation of lr on the logit scale
        - lr_age: age-group modulation of lr on the logit scale
        - beta: RL inverse temperature (scaled by 10 internally)
        - alpha_wm: symmetric prior strength for WM Dirichlet (>=0)
        - forget0: base forgetting rate per trial for WM counts (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr0, lr_set, lr_age, beta, alpha_wm, forget0 = model_parameters
    softmax_beta = beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective learning rate depends on set size and age
        lr_logit = lr0 + lr_set * (3.0 / float(nS) - 0.5) + lr_age * age_group
        lr_eff = 1.0 / (1.0 + np.exp(-lr_logit))
        lr_eff = float(np.clip(lr_eff, 1e-4, 1.0))

        # WM forgetting dependent on set size and age
        forget = forget0 * (float(nS) / 3.0) * (1.0 + 0.5 * age_group)
        forget = float(np.clip(forget, 0.0, 1.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))    # will hold the normalized WM probabilities
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Dirichlet-like counts per state-action for WM
        counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Decay counts each trial (forgetting)
            counts *= (1.0 - forget)

            # RL policy
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM posterior mean and reliability
            N_s = float(np.sum(counts[s, :]))
            wm_prob = (counts[s, :] + alpha_wm / nA) / max(N_s + alpha_wm, 1e-12)
            # Convert to softmax policy for consistency with template
            denom_wm = np.sum(np.exp(softmax_beta_wm * (wm_prob - wm_prob[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration weight from reliability N/(N+alpha)
            wm_weight = N_s / max(N_s + alpha_wm, 1e-12)
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with lr_eff
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta

            # WM update: increment counts when reward arrives
            if r > 0.5:
                counts[s, a] += 1.0
                # keep w as normalized current posterior snapshot (not required for p_wm but keeps template variables meaningful)
                w[s, :] = (counts[s, :] + alpha_wm / nA) / max(np.sum(counts[s, :]) + alpha_wm, 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p