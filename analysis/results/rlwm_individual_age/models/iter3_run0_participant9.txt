def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce–Hall attention and reward-gated Working Memory (WM) with set-size interference.

    Idea
    - RL learning rate adapts with surprise (absolute prediction error) per Pearce–Hall.
    - WM is a near one-shot store upon reward, but decays/interferes toward uniform as set size increases.
    - Younger participants rely relatively more on WM than older participants (age modulates WM mixture).

    Parameters (5)
    - alpha_base: base RL learning rate (0..1)
    - k_attn: surprise gain for RL learning rate (>=0). Effective lr = clip(alpha_base + k_attn*|PE|, 0..1)
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - wm_weight_base: base WM mixture weight (0..1). Age-scaled: young=1.0x, old=0.7x
    - wm_interf: WM interference strength per extra item beyond 3 (>=0). Decay rate = 1 - wm_interf*((nS-3)/3)

    Inputs
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size for the current block on each trial
    - age: array with a single value repeated; <=45 => young (0), >45 => old (1)
    - model_parameters: list of 5 parameters as above

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_base, k_attn, softmax_beta, wm_weight_base, wm_interf = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # highly deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-scaled WM mixture; also scale by load (3/nS)
        wm_mix = max(0.0, min(1.0, wm_weight_base)) * (3.0 / float(nS))
        if age_group == 1:
            wm_mix *= 0.7  # older: reduced reliance on WM

        # Set-size dependent decay (interference toward uniform)
        interf = max(0.0, wm_interf) * ((float(nS) - 3.0) / 3.0)
        decay = max(0.0, min(1.0, 1.0 - interf))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (softmax)
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy (softmax over current WM values)
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce–Hall attention
            pe = r - Q_s[a]
            alpha_eff = alpha_base + k_attn * abs(pe)
            alpha_eff = max(0.0, min(1.0, alpha_eff))
            q[s, a] += alpha_eff * pe

            # WM decay/interference toward uniform
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            # Reward-gated WM storage: if rewarded, set one-hot memory for (s,a)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Confidence-based arbitration and WM decay, with age-dependent lapse.

    Idea
    - RL uses a single learning rate.
    - Arbitration dynamically shifts toward WM after recent success and toward RL after recent failure.
      Implemented via a running confidence trace per state updated by reward prediction outcomes.
    - WM suffers exponential decay with a global decay parameter that is applied each trial and interacts
      with set size (3/nS scales the WM weight).
    - Older participants have higher choice lapse.

    Parameters (6)
    - alpha_rl: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - wm_base: base WM mixture weight (0..1)
    - k_conf: confidence gain controlling how strongly recent outcomes bias toward WM (>=0)
    - lapse_base: base lapse probability added to the final policy (0..0.2 recommended)
    - wm_decay: per-trial WM decay toward uniform (0..1). Higher means faster forgetting.

    Inputs
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size for the current block on each trial
    - age: array with a single value repeated; <=45 => young (0), >45 => old (1)
    - model_parameters: list of 6 parameters as above

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_rl, softmax_beta, wm_base, k_conf, lapse_base, wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Confidence trace per state: initialized neutral (0)
        conf = np.zeros(nS)

        # Base mixture scaled by load
        wm_base_load = max(0.0, min(1.0, wm_base)) * (3.0 / float(nS))

        # Age-dependent lapse
        lapse = max(0.0, lapse_base) * (1.0 + 0.5 * age_group)
        lapse = min(lapse, 0.3)  # cap to ensure numerical stability

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM softmax
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Confidence-based arbitration:
            # conf[s] in (-inf, inf) mapped via sigmoid to [0,1], then combined with base and load
            wm_conf = 1.0 / (1.0 + np.exp(-conf[s]))
            wm_mix = wm_base_load * (0.5 + 0.5 * wm_conf)  # amplify WM when confident, attenuate when not

            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_mix = max(p_mix, 1e-12)

            # Lapse mixture (uniform random) with age-dependent weight
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            log_p += np.log(p_final)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe

            # WM decay toward uniform (applied each trial)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage biased by outcome: stronger push to one-hot if rewarded, weaker if not
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # if not rewarded, slightly suppress the chosen action representation
                w[s, a] = 0.5 * w[s, a] + 0.5 * (1.0 / nA)

            # Confidence update: move toward 1 with positive outcome, toward -1 with negative outcome
            conf[s] += k_conf * (2.0 * r - 1.0)
            # Optional mild leak to avoid saturation
            conf[s] *= 0.98

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with swap (binding) errors that increase with set size and age.

    Idea
    - RL is standard delta rule.
    - WM stores rewarded state-action pairs one-shot, but retrieval may suffer "swap" errors:
      the WM policy for the current state is contaminated by WM contents from other states.
      Swap probability increases with set size and more so for older adults.
    - WM also mildly decays toward uniform.

    Parameters (6)
    - alpha_rl: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - wm_weight: base WM mixture weight (0..1), scaled by load (3/nS)
    - swap_base: base swap probability at nS=6 relative to 3 (>=0)
    - age_gain: multiplicative increase in swap with age (>=0). swap *= (1 + age_gain*age_group)
    - wm_decay: WM decay toward uniform each retrieval/update (0..1)

    Inputs
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size for the current block on each trial
    - age: array with a single value repeated; <=45 => young (0), >45 => old (1)
    - model_parameters: list of 6 parameters as above

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_rl, softmax_beta, wm_weight, swap_base, age_gain, wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM matrix: each state's row stores a distribution over actions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled WM mixture
        wm_mix = max(0.0, min(1.0, wm_weight)) * (3.0 / float(nS))

        # Swap probability scales with set size and age
        size_factor = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for 3, 1 for 6
        swap_p = max(0.0, swap_base) * size_factor
        swap_p *= (1.0 + max(0.0, age_gain) * age_group)
        swap_p = min(swap_p, 0.95)  # cap to avoid degenerate cases

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM retrieval with swap: convex combination of correct state's WM and average of other states' WM
            if nS > 1:
                others = [i for i in range(nS) if i != s]
                w_other_avg = np.mean(w[others, :], axis=0)
            else:
                w_other_avg = w[s, :]

            W_retr = (1.0 - swap_p) * w[s, :] + swap_p * w_other_avg

            W_shift = W_retr - np.max(W_retr)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe

            # WM decay toward uniform for the accessed state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM write: if rewarded, store one-hot; if not, slight soft penalty on chosen
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Move a bit toward uniform but keep some memory
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p