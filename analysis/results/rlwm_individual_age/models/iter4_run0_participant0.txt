Below are three standalone cognitive models that implement different RL-WM mechanisms while respecting the task structure, block resets, set-size effects, and age-group modulation. Each function returns the negative log-likelihood of the observed choices.

Note: Assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with softmax + decaying overwrite WM (set-size and age modulate WM decay).

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM: per-state probability vector over actions that is
      (i) pulled toward uniform by a decay that increases with set size and for older adults,
      (ii) overwritten toward the most recent rewarded action when reward is obtained.
      WM policy is a high-temperature softmax over the WM vector.
    - Mixing: fixed mixture weight between WM and RL.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1]
    - wm_weight: base mixture weight for WM in (-inf, inf); squashed with sigmoid
    - softmax_beta: RL inverse temperature; internally scaled by 10
    - wm_decay_base: base WM decay toward uniform per visit (>=0)
    - wm_overwrite: overwrite strength toward rewarded action on reward in [0,1]
    - age_wm_factor: multiplicative increase in WM decay for older adults (>=0)

    Age and set-size effects
    - Effective WM decay = wm_decay_base * (set_size/3) * (1 + age_group * age_wm_factor)
      so larger set sizes and older age increase decay (noisier WM).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_overwrite, age_wm_factor = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Mixture weight squashed to [0,1]
    wm_weight = 1.0 / (1.0 + np.exp(-wm_weight))

    age_group = 0 if age[0] <= 45 else 1
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM probability vectors start uniform
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL softmax probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM softmax over current WM distribution w[s,:]
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform with set-size- and age-dependent rate
            decay = wm_decay_base * (float(nS) / 3.0) * (1.0 + age_group * age_wm_factor)
            decay = max(0.0, min(decay, 1.0))
            w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)

            # WM overwrite toward rewarded action if reward obtained
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_overwrite) * w[s, :] + wm_overwrite * onehot

            # Renormalize to avoid drift
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with choice stickiness + WM error-driven suppression (set-size and age modulate WM temperature).

    Mechanism
    - RL: tabular Q-learning with softmax; includes a state-dependent choice stickiness bias that
      adds kappa to the last chosen action in that state.
    - WM: maintains a per-state preference vector. Reward increases preference for the chosen action;
      non-reward suppresses preference for the chosen action (error-driven avoidance).
      WM policy is a softmax over WM preferences with an effective temperature that becomes noisier
      with larger set size and for older adults.
    - Mixing: fixed mixture weight between WM and RL.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1]
    - wm_weight: base mixture weight for WM in (-inf, inf); squashed with sigmoid
    - softmax_beta: RL inverse temperature; internally scaled by 10
    - kappa_sticky: choice stickiness weight added to last action in a state (can be >=0)
    - wm_suppress: WM update step size (positive for reward enhancement; negative applied on errors)
    - age_wm_temp_increase: increases WM noise for older adults (>=0)

    Age and set-size effects
    - WM inverse temperature = 50 / (1 + (set_size/3) * (1 + age_group * age_wm_temp_increase))

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa_sticky, wm_suppress, age_wm_temp_increase = model_parameters
    softmax_beta *= 10.0
    wm_weight = 1.0 / (1.0 + np.exp(-wm_weight))

    softmax_beta_wm_base = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM preferences; initialize neutral
        w = np.zeros((nS, nA))
        # Track last action per state for stickiness, initialize to -1 (none)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL with stickiness bias on current state's last action
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += kappa_sticky

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy: softmax over WM preferences with set-size- and age-dependent temperature
            W_s = w[s, :]
            size_noise = (float(nS) / 3.0)
            softmax_beta_wm_eff = softmax_beta_wm_base / (1.0 + size_noise * (1.0 + age_group * age_wm_temp_increase))

            denom_wm = np.sum(np.exp(softmax_beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: error-driven suppression vs reward-driven enhancement
            if r > 0.0:
                w[s, a] += abs(wm_suppress)
            else:
                w[s, a] -= abs(wm_suppress)

            # Optional mild normalization to keep scale bounded
            # Centering: subtract mean to avoid runaway growth
            w[s, :] -= np.mean(w[s, :])

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic WM with familiarity-gated arbitration (set-size and age modulate WM gate).

    Mechanism
    - RL: tabular Q-learning with softmax.
    - Episodic WM: stores the last rewarded action for each state and a confidence value.
      WM policy is a high-temperature softmax on a one-hot vector scaled by confidence; 
      if no rewarded action is known, WM is uniform.
    - Arbitration: WM contributes only when the state's familiarity is below a threshold.
      Familiarity counts visits to a state. The threshold decreases with larger set sizes
      and with older age, reducing WM engagement.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1]
    - wm_weight_base: base WM mixture weight in (-inf, inf); squashed with sigmoid
    - softmax_beta: RL inverse temperature; internally scaled by 10
    - theta_base: base familiarity threshold (>=0)
    - age_theta_shift: reduction in threshold for older adults (>=0)
    - wm_temp_base: base WM inverse temperature scaling the one-hot confidence (>=0)

    Age and set-size effects
    - Effective familiarity threshold: theta_eff = (theta_base / (set_size/3)) - age_theta_shift * age_group
      Larger set sizes and older age reduce the likelihood of WM being engaged.
    - Trial-level WM weight: wm_weight = sigmoid(wm_weight_base) if familiarity <= theta_eff, else 0.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, theta_base, age_theta_shift, wm_temp_base = model_parameters
    softmax_beta *= 10.0
    wm_weight_base = 1.0 / (1.0 + np.exp(-wm_weight_base))
    softmax_beta_wm_high = 50.0  # cap for very confident episodic memory

    age_group = 0 if age[0] <= 45 else 1
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # Episodic WM store: for each state, last rewarded action (-1 if none) and confidence c in [0,1]
        wm_last_rewarded = -1 * np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS)

        # Familiarity counters
        familiarity = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # Compute effective WM gate based on familiarity, set size, and age
            size_scale = max(1e-6, float(nS) / 3.0)
            theta_eff = (theta_base / size_scale) - age_theta_shift * age_group
            theta_eff = max(0.0, theta_eff)

            # Determine WM mixture weight this trial
            use_wm = 1.0 if familiarity[s] <= theta_eff else 0.0
            wm_weight = wm_weight_base * use_wm

            # WM policy from episodic memory
            if wm_last_rewarded[s] >= 0 and wm_conf[s] > 0.0:
                # Create a one-hot preference scaled by confidence
                pref = np.zeros(nA)
                pref[wm_last_rewarded[s]] = 1.0
                # Effective WM inverse temperature scales with confidence
                beta_wm_eff = min(softmax_beta_wm_high, wm_temp_base * (1.0 + 9.0 * wm_conf[s]))
                denom_wm = np.sum(np.exp(beta_wm_eff * (pref - pref[a])))
                p_wm = 1.0 / (denom_wm + eps)
            else:
                # No episodic knowledge -> uniform
                p_wm = 1.0 / nA

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update episodic WM memory on reward
            if r > 0.0:
                wm_last_rewarded[s] = a
                # Increase confidence, capped at 1
                wm_conf[s] = min(1.0, 0.5 + 0.5 * wm_conf[s])
            else:
                # Decay confidence slightly on errors
                wm_conf[s] = max(0.0, 0.9 * wm_conf[s])

            # Update familiarity counter
            familiarity[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p

Notes on parameter impacts by set size and age group:
- Model 1: WM decay increases with set size and age (age_wm_factor).
- Model 2: WM temperature becomes noisier with larger set size and for older adults (age_wm_temp_increase). RL stickiness is independent of age/set size.
- Model 3: WM engagement gate (theta_eff) decreases with larger set sizes and with older age (age_theta_shift); WM temperature scales with WM confidence.