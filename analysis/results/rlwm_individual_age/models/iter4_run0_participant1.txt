def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + graded WM with reward-gated encoding and age/set-size arbitration.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM is an associative table w[s,a] that is updated when rewards are observed.
      It decays toward a uniform prior and sharpens the encoded action when r=1.
    - Arbitration weight for WM is a logistic transform of a base weight penalized
      by set size and age.
    - WM policy is a high-temperature softmax over w[s,:] (deterministic-like).

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = wm_weight_base (real): base logit for WM weight before penalties
    - model_parameters[3] = setsize_penalty (>=0): penalty per extra item beyond 3 for WM weight
    - model_parameters[4] = age_penalty (>=0): penalty applied if age_group==1 to WM weight
    - model_parameters[5] = wm_alpha in [0,1]: WM encoding/decay rate

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen action indices per trial (0..2)
    - rewards: array of rewards per trial (0 or 1)
    - blocks: array of block indices per trial
    - set_sizes: array of set size (3 or 6) for each trial
    - age: array with a single value (participant age)
    - model_parameters: list of parameters (length 6)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, setsize_penalty, age_penalty, wm_alpha = model_parameters

    # Parameter constraints/transformations
    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-8) * 10.0
    setsize_penalty = max(setsize_penalty, 0.0)
    age_penalty = max(age_penalty, 0.0)
    wm_alpha = min(max(wm_alpha, 0.0), 1.0)

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM associative strengths
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # WM prior (uniform)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy: probability of chosen action a
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over WM weights
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration weight: logistic of penalized base
            # logit(wm_weight) = wm_weight_base - setsize_penalty * max(0, nS_t-3) - age_penalty * age_group
            logit_w = wm_weight_base - setsize_penalty * max(0, nS_t - 3) - age_penalty * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-logit_w))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Decay current state's WM row toward uniform prior
            # - If rewarded, move row toward a one-hot on action a
            # - If not rewarded, only decay (no sharpening)
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + episodic WM that stores only rewarded actions,
    with set-size and age reducing WM arbitration weight.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores the most recent rewarded action for each state (if any), yielding a deterministic policy.
    - WM arbitration weight decreases exponentially with set size and is reduced for older adults.

    Parameters (6):
    - model_parameters[0] = lr_pos in [0,1]: RL learning rate for positive PE (r - Q > 0)
    - model_parameters[1] = lr_neg in [0,1]: RL learning rate for negative PE (r - Q < 0)
    - model_parameters[2] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[3] = wm_weight_base (>=0): base WM mixture weight at set size 3, young
    - model_parameters[4] = setsize_gamma (>=0): exponential decay rate of WM weight with set size
    - model_parameters[5] = age_wm_drop (>=0): subtractive drop in WM weight if age_group==1

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age
    - model_parameters

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, setsize_gamma, age_wm_drop = model_parameters

    # Constrain parameters
    lr_pos = min(max(lr_pos, 0.0), 1.0)
    lr_neg = min(max(lr_neg, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-8) * 10.0
    wm_weight_base = max(wm_weight_base, 0.0)
    setsize_gamma = max(setsize_gamma, 0.0)
    age_wm_drop = max(age_wm_drop, 0.0)

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: -1 if none, else the stored rewarded action
        stored = -np.ones(nS, dtype=int)
        # For softmax form, we maintain w table but only use deterministic policy when stored exists
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: deterministic if stored, uniform if not
            if stored[s] >= 0:
                W_s = np.full(nA, -np.inf)
                W_s[stored[s]] = 0.0  # softmax makes chosen prob ~1 for stored action
                # compute chosen prob
                denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = 1.0 / max(denom_wm, eps)
            else:
                p_wm = 1.0 / nA

            # WM weight decays with set size and drops for older adults
            # wm_weight = wm_weight_base * exp(-setsize_gamma * max(0, nS_t-3)) - age_wm_drop*age_group
            wm_weight = wm_weight_base * np.exp(-setsize_gamma * max(0, nS_t - 3)) - age_wm_drop * age_group
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0.0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update: store action if rewarded; else keep previous (forgetting not modeled here)
            # Also keep w consistent with stored for completeness (not used directly for policy)
            if r > 0.0:
                stored[s] = a
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy-based arbitration + capacity-limited, leaky WM.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM is a leaky associative table that is strengthened for the chosen action on every trial,
      with stronger strengthening when reward is obtained.
    - Arbitration weight for WM increases when RL policy is uncertain (high entropy),
      but is limited by an effective capacity term that decreases with set size and with age.
    - WM policy is a high-beta softmax over WM strengths.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = wm_decay in [0,1]: per-trial leak of WM weights toward uniform
    - model_parameters[3] = wm_reward_boost (>=0): extra WM strengthening when r=1
    - model_parameters[4] = capacity_K (>0): nominal WM capacity in items
    - model_parameters[5] = age_wm_bias (>=0): penalty applied to WM weight if age_group==1

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age
    - model_parameters

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_decay, wm_reward_boost, capacity_K, age_wm_bias = model_parameters

    # Constrain parameters
    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-8) * 10.0
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    wm_reward_boost = max(wm_reward_boost, 0.0)
    capacity_K = max(capacity_K, 1e-8)
    age_wm_bias = max(age_wm_bias, 0.0)

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # RL entropy (base-e) for arbitration
            # compute full RL policy for state s
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi = pi / np.sum(pi)
            entropy = -np.sum(pi * np.log(np.maximum(pi, eps)))
            # Normalize entropy to [0,1] by dividing by log(nA)
            entropy_norm = entropy / np.log(nA)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Capacity-limited availability term: min(1, K/nS)
            cap_avail = min(1.0, capacity_K / max(1, nS_t))
            # Age penalty
            cap_avail = max(0.0, cap_avail - age_wm_bias * age_group)
            # Entropy-based gating: more WM when RL is uncertain
            wm_weight = entropy_norm * cap_avail
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leak toward uniform and strengthen chosen action
            # Strengthening is larger when rewarded
            leak = wm_decay
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]
            boost = 1.0 + wm_reward_boost * r
            # Move mass toward chosen action while keeping normalization approximately bounded
            add = np.zeros(nA)
            add[a] = leak * boost
            w[s, :] = w[s, :] + add
            # Renormalize row to sum to 1 to keep it interpretable as a soft distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p