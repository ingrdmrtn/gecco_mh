def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with age-modulated capacity and decay.

    Mechanism:
    - RL: delta-rule update with softmax action selection.
    - WM: for each state, a working-memory map over actions. Rewarded choices overwrite WM
      with a near one-hot code; between trials WM decays toward uniform.
    - Mixture: policy is a convex combination of WM policy (high beta) and RL policy.
      WM weight scales with an effective capacity k_eff relative to set size (nS).
      Age reduces capacity via k_eff = max(0, cap_k - delta_k_age * age_group).

    Parameters (model_parameters):
    - alpha_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; scaled by 10 internally
    - wm_phi: base WM reliance (0..1), further scaled by capacity ratio
    - cap_k: WM capacity in number of items (>=0)
    - delta_k_age: reduction in capacity when age_group==1 (>=0)
    - wm_decay: WM decay/encoding noise (0..1); higher means more leak to uniform and noisier encoding

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_rl, beta_rl, wm_phi, cap_k, delta_k_age, wm_decay = model_parameters
    beta_rl *= 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity after age penalty
        k_eff = max(0.0, cap_k - delta_k_age * (1 if age_group == 1 else 0))
        cap_ratio = min(1.0, k_eff / max(1.0, float(nS)))
        wm_weight = np.clip(wm_phi * cap_ratio, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            wm_logits = beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha_rl * delta

            # WM decay toward uniform each time the state is visited
            w[s, :] = (1.0 - wm_decay) * W_s + wm_decay * (1.0 / nA)

            # WM rewarded overwrite (noisy one-hot)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * one_hot + wm_decay * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM with set-size thresholding and age-dependent lapse.

    Mechanism:
    - RL: separate learning rates for positive and negative prediction errors; softmax policy.
    - WM: stores rewarded state-action as near one-hot and leaks toward uniform.
      WM reliance decreases smoothly as set size exceeds a threshold.
    - Mixture: WM/RL mixture using set-size dependent WM weight.
    - Age effect: additional lapse-to-uniform mixture increases for older adults.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for rewards (0..1)
    - alpha_neg: RL learning rate for non-rewards (0..1)
    - beta_rl: RL inverse temperature; scaled by 10 internally
    - wm_weight0: base WM weight prior to set-size modulation
    - ss_threshold: set-size threshold; larger nS relative to this reduces WM reliance
    - age_lapse: additional lapse probability applied if age_group==1 (0..1)

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_pos, alpha_neg, beta_rl, wm_weight0, ss_threshold, age_lapse = model_parameters
    beta_rl *= 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    lapse = np.clip(age_lapse * (1 if age_group == 1 else 0), 0.0, 1.0)
    nA = 3

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM weight via a smooth threshold (sigmoid)
        # leak_ss ~ 0 for nS << threshold, ~1 for nS >> threshold
        leak_ss = 1.0 / (1.0 + np.exp(-(nS - ss_threshold)))
        wm_weight = np.clip(wm_weight0 * (1.0 - leak_ss), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            wm_logits = beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture WM/RL
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Age-dependent lapse to uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] = Q_s[a] + alpha * pe

            # WM leak toward uniform each visit
            leak = leak_ss  # reuse set-size leakage as WM decay
            w[s, :] = (1.0 - leak) * W_s + leak * (1.0 / nA)
            # Rewarded encoding toward one-hot
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - leak) * one_hot + leak * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay + uncertainty-based arbitration to WM, age penalty on arbitration, and probabilistic WM encoding.

    Mechanism:
    - RL: delta-rule with per-visit forgetting toward uniform (q_decay) and softmax choice.
    - WM: when rewarded, encodes the chosen action as a one-hot with probabilistic strength
      (wm_store_prob). Between rewards, WM is stable for that state (no explicit leak).
    - Arbitration: WM weight increases when RL is confident (low entropy). Weight also
      decreases with larger set sizes via a 3/nS factor. Age reduces arbitration via a penalty
      on the WM weight logit.

    Parameters (model_parameters):
    - alpha_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; scaled by 10 internally
    - wm_store_prob: strength of WM encoding after reward (0..1)
    - arbit_slope: sensitivity that maps (confidence * 3/nS) into WM weight via sigmoid
    - age_penalty: reduction in the WM weight logit when age_group==1 (>=0)
    - q_decay: RL decay toward uniform on each state visit (0..1)

    Inputs:
    - states, actions, rewards: arrays of equal length with per-trial data
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array with a single age value repeated
    Returns:
    - Negative log-likelihood of observed choices under the model
    """
    alpha_rl, beta_rl, wm_store_prob, arbit_slope, age_penalty, q_decay = model_parameters
    beta_rl *= 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    nA = 3
    log_nA = np.log(nA)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        age_pen = age_penalty * (1 if age_group == 1 else 0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy and confidence (1 - normalized entropy)
            Q_s = q[s, :]
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            p_rl_vec = rl_exp / np.sum(rl_exp)
            p_rl = max(p_rl_vec[a], 1e-12)

            entropy = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            conf = 1.0 - (entropy / log_nA)  # 0..1, higher means more confident

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            p_wm_vec = wm_exp / np.sum(wm_exp)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Arbitration: more WM when RL is confident; reduced by set size and age
            # scale confidence by 3/nS so larger sets reduce WM weight
            conf_scaled = conf * (3.0 / float(nS))
            wm_logit = arbit_slope * (conf_scaled) - age_pen
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL decay toward uniform for visited state, then delta update
            q[s, :] = (1.0 - q_decay) * Q_s + q_decay * (1.0 / nA)
            Q_s = q[s, :]  # refresh after decay
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha_rl * pe

            # WM encoding: expected overwrite toward one-hot upon reward
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_store_prob) * W_s + wm_store_prob * one_hot

        blocks_log_p += log_p

    return -blocks_log_p