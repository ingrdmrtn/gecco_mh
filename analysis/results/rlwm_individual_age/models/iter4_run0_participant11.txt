def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic WM encoding with set-size-dependent interference.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM stores a probabilistic policy per state, but encoding is stochastic and harmed by set-size-driven interference.
    - Older age further reduces WM encoding and increases interference.

    Parameters (len=6):
    - lr: RL learning rate in (0,1).
    - wm_weight: base mixture weight for WM in (0,1).
    - softmax_beta: base RL inverse temperature (scaled by 10 internally).
    - enc_prob_base: baseline probability to encode a correct association into WM upon reward.
    - interference_rate: base WM interference/decay rate per trial; increases with set size and age.
    - age_wm_scaler: proportional reduction of WM encoding and increase of interference for older group (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, enc_prob_base, interference_rate, age_wm_scaler = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic when WM is confident
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective mixture weight does not exceed [0,1]; WM influence decreases with larger set sizes and with age
        wm_weight_eff = wm_weight / (1.0 + max(0, nS - 3))
        wm_weight_eff *= (1.0 - 0.3 * age_group)  # older rely less on WM
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # Interference/decay grows with set size and age
        interf_eff = interference_rate * (1.0 + 0.5 * max(0, nS - 3))
        interf_eff *= (1.0 + age_group * age_wm_scaler)
        interf_eff = np.clip(interf_eff, 0.0, 1.0)

        # Encoding probability for rewarded associations, harmed by set size and age
        enc_prob_eff = enc_prob_base * np.exp(-interf_eff * max(0, nS - 3))
        enc_prob_eff *= (1.0 - 0.5 * age_group * age_wm_scaler)
        enc_prob_eff = np.clip(enc_prob_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            prefs = softmax_beta * Q_s
            prefs -= np.max(prefs)
            p_rl_vec = np.exp(prefs)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            wm_prefs = softmax_beta_wm * W_s
            wm_prefs -= np.max(wm_prefs)
            exp_wm = np.exp(wm_prefs)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay/interference toward uniform every trial (set-size and age dependent)
            w = (1.0 - interf_eff) * w + interf_eff * w_0

            # If rewarded, encode with probability enc_prob_eff by moving toward one-hot
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Stochastic encoding magnitude proportional to enc_prob_eff
                w[s, :] = (1.0 - enc_prob_eff) * w[s, :] + enc_prob_eff * target

            # Normalize to maintain valid probabilities
            w_sum = np.sum(w, axis=1, keepdims=True)
            w = w / np.clip(w_sum, 1e-12, None)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM (Dirichlet counts) with set-size and age effects on prior concentration.

    Idea:
    - WM maintains a Dirichlet posterior over action probabilities per state.
    - Larger set sizes and older age reduce the effective prior concentration, yielding higher uncertainty.
    - Choices are a mixture of RL softmax policy and WM posterior mean policy.

    Parameters (len=6):
    - lr: RL learning rate in (0,1).
    - wm_weight: base mixture weight for WM in (0,1).
    - softmax_beta: base RL inverse temperature (scaled by 10 internally).
    - alpha0_base: base Dirichlet concentration per action (>0).
    - setsize_alpha_penalty: nonnegative; how much alpha0 decreases with larger set size.
    - age_alpha_penalty: nonnegative; additional alpha0 decrease for older group.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha0_base, setsize_alpha_penalty, age_alpha_penalty = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # not used directly for WM, but kept for template consistency
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # Dirichlet parameters for each state
        alpha0_eff = alpha0_base - setsize_alpha_penalty * max(0, nS - 3) - age_alpha_penalty * age_group
        alpha0_eff = max(alpha0_eff, 1e-3)
        dirichlet_alpha = alpha0_eff * np.ones((nS, nA))
        # For numerical stability in expectations
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight decreases with set size and age
        wm_weight_eff = wm_weight / (1.0 + 0.5 * max(0, nS - 3))
        wm_weight_eff *= (1.0 - 0.25 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy
            prefs = softmax_beta * Q_s
            prefs -= np.max(prefs)
            p_rl_vec = np.exp(prefs)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy = posterior mean of Dirichlet over actions
            alpha_s = dirichlet_alpha[s, :]
            p_wm_vec = alpha_s / np.sum(alpha_s)
            p_wm = p_wm_vec[a]

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Bayesian WM update: positive outcome increases chosen action concentration;
            # negative outcome slightly increases competitors to capture "anti" evidence.
            if r > 0.5:
                dirichlet_alpha[s, a] += 1.0
            else:
                # distribute a small amount to other actions (not the chosen one)
                incr = 0.2
                for aa in range(nA):
                    if aa != a:
                        dirichlet_alpha[s, aa] += incr

            # Keep a minimal floor to avoid degeneracy
            dirichlet_alpha[s, :] = np.clip(dirichlet_alpha[s, :], 1e-6, None)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + uncertainty-based arbitration with WM noise.

    Idea:
    - WM stores probabilistic action maps W that learn quickly from feedback.
    - Arbitration weight is computed per state from WM reliability (1 - entropy), with a threshold that increases
      with set size and for older age. WM is noisier with larger set size and age.
    - Final choice is a mixture of WM and RL policies with a dynamic WM weight.

    Parameters (len=6):
    - lr: RL learning rate in (0,1).
    - wm_weight: base maximum WM contribution (0,1).
    - softmax_beta: base RL inverse temperature (scaled by 10 internally).
    - wm_noise_base: base mixing with uniform in WM policy (>=0).
    - arb_slope: slope controlling sharpness of arbitration (sigmoid) (>0).
    - age_arb_bias: increases arbitration threshold for older age (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_noise_base, arb_slope, age_arb_bias = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Base WM contribution reduced with set size and age
        wm_weight_block = wm_weight / (1.0 + 0.5 * max(0, nS - 3))
        wm_weight_block *= (1.0 - 0.2 * age_group)
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        # WM noise increases with set size and age (more entropy)
        wm_noise = wm_noise_base * (1.0 + 0.5 * max(0, nS - 3)) * (1.0 + 0.5 * age_group)
        wm_noise = np.clip(wm_noise, 0.0, 1.0)

        # Arbitration threshold increases with set size and age (harder to trust WM)
        base_threshold = 1.0 / (1.0 + max(1, nS))  # smaller when set small, larger when set large
        arb_threshold = base_threshold + age_arb_bias * age_group
        arb_threshold = np.clip(arb_threshold, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            prefs = softmax_beta * Q_s
            prefs -= np.max(prefs)
            p_rl_vec = np.exp(prefs)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute a noisy WM policy by mixing with uniform
            wm_prefs = softmax_beta_wm * W_s
            wm_prefs -= np.max(wm_prefs)
            exp_wm = np.exp(wm_prefs)
            p_wm_vec_clean = exp_wm / np.sum(exp_wm)
            p_wm_vec = (1.0 - wm_noise) * p_wm_vec_clean + wm_noise * (1.0 / nA)
            p_wm = p_wm_vec[a]

            # WM reliability via normalized negative entropy
            entropy = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))
            max_entropy = np.log(nA)
            reliability = 1.0 - entropy / max_entropy  # 0..1

            # Dynamic WM mixture via sigmoid arbitration around threshold
            x = reliability - arb_threshold
            wm_mix = wm_weight_block * (1.0 / (1.0 + np.exp(-arb_slope * x)))
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM fast learning toward rewarded action; mild unlearning when not rewarded
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.6 * w[s, :] + 0.4 * target
            else:
                # Small drift toward uniform when feedback is negative
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Normalize
            w_sum = np.sum(w, axis=1, keepdims=True)
            w = w / np.clip(w_sum, 1e-12, None)

        blocks_log_p += log_p

    return -blocks_log_p