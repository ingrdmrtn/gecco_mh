def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with surprise-gated WM engagement and decay; asymmetric RL learning.

    Idea:
    - Choices are a mixture of an RL policy and a WM policy.
    - WM engagement is dynamically gated by state-action "surprise" |r - Q(s,a)|.
    - WM engagement is reduced for larger set sizes and for older adults.
    - WM traces decay toward uniform and are updated toward one-hot after outcomes.
    - RL learns with asymmetric learning rates for positive vs negative prediction errors.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10 for range
    - wm_gain0: Baseline logit gain of WM mixture (can be negative or positive)
    - wm_decay: Trial-by-trial decay of WM toward uniform (0..1)
    - gate_surprise: Positive slope for surprise-based gating (higher = more WM when surprised)

    Age and set-size use:
    - WM mixture weight = sigmoid(wm_gain0 + gate_surprise*surprise + b_age + b_size),
      where b_age = 0 for young, -0.5 for old; b_size = -0.3 when nS=6, 0 when nS=3.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_gain0, wm_decay, gate_surprise = model_parameters
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0  # WM policy near-deterministic

    age_group = 0 if age[0] <= 45 else 1
    b_age = 0.0 if age_group == 0 else -0.5  # older adults downweight WM reliance

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size bias on WM engagement
        b_size = 0.0 if nS <= 3 else -0.3

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise-gated WM mixture weight
            surprise = abs(r - Q_s[a])
            wm_logit = wm_gain0 + gate_surprise * surprise + b_age + b_size
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] = q[s, a] + lr * pe

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update toward one-hot of chosen action; stronger on reward
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with set-size scaled WM precision and choice stickiness.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM has retrieval noise (temperature) that worsens with larger set size and for older adults.
    - WM mixture weight decreases with set size via a power-law; also reduced with age.
    - RL includes choice stickiness that biases toward repeating the previous action in the block.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - kappa_stick: Choice stickiness added to the chosen action's value on next trial (>=0)
    - wm_weight0: Base WM mixture weight before set-size and age scaling (0..1)
    - wm_noise: Base WM noise (higher = noisier; translates to lower WM inverse-temp)
    - age_wm_drop: Additional WM weight reduction for older adults (>=0)

    Age and set-size use:
    - WM inverse temperature = 1 / (wm_noise * size_factor * age_factor_temp),
      where size_factor = nS/3, age_factor_temp = 1.2 if old else 1.0.
    - WM weight = wm_weight0 * (3/nS)^0.8, then multiplied by (1 - age_wm_drop) if old.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, kappa_stick, wm_weight0, wm_noise, age_wm_drop = model_parameters
    softmax_beta = softmax_beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    age_factor_temp = 1.2 if age_group == 1 else 1.0  # older: noisier WM (lower inverse-temp)
    wm_weight_age_scale = (1.0 - age_wm_drop) if age_group == 1 else 1.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Stickiness: maintain previous action per state
        prev_action = np.full(nS, -1, dtype=int)

        # Set-size scaling for WM
        size_factor = max(1.0, nS / 3.0)  # 1 for 3, 2 for 6
        wm_beta = 1.0 / (wm_noise * size_factor * age_factor_temp)
        wm_beta = np.clip(wm_beta, 0.1, 100.0)

        wm_weight = wm_weight0 * (3.0 / float(nS)) ** 0.8
        wm_weight *= wm_weight_age_scale
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with stickiness bias
            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += kappa_stick

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with set-size and age-scaled precision
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] = q[s, a] + lr * pe

            # Update stickiness memory
            prev_action[s] = a

            # WM slow decay and corrective update
            w = 0.95 * w + 0.05 * w_0
            # Move WM toward one-hot of chosen action proportionally to reward
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - 0.4 * r) * w[s, :] + (0.4 * r) * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility trace combined with episodic one-shot memory (WM cache).

    Idea:
    - RL uses an eligibility trace to propagate recent choices' credit.
    - In parallel, an episodic/WM cache stores the last rewarded action per state.
    - If a cached action exists, the WM policy is near-deterministic for that state.
    - Probability of successfully retrieving and using the cache declines with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - lambda_et: Eligibility trace decay (0..1)
    - epi_prob0: Base probability to use episodic cache when available (0..1)
    - setsize_slope: How strongly set size reduces episodic usage (>=0)
    - age_bias_wm: Additional reduction of episodic usage for older adults (>=0)

    Age and set-size use:
    - Effective episodic usage probability:
      p_epi = epi_prob0 * exp(-setsize_slope*(nS-3)) * (1 - age_bias_wm) if old, else *1.
    - WM cache precision is high (beta_wm=50), representing near-deterministic recall.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, lambda_et, epi_prob0, setsize_slope, age_bias_wm = model_parameters
    softmax_beta = softmax_beta * 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    age_scale = (1.0 - age_bias_wm) if age_group == 1 else 1.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility trace
        w = (1.0 / nA) * np.ones((nS, nA))  # used to form WM policy from cache
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Episodic cache: -1 if none, otherwise last rewarded action index
        cache = -1 * np.ones(nS, dtype=int)

        # Effective probability to use episodic memory when available
        p_epi_base = epi_prob0 * np.exp(-setsize_slope * max(0, nS - 3)) * age_scale
        p_epi_base = float(np.clip(p_epi_base, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy derived from cache
            if cache[s] >= 0:
                # Build a peaked distribution around cached action
                W_s = np.full(nA, 0.0)
                W_s[cache[s]] = 1.0
                p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))
                wm_weight = p_epi_base
            else:
                # No cache; WM policy is uniform
                W_s = w[s, :]
                p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))
                wm_weight = 0.0

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            pe = r - q[s, a]
            # Increment trace for chosen state-action, decay others
            e = lambda_et * e
            e[s, a] += 1.0
            q += lr * pe * e

            # Update episodic cache on rewarded trials
            if r > 0.5:
                cache[s] = a
                # Refresh WM row to reflect cached action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * one_hot
            else:
                # Mild decay toward uniform when no reward
                w = 0.98 * w + 0.02 * w_0

        blocks_log_p += log_p

    return -blocks_log_p