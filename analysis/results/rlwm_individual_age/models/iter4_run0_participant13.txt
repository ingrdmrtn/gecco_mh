Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways. Each function follows the requested signature, uses all parameters meaningfully (max 6), incorporates age group (0=young, 1=old) and set size effects, and returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with lapse noise modulated by set size and age.

    Mechanism
    - Choices are a mixture of an RL policy and a WM policy.
    - A lapse (uniform-random) component increases with set size and can be further
      shifted by age group.
    - WM is near-deterministic one-shot storage on rewarded trials (no decay here).
      WM precision is high (fixed). Mixture weight is constant across blocks.

    Parameters (6)
    - model_parameters:
        0) lr: RL learning rate (0..1)
        1) beta_base: base RL inverse temperature; scaled by 10 internally
        2) wm_weight_base: base WM mixture weight (0..1 after sigmoid)
        3) lapse_base: base lapse propensity (logit space; converted via sigmoid)
        4) size_lapse_scale: how much lapse increases with larger set size (logit scale)
        5) age_lapse_shift: additive lapse shift for older group (logit scale; 0 for young)
    Age and set-size usage
    - lapse = sigmoid(lapse_base + size_lapse_scale*(set_size-3) + age_group*age_lapse_shift)
    - WM precision is fixed high; WM mixture weight = sigmoid(wm_weight_base)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_weight_base, lapse_base, size_lapse_scale, age_lapse_shift = model_parameters
    softmax_beta = 10.0 * beta_base

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM table; one-shot write on reward
        softmax_beta_wm = 50.0  # near-deterministic WM

        # Fixed WM mixture weight
        wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_base))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size and age dependent lapse
            lapse_logit = lapse_base + size_lapse_scale * max(0, nS - 3) + age_group * age_lapse_shift
            lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
            lapse = np.clip(lapse, 0.0, 1.0)

            # RL choice prob of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice prob of chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot write on reward, no change otherwise
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM with set-size-dependent WM decay and age-dependent RL learning rate.

    Mechanism
    - Choices are a mixture of an RL policy and a WM policy (high-precision WM).
    - WM entries decay toward uniform each trial; decay increases with set size.
    - Age group modulates the RL learning rate (older group may learn slower/faster).
    - WM mixture weight is constant; the effective contribution decays because WM contents do.

    Parameters (6)
    - model_parameters:
        0) lr_base: base RL learning rate (0..1, adjusted by age)
        1) beta_base: base RL inverse temperature; scaled by 10 internally
        2) wm_weight_base: base WM mixture weight (0..1 after sigmoid)
        3) wm_decay_base: base WM decay (logit scale; converted via sigmoid to 0..1)
        4) size_decay_scale: how much WM decay increases with larger set size (logit scale)
        5) age_lr_shift: additive shift to RL learning rate for older group (can be negative or positive)
           Effective lr = clip(lr_base + age_group*age_lr_shift, 0..1)

    Age and set-size usage
    - RL learning rate adjusted by age_group.
    - WM decay per trial: decay = sigmoid(wm_decay_base + size_decay_scale*(set_size-3)).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_base, beta_base, wm_weight_base, wm_decay_base, size_decay_scale, age_lr_shift = model_parameters
    softmax_beta = 10.0 * beta_base

    age_group = 0 if age[0] <= 45 else 1

    # Age-dependent RL learning rate
    lr = lr_base + age_group * age_lr_shift
    lr = np.clip(lr, 0.0, 1.0)

    wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_base))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size dependent decay
        decay_logit = wm_decay_base + size_decay_scale * max(0, nS - 3)
        wm_decay = 1.0 / (1.0 + np.exp(-decay_logit))
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        softmax_beta_wm = 50.0  # high precision WM

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: write on reward, otherwise leave content but apply global decay
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Decay all WM rows toward uniform baseline
            w = (1.0 - wm_decay) * w + wm_decay * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + gated WM with probabilistic storage; WM availability drives mixture weight.

    Mechanism
    - RL runs with a standard softmax policy.
    - WM stores a one-hot association for a state only when rewarded AND a gate opens.
      Gate opening probability is modulated by set size and age group.
    - WM precision is controlled by wm_beta.
    - Mixture weight is not a fixed parameter; it is derived online as the WM confidence
      for that state (max entry in the WM row), thus reflecting whether memory is present.

    Parameters (6)
    - model_parameters:
        0) lr: RL learning rate (0..1)
        1) beta_base: base RL inverse temperature; scaled by 10 internally
        2) wm_beta: WM inverse temperature (precision of retrieval)
        3) wm_gate_base: base logit of WM gate opening probability on rewarded trials
        4) size_gate_penalty: increases with larger set sizes; reduces gate opening (logit scale)
        5) age_wm_gate_shift: additive gate shift for older group (logit scale; 0 for young)

    Age and set-size usage
    - Gate logit = wm_gate_base - size_gate_penalty*(set_size-3) + age_group*age_wm_gate_shift
    - WM mixture weight on each trial is computed as confidence = max(w[s,:]) (in [1/3, 1]),
      so when no memory is stored (uniform), WM contributes little; when stored, it dominates.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_beta, wm_gate_base, size_gate_penalty, age_wm_gate_shift = model_parameters
    softmax_beta = 10.0 * beta_base
    softmax_beta_wm = max(1e-6, wm_beta)

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Gate probability depends on set size and age
        gate_logit = wm_gate_base - size_gate_penalty * max(0, nS - 3) + age_group * age_wm_gate_shift
        gate_p = 1.0 / (1.0 + np.exp(-gate_logit))
        gate_p = np.clip(gate_p, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM store; starts uniform

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise WM weight from confidence (max entry of WM row)
            wm_weight = float(np.max(W_s))
            wm_weight = np.clip(wm_weight, 1.0 / nA, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM gating and update: on reward, store with probability gate_p
            if r > 0.0:
                # Use gate probability by blending expected update (deterministic approximation)
                # Expected WM after gating = gate_p * one_hot + (1 - gate_p) * current
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = gate_p * one_hot + (1.0 - gate_p) * w[s, :]

        blocks_log_p += log_p

    return -blocks_log_p