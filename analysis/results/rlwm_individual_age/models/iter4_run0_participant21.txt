def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with surprise-driven associability and WM decay.

    Mechanisms:
    - RL: tabular Q-learning with dynamic associability (Pearce-Hall style). An associability term increases with absolute PE and decays toward a baseline each visit.
    - WM: per-state cache storing the most recent rewarded action deterministically; memory traces decay toward uniform each time the state is visited.
    - Policy: mixture of WM and RL policies.
    - Set-size effect: WM reliance is down-weighted by 3/nS (lower under higher load).
    - Age effect: older adults show faster WM decay and weaker WM reliance; associability decays faster.

    Parameters (list of 6):
    - model_parameters[0] = alpha0 in (0,1): baseline RL learning-rate scale.
    - model_parameters[1] = beta (>=0): RL inverse temperature (scaled by 10 internally).
    - model_parameters[2] = wm_w0 (real): baseline WM mixture weight before set-size scaling (logit transformed).
    - model_parameters[3] = phi in (0,1): associability learning rate (how fast associability tracks |PE|).
    - model_parameters[4] = eta in (0,1): associability decay toward baseline each visit.
    - model_parameters[5] = wm_decay in (0,1): per-visit decay of WM traces toward uniform.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eps = 1e-12

    alpha0, beta, wm_w0, phi, eta, wm_decay = model_parameters
    # squash to valid ranges
    alpha0 = 1.0 / (1.0 + np.exp(-alpha0))
    phi = 1.0 / (1.0 + np.exp(-phi))
    eta = 1.0 / (1.0 + np.exp(-eta))
    wm_decay = 1.0 / (1.0 + np.exp(-wm_decay))
    softmax_beta = abs(beta) * 10.0
    softmax_beta_wm = 50.0

    # age group: 0 young, 1 old
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        sel = (blocks == b)
        block_actions = actions[sel]
        block_rewards = rewards[sel]
        block_states = states[sel]
        nA = 3
        nS = int(set_sizes[sel][0])

        # Initialize RL Q-values and WM cache
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Associability per state-action, start moderate
        assoc = 0.5 * np.ones((nS, nA))

        # WM weight adjusted by set size and age
        wm_weight_base = 1.0 / (1.0 + np.exp(-wm_w0))
        wm_weight_block = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)

        # Age effects: older -> more WM decay and lower WM reliance; faster assoc decay
        if age_group == 1:
            wm_weight_block *= 0.8
            wm_decay_eff = np.clip(wm_decay * 1.25, 0.0, 1.0)
            eta_eff = np.clip(eta * 1.25, 0.0, 1.0)
        else:
            wm_decay_eff = wm_decay
            eta_eff = eta

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Policies
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with associability-modulated learning rate
            pe = r - q[s, a]
            alpha_sa = alpha0 * assoc[s, a]
            q[s, a] += alpha_sa * pe

            # Update associability toward |PE|
            assoc[s, a] = (1.0 - eta_eff) * assoc[s, a] + eta_eff * (phi * abs(pe) + (1 - phi) * assoc[s, a])

            # WM decay toward uniform for this state on each visit
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w0[s, :]

            # On reward, store a deterministic one-hot template
            if r == 1:
                w[s, :] = ((0.0) / (nA - 1)) * np.ones(nA)
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with set-size-dependent decision noise and lapse.

    Mechanisms:
    - RL: tabular Q-learning with single learning rate.
    - WM: limited-capacity cache that can hold up to K states per block; stored states have deterministic one-hot WM policy.
    - Policy: mixture of WM and RL; WM contributes only if the current state is resident in the cache.
    - Decision noise: RL inverse temperature increases as set size decreases; lapse probability increases with set size and age.
    - Set-size effect: both WM availability (through K) and softmax temperature depend on set size (3 vs 6).
    - Age effect: older adults have reduced effective capacity and higher lapse.

    Parameters (list of 6):
    - model_parameters[0] = alpha in (0,1): RL learning rate.
    - model_parameters[1] = beta0 (real >= 0): base RL inverse temperature (scaled by 10 internally).
    - model_parameters[2] = beta_ss_gain (real): gain added to beta when set size is small (scales with 3/nS).
    - model_parameters[3] = K_raw (real): working memory capacity (mapped to [1, 6]).
    - model_parameters[4] = wm_w0 (real): baseline WM reliance (logit transformed) when state is in cache.
    - model_parameters[5] = lapse_raw (real): lapse propensity (logit transformed to [0,1]), scaled by set size and age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eps = 1e-12

    alpha, beta0, beta_ss_gain, K_raw, wm_w0, lapse_raw = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    wm_weight_base = 1.0 / (1.0 + np.exp(-wm_w0))
    lapse_base = 1.0 / (1.0 + np.exp(-lapse_raw))
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        sel = (blocks == b)
        block_actions = actions[sel]
        block_rewards = rewards[sel]
        block_states = states[sel]
        nA = 3
        nS = int(set_sizes[sel][0])

        # Effective inverse temperature scales with set size
        softmax_beta = (abs(beta0) + beta_ss_gain * (3.0 / float(nS))) * 10.0
        # Effective capacity in [1, 6], and age reduces it
        K_eff = 1.0 + 5.0 * (1.0 / (1.0 + np.exp(-K_raw)))
        if age_group == 1:
            K_eff = max(1.0, K_eff - 1.0)

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Cache for states held in WM, FIFO replacement
        cache = []
        in_cache = np.zeros(nS, dtype=bool)

        # Lapse scales with set size and age
        lapse = lapse_base * (nS / 6.0) * (1.25 if age_group == 1 else 1.0)
        lapse = np.clip(lapse, 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM mixture weight active only if state is cached; also scaled by set size
            if in_cache[s]:
                wm_weight = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)
                W_s = w[s, :]
                denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = 1.0 / max(denom_wm, eps)
            else:
                wm_weight = 0.0
                p_wm = 1.0 / nA  # unused but defined

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Lapse: with probability lapse, choose uniformly at random
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM cache update on reward: store and manage capacity
            if r == 1:
                # If not in cache, add; evict oldest if capacity exceeded
                if not in_cache[s]:
                    cache.append(s)
                    in_cache[s] = True
                    if len(cache) > int(np.floor(K_eff)):
                        s_evict = cache.pop(0)
                        in_cache[s_evict] = False
                        w[s_evict, :] = w0[s_evict, :].copy()
                # Store deterministic correct action
                w[s, :] = ((0.0) / (nA - 1)) * np.ones(nA)
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with meta-control gating and RL decay.

    Mechanisms:
    - RL: tabular Q-learning with learning rate and per-visit decay of Q-values toward uniform (forgetting).
    - WM: graded storage; on each visit, WM moves toward a one-hot for the chosen action with strength wm_eta when rewarded, and weakly when not rewarded.
    - Meta-control: trial-wise gate between WM and RL increases with RL policy conflict (entropy) and decreases with set size; baseline gate has an age shift.
    - Policy: mixture using the gated WM weight.

    Set-size effect:
    - Gating weight is scaled by 3/nS, reducing WM influence under higher load.

    Age effect:
    - Baseline gating bias shifted by gate_age_shift for older adults.
    - Older adults have stronger RL decay (more forgetting).

    Parameters (list of 6):
    - model_parameters[0] = alpha in (0,1): RL learning rate.
    - model_parameters[1] = beta (>=0): RL inverse temperature (scaled by 10 internally).
    - model_parameters[2] = wm_eta in (0,1): WM learning strength toward one-hot on reward; half as strong on no-reward.
    - model_parameters[3] = gate_bias (real): baseline gating bias toward WM (logit space).
    - model_parameters[4] = gate_age_shift (real): additive shift to gate_bias for older adults.
    - model_parameters[5] = q_decay in (0,1): per-visit RL decay toward uniform.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eps = 1e-12

    alpha, beta, wm_eta, gate_bias, gate_age_shift, q_decay = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    wm_eta = 1.0 / (1.0 + np.exp(-wm_eta))
    q_decay = 1.0 / (1.0 + np.exp(-q_decay))
    softmax_beta = abs(beta) * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        sel = (blocks == b)
        block_actions = actions[sel]
        block_rewards = rewards[sel]
        block_states = states[sel]
        nA = 3
        nS = int(set_sizes[sel][0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted baseline gate
        gate_base = gate_bias + (gate_age_shift if age_group == 1 else 0.0)

        # Age-adjusted RL decay
        q_decay_eff = np.clip(q_decay * (1.25 if age_group == 1 else 1.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # Approximate conflict via softmax entropy of RL policy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            probs = np.exp(logits)
            probs /= max(np.sum(probs), eps)
            entropy = -np.sum(probs * np.log(np.clip(probs, eps, 1.0))) / np.log(nA)  # normalized to [0,1]

            # Trial-wise gate toward WM: sigmoid of bias + entropy, scaled by set size
            gate_pre = gate_base + entropy  # fixed unit weight on entropy
            gate = 1.0 / (1.0 + np.exp(-gate_pre))
            wm_weight = np.clip(gate * (3.0 / float(nS)), 0.0, 1.0)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update and decay
            pe = r - q[s, a]
            q[s, a] += alpha * pe
            # decay entire state's Q toward uniform each visit
            q[s, :] = (1.0 - q_decay_eff) * q[s, :] + q_decay_eff * (1.0 / nA)

            # WM update: move toward one-hot for chosen action
            target = np.ones(nA) * (0.0 / (nA - 1))
            target[a] = 1.0
            eta_now = wm_eta if r == 1 else 0.5 * wm_eta
            w[s, :] = (1.0 - eta_now) * w[s, :] + eta_now * target

        blocks_log_p += log_p

    return -blocks_log_p