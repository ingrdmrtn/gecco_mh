def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with encoding and age-/load-dependent forgetting.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores a one-shot mapping for a state after rewarded feedback with some encoding probability.
    - WM contribution is scaled down under higher set size and suffers stronger forgetting for older adults.
    - Arbitration is a fixed mixture between WM and RL per block, scaled by set size.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_base, p_encode, forget_rate_base, age_forget_boost]
      - lr: RL learning rate (0..1).
      - softmax_beta: RL inverse temperature; internally scaled x10.
      - wm_base: baseline WM mixture weight (0..1) before set size scaling.
      - p_encode: probability to encode a rewarded association into WM (0..1).
      - forget_rate_base: baseline WM forgetting rate toward uniform per trial (0..1).
      - age_forget_boost: multiplicative boost to forgetting if age_group==1 (>=0).

    Age use
    -------
    - age_group = 0 if age <= 45, else 1.
    - WM forgetting rate increases by (1 + age_forget_boost) for older adults.

    Returns
    -------
    Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_base, p_encode, forget_rate_base, age_forget_boost = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled mixture weight (stronger WM at set size 3 than 6)
        wm_w_eff = np.clip(wm_base * (3.0 / max(1, nS)), 0.0, 1.0)
        # Age- and load-modulated forgetting rate
        forget_rate = forget_rate_base * (1.0 + age_forget_boost * age_group) * (nS / 3.0)
        forget_rate = np.clip(forget_rate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Policies
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM forgetting toward uniform on the visited state
            w[s, :] = (1.0 - forget_rate) * w[s, :] + forget_rate * w_0[s, :]

            # WM encoding on reward: one-shot, if encoded overwrite with one-hot
            if r >= 0.5:
                if np.random.rand() < np.clip(p_encode, 0.0, 1.0):
                    w[s, :] = 0.0
                    w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-weighted arbitration to WM and age-modulated learning rate.

    Idea:
    - RL learns Q with a single LR; younger adults learn faster (age-modulated LR).
    - WM stores rewarded mappings and decays toward uniform each trial.
    - Arbitration weight increases when RL is uncertain (high entropy) and for smaller set sizes.
      Weight = sigmoid(wm_bias + arb_slope*(H_max - H(Q_s)) + log(3/set_size)).
      Lower (H_max - H) => higher uncertainty => larger weight to WM.
    - This integrates load (set size), RL uncertainty, and age.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_bias, arb_slope, age_lr_mult, wm_decay]
      - lr: baseline RL learning rate.
      - softmax_beta: RL inverse temperature; internally scaled x10.
      - wm_bias: baseline bias toward WM in arbitration (logit space).
      - arb_slope: sensitivity of arbitration to RL certainty (H_max - H).
      - age_lr_mult: multiplicative boost to LR for younger adults (applied if age_group==0).
      - wm_decay: WM decay rate toward uniform per trial (0..1).

    Age use
    -------
    - If age_group==0 (younger), lr_eff = lr * (1 + age_lr_mult); else lr_eff = lr.

    Returns
    -------
    Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_bias, arb_slope, age_lr_mult, wm_decay = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    nA = 3
    H_max = np.log(nA)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probabilities (full softmax for entropy computation)
            Q_s = q[s, :].copy()
            rl_logits = beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)
            p_rl = np.clip(rl_probs[a], eps, 1.0)

            # RL certainty via entropy
            H = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))
            certainty = H_max - H  # higher means more certain
            # Set-size term favors WM at smaller set sizes
            setsize_term = np.log(3.0 / max(1, nS))
            # Arbitration weight (sigmoid)
            wm_weight = 1.0 / (1.0 + np.exp(-(wm_bias + arb_slope * certainty + setsize_term)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # WM policy
            W_s = w[s, :].copy()
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)
            p_wm = np.clip(wm_probs[a], eps, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with age-modulated learning rate
            lr_eff = lr * (1.0 + (age_lr_mult if age_group == 0 else 0.0))
            pe = r - q[s, a]
            q[s, a] += lr_eff * pe

            # WM update: decay then overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM implementing win-stay/lose-shift with set-size power-law scaling and age-sensitive lose-shift.

    Idea:
    - RL with a single learning rate.
    - WM component encodes a simple heuristic:
        - After a win in a state: boost probability of repeating the last action (win-stay).
        - After a loss in a state: suppress the last action (lose-shift) by boosting the other actions.
      The strength of this heuristic is parameterized.
    - WM mixture weight scales with (3/set_size)^gamma, capturing strong load effects.
    - Older adults show weaker lose-shift (age_ls_penalty reduces the lose-shift component only for older).

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_weight, gamma_set, ls_strength, age_ls_penalty]
      - lr: RL learning rate.
      - softmax_beta: RL inverse temperature; internally scaled x10.
      - wm_weight: baseline WM mixture weight before set-size scaling (0..1).
      - gamma_set: power controlling how fast WM weight drops with set size (>=0).
      - ls_strength: strength of win-stay/lose-shift logits added to WM.
      - age_ls_penalty: fractional reduction of lose-shift strength for older adults (0..1).

    Age use
    -------
    - Lose-shift strength is scaled by (1 - age_ls_penalty) only if age_group==1 (older).
      Win-stay is unaffected by age.

    Returns
    -------
    Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, gamma_set, ls_strength, age_ls_penalty = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        # For WM heuristic we keep track of last action and last reward per state
        last_action = -np.ones(nS, dtype=int)
        last_rew = -np.ones(nS)  # -1 means unknown; else 0 or 1

        # Set-size scaled mixture
        wm_w_eff = np.clip(wm_weight * (3.0 / max(1, nS)) ** np.maximum(0.0, gamma_set), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM heuristic policy via logits
            wm_logits = np.zeros(nA)
            if last_action[s] >= 0 and last_rew[s] >= 0:
                la = last_action[s]
                if last_rew[s] >= 0.5:
                    # Win-stay: boost last action
                    wm_logits[la] += ls_strength
                else:
                    # Lose-shift: suppress last action by boosting others
                    ls_eff = ls_strength * (1.0 - (age_ls_penalty if age_group == 1 else 0.0))
                    for aa in range(nA):
                        if aa != la:
                            wm_logits[aa] += ls_eff
            # Softmax WM policy
            wm_logits = softmax_beta_wm * (wm_logits - np.max(wm_logits))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)
            p_wm = np.clip(wm_probs[a], eps, 1.0)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update WM heuristic memory
            last_action[s] = a
            last_rew[s] = r

        blocks_log_p += log_p

    return -float(blocks_log_p)