Below are three standalone cognitive models that instantiate different RL+WM mechanisms tailored to set size and age group effects. Each function returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with capacity-limited WM reliance and set-size-driven WM decay, modulated by age.

    Core ideas:
    - RL: single learning rate; inverse temperature depends on age via an additive age effect.
    - WM: mixture weight is a sigmoid of (capacity - set size). Capacity is reduced with age.
    - WM decay towards uniform increases when set size exceeds capacity.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: base RL softmax inverse temperature (scaled by 10 internally)
    - beta_age_effect: additive modulation of beta for older adults (beta = (base + age*beta_age_effect)*10)
    - wm_capacity_base: baseline WM capacity in number of state-action pairs
    - wm_capacity_age_effect: capacity decrement applied for older adults (capacity = base - age*effect)
    - wm_strength: slope parameter used to compute WM mixture weight from (capacity - set size)
    
    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards (0/1) per trial
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (constant within block: 3 or 6)
    - age: array with the same age value repeated; age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta_base, beta_age_effect, wm_capacity_base, wm_capacity_age_effect, wm_strength = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = (beta_base + age_group * beta_age_effect) * 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity and mixture weight
        wm_capacity = wm_capacity_base - age_group * wm_capacity_age_effect
        # Mixture weight: higher when capacity exceeds set size
        mix = 1.0 / (1.0 + np.exp(-wm_strength * (wm_capacity - nS)))
        mix = float(np.clip(mix, 0.0, 1.0))

        # WM decay increases when set size exceeds capacity
        overload = max(nS - wm_capacity, 0.0)
        if nS > 0:
            leak = np.clip(overload / float(nS), 0.0, 1.0)
        else:
            leak = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL softmax prob of chosen action (centered form matching template)
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            # WM softmax prob
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM passive decay toward uniform (set-size dependent via leak)
            w = (1.0 - leak) * w + leak * w0

            # WM storage on rewarded trials
            if r > 0.5:
                # Strong one-shot binding for the rewarded state-action
                w[s, :] = w0[s, :]
                w[s, a] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with load- and age-adjusted exploration and RL forgetting; fixed WM mixture.

    Core ideas:
    - RL inverse temperature decreases with set size and age (divisive scaling).
    - RL Q-values forget toward uniform with a load- and age-inflated forgetting rate.
    - WM uses a fixed mixture weight, but WM traces are passively evicted as a function of load and age.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: base RL inverse temperature before load/age scaling (scaled by 10 internally)
    - k_set: scaling of exploration and WM eviction with set size
    - k_age: scaling of exploration and WM eviction with age group (0 young, 1 old)
    - rl_forget: base RL forgetting rate per trial (0..1), amplified by load/age
    - wm_reliance: fixed mixing weight for WM policy (0..1)
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: see cognitive_model1

    Returns:
    - Negative log-likelihood.
    """
    lr, beta_base, k_set, k_age, rl_forget, wm_reliance = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        q0 = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Load and age scalars
        load = max(nS - 3, 0) / 3.0  # 0 for 3, 1 for 6
        # Effective RL temperature after divisive scaling
        beta = beta_base * 10.0 / (1.0 + k_set * load + k_age * age_group)

        # RL forgetting increases with load and age
        f_eff = rl_forget * (1.0 + k_set * load + k_age * age_group)
        f_eff = np.clip(f_eff, 0.0, 1.0)

        # WM eviction/decay rate shaped by same factors via a sigmoid
        evict_logit = k_set * load + k_age * age_group
        evict = 1.0 / (1.0 + np.exp(-evict_logit))  # in (0,1)

        mix = float(np.clip(wm_reliance, 0.0, 1.0))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting toward uniform
            q = (1.0 - f_eff) * q + f_eff * q0

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # WM eviction towards uniform (before computing policy to reflect current memory)
            w = (1.0 - evict) * w + evict * w0
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM storage on reward
            if r > 0.5:
                w[s, :] = w0[s, :]
                w[s, a] = 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and age-dependent WM decay.

    Core ideas:
    - Arbitration uses the relative uncertainty (entropy) of RL vs. WM policies each trial:
      if RL is more uncertain than WM, weight shifts to WM, and vice versa.
    - WM decays toward uniform with a base rate that increases with age; load scales decay.
    - RL uses a standard softmax with a single inverse temperature.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta0: RL inverse temperature (scaled by 10 internally)
    - arb_slope: slope for the arbitration sigmoid over entropy difference
    - wm_decay_base: base WM decay logit; transformed via sigmoid to rate
    - wm_decay_age: additive age effect on WM decay logit (older -> larger decay)
    - wm_bias: additive bias term in arbitration toward WM for young (positive favors WM)
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: see cognitive_model1

    Returns:
    - Negative log-likelihood.
    """
    lr, beta0, arb_slope, wm_decay_base, wm_decay_age, wm_bias = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = beta0 * 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay rate: sigmoid(logit) scaled by load
        load = max(nS - 3, 0) / 3.0  # 0 for 3, 1 for 6
        decay_logit = wm_decay_base + wm_decay_age * age_group
        decay_base = 1.0 / (1.0 + np.exp(-decay_logit))  # in (0,1)
        leak = np.clip(decay_base * (1.0 + load), 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL policy and entropy
            Q_s = q[s, :]
            logits_rl = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))

            # WM decay toward uniform
            w = (1.0 - leak) * w + leak * w0
            W_s = w[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, 1e-12, 1.0)))

            # Entropy-based arbitration: more weight to the less uncertain policy
            # We add a WM bias for young participants (age_group=0); older get no bias from wm_bias term
            bias_term = wm_bias * (1 - age_group)
            arb_input = bias_term + (H_rl - H_wm)
            mix = 1.0 / (1.0 + np.exp(-arb_slope * arb_input))
            mix = float(np.clip(mix, 0.0, 1.0))

            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM storage on reward
            if r > 0.5:
                w[s, :] = w0[s, :]
                w[s, a] = 1.0

    return nll

Notes on age and set-size influences:
- Model 1: Age reduces WM capacity and lowers WM reliance when set size exceeds capacity; RL temperature also shifts with age.
- Model 2: Set size and age reduce RL inverse temperature and increase RL forgetting; WM reliance is fixed but WM eviction increases with set size and age.
- Model 3: WM decay rate increases with age and set size; arbitration dynamically favors the less uncertain system and adds a young-specific WM bias.