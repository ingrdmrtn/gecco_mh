Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) mechanisms, incorporate set-size (3 vs 6) and age effects, and return the negative log-likelihood of the observed choices. Each model uses no more than 6 parameters, uses every parameter meaningfully, resets at block boundaries, and uses the participantâ€™s age group.

Note: Assume numpy (np) is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with interference-based WM decay, load and age effects, and state-dependent perseveration.

    Mechanism
    - RL: Q-learning with softmax.
    - WM: a probability table W[s,a] that is pulled toward a one-hot for rewarded actions in that state.
          Between trials, WM decays toward uniform due to interference that grows with load (set size)
          and is modulated by age.
    - Arbitration: mixture based on current WM confidence (peakiness of W at the queried state).
    - Perseveration: a state-dependent choice trace that biases the softmax toward the previously chosen action.

    Parameters (len=6):
      0) lr_rl           : RL learning rate (0..1)
      1) beta            : RL inverse temperature (>0), internally scaled by 10
      2) wm_peak         : WM overwrite strength toward one-hot on rewarded trials (0..1)
      3) interference    : Base WM decay-to-uniform per trial (0..1)
      4) age_interf_shift: Age modulation of interference (positive increases decay for old, decreases for young)
      5) perseveration   : Strength of state-dependent choice trace added to Q (>=0)

    Returns
      Negative log-likelihood of observed choices.
    """
    lr_rl, beta, wm_peak, interference, age_interf_shift, perseveration = model_parameters
    beta = beta * 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        bsz = int(set_sizes[mask][0])

        nA = 3
        nS = bsz

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        U = (1.0 / nA) * np.ones((nS, nA))

        # State-dependent choice trace for perseveration
        C = np.zeros((nS, nA))
        choice_decay = 0.5  # fixed decay, perseveration parameter scales the magnitude

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL softmax with perseveration bias
            Q_eff = Q[s, :] + perseveration * C[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_eff - Q_eff[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM softmax
            W_s = W[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: WM weight from current WM confidence (peakiness)
            conf = max(W_s) - min(W_s)  # in [0,1]
            wm_weight = np.clip(conf, 0.0, 1.0)

            # Mixture probability
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe

            # WM global decay toward uniform with load and age effects
            # Age effect: young reduce interference; old increase
            age_effect = -abs(age_interf_shift) if age_group == 0 else abs(age_interf_shift)
            load_scale = 1.0 + 0.5 * max(0, nS - 3)  # 1 for 3, 2.5 for 6
            gamma = np.clip((interference + age_effect) * load_scale, 0.0, 1.0)
            W = (1.0 - gamma) * W + gamma * U

            # WM encoding on reward: push toward one-hot
            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = (1.0 - wm_peak) * W[s, :] + wm_peak * target

            # Update choice trace (state-dependent)
            C *= (1.0 - choice_decay)
            C[s, a] += 1.0

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM with Dirichlet counts and uncertainty-weighted arbitration, plus lapse.
    
    Mechanism
    - RL: Q-learning with softmax.
    - WM: For each state, track Dirichlet counts over actions (deterministic mapping hypothesis).
          WM policy is the mean of the Dirichlet (categorical parameters).
    - Arbitration: WM weight is 1 - normalized entropy of WM's categorical at that state, penalized by load
      and age (older -> more uncertainty; younger -> less).
    - Lapse: final policy is blended with uniform via epsilon.

    Parameters (len=6):
      0) lr_rl              : RL learning rate (0..1)
      1) beta_rl            : RL inverse temperature (>0), internally scaled by 10
      2) alpha0             : Dirichlet prior concentration (>0), regularizes WM beliefs
      3) age_uncert_bias    : Age modulation of WM certainty (young: decreases uncertainty; old: increases)
      4) load_noise         : Additional uncertainty per unit load (0..1), applied when set size increases
      5) lapse              : Lapse rate mixing with uniform (0..1)

    Returns
      Negative log-likelihood of observed choices.
    """
    lr_rl, beta_rl, alpha0, age_uncert_bias, load_noise, lapse = model_parameters
    beta_rl = beta_rl * 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    logK = np.log(3.0)

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        bsz = int(set_sizes[mask][0])

        nA = 3
        nS = bsz

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Dirichlet counts per state
        Alpha = np.full((nS, nA), alpha0, dtype=float)

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # WM categorical from Dirichlet mean
            alpha_s = Alpha[s, :]
            p_cat = alpha_s / np.sum(alpha_s)

            # RL softmax probability for the chosen action
            Q_s = Q[s, :]
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM probability of chosen action
            # Equivalent to softmax with very high beta on log p; direct p is fine
            p_wm = max(p_cat[a], eps)

            # Uncertainty-based arbitration
            # Normalized entropy in [0,1]
            ent = -np.sum(p_cat * np.log(np.maximum(p_cat, eps))) / logK
            # Load effect: add uncertainty penalty with set size
            load_pen = load_noise * max(0, (nS - 3) / 3.0)  # 0 for 3, ~1 for 6 if load_noise=1
            # Age effect: old increases uncertainty; young decreases
            age_pen = abs(age_uncert_bias) if age_group == 1 else -abs(age_uncert_bias)
            # Effective uncertainty
            u_eff = np.clip(ent + load_pen + age_pen, 0.0, 1.0)
            wm_weight = 1.0 - u_eff
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture and lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe

            # Dirichlet update: on reward, strengthen chosen action strongly; on no reward, weak diffuse
            if r > 0.5:
                Alpha[s, a] += 1.0
            else:
                # small symmetric diffusion to reflect uncertainty (kept constant, not a parameter)
                Alpha[s, :] += 0.05

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Win-Stay WM with leaky success history arbitration, age- and load-dependent decay,
    and exploration epsilon.

    Mechanism
    - RL: Q-learning with softmax; beta is modulated by recent success history (meta-temperature).
    - WM: Win-Stay memory per state. A memory strength m_s in [0,1] tracks confidence in the last rewarded
      action at that state; decays over trials with load- and age-modulated rate. WM policy is a mixture of
      one-hot at the last rewarded action and uniform, weighted by m_s.
    - Arbitration: WM weight is a leaky accumulator of recent rewards (block-level success history H), which
      also decays with the same mechanism. Higher H increases reliance on WM.
    - Exploration: final policy mixed with uniform by epsilon.

    Parameters (len=6):
      0) lr_rl           : RL learning rate (0..1)
      1) beta_base       : Base RL inverse temperature (>0), internally scaled by 10
      2) wm_ws_strength  : Increment to WM memory strength on reward (0..1)
      3) hist_decay      : Base decay rate for both WM memory and success history (0..1)
      4) age_decay_shift : Age modulation of decay (young reduce; old increase)
      5) epsilon         : Exploration rate (0..1) blending with uniform

    Returns
      Negative log-likelihood of observed choices.
    """
    lr_rl, beta_base, wm_ws_strength, hist_decay, age_decay_shift, epsilon = model_parameters
    beta_base = beta_base * 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        bsz = int(set_sizes[mask][0])

        nA = 3
        nS = bsz

        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM: last rewarded action per state and its strength
        last_win_action = -np.ones(nS, dtype=int)
        m = np.zeros(nS, dtype=float)  # memory strength in [0,1]

        # Leaky success history for arbitration
        H = 0.5

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # Effective decay includes load and age
            load_term = 0.2 * max(0, nS - 3)  # 0 for 3, 0.6 for 6
            age_term = abs(age_decay_shift) if age_group == 1 else -abs(age_decay_shift)
            decay_eff = np.clip(hist_decay + load_term + age_term, 0.0, 1.0)

            # WM policy at state s
            if last_win_action[s] >= 0:
                p_wm_vec = (1.0 - m[s]) * (np.ones(nA) / nA)
                p_wm_vec[last_win_action[s]] += m[s]
            else:
                p_wm_vec = np.ones(nA) / nA
            p_wm = max(p_wm_vec[a], eps)

            # RL softmax with meta-temperature depending on success history H and age
            # Young: stronger modulation; Old: weaker
            if age_group == 0:
                beta_eff = beta_base * (1.0 + 0.5 * H)
            else:
                beta_eff = beta_base * (1.0 + 0.2 * H)
            Q_s = Q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # Arbitration via success history H
            wm_weight = np.clip(H, 0.0, 1.0)

            # Mixture and epsilon exploration
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe

            # Update WM memory and decay
            # Decay memory globally
            m = (1.0 - decay_eff) * m
            # On reward, set last_win_action and increase memory strength
            if r > 0.5:
                last_win_action[s] = a
                m[s] = np.clip(m[s] + wm_ws_strength, 0.0, 1.0)

            # Update success history (leaky integrator)
            H = (1.0 - decay_eff) * H + decay_eff * (1.0 if r > 0.5 else 0.0)

    return -float(total_log_p)