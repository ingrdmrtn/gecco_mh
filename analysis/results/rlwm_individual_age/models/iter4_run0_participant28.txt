def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL with eligibility traces + WM with write/decay; arbitration via WM confidence penalized by set size and age.
    - RL: tabular Q-learning with accumulating eligibility traces across state-action pairs within a block.
    - WM: fast supervised memory that sharpens toward the rewarded action and decays toward uniform otherwise.
    - Arbitration: on each trial, compute WM confidence (peak minus mean) and map to a WM mixture weight via a sigmoid.
      Confidence is penalized by set size and age (older or larger set sizes reduce WM reliance).
    - Policy: mixture of WM softmax and RL softmax.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled internally by 10)
    - lambda_e: eligibility trace decay (0..1); higher maintains broader credit assignment within the block
    - wm_learn: WM write-in rate toward one-hot on reward (0..1)
    - wm_decay: WM decay toward uniform on non-reward (0..1)
    - age_wm_penalty: additive penalty applied to WM confidence if age_group==1, subtracted if age_group==0 (>=0)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, lambda_e, wm_learn, wm_decay, age_wm_penalty = model_parameters
    softmax_beta = beta_rl * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces for all (s,a)
        e = np.zeros((nS, nA))

        # Set-size penalty term (larger nS => more penalty) and age modulation
        ss_pen = np.log(max(2, nS))  # 0.69 for 2, ~1.1 for 3, ~1.79 for 6
        age_pen = age_wm_penalty if age_group == 1 else -age_wm_penalty

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration by WM confidence with set-size and age penalty
            wm_conf = np.max(W_s) - np.mean(W_s)
            conf_adj = wm_conf - (ss_pen + age_pen)
            wm_weight = 1.0 / (1.0 + np.exp(-5.0 * conf_adj))  # sigmoid with slope 5

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # Decay traces and set current (s,a)
            e *= lambda_e
            e[s, a] += 1.0
            q += lr * delta * e
            # Optional small normalization to keep Q bounded
            # q = np.clip(q, -10, 10)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL with forgetting + WM error-correcting memory; arbitration by WM entropy and lapse that scales with set size and age.
    - RL: delta rule plus forgetting toward uniform Q to capture interference in larger sets.
    - WM: reward-driven sharpening toward chosen action, error-driven suppression (punish) of chosen action.
    - Arbitration: WM weight computed from WM entropy (lower entropy => higher WM reliance), scaled down with set size.
      A global lapse mixes in uniform random choice, with lapse increasing with set size and age.
    - Policy: convex combination of WM, RL, and uniform (lapse).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled internally by 10)
    - wm_eta: WM reward-driven learning rate toward one-hot (0..1)
    - wm_punish: WM error-driven suppression strength on chosen action (0..1)
    - forget: RL forgetting rate toward uniform (0..1)
    - age_lapse_boost: added lapse when old; subtracted when young (can be >=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_eta, wm_punish, forget, age_lapse_boost = model_parameters
    softmax_beta = beta_rl * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects: scale WM reliance and lapse
        ss_scale = np.log(max(2, nS))  # increases with set size
        lapse_base = 0.02 * ss_scale
        lapse_age = age_lapse_boost if age_group == 1 else max(0.0, 0.02 - age_lapse_boost)
        lapse = np.clip(lapse_base + lapse_age, 0.0, 0.3)  # cap lapse

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM entropy-based arbitration (lower entropy => higher weight), scaled down by set size
            ps_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            ps_wm /= np.sum(ps_wm)
            ent = -np.sum(ps_wm * np.log(np.clip(ps_wm, eps, 1.0)))
            ent_norm = ent / np.log(nA)  # 0..1
            wm_weight = (1.0 - ent_norm) / (1.0 + ss_scale)  # larger sets -> smaller weight

            # Combine with lapse to uniform policy
            p_uniform = 1.0 / nA
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * p_uniform
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q = (1.0 - forget) * q + forget * (1.0 / nA)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * one_hot
            else:
                # Punish chosen action and renormalize softly toward uniform
                w[s, a] = (1.0 - wm_punish) * w[s, a]
                # Distribute removed mass toward others proportional to current
                remainder = 1.0 - w[s, a]
                others = [i for i in range(nA) if i != a]
                mass_others = np.sum(w[s, others])
                if mass_others > 0:
                    w[s, others] = w[s, others] / mass_others * remainder
                else:
                    w[s, others] = remainder / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Volatility-gated arbitration (error-tracking) between RL and WM.
    - RL: standard delta rule.
    - WM: reward-dependent Hebbian write-in, error-dependent decay to uniform.
    - Volatility tracker v: exponentially weighted moving average of unsigned prediction error.
      Higher v indicates more uncertainty/instability (e.g., due to larger set sizes),
      which down-weights WM reliance. Aging increases effective volatility.
    - Arbitration: WM weight = sigmoid(bias - gain * v_ss_age), where v_ss_age is volatility scaled by set size and age.
    - Policy: mixture of RL and WM softmax.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled internally by 10)
    - wm_write: WM write-in strength on reward (0..1)
    - k_v: volatility update rate (0..1) for unsigned RPE
    - arb_bias: baseline bias toward WM (>0 favors WM, <0 favors RL)
    - age_vol_boost: scales volatility upward when old; scales downward when young

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_write, k_v, arb_bias, age_vol_boost = model_parameters
    softmax_beta = beta_rl * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize volatility tracker
        v = 0.0
        # Set-size scaling: larger sets amplify volatility impact
        ss_gain = np.sqrt(max(1, nS)) / np.sqrt(3.0)
        # Age scaling: older -> increase volatility, younger -> decrease
        age_gain = (1.0 + age_vol_boost) if age_group == 1 else max(0.1, 1.0 - age_vol_boost)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration via volatility-gated sigmoid
            v_eff = v * ss_gain * age_gain
            wm_weight = 1.0 / (1.0 + np.exp(-(arb_bias - 3.0 * v_eff)))  # higher v reduces WM if arb_bias not too large

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Volatility update from unsigned RPE
            v = (1.0 - k_v) * v + k_v * abs(delta)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_write) * w[s, :] + wm_write * one_hot
            else:
                decay = wm_write  # reuse as decay on errors for parsimony
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p