def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with leak and age-/set-size-modulated WM weight.

    Idea:
    - RL learns state-action values with a single learning rate and softmax policy.
    - WM stores the last rewarded action per state (one-trial memory), with leak toward uniform.
    - WM contribution is reduced when set size is large relative to WM capacity, and further reduced in older adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline mixture weight for WM (0..1)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_capacity: number of items (states) WM can hold effectively (>0)
    - wm_leak: per-visit leak of WM toward uniform (0..1)
    - age_wm_penalty: proportional reduction of WM weight for older group (0..1)

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (block-constant; values 3 or 6)
    - age: array with single repeated value of participant age
    - model_parameters: list/tuple of parameters as defined above

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_capacity, wm_leak, age_wm_penalty = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL
    softmax_beta_wm = 50.0  # very deterministic WM policy
    eps = 1e-12

    # Age group coding: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and WM weights start uniform
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight scales with capacity and age
        capacity_scale = min(1.0, max(0.0, wm_capacity) / max(1.0, nS))
        wm_weight_eff = wm_weight_base * capacity_scale
        if age_group == 1:
            wm_weight_eff *= (1.0 - np.clip(age_wm_penalty, 0.0, 1.0))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy (deterministic softmax over W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform; if rewarded, store one-hot for chosen action
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Write more strongly toward the chosen rewarded action
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + entropy-based arbitration to WM, with age bias and WM refresh.

    Idea:
    - RL learns with a single learning rate and softmax policy with a base inverse temperature.
    - WM stores the last rewarded action per state; if not rewarded, WM gently refreshes toward uniform.
    - Arbitration (WM mixture weight) increases when RL is uncertain (high entropy).
      Age adds a bias (older -> more WM reliance or less, depending on parameter sign).
      Set size indirectly influences arbitration via RL entropy (harder sets -> higher entropy).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: base RL inverse temperature (scaled by 10 internally)
    - wm_mix_base: base logit for WM mixture (can be any real; sigmoid applied)
    - entropy_slope: slope scaling of RL entropy into WM logit (>=0 suggests more WM when uncertain)
    - age_entropy_bias: additive shift to WM logit if older (can be +/-)
    - wm_refresh: WM refresh/forget parameter toward uniform on each visit (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays
    - model_parameters: list/tuple of parameters as defined above

    Returns:
    - Negative log-likelihood.
    """
    lr, beta_base, wm_mix_base, entropy_slope, age_entropy_bias, wm_refresh = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    def softmax_probs(values, beta):
        # stable softmax
        v = values - np.max(values)
        ex = np.exp(beta * v)
        return ex / np.sum(ex)

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy and entropy
            p_rl_vec = softmax_probs(Q_s, softmax_beta)
            p_rl = np.clip(p_rl_vec[a], eps, 1.0)
            H_rl = entropy(p_rl_vec)  # in [0, ln(3)]

            # WM policy (near-deterministic)
            w_logits = W_s.copy()
            w_logits = w_logits - np.max(w_logits)
            ex_w = np.exp(softmax_beta_wm * w_logits)
            p_wm_vec = ex_w / np.sum(ex_w)
            p_wm = np.clip(p_wm_vec[a], eps, 1.0)

            # Entropy-based arbitration on the logit scale
            wm_logit = wm_mix_base + entropy_slope * H_rl + (age_entropy_bias * age_group)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture probability
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: refresh toward uniform; if rewarded, store chosen action
            w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WSLS-style WM, with size-dependent WM dilution and age boost.

    Idea:
    - RL has separate learning rates for positive and negative outcomes (asymmetric learning).
    - WM implements a win-stay / lose-reset mechanism:
        - After reward: WM for that state becomes one-hot on chosen action.
        - After no reward: WM for that state resets toward uniform (lose-shift tendency).
    - WM mixture weight declines with set size via a power law. Older adults can receive a WM boost/penalty.

    Parameters (model_parameters):
    - lr_pos: RL learning rate after reward (0..1)
    - lr_neg: RL learning rate after no reward (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - gamma_size: exponent controlling how WM weight declines with set size (>=0)
    - age_wm_boost: multiplicative WM boost for older group (can be negative to penalize)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: list/tuple of parameters as defined above

    Returns:
    - Negative log-likelihood.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gamma_size, age_wm_boost = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Size-dependent dilution of WM weight
        size_factor = 1.0 / (1.0 + max(0.0, float(nS - 1)) ** max(0.0, gamma_size))
        wm_weight_eff = wm_weight * size_factor
        if age_group == 1:
            wm_weight_eff *= (1.0 + age_wm_boost)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM near-deterministic softmax based on W_s
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            lr_used = lr_pos if r > 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr_used * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WSLS-like: reward -> one-hot on chosen; no reward -> reset toward uniform
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p