def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + surprise-gated WM with load- and age-adjusted arbitration.

    Mechanisms:
    - RL system: tabular Q-learning with softmax action selection.
    - WM system: associative matrix W that is updated toward a one-hot choice when "surprise" is high.
      Surprise is |reward - expected value|; stronger surprise gates stronger WM storage.
    - Arbitration: mixture weight of WM vs RL decreases with set size and old age.
      WM reliability (inverse temperature) also degrades with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - beta_rl: RL inverse temperature; internally scaled by 10.
    - wm_weight0: baseline WM mixture weight in [0,1].
    - gate_sensitivity: scales the surprise gating strength (>=0); larger -> stronger WM updates on surprising outcomes.
    - beta_wm0: baseline WM inverse temperature (>=0); internally scaled by 10 and modulated by load/age.
    - age_load_factor: additional multiplicative penalty for age on WM components (>=0).

    Age/load effects:
    - Effective WM weight: wm_weight = wm_weight0 * (3/nS) / (1 + age_load_factor*age_group).
    - Effective WM inverse temperature: beta_wm = 10*beta_wm0 * (3/nS) / (1 + age_load_factor*age_group).
    - Surprise gate on WM update: g = sigmoid(gate_sensitivity * |r - Q(s,a)|).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_weight0, gate_sensitivity, beta_wm0, age_load_factor = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration and WM precision depend on load and age
        wm_weight = wm_weight0 * (3.0 / max(1.0, float(nS)))
        wm_weight = wm_weight / (1.0 + age_load_factor * age_group)
        wm_weight = min(max(wm_weight, 0.0), 1.0)

        beta_wm = 10.0 * beta_wm0 * (3.0 / max(1.0, float(nS)))
        beta_wm = beta_wm / (1.0 + age_load_factor * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # WM policy: softmax over W with effective beta_wm
            W_s = w[s, :]
            # Stable softmax vector then pick chosen action's prob
            wm_logits = beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(wm_logits)
            p_wm_vec /= (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Surprise-gated WM update toward chosen action
            # Gate strength via logistic of absolute TD error
            g = 1.0 / (1.0 + np.exp(-gate_sensitivity * abs(delta)))
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            w[s, :] = (1.0 - g) * w[s, :] + g * onehot
            # Keep rows normalized for numerical stability
            w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with recency and interference.

    Mechanisms:
    - RL system: tabular Q-learning with softmax.
    - WM system: associative W that stores recent correct mappings with limited capacity; outside capacity, memory is noisy.
      Capacity limit implemented via mixture weight proportional to (effective_capacity / set_size).
      Global decay of WM increases with set size and is stronger for older participants.
    - Arbitration: WM contribution scales with capacity occupancy; if many items (nS) exceed capacity, RL dominates.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - beta_rl: RL inverse temperature; internally scaled by 10.
    - wm_capacity: nominal number of items that WM can hold (>=1, <= set size typical range).
    - wm_mix0: baseline mixture coefficient for WM in [0,1].
    - decay_rate: base WM decay toward uniform per trial in [0,1].
    - age_penalty: additional penalty factor for capacity and decay with age (>=0).

    Age/load effects:
    - Effective capacity: cap_eff = max(1, wm_capacity - age_penalty*age_group).
    - WM mixture: wm_weight = wm_mix0 * min(1, cap_eff / nS).
    - WM decay: d = clamp(decay_rate * (nS/3) * (1 + age_penalty*age_group), [0,1]).

    WM policy:
    - Deterministic softmax with high inverse temperature; as memory decays, W becomes uniform and policy flattens.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_capacity, wm_mix0, decay_rate, age_penalty = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and decay
        cap_eff = max(1.0, float(wm_capacity) - age_penalty * age_group)
        wm_weight = wm_mix0 * min(1.0, cap_eff / max(1.0, float(nS)))
        wm_weight = min(max(wm_weight, 0.0), 1.0)

        d = decay_rate * (float(nS) / 3.0) * (1.0 + age_penalty * age_group)
        d = min(max(d, 0.0), 1.0)

        beta_wm = 50.0  # high precision when memory is intact

        # Track recency by boosting the just-updated state's WM row; interference handled by global decay d.
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # WM policy from W
            W_s = w[s, :]
            logits = beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(logits)
            p_wm_vec /= (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Arbitration with capacity constraint
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global decay/interference toward uniform
            w = (1.0 - d) * w + d * w_0

            # WM "store" on rewarded trials: push W toward one-hot for the current state
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Storage strength also scales with capacity occupancy; fewer items -> stronger storage
                occupancy = min(1.0, cap_eff / max(1.0, float(nS)))
                eta_store = 0.5 + 0.5 * occupancy  # in [0.5,1]
                w[s, :] = (1.0 - eta_store) * w[s, :] + eta_store * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + uncertainty-weighted WM arbitration.

    Mechanisms:
    - RL system: Q-learning with separate learning rates for positive vs negative prediction errors.
    - WM system: associative W updated Hebbianly on reward, with interference-based noise that increases with load and age.
    - Arbitration: trial-wise mixing weight based on relative uncertainty (entropy) of RL vs WM policies:
      more weight goes to the lower-entropy (more confident) system, with a sensitivity k_arbit.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (r - Q > 0) in [0,1].
    - lr_neg: RL learning rate for negative PE (r - Q < 0) in [0,1].
    - beta_rl: RL inverse temperature; internally scaled by 10.
    - wm_eta: WM learning rate toward one-hot on rewarded trials in [0,1].
    - interference: base WM interference/noise factor in [0,1]; larger -> more decay per trial.
    - k_arbit: arbitration sensitivity to entropy difference (>=0).

    Age/load effects on WM noise:
    - d = clamp(interference * ((nS - 3)/3)_+ * (1 + 0.5*age_group), [0,1]).
      Higher d -> stronger decay of W toward uniform and lower effective WM precision.

    Arbitration:
    - Compute softmax policies p_rl_vec and p_wm_vec.
    - Entropies H_rl and H_wm (nats). WM weight = sigmoid(k_arbit * (H_rl - H_wm)).
      Shifted by age/load via the decay d to modestly penalize WM: wm_weight *= (1 - 0.5*d).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_rl, wm_eta, interference, k_arbit = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay due to interference increasing with load and age
        d = interference * max(0.0, (float(nS) - 3.0) / 3.0) * (1.0 + 0.5 * age_group)
        d = min(max(d, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy vector
            Q_s = q[s, :]
            rl_logits = beta_rl * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(rl_logits)
            p_rl_vec /= (p_rl_vec.sum() + 1e-12)
            p_rl = p_rl_vec[a]

            # WM policy vector; precision reduced by decay d
            beta_wm_eff = 50.0 * (1.0 - d)
            W_s = w[s, :]
            wm_logits = beta_wm_eff * (W_s - np.max(W_s))
            p_wm_vec = np.exp(wm_logits)
            p_wm_vec /= (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Entropy-based arbitration
            H_rl = -np.sum(p_rl_vec * (np.log(p_rl_vec + 1e-12)))
            H_wm = -np.sum(p_wm_vec * (np.log(p_wm_vec + 1e-12)))
            wm_weight = 1.0 / (1.0 + np.exp(-k_arbit * (H_rl - H_wm)))
            # Penalize WM under high decay/load/age
            wm_weight *= (1.0 - 0.5 * d)
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s, a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # WM decay globally
            w = (1.0 - d) * w + d * w_0

            # WM Hebbian store on rewarded trials
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p