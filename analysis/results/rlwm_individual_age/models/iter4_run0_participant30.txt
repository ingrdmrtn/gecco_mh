def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM arbitration via RL uncertainty (entropy) with WM decay modulated by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr_rl, softmax_beta, wm_lr, wm_decay_base, gate_gain, age_slope]
        - lr_rl: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (>0), scaled by 10 internally
        - wm_lr: WM learning rate (0..1)
        - wm_decay_base: base WM decay toward prior (0..1)
        - gate_gain: sensitivity of arbitration to RL uncertainty (>=0)
        - age_slope: increases WM decay and arbitration threshold for older group (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_rl, softmax_beta, wm_lr, wm_decay_base, gate_gain, age_slope = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM decay increases with set size and age
        size_factor = 3.0 / float(nS)  # 1.0 for 3, 0.5 for 6
        wm_decay_eff = wm_decay_base + (1.0 - size_factor) * wm_decay_base + age_slope * age_group * (1.0 - wm_decay_base)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax policy and entropy
            Q_s = q[s, :]
            Q_centered = softmax_beta * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(Q_centered))
            # Full RL policy for entropy
            logits = softmax_beta * Q_s
            denom = np.sum(np.exp(logits))
            pi = np.exp(logits) / max(denom, 1e-12)
            # Normalized entropy in [0,1]
            H = -np.sum(pi * (np.log(pi + 1e-12))) / np.log(nA)

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: more WM when RL is uncertain (high entropy)
            # Age increases arbitration threshold (older rely less on WM for a given entropy)
            threshold = 0.5 + 0.25 * (1.0 - size_factor) + 0.25 * age_group
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_gain * (H - threshold)))
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM global decay toward prior, then local learning
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0
            w[s, a] += wm_lr * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM with slot-like gating, noisy encoding, and lapse errors modulated by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr, softmax_beta, slots_base, wm_enc_noise, lapse_base, age_lapse_scale]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature (>0), scaled by 10 internally
        - slots_base: baseline number of WM slots (0..6)
        - wm_enc_noise: WM encoding noise/leak (0..1); higher = noisier and faster decay
        - lapse_base: base lapse probability (0..1)
        - age_lapse_scale: multiplicative increase of lapse for older group (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, slots_base, wm_enc_noise, lapse_base, age_lapse_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective number of WM slots scales down with set size and age
        size_factor = 3.0 / float(nS)
        slots_eff = slots_base * size_factor * (1.0 - 0.5 * age_group)
        slots_eff = np.clip(slots_eff, 0.0, float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Slot-like gating: WM dominates only for a subset of states (early-learned or prioritized)
            # Here we approximate priority by state index; smoother gating via sigmoid around slots_eff.
            # Weight ~1 when s index within slots_eff, else ~0.
            gate = 1.0 / (1.0 + np.exp(5.0 * (s - (slots_eff - 0.5))))  # sharp transition at slots_eff
            p_mix = gate * p_wm + (1.0 - gate) * p_rl

            # Lapse probability increases with set size and age
            epsilon = lapse_base * (float(nS) / 3.0) * (1.0 + age_lapse_scale * age_group)
            epsilon = np.clip(epsilon, 0.0, 0.5)
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay scales with set size via wm_enc_noise, then noisy encoding on rewarded trials
            leak = np.clip(wm_enc_noise * (float(nS) / 6.0), 0.0, 1.0)
            w = (1.0 - leak) * w + leak * w_0

            # Noisy one-shot encoding when reward arrives: move toward one-hot with noise
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                target = (1.0 - wm_enc_noise) * target + wm_enc_noise * (1.0 / nA)
                w[s, :] = (1.0 - wm_enc_noise) * w[s, :] + wm_enc_noise * w_0[s, :]
                w[s, :] += 0.5 * (target - w[s, :])  # partial overwrite

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controlled arbitration: WM confidence vs RL prediction error, with age-dependent temperature
    and set-size-dependent RL learning rate.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6); constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr_base, softmax_beta, wm_lr, meta_sensitivity, age_temp_penalty, size_lr_penalty]
        - lr_base: baseline RL learning rate (0..1)
        - softmax_beta: baseline inverse temperature (>0), scaled by 10 internally
        - wm_lr: WM learning rate (0..1)
        - meta_sensitivity: arbitration sensitivity to (WM confidence - |RL PE|) (>=0)
        - age_temp_penalty: temperature reduction factor for older group (>=0)
        - size_lr_penalty: reduces RL learning as set size increases (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_base, softmax_beta, wm_lr, meta_sensitivity, age_temp_penalty, size_lr_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age-dependent temperature (older -> lower effective beta via penalty)
        beta_eff = softmax_beta * np.exp(-age_temp_penalty * age_group)

        # Set-size-dependent RL learning rate
        size_penalty = 1.0 + size_lr_penalty * max(0.0, (nS - 3) / 3.0)
        lr_eff = lr_base / size_penalty
        lr_eff = np.clip(lr_eff, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay increases with set size and age (non-parametric adjustment in addition to meta-control)
        base_decay = 1.0 - (3.0 / float(nS))  # 0 for 3, 0.5 for 6
        wm_decay_eff = np.clip(base_decay + 0.2 * age_group, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy and confidence
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            wm_conf = np.max(W_s) - np.min(W_s)  # 0..1 measure of WM sharpness

            # Arbitration: more WM when WM is confident and RL PE is small
            pe = abs(r - Q_s[a])
            arb_signal = wm_conf - pe  # positive -> favor WM
            wm_weight = 1.0 / (1.0 + np.exp(-meta_sensitivity * arb_signal))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta

            # WM decay and update
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0
            w[s, a] += wm_lr * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p