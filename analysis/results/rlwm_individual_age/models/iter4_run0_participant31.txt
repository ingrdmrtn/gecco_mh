def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dynamic arbitration driven by WM match vs RL uncertainty, with age- and set-size-modulated WM encoding.

    Controller summary:
    - RL: Standard Rescorla-Wagner with single learning rate and softmax.
    - WM: One-shot associative memory that encodes the rewarded action for a state
          with a probability that declines with set size and with older age.
          WM traces decay toward uniform with an age- and set-size-scaled rate.
    - Arbitration: Trial-wise mixture weight determined by a logistic function of
          WM match strength (how peaked WM is) minus RL uncertainty (entropy of
          softmax over Q). This enables greater WM reliance when WM is confident
          and RL is uncertain.

    Parameters
    ----------
    states : array-like
        State index per trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward (0/1) per trial.
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, softmax_beta, wm_encode_base, wm_decay_base, arb_k, age_decay_mult]
        - lr: RL learning rate (0..1).
        - softmax_beta: Base RL inverse temperature; internally scaled by 10.
        - wm_encode_base: Baseline probability to encode WM after reward.
        - wm_decay_base: Baseline WM decay toward uniform per visit.
        - arb_k: Arbitration sensitivity; larger means more decisive switching.
        - age_decay_mult: Multiplier on WM decay for older vs younger.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_encode_base, wm_decay_base, arb_k, age_decay_mult = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age- and set-size-modulated WM decay and encoding probability
        wm_decay_eff = wm_decay_base * (nS / 3.0) * (1.0 + age_decay_mult * age_group)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)
        wm_encode_p = wm_encode_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_encode_p = np.clip(wm_encode_p, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM match strength vs RL uncertainty (entropy)
            # WM match: deviation from uniform (0 to 2/3)
            wm_match = np.max(W_s) - (1.0 / nA)
            # RL uncertainty: normalized entropy (0..1)
            prl = np.exp(softmax_beta * (Q_s - np.max(softmax_beta * Q_s)))
            prl = prl / np.sum(prl)
            entropy = -np.sum(prl * np.log(np.clip(prl, 1e-12, 1.0)))
            max_entropy = np.log(nA)
            rl_uncert = entropy / max_entropy

            gate = 1.0 / (1.0 + np.exp(-arb_k * (wm_match - rl_uncert)))
            wm_weight_eff = np.clip(gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM encoding if reward delivered
            if r > 0.0:
                if np.random.rand() < wm_encode_p:
                    w[s, :] = w_0[s, :].copy()
                    w[s, a] = 1.0
                    w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Kalman RL with volatility and interference-based WM decay.

    Controller summary:
    - RL: Bayesian (Kalman) value learning per (state,action) with process noise (volatility)
      and observation noise; trial-wise adaptive learning rate (Kalman gain).
    - WM: Recency-based associative trace per state that shifts probability mass
      toward recently rewarded action; decays toward uniform with interference that
      increases with set size and is worse for older age.
    - Arbitration: Fixed mixture weight for WM vs RL policies (but WM strength itself
      is shaped by the recency trace); RL choice via softmax.

    Parameters
    ----------
    states : array-like
        State per trial.
    actions : array-like
        Action per trial (0..2).
    rewards : array-like
        Reward (0/1).
    blocks : array-like
        Block index.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Age (repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [volatility, obs_noise, softmax_beta, wm_weight, interf_slope, age_interf_bonus]
        - volatility: Process noise added to value uncertainty each visit.
        - obs_noise: Observation noise of rewards (affects Kalman gain).
        - softmax_beta: RL inverse temperature, scaled by 10 internally.
        - wm_weight: Mixture weight between WM and RL policies (0..1).
        - interf_slope: Scales WM decay with set size (per visit).
        - age_interf_bonus: Additional interference multiplier for older group.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    volatility, obs_noise, softmax_beta, wm_weight, interf_slope, age_interf_bonus = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Kalman RL variables
        q = (1.0 / nA) * np.ones((nS, nA))
        var = 0.5 * np.ones((nS, nA))  # initial uncertainty

        # WM variables
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM interference-driven decay rate
        wm_decay_eff = np.clip(interf_slope * (nS / 3.0) * (1.0 + age_interf_bonus * age_group), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight * (3.0 / nS) * (1.0 - 0.2 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # Kalman RL update for the chosen (s,a)
            # Predict step: add process noise (volatility)
            var[s, a] += volatility

            # Kalman gain
            K = var[s, a] / (var[s, a] + obs_noise)
            pe = r - q[s, a]
            q[s, a] += K * pe
            var[s, a] = (1.0 - K) * var[s, a]

            # WM: decay toward uniform; strengthen last rewarded action
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0.0:
                # Shift mass toward rewarded action proportionally to its remaining headroom
                gain = 0.8
                w[s, a] += gain * (1.0 - w[s, a])
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with exploration bonus and error-gated WM arbitration.

    Controller summary:
    - RL: Rescorla-Wagner with softmax plus directed exploration bonus inversely
      proportional to visit count; younger participants get a larger exploration bonus.
    - WM: One-shot store of rewarded action; decays toward uniform modestly.
    - Arbitration: WM is engaged more strongly when absolute recent RPE in the state
      is high (indicating RL unreliability) via a sigmoid gate; WM reliance also
      decreases with set size and with older age.

    Parameters
    ----------
    states : array-like
        State per trial.
    actions : array-like
        Action per trial (0..2).
    rewards : array-like
        Reward (0/1).
    blocks : array-like
        Block index.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Age (repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, softmax_beta, wm_weight_base, rpe_gate_gain, explore_bonus]
        - lr: RL learning rate.
        - softmax_beta: RL inverse temperature; scaled by 10 internally.
        - wm_weight_base: Baseline WM mixture weight before gating/modulation.
        - rpe_gate_gain: Sensitivity of WM gate to recent unsigned RPE.
        - explore_bonus: Coefficient for directed exploration bonus 1/sqrt(N).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, rpe_gate_gain, explore_bonus = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for exploration bonus
        N = np.zeros((nS, nA)) + 1.0  # start at 1 to avoid div-by-zero

        # Track recent unsigned RPE per state for gating
        recent_abs_pe = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with directed exploration bonus; younger explore more
            bonus_scale = explore_bonus * (1.0 + 0.5 * (1 - age_group))
            bonus = bonus_scale / np.sqrt(np.maximum(N[s, :], 1.0))
            Q_s = q[s, :].copy() + bonus

            # WM policy
            W_s = w[s, :].copy()

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Error-gated WM arbitration
            gate = 1.0 / (1.0 + np.exp(-rpe_gate_gain * recent_abs_pe[s]))
            wm_weight_eff = wm_weight_base * gate * (3.0 / nS) * (1.0 - 0.3 * age_group)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update recent absolute PE trace (simple exponential filter)
            recent_abs_pe[s] = 0.7 * recent_abs_pe[s] + 0.3 * abs(pe)

            # WM decay and encode on reward
            wm_decay_eff = 0.1 * (nS / 3.0) * (1.0 + 0.4 * age_group)
            wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

            # Increment visit counts
            N[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p