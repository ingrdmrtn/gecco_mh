def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(α, β) with eligibility traces, mixed with capacity-limited WM recall and a lapse process.

    Mechanism
    - RL: standard delta-rule with an eligibility trace that persists within state-action.
    - WM: one-shot update on rewarded trials; decays towards uniform with higher set size.
    - Capacity-limited recall: WM policy is attenuated by a recall probability that falls with set size
      and is reduced for older adults.
    - Final choice: mixture of WM and RL plus a uniform lapse.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_mix, trace_lambda, p_recall_base, lapse]
        - alpha: RL learning rate (0..1)
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_mix: baseline WM/RL mixture weight (0..1)
        - trace_lambda: eligibility trace persistence for RL (0..1)
        - p_recall_base: baseline WM recall probability at set size 3
        - lapse: lapse probability mixing in uniform random choice (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_mix, trace_lambda, p_recall_base, lapse = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    # Age reduces WM recall; here young (age_group=0) => no reduction.
    age_recall_scale = 0.8 if age_group == 1 else 1.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q and WM W
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with possible set-size-induced noise (implicit through scaling below)
            Q_s = q[s, :]
            beta_eff = softmax_beta  # no additional set-size scaling here; WM handles load
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy with capacity-limited recall
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_full = 1.0 / max(denom_wm, 1e-12)

            # Recall probability decreases with set size and age
            # p_recall = p_recall_base at 3; scales as 3/nS for 6
            p_recall = np.clip(p_recall_base * (3.0 / nS) * age_recall_scale, 0.0, 1.0)

            # Effective WM policy: mixture of recalled WM and uniform (if unrecalled)
            p_wm = p_recall * p_wm_full + (1.0 - p_recall) * (1.0 / nA)

            # Combine WM and RL with a mixture; then add lapse
            wm_weight_eff = np.clip(wm_mix, 0.0, 1.0)
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # Decay traces and set current trace
            e *= trace_lambda
            e[s, a] += 1.0
            # Update all Q with traces
            q += alpha * delta * e

            # WM update: decay plus one-shot strengthening on rewarded trials
            # Decay towards uniform; faster with higher set size
            wm_decay = 0.10 * (nS / 3.0)  # 0.10 at 3, 0.20 at 6
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                # Write chosen action strongly, normalize
                write = 0.7
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(α, β) mixed with WM subject to confusion/interference across states.

    Mechanism
    - RL: delta-rule.
    - WM: fast, reward-gated update, but retrieval can confuse the current state with
      other states. Confusion grows with set size and more for older adults.
    - Final choice: WM/RL mixture after WM interference, without lapse.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight, confusion_rate, interference_set_gain, age_penalty]
        - alpha: RL learning rate (0..1)
        - softmax_beta: base inverse temperature for RL (scaled by 10)
        - wm_weight: baseline WM weight in the mixture (0..1)
        - confusion_rate: base probability of retrieving the wrong state from WM (0..1)
        - interference_set_gain: multiplicative increase in confusion from set size (>=0)
        - age_penalty: factor scaling confusion for older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight, confusion_rate, interference_set_gain, age_penalty = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM retrieval with confusion across states
            # Compute WM softmax for current state and for the average of other states
            W_s = w[s, :]
            denom_s = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_s = 1.0 / max(denom_s, 1e-12)

            if nS > 1:
                mean_other = np.mean(np.delete(w, s, axis=0), axis=0)
            else:
                mean_other = w[s, :]

            denom_o = np.sum(np.exp(softmax_beta_wm * (mean_other - mean_other[a])))
            p_wm_o = 1.0 / max(denom_o, 1e-12)

            # Confusion probability increases with set size and age
            set_gain = 1.0 + interference_set_gain * max(0, nS - 3) / 3.0
            age_gain = 1.0 + age_penalty * age_group
            p_confuse = np.clip(confusion_rate * set_gain * age_gain, 0.0, 1.0)

            p_wm = (1.0 - p_confuse) * p_wm_s + p_confuse * p_wm_o

            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM dynamics: decay with load + reward-gated writing
            decay = 0.1 + 0.2 * max(0, nS - 3) / 3.0  # 0.1 at 3, 0.3 at 6
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                write = 0.6
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-controlled WM gating driven by surprise, with entropy-based load penalty.

    Mechanism
    - RL: delta-rule with inverse temperature reduced by set size and (for older adults) by age.
    - WM: one-shot rewarded updates; stable but subject to set-size decay.
    - Meta-control: the WM mixture weight is dynamically adapted by surprise |δ|; higher surprise
      increases WM reliance. A load penalty reduces WM reliance with larger set size. Age reduces
      the meta-learning rate and also temperature through an age penalty.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight0, meta_lr, ent_bonus, age_temp_penalty]
        - alpha: RL learning rate (0..1)
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_weight0: initial WM mixture weight (0..1)
        - meta_lr: learning rate for adapting WM weight toward surprise (0..1)
        - ent_bonus: penalty factor applied to WM weight as set size increases (>=0)
        - age_temp_penalty: reduces RL temperature for older adults (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight0, meta_lr, ent_bonus, age_temp_penalty = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    # Age reduces effective temperature and meta-learning; young unaffected.
    temp_age_scale = 1.0 - age_temp_penalty * age_group
    meta_lr_eff = meta_lr * (0.7 if age_group == 1 else 1.0)

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Dynamic WM weight
        wm_weight_dyn = np.clip(wm_weight0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size cost for decision noise and WM penalty
            set_cost = max(0, nS - 3) / 3.0

            # RL choice with set-size and age effects on beta
            beta_eff = max(1e-6, softmax_beta * temp_age_scale / (1.0 + set_cost))
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM choice
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Effective WM weight penalized by load
            wm_weight_eff = np.clip(wm_weight_dyn * (1.0 - ent_bonus * set_cost), 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and compute surprise for meta-control
            delta = r - Q_s[a]
            q[s, a] += alpha * delta
            surprise = abs(delta)

            # Meta-learn WM weight toward surprise signal
            wm_weight_dyn += meta_lr_eff * (surprise - wm_weight_dyn)
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            # WM dynamics: decay with load and reward-gated writing
            decay = 0.08 + 0.16 * set_cost  # modest decay: 0.08 at 3, 0.24 at 6
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                write = 0.65
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p