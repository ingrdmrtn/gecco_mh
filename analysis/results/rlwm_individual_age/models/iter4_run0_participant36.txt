def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + associative WM with age- and load-modulated mixture and within-state perseveration.

    Summary:
    - RL Q-learning combines with an associative WM store.
    - WM policy is near-deterministic softmax over the currently stored action for a state.
    - WM mixture weight is reduced with set size (3->6) and in older adults.
    - Within-state perseveration bias (repeat last action in that state).
    - Positive outcomes strengthen both RL Q and WM; negative outcomes decay WM toward uniform.

    Parameters (list of 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1] (modulated by age and set size per trial).
    - softmax_beta: base RL inverse temperature; multiplied by 10 internally.
    - kappa: within-state perseveration bias added to the last action's RL logit.
    - rho: reward-sensitivity scaling for positive prediction errors (>0 amplifies learning from rewards).
    - wm_decay: WM decay rate toward uniform on unrewarded trials in [0,1].

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: 1D array with a single repeated value; age_group = 0 (<=45), 1 (>45).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa, rho, wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    nA = 3
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action_state = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy with within-state perseveration bias
            logits_rl = softmax_beta * Q_s
            if last_action_state[s] >= 0:
                logits_rl[last_action_state[s]] += kappa
            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy (near-deterministic)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Age and load modulation of WM mixture
            wm_weight_eff = wm_weight * (1.0 - 0.30 * age_group) * (3.0 / float(nS))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with reward-sensitivity scaling on positive PE
            pe = r - Q_s[a]
            lr_eff = lr * (rho if pe > 0 else 1.0)
            q[s, a] += lr_eff * pe

            # WM update: rewarded -> store one-hot; else decay toward uniform
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Update last action for perseveration
            last_action_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM (Dirichlet counts) with surprise-driven negative feedback and age/load decay.

    Summary:
    - RL Q-learning provides graded values.
    - WM is a per-state Dirichlet over actions; policy uses near-deterministic softmax over log posterior.
    - When rewarded, WM increments chosen action count; when not, distributes "surprise-driven" credit to alternatives.
    - WM mixture weight reduced by set size and age; counts decay toward prior with age/load.
    - Combines WM and RL by mixture.

    Parameters (list of 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1] (modulated by age and set size per trial).
    - softmax_beta: base RL inverse temperature; multiplied by 10 internally.
    - alpha0: Dirichlet prior concentration per action (>0).
    - surprise_gain: scale of negative-feedback redistribution to nonchosen actions (>=0).
    - age_penalty: scales WM decay toward prior with age and set size (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: 1D array with a single repeated value; age_group = 0 (<=45), 1 (>45).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha0, surprise_gain, age_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    nA = 3
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # Dirichlet counts for WM (initialize with prior alpha0)
        counts = alpha0 * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # used only to compute uniform if needed

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # WM posterior over actions
            post = counts[s, :].copy()
            post = np.maximum(post, 1e-12)
            W_s = post / np.sum(post)

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy using log-probs as logits
            logits_wm = softmax_beta_wm * (np.log(W_s + 1e-12))
            denom_wm = np.sum(np.exp(logits_wm - logits_wm[a]))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Age/load modulation of WM mixture
            wm_weight_eff = wm_weight * (1.0 - 0.35 * age_group) / (1.0 + 0.5 * (nS - 3.0))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: reward -> increment chosen; no reward -> redistribute to others by surprise
            if r > 0.0:
                counts[s, a] += 1.0
            else:
                surprise = 1.0 - W_s[a]
                incr = 0.0
                if nA > 1:
                    incr = surprise_gain * surprise / (nA - 1.0)
                for a2 in range(nA):
                    if a2 != a:
                        counts[s, a2] += incr

            # WM decay toward prior with age and load each trial
            decay = np.clip(age_penalty * age_group * max(0.0, (nS - 3.0) / 3.0), 0.0, 1.0)
            counts[s, :] = (1.0 - decay) * counts[s, :] + decay * alpha0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-gated mixture, age/load-dependent WM forgetting, and RL lapse.

    Summary:
    - RL Q-learning with inverse temperature and an age/load-driven lapse (uniform noise).
    - WM is an associative store with decay; policy is near-deterministic softmax over WM weights.
    - Mixture weight is wm_weight scaled by a dynamic gate based on entropy difference (RL vs WM).
    - Gate is more conservative with higher set size and in older adults via parameters.

    Parameters (list of 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1] (multiplied by dynamic uncertainty gate).
    - softmax_beta: base RL inverse temperature; multiplied by 10 internally.
    - gate_sensitivity: scales entropy-difference gating (higher -> stronger gating).
    - wm_forget: base WM decay rate toward uniform (scaled up by age and set size).
    - noise_floor: base RL lapse rate mixed with uniform, scaled by age and set size.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: 1D array with a single repeated value; age_group = 0 (<=45), 1 (>45).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_sensitivity, wm_forget, noise_floor = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    nA = 3
    log_nA = np.log(nA)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute age/load scalers
        load_scale = 1.0 + (nS - 3.0) / 3.0  # 1 for 3, ~1.999 for 6
        forget_rate = np.clip(wm_forget * (1.0 + 0.5 * age_group) * load_scale, 0.0, 1.0)
        lapse_rate = np.clip(noise_floor * (age_group + 0.5 * (nS - 3.0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probabilities (for entropy and chosen prob)
            logits_rl = softmax_beta * Q_s
            max_logit = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_logit)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = max(p_rl_vec[a], 1e-300)

            # Apply lapse to RL chosen probability (mixture with uniform)
            p_rl = (1.0 - lapse_rate) * p_rl + lapse_rate * (1.0 / nA)

            # WM near-deterministic softmax
            logits_wm = softmax_beta_wm * W_s
            max_w = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_w)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = max(p_wm_vec[a], 1e-300)

            # Uncertainty gate based on entropy difference
            # Entropy of RL and WM (in nats)
            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))
            gate_input = gate_sensitivity * (H_rl - H_wm) / max(log_nA, 1e-12)
            # Age/load conservative factor reduces gate strength
            gate_scale = 1.0 / (1.0 + 0.5 * age_group + 0.5 * (nS - 3.0))
            g = 1.0 / (1.0 + np.exp(-gate_scale * gate_input))
            wm_weight_eff = np.clip(wm_weight * g, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update with age/load-dependent forgetting
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = (1.0 - forget_rate) * w[s, :] + forget_rate * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p