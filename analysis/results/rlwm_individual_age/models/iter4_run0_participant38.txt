Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) with age- and set-size–sensitive mechanisms. Each function returns the negative log-likelihood of the observed choices.

Note: Assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM arbitration with uncertainty-driven WM gating, age- and set-size–dependent WM precision,
    and a small lapse component.

    Mechanisms:
    - RL: tabular Q-learning with softmax policy.
    - WM: one-shot storage of rewarded mappings, with precision decreasing with set size and age.
      WM contents decay towards uniform when not reinforced.
    - Arbitration: trial-wise mixture weight shifts toward the system with higher confidence (lower
      entropy), with an age penalty on WM reliance.
    - Lapse: small probability of a uniform random choice.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta: base inverse temperature for RL softmax (scaled x10 internally)
    - wm_base: baseline WM mixture bias (can be any real; passed through sigmoid inside)
    - wm_precision: base WM precision scaler (>=0) controlling WM softmax temperature
    - age_wm_penalty: scales the penalty on WM reliance for older adults and large set sizes (>=0)
    - lapse: lapse probability (0..0.2 recommended); mixed with uniform policy

    Age and set-size effects:
    - WM precision: beta_wm = 50 * wm_precision / (1 + (nS/3) * (1 + age_group))
    - WM mixture weight: starts from sigmoid(wm_base) and is adjusted by confidence difference
      (WM vs RL) and penalized by age_wm_penalty * age_group * (nS/3).
    - WM decay: increases with set size and age: decay = 0.05 * (nS/3) * (1 + 0.5*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_base, wm_precision, age_wm_penalty, lapse = model_parameters
    beta_eff = beta * 10.0
    lapse = np.clip(lapse, 0.0, 0.5)

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age-dependent WM parameters
        beta_wm = 50.0 * max(0.0, wm_precision) / (1.0 + (nS / 3.0) * (1.0 + 1.0 * age_group))
        beta_wm = max(1.0, beta_wm)
        wm_decay = np.clip(0.05 * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.5)

        # Baseline WM mixture bias
        base_gate = 1.0 / (1.0 + np.exp(-wm_base))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (softmax)
            Q_s = q[s, :].copy()
            logits_rl = beta_eff * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl)
            probs_rl /= max(np.sum(probs_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)

            # WM policy (softmax over WM weights)
            W_s = w[s, :].copy()
            logits_wm = beta_wm * (W_s - np.max(W_s))
            probs_wm = np.exp(logits_wm)
            probs_wm /= max(np.sum(probs_wm), 1e-12)
            p_wm = max(probs_wm[a], 1e-12)

            # Confidence (1 - normalized entropy)
            def conf_from_probs(p):
                H = -np.sum(p * np.log(np.clip(p, 1e-12, 1.0)))
                Hmax = np.log(nA)
                return max(0.0, 1.0 - H / Hmax)

            conf_rl = conf_from_probs(probs_rl)
            conf_wm = conf_from_probs(probs_wm)

            # Arbitration: shift baseline weight toward higher confidence system,
            # penalize WM reliance by age and set size
            penalty = max(0.0, age_wm_penalty) * age_group * (nS / 3.0)
            wm_weight = base_gate + 0.5 * (conf_wm - conf_rl) - penalty
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay towards uniform
            w = (1.0 - wm_decay) * w + wm_decay * w0
            # Reward-gated WM write: stronger when rewarded
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend new memory into WM trace
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-rate RL with forgetting and slot-limited WM capacity.

    Mechanisms:
    - RL: Q-learning with separate learning rates for positive vs negative prediction errors.
    - RL forgetting: Q-values decay toward uniform each trial; decay increases with set size and age.
    - WM: limited number of state slots (capacity) that can be actively stored with near-deterministic retrieval.
      Capacity decreases with set size and for older adults.
    - Arbitration: mixture weight equals fraction of states that can be held (capacity / set size).

    Parameters (model_parameters):
    - lr_pos: learning rate for positive PEs (0..1)
    - lr_neg: learning rate for negative PEs (0..1)
    - beta: inverse temperature for RL softmax (scaled x10 internally)
    - forget_base: base RL forgetting rate per trial (>=0)
    - cap_base: baseline WM capacity (in states; >=0)
    - age_forget_boost: additional forgetting scaling for older adults (>=0)

    Age and set-size effects:
    - RL forgetting per trial: forget = forget_base * (nS/3) * (1 + age_forget_boost*age_group)
    - WM capacity: cap = max(0, cap_base - 1.0*age_group); effective wm_weight = min(1, cap/nS)
      Older adults and larger set sizes reduce the proportion of states supported by WM.
    - WM policy temperature is high (beta_wm=50) when state is in WM; otherwise uniform.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta, forget_base, cap_base, age_forget_boost = model_parameters
    beta_eff = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM storage: which states are currently stored and their one-hot preferred action
        stored = np.zeros(nS, dtype=bool)
        wm_pref = -np.ones(nS, dtype=int)

        # Capacity and forgetting settings
        cap = max(0.0, cap_base - 1.0 * age_group)
        wm_weight = np.clip(cap / max(1.0, nS), 0.0, 1.0)
        forget = max(0.0, forget_base) * (nS / 3.0) * (1.0 + max(0.0, age_forget_boost) * age_group)
        forget = np.clip(forget, 0.0, 0.5)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            logits_rl = beta_eff * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl)
            probs_rl /= max(np.sum(probs_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)

            # WM policy
            if stored[s] and wm_pref[s] >= 0:
                # Near-deterministic retrieval
                W_s = np.zeros(nA)
                W_s[wm_pref[s]] = 1.0
                logits_wm = 50.0 * (W_s - np.max(W_s))
                probs_wm = np.exp(logits_wm)
                probs_wm /= max(np.sum(probs_wm), 1e-12)
            else:
                probs_wm = np.ones(nA) / nA
            p_wm = max(probs_wm[a], 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL forgetting toward uniform
            q = (1.0 - forget) * q + forget * (1.0 / nA)

            # RL update with dual rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM storage/update: if rewarded, attempt to store this state-action
            if r > 0.5:
                # If capacity not exceeded, store; else replace a random stored state with probability
                if np.sum(stored) < int(np.floor(cap + 1e-9)):
                    stored[s] = True
                    wm_pref[s] = a
                else:
                    # Simple replacement: with small probability replace an existing stored state
                    # Prefer to keep recently reinforced states; here we just probabilistically overwrite
                    replace_prob = 0.25
                    if np.random.rand() < replace_prob:
                        idxs = np.where(stored)[0]
                        if len(idxs) > 0:
                            j = idxs[np.random.randint(len(idxs))]
                            stored[j] = False
                        stored[s] = True
                        wm_pref[s] = a
            else:
                # If repeatedly not rewarded, occasionally drop from WM (age and set size implicit via lower wm_weight)
                drop_prob = 0.05 * (nS / 3.0) * (1.0 + 0.5 * age_group)
                if stored[s] and (np.random.rand() < drop_prob):
                    stored[s] = False
                    wm_pref[s] = -1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controlled RL temperature with WM interference.

    Mechanisms:
    - RL: Q-learning with softmax; inverse temperature is dynamically modulated by recent reward rate
      and age (older adults show lower dynamic gain).
    - WM: stores rewarded state-action pairs, but suffers interference that spreads value mass to
      non-chosen actions; interference scales with set size and age.
    - Arbitration: fixed WM mixture baseline scaled by set size; combined with RL policy.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: base RL inverse temperature (scaled x10 internally)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - interference: base cross-action interference in WM (>=0)
    - meta_gain: gain on RL temperature based on running reward rate (>=0)
    - age_meta_bias: reduces RL temperature for older adults (>=0)

    Age and set-size effects:
    - RL temperature per trial: beta_t = (beta_base*10) * (1 + meta_gain*(rbar - 0.5) - age_meta_bias*age_group)
      where rbar is the running average reward in the current block.
    - WM interference: spread = interference * (nS/3) * (1 + age_group); this pushes WM traces toward
      other actions, degrading WM precision in larger sets and older adults.
    - WM mixture: wm_weight = wm_weight_base * (3/nS), capped to [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_weight_base, interference, meta_gain, age_meta_bias = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight = np.clip(wm_weight_base * (3.0 / max(1.0, nS)), 0.0, 1.0)
        spread_base = max(0.0, interference) * (nS / 3.0) * (1.0 + 1.0 * age_group)
        beta_base_eff = beta_base * 10.0
        meta_gain = max(0.0, meta_gain)
        age_meta_bias = max(0.0, age_meta_bias)

        # Running reward rate
        rbar = 0.5

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL temperature modulation
            beta_t = beta_base_eff * (1.0 + meta_gain * (rbar - 0.5) - age_meta_bias * age_group)
            beta_t = max(0.1, beta_t)

            # RL policy
            Q_s = q[s, :].copy()
            logits_rl = beta_t * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl)
            probs_rl /= max(np.sum(probs_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)

            # WM policy (deterministic-ish softmax over WM weights with fixed high temperature)
            logits_wm = 50.0 * (w[s, :] - np.max(w[s, :]))
            probs_wm = np.exp(logits_wm)
            probs_wm /= max(np.sum(probs_wm), 1e-12)
            p_wm = max(probs_wm[a], 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update with interference:
            # - Always decay slightly toward uniform
            # - If rewarded, boost chosen action but spread some mass to other actions
            w = 0.95 * w + 0.05 * w0
            if r > 0.5:
                spread = np.clip(spread_base, 0.0, 0.8)
                boost = 1.0 - spread
                # Distribute spread equally over non-chosen actions
                w[s, :] = (1.0 - 0.5) * w[s, :]  # partial overwrite to keep some history
                w[s, a] += 0.5 * boost
                others = [i for i in range(nA) if i != a]
                w[s, others] += (0.5 * spread) / (nA - 1)

            # Update running reward rate
            rbar = 0.9 * rbar + 0.1 * r

        blocks_log_p += log_p

    return -blocks_log_p

How age and set size are used across models:
- Model 1: Older age and larger set sizes reduce WM precision and increase WM decay; arbitration penalizes WM reliance more for older adults and larger sets.
- Model 2: Older age increases RL forgetting; WM capacity effectively lowers for older adults, and the arbitration weight equals capacity/set size, thus shrinking with larger set sizes.
- Model 3: Older age reduces the meta-controlled RL temperature; WM interference increases with set size and age, degrading WM selectivity; WM mixture is reduced for larger sets.