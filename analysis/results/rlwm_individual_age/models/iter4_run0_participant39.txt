def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) with age- and load-dependent effective capacity.

    Mechanism:
    - RL: tabular Q-learning with softmax choice.
    - WM: stores the last rewarded action per state and generates a near-deterministic policy.
    - Effective WM recall probability increases when set size <= capacity and decreases otherwise.
      Older age reduces effective capacity.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight for WM vs RL.
    - softmax_beta: float > 0, RL inverse temperature (will be multiplied by 10 by template rule).
    - K_cap: float > 0, nominal WM capacity (in number of state-action mappings).
    - wm_decay: float in [0,1], per-trial decay rate of WM traces toward uniform.
    - age_capacity_penalty: float in [0,1], proportional capacity reduction for older group.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K_cap, wm_decay, age_capacity_penalty = model_parameters
    softmax_beta *= 10  # per template scaling

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # near-deterministic WM softmax
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM representations
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM policy distribution per state
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # baseline (uniform)

        # For WM recall: track last rewarded action per state (-1 = none yet)
        last_rewarded = -1 * np.ones(nS, dtype=int)

        # Age- and load-dependent effective capacity (bounded â‰¥ 0)
        K_eff = max(0.0, K_cap * (1.0 - age_capacity_penalty * age_group))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # Recall probability increases when nS <= K_eff; otherwise decreases smoothly.
            # We use a sigmoid-like mapping via a soft threshold around K_eff.
            # p_recall = 1 / (1 + exp((nS - K_eff))) approximated by a bounded rational form:
            p_recall = 1.0 / (1.0 + np.exp(nS - K_eff))  # in (0,1)
            p_recall = np.clip(p_recall, 0.0, 1.0)

            if last_rewarded[s] >= 0:
                # WM distribution is a softmax over w[s], which we keep peaked at remembered action
                W_s = w[s, :]
                denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm_det = 1.0 / max(denom_wm, eps)
                # Mixture within WM: either recall the stored mapping or fallback to uniform if recall fails
                p_wm = p_recall * p_wm_det + (1.0 - p_recall) * (1.0 / nA)
            else:
                # No memory yet
                p_wm = 1.0 / nA

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then imprint if rewarded
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # If rewarded, set sharp peak on chosen action
            if r == 1:
                last_rewarded[s] = a
                w[s, :] = 0.0
                w[s, a] = 1.0
            # Normalize for safety
            w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age/load-modulated inverse temperature + WM with confusion noise.

    Mechanism:
    - RL: tabular Q-learning. The effective RL gain (like inverse temperature) is reduced
      by older age and larger set size, implemented by scaling Q_s before softmax.
    - WM: stores last rewarded action per state; when retrieved, assigns (1 - wm_confusion)
      probability to the remembered action and spreads wm_confusion across other actions.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight0: float in [0,1], base WM/RL mixture weight (kept constant here).
    - softmax_beta: float > 0, base RL inverse temperature (will be multiplied by 10).
    - beta_age_pen: float >= 0, multiplicative penalty on RL selectivity for older participants.
    - beta_load_pen: float >= 0, multiplicative penalty exponent for larger set sizes (nS/3)^beta_load_pen.
    - wm_confusion: float in [0,1], probability mass that WM misallocates away from the remembered action.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, beta_age_pen, beta_load_pen, wm_confusion = model_parameters
    softmax_beta *= 10  # per template scaling

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # not directly used; WM is specified via confusion distribution
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # will track WM peaked distributions
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last rewarded action (-1 if none)
        remembered = -1 * np.ones(nS, dtype=int)

        # Precompute RL beta modulation factor for this block
        load_factor = (nS / 3.0) ** max(beta_load_pen, 0.0)  # >=1 for nS >= 3
        age_factor = 1.0 / (1.0 + beta_age_pen * age_group)  # <=1 if old and beta_age_pen>0
        beta_scale = age_factor / load_factor                 # reduces effective selectivity

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Scale Q_s to emulate beta modulation without altering the template's beta usage
            Q_s = q[s, :] * beta_scale

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with confusion:
            if remembered[s] >= 0:
                p_wm = (1.0 - wm_confusion) if a == remembered[s] else (wm_confusion / (nA - 1))
            else:
                p_wm = 1.0 / nA

            # Mixture
            p_total = wm_weight0 * p_wm + (1.0 - wm_weight0) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: keep a peaked distribution when rewarded, else decay slightly
            if r == 1:
                remembered[s] = a
                w[s, :] = np.full(nA, wm_confusion / (nA - 1))
                w[s, a] = 1.0 - wm_confusion
            else:
                # gentle decay toward uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                w[s, :] /= max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Win-Stay/Lose-Shift (WSLS) heuristic as WM policy, with age/load bias on heuristic strength.

    Mechanism:
    - RL: tabular Q-learning with softmax choice.
    - WM/Heuristic: if the last encounter of a state was rewarded, stay with that action
      with probability ws_prob; if it was not rewarded, switch away with probability ls_prob.
    - Older age and larger set size reduce the strength of the heuristic via a bias parameter.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight for WM (heuristic) vs RL.
    - softmax_beta: float > 0, RL inverse temperature (will be multiplied by 10).
    - ws_prob: float in [0,1], base probability to repeat last action after reward (win-stay).
    - ls_prob: float in [0,1], base probability to switch after no reward (lose-shift).
    - age_load_bias: float >= 0, reduces heuristic strength with age and load.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, ws_prob, ls_prob, age_load_bias = model_parameters
    softmax_beta *= 10  # per template scaling

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # not used directly; heuristic defines p_wm
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # auxiliary WM trace (decays/peaks on reward)
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and last outcome for each state
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1 * np.ones(nS, dtype=int)  # -1 unknown, 0 or 1 known

        # Compute heuristic attenuation by age and load (0=no attenuation, larger=more reduction)
        heuristic_scale = 1.0 / (1.0 + age_load_bias * (age_group + (nS - 3) / 3.0))
        heuristic_scale = np.clip(heuristic_scale, 0.0, 1.0)

        # Effective ws/ls after attenuation
        ws_eff = np.clip(ws_prob * heuristic_scale, 0.0, 1.0)
        ls_eff = np.clip(ls_prob * heuristic_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM/Heuristic policy:
            if last_action[s] == -1 or last_reward[s] == -1:
                # No history
                p_wm = 1.0 / nA
            else:
                if last_reward[s] == 1:
                    # Win-Stay: repeat last_action with prob ws_eff
                    if a == last_action[s]:
                        p_wm = ws_eff + (1.0 - ws_eff) * (1.0 / nA)
                    else:
                        p_wm = (1.0 - ws_eff) * (1.0 / nA)
                else:
                    # Lose-Shift: pick any action except last_action with prob ls_eff split evenly
                    if a == last_action[s]:
                        p_wm = (1.0 - ls_eff) * (1.0 / nA)
                    else:
                        # distribute ls_eff across the two alternative actions
                        p_wm = (ls_eff / (nA - 1)) + (1.0 - ls_eff) * (1.0 / nA)

            # Mixture of heuristic and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update history for heuristic
            last_action[s] = a
            last_reward[s] = r

            # WM trace update (auxiliary): decay toward uniform, then peak on rewarded action
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0
            w[s, :] /= max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p