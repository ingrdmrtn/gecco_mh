Below are three distinct cognitive models (RL+WM variants) that explain choices in the rlwm task. Each function:
- Matches the required signature and returns the negative log-likelihood.
- Uses all parameters meaningfully (≤ 6 per model).
- Implements a block-wise RL–WM arbitration with set size and age effects.
- Fills in the WM policy and WM updating within the given template structure.

Note: Assumes numpy as np is already imported (no imports inside functions).

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age/load-dependent arbitration and lapses.

    Idea:
    - RL learns Q-values with a standard delta rule and softmax policy.
    - WM is capacity-limited: only a fraction of states can be effectively represented.
      Effective encoding strength is proportional to K/nS (capped at 1).
    - Arbitration weight is down-weighted by age group (older=1) and grows with capacity coverage (K/nS).
    - Lapse rate increases with load (set size) and with age.
    - WM decays toward uniform on every trial; rewarded trials push WM toward the chosen action.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: base arbitration weight (0..1) for WM before load/age scaling.
    - softmax_beta: base RL inverse temperature; internally scaled by 10.
    - K: WM capacity (in number of items; positive). Effective participation ~ min(1, K/nS).
    - lapse_base: base lapse scale; lapse_t = sigmoid(lapse_base * load), load = age_group + (nS-3)/3.
    - encode_gain: WM encoding gain (0..1). Also sets a complementary decay magnitude.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, K, lapse_base, encode_gain = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load term guides lapse and arbitration scaling
        load = age_group + max(nS - 3, 0) / 3.0

        # Effective capacity coverage
        coverage = float(np.clip(K / max(nS, 1), 0.0, 1.0))

        # Arbitration base: down-weighted by age; up-weighted by coverage
        wm_weight_block = wm_weight_base * coverage * (1.0 - 0.35 * age_group)
        wm_weight_block = float(np.clip(wm_weight_block, 0.0, 1.0))

        # Lapse rate grows with load; sigmoid squashes to 0..1
        lapse_t = 1.0 / (1.0 + np.exp(-lapse_base * load))
        lapse_t = float(np.clip(lapse_t, 0.0, 1.0))

        # WM decay increases when encode_gain is low and load is high
        wm_decay = (1.0 - encode_gain) * (1.0 + 0.5 * load)
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM softmax policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mix with arbitration and lapse
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse_t) * p_mix + lapse_t * (1.0 / nA)
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform every trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-driven encoding toward chosen action with strength set by encode_gain and coverage
            if encode_gain > 0.0 and r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                encode_alpha = float(np.clip(encode_gain * coverage, 0.0, 1.0))
                w[s, :] = (1.0 - encode_alpha) * w[s, :] + encode_alpha * onehot

            # Renormalize
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Dirichlet-like WM counting with age/load-suppressed WM arbitration.

    Idea:
    - RL: standard delta rule and softmax.
    - WM: maintains normalized pseudo-counts over actions per state (Dirichlet-like).
      Successful trials increment the chosen action; WM starts with concentration 'wm_concentration'.
    - Arbitration weight decreases exponentially with age and set size.
    - WM updates at a rate 'wm_update_rate', with implicit regularization by concentration.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: base RL inverse temperature; internally scaled by 10.
    - wm_weight_base: base WM arbitration weight (0..1) before age/size modulation.
    - wm_concentration: initial Dirichlet concentration per action (>0), controls prior strength.
    - wm_update_rate: rate to increment WM counts on rewarded trials (>=0).
    - weight_drop: non-negative factor; wm_weight *= exp(-weight_drop * load),
                   load = age_group + (nS-3)/3.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_concentration, wm_update_rate, weight_drop = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM as Dirichlet-like pseudo-counts per state-action; start with symmetric prior
        counts = wm_concentration * np.ones((nS, nA))
        # Convert counts to probabilities W by normalization
        w = counts / counts.sum(axis=1, keepdims=True)
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load = age_group + max(nS - 3, 0) / 3.0
        # Exponential suppression of WM arbitration with age and load
        wm_weight_block = wm_weight_base * np.exp(-weight_drop * load)
        wm_weight_block = float(np.clip(wm_weight_block, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy from normalized counts
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Arbitration
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: increase counts for the chosen action on rewarded trials
            if wm_update_rate > 0.0 and r > 0:
                counts[s, a] += wm_update_rate

            # Recompute W_s by normalizing counts row
            row_sum = counts[s, :].sum()
            if row_sum > 0:
                w[s, :] = counts[s, :] / row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM as a win-stay trace with age/load-sensitive decay and arbitration.

    Idea:
    - RL: standard delta rule and softmax.
    - WM: encodes a strong preference for the last rewarded action in each state (win-stay trace).
      After reward, WM for that state shifts toward the chosen action.
      After no reward, WM softly diffuses toward uniform (lose-shift tendency).
    - WM decay increases with age and set size; arbitration is reduced under higher load.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: base RL inverse temperature; internally scaled by 10.
    - wm_weight_base: base WM arbitration weight (0..1) before load scaling.
    - ws_strength: strength (0..1) to push WM toward the chosen action on rewarded trials.
    - ws_decay_base: base WM decay toward uniform each trial (0..1).
    - load_gain: non-negative; scales both arbitration reduction and decay increase with load.
                 load = age_group + (nS-3)/3.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, ws_strength, ws_decay_base, load_gain = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load = age_group + max(nS - 3, 0) / 3.0

        # Arbitration weight reduced with load
        wm_weight_block = wm_weight_base / (1.0 + load_gain * load)
        wm_weight_block = float(np.clip(wm_weight_block, 0.0, 1.0))

        # Decay increases with load
        wm_decay = ws_decay_base * (1.0 + load_gain * load)
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Arbitration
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Decay toward uniform each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) If rewarded, push toward chosen action (win-stay trace)
            if r > 0 and ws_strength > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - ws_strength) * w[s, :] + ws_strength * onehot
            # 3) If not rewarded, induce mild lose-shift by slightly de-emphasizing chosen action
            elif r <= 0 and wm_decay < 1.0:
                # redistribute a small portion from chosen action to others
                lose_shift = 0.5 * wm_decay  # magnitude tied to decay
                lose_shift = float(np.clip(lose_shift, 0.0, 1.0))
                decrement = lose_shift * w[s, a]
                w[s, a] -= decrement
                redistribute = decrement / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            # Renormalize
            row_sum = w[s, :].sum()
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p