def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-adaptive mixing and age/load-dependent interference.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr, softmax_beta, wm_bias, k_uncert, k_age_size, beta_gain]
        - lr: RL learning rate.
        - softmax_beta: inverse temperature for RL policy; internally scaled by *10.
        - wm_bias: baseline log-odds for WM mixture weight.
        - k_uncert: decreases WM mixture weight when RL is uncertain (higher state entropy/uncertainty).
        - k_age_size: additional reduction of WM weight as age group and set size increase.
                      Applied to log-odds: -(age_group)*(nS-3)*k_age_size.
        - beta_gain: increases RL inverse temperature when RL is confident (low uncertainty).
                     beta_eff = softmax_beta * (1 + beta_gain * confidence).
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    
    Notes
    -----
    - Set size (nS) reduces WM contribution via the k_age_size term and increases interference
      in WM updates (through stronger decay when nS is larger).
    - Age group (0=young, 1=old) reduces WM contribution via the same k_age_size term.
    """
    lr, softmax_beta, wm_bias, k_uncert, k_age_size, beta_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM assumed to be sharp when item is present
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track counts for uncertainty estimation (state-action visitation)
        counts = np.ones((nS, nA))  # start with 1 to avoid zero division

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Uncertainty: use normalized entropy over Q_s as proxy (softmax over Q as policy proxy)
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / np.sum(pi_rl)
            entropy = -np.sum(pi_rl * np.log(np.maximum(pi_rl, 1e-12)))
            max_entropy = np.log(nA)
            uncertainty = entropy / max_entropy  # 0..1
            confidence = 1.0 - uncertainty

            # RL policy with confidence-adjusted beta
            beta_eff = softmax_beta * (1.0 + beta_gain * confidence)
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM mixture weight: logistic with reductions by uncertainty and age*load
            wm_logit = wm_bias - k_uncert * uncertainty - k_age_size * (age_group * (nS - 3))
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            # WM policy (sharply peaked on stored action, if any)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM updating: interference-dependent decay + overwrite on reward
            # Larger set sizes and older age lead to stronger decay (more interference)
            decay_base = 0.85
            extra_interference = 0.03 * (nS - 3) + 0.02 * age_group
            decay = np.clip(decay_base - extra_interference, 0.5, 0.99)
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            if r > 0.0:
                # On successful feedback, encode item strongly
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

            # Update counts for uncertainty tracking
            counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with reward-contingent choice kernel (win-stay/lose-shift bias) and age/load
    shifting reliance toward the kernel rather than WM.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr, softmax_beta, kappa_win, kappa_lose, wm_gate, age_load_to_kernel]
        - lr: RL learning rate.
        - softmax_beta: inverse temperature for RL policy; internally scaled by *10.
        - kappa_win: increment to choice kernel for chosen action after reward=1 (win-stay).
        - kappa_lose: decrement to choice kernel for chosen action after reward=0 (lose-shift).
        - wm_gate: baseline log-odds controlling WM mixture weight.
        - age_load_to_kernel: age*load increases reliance on kernel and reduces WM weight.
                              WM log-odds -= age_load_to_kernel * (age_group * (nS - 3));
                              kernel influence += same factor.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    
    Notes
    -----
    - Set size (nS) and age group jointly modulate the WM weight downward and shift policy
      toward habit-like choice kernel influence.
    """
    lr, softmax_beta, kappa_win, kappa_lose, wm_gate, age_load_to_kernel = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Choice kernel (state-independent bias over actions)
        kernel = np.zeros(nA)

        # Age*load influence value
        bias_shift = age_load_to_kernel * (age_group * (nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy using Q plus kernel bias
            Q_s = q[s, :]
            Q_eff = Q_s + kernel  # additive bias
            denom_rl = np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))
            p_rl = 1.0 / denom_rl

            # WM weight reduced by age*load
            wm_logit = wm_gate - bias_shift
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Total policy: combine WM and RL (which already contains kernel bias)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM update with mild decay + overwrite on reward
            decay = 0.85
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

            # Choice kernel update (win-stay / lose-shift)
            # Bias_shift further increases the kernel's absolute magnitude under age*load
            if r > 0.0:
                kernel[a] += (kappa_win + bias_shift)
            else:
                kernel[a] -= (kappa_lose + bias_shift)

            # Normalize kernel softly to avoid unbounded growth
            kernel = np.clip(kernel, -5.0, 5.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value forgetting and WM capacity-limited encoding with age/load-scaling.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr, softmax_beta, q_decay, wm_capacity_base, age_size_interference, wm_persistence]
        - lr: RL learning rate.
        - softmax_beta: inverse temperature for RL policy; internally scaled by *10.
        - q_decay: per-trial decay toward uniform for Q-values (captures forgetting/interference).
        - wm_capacity_base: baseline log-odds of successfully encoding into WM at set size 3, young.
        - age_size_interference: decreases WM encoding log-odds as age group and set size increase:
                                 wm_logit -= age_size_interference * (age_group + (nS - 3)).
        - wm_persistence: WM trace persistence (higher -> slower decay), applied as exponent to base decay.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    
    Notes
    -----
    - Set size (nS) reduces WM encoding probability and speeds decay via the interference term.
    - Age group (0=young, 1=old) reduces WM encoding probability via the same interference term.
    - RL values forget toward uniform at rate q_decay, capturing increased interference in larger sets.
    """
    lr, softmax_beta, q_decay, wm_capacity_base, age_size_interference, wm_persistence = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply RL forgetting toward uniform
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * (1.0 / nA)

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM encoding probability (capacity-limited, age/load reduced)
            wm_logit = wm_capacity_base - age_size_interference * (age_group + (nS - 3))
            p_encode = 1.0 / (1.0 + np.exp(-wm_logit))

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture weight derived from current WM certainty (norm away from uniform)
            wm_certainty = np.max(W_s) - (1.0 / nA)
            wm_weight = np.clip(wm_certainty * 3.0, 0.0, 1.0)  # 0 when uniform, ~1 when one-hot

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM decay with persistence modulated by load (more load -> less persistence)
            base_decay = 0.90
            load_factor = 1.0 - 0.05 * (nS - 3) - 0.05 * age_group
            effective_decay = np.clip(base_decay ** (wm_persistence * load_factor), 0.5, 0.99)
            w[s, :] = effective_decay * w[s, :] + (1.0 - effective_decay) * w_0[s, :]

            # WM encoding on reward with probability p_encode
            if r > 0.0:
                # Stochastic encoding: approximate by blending toward one-hot using p_encode
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_encode) * w[s, :] + p_encode * one_hot

        blocks_log_p += log_p

    return -blocks_log_p