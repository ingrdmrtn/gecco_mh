def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with age-modulated capacity and WM decay.

    Mechanisms
    ----------
    - RL: model-free Q-learning with inverse temperature beta_rl.
    - WM: slot-like cache of correct associations when rewarded; retrieved via a high beta_wm.
    - Arbitration: mixture weight equals the fraction of items that fit in WM capacity.
      Effective WM capacity is reduced in older adults and hurts more at larger set sizes.
    - WM decay: stored associations decay toward uniform; decay is faster in older adults.

    Parameters
    ----------
    states : array-like of int
        State index for each trial within block (0..set_size-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int
        Reward (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like
        Single value repeated; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, beta_rl, C_base, wm_decay, age_effect, beta_wm]
        - alpha: RL learning rate (0..1).
        - beta_rl: RL inverse temperature; scaled internally by 10.
        - C_base: baseline WM capacity in items (>=1).
        - wm_decay: per-trial WM decay rate toward uniform (0..1).
        - age_effect: reduction in capacity and increase in decay for older adults (>=0).
        - beta_wm: WM inverse temperature; scaled internally by 10.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_rl, C_base, wm_decay, age_effect, beta_wm = model_parameters
    softmax_beta = beta_rl * 10.0
    beta_wm = beta_wm * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-modulated effective capacity and WM decay
        C_eff = max(1.0, C_base - age_effect * age_group)
        wm_decay_eff = np.clip(wm_decay * (1.0 + 0.5 * age_group), 0.0, 1.0)

        # Arbitration weight based on fraction of items that fit into capacity
        wm_weight = np.clip(C_eff / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (deterministic when a clear item is cached)
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # WM decay toward baseline
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            # WM write on rewarded trials (store the correct association)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and age- and load-modulated WM noise + RL forgetting.

    Mechanisms
    ----------
    - RL: Q-learning with inverse temperature beta_rl and global decay toward uniform (forgetting).
    - WM: associative map updated on reward; retrieved with an effective beta_wm that decreases
      with set size and for older adults (more WM noise).
    - Arbitration: weight is based on relative certainty (1 - normalized entropy) of each system.

    Parameters
    ----------
    states : array-like of int
        State index for each trial within block (0..set_size-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int
        Reward (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like
        Single value repeated; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, beta_rl, kappa_unc, wm_noise_age, beta_wm_base, decay_q]
        - alpha: RL learning rate (0..1).
        - beta_rl: RL inverse temperature; scaled internally by 10.
        - kappa_unc: load sensitivity of WM noise (>=0).
        - wm_noise_age: extra WM noise factor for older adults (>=0).
        - beta_wm_base: base WM inverse temperature; scaled internally by 10.
        - decay_q: per-trial global RL forgetting toward uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_rl, kappa_unc, wm_noise_age, beta_wm_base, decay_q = model_parameters
    softmax_beta = beta_rl * 10.0
    beta_wm_base = beta_wm_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0
    nA = 3
    H_max = np.log(nA)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL global forgetting toward uniform
            q = (1.0 - decay_q) * q + decay_q * (1.0 / nA)

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM effective temperature decreases with load and age (more noise => lower beta)
            load = max(0, nS - 3)
            beta_wm_eff = beta_wm_base / (1.0 + wm_noise_age * age_group + kappa_unc * load)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Compute normalized entropies
            # RL distribution vector
            pi_rl = np.exp(softmax_beta * Q_s)
            pi_rl = pi_rl / np.sum(pi_rl)
            H_rl = -np.sum(np.clip(pi_rl, 1e-12, 1.0) * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            u_rl = 1.0 - (H_rl / H_max)

            # WM distribution vector
            pi_wm = np.exp(beta_wm_eff * W_s)
            pi_wm = pi_wm / np.sum(pi_wm)
            H_wm = -np.sum(np.clip(pi_wm, 1e-12, 1.0) * np.log(np.clip(pi_wm, 1e-12, 1.0)))
            u_wm = 1.0 - (H_wm / H_max)

            # Arbitration: normalize certainties to a convex weight
            wm_weight = u_wm / max(u_wm + u_rl, 1e-12)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # WM decay toward baseline and write on reward
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with precision-weighted arbitration using Bayesian-inspired reliability proxies.

    Mechanisms
    ----------
    - RL: Q-learning with inverse temperature beta_rl.
    - WM: associative cache updated on reward; decays by rho_wm_decay toward uniform.
    - Arbitration: compute system "precision" as the spread of its policy (proxy for reliability).
      WM precision is penalized by age and load-dependent interference; RL precision has its own scale.
      The mixture weight is WM_precision / (WM_precision + RL_precision).

    Parameters
    ----------
    states : array-like of int
        State index for each trial within block (0..set_size-1).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of int
        Reward (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6).
    age : array-like
        Single value repeated; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, beta_rl, sigma0_wm, sigma0_rl, interf_load, rho_wm_decay]
        - alpha: RL learning rate (0..1).
        - beta_rl: RL inverse temperature; scaled internally by 10.
        - sigma0_wm: base WM noise scale (>0) for precision computation.
        - sigma0_rl: base RL noise scale (>0) for precision computation.
        - interf_load: interference growth with set size for WM (>0).
        - rho_wm_decay: WM decay rate toward uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_rl, sigma0_wm, sigma0_rl, interf_load, rho_wm_decay = model_parameters
    softmax_beta = beta_rl * 10.0
    beta_wm_fixed = 50.0  # WM retrieval assumed near-deterministic when stored

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)
            pi_rl = np.exp(softmax_beta * Q_s)
            pi_rl = pi_rl / np.sum(pi_rl)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm_fixed * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)
            pi_wm = np.exp(beta_wm_fixed * W_s)
            pi_wm = pi_wm / np.sum(pi_wm)

            # Precision proxies: spread between best and average
            spread_rl = max(np.max(pi_rl) - np.mean(pi_rl), 0.0)
            spread_wm = max(np.max(pi_wm) - np.mean(pi_wm), 0.0)

            # WM precision penalized by age and load interference
            load = max(0, nS - 3)
            sigma_wm = max(1e-6, sigma0_wm * (1.0 + age_group) * (1.0 + interf_load * load))
            sigma_rl = max(1e-6, sigma0_rl)

            prec_wm = spread_wm / sigma_wm
            prec_rl = spread_rl / sigma_rl

            wm_weight = prec_wm / max(prec_wm + prec_rl, 1e-12)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # WM decay and write
            w[s, :] = (1.0 - rho_wm_decay) * w[s, :] + rho_wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p