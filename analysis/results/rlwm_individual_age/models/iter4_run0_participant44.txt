def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM (slot model with age- and set-size-dependent capacity and decay).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10 internally)
    - K_base: baseline WM capacity (in items/slots; effective WM weight ~ K_eff / set_size)
    - age_k_drop: reduction in WM capacity for older adults (>=0; applied if age_group==1)
    - wm_decay_rate: decay rate of WM traces toward uniform when no reward (>=0)

    Mechanism:
    - RL: delta rule with softmax policy.
    - WM: keeps a per-state associative vector; on rewarded trials, stores a one-shot mapping
      by setting the chosen action high (1) and normalizing; on non-reward trials, decays toward
      a uniform prior. WM policy is a high-precision softmax over the WM vector.
    - Mixture: p_total = w_mix * p_wm + (1 - w_mix) * p_rl, where w_mix â‰ˆ K_eff / set_size,
      K_eff = max(0, K_base - age_k_drop * age_group). Larger set sizes and older age thus
      reduce WM contribution.

    Age usage:
    - age_group = 1 (older) reduces K_eff by age_k_drop.
    Set-size usage:
    - Larger set sizes reduce mixture weight via division by set size and speed WM decay.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, K_base, age_k_drop, wm_decay_rate = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity and mixture
        K_eff = max(0.0, K_base - age_k_drop * age_group)
        wm_weight_eff = np.clip(K_eff / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (high precision)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: rewarded => one-shot store; no reward => decay toward uniform
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Stronger decay when set size is large and for older adults
                decay = wm_decay_rate * (1.0 + 0.5 * (nS - 3) + 0.5 * age_group)
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + one-shot WM with age- and set-size-scaled precision and mixture.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL (scaled by 10)
    - wm_oneshot: baseline probability weight on WM after encoding (0..1, will be scaled)
    - size_sensitivity: how strongly larger set sizes reduce WM weight (>=0)
    - age_wm_noise: increases WM noise for older adults in WM policy (>=0)

    Mechanism:
    - RL: standard delta-rule, softmax choice.
    - WM: one-shot encoding on rewarded trials to a sharp distribution over the chosen action.
      On non-reward, minimal update (light decay to uniform).
    - WM policy: softmax over WM store with precision reduced by age and set size.
    - Mixture weight: wm_weight = wm_oneshot * exp(-size_sensitivity*(set_size-3))
      further scaled by (1 - 0.5*age_group). Thus older adults and larger set sizes reduce WM use.

    Age usage:
    - Reduces WM weight multiplicatively and increases WM policy noise.
    Set-size usage:
    - Exponentially reduces WM weight and WM precision with larger set sizes.

    Returns:
    - Negative log-likelihood.
    """
    lr, softmax_beta, wm_oneshot, size_sensitivity, age_wm_noise = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm_base = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture weight scales with set size and age
        size_factor = np.exp(-size_sensitivity * max(0, nS - 3))
        age_factor = (1.0 - 0.5 * age_group)
        wm_weight_eff = np.clip(wm_oneshot * size_factor * age_factor, 0.0, 1.0)

        # WM precision reduced by set size and age
        beta_wm_eff = softmax_beta_wm_base / (1.0 + 0.5 * max(0, nS - 3) + age_wm_noise * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM updates: one-shot on reward; light decay otherwise
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Light decay; still a function of set size and age to capture distraction
                d = 0.1 * (1.0 + 0.3 * max(0, nS - 3) + 0.4 * age_group)
                d = np.clip(d, 0.0, 1.0)
                w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with reward-triggered consolidation and lateral inhibition, plus age-driven motor/action bias.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL (scaled by 10)
    - base_mix: baseline WM mixture logit, converted via sigmoid to weight
    - inh: lateral inhibition strength in WM on rewarded trials (>=0)
    - age_bias: extra bias toward action 0 in older adults (>=0; adds to action 0 in both policies)
    - size_wm_drop: penalty per additional item (above 3) reducing WM mixture (>=0)

    Mechanism:
    - RL: delta rule with softmax.
    - WM policy: softmax over a WM store. On reward, consolidate by boosting the chosen action and
      suppressing others (lateral inhibition). On non-reward, traces decay toward uniform; decay is
      stronger for larger set sizes and in older adults.
    - Mixture: wm_weight = sigmoid(base_mix - size_wm_drop*(set_size-3) - 0.5*age_group).
    - Action bias: Older adults show a motor/response bias toward action 0, implemented by adding
      age_bias to action 0 logits for both RL and WM policies when age_group == 1.

    Age usage:
    - Reduces WM mixture weight and adds an action bias toward action 0.
    Set-size usage:
    - Reduces WM mixture weight and speeds WM decay.

    Returns:
    - Negative log-likelihood.
    """
    lr, softmax_beta, base_mix, inh, age_bias, size_wm_drop = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture via sigmoid with penalties for set size and age
        mix_logit = base_mix - size_wm_drop * max(0, nS - 3) - 0.5 * age_group
        wm_weight_eff = 1.0 / (1.0 + np.exp(-mix_logit))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add age-driven response bias (toward action 0) to both policies' logits
            if age_group == 1:
                Q_s_biased = Q_s.copy()
                W_s_biased = W_s.copy()
                Q_s_biased[0] += age_bias
                W_s_biased[0] += age_bias
            else:
                Q_s_biased = Q_s
                W_s_biased = W_s

            # RL policy with bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s_biased - Q_s_biased[a])))

            # WM policy with bias
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_biased - W_s_biased[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: reward-triggered consolidation with lateral inhibition; otherwise decay
            if r > 0:
                # Boost chosen action, suppress others by 'inh'
                temp = np.ones(nA) * (-inh)
                temp[a] = 1.0
                # Convert to probabilities via softmax-like transform using high precision
                logits = temp
                logits -= np.max(logits)
                ww = np.exp(logits)
                ww /= np.sum(ww)
                w[s, :] = ww
            else:
                # Decay toward uniform; stronger with larger set size and in older adults
                d = 0.2 * (1.0 + 0.4 * max(0, nS - 3) + 0.6 * age_group)
                d = np.clip(d, 0.0, 1.0)
                w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])

        total_log_p += log_p

    return -total_log_p