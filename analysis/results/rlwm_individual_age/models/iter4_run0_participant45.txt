def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with age- and set-size-dependent WM precision and arbitration; RL perseveration bias.

    Overview:
    - RL learns Q-values with a single learning rate and includes a within-state perseveration bias.
    - WM stores the last rewarded action per state; its policy is near-deterministic but scaled by an
      effective WM precision that declines with set size and in older adults.
    - Arbitration between RL and WM depends on a logistic transform of effective precision, which
      incorporates set size (load) and age group.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age; age group coded as 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_base, wm_precision_base, wm_old_penalty, arb_bias, perseveration]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature; internally scaled by 10.
        - wm_precision_base: baseline WM precision (scales WM inverse temperature).
        - wm_old_penalty: fractional reduction (0..1) of WM precision for older adults (applied if age_group=1).
        - arb_bias: gain term controlling sharpness of arbitration logistic mapping.
        - perseveration: additive bias to repeat the last action chosen in that state within RL.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_precision_base, wm_old_penalty, arb_bias, perseveration = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0  # baseline WM determinism

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action within each state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective WM precision depends on set size (1/nS) and age penalty
        wm_prec_age = wm_precision_base * (1.0 - age_group * wm_old_penalty)
        wm_prec_eff = max(0.0, wm_prec_age) * (3.0 / float(nS))  # higher precision in smaller sets

        # Arbitration weight via logistic transform of effective precision
        # Bound in (0,1), more deterministic arbitration when arb_bias is high
        wm_weight = 1.0 / (1.0 + np.exp(-arb_bias * (wm_prec_eff - 0.5)))  # center at 0.5

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration on last action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy: softmax over WM weights with precision scaling
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            beta_wm_eff = softmax_beta_wm * max(1e-3, wm_prec_eff)
            exp_wm = np.exp(beta_wm_eff * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: write on reward; else slight decay toward uniform
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # decay toward uniform proportional to load (more decay at larger set size)
                decay = 1.0 - (3.0 / float(nS))  # 0 at nS=3, 0.5 at nS=6
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and set-size-dependent Q-decay; WM with age-dependent noise and load-based leak.

    Overview:
    - RL uses eligibility traces (lambda) so that recent state-actions receive credit assignment.
      We add a set-size-dependent decay on Q-values to capture forgetting under high load.
    - WM stores the last rewarded action, with a load-dependent leak toward uniform and age-dependent noise
      reducing its determinism.
    - Arbitration weight scales with set size (down-weighted at 6) and with age-related noise.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age; age group coded as 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_base, lam, q_decay_ss6, wm_noise_old, wm_weight_base]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature; internally scaled by 10.
        - lam: eligibility trace decay (0..1).
        - q_decay_ss6: per-trial Q-value decay applied only in set size 6 blocks (0..1).
        - wm_noise_old: proportional increase in WM noise for older adults (0..1).
        - wm_weight_base: baseline arbitration weight for WM before load/age adjustments (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, lam, q_decay_ss6, wm_noise_old, wm_weight_base = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces for state-action pairs in this block
        e = np.zeros((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration: down-weight WM at higher load and when older (via increased noise)
        # Scale base weight by relative capacity 3/nS and by (1 - age noise)
        wm_weight = wm_weight_base * (3.0 / float(nS)) * (1.0 - age_group * wm_noise_old)
        wm_weight = min(1.0, max(0.0, wm_weight))

        # WM effective beta reduced by age-related noise
        beta_wm_eff = softmax_beta_wm * (1.0 - age_group * wm_noise_old)
        beta_wm_eff = max(1e-3, beta_wm_eff)

        # Q-value decay if high load
        q_decay = q_decay_ss6 if nS == 6 else 0.0
        q_decay = min(max(q_decay, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply global Q decay under high load
            if q_decay > 0.0:
                q *= (1.0 - q_decay)

            # RL policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(beta_wm_eff * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Increment trace for current state-action; decay others
            e *= lam
            e[s, a] += 1.0
            delta = r - q[s, a]
            q += lr * delta * e  # credit assignment along trace

            # WM update: rewarded write, else load-based leak to uniform
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                leak = 1.0 - (3.0 / float(nS))  # higher leak for larger sets
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus (UCB-style) and age-dependent lapse; WM with gated arbitration.

    Overview:
    - RL augments Q-values with an uncertainty/exploration bonus inversely proportional to sqrt of visit counts.
      This helps capture exploratory choices early, especially under high load.
    - WM uses a recency-weighted memory that decays more at larger set sizes; on reward it boosts the chosen action.
    - Arbitration weight is determined by a logistic gating signal that is penalized in set size 6.
    - An age-dependent lapse mixes a uniform random choice into the final policy, increasing in older adults.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age; age group coded as 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_base, uct_c, wm_gate_base, wm_gate_ss6_penalty, lapse_old]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature; internally scaled by 10.
        - uct_c: strength of the uncertainty/exploration bonus (>0).
        - wm_gate_base: baseline gating drive for WM arbitration (logit scale).
        - wm_gate_ss6_penalty: reduction applied to gating drive when set size is 6 (>=0).
        - lapse_old: additional lapse probability added if age_group=1 (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, uct_c, wm_gate_base, wm_gate_ss6_penalty, lapse_old = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # Visit counts per (s,a) for uncertainty bonus
        N = np.ones((nS, nA))  # start at 1 to avoid division by zero

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM gate on logit scale with set-size penalty; convert to weight via logistic
        gate_drive = wm_gate_base - (wm_gate_ss6_penalty if nS == 6 else 0.0)
        wm_weight = 1.0 / (1.0 + np.exp(-gate_drive))
        wm_weight = min(1.0, max(0.0, wm_weight))

        # Age-dependent lapse probability
        lapse = age_group * lapse_old
        lapse = min(1.0, max(0.0, lapse))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with UCB-like bonus
            bonus = uct_c / np.sqrt(N[s, :] + 1e-9)
            Q_aug = q[s, :] + bonus
            Qc = Q_aug - np.max(Q_aug)
            exp_rl = np.exp(softmax_beta * Qc)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy: recency-weighted memory with load-based decay
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            exp_wm = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Arbitration mixture
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Lapse adds a uniform component
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and count update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            N[s, a] += 1.0

            # WM update: decay toward uniform (stronger for larger sets), reward boosts chosen action
            decay = 1.0 - (3.0 / float(nS))  # 0 at nS=3, 0.5 at nS=6
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                w[s, a] += 1.0
                # renormalize to keep a distribution-like vector
                if np.sum(w[s, :]) > 0:
                    w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p