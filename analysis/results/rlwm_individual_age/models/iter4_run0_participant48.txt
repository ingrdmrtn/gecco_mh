Below are three standalone cognitive models that compute the negative log-likelihood of the participant’s choices. Each model blends RL and WM differently, modulates behavior by set size (3 vs 6), and uses the participant’s age group (young=0, old=1; this participant is old) in meaningful ways. All parameters are used, and each model keeps the core RL+WM structure consistent with the provided template (softmax_beta scaling, WM softmax set to be very deterministic, RL Q-learning, WM decay and overwrite on reward).

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted arbitration RL+WM with age- and load-sensitive WM precision.

    RL policy is standard softmax over Q-values. WM policy is a softmax over a fast-updating
    associative store that overwrites the mapping upon reward. Arbitration weight (WM vs RL)
    is computed from the relative certainty of the two systems:
      - WM certainty ~ effective WM precision (modulated by load and age)
      - RL uncertainty ~ entropy of the RL policy
    The WM precision decreases with larger set size and with older age.

    Parameters
    ----------
    states : array-like, int
        Trial-by-trial state indices within block (0..nS-1).
    actions : array-like, int
        Chosen actions (0..2).
    rewards : array-like, int
        Rewards (0 or 1).
    blocks : array-like, int
        Block indices.
    set_sizes : array-like, int
        Block set size (3 or 6) per trial.
    age : array-like, int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, softmax_beta, wm_precision, wm_forget, age_wm_deficit, load_penalty]
        - lr: RL learning rate (0..1).
        - softmax_beta: base RL inverse temperature (scaled internally by 10).
        - wm_precision: baseline WM precision at set size 3 (scales WM softmax).
        - wm_forget: WM decay per trial toward uniform (0..1).
        - age_wm_deficit: reduction in effective WM precision for older adults.
        - load_penalty: exponent controlling how set size reduces WM precision.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_precision, wm_forget, age_wm_deficit, load_penalty = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM precision depends on load and age
        load_factor = (3.0 / nS) ** max(load_penalty, 0.0)
        wm_precision_eff = wm_precision * load_factor - age_wm_deficit * age_group
        wm_precision_eff = max(wm_precision_eff, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            beta_wm_eff = softmax_beta_wm * (1.0 + wm_precision_eff)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # RL uncertainty via entropy (smaller entropy => more certainty)
            # Compute full RL distribution for entropy
            rl_logits = softmax_beta * Q_s
            rl_logits -= np.max(rl_logits)
            rl_probs = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            rl_entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))

            # Arbitration: WM weight increases with WM precision and decreases with RL certainty (i.e., increases with RL entropy)
            wm_weight = wm_precision_eff / (wm_precision_eff + rl_entropy + 1e-8)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay and overwrite on reward
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with recall gating: WM contributes with a recall probability that decays with load,
    while RL temperature increases with load and in older adults (greater decision noise).

    WM is an associative store with decay and overwrite on reward. Its policy is deterministic,
    but it only contributes with probability p_recall, which declines with set size and is further
    reduced in older adults. Otherwise, the choice is governed by RL.

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..nS-1).
    actions : array-like, int
        Chosen action (0..2).
    rewards : array-like, int
        Binary feedback (0 or 1).
    blocks : array-like, int
        Block indices.
    set_sizes : array-like, int
        Set size per block, repeated on trials.
    age : array-like, int
        Participant age. Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_base, wm_recall_base, wm_decay, age_beta_increase, load_recall_drop]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature (scaled by 10 internally).
        - wm_recall_base: baseline recall probability at set size 3.
        - wm_decay: WM decay toward uniform (0..1 per trial).
        - age_beta_increase: increase in RL noise (i.e., reduce effective beta) in older adults.
        - load_recall_drop: reduction in recall probability moving from set size 3 to 6.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_recall_base, wm_decay, age_beta_increase, load_recall_drop = model_parameters
    softmax_beta = beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL beta becomes less decisive (effectively lower) with load and in older adults
        load_noise_scale = 1.0 / (1.0 + (nS - 3) / 3.0)  # 1.0 at nS=3, 0.5 at nS=6
        beta_eff = softmax_beta * load_noise_scale / (1.0 + age_beta_increase * age_group)
        beta_eff = max(beta_eff, 1e-3)

        # WM recall probability decreases with load and with age (via same load factor and parameter)
        recall_drop = load_recall_drop if nS > 3 else 0.0
        p_recall = wm_recall_base - recall_drop
        # Age further reduces recall
        p_recall -= 0.5 * load_recall_drop * age_group
        p_recall = np.clip(p_recall, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with effective beta
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy for state s
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture: WM contributes with probability p_recall, otherwise RL
            p_total = p_recall * p_wm + (1.0 - p_recall) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay and overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with choice-kernel bias and age-by-load lapse.

    Choices are governed by a mixture of RL and WM, plus:
      - A choice kernel (global action frequency tracker) adds bias to RL preferences.
      - A lapse probability that increases with both age and set size, causing uniform random choices.

    Parameters
    ----------
    states : array-like, int
        Trial-by-trial states (0..nS-1).
    actions : array-like, int
        Chosen actions (0..2).
    rewards : array-like, int
        Rewards (0/1).
    blocks : array-like, int
        Block indices.
    set_sizes : array-like, int
        Set size (3 or 6) per trial.
    age : array-like, int
        Participant age. Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, softmax_beta, wm_weight_base, choice_kernel_lr, age_load_lapse, wm_decay]
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled by 10 internally).
        - wm_weight_base: baseline WM weight at set size 3.
        - choice_kernel_lr: learning rate for choice kernel (0..1).
        - age_load_lapse: added lapse at set size 6 for older adults (scaled by load and age).
        - wm_decay: WM decay toward uniform (0..1 per trial).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, choice_kernel_lr, age_load_lapse, wm_decay = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global choice kernel over actions (state-independent), initialized uniform
        ck = (1.0 / nA) * np.ones(nA)

        # WM weight diminishes with load; lapse increases with age and load
        load_scale = 3.0 / nS  # 1.0 at 3, 0.5 at 6
        wm_weight = np.clip(wm_weight_base * load_scale, 0.0, 1.0)

        lapse = 0.0
        if nS > 3 and age_group == 1:
            lapse = np.clip(age_load_lapse, 0.0, 0.3)  # cap lapse to keep probabilities sensible

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL with choice-kernel bias
            Q_s = q[s, :].copy()
            # Add choice kernel bias as logit shift
            Q_bias = Q_s + ck
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_bias - Q_bias[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL, then add lapse to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay and overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Choice kernel update: move probability mass toward chosen action
            ck = (1.0 - choice_kernel_lr) * ck
            ck[a] += choice_kernel_lr

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes:
- Model 1 learns an RL Q-table and a WM store; arbitration favors WM when its precision is high and RL is uncertain. Older age and larger set sizes reduce WM precision.
- Model 2 uses a recall-gated WM: WM contributes with probability p_recall that drops under load and in older adults; RL grows noisier with load and age.
- Model 3 introduces a state-independent choice kernel biasing RL and a lapse that specifically increases in older adults under high load, capturing age-by-load increases in random responding.