Below are three standalone cognitive models (functions) that implement RL+WM mechanisms with different twists. Each follows the requested signature and returns the negative log-likelihood of the observed choices. All parameters are used meaningfully, include set-size and age effects, and stay within a maximum of six parameters. I assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with eligibility traces and age-/set-size-dependent WM decay.

    Idea
    - RL: Q-learning with replacing eligibility traces to allow credit to persist over repetitions;
      the trace persistence is reduced in older adults.
    - WM: One-shot binding of the last rewarded action per state. WM contribution decays toward
      uniform with an interference rate that increases with set size and with age.
    - Mixture: action probability is a mixture of RL softmax and WM softmax. WM weight is reduced
      multiplicatively by load (set size) and age.

    Parameters
    ----------
    states : array-like
        State index per trial (0..nS-1 for each block).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Binary reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6) within a block.
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr, beta_base, wm_base, lambda_trace, age_decay_wm, set_interf_wm]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; scaled internally.
        - wm_base: base WM mixture weight (0..1).
        - lambda_trace: eligibility trace parameter (0..1); reduced in older adults.
        - age_decay_wm: proportional reduction of WM influence in older adults (0..1).
        - set_interf_wm: WM interference strength per item beyond 3 (>=0).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr, beta_base, wm_base, lambda_trace, age_decay_wm, set_interf_wm = model_parameters

    # Age group coding: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1

    # Inverse temperatures
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM when intact

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values, WM weights, and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # Age-adjusted trace parameter
        lam_eff = np.clip(lambda_trace * (1.0 - 0.3 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load- and age-dependent WM mixing weight
            excess = max(0, nS_t - 3)
            load_factor = np.exp(-set_interf_wm * excess)  # in (0,1], decreases with excess
            age_factor = (1.0 - age_decay_wm * age_group)  # reduces WM in older adults
            wm_weight_eff = np.clip(wm_base * load_factor * age_factor, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility traces (replacing traces, gamma=1 here)
            pe = r - q[s, a]
            e *= lam_eff
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += lr * pe * e

            # WM decay toward uniform depends on load and age
            # Higher excess and being older increase decay rate
            decay_rate = 1.0 - np.exp(-set_interf_wm * excess)  # in [0,1)
            decay_rate = np.clip(decay_rate * (1.0 + 0.5 * age_group), 0.0, 1.0)
            w = (1.0 - decay_rate) * w + decay_rate * w_0

            # WM one-shot write on rewarded trials
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with asymmetric RL learning, separate RL/WM temperatures,
    and an age-/load-dependent lapse to random choice.

    Idea
    - RL: Q-learning with asymmetric learning from negative outcomes captured by a multiplicative
      asymmetry factor (older adults often downweight negative feedback).
    - WM: one-shot mapping updated on reward; moderate decay that grows with load and age.
    - Policy: convex mixture of RL-softmax and WM-softmax, wrapped by an epsilon-lapse that grows
      with set size and age to reflect increased random exploration or attentional lapses.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr_base, asym_neg, beta_rl, beta_wm, eps0, eps_load_age]
        - lr_base: base RL learning rate for positive outcomes (0..1).
        - asym_neg: scales learning from negative PEs (0..1), lr_neg = lr_base * (1 - asym_neg).
        - beta_rl: RL inverse temperature (scaled internally).
        - beta_wm: WM inverse temperature (scaled internally around a high value).
        - eps0: base lapse rate to random choice (0..1).
        - eps_load_age: additional lapse per excess item times (1+age), >=0.

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr_base, asym_neg, beta_rl, beta_wm, eps0, eps_load_age = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_rl * 10.0
    # WM temperature starts high but can be slightly relaxed
    softmax_beta_wm = 10.0 + 40.0 * np.clip(beta_wm, 0.0, 1.0)  # in [10,50]

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM mixture weight: simple structural load effect (no extra parameter here)
            excess = max(0, nS_t - 3)
            wm_weight = 1.0 / (1.0 + excess)  # 1.0 for setsize=3, 0.5 for setsize=6
            wm_weight = wm_weight * (0.9 if age_group == 1 else 1.0)  # slight reduction in older
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Lapse rate grows with load and age
            eps_t = eps0 + eps_load_age * excess * (1.0 + age_group)
            eps_t = np.clip(eps_t, 0.0, 0.99)

            # Mixture without lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Inject lapse to uniform policy
            p_total = (1.0 - eps_t) * p_mix + eps_t * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            pe = r - q[s, a]
            lr_pos = lr_base
            lr_neg = lr_base * (1.0 - np.clip(asym_neg, 0.0, 1.0))
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM decay increases with load and age
            decay_base = 0.05
            decay = np.clip(decay_base + 0.1 * excess, 0.0, 1.0)
            decay *= (1.0 + 0.5 * age_group)
            w = (1.0 - decay) * w + decay * w_0

            # WM one-shot write on reward
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Arbitration model: WM usage depends on RL uncertainty and WM capacity limits.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: one-shot mapping; small decay. WM effectiveness is limited by a capacity parameter that
      is reduced in older adults; effective WM influence scales with K_eff / set_size.
    - Arbitration: rely more on WM when RL uncertainty (entropy) is high; arbitration sharpness
      controlled by a sensitivity parameter and shifted by age bias. WM policy has controllable noise.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr, beta_base, wm_capacity, arb_sensitivity, age_bias, wm_noise]
        - lr: RL learning rate (0..1).
        - beta_base: RL inverse temperature, scaled internally.
        - wm_capacity: nominal WM capacity (around 3..6). Larger -> better WM coverage.
        - arb_sensitivity: controls how strongly RL uncertainty shifts weight to WM (>=0).
        - age_bias: capacity decrement in older adults (>=0).
        - wm_noise: reduces WM determinism, stronger in older adults (0..1).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr, beta_base, wm_capacity, arb_sensitivity, age_bias, wm_noise = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0
    # WM temperature reduced by wm_noise, especially in older adults
    wm_noise_eff = np.clip(wm_noise * (1.0 + 0.5 * age_group), 0.0, 0.9)
    softmax_beta_wm = 50.0 * (1.0 - wm_noise_eff) + 5.0  # keep >5 to avoid pure uniform

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]

            # RL softmax choice prob for chosen action (as provided)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL uncertainty (entropy of full softmax)
            logits = softmax_beta * Q_s
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)
            entropy = -np.sum(pi * np.log(np.clip(pi, eps, 1.0))) / np.log(nA)  # normalized [0,1]

            # WM policy for chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM availability via capacity K_eff relative to set size
            K_eff = max(0.0, wm_capacity - age_bias * age_group)
            availability = np.clip(K_eff / max(1.0, float(nS_t)), 0.0, 1.0)

            # Arbitration: more WM weight when RL is uncertain
            # Shift threshold slightly higher for older adults (need more uncertainty to switch)
            threshold = 0.5 + 0.1 * age_group
            arb_signal = entropy - threshold
            wm_weight = 1.0 / (1.0 + np.exp(-arb_sensitivity * arb_signal))
            wm_weight *= availability  # limit by capacity and set size
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Final mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay (light, to allow capacity-limited accumulation)
            decay = 0.05 * (1.0 + 0.3 * age_group)
            w = (1.0 - decay) * w + decay * w_0

            # WM write on reward
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size and age impacts across models
- Model 1: WM mixture weight decreases with set size via an exponential interference factor and with age via age_decay_wm. Eligibility trace persistence is reduced in older adults, potentially impairing multi-trial credit assignment.
- Model 2: Lapse to random choice increases with set size and age; WM weight structurally shrinks at larger set sizes and slightly in older adults; RL negative learning is downweighted by asym_neg.
- Model 3: WM influence scales with capacity relative to set size (K_eff / set_size), and older adults have reduced effective capacity via age_bias. Arbitration favors WM when RL uncertainty is high, but older adults require higher uncertainty to switch (threshold shift) and experience noisier WM (wm_noise amplified by age).