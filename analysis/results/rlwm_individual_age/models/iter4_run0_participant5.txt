def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: Hybrid RL + Reward-gated WM with set-size-modulated RL temperature and age-modulated WM decay.

    Description:
    - RL: Q-learning with softmax action selection. The inverse-temperature increases as set size decreases.
    - WM: Reward-gated, decaying one-hot memory. On rewarded trials, WM moves toward the chosen action; otherwise only decays.
    - Mixture: Action probability is a convex combination of WM and RL policies with a base WM weight.
    - Set size effect: Larger set size lowers RL inverse temperature via a power-law factor.
    - Age effect: Older group has faster WM decay (less stable WM).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight_base: Base mixture weight for WM (0..1)
    - beta_base: Base inverse temperature for RL (scaled internally)
    - wm_decay_base: Baseline WM decay rate toward uniform (0..1)
    - wm_lr_pos: WM learning rate toward one-hot upon reward (0..1)
    - ss_temp_slope: Exponent controlling how set size scales RL temperature (>=0). beta_eff = beta_base*(3/nS)^ss_temp_slope

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, beta_base, wm_decay_base, wm_lr_pos, ss_temp_slope = model_parameters
    softmax_beta_base = beta_base * 10.0  # higher upper bound per template

    age_group = 0 if age[0] <= 45 else 1
    # Age effect: older => stronger decay (less stable WM content)
    age_decay_mult = 1.0 + 0.5 * age_group

    softmax_beta_wm = 50.0  # near-deterministic WM policy when confident
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size modulated RL temperature
        ss_factor_temp = (3.0 / float(nS)) ** max(0.0, ss_temp_slope)
        softmax_beta = softmax_beta_base * ss_factor_temp

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax of WM weights)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (age-modulated)
            wm_decay_eff = np.clip(wm_decay_base * age_decay_mult, 0.0, 1.0)
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM reward-gated strengthening toward the chosen action
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr_pos) * w[s, :] + wm_lr_pos * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: RL with WM capacity gating (probabilistic slots), age-reduced capacity, and WM noise.

    Description:
    - RL: Q-learning with softmax action selection.
    - WM: Each state can be stored in a limited-capacity cache with probability proportional to K_eff / set size.
      If stored and rewarded, WM keeps a one-hot association; otherwise it remains closer to uniform.
    - Mixture: WM policy mixed with RL policy. Effective WM weight scales by the probability that the state is cached.
    - Set size effect: Larger set size reduces probability that a given state is in WM (via K_eff / nS).
    - Age effect: Older age reduces K_eff (fewer usable WM slots).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: Inverse temperature for RL (scaled internally by 10)
    - wm_weight0: Base WM mixture weight when the state is certainly in WM (0..1)
    - K_base: Baseline WM capacity in number of states (>=0)
    - wm_noise: WM policy noise (higher => flatter WM softmax; effective WM beta = 1/(wm_noise + 1e-6))
    - age_penalty: Amount by which age reduces effective capacity (multiplies age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, K_base, wm_noise, age_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    beta_wm = 1.0 / (wm_noise + 1e-6)  # convert noise to inverse temperature
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: associative strengths per state-action, start uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity reduced by age
        K_eff = max(0.0, K_base - age_penalty * age_group)
        p_inWM = np.clip(K_eff / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (noisy softmax over WM weights)
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture weight scaled by probability that s is within WM slots
            wm_weight_eff = np.clip(wm_weight0 * p_inWM, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: if rewarded, push toward one-hot; else softly relax toward uniform
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong commit on reward when state is likely in WM
                commit = np.clip(p_inWM, 0.0, 1.0)
                w[s, :] = (1.0 - commit) * w[s, :] + commit * onehot
            else:
                # On no reward, partial relaxation toward uniform (proportional to p_inWM)
                relax = 0.2 * p_inWM
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: RL with eligibility trace + Bayesian WM (Dirichlet) and confidence-based arbitration.

    Description:
    - RL: Q-learning augmented with an eligibility trace (bandit-style lambda) to allow transient carry-over.
    - WM: For each state, maintain Dirichlet-like counts over actions (reward-shaped).
      Policy is softmax over the WM posterior mean; WM temperature increases with confidence.
    - Arbitration: WM vs RL mixture weight is a sigmoid function of WM confidence (1 - normalized entropy),
      with the confidence threshold shifted by set size and age (older and larger sets need higher confidence to favor WM).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: Inverse temperature for RL (scaled internally by 10)
    - wm_conf_slope: Slope of the sigmoid mapping from confidence to WM mixture weight (>=0)
    - etrace_lambda: Eligibility trace decay (0..1)
    - wm_beta_base: Base inverse temperature for WM policy (>=0)
    - age_bias: Additive bias that increases the confidence threshold for older participants

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_conf_slope, etrace_lambda, wm_beta_base, age_bias = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # WM Dirichlet counts initialized uniform (pseudo-counts = 1)
        m = np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # WM posterior mean and confidence
            W_s = m[s, :] / np.maximum(1e-12, np.sum(m[s, :]))
            # Confidence via normalized entropy
            entropy = -np.sum(W_s * np.log(np.maximum(W_s, 1e-12)))
            max_entropy = np.log(nA)
            confidence = 1.0 - (entropy / max_entropy)

            # WM softmax temperature scales with confidence
            beta_wm = wm_beta_base * (1.0 + 2.0 * confidence)

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Arbitration: sigmoid of confidence with threshold depending on set size and age
            # Larger set size and older age increase the threshold (need more confidence to rely on WM)
            size_shift = 0.2 * ((nS - 3) / 3.0)  # 0 for nS=3, ~0.2 for nS=6
            threshold = 0.5 + size_shift + age_bias * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-wm_conf_slope * (confidence - threshold)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace (bandit: gamma=1)
            delta = r - Q_s[a]
            e *= etrace_lambda
            e[s, a] = 1.0
            q += lr * delta * e

            # WM update (Dirichlet counts; reward-shaped)
            # Reward adds evidence to chosen action; no-reward spreads evidence to others.
            if r > 0.0:
                m[s, a] += 1.0
            else:
                add = 1.0 / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        m[s, aa] += add

        blocks_log_p += log_p

    return -blocks_log_p