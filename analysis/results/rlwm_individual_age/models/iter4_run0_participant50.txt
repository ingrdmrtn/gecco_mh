Below are three alternative cognitive models, each as a standalone Python function. They all:

- Combine a model-free RL policy with a working-memory (WM) policy.
- Respect block resets and set size.
- Use age group (young=0, old=1) in a meaningful way.
- Return the negative log-likelihood of the observed choices.
- Use at most 6 parameters and use all parameters meaningfully.

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-gated WM with age-modulated capacity and RL temperature.

    Idea
    - RL and WM policies are mixed.
    - WM engagement is gated by an effective capacity: higher when set size is below capacity,
      and reduced for older adults.
    - Older adults also have reduced RL inverse temperature (more choice noise).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_gain0, cap_center, age_cap_slope, beta_age_slope]
        - lr: RL learning rate (0..1).
        - beta_base: baseline inverse temperature for RL; internally scaled (*10).
        - wm_gain0: baseline WM mixture gain (0..1).
        - cap_center: subjective capacity center (around 3..6); larger => more WM at nS<=cap_center.
        - age_cap_slope: additional reduction of effective capacity for older adults (>=0 reduces WM when old).
        - beta_age_slope: proportional reduction of RL beta for older adults (>=0 reduces beta when old).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_gain0, cap_center, age_cap_slope, beta_age_slope = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age-modulated RL temperature
        softmax_beta = beta_base * 10.0 * max(0.05, (1.0 - beta_age_slope * age_group))

        # Capacity gate: WM weight increases when nS below effective capacity
        # gate = sigmoid((cap_center_eff - nS))
        cap_center_eff = cap_center - age_cap_slope * age_group
        gate = 1.0 / (1.0 + np.exp(-(cap_center_eff - nS)))
        wm_weight = float(np.clip(wm_gain0 * gate, 0.0, 1.0))

        # Policies
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        softmax_beta_wm = 50.0  # near-deterministic WM retrieval

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store rewarded association as one-hot; no decay here (capacity gate already handles WM use)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Uncertainty-driven arbitration and age-selective epsilon exploration.

    Idea
    - Choices arise from a mixture of RL and WM.
    - WM weight increases when RL is uncertain (small Q spread) and decreases with larger set size.
    - Older adults show additional epsilon-greedy exploration noise.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_w0, k_uncert, ss_wm_drop, age_explore]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_w0: baseline WM weight scale (0..1).
        - k_uncert: sensitivity of WM weight to RL confidence/spread (>=0).
        - ss_wm_drop: reduction of WM weight with larger set size (>=0).
        - age_explore: epsilon exploration added only for older adults (>=0, yields epsilon=age_explore if old else 0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_w0, k_uncert, ss_wm_drop, age_explore = model_parameters

    eps_const = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0

        # Set-size penalty for WM weight
        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        softmax_beta_wm = 50.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps_const)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps_const)
            p_wm = float(pi_wm[a])

            # RL confidence (spread): larger spread => more confident => less WM reliance
            spread = float(np.max(Q_s) - np.mean(Q_s))
            wm_weight = wm_w0 * np.exp(-k_uncert * spread) * np.exp(-ss_wm_drop * ss_penalty)
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture then epsilon exploration (age-selective)
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            epsilon = float(np.clip(age_explore * age_group, 0.0, 0.5))
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, eps_const)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (mild interference across time)
            decay = 0.05 + 0.10 * ss_penalty  # more decay in larger set size
            w = (1.0 - decay) * w + decay * w_0

            # WM store on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce-Hall adaptive learning + WM with age/set-size precision loss.

    Idea
    - RL learning rate adapts per state based on surprise (unsigned prediction error),
      following a Pearce-Hall style rule.
    - WM contributes with a tunable precision (softmax temperature) that drops with
      set size and more strongly for older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [alpha0, k_ph, beta_base, wm_weight0, wm_temp0, age_wm_temp]
        - alpha0: initial RL learning rate per state (0..1).
        - k_ph: Pearce-Hall update rate for learning-rate adaptation (0..1).
        - beta_base: RL inverse temperature; internally scaled (*10).
        - wm_weight0: baseline WM mixture weight (0..1).
        - wm_temp0: baseline WM precision factor (>0, higher => more deterministic WM).
        - age_wm_temp: extra precision loss term for older adults (>=0 increases noise when old).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha0, k_ph, beta_base, wm_weight0, wm_temp0, age_wm_temp = model_parameters

    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        ss_penalty = (max(3, nS) - 3) / 3.0  # 0 for 3, 1 for 6

        # Initialize RL values and per-state learning rates
        q = (1.0 / nA) * np.ones((nS, nA))
        alpha_s = np.clip(alpha0 * np.ones(nS), 0.0, 1.0)

        # WM store with temperature scaled by set size and age (precision loss)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # WM precision: base scaled by factor; larger ss and older => lower precision
        wm_temp = max(1e-3, wm_temp0 / (1.0 + ss_penalty + age_group * abs(age_wm_temp)))
        softmax_beta_wm = 20.0 * wm_temp

        wm_weight = float(np.clip(wm_weight0, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm)
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with adaptive alpha (Pearce-Hall)
            delta = r - Q_s[a]
            q[s, a] += alpha_s[s] * delta
            # Update alpha toward absolute PE
            alpha_s[s] = (1.0 - k_ph) * alpha_s[s] + k_ph * abs(delta)
            alpha_s[s] = float(np.clip(alpha_s[s], 0.0, 1.0))

            # WM mild decay toward uniform, stronger with larger set size
            decay = 0.05 + 0.10 * ss_penalty
            w = (1.0 - decay) * w + decay * w_0

            # WM store on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)