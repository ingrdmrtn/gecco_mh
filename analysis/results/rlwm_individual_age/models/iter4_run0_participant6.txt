def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited one-shot WM with age- and set-size-scaled mixture and writing.

    Mechanism:
    - RL: tabular Q-learning over state-action values with softmax policy.
    - WM: fast "one-shot" storage of rewarded action for a state; policy is near-deterministic softmax over WM trace.
    - Arbitration: mixture weight scales down with larger set size and for older age.
    - WM writing probability also scales with set size and age (capacity/interference).

    Parameters (5):
    - lr: RL learning rate (0..1).
    - wm_weight_base: baseline mixture weight for WM at set size 3 and young adults (0..1).
    - softmax_beta: RL inverse temperature; internally multiplied by 10 for a higher range.
    - wm_recall_base: baseline WM write/recall strength at set size 3 (0..1).
    - age_recall_penalty: proportional reduction in WM weight and write probability for older adults (>=0).

    Age and set-size effects:
    - Effective WM mixture weight = wm_weight_base * (3/nS) * (1 - age_recall_penalty*age_group).
    - WM write strength p_store = wm_recall_base * (3/nS) * (1 - age_recall_penalty*age_group).
    """
    lr, wm_weight_base, softmax_beta, wm_recall_base, age_recall_penalty = model_parameters
    softmax_beta *= 10.0  # higher dynamic range
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM tables initialized to uniform
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight for this block (capacity and age scaled)
        wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - age_recall_penalty * age_group)
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

        # Effective WM write strength (probabilistic blend toward one-hot)
        p_store_base = wm_recall_base * (3.0 / nS) * (1.0 - age_recall_penalty * age_group)
        p_store_base = float(np.clip(p_store_base, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action (near-deterministic softmax over WM trace)
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = float(pi_wm[a])

            # Mixture arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-triggered one-shot write with probabilistic strength
            if r > 0.5:
                # Blend current WM trace toward a one-hot on the chosen action
                w[s, :] = (1.0 - p_store_base) * w[s, :]
                w[s, a] += p_store_base * (1.0 - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with exploration-bias by age, and WM with noise floor that grows with load and age.

    Mechanism:
    - RL: separate learning rates for positive/negative prediction errors; age shifts exploration via temperature.
    - WM: fast trace updated toward one-hot on reward and repelled on negative feedback.
    - WM policy includes a noise floor (lapses) that increases with set size and with age.
    - Arbitration: fixed baseline WM weight scaled down by set size and for older adults.

    Parameters (6):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: RL inverse temperature base; internally scaled by 10, then age-adjusted.
    - wm_weight_base: baseline WM mixture weight at set size 3 and young (0..1).
    - wm_noise: baseline WM noise floor at set size 3 and young (0..1), mixed with uniform.
    - age_temp_shift: multiplicative shift of RL inverse temperature by age sign (+ favors young, - favors older).

    Age and set-size effects:
    - Effective RL beta = softmax_beta * (1 + age_temp_shift * (1 - 2*age_group)).
    - WM noise = wm_noise + 0.05*(nS-3) + 0.05*age_group, clipped to [0, 0.49].
    - WM mixture weight = wm_weight_base * (3/nS) * (1 - 0.3*age_group).
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_noise, age_temp_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted RL temperature
        beta_rl = softmax_beta * (1.0 + age_temp_shift * (1 - 2 * age_group))
        beta_rl = float(max(beta_rl, eps))

        # WM mixture weight and noise floor
        wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))
        wm_noise_eff = wm_noise + 0.05 * (nS - 3) + 0.05 * age_group
        wm_noise_eff = float(np.clip(wm_noise_eff, 0.0, 0.49))  # keep some sensitivity

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (full softmax with age-adjusted beta)
            Q_s = q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = float(pi_rl[a])

            # WM policy with noise floor
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            pi_wm_noisy = (1.0 - wm_noise_eff) * pi_wm + wm_noise_eff * (1.0 / nA)
            p_wm = float(pi_wm_noisy[a])

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update:
            if r > 0.5:
                # Move toward one-hot for chosen action
                w[s, :] *= (1.0 - 1.0)  # zero out then set chosen via blend below
                w[s, a] = 1.0
                # Smooth slightly to avoid numerical issues
                w[s, :] = 0.99 * w[s, :] + 0.01 * w_0[s, :]
            else:
                # Penalize chosen action probability and renormalize (repulsion)
                eta = 0.5 * (3.0 / nS)
                eta = float(np.clip(eta, 0.0, 0.9))
                drop = eta * w[s, a]
                w[s, a] -= drop
                w[s, :] += drop / (nA - 1)
                # small smoothing
                w[s, :] = 0.99 * w[s, :] + 0.01 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    PE-gated arbitration with perseveration bias and WM precision control.

    Mechanism:
    - RL: standard Q-learning; action selection includes a perseveration bias to repeat the last action in a state.
    - WM: fast trace with tunable precision (scales WM inverse temperature).
    - Arbitration: WM mixture weight is a logistic function of (low set size, small PE) with an age-dependent bias.
      Larger absolute prediction errors reduce reliance on WM; smaller set sizes and young age increase it.

    Parameters (6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_precision: scales WM inverse temperature (multiplicative on 50).
    - gate_bias_age: additive bias term in the WM gate, multiplied by (1 - 2*age_group).
    - perseveration_kappa: strength of perseveration bias on RL logits.
    - setsize_gate_slope: how strongly smaller set sizes increase WM gate.

    Age and set-size effects:
    - Gate logit = setsize_gate_slope*(3 - nS) - gate_PE_scale*|PE| + gate_bias_age*(1 - 2*age_group).
    - Perseveration is present but effectively weaker for larger set sizes via (3/nS).
    """
    lr, softmax_beta, wm_precision, gate_bias_age, perseveration_kappa, setsize_gate_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0
    gate_PE_scale = 5.0  # fixed sensitivity of gate to PE magnitude

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # track perseveration per state
        prev_pe = np.zeros(nS)  # track last PE per state (for gating)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            bias = np.zeros_like(Q_s)
            if last_action[s] >= 0:
                # Set-size-scaled perseveration
                bias[last_action[s]] += perseveration_kappa * (3.0 / nS)
            logits_rl = logits_rl + bias
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = float(pi_rl[a])

            # WM policy with precision control
            W_s = w[s, :]
            logits_wm = (softmax_beta_wm * max(wm_precision, eps)) * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = float(pi_wm[a])

            # Gated mixture weight based on PE magnitude, set size, and age bias
            pe_mag = abs(prev_pe[s])
            gate_input = setsize_gate_slope * (3.0 - nS) - gate_PE_scale * pe_mag + gate_bias_age * (1 - 2 * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))

            # Mixture and log-likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: write toward chosen action; stronger on reward
            alpha_wm = 0.8 * r + 0.2 * (1.0 - r)  # 0.8 on reward, 0.2 on no-reward
            # Decay toward uniform then push chosen up
            w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * w_0[s, :]
            w[s, a] += alpha_wm * (1.0 - w[s, a])
            # Renormalize for numerical stability
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update trackers
            last_action[s] = a
            prev_pe[s] = pe

        blocks_log_p += log_p

    return -blocks_log_p