Here are three alternative cognitive models that implement RLâ€“WM hybrids with different mechanisms for arbitration, memory maintenance, and age/set-size effects. Each returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy-based arbitration and load-sensitive WM precision, plus RL decay.

    Mechanism
    - RL: tabular Q-learning with softmax; Q decays toward uniform (to model forgetting).
    - WM: stores last rewarded action per state; WM representation decays toward uniform
      with a rate that increases with set size and age.
    - Arbitration: combines WM and RL using a sigmoid of (WM confidence - RL entropy),
      then scaled by a load-sensitive WM capacity factor modulated by age.

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials. Age group: 0 if <=45 else 1.
    model_parameters : list/tuple
        [lr, beta, wm_base, arb_gain, age_wm_boost, q_decay]
        - lr: RL learning rate (0..1).
        - beta: base inverse temperature (scaled by 10 internally).
        - wm_base: baseline WM decay/capacity rate (>0). Higher means faster WM decay.
        - arb_gain: slope of arbitration sigmoid over (WM confidence - RL entropy).
        - age_wm_boost: boost to WM capacity for young (applied as multiplier when age_group=0).
        - q_decay: per-visit decay of Q toward uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_base, arb_gain, age_wm_boost, q_decay = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice probability for chosen action
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over current WM weights (deterministic)
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration signal components
            # - WM confidence: distance from uniform (max probability minus 1/nA)
            wm_conf = float(np.max(W_s) - (1.0 / nA))
            wm_conf = np.clip(wm_conf, 0.0, 1.0)

            # - RL entropy (normalized 0..1)
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / max(np.sum(pi_rl), 1e-12)
            rl_entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            rl_entropy_norm = rl_entropy / np.log(nA)

            # Load- and age-modulated WM capacity factor
            # Higher set size reduces capacity; young receive a boost.
            load_scale = 1.0 / (1.0 + (nS - 1.0))  # 1 for nS=1, 0.5 for nS=3, ~0.33 for nS=6
            age_boost = (1.0 + age_wm_boost) if age_group == 0 else 1.0
            cap = np.clip(age_boost * load_scale, 0.0, 1.0)

            # Arbitration weight via sigmoid over confidence minus entropy, scaled by capacity
            arb_input = arb_gain * (wm_conf - rl_entropy_norm)
            wm_weight = cap * (1.0 / (1.0 + np.exp(-arb_input)))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            base = 1.0 / nA
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * base
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the visited state
            # Decay rate increases with set size and age.
            lam = 1.0 - np.exp(-wm_base * (max(1.0, float(nS)) - 1.0) * (1.0 + 0.5 * age_group))
            lam = float(np.clip(lam, 0.0, 1.0))
            w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

            # If rewarded, write the winning action strongly into WM
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-adaptive learning rate and WM gating; age-dependent lapse.

    Mechanism
    - RL: Q-learning with a per-state surprise trace phi(s) that up-regulates the
      learning rate on surprising outcomes.
    - WM: one-shot cache; rewarded actions overwrite WM for the state; unrewarded choices
      are inhibited in WM (weight reduction), then renormalized.
    - Arbitration: fixed WM mixture attenuated by set size; age adds a lapse probability.

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials. Age group: 0 if <=45 else 1.
    model_parameters : list/tuple
        [lr_base, lr_sens, phi_decay, beta, wm_mix_base, age_lapse]
        - lr_base: baseline RL learning rate (0..1).
        - lr_sens: sensitivity of learning rate to surprise phi (>=0).
        - phi_decay: rate at which phi tracks |PE| (0..1).
        - beta: inverse temperature for RL softmax (scaled by 10 internally).
        - wm_mix_base: baseline WM mixture weight before load/age adjustments (0..1).
        - age_lapse: added lapse probability for older group (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_base, lr_sens, phi_decay, softmax_beta, wm_mix_base, age_lapse = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        phi = np.zeros(nS)  # per-state surprise trace

        # WM mixture attenuates with set size; slight reduction for older adults implicitly through lapse
        wm_weight_block = wm_mix_base * np.exp(-(max(1, nS) - 3.0) / 2.0)
        wm_weight_block = float(np.clip(wm_weight_block, 0.0, 1.0))

        lapse = float(np.clip(age_lapse * age_group, 0.0, 0.5))  # only older group lapses

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice probability for chosen action
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy probability for chosen action
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with lapse
            mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Compute PE and update surprise trace
            pe = r - q[s, a]
            phi[s] = (1.0 - phi_decay) * phi[s] + phi_decay * abs(pe)
            lr_t = np.clip(lr_base + lr_sens * phi[s], 0.0, 1.0)

            # RL update
            q[s, a] += lr_t * pe

            # WM update:
            # - Reward: overwrite WM with one-hot
            # - No reward: inhibit chosen action in WM, then renormalize
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, a] *= 0.5
                # Small recovery for non-chosen actions toward uniform
                not_a = [aa for aa in range(nA) if aa != a]
                for aa in not_a:
                    w[s, aa] = 0.5 * w[s, aa] + 0.5 * w_0[s, aa]
                # Renormalize to avoid degeneracy
                total = np.sum(w[s, :])
                if total <= 1e-8:
                    w[s, :] = w_0[s, :]
                else:
                    w[s, :] /= total

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration and recency-based WM retrieval probability.

    Mechanism
    - RL: Q-learning with a directed exploration bonus proportional to 1/sqrt(visits),
      encouraging uncertainty-driven choices early on.
    - WM: rewarded action cache with recency- and load-dependent decay; age accelerates decay.
    - Arbitration: probability of using WM is a logistic function of recency, load (set size),
      and an overall WM bias parameter.

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials. Age group: 0 if <=45 else 1.
    model_parameters : list/tuple
        [alpha_rl, beta, bonus_gain, recency_decay, age_mem_pen, wm_use_bias]
        - alpha_rl: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (scaled by 10 internally).
        - bonus_gain: magnitude of directed exploration bonus (>=0).
        - recency_decay: governs WM decay with time since last visit and load (>=0).
        - age_mem_pen: additional decay pressure for older group (>=0).
        - wm_use_bias: baseline bias to use WM (positive favors WM).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_rl, softmax_beta, bonus_gain, recency_decay, age_mem_pen, wm_use_bias = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for directed exploration and state recency tracking
        visits = np.zeros((nS, nA))  # counts per state-action
        last_seen = -1 * np.ones(nS)  # last trial index seen per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Time since last visit to state
            t_since = 0.0 if last_seen[s] < 0 else float(t - last_seen[s])
            last_seen[s] = t

            # RL choice probability: add directed exploration bonus
            bonus = bonus_gain / np.sqrt(np.maximum(1.0, visits[s, :]))
            Q_eff = q[s, :] + bonus
            denom_rl = np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy probability from WM weights
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM retrieval probability: logistic of recency, load, and age
            load_term = np.log(max(1, nS))  # larger for larger set sizes
            wm_logit = wm_use_bias - recency_decay * (t_since + 1.0) * load_term - age_mem_pen * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha_rl * pe

            # Update visit counts after learning
            visits[s, a] += 1.0

            # WM decay with recency and load, then reward-based overwrite
            decay_strength = 1.0 - np.exp(-recency_decay * (t_since + 1.0) * load_term * (1.0 + 0.5 * age_group))
            decay_strength = float(np.clip(decay_strength, 0.0, 1.0))
            w[s, :] = (1.0 - decay_strength) * w[s, :] + decay_strength * w_0[s, :]

            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size influences
- Model 1: WM capacity scales down with set size and is boosted for younger adults; WM decays faster with larger set size and for older adults; arbitration compares WM confidence to RL entropy.
- Model 2: Older adults have an added lapse probability; WM mixture is attenuated exponentially with set size. Surprise increases RL learning rate via phi.
- Model 3: WM retrieval probability decreases with time since last visit and with load; older age increases effective decay. RL includes a directed exploration bonus that does not depend on age or set size.