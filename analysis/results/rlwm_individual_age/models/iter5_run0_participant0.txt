def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with load- and age-modulated mixture and asymmetric WM encoding.

    Mechanism
    - RL: Tabular Q-learning with softmax policy (inverse temperature scaled by 10).
    - WM: High-gain softmax over a per-state categorical memory vector.
      When rewarded, WM is pushed toward the chosen action; when not rewarded,
      it is mildly pushed away (asymmetric encoding).
    - Mixing: Trial-wise WM mixture is a logistic transform of a baseline (wm_logit)
      reduced by task load that combines set size and age.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1].
    - wm_logit: baseline logit for WM mixture; transformed via sigmoid to [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - eta_pos: WM encoding strength on rewarded trials in [0,1].
    - neg_ratio: ratio in [0,1] setting eta_neg = neg_ratio * eta_pos.
    - k_load_age: sensitivity of WM mixture to load = (set_size/3 + age_group); higher reduces WM use.

    Age group coding
    - age_group = 0 for young (<=45), 1 for old (>45). Here age=24 => young.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_logit, softmax_beta, eta_pos, neg_ratio, k_load_age = model_parameters
    softmax_beta *= 10.0  # higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM policy

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline (not directly used but kept per template)

        # Effective WM mixture after accounting for load (set size and age)
        # Load term increases with set size and for older adults
        load = (float(nS) / 3.0) + float(age_group)
        wm_weight = 1.0 / (1.0 + np.exp(-(wm_logit - k_load_age * load)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            # WM policy (high-gain softmax over WM vector)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: asymmetric encoding depending on reward
            eta_neg = max(0.0, min(1.0, neg_ratio)) * max(0.0, min(1.0, eta_pos))
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - eta_pos) * w[s, :] + eta_pos * target
            else:
                # Gentle push away from the chosen action on errors
                anti = np.ones(nA) / nA
                anti[a] = 0.0
                anti = anti / np.sum(anti)
                w[s, :] = (1.0 - eta_neg) * w[s, :] + eta_neg * anti

            # Renormalize to avoid drift
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + leaky WM store with reward-driven boosts; load- and age-modulated WM weight.

    Mechanism
    - RL: Tabular Q-learning with softmax policy.
    - WM: A per-state categorical distribution w[s] that leaks toward uniform each trial.
      Rewarded choices receive an additive boost to the chosen action before renormalization.
    - Mixing: WM mixture is reduced by task load (set size and age).

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1].
    - wm_logit: baseline mixture logit; transformed to [0,1] via sigmoid.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_leak: per-visit leak toward uniform in [0,1].
    - boost: reward-driven boost magnitude added to the chosen action (>=0).
    - k_load_age: reduction of WM mixture per unit load (set_size/3 + age_group).

    Age group coding
    - 0 for young (<=45), 1 for old (>45). Here age=24 => young.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_logit, softmax_beta, wm_leak, boost, k_load_age = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load = (float(nS) / 3.0) + float(age_group)
        wm_weight = 1.0 / (1.0 + np.exp(-(wm_logit - k_load_age * load)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform
            wm_leak_clipped = max(0.0, min(1.0, wm_leak))
            w[s, :] = (1.0 - wm_leak_clipped) * w[s, :] + wm_leak_clipped * (1.0 / nA)

            # Reward-driven boost to chosen action (then renormalize)
            if r > 0.0 and boost > 0.0:
                w[s, a] += boost

            # Renormalize and floor
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with PE-based arbitration and capacity-limited encoding.

    Mechanism
    - RL: Tabular Q-learning with softmax policy.
    - WM: One-shot encoding toward the chosen action with strength proportional
      to an effective capacity term that decreases with set size and for older adults.
    - Arbitration: Mixture weight increases when the absolute RL prediction error
      is small (confidence high), and decreases when the PE is large; age reduces this sensitivity.

    Parameters (model_parameters)
    - lr: RL learning rate in [0,1].
    - wm_logit: baseline mixture logit; transformed via sigmoid.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - k_pe: sensitivity of WM mixture to (1 - |PE|); larger k_pe increases reliance on WM when PE small.
    - rho: base WM overwrite strength in [0,1].
    - age_penalty: reduces WM encoding capacity for older adults (>=0).

    Age and set-size effects
    - Effective WM encoding strength per trial: rho_eff = rho * (3/set_size) * (1 - age_group * age_penalty),
      clipped to [0,1].
    - Effective mixture per trial: sigmoid(wm_logit + k_pe * (1 - |PE|)), where PE is computed from RL before update.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_logit, softmax_beta, k_pe, rho, age_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # Compute RL PE before updating
            pe = r - Q_s[a]
            conf = 1.0 - np.minimum(1.0, np.maximum(0.0, abs(pe)))  # conf in [0,1]
            wm_weight = 1.0 / (1.0 + np.exp(-(wm_logit + k_pe * conf)))

            # Policies
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / (denom_rl + eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / (denom_wm + eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            q[s, a] += lr * pe

            # WM encoding with capacity limit
            cap = (3.0 / float(nS)) * (1.0 - float(age_group) * max(0.0, age_penalty))
            rho_eff = np.clip(rho * cap, 0.0, 1.0)

            if rho_eff > 0.0:
                target = np.zeros(nA)
                # Encode toward chosen action when rewarded; weak encode otherwise
                if r > 0.0:
                    target[a] = 1.0
                    w[s, :] = (1.0 - rho_eff) * w[s, :] + rho_eff * target
                else:
                    # On error, slight tilt away from chosen action
                    anti = np.ones(nA) / nA
                    anti[a] = 0.0
                    anti = anti / np.sum(anti)
                    w[s, :] = (1.0 - 0.5 * rho_eff) * w[s, :] + (0.5 * rho_eff) * anti

                w[s, :] = np.maximum(w[s, :], eps)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p