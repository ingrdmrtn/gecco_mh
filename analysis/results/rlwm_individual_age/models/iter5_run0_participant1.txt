def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + graded WM with confidence-based arbitration and age- and set-sizeâ€“dependent WM precision.

    Idea:
    - RL learns Q-values with a single learning rate and softmax choice.
    - WM stores state-action preferences that decay toward uniform unless reinforced.
    - WM choice is a softmax over WM preferences with a WM inverse temperature that
      decreases with set size and for older adults.
    - Arbitration weight is proportional to WM confidence (max probability) scaled by a persistence parameter
      and reduced for older adults.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = wm_persist in [0,1]: WM retention; 1=no decay, 0=immediate reset to uniform
    - model_parameters[3] = wm_beta_base (>0): base WM inverse temperature multiplier
    - model_parameters[4] = setsize_wm_cost (>=0): decreases WM precision as set size increases
    - model_parameters[5] = age_wm_bias in [0,1]: reduces WM influence for older adults

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays
    - model_parameters: list/array of parameters

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_persist, wm_beta_base, setsize_wm_cost, age_wm_bias = model_parameters

    # Clamp and scale
    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_persist = min(max(wm_persist, 0.0), 1.0)
    wm_beta_base = max(wm_beta_base, 1e-6)
    setsize_wm_cost = max(setsize_wm_cost, 0.0)
    age_wm_bias = min(max(age_wm_bias, 0.0), 1.0)

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM preferences and uniform baseline
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL softmax probability of chosen action (efficient form)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM precision depends on set size and age
            # Higher set size => lower WM beta; older => lower WM beta
            wm_beta = wm_beta_base * np.exp(-setsize_wm_cost * max(nS_t - 3, 0)) * (1.0 - 0.5 * age_wm_bias * age_group)
            wm_beta = max(wm_beta, 1e-6) * 50.0  # scale to high range

            # WM policy from current preferences
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(wm_beta * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM confidence (max prob under WM policy)
            wm_policy = np.exp(wm_beta * (W_s - np.max(W_s)))
            wm_policy /= max(np.sum(wm_policy), eps)
            wm_conf = float(np.max(wm_policy))

            # Arbitration weight: confidence scaled, reduced for older adults
            w_wm = wm_conf * wm_persist * (3.0 / max(nS_t, 1.0))
            w_wm *= (1.0 - age_wm_bias * age_group)
            w_wm = min(max(w_wm, 0.0), 1.0)

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for visited state
            w[s, :] = wm_persist * w[s, :] + (1.0 - wm_persist) * w0[s, :]

            # WM encoding: if rewarded, move strongly toward chosen action
            if r > 0.0:
                # Overwrite with a peaked distribution on the rewarded action
                w[s, :] = (1.0 - 1e-6) * 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + cached WM with uncertainty-based arbitration, age-dependent WM noise, and lapse.

    Idea:
    - RL learns Q-values with a single learning rate and softmax choice.
    - WM caches the last rewarded action for each state; if present, WM proposes that action.
    - Arbitration weight increases with estimated RL uncertainty (low visit counts),
      and decreases with set size; older adults have noisier WM policies.
    - A small lapse probability mixes in uniform responding.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = wm_strength in [0,1]: maximum arbitration weight for WM
    - model_parameters[3] = uncert_sensitivity (>0): gain mapping uncertainty to WM weight
    - model_parameters[4] = age_noise in [0,1]: WM noise added for older adults (0=noise-free)
    - model_parameters[5] = lapse in [0,0.2]: lapse probability

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_strength, uncert_sensitivity, age_noise, lapse = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_strength = min(max(wm_strength, 0.0), 1.0)
    uncert_sensitivity = max(uncert_sensitivity, 1e-6)
    age_noise = min(max(age_noise, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.2)

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))  # for uncertainty estimation

        cached_action = -np.ones(nS, dtype=int)  # WM cache

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (deterministic cache with age-dependent noise)
            if cached_action[s] >= 0:
                wm_policy = np.zeros(nA)
                wm_policy[cached_action[s]] = 1.0
            else:
                wm_policy = np.ones(nA) / nA

            # Age-dependent WM noise: older adults mix uniform into WM policy
            noise_level = age_noise * age_group
            wm_policy = (1.0 - noise_level) * wm_policy + noise_level * (np.ones(nA) / nA)
            p_wm = wm_policy[a]

            # Uncertainty from counts at this state
            c_state = np.sum(counts[s, :])
            # Map uncertainty to [0,1]: high when counts are low
            uncertainty = 1.0 / (1.0 + c_state)
            # Arbitration weight increases with uncertainty, decreases with set size
            w_wm = wm_strength * (1.0 - (nS_t - 3) / max(nS_t, 1.0)) * (1.0 - np.exp(-uncert_sensitivity * uncertainty))
            w_wm = min(max(w_wm, 0.0), 1.0)

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            # Lapse
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update counts
            counts[s, a] += 1.0

            # Update WM cache on reward
            if r > 0.0:
                cached_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + interference-driven WM with confidence arbitration.

    Idea:
    - RL learns Q-values with a single learning rate and softmax choice.
    - WM maintains, for each state, a preference distribution over actions.
      Rewarded actions shift the distribution toward a sharp peak (wm_peak),
      while ongoing interference pushes WM toward uniform at a rate that
      increases with set size and with age.
    - Arbitration weight is proportional to the peakedness (max probability) of the WM distribution.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = wm_peak in (0,1]: strength of WM peaking toward the rewarded action
    - model_parameters[3] = interference_rate in [0,1]: base WM interference toward uniform per trial
    - model_parameters[4] = setsize_interf_gain (>=0): scales interference with set size
    - model_parameters[5] = age_interf_gain (>=0): additional interference for older adults

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_peak, interference_rate, setsize_interf_gain, age_interf_gain = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_peak = min(max(wm_peak, 1e-6), 1.0)
    interference_rate = min(max(interference_rate, 0.0), 1.0)
    setsize_interf_gain = max(setsize_interf_gain, 0.0)
    age_interf_gain = max(age_interf_gain, 0.0)

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM preferences start uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        softmax_beta_wm = 50.0  # deterministic-ish WM readout

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM confidence = peakedness of distribution
            wm_policy = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_policy /= max(np.sum(wm_policy), eps)
            wm_conf = float(np.max(wm_policy))

            # Arbitration weight scales with WM confidence and is lower for larger set sizes
            w_wm = wm_conf * (3.0 / max(nS_t, 1.0))
            # Age increases interference rather than directly changing weight, but confidence captures that.

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Global WM interference toward uniform each trial
            gamma = interference_rate * (1.0 + setsize_interf_gain * max(nS_t - 3, 0)) * (1.0 + age_interf_gain * age_group)
            gamma = min(max(gamma, 0.0), 1.0)
            w = (1.0 - gamma) * w + gamma * w0

            # Reward-driven peaking toward chosen action for the current state
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_peak) * w[s, :] + wm_peak * onehot

        blocks_log_p += log_p

    return -blocks_log_p