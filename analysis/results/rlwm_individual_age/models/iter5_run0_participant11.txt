def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic slot-like WM with reward-gated writing and noisy retrieval.

    Mechanism:
    - Model-free RL learns Q-values.
    - A slot-like WM holds a probabilistic policy per state, with a memory strength m[s] in [0,1].
    - After rewarded trials, the chosen action is written to WM with probability modulated by set size and age.
    - WM retrieval is noisy when memory strength is low and becomes more reliable as m[s] increases.
    - Final choice is a mixture of WM and RL, with WM influence reduced by larger set size and by age group.

    Parameters (list of length 5):
    - lr: RL learning rate in (0,1).
    - wm_weight_base: base mixture weight for WM influence in (0,1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_write_prob: base probability/step-size of writing to WM after reward, in (0,1).
    - retrieval_noise: base retrieval noise for WM in (0,1), larger => noisier WM; scaled by set size and age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_write_prob, retrieval_noise = model_parameters

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM when confident

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM policy table and default
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Slot-like memory strength per state
        m = np.zeros(nS)

        # Mixture scaling by set size and age
        wm_weight_eff = wm_weight_base / (1.0 + max(0, nS - 3))
        wm_weight_eff *= (1.0 - 0.35 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # Effective write probability and retrieval noise with set size and age
        write_prob_eff = wm_write_prob / (1.0 + 0.5 * max(0, nS - 3))
        write_prob_eff *= (1.0 - 0.3 * age_group)
        write_prob_eff = np.clip(write_prob_eff, 0.0, 1.0)

        retrieval_noise_eff_base = retrieval_noise * (1.0 + 0.5 * max(0, nS - 3)) * (1.0 + 0.5 * age_group)
        retrieval_noise_eff_base = np.clip(retrieval_noise_eff_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action (softmax shortcut)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for state s
            W_s = w[s, :]
            # Deterministic WM proposal
            wm_prefs = softmax_beta_wm * W_s
            wm_prefs -= np.max(wm_prefs)
            exp_wm = np.exp(wm_prefs)
            p_wm_vec = exp_wm / np.sum(exp_wm)

            # Retrieval noise depends on memory strength m[s]; more noise when memory is weak
            noise_s = retrieval_noise_eff_base * (1.0 - m[s])
            noise_s = np.clip(noise_s, 0.0, 1.0)
            p_wm_vec = (1.0 - noise_s) * p_wm_vec + noise_s * (1.0 / nA)

            p_wm = p_wm_vec[a]

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated writing strengthens memory for the chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Update WM policy toward the rewarded action
                w[s, :] = (1.0 - write_prob_eff) * w[s, :] + write_prob_eff * target
                # Increase memory strength
                m[s] = np.clip(m[s] + write_prob_eff * (1.0 - m[s]), 0.0, 1.0)
            else:
                # On non-reward, slight drift back toward default and mild weakening of memory
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                m[s] = np.clip(m[s] * (1.0 - 0.1 * write_prob_eff), 0.0, 1.0)

            # Normalize WM rows
            row_s = np.sum(w[s, :])
            if row_s > 0:
                w[s, :] /= row_s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size/age-dependent WM decay.

    Mechanism:
    - RL learns Q-values (softmax).
    - WM stores a probabilistic policy per state, updated toward rewarded actions and decaying to uniform.
    - Arbitration weight for WM is computed by a logistic function of WM confidence minus RL entropy,
      with biases controlled by set size and age.
    - Larger set size and older age increase WM decay and reduce WM arbitration.

    Parameters (list of length 6):
    - lr: RL learning rate in (0,1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_slope: slope for arbitration sigmoid mapping (confidence difference) to WM weight.
    - wm_bias: base bias for arbitration sigmoid.
    - decay_base: base WM decay rate toward uniform in (0,1).
    - age_bias: additive bias applied to arbitration for age group (negative values reduce WM use in older).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_slope, wm_bias, decay_base, age_bias = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay worsens with set size and age
        decay_eff = decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL probability for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL full policy for entropy
            rl_prefs = softmax_beta * Q_s
            rl_prefs -= np.max(rl_prefs)
            exp_rl = np.exp(rl_prefs)
            rl_pol = exp_rl / np.sum(exp_rl)
            rl_entropy = -np.sum(rl_pol * np.log(np.clip(rl_pol, 1e-12, 1.0)))  # higher = more uncertain

            # WM policy and confidence
            W_s = w[s, :]
            wm_prefs = softmax_beta_wm * W_s
            wm_prefs -= np.max(wm_prefs)
            exp_wm = np.exp(wm_prefs)
            wm_pol = exp_wm / np.sum(exp_wm)
            p_wm = wm_pol[a]
            wm_conf = np.max(wm_pol) - (1.0 / nA)  # 0 when uniform, increases with confidence

            # Arbitration: sigmoid of confidence difference, with set-size and age biases
            setsize_bias = -0.5 * max(0, nS - 3)  # penalize WM with larger set size
            arb_input = wm_bias + wm_slope * (wm_conf - rl_entropy) + setsize_bias + age_bias * age_group
            wm_weight_t = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update toward rewarded action, else mild drift to uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - 0.5) * w[s, :] + 0.5 * target  # fast encoding on reward
            else:
                # slight avoidance of chosen action
                w[s, a] = 0.9 * w[s, a]
                # renormalize local row softly
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Global WM decay toward uniform (set-size and age dependent)
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # Normalize rows
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums = np.clip(row_sums, 1e-12, None)
            w = w / row_sums

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian WM (Dirichlet counts) with set-size/age-driven interference.

    Mechanism:
    - RL learns Q-values.
    - WM maintains Dirichlet pseudo-counts per state-action (initial alpha0).
      The WM policy is the posterior mean over actions (counts normalized).
    - Reward increments WM counts for the chosen action.
    - Interference: each trial contaminates the WM counts of other states toward the current choice,
      with strength growing in larger set sizes and with age.

    Parameters (list of length 6):
    - lr: RL learning rate in (0,1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight_base: base mixture weight for WM policy in (0,1).
    - alpha0: initial Dirichlet concentration per action (>0).
    - interference_gamma: base interference rate in (0,1).
    - age_interf_gain: nonnegative gain scaling that increases interference with age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, alpha0, interference_gamma, age_interf_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # used only if converting to softmax; here we use normalized counts

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM as Dirichlet counts; start with symmetric alpha0
        w_counts = np.full((nS, nA), max(alpha0, 1e-6))
        # We'll also keep a normalized version for convenience
        w = (w_counts.T / np.sum(w_counts, axis=1)).T
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture weight reduced by set size and age
        wm_weight_eff = wm_weight_base / (1.0 + 0.5 * max(0, nS - 3))
        wm_weight_eff *= (1.0 - 0.4 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # Interference strength increases with set size and age
        interf_eff = interference_gamma * (nS / 3.0) * (1.0 + age_interf_gain * age_group)
        interf_eff = np.clip(interf_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL probability for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM posterior mean policy from counts
            counts_s = w_counts[s, :]
            wm_pol = counts_s / np.sum(counts_s)
            p_wm = wm_pol[a]

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward increments chosen action count; non-reward adds small smoothing
            if r > 0.5:
                w_counts[s, a] += 1.0
            else:
                # slight symmetric smoothing to keep counts well-behaved
                w_counts[s, :] = np.maximum(w_counts[s, :] + 0.01 * (w_0[s, :] * np.sum(w_counts[s, :]) - w_counts[s, :]), 1e-6)

            # Interference: diffuse the chosen action toward other states
            if nS > 1 and interf_eff > 0.0:
                for s_other in range(nS):
                    if s_other == s:
                        continue
                    # Move a small fraction of s_other counts toward emphasizing the chosen action a
                    total_other = np.sum(w_counts[s_other, :])
                    target = np.copy(w_counts[s_other, :])
                    # Target puts more mass on action a while conserving total in expectation
                    emphasize = np.zeros(nA)
                    emphasize[a] = total_other
                    w_counts[s_other, :] = (1.0 - interf_eff) * w_counts[s_other, :] + interf_eff * (0.5 * target + 0.5 * emphasize)

            # Recompute normalized WM policy table
            w_sums = np.sum(w_counts, axis=1, keepdims=True)
            w_sums = np.clip(w_sums, 1e-12, None)
            w = w_counts / w_sums

        blocks_log_p += log_p

    return -blocks_log_p