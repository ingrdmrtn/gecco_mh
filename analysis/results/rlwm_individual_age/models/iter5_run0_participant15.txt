def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Capacity-limited WM with uncertainty-based arbitration and perseveration.

    Overview:
    - Policy is a mixture of RL softmax and WM softmax. Arbitration uses an uncertainty signal based on
      WM capacity relative to set size: higher capacity relative to set size -> greater WM control.
    - WM decays toward uniform at a rate determined by capacity vs. set size, and encodes more strongly after reward.
    - RL updates with a single learning rate.
    - Action perseveration bias (stickiness) is applied to both RL and WM logits.
    - Age reduces effective WM capacity; larger set size reduces WM influence.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - K_base: Base WM capacity (in items) for young group; older group has reduced effective capacity
    - wm_encode: Strength/probability of WM encoding update per rewarded trial in [0,1]
    - arb_theta: Arbitration slope; larger values make WM-RL arbitration more categorical
    - persev: Perseveration weight added to the last chosen action's logit

    Inputs:
    - states: array of states per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices
    - set_sizes: array of set sizes per trial (constant within block; values 3 or 6)
    - age: array with a single repeated value for participant age
    - Returns: Negative log-likelihood of the observed choices
    """
    lr, beta_rl, K_base, wm_encode, arb_theta, persev = model_parameters

    beta_rl *= 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity reduced by age
        K_eff = max(0.1, K_base * (1.0 - 0.5 * age_group))  # older adults: lower effective capacity

        prev_action = None
        log_p = 0.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Perseveration bias on logits
            bias = np.zeros(nA)
            if prev_action is not None:
                bias[prev_action] += persev

            # RL policy
            rl_logits = beta_rl * Q_s + bias
            rl_logits -= np.max(rl_logits)
            rl_probs = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            p_rl = max(rl_probs[a], 1e-12)

            # WM policy
            wm_logits = beta_wm * W_s + bias
            wm_logits -= np.max(wm_logits)
            wm_probs = np.exp(wm_logits) / np.sum(np.exp(wm_logits))
            p_wm = max(wm_probs[a], 1e-12)

            # Arbitration based on relative capacity precision
            # Precision proxy: prec in [0,1] increases when K_eff >= nS; 0 when K_eff << nS
            prec = np.clip(K_eff / float(nS), 0.0, 1.0)
            # WM weight via sigmoid around prec=1 with slope arb_theta
            wm_weight = 1.0 / (1.0 + np.exp(-arb_theta * (prec - 0.5)))  # center at 0.5 precision

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform; faster decay when capacity << set size
            decay_rate = np.clip(1.0 - prec, 0.0, 1.0)
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w0[s, :]

            # WM encoding: stronger after reward; light encoding after non-reward
            alpha_enc = wm_encode if r > 0.5 else 0.2 * wm_encode
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            w[s, :] = (1.0 - alpha_enc) * w[s, :] + alpha_enc * one_hot

            prev_action = a

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM retrieval interference and lapse.

    Overview:
    - RL uses separate learning rates for positive and negative prediction errors and a softmax policy.
    - WM stores one-shot mappings on rewarded trials; retrieval is noisy due to interference that increases with set size and age.
    - Policy is a mixture of WM and RL, with WM influence reduced for larger set sizes and older adults.
    - A lapse term adds uniform random choice with probability increasing with set size and age.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE in [0,1]
    - lr_neg: RL learning rate for negative PE in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - interference: Base WM interference parameter (controls probability of retrieving a wrong action)
    - wm_weight0: Base WM mixture weight in [0,1]
    - lapse_base: Base lapse probability in [0,1]

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age
    - Returns: Negative log-likelihood of the observed choices
    """
    lr_pos, lr_neg, beta_rl, interference, wm_weight0, lapse_base = model_parameters

    beta_rl *= 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: one-hot of believed correct action per state; start uniform
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            rl_logits = beta_rl * Q_s
            rl_logits -= np.max(rl_logits)
            rl_probs = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            p_rl = max(rl_probs[a], 1e-12)

            # WM retrieval with interference:
            # If WM has a peaked mapping, treat it as target; interference swaps to other actions.
            peak = np.argmax(W_s)
            conf = W_s[peak]  # how peaked the mapping is
            # Interference probability increases with set size and age
            p_swap = np.clip(interference * (nS / 6.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            # Effective WM distribution: with prob conf*(1-p_swap), take peak; with conf*p_swap, choose among non-peak; with (1-conf), uniform
            wm_probs = np.ones(nA) / nA
            if conf > (1.0 / nA):
                wm_probs = (1.0 - conf) * (np.ones(nA) / nA)
                non_peak = [i for i in range(nA) if i != peak]
                mix = np.zeros(nA)
                mix[peak] = 1.0 - p_swap
                for j in non_peak:
                    mix[j] = p_swap / (nA - 1)
                wm_probs += conf * mix

            wm_probs = np.clip(wm_probs, 1e-12, 1.0)
            wm_probs /= np.sum(wm_probs)
            # Optional deterministic sharpening
            wm_logits = beta_wm * np.log(wm_probs)
            wm_logits -= np.max(wm_logits)
            wm_probs = np.exp(wm_logits) / np.sum(np.exp(wm_logits))
            p_wm = max(wm_probs[a], 1e-12)

            # WM weight reduced by load and age
            wm_weight = wm_weight0 * (3.0 / float(nS)) * (1.0 - 0.3 * age_group)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Lapse increases with load and age
            lapse = np.clip(lapse_base * (1.0 + 0.3 * (nS == 6) + 0.5 * age_group), 0.0, 0.5)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: one-shot encode after reward, slight weakening otherwise
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # small diffusion toward uniform if not rewarded (interference-like)
                w[s, :] = 0.9 * w[s, :] + 0.1 * (np.ones(nA) / nA)

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + meta-control over WM vs RL based on reliability-cost tradeoff.

    Overview:
    - RL: delta rule with replacing eligibility traces; softmax policy.
    - WM: stores deterministic mappings on rewarded trials; softmax with high inverse temperature.
    - Meta-controller computes a WM mixture weight from the tradeoff between WM reliability and its cognitive cost.
      Reliability is a running estimate of WM success; cost increases with set size and age.
    - Policy: p_total = w_meta * p_wm + (1 - w_meta) * p_rl.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - lambda_et: Eligibility trace decay (0..1). Higher -> longer-lasting traces.
    - wm_cost_base: Base WM cost; effective cost increases with set size and age
    - wm_rel_lr: Learning rate for updating WM reliability estimate (0..1)
    - beta_meta: Slope of meta-controller sigmoid; larger -> more categorical arbitration

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age
    - Returns: Negative log-likelihood of the observed choices
    """
    lr, beta_rl, lambda_et, wm_cost_base, wm_rel_lr, beta_meta = model_parameters

    beta_rl *= 10.0
    beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL values and eligibility traces per state-action
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # WM store: last rewarded action per state (deterministic when known)
        w = (1.0 / nA) * np.ones((nS, nA))

        # Meta-controller: initialize global WM reliability estimate
        wm_rel = 0.5

        log_p = 0.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            rl_logits = beta_rl * Q_s
            rl_logits -= np.max(rl_logits)
            rl_probs = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            p_rl = max(rl_probs[a], 1e-12)

            # WM policy
            wm_logits = beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            wm_probs = np.exp(wm_logits) / np.sum(np.exp(wm_logits))
            p_wm = max(wm_probs[a], 1e-12)

            # Meta-control: compute WM weight from reliability - cost
            cost = wm_cost_base * (float(nS) / 3.0) * (1.0 + 0.5 * age_group)
            # Utility difference: WM advantage
            adv = wm_rel - cost
            w_meta = 1.0 / (1.0 + np.exp(-beta_meta * adv))
            w_meta = np.clip(w_meta, 0.0, 1.0)

            p_total = w_meta * p_wm + (1.0 - w_meta) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces (replacing traces per state)
            # Decay all traces
            e *= lambda_et
            # Set chosen state-action trace to 1 (replacing trace)
            e[s, :] *= 0.0
            e[s, a] = 1.0
            pe = r - q[s, a]
            q += lr * pe * e

            # WM update: deterministic encode after reward; slight diffusion otherwise
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * (np.ones(nA) / nA)

            # Update WM reliability: only informative if WM has a peaked guess for this state
            wm_peaked = (np.max(W_s) > (1.0 / nA))
            signal = r if wm_peaked else 0.5  # neutral if WM had no info
            wm_rel += wm_rel_lr * (signal - wm_rel)
            wm_rel = np.clip(wm_rel, 0.0, 1.0)

        total_log_p += log_p

    return -total_log_p