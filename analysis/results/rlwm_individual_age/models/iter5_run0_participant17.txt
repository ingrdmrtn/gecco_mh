def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + surprise-gated Working Memory (WM) with decaying choice bias.

    Mechanism:
    - RL: standard Q-learning with softmax action selection (inverse temperature scaled by 10).
    - WM: stores one-hot associations when feedback is received. Storage is gated by surprise
      (unsigned prediction error), modulated by load and age. WM also leaks toward uniform.
    - Mixture: WM and RL policies are linearly combined with a base WM weight, scaled down by load
      (set size) and scaled up for younger adults via an age parameter.
    - Choice bias: a state-specific decaying bias accumulates for the last chosen action and
      adds to RL logits (captures perseveration/exploitation). The bias decays over trials.

    Parameters (list; all used):
    - lr: RL learning rate (0..1).
    - softmax_beta: base RL inverse temperature; multiplied by 10 internally.
    - wm_weight_base: baseline WM mixture weight (0..1).
    - gate_gain: sensitivity of WM gating to unsigned prediction error (0..10).
    - bias_decay: decay rate of the choice bias toward zero (0..1).
    - age_wm_scale: scales WM influence for younger vs older (positive values boost WM for young).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, gate_gain, bias_decay, age_wm_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        bias = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # RL policy with choice bias on logits
            Q_s = q[s, :]
            logits_rl = softmax_beta * Q_s + bias[s, :]
            logits_rl -= np.max(logits_rl)
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = max(pi_rl[a], eps)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Mixture weight: base scaled by load and age (young get a boost)
            load_scale = 3.0 / set_size
            age_scale = 1.0 + age_wm_scale * (1.0 - age_group)  # only boosts for young if positive
            wm_weight_eff = np.clip(wm_weight_base * load_scale * age_scale, 0.0, 1.0)

            p_total = np.clip(wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Surprise-gated WM storage and leak
            # Surprise from RL: unsigned PE, mapped via sigmoid controlled by gate_gain.
            surprise = abs(delta)
            gate = 1.0 / (1.0 + np.exp(-gate_gain * (surprise - 0.5)))  # low at small PE, high at large PE
            # Load reduces effective gate; older adults have lower effective gating if age_group==1.
            gate *= (3.0 / set_size) * (1.0 - 0.3 * age_group)
            gate = np.clip(gate, 0.0, 1.0)

            # If rewarded, strengthen the chosen action representation in WM
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - gate) * w[s, :] + gate * target

            # Leak toward uniform (stronger leak with higher load and for older adults)
            leak = np.clip(0.1 + 0.2 * (set_size / 3.0) + 0.2 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Update and decay the choice bias (perseveration)
            bias *= (1.0 - bias_decay)
            bias[s, a] += (1.0 - 0.3 * age_group) * 0.5  # smaller accumulation for older adults

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay and load/age-dependent WM with interference-based overwriting.

    Mechanism:
    - RL: Q-learning with decay toward uniform (captures forgetting), softmax policy (beta*10).
    - WM: one-hot items stored on reward; interference causes partial overwriting as a function
      of set size. WM also leaks to uniform each trial.
    - Mixture: WM vs RL weight is a logistic transform of a base WM weight with an age-dependent
      shift; higher set size reduces WM influence.

    Parameters (list; all used):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; multiplied by 10 internally.
    - wm_weight_base: base WM weight before logistic squashing.
    - rl_decay: RL decay rate toward uniform (0..1).
    - wm_interference: probability mass shifted to competing actions due to interference (0..1).
    - age_mix_shift: additive shift in the WM mixture logit for young vs old (positive favors young).

    Returns negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, rl_decay, wm_interference, age_mix_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = max(pi_rl[a], eps)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Mixture weight via logistic transform with age and load
            # Load pushes the mixture toward RL as set size increases.
            load_term = -1.0 * (set_size - 3) / 3.0  # 0 at size=3, -1 at size=6
            age_term = age_mix_shift * (1.0 - age_group) - 0.2 * age_group
            logit_w = wm_weight_base + load_term + age_term
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = np.clip(wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)

            # WM update: on reward, store one-hot with interference spreading to non-chosen actions
            if r > 0:
                # allocate (1 - interference) to chosen, distribute interference across others
                w_chosen = max(0.0, 1.0 - wm_interference * (set_size / 3.0) * (1.0 + 0.5 * age_group))
                w_other_total = 1.0 - w_chosen
                per_other = w_other_total / (nA - 1)
                target = np.ones(nA) * per_other
                target[a] = w_chosen
                w[s, :] = target

            # WM leak toward uniform each trial (stronger with load and for older adults)
            leak = np.clip(0.05 + 0.15 * (set_size / 3.0) + 0.15 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted arbitration between RL and WM.

    Mechanism:
    - RL: Q-learning plus an uncertainty estimate per state-action via a running variance of TD errors.
      Policy is softmax with fixed beta (scaled by 10).
    - WM: one-hot associative memory with decay toward uniform.
    - Arbitration: the mixture weight favors the system with lower uncertainty.
      WM uncertainty is approximated by the entropy of its state-specific distribution;
      RL uncertainty is approximated by the mean variance across actions in the state.
      Age modulates how strongly uncertainty impacts arbitration; load reduces WM confidence.

    Parameters (list; all used):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; multiplied by 10 internally.
    - wm_decay: WM decay rate toward uniform (0..1).
    - var_lr: learning rate for updating RL variance estimates (0..1).
    - wm_weight_base: baseline WM mixture weight (0..1).
    - age_uncert_gain: weight of uncertainty-based arbitration, scaled by age (can be positive/negative).

    Returns negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_decay, var_lr, wm_weight_base, age_uncert_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        v = np.ones((nS, nA)) * 0.25  # initial variance proxy
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = max(pi_rl[a], eps)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Uncertainty estimates
            # RL uncertainty: mean variance across actions at state s
            rl_uncert = np.mean(v[s, :])
            rl_conf = 1.0 / (1.0 + rl_uncert)  # higher is more confident

            # WM uncertainty: normalized entropy of W_s
            w_safe = np.clip(W_s, eps, 1.0)
            w_safe = w_safe / np.sum(w_safe)
            h = -np.sum(w_safe * np.log(w_safe)) / np.log(nA)  # in [0,1]
            wm_conf = 1.0 - h  # higher is more confident

            # Age and load modulate arbitration
            # Load reduces WM confidence; older adults rely less on uncertainty cue (smaller gain)
            wm_conf_eff = np.clip(wm_conf * (3.0 / set_size), 0.0, 1.0)
            gain = age_uncert_gain * (1.0 - 0.5 * age_group)

            wm_weight_uncert = wm_conf_eff / (wm_conf_eff + (1.0 - rl_conf) + eps)
            wm_weight_uncert = np.clip(wm_weight_uncert, 0.0, 1.0)

            wm_weight_eff = np.clip((1.0 - gain) * wm_weight_base + gain * wm_weight_uncert, 0.0, 1.0)

            p_total = np.clip(wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL updates
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Update variance proxy with squared PE for the chosen action
            v[s, a] = (1.0 - var_lr) * v[s, a] + var_lr * (delta ** 2)

            # WM update: rewarded one-hot with decay
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - 0.7) * w[s, :] + 0.7 * target  # strong store on reward

            # WM decay toward uniform each trial, scaled by load and age
            decay_eff = np.clip(wm_decay * (set_size / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p