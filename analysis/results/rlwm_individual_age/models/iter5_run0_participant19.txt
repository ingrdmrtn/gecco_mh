Below are three alternative cognitive models for the RL+WM task. Each is a standalone Python function that:

- Takes inputs: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Implements a block-wise RL + WM mixture with distinct mechanisms
- Uses all parameters meaningfully (≤ 6 per model)
- Returns the negative log-likelihood of observed choices
- Incorporates age group (0=young, 1=old) and set size effects

Note: Assume numpy (np) and other basics are already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated WM with set-size-specific WM noise and decay.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: one-shot rewarded cache; when r=1, WM for that state becomes a one-hot vector for the chosen action.
          Otherwise, WM decays toward uniform.
    - Arbitration: WM weight is a function of RL uncertainty (entropy) via a sigmoid gate (unc_slope),
                   biased by age (age_gate_bias), and reduced by set-size-dependent WM noise when nS=6.
      wm_weight_eff = sigmoid(age_gate_bias + unc_slope * H_rl_norm) * (1 - noise),
      where noise = wm_noise_size6 if set size=6 else 0.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_decay, wm_noise_size6, age_gate_bias, unc_slope]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature (scaled by *10 internally)
      wm_decay: WM decay toward uniform on each trial in [0,1]
      wm_noise_size6: additional mixture noise that down-weights WM when set size=6 (0..1)
      age_gate_bias: age-dependent bias term in the WM gate; higher -> more WM reliance (young has no penalty)
      unc_slope: slope that increases WM weight when RL policy is uncertain (high entropy)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_decay, wm_noise_size6, age_gate_bias, unc_slope = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-specific WM noise
        noise = float(np.clip(wm_noise_size6, 0.0, 1.0)) if nS == 6 else 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # RL uncertainty (entropy of RL softmax)
            # Compute full RL softmax for entropy calculation
            rl_probs = np.exp(beta_eff * (Q_s - np.max(Q_s)))
            rl_probs /= np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))
            H_max = np.log(nA)
            H_norm = H_rl / max(H_max, eps)

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-gated WM mixture weight (age bias applies equally across ages,
            # but age_group can be incorporated by adjusting the bias: young=0 -> no penalty now)
            # Here, we treat age_gate_bias as an intercept; optionally, old could reduce it elsewhere,
            # but since the participant is young (age_group=0), the raw bias applies.
            wm_gate = sigmoid(age_gate_bias + unc_slope * H_norm)

            wm_weight_eff = wm_gate * (1.0 - noise)
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM reward-gated one-shot encoding
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    WM encoding-failure model with set-size-dependent encoding probability and age shift.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: when a reward is received, the correct action is encoded into WM with probability p_enc,
          otherwise WM keeps decaying toward uniform. p_enc decreases with set size and is shifted by age.
          WM policy is deterministic when a trace exists (softmax_beta_wm=50).
    - Arbitration: convex mixture with a base WM weight scaled by the expected encoding success p_enc.

      p_enc = sigmoid(size_coef * (3 - nS) + age_shift * (1 - 2*age_group))
      (so p_enc is higher in small set size; age_shift>0 favors young > old)

      wm_weight_eff = wm_weight_base * p_enc

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_decay, wm_weight_base, size_coef, age_shift]
      lr: RL learning rate
      softmax_beta: RL inverse temperature (scaled by *10 internally)
      wm_decay: per-visit WM decay toward uniform
      wm_weight_base: baseline influence of WM in the mixture [0,1]
      size_coef: how strongly p_enc decreases with larger set sizes (positive -> more drop at nS=6)
      age_shift: age-dependent bias in p_enc; positive -> young advantage (since (1-2*age_group)=+1 for young)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_decay, wm_weight_base, size_coef, age_shift = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Encoding probability as a function of set size and age
        # Note: (3 - nS) is +0 for nS=3, -3 for nS=6; positive size_coef increases p_enc for small set sizes.
        p_enc = sigmoid(size_coef * (3 - nS) + age_shift * (1 - 2 * age_group))
        p_enc = float(np.clip(p_enc, 0.0, 1.0))

        wm_weight_eff = float(np.clip(wm_weight_base * p_enc, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on rewarded trials succeeds with probability p_enc
            if r > 0:
                if np.random.rand() < p_enc:
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                # else: no update beyond decay (encoding failure)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-speed system: RL + fast WM-Q with decay; arbitration by reliability and age, WM precision drops in set size 6.

    Mechanism:
    - RL: standard Q-learning with softmax.
    - WM fast-Q: a parallel fast-learning, high-precision but decaying Q-like system updated every trial
                 with learning rate alpha_wm and decayed toward uniform.
    - WM policy: softmax on WM-Q; precision (inverse temperature) is reduced when set size=6 via wm_temp6_mult.
    - Arbitration: mixture weight for WM is modulated by age via age_wm_weight (young > old when positive).
                   Here, wm_weight is proportional to the relative confidence of WM vs RL, operationalized
                   as the advantage (max-min) of each system on the current state.

      wm_beta = softmax_beta_wm_base / (1 + (nS==6)*(wm_temp6_mult))
      conf_rl = max(Q_s) - min(Q_s); conf_wm = max(WM_Q_s) - min(WM_Q_s)
      wm_weight_eff = sigmoid(age_wm_weight * (1 - 2*age_group)) * (conf_wm / (conf_wm + conf_rl + 1e-8))

    Parameters (6):
    - model_parameters = [lr, softmax_beta, alpha_wm, wm_decay, wm_temp6_mult, age_wm_weight]
      lr: RL learning rate
      softmax_beta: RL inverse temperature (scaled by *10)
      alpha_wm: fast WM-Q learning rate (0..1)
      wm_decay: WM-Q decay toward uniform each visit (0..1)
      wm_temp6_mult: multiplicative drop in WM inverse temperature when set size=6 (>=0)
      age_wm_weight: age-dependent scaling of WM weight inside a sigmoid (positive -> young advantage)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, alpha_wm, wm_decay, wm_temp6_mult, age_wm_weight = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm_base = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL Q
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM fast-Q
        wq = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM temperature adjustment by set size
        wm_beta = softmax_beta_wm_base / (1.0 + (wm_temp6_mult if nS == 6 else 0.0))

        # Age-based WM availability factor
        age_factor = sigmoid(age_wm_weight * (1 - 2 * age_group))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM-Q policy
            WM_Q_s = wq[s, :]
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (WM_Q_s - WM_Q_s[a])))

            # Reliability-based arbitration weight
            conf_rl = np.max(Q_s) - np.min(Q_s)
            conf_wm = np.max(WM_Q_s) - np.min(WM_Q_s)
            denom = conf_wm + conf_rl + 1e-8
            wm_rel = conf_wm / denom
            wm_weight_eff = float(np.clip(age_factor * wm_rel, 0.0, 1.0))

            # Mixture probability
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM-Q decay toward uniform
            wq[s, :] = (1.0 - wm_decay) * wq[s, :] + wm_decay * w_0[s, :]

            # WM-Q learning (fast)
            wq[s, a] += alpha_wm * (r - WM_Q_s[a])

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes and set size/age impacts:
- Model 1: WM is engaged more when RL is uncertain (high entropy). In large set sizes (6), added WM noise reduces WM influence. Age enters via the gate bias; being young (this participant) aligns with higher WM engagement if age_gate_bias > 0.
- Model 2: WM encoding of rewarded actions can fail more often at larger set sizes; young participants get a positive age shift in encoding probability if age_shift > 0. WM influence is scaled by expected encoding success.
- Model 3: WM is a fast-learning, decaying Q-system. Its precision degrades in set size 6 (wm_temp6_mult > 0), and arbitration favors the more “confident” system. Age scales overall WM contribution via a sigmoid of age_wm_weight.

These three models explore different mechanisms (uncertainty-gating, encoding failure, and dual-speed confidence arbitration) while respecting the RL+WM mixture template and using all parameters meaningfully.