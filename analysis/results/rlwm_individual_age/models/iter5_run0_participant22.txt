def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with probabilistic WM encoding gated by age and set size.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores action-outcome associations but only encodes them with a probability
      that depends on set size and age (encoding is harder with larger set sizes and
      for older adults).
    - WM policy is near-deterministic (high beta) over its current table; likelihood
      uses a mixture of WM and RL policies weighted by wm_weight.
    - WM updates are expectation-based using the encoding probability (no randomness),
      creating a graded memory trace.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 in each block).
    actions : array-like of int
        Observed action per trial (0..2).
    rewards : array-like of float
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, p_encode_base, p_encode_age_delta, p_encode_setsize_delta]
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM policy (0..1).
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - p_encode_base: baseline WM encoding probability.
        - p_encode_age_delta: reduction in encoding probability for older adults.
        - p_encode_setsize_delta: reduction per item beyond 3 (scaled by (nS-3)/3).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, p_encode_base, p_encode_age_delta, p_encode_setsize_delta = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective encoding probability given set size and age
        setsize_penalty = max(0.0, (nS - 3) / 3.0)  # 0 for 3, 1 for 6
        p_enc = p_encode_base - p_encode_setsize_delta * setsize_penalty - p_encode_age_delta * age_group
        p_enc = np.clip(p_enc, 0.0, 1.0)

        wm_w_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: as provided in the template
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over WM table with large beta
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (expectation over encoding):
            # If reward, move WM row toward one-hot on chosen action with strength p_enc.
            # If no reward, softly revert toward uniform with strength p_enc.
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * target
            else:
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with set-size dependent WM decay and age-dependent RL temperature.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM decays toward uniform between visits; decay increases with larger set size
      (more interference).
    - RL inverse temperature is reduced for older adults (no change for young here),
      modeling more decision noise with age.
    - WM policy is near-deterministic; action selection is a mixture of WM and RL.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Observed actions.
    rewards : array-like of float
        Rewards (0/1).
    blocks : array-like of int
        Block indices.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, wm_decay_base, wm_decay_setsize_slope, beta_age_delta]
        - lr: RL learning rate.
        - wm_weight: mixture weight for WM.
        - softmax_beta: baseline RL inverse temperature; internally x10.
        - wm_decay_base: baseline WM decay toward uniform (0..1).
        - wm_decay_setsize_slope: additional decay when nS=6 (scaled by (nS-3)/3).
        - beta_age_delta: reduction of RL beta for older adults.

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_decay_setsize_slope, beta_age_delta = model_parameters
    age_group = 0 if age[0] <= 45 else 1
    # Age reduces RL beta
    beta_rl = max(0.0, softmax_beta - beta_age_delta * age_group) * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set size dependent WM decay
        setsize_scale = max(0.0, (nS - 3) / 3.0)  # 0 for 3, 1 for 6
        wm_decay = np.clip(wm_decay_base + wm_decay_setsize_slope * setsize_scale, 0.0, 1.0)
        wm_w_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply WM decay on each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on reward, push toward chosen action; on no reward, keep decayed state
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited WM arbitration and a choice-kernel bias.

    Idea:
    - RL learns Q-values (single lr).
    - WM stores rewarded associations; its influence scales with an effective capacity K:
        wm_weight_eff = wm_weight_base * min(1, K_eff / nS).
      Older adults have reduced K (age penalty).
    - A state-specific choice kernel adds perseveration to the last chosen action
      and is learned with its own learning rate; it biases both RL and WM policies.
    - WM policy near-deterministic; mixture with RL as usual.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Observed actions.
    rewards : array-like of float
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, softmax_beta, wm_weight_base, K_base, K_age_delta, choice_kernel_lr]
        - lr: RL learning rate.
        - softmax_beta: RL inverse temperature; internally scaled x10.
        - wm_weight_base: baseline WM mixture weight before capacity scaling.
        - K_base: baseline WM capacity (in items).
        - K_age_delta: capacity reduction for older adults.
        - choice_kernel_lr: learning rate for choice kernel (0..1) controlling perseveration.

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, softmax_beta, wm_weight_base, K_base, K_age_delta, choice_kernel_lr = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Effective capacity
    K_eff = max(0.0, K_base - K_age_delta * age_group)

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Choice kernel initialized uniform; we store additive biases per state-action
        ck = np.zeros((nS, nA))

        # Capacity-scaled WM weight
        cap_scale = min(1.0, K_eff / max(1.0, nS))
        wm_w_eff = np.clip(wm_weight_base * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Bias both policies by choice kernel
            Q_s = q[s, :] + ck[s, :]
            W_s = w[s, :] + ck[s, :]

            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: only strengthen on reward, else slight relaxation toward uniform
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Choice kernel update: move mass toward chosen action within the state
            # Implemented as soft advantage for chosen action and slight suppression for others
            ck[s, :] = (1.0 - choice_kernel_lr) * ck[s, :]
            ck[s, a] += choice_kernel_lr

        blocks_log_p += log_p

    return -float(blocks_log_p)