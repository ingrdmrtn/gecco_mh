def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-process RL + WM with learned WM gating that depends on reward history,
    with age- and set-size-modulated WM reliance.

    Mechanism:
    - RL: tabular Q-learning with softmax policy.
    - WM: deterministic softmax over a sharp associative table that stores rewarded state-action pairs.
    - A per-state WM gate g_s is learned online: it is potentiated by reward and decays over time.
    - The final mixture weight is mix = sigmoid(wm_weight_base + age_bias) * g_s, so WM contributes more
      after successful learning in that state and less in high set-size or for the age group with lower bias.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM reliance (logit space), transformed by sigmoid for mixing
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - gate_learn: how strongly reward increases the WM gate for the current state (0..1+)
    - gate_decay: base decay rate of the WM gate per trial (0..1); higher -> faster decay
    - age_setsize_effect: shifts WM reliance by age group in logit space; positive -> higher WM reliance for young, lower for old

    Inputs:
    - states: int array of state indices per trial
    - actions: int array of chosen action indices per trial
    - rewards: float/binary array (0/1) per trial
    - blocks: int array of block indices
    - set_sizes: int array with the block's set size repeated per trial
    - age: array with single number repeated; age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, gate_learn, gate_decay, age_setsize_effect = model_parameters

    # Age group
    age_group = 0 if age[0] <= 45 else 1

    # Scale RL inverse temperature
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state WM gate (starts low)
        g = np.zeros(nS)

        # Set-size factor (0 for 3, 1 for 6)
        setsize_factor = max(nS - 3, 0) / 3.0

        # Age effect on WM reliance: young (+delta), old (-delta)
        age_bias = age_setsize_effect * (1.0 - 2.0 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as provided in template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with high beta (near-deterministic)
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # Learned WM gate with decay; decay is stronger with larger set size
            decay_eff = np.clip(gate_decay * (1.0 + setsize_factor), 0.0, 1.0)
            g[s] = (1.0 - decay_eff) * g[s] + gate_learn * r
            g[s] = np.clip(g[s], 0.0, 1.0)

            # Baseline WM weight from age group (logit -> prob), then modulate by gate
            wm_base = sigmoid(wm_weight_base + age_bias)
            mix = np.clip(wm_base * g[s], 0.0, 1.0)

            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: global mild decay to baseline, and reward-triggered write
            # Decay grows with set size to capture interference
            w = (1.0 - 0.2 * setsize_factor) * w + 0.2 * setsize_factor * w_0
            if r > 0.5:
                # Write sharp association for rewarded state-action
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with set-size-driven interference and age-modulated WM precision.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: stores rewarded associations; a global interference leak pulls WM toward uniform, and the
      leak increases when set size exceeds a capacity parameter.
    - WM policy precision (beta_wm) is modulated by age group: beta_wm_eff = 50 * (1 + beta_age_scale * sign_young_old).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: fixed WM mixture weight (0..1 after clipping)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_capacity: effective number of items that can be reliably maintained (1..6)
    - interference_rate: leak strength per excess item beyond capacity (>=0)
    - beta_age_scale: scales WM precision by age (positive -> higher precision for young, lower for old)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity, interference_rate, beta_age_scale = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    # RL inverse temperature
    softmax_beta *= 10.0

    # Age-modulated WM precision
    sign = (1.0 - 2.0 * age_group)  # +1 young, -1 old
    softmax_beta_wm = 50.0 * max(0.1, 1.0 + beta_age_scale * sign)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute interference leak based on capacity shortfall
        excess = max(nS - wm_capacity, 0.0)
        leak = np.clip(interference_rate * (excess / max(wm_capacity, 1.0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with age-modulated precision
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            mix = np.clip(wm_weight, 0.0, 1.0)
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global interference (toward uniform)
            w = (1.0 - leak) * w + leak * w_0

            # Rewarded writes produce a sharp trace for the state
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-driven exploration and age-dependent sensitivity to uncertainty.

    Mechanism:
    - RL: tabular Q-learning, but inverse temperature is dynamically reduced when WM is uncertain
      (higher entropy in WM row), encouraging exploration. This uncertainty effect is stronger for
      the older age group if age_explore_effect > 0.
    - WM: deterministic policy based on sharp traces; WM decays toward uniform with a rate that grows
      with set size (interference), controlled by wm_decay.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: fixed WM mixture weight (0..1 after clipping)
    - softmax_beta: baseline RL inverse temperature (scaled by 10 internally)
    - explore_bonus: scales how much WM uncertainty reduces RL beta (>=0)
    - age_explore_effect: multiplicative factor on explore_bonus for older vs younger (>=-1 to allow reductions)
    - wm_decay: base WM decay rate per trial; multiplied by set-size factor (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, explore_bonus, age_explore_effect, wm_decay = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    # Baseline RL inverse temperature
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    def normalized_entropy(p):
        # Max entropy for 3 actions is log(3)
        return entropy(p) / np.log(3.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        setsize_factor = max(nS - 3, 0) / 3.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # WM policy distribution
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # RL policy with uncertainty-adjusted beta
            # Compute WM uncertainty for the current state
            H = normalized_entropy(pi_wm)  # in [0,1]
            age_scale = 1.0 + age_explore_effect * age_group
            beta_eff = softmax_beta / (1.0 + explore_bonus * age_scale * H * (1.0 + setsize_factor))
            beta_eff = max(beta_eff, 1e-3)  # avoid zero/infinite temperature

            # Template-style probability for chosen action under RL softmax with beta_eff
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            mix = np.clip(wm_weight, 0.0, 1.0)
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay increases with set size
            leak = np.clip(wm_decay * (1.0 + setsize_factor), 0.0, 1.0)
            w = (1.0 - leak) * w + leak * w_0

            # Rewarded writes create sharp associations
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p