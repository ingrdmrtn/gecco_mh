def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited WM with age- and set-size-dependent WM gating and decay.

    Mechanism:
    - Action probability on each trial is a convex mixture of RL and WM policies for the chosen action.
    - RL uses a softmax policy with inverse temperature softmax_beta*10 and a single learning rate lr.
    - WM policy is a high-precision softmax over a state-specific distribution w[s,:] (beta fixed at 50).
    - WM mixture weight is capacity-limited: effective WM weight decreases with set size (nS) and with age group.
      wm_weight_eff = clip(wm_weight_base * min(1, cap_ratio/nS) * (1 - age_drop*age_group), 0, 1)
    - WM memory decays toward uniform each time the state is visited. If reward=1, WM is updated toward the chosen action.
      The reward-driven WM update strength is tied to (1 - wm_decay), so a single parameter controls both decay and plasticity.

    Parameters (6):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - softmax_beta: RL softmax inverse temperature scale (>0), multiplied by 10 internally
    - cap_ratio: effective WM capacity in items (e.g., around 3..4). Governs gating with set size.
    - wm_decay: WM decay toward uniform per visit (0..1). Also sets WM learning strength as (1 - wm_decay).
    - age_drop: strength of age penalty on WM gating (>=0). Applied if age_group=1.

    Age use:
    - age_group = 0 if age <= 45 else 1; reduces WM contribution via the age_drop factor.

    Set size use:
    - Larger set sizes (nS=6) reduce WM contribution via min(1, cap_ratio/nS).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, cap_ratio, wm_decay, age_drop = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action (softmax, scalar for chosen action)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM probability of chosen action (high-precision softmax over WM values)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Capacity-limited WM gating with age penalty
            cap_factor = min(1.0, float(cap_ratio) / float(nS))
            wm_weight_eff = wm_weight_base * cap_factor * (1.0 - age_drop * age_group)
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            # Mixture probability for chosen action
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)
            # Reward-driven WM plasticity toward chosen action; strength tied to (1 - wm_decay)
            if r > 0.5:
                k = (1.0 - wm_decay)
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k) * w[s, :] + k * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + WM with novelty-driven RL bias and leak, and age/set-size adjusted WM weighting.

    Mechanism:
    - Action probability is a convex mixture of RL and WM policies for the chosen action.
    - RL softmax probability uses inverse temperature softmax_beta*10, with a single learning rate lr.
    - Novelty bonus: actions not yet rewarded in the current state receive an additive bonus to Q before policy.
    - Leak: after each update, all Q-values in the visited state leak toward the uniform mean by 'leak' amount.
    - WM policy is high-precision softmax over a WM table w[s,:] that stores last rewarded action with decay.
    - WM mixture weight is reduced by set size and age via a multiplicative factor exp(-gamma_ss*(nS-3))*(1 - age_effect*age_group).

    Parameters (6):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature scale (>0), multiplied by 10 internally
    - novelty_bonus: additive bonus to Q for actions never observed as rewarded in a state (>=0)
    - leak: RL value leak toward uniform per visit (0..1)
    - age_effect: magnitude by which age_group=1 reduces WM weight (0..1)

    Age use:
    - Reduces WM contribution multiplicatively by (1 - age_effect) if older.

    Set size use:
    - Larger set sizes reduce WM weight via exp(-gamma_ss*(nS-3)), with gamma_ss=0.5 fixed inside.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, novelty_bonus, leak, age_effect = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic

    nA = 3
    eps = 1e-12
    gamma_ss = 0.5  # fixed set-size sensitivity for WM gating

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Track whether each action has ever been rewarded in a state (for novelty bonus)
        ever_rewarded = np.zeros((nS, nA), dtype=bool)

        # WM decay rate fixed via a mild decay; tie to leak to keep param count <=6
        wm_decay = min(1.0, max(0.0, 0.5 * leak))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply novelty bonus to a copy of Q for policy computation
            Q_s = q[s, :].copy()
            novel_mask = ~ever_rewarded[s, :]
            Q_s = Q_s + novelty_bonus * novel_mask.astype(float)

            W_s = w[s, :]

            # RL policy prob of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy prob of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM mixture weight with set size and age attenuation
            ss_factor = np.exp(-gamma_ss * max(0, nS - 3))
            age_factor = (1.0 - age_effect * age_group)
            wm_weight_eff = wm_weight * ss_factor * age_factor
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            # Mixture probability
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update on actual Q (no novelty bonus in learning)
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # RL leak toward uniform after visiting this state
            q_mean = np.mean(q[s, :])
            q[s, :] = (1.0 - leak) * q[s, :] + leak * q_mean

            # Update ever_rewarded tracker
            if r > 0.5:
                ever_rewarded[s, a] = True

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)
            # If reward, store the action strongly (strength tied to 1 - wm_decay)
            if r > 0.5:
                k = (1.0 - wm_decay)
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k) * w[s, :] + k * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + WM with PE-sensitive WM gating and interference-driven WM degradation.

    Mechanism:
    - Action probability is a mixture of RL and WM policies for the chosen action.
    - RL uses a softmax with inverse temperature softmax_beta*10 and a single learning rate lr.
    - WM policy is high-precision softmax over w[s,:].
    - WM gating depends on positive prediction errors (PE): recent success increases WM reliance.
      wm_weight_eff = clip(wm_weight * (1 + pe_sens * max(0, PE)) * ss_factor * age_factor, 0, 1)
    - WM interference: larger set sizes cause extra decay of WM on each visit; older age amplifies it.
    - Negative PE triggers partial eviction of WM for the state (pushes w[s,:] toward uniform).

    Parameters (6):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature scale (>0), multiplied by 10 internally
    - wm_weight: baseline WM mixture weight (0..1)
    - pe_sens: sensitivity of WM gating to positive PE (>=0)
    - wm_interf: WM interference strength with set size (>=0)
    - age_penalty: additional interference multiplier for older group (>=0)

    Age use:
    - Older group (age_group=1) increases WM interference and reduces WM gating via age_factor.

    Set size use:
    - Larger set sizes increase WM interference and reduce gating via ss_factor = 3/nS.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, pe_sens, wm_interf, age_penalty = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM probability of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute current PE for gating
            pe = r - Q_s[a]
            pos_pe = max(0.0, pe)

            # Gating factors: set size and age
            ss_factor = 3.0 / float(max(3, nS))  # 1.0 for nS=3, 0.5 for nS=6
            age_factor = 1.0 - 0.3 * age_group   # modest direct age reduction of gating

            wm_weight_eff = wm_weight * (1.0 + pe_sens * pos_pe) * ss_factor * age_factor
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            q[s, a] += lr * pe

            # WM interference and updates
            # Base decay per visit due to interference (higher with larger set size, and with age)
            extra_decay = wm_interf * max(0, nS - 3) / 3.0
            extra_decay += age_penalty * age_group
            extra_decay = float(np.clip(extra_decay, 0.0, 1.0))

            # Apply decay toward uniform
            w[s, :] = (1.0 - extra_decay) * w[s, :] + extra_decay * (1.0 / nA)

            # Positive PE (success): strengthen WM toward chosen action
            if pos_pe > 0.0:
                k_pos = min(1.0, 0.7 + 0.3 * pos_pe)  # stronger push on clear successes
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k_pos) * w[s, :] + k_pos * onehot

            # Negative PE: partial eviction (reduce peakedness)
            if pe < 0.0:
                evict = min(1.0, 0.4 + 0.4 * (-pe))
                w[s, :] = (1.0 - evict) * w[s, :] + evict * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p