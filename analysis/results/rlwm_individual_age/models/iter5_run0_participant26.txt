Below are three standalone cognitive models tailored to the RL-WM task, each returning the negative log-likelihood of the observed choices. They all implement a hybrid RL and working-memory process, with age group and set size affecting behavior in distinct, testable ways. Each model uses no more than 6 parameters and uses all parameters meaningfully.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q-value decay (forgetting) and noisy WM storage, with arbitration scaled by set size and age.

    Mechanism
    - RL: TD(0) learning with per-trial Q-value decay toward uniform (forgetting). Forgetting increases with set size
      and is amplified in older adults.
    - WM: one-shot storage of rewarded actions with noise (mixture with uniform). No active decay parameter; memory is
      overwritten only on reward.
    - Arbitration: fixed WM mixture per block scaled by 3/nS (smaller set sizes favor WM). 

    Parameters
    - model_parameters: [lr, softmax_beta, wm_base, wm_noise, q_decay, age_q_increase]
        - lr: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL action selection (internally scaled by 10).
        - wm_base: baseline WM arbitration weight (0..1) at set size 3 (scaled by 3/nS).
        - wm_noise: noise in WM storage (0..1); on reward, WM becomes (1-wm_noise)*onehot + wm_noise*uniform.
        - q_decay: base forgetting rate for RL (0..1) applied each trial; increases with set size.
        - age_q_increase: multiplicative increase of q_decay in older adults (>=0). Young=0, Old=1.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_base, wm_noise, q_decay, age_q_increase = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration weight scales with set size
        wm_weight_block = wm_base * (3.0 / max(1.0, float(nS)))
        wm_weight_block = min(1.0, max(0.0, wm_weight_block))

        # RL decay magnitude increases with set size and age
        decay_eff = q_decay * (float(nS) / 3.0) * (1.0 + age_q_increase * age_group)
        decay_eff = min(1.0, max(0.0, decay_eff))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL choice probability
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM choice probability
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL decay toward uniform (forgetting)
            q = (1.0 - decay_eff) * q + decay_eff * (1.0 / nA) * np.ones_like(q)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: overwrite only on reward with noise
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_noise) * target + wm_noise * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated arbitration with eligibility traces and WM gating failures.

    Mechanism
    - RL: TD(0) with eligibility traces over state-action pairs (spillover credit).
    - WM: one-shot storage on reward; otherwise unchanged.
    - Arbitration: weight on RL increases with trial-by-trial surprise (|PE|). WM is additionally weakened by a
      gating-failure probability that increases with set size and in older adults.

    Parameters
    - model_parameters: [lr, softmax_beta, wm_gate, surprise_to_rl, gate_age_boost, elig]
        - lr: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL (scaled by 10).
        - wm_gate: baseline WM access probability at set size 3 (0..1); effective WM weight scales with 3/nS.
        - surprise_to_rl: slope mapping surprise (|PE|) into RL preference via a sigmoid (>0).
        - gate_age_boost: multiplicative increase of WM gating failures for older adults (>=0).
        - elig: eligibility trace decay (0..1).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_gate, surprise_to_rl, gate_age_boost, elig = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        e = np.zeros((nS, nA))  # eligibility trace

        # Baseline WM access (gating) reduced with set size
        wm_access = wm_gate * (3.0 / max(1.0, float(nS)))
        wm_access = min(1.0, max(0.0, wm_access))

        # Older adults: more gating failures
        fail_multiplier = 1.0 + gate_age_boost * age_group
        wm_access_eff = wm_access / max(1e-6, fail_multiplier)
        wm_access_eff = min(1.0, max(0.0, wm_access_eff))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL prob
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM prob
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Surprise-based arbitration toward RL
            pe_now = r - Q_s[a]
            surprise = abs(pe_now)  # in [0,1]
            # Map surprise to RL weight via sigmoid centered near 0.5
            w_rl = 1.0 / (1.0 + np.exp(-surprise_to_rl * (surprise - 0.5)))
            # Effective WM weight accounts for gating failures
            wm_weight = (1.0 - w_rl) * wm_access_eff

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # Eligibility decay
            e *= elig
            e[s, a] += 1.0

            # RL update with traces
            q += lr * pe_now * e

            # WM update on reward
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target  # deterministic storage

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted WM via Dirichlet counts + RL, with beta modulated by set size and age.

    Mechanism
    - WM: per-state Dirichlet posterior over actions; WM policy is the posterior mean. Uncertainty (normalized entropy)
      drives arbitration: more uncertainty -> rely more on RL.
    - RL: standard TD(0).
    - Temperature: RL inverse temperature decreases with larger set sizes and with older age.

    Parameters
    - model_parameters: [lr, softmax_beta, alpha0, unc_slope, beta_size_gain, age_beta_drop]
        - lr: RL learning rate (0..1).
        - softmax_beta: baseline RL inverse temperature (scaled by 10).
        - alpha0: Dirichlet prior strength per action (>0); also sets WM certainty scale in arbitration.
        - unc_slope: exponent controlling how sharply uncertainty down-weights WM (>=0).
        - beta_size_gain: scales how much set size reduces RL beta; beta_eff = beta*(3/nS)^beta_size_gain.
        - age_beta_drop: proportional drop of RL beta in older adults (0..1). Young=0 effect; Old reduces beta.

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, alpha0, unc_slope, beta_size_gain, age_beta_drop = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # Dirichlet counts for WM posterior
        counts = alpha0 * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # will hold posterior mean each trial
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL beta per block
        beta_size_factor = (3.0 / max(1.0, float(nS))) ** beta_size_gain
        beta_age_factor = (1.0 - age_beta_drop * age_group)
        beta_age_factor = max(0.0, beta_age_factor)
        beta_eff = softmax_beta * beta_size_factor * beta_age_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # WM posterior and uncertainty
            post = counts[s, :] / np.sum(counts[s, :])
            w[s, :] = post

            # Normalized entropy in [0,1]
            H = -np.sum(post * np.log(post + eps))
            Hmax = np.log(nA)
            U = H / max(eps, Hmax)

            # Arbitration weight: more uncertainty -> smaller WM weight
            wm_weight = 1.0 / (1.0 + (U / max(eps, alpha0)) ** max(0.0, unc_slope))
            # Also scale naturally with set size (implicit via uncertainty); no extra size factor needed.

            # Choice probabilities
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: rewarded action strengthens its Dirichlet count
            if r > 0.5:
                counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set size effects
- Model 1: Older age increases RL forgetting (q-decay), especially in larger set sizes; WM weight decreases with set size via 3/nS.
- Model 2: Larger set sizes and older age reduce WM access; surprise boosts reliance on RL dynamically via a sigmoid.
- Model 3: RL temperature is reduced by larger set sizes and in older adults; WM arbitration weight is driven by WM uncertainty derived from Dirichlet counts.