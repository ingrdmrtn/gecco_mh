def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + WM with capacity-limited WM mixture scaled by set size and age, with error-driven WM decay
    - RL: standard delta-rule with single learning rate and softmax policy.
    - WM: fast table that stores action probabilities per state; positive feedback writes toward a one-hot;
      negative feedback decays toward uniform.
    - Mixture: fixed base WM weight is multiplicatively scaled by a capacity factor C/set_size (capped at 1),
      and an age-dependent factor. Older age reduces effective WM capacity; younger increases it.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: baseline WM mixture weight before scaling (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10 for numerical range.
    - capacity_C: WM capacity in number of states; effective WM weight scales as min(1, C / set_size).
    - age_capacity_mult: multiplicative factor applied based on age group: wm *= (1 + age_capacity_mult) if young, (1 - age_capacity_mult) if old.
    - wm_decay_error: on negative feedback, WM weights decay toward uniform with this strength (0..1).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, capacity_C, age_capacity_mult, wm_decay_error = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute block-level WM mixture scaling from capacity and age
        capacity_scale = min(1.0, max(0.0, capacity_C) / max(1.0, nS))
        if age_group == 0:
            age_scale = 1.0 + age_capacity_mult
        else:
            age_scale = max(0.0, 1.0 - age_capacity_mult)
        wm_weight_block = np.clip(wm_weight_base * capacity_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot  # with deterministic WM, write the correct action strongly
            else:
                # On errors, decay toward uniform at rate wm_decay_error
                w[s, :] = (1.0 - wm_decay_error) * w[s, :] + wm_decay_error * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL + WM with uncertainty-gated WM arbitration and age-shifted sensitivity
    - RL: delta-rule with single learning rate and softmax policy.
    - WM: fast action memory updated on rewards; partial write-in strength.
    - Arbitration: the WM mixture weight increases when RL is uncertain (high entropy),
      decreases when RL is confident (low entropy). Sensitivity to uncertainty is shifted by age.
      Larger set sizes reduce the effective sensitivity to uncertainty.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight0: baseline WM mixture intercept (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10.
    - entropy_slope: slope controlling how RL policy entropy increases WM reliance (>=0).
    - age_entropy_shift: additive shift to entropy_slope based on age (added if young, subtracted if old).
    - wm_write_strength: WM write-in strength toward one-hot on reward, and mild decay otherwise (0..1).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight0, softmax_beta, entropy_slope, age_entropy_shift, wm_write_strength = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size reduces the impact of uncertainty on arbitration
        ss_scale = 1.0 / max(1.0, nS / 3.0)  # 1 for set size 3, ~0.5 for set size 6

        # Age shifts sensitivity to uncertainty: young more sensitive, old less
        if age_group == 0:
            entropy_slope_eff = max(0.0, entropy_slope + abs(age_entropy_shift))
        else:
            entropy_slope_eff = max(0.0, entropy_slope - abs(age_entropy_shift))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL action probabilities for entropy computation
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi /= np.sum(pi)

            # RL policy likelihood of chosen action (as in template style)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Entropy of RL policy
            H = -np.sum(pi * (np.log(pi + eps)))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight increases with uncertainty, scaled by set size and age
            wm_weight = wm_weight0 + (entropy_slope_eff * ss_scale) * (H / np.log(nA))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_write_strength) * w[s, :] + wm_write_strength * one_hot
            else:
                # mild decay toward uniform when error occurs
                decay = 0.5 * wm_write_strength
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL + WM with set-size/age-driven lapses and interference-based WM decay
    - RL: delta-rule with single learning rate and softmax policy.
    - WM: stores one-hot-like associations upon reward. Between visits and on errors,
      WM suffers interference/decay toward uniform.
    - Lapses: mixture includes a uniform lapse component whose probability increases with set size
      and is amplified by age. This lapse reduces both RL and WM influence when cognitive load is high.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_mix0: baseline WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10.
    - lapse_k_setsize: slope controlling how lapse grows with set size (>=0).
    - age_lapse_mult: multiplicative factor on lapse for older participants (>0); for young, divides lapse by the same factor.
    - wm_interference: WM decay toward uniform after any non-rewarded trial and small passive decay each trial (0..1).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_mix0, softmax_beta, lapse_k_setsize, age_lapse_mult, wm_interference = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Lapse rate scales with set size; age amplifies or attenuates it
        base_lapse = np.clip(lapse_k_setsize * max(0.0, nS - 3) / 3.0, 0.0, 0.9)  # 0 at set size 3, up to ~lapse_k_setsize at 6
        if age_group == 1:
            lapse = np.clip(base_lapse * max(1e-3, age_lapse_mult), 0.0, 0.9)
        else:
            lapse = np.clip(base_lapse / max(1e-3, age_lapse_mult), 0.0, 0.9)

        # WM mixture weight is reduced by lapse because lapses bypass controlled policies
        wm_weight_block = np.clip(wm_mix0 * (1.0 - lapse), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uniform lapse policy
            p_lapse = 1.0 / nA

            # Combine WM, RL, and lapses: first mix WM and RL, then blend with lapse
            p_control = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * p_control + lapse * p_lapse
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_interference) * w[s, :] + wm_interference * one_hot
            else:
                # Interference/decay on non-reward trials
                w[s, :] = (1.0 - wm_interference) * w[s, :] + wm_interference * w_0[s, :]

            # Passive small interference each trial on all states to mimic load-induced decay
            w = (1.0 - 0.1 * wm_interference) * w + 0.1 * wm_interference * w_0

        blocks_log_p += log_p

    return -blocks_log_p