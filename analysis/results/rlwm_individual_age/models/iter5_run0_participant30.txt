Below are three alternative cognitive models that follow the requested RL+WM template structure, incorporate age group and set size effects meaningfully, and return the negative log-likelihood of the observed choices. Each uses no more than 6 parameters and avoids the exact parameter combinations you’ve tried before.

Note: These functions assume numpy (np) is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited working memory with state-level gating and age-dependent decay.
    WM stores up to 'wm_slots' states with high-fidelity policy; others rely more on RL.
    WM decays toward a uniform prior with a decay rate that increases with set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6), constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr_rl, beta_base, wm_beta, wm_slots, decay_slope, age_decay_boost]
        - lr_rl: RL learning rate (0..1)
        - beta_base: RL inverse temperature (>0), scaled by 10 internally
        - wm_beta: WM inverse temperature (>0), applied directly (deterministic when large)
        - wm_slots: number of states WM can store with high fidelity (>=1)
        - decay_slope: increases WM decay as set size increases (>=0)
        - age_decay_boost: multiplicative boost on decay for older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_rl, beta_base, wm_beta, wm_slots, decay_slope, age_decay_boost = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track order of first appearance of states to implement slot gating
        seen_order = []
        state_rank = -1 * np.ones(nS, dtype=int)

        # Effective WM decay increases with set size and with age
        base_decay = np.clip(decay_slope * max(0, (nS - 3) / 3.0), 0.0, 1.0)
        wm_decay_eff = np.clip(base_decay * (1.0 + age_decay_boost * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update seen order/rank
            if state_rank[s] < 0:
                state_rank[s] = len(seen_order)
                seen_order.append(s)

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))

            # Capacity-based arbitration: full WM weight if within slots, else reduced
            within_capacity = 1.0 if state_rank[s] < wm_slots else 0.0
            # Smooth transition by residual capacity fraction with set size
            capacity_frac = np.clip(wm_slots / float(nS), 0.0, 1.0)
            wm_weight = 0.75 * within_capacity + 0.25 * capacity_frac

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM: leaky toward prior, then fast integration of current outcome
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0
            w[s, a] += (1.0 - w[s, a]) * r  # reinforce chosen action proportional to reward

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Set-size–driven WM-RL mixture with age-dependent lapse and WM learning.
    WM learns quickly, RL learns slowly; mixture weight shrinks with larger set sizes.
    Adds an age-sensitive lapse process that mixes in uniform random responding.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6), constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr_rl, beta_base, wm_lr, w_bias, kappa_size, age_lapse_increase]
        - lr_rl: RL learning rate (0..1)
        - beta_base: RL inverse temperature (>0), scaled by 10 internally
        - wm_lr: WM learning rate (0..1)
        - w_bias: baseline WM mixture bias (can be positive or negative)
        - kappa_size: decreases WM weight as set size increases (>=0)
        - age_lapse_increase: adds lapse probability for older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_rl, beta_base, wm_lr, w_bias, kappa_size, age_lapse_increase = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture weight as a logistic function of set size
        # Higher set size -> lower WM weight
        size_term = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        wm_weight = 1.0 / (1.0 + np.exp(-(w_bias - kappa_size * size_term)))

        # Age-dependent lapse (uniform random responding)
        lapse = np.clip(0.02 + age_lapse_increase * age_group, 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse-augmented mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / 3.0)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM update: fast delta rule without global decay (uses wm_lr)
            w[s, a] += wm_lr * (r - w[s, a])

            # Optional light re-centering toward prior to avoid saturation
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Evidence-accumulating WM with confidence-gated arbitration and set-size retrieval noise.
    WM accumulates positive evidence counts for (state, action) pairs; a confidence function
    (relative to a threshold) gates arbitration with RL. Set size adds WM retrieval noise.
    Age modulates RL temperature.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size on each trial (3 or 6), constant within block.
    age : array-like
        Participant age repeated for each trial.
    model_parameters : list or array
        [lr_rl, beta_base, wm_conf_slope, theta_evidence, age_temp_bonus, size_wm_noise]
        - lr_rl: RL learning rate (0..1)
        - beta_base: RL inverse temperature (>0), scaled by 10 internally
        - wm_conf_slope: slope for mapping evidence surplus to WM confidence (>=0)
        - theta_evidence: evidence threshold for confident WM (>=0)
        - age_temp_bonus: increases RL temperature for younger adults (>=0)
        - size_wm_noise: increases WM noise with larger set sizes (>=0)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_rl, beta_base, wm_conf_slope, theta_evidence, age_temp_bonus, size_wm_noise = model_parameters
    # Age-dependent RL temperature: younger -> hotter (larger beta) with bonus
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0 * (1.0 + age_temp_bonus * (1 - age_group))

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM "evidence" counts for reward supporting each action in each state
        evidence = np.zeros((nS, nA))
        # For compatibility with the template variables
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM retrieval noise increases with set size
        size_term = (nS - 3) / 3.0
        wm_beta_eff = 50.0 / (1.0 + size_wm_noise * size_term)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: derive a distribution from evidence counts
            # Convert evidence to a pseudo-probability by normalizing, then softmax
            if np.sum(evidence[s, :]) > 0:
                ev_prob = evidence[s, :] / max(np.sum(evidence[s, :]), 1e-12)
            else:
                ev_prob = np.ones(nA) / nA
            W_s = ev_prob  # treat as "preferences"
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # WM confidence based on best-minus-second-best evidence relative to threshold
            sorted_ev = np.sort(evidence[s, :])[::-1]
            top = sorted_ev[0]
            second = sorted_ev[1] if nA > 1 else 0.0
            surplus = top - second
            wm_conf = 1.0 / (1.0 + np.exp(-wm_conf_slope * (surplus - theta_evidence)))

            wm_weight = wm_conf
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM evidence update: add evidence for rewarded chosen action
            if r > 0.0:
                evidence[s, a] += 1.0

            # Keep "w" variables consistent with an interpretable WM map (optional)
            # Project evidence into w via normalized counts to maintain template structure
            total_ev = np.sum(evidence[s, :])
            if total_ev > 0:
                w[s, :] = evidence[s, :] / total_ev
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p