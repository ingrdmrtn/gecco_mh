def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and probabilistic WM encoding under interference.

    Model summary:
    - RL controller: tabular Q-learning with an eligibility trace that assigns
      credit to recent state-action pairs within a block.
    - WM controller: stores state-action associations as a categorical distribution
      over actions per state (w). WM encodes the rewarded action with some
      probability and otherwise is subject to interference/decay toward uniform.
    - Arbitration: fixed WM mixture weight (wm_weight) modulated implicitly by how
      informative WM distribution is; policy combines WM and RL probabilities.
    - Set size and age effects: larger set size (6) and older age (age group 1)
      increase WM interference (decay) and reduce WM encoding probability.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, wm_weight, softmax_beta, lambda_et, wm_encode_prob, age_interf_mult]
        - lr: RL learning rate (0..1).
        - wm_weight: Base arbitration weight for WM (0..1).
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - lambda_et: Eligibility trace decay (0..1).
        - wm_encode_prob: Baseline probability of encoding a rewarded action into WM.
        - age_interf_mult: Multiplier increasing WM interference for older age.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_et, wm_encode_prob, age_interf_mult = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces over state-action pairs
        e = np.zeros((nS, nA))

        # Interference/decay toward uniform increases with set size and age
        wm_decay = 0.05 * (nS / 3.0) * (1.0 + age_interf_mult * age_group)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        # WM encoding probability reduced by set size and age
        wm_encode_eff = wm_encode_prob * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_encode_eff = np.clip(wm_encode_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic softmax over WM values
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with eligibility traces
            pe = r - q[s, a]
            # Replacing traces: set current to 1, decay others
            e *= lambda_et
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += lr * pe * e

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leaky decay toward uniform (interference)
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Probabilistic encoding of rewarded action as near one-hot
            if r > 0.0:
                if np.random.rand() < wm_encode_eff:
                    w[s, :] = 1e-6 + 0.0 * w[s, :]
                    w[s, a] = 1.0
                    w[s, :] /= np.sum(w[s, :])
                else:
                    # If not encoded, keep the leaky distribution
                    pass
            else:
                # After no reward, do not encode; WM just continues to leak
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with unchosen-action forgetting and WM with error-correcting updates plus lapses.

    Model summary:
    - RL controller: Q-learning on chosen action plus controlled forgetting of
      unchosen actions toward a neutral prior (uniform), capturing limited memory.
    - WM controller: stores a distribution over actions per state; after reward,
      pushes probability mass toward the chosen action; after no reward, suppresses
      the chosen action and redistributes probability to the others.
    - Lapses: with probability depending on set size and age, choices are random.
    - Arbitration: fixed WM mixture weight (wm_weight) between WM and RL policies.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial.
    rewards : array-like
        Reward (0/1) per trial.
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, wm_weight, softmax_beta, q_forget_rate, lapse_rate_base, age_lapse_mult]
        - lr: RL learning rate (0..1).
        - wm_weight: Base arbitration weight for WM (0..1).
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - q_forget_rate: Per-trial decay of all Q-values toward uniform (unchosen forgetting).
        - lapse_rate_base: Baseline lapse probability.
        - age_lapse_mult: Multiplier increasing lapses for older age.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, q_forget_rate, lapse_rate_base, age_lapse_mult = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Lapse probability increases with set size and age
        lapse = lapse_rate_base * (nS / 3.0) * (1.0 + age_lapse_mult * age_group)
        lapse = np.clip(lapse, 0.0, 0.5)

        # Small WM leak toward uniform, stronger with set size and age
        wm_leak = 0.02 * (nS / 3.0) * (1.0 + 0.5 * age_group)
        wm_leak = np.clip(wm_leak, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration (WM vs RL), then apply lapses to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update: chosen action Q-learning
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # RL forgetting: drift all Q(s, :) toward uniform baseline
            q[s, :] = (1.0 - q_forget_rate) * q[s, :] + q_forget_rate * (1.0 / nA)

            # WM leak toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # WM error-correcting update
            if r > 0.0:
                # Reward: push mass toward the chosen action
                gain = 0.9
                w[s, a] += (1.0 - w[s, a]) * gain
            else:
                # No reward: suppress chosen action and redistribute to others
                suppress = 0.5
                delta = suppress * w[s, a]
                w[s, a] -= delta
                redistribute = delta / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            # Normalize to ensure a valid distribution
            w[s, :] = np.maximum(w[s, :], 1e-8)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with nonlinear PE and uncertainty-based arbitration with leaky WM.

    Model summary:
    - RL controller: standard Q-learning but with a nonlinear prediction error
      exponent (nu) that can amplify or compress updates.
    - WM controller: leaky integrator toward a one-hot of rewarded actions; leak
      increases with set size and with age.
    - Arbitration: dynamic WM weight based on relative uncertainty (entropy) of
      RL vs WM distributions, passed through a sigmoid and blended with a base
      WM weight. Older age biases arbitration away from WM.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial.
    rewards : array-like
        Reward (0/1) per trial.
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, wm_weight, softmax_beta, wm_leak_base, arb_gain, nu_age_shift]
        - lr: RL learning rate (0..1).
        - wm_weight: Base WM weight (0..1) prior to uncertainty arbitration.
        - softmax_beta: Base RL inverse temperature (scaled by 10 internally).
        - wm_leak_base: Baseline WM leak toward uniform.
        - arb_gain: Gain on entropy difference in arbitration sigmoid.
        - nu_age_shift: Adds 0.5*age_group to the PE exponent (nu) to capture
          age-related PE nonlinearity and arbitration bias.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_leak_base, arb_gain, nu_age_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM leak increases with set size and age
        wm_leak = wm_leak_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
        wm_leak = np.clip(wm_leak, 0.0, 1.0)

        # PE nonlinearity exponent (nu), larger with age_group if nu_age_shift > 0
        nu = 1.0 + 0.5 * age_group + nu_age_shift * age_group
        nu = max(0.1, nu)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probabilities as softmax over Q values
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-based arbitration:
            # Compute entropies of RL and WM distributions for this state.
            # Derive RL distribution with the same softmax that generated p_rl.
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs /= np.sum(rl_probs)
            wm_probs = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_probs /= np.sum(wm_probs)

            def entropy(p):
                pp = np.clip(p, 1e-12, 1.0)
                return -np.sum(pp * np.log(pp))

            H_rl = entropy(rl_probs)
            H_wm = entropy(wm_probs)

            # Higher weight to the less uncertain controller (lower entropy)
            # wm_dynamic increases when WM is more certain than RL (H_rl - H_wm > 0)
            wm_dynamic = 1.0 / (1.0 + np.exp(-arb_gain * (H_rl - H_wm)))
            # Blend with base WM weight
            wm_weight_eff = np.clip(0.5 * wm_weight + 0.5 * wm_dynamic * (1.0 - 0.3 * age_group), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with nonlinear PE
            pe = r - q[s, a]
            pe_eff = np.sign(pe) * (np.abs(pe) ** nu)
            q[s, a] += lr * pe_eff

            # WM leak and reinforcement toward the rewarded action when r=1
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            if r > 0.0:
                # Move WM toward a one-hot on the chosen action
                target = np.zeros(3)
                target[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * target
                w[s, :] = np.maximum(w[s, :], 1e-8)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p