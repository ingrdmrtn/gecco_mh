def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recall-based WM with set-size- and age-modulated recall and decay.

    Idea
    - RL learns Q-values with a single learning rate.
    - WM stores a fast one-shot trace of rewarded action per state.
    - WM's effective contribution is scaled by a recall probability that
      decreases with set size and is penalized in older adults.
    - WM traces decay toward uniform with a base decay, which increases with set size
      and is aggravated by age.
    - Policy is a mixture of RL softmax and (near-deterministic) WM softmax.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight0, recall_base, decay_base, age_penalty]
        - alpha: RL learning rate
        - softmax_beta: base inverse temperature for RL (scaled internally by x10)
        - wm_weight0: baseline WM mixture weight
        - recall_base: base logit of WM recall probability
        - decay_base: base WM decay rate per trial (toward uniform)
        - age_penalty: linear penalty applied to WM recall and decay for older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight0, recall_base, decay_base, age_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-sizeâ€“dependent penalties
        load = max(0, nS - 3)  # 0 for 3, 3 for 6
        # Recall probability logit is reduced by load and age
        recall_logit = recall_base - 0.8 * load - age_penalty * age_group
        recall_prob = 1.0 / (1.0 + np.exp(-recall_logit))
        # WM decay increases with load and age
        wm_decay = np.clip(decay_base + 0.1 * load + 0.1 * age_penalty * age_group, 0.0, 1.0)
        # WM mixture reduced by load; young vs old difference through age_penalty
        wm_weight_eff_block = np.clip(wm_weight0 / (1.0 + 0.6 * load) - 0.2 * age_penalty * age_group, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: with probability recall_prob we use near-deterministic softmax on W,
            # otherwise fallback to uniform (as if not recalled).
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = recall_prob * p_wm_det + (1.0 - recall_prob) * (1.0 / nA)

            # Mixture
            mix = np.clip(wm_weight_eff_block, 0.0, 1.0)
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM write: strong one-shot update after reward, mild after no reward
            if r > 0.0:
                write = 0.8
            else:
                write = 0.2
            w[s, :] = (1.0 - write) * w[s, :]
            w[s, a] += write
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-split learning + Bayesian WM counts with entropy gating.

    Idea
    - RL uses separate learning rates for rewards vs non-rewards.
    - WM maintains Dirichlet-like counts per state-action (a simple Bayesian memory).
      Policy is based on the posterior mean (normalized counts).
    - The mixture weight is adjusted by the WM's action entropy: confident WM (low entropy)
      gets higher weight; noisy WM (high entropy) gets lower weight.
    - Set size reduces the baseline WM mixture; age reduces mixture further and amplifies
      the entropy gate.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, wm_weight0, entropy_temp, age_bias]
        - alpha_pos: RL learning rate for rewards
        - alpha_neg: RL learning rate for non-rewards
        - beta_base: RL inverse temperature base (scaled internally by x10)
        - wm_weight0: baseline WM weight before gating
        - entropy_temp: sensitivity of mixture gating to WM entropy
        - age_bias: increases age-related reduction of WM and increases gating sensitivity

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_base, wm_weight0, entropy_temp, age_bias = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age effects: older group has slightly noisier RL and stronger entropy penalty on WM
    if age_group == 1:
        softmax_beta *= 0.85
        entropy_temp *= (1.0 + 0.5 * age_bias)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM as Dirichlet counts; start with symmetric prior (pseudo-counts)
        prior = 0.5
        counts = prior * np.ones((nS, nA))
        # We'll derive w as normalized counts each trial
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age reduce baseline WM weight
        load = max(0, nS - 3)
        wm_weight_base = wm_weight0 / (1.0 + 0.7 * load) - 0.15 * age_bias * age_group
        wm_weight_base = np.clip(wm_weight_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # WM distribution from counts
            W_s = counts[s, :] / np.sum(counts[s, :])

            # RL choice prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice prob (deterministic softmax on W_s)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy gating: high entropy -> downweight WM
            entropy = -np.sum(W_s * np.log(np.clip(W_s, 1e-12, 1.0)))
            # Max entropy for nA=3
            max_entropy = np.log(3.0)
            norm_entropy = entropy / max_entropy  # in [0,1]
            gate = 1.0 / (1.0 + np.exp(entropy_temp * (norm_entropy - 0.5)))  # sigmoid decreasing in entropy
            wm_weight_eff = np.clip(wm_weight_base * gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta > 0.0 else alpha_neg
            q[s, a] += alpha * delta

            # WM "Bayesian" update: increment counts; add reward-weighted boost
            # Rewarded actions get a stronger increment than non-rewarded.
            incr = 1.0 + 1.5 * r
            counts[s, a] += incr

            # Light forgetting toward prior under higher load and with age
            forget = 0.02 + 0.06 * load + 0.05 * age_bias * age_group
            if forget > 0.0:
                counts[s, :] = (1.0 - forget) * counts[s, :] + forget * (prior * np.ones(nA))

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with choice-kernel bias and WM overlap interference.

    Idea
    - RL with a single learning rate and temperature.
    - WM stores fast traces; its effectiveness is reduced by interference that
      scales with set size and a WM-overlap parameter (how much chosen action spills
      onto other actions in the WM representation).
    - A choice kernel (recency bias) biases selection toward previously chosen actions;
      its strength is modulated by age (older adults rely more on such biases).
    - Policy combines RL and WM, then is shifted by a soft bias from the choice kernel.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, beta, wm_weight0, choice_lr, kernel_weight, wm_overlap]
        - alpha: RL learning rate
        - beta: base RL inverse temperature (scaled internally by x10)
        - wm_weight0: baseline WM mixture weight
        - choice_lr: learning rate for the choice kernel (recency trace)
        - kernel_weight: base weight of the choice kernel bias
        - wm_overlap: fraction of WM write that spreads to non-chosen actions

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_weight0, choice_lr, kernel_weight, wm_overlap = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age effects: older adults rely more on choice kernel, slightly less on WM
    kernel_weight_age = np.clip(kernel_weight * (1.0 + 0.5 * age_group), 0.0, 2.0)
    wm_weight_age = np.clip(wm_weight0 - 0.15 * age_group, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Choice kernel per state
        ck = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects: reduce WM weight and increase overlap interference with load
        load = max(0, nS - 3)
        wm_weight_block = np.clip(wm_weight_age / (1.0 + 0.7 * load), 0.0, 1.0)
        overlap_eff = np.clip(wm_overlap + 0.1 * load, 0.0, 1.0)
        # WM decay increases with load
        wm_decay = np.clip(0.1 + 0.1 * load, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            CK_s = ck[s, :]

            # RL policy probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix RL and WM into a base policy
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl

            # Convert base policy to action-prob vector to apply choice-kernel bias
            # We reconstruct numerator terms for chosen vs others:
            # For numerical stability, approximate by softmax over a combined score
            # Create combined scores for actions using log-probs proxy:
            # score_k ~ log policy mass each action would get; we approximate by
            # combining RL and WM preferences linearly via their softmax numerators.
            rl_pref = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            wm_pref = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            mix_pref = wm_weight_block * wm_pref + (1.0 - wm_weight_block) * rl_pref
            mix_pref = np.clip(mix_pref, 1e-12, None)
            mix_pref = mix_pref / np.sum(mix_pref)

            # Apply choice-kernel bias multiplicatively then renormalize
            bias = np.exp(kernel_weight_age * (CK_s - np.mean(CK_s)))
            biased = mix_pref * bias
            biased = biased / np.sum(biased)
            p_total = np.clip(biased[a], 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform, then write with overlap interference
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            write = 0.7 if r > 0.0 else 0.2
            w[s, :] = (1.0 - write) * w[s, :]
            # Overlap: spill a fraction of the write to non-chosen actions
            spill = overlap_eff * write
            main = (1.0 - overlap_eff) * write
            w[s, a] += main
            w[s, :] += spill / (nA - 1.0)
            w[s, a] -= spill / (nA - 1.0)  # keep total write fixed at 'write'
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Choice-kernel update: recency-smoothed indicator of last action
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            ck[s, :] = (1.0 - choice_lr) * ck[s, :] + choice_lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p