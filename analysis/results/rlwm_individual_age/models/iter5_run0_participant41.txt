def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age- and load-dependent gating and eligibility traces.

    Mechanism
    - RL: tabular Q-learning with eligibility trace that boosts credit assignment to the last chosen action.
    - WM: one-shot storage of the rewarded mapping (state->action) with decay; WM gating probability is reduced by age and larger set sizes (capacity pressure).
    - Arbitration: mixture of RL and WM policies; WM contributes more when item is likely in WM.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial; used to compute age group (0=young, 1=old).
    model_parameters : list or array, length 6
        [alpha, beta, lambda_et, wm_gate_base, age_load_scale, wm_decay]
        - alpha: RL learning rate.
        - beta: RL inverse temperature; internally scaled by x10.
        - lambda_et: eligibility trace strength added to the chosen action to sharpen learning on that trial.
        - wm_gate_base: baseline log-odds that a rewarded mapping is stored in WM (higher => more storage).
        - age_load_scale: how strongly age group and set size reduce WM storage probability.
                          gate_logit -= age_load_scale * (age_group + (nS - 3)).
        - wm_decay: per-trial WM decay towards uniform.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lambda_et, wm_gate_base, age_load_scale, wm_decay = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy probability of chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM mixture weight determined by storage/gating likelihood under age and load
            gate_logit = wm_gate_base - age_load_scale * (age_group + (nS - 3))
            wm_weight = 1.0 / (1.0 + np.exp(-gate_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with an eligibility-like boost on the chosen action
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + alpha * pe * (1.0 + lambda_et)

            # WM decay towards uniform each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, store the mapping in WM with a probability determined by the same gate
            # Here implemented deterministically in expectation by moving w[s] toward the one-hot
            if r > 0.0:
                prob_store = 1.0 / (1.0 + np.exp(-gate_logit))
                # Move toward one-hot with strength proportional to prob_store
                target = w_0[s, :].copy()
                target[a] = 1.0
                w[s, :] = (1.0 - prob_store) * w[s, :] + prob_store * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and action stickiness; age/load bias arbitration.

    Mechanism
    - RL: tabular Q-learning.
    - WM: delta-rule toward one-hot of rewarded action (state-specific associative memory).
    - Arbitration: relative certainty determines WM weight:
        wm_weight = sigmoid(arbitration_gain * (Cert_WM - Cert_RL) - age_load_bias*(age_group + (nS-3)) )
      where Cert_X is (1 - entropy) of the corresponding policy for the current state.
      Older age and larger set sizes bias against WM.
    - Stickiness: action perseveration adds a bias toward repeating last action within the block.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial; used to compute age group (0=young, 1=old).
    model_parameters : list or array, length 6
        [alpha, beta, wm_lr, arbitration_gain, age_load_bias, stickiness]
        - alpha: RL learning rate.
        - beta: baseline inverse temperature for RL policy; internally scaled by x10.
        - wm_lr: WM learning rate toward one-hot template when rewarded; otherwise drift to uniform.
        - arbitration_gain: how strongly certainty difference controls WM weight.
        - age_load_bias: subtracts from WM weight proportional to age_group and set size (nS-3).
        - stickiness: added to the softmax logit of the previously chosen action in the block.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, wm_lr, arbitration_gain, age_load_bias, stickiness = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    def softmax_probs(vec, beta_local):
        ex = np.exp(beta_local * (vec - np.max(vec)))
        return ex / np.sum(ex)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = None
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness bias
            logits_rl = q[s, :].copy()
            if last_action is not None:
                logits_rl[last_action] += stickiness
            p_rl_vec = softmax_probs(logits_rl, softmax_beta)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy (deterministic-like)
            p_wm_vec = softmax_probs(w[s, :], softmax_beta_wm)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Certainty estimates via 1 - entropy (normalized by log(nA))
            def certainty(p_vec):
                H = -np.sum(np.clip(p_vec, 1e-12, 1.0) * np.log(np.clip(p_vec, 1e-12, 1.0)))
                return 1.0 - H / np.log(nA)

            cert_rl = certainty(p_rl_vec)
            cert_wm = certainty(p_wm_vec)

            wm_logit = arbitration_gain * (cert_wm - cert_rl) - age_load_bias * (age_group + (nS - 3))
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM update: move toward one-hot if rewarded; else drift to uniform
            if r > 0.0:
                target = w_0[s, :].copy()
                target[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target
            else:
                drift = wm_lr / 2.0
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-adaptive learning and within-block WM weight fatigue modulated by age and load.

    Mechanism
    - RL: learning rate adapts to surprise magnitude: alpha_t = alpha_base + k_surprise * |PE| * (1 - alpha_base).
    - WM: item-based store on reward; WM influence starts high then fatigues across trials within the block.
    - Arbitration: wm_weight_t = sigmoid(logit(wm_start) - wm_fall_rate * trial_index * (1 + age_load_amp*(age_group + (nS-3)))).
      Older age and larger set sizes accelerate WM fatigue.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial; used to compute age group (0=young, 1=old).
    model_parameters : list or array, length 6
        [alpha_base, beta, wm_start, wm_fall_rate, age_load_amp, k_surprise]
        - alpha_base: baseline RL learning rate.
        - beta: RL inverse temperature; internally scaled by x10.
        - wm_start: initial WM weight at the start of each block (probability in [0,1] after logistic transform).
        - wm_fall_rate: rate at which WM reliance decreases with within-block trial index.
        - age_load_amp: multiplicative factor by which age/load speeds up WM fatigue.
        - k_surprise: gain controlling how much |PE| increases the RL learning rate.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta, wm_start, wm_fall_rate, age_load_amp, k_surprise = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    def to_logit(p):
        p = np.clip(p, 1e-6, 1.0 - 1e-6)
        return np.log(p / (1.0 - p))

    wm_start_logit = to_logit(np.clip(wm_start, 1e-6, 1 - 1e-6))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy prob for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy prob for chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Time-varying WM weight with age/load-accelerated fatigue
            fatigue_scale = 1.0 + age_load_amp * (age_group + (nS - 3))
            wm_logit_t = wm_start_logit - wm_fall_rate * t * fatigue_scale
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit_t))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Surprise-adaptive RL learning rate
            pe = r - Q_s[a]
            alpha_t = alpha_base + k_surprise * abs(pe) * (1.0 - alpha_base)
            alpha_t = np.clip(alpha_t, 0.0, 1.0)
            q[s, a] = Q_s[a] + alpha_t * pe

            # WM storage on reward (replace/refresh); otherwise slight decay to uniform
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
            else:
                decay = 0.1
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p