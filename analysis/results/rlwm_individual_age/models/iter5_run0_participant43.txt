def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-gated arbitration and action stickiness; age shifts arbitration toward RL.

    Mechanism
    - Policy is a convex combination of RL and WM distributions, where the WM weight is dynamic:
      wm_weight_t = sigmoid(logit(wm_weight0) + arb_gain*(H_rl - H_wm) + age_arb_shift*age_group).
      When RL is uncertain (high entropy) and WM is precise (low entropy), arbitration favors WM.
      Older adults (age_group=1) are biased by age_arb_shift toward RL or WM depending on sign.
    - Action stickiness adds a same-action bias to both RL and WM softmax distributions.
    - RL uses standard TD(0).
    - WM is a fast store that is made nearly deterministic upon reward and otherwise decays toward uniform.

    Parameters
    ----------
    model_parameters : [lr, wm_weight0, softmax_beta, stickiness, arb_gain, age_arb_shift]
      - lr: RL learning rate (0..1)
      - wm_weight0: baseline arbitration weight for WM (0..1)
      - softmax_beta: RL inverse temperature; scaled by 10 internally
      - stickiness: choice perseveration strength added to previous action's logit (>=0)
      - arb_gain: sensitivity of arbitration to entropy difference (>=0)
      - age_arb_shift: additive bias to arbitration for older adults (can be +/-)

    Returns
    -------
    float
      Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, stickiness, arb_gain, age_arb_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        prev_a = None
        log_p = 0.0

        # Modulate WM precision by set size (less precise in larger sets)
        wm_precision_scale = 1.0 / (1.0 + 0.5 * (nS - 3))
        beta_wm_eff = softmax_beta_wm * wm_precision_scale

        base_logit = logit(wm_weight0) + age_arb_shift * age_group

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax distribution
            Q_center = Q_s - np.max(Q_s)
            rl_logits = softmax_beta * Q_center

            # WM softmax distribution
            W_center = W_s - np.max(W_s)
            wm_logits = beta_wm_eff * W_center

            # Add stickiness (bias last action's logit)
            if prev_a is not None:
                rl_logits[prev_a] += stickiness
                wm_logits[prev_a] += stickiness

            # Softmax
            p_vec_rl = np.exp(rl_logits)
            p_vec_rl /= np.sum(p_vec_rl)
            p_vec_wm = np.exp(wm_logits)
            p_vec_wm /= np.sum(p_vec_wm)

            # Entropy-based arbitration
            H_rl = -np.sum(p_vec_rl * np.log(np.clip(p_vec_rl, eps, 1.0)))
            H_wm = -np.sum(p_vec_wm * np.log(np.clip(p_vec_wm, eps, 1.0)))
            wm_gate = sigmoid(base_logit + arb_gain * (H_rl - H_wm))

            # Mixture policy
            p_total = wm_gate * p_vec_wm[a] + (1.0 - wm_gate) * p_vec_rl[a]
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay and rewarded overwrite
            w = 0.9 * w + 0.1 * w_0  # mild passive decay toward uniform each trial
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

            prev_a = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM with lapse probability that increases with set size and age.

    Mechanism
    - Policy is a mixture of: lapse (uniform), WM softmax, and RL softmax.
      p = lapse * U + (1-lapse) * [wm_weight * p_wm + (1-wm_weight) * p_rl]
    - Lapse increases with cognitive load (set size) and further with age.
    - RL uses standard TD(0).
    - WM is a one-shot associative store upon reward with leak toward uniform.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_weight0, wm_leak, lapse_base, age_lapse_boost]
      - lr: RL learning rate (0..1)
      - softmax_beta: RL inverse temperature; scaled by 10 internally
      - wm_weight0: base weight of WM in arbitration (0..1)
      - wm_leak: WM leak per trial toward uniform (0..1)
      - lapse_base: base lapse rate in low load young (0..1)
      - age_lapse_boost: multiplicative increase in lapse for older adults (>=0)

    Returns
    -------
    float
      Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_weight0, wm_leak, lapse_base, age_lapse_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 40.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age dependent lapse
        load_factor = 1.0 + 0.6 * (nS - 3)  # 1.0 for 3, 1.6 for 6
        lapse = np.clip(lapse_base * load_factor * (1.0 + age_lapse_boost * age_group), 0.0, 0.99)

        # WM weight is also reduced when lapse is high (less WM engagement under high load)
        wm_w_block = np.clip(wm_weight0 * (1.0 - 0.5 * (nS - 3)), 0.0, 1.0)

        # WM precision declines with set size
        beta_wm_eff = softmax_beta_wm / load_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            Q_center = Q_s - np.max(Q_s)
            p_vec_rl = np.exp(softmax_beta * Q_center)
            p_vec_rl /= np.sum(p_vec_rl)

            # WM policy
            W_center = W_s - np.max(W_s)
            p_vec_wm = np.exp(beta_wm_eff * W_center)
            p_vec_wm /= np.sum(p_vec_wm)

            # Mixture with lapse
            p_mix = wm_w_block * p_vec_wm[a] + (1.0 - wm_w_block) * p_vec_rl[a]
            p_total = lapse * (1.0 / nA) + (1.0 - lapse) * p_mix
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM leak and overwrite on reward
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with error-driven meta-learning of learning rate + WM gated by running success; age reduces RL precision.

    Mechanism
    - RL learning rate adapts online as a function of recent absolute prediction error (APE):
        lr_t = sigmoid(logit(lr0) + err_gain * (APE_ema - 0.5))
      Larger recent errors increase lr_t; smaller errors decrease it.
    - WM weight increases with recent success (1 - APE_ema) and decreases with set size:
        wm_w_t = sigmoid(logit(wm_weight0) + k_success*(0.5 - APE_ema) - k_load*(nS-3))
      where k_success = 2.0 is fixed internally and k_load is tied to err_gain to limit parameters.
    - Older adults have reduced RL precision (lower beta) via age_beta_scale.
    - WM is a fast store that overwrites on reward and decays toward uniform.

    Parameters
    ----------
    model_parameters : [lr0, softmax_beta, wm_weight0, wm_decay, err_gain, age_beta_scale]
      - lr0: base RL learning rate (0..1)
      - softmax_beta: base RL inverse temperature; scaled by 10 internally
      - wm_weight0: baseline WM arbitration weight (0..1)
      - wm_decay: WM decay per trial toward uniform (0..1)
      - err_gain: sensitivity of meta-learning and load-based WM downscaling (>=0)
      - age_beta_scale: factor reducing RL temperature in older adults (>=0)

    Returns
    -------
    float
      Negative log-likelihood of observed choices under the model.
    """
    lr0, softmax_beta, wm_weight0, wm_decay, err_gain, age_beta_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # Age reduces RL precision
    beta_age_factor = 1.0 / (1.0 + age_beta_scale * age_group)
    softmax_beta_eff_base = softmax_beta * beta_age_factor

    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running exponential moving average of absolute prediction error
        ape_ema = 0.5  # initialized to moderate uncertainty
        ema_alpha = 0.3

        # Load penalty coefficient reused from err_gain to keep parameter count
        k_success = 2.0
        k_load = err_gain

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Meta-learned learning rate
            lr_t = sigmoid(logit(lr0) + err_gain * (ape_ema - 0.5))

            # RL policy
            Q_s = q[s, :].copy()
            Q_center = Q_s - np.max(Q_s)
            p_vec_rl = np.exp(softmax_beta_eff_base * Q_center)
            p_vec_rl /= np.sum(p_vec_rl)

            # WM policy
            W_s = w[s, :].copy()
            W_center = W_s - np.max(W_s)
            # WM precision declines with set size
            beta_wm_eff = softmax_beta_wm / (1.0 + 0.5 * (nS - 3))
            p_vec_wm = np.exp(beta_wm_eff * W_center)
            p_vec_wm /= np.sum(p_vec_wm)

            # WM arbitration weight depends on success and load
            wm_logit = logit(wm_weight0) + k_success * (0.5 - ape_ema) - k_load * (nS - 3)
            wm_w_t = sigmoid(wm_logit)

            p_total = wm_w_t * p_vec_wm[a] + (1.0 - wm_w_t) * p_vec_rl[a]
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr_t * pe

            # Update APE EMA
            ape = abs(pe)
            ape_ema = (1.0 - ema_alpha) * ape_ema + ema_alpha * ape

            # WM decay and rewarded overwrite
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)