def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age- and load-dependent binding noise.

    Idea:
    - RL learns slowly via delta rule.
    - WM stores one-shot, state-specific action after rewarded feedback, but capacity is limited (slots).
      The fraction of items that can be held in WM determines the WM mixing weight.
    - Binding noise in WM grows with set size and age, reducing WM precision.
    - WM traces decay toward uniform after non-rewarded trials.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10 internally)
    - K_base: baseline WM capacity (slots) for younger adults (>=0)
    - age_K_drop: capacity drop (slots) for older adults (>=0)
    - wm_decay: decay toward uniform for WM on non-rewarded trials (0..1)
    - epsilon_wm: WM binding noise base level (0..1), scaled up by set size and age

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, K_base, age_K_drop, wm_decay, epsilon_wm = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # very deterministic baseline for WM, before noise

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity (slots) reduced by age
        K_eff = max(0.0, K_base - age_group * age_K_drop)
        # Convert capacity to WM mixture weight based on load
        wm_weight_block = np.clip(K_eff / max(1.0, float(nS)), 0.0, 1.0)

        # Age- and load-dependent WM binding noise
        noise_scale = 1.0 + 0.5 * (nS - 3) + 0.8 * age_group
        epsilon_eff = np.clip(epsilon_wm * noise_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with binding noise (mixture with uniform)
            W_s = w[s, :]
            p_wm_clean = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - epsilon_eff) * p_wm_clean + epsilon_eff * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: one-shot encode on reward; otherwise decay toward uniform
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])  # keep as prob. dist.
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and recency-based WM that decays with time, set size, and age.

    Idea:
    - RL has separate learning rates for positive and negative prediction errors.
    - WM stores recently rewarded state-action bindings; their influence decays with the time since
      last successful encoding. Decay rate grows with set size and with age.
    - WM policy is precise when information is fresh; otherwise RL dominates.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for positive PE (0..1)
    - alpha_neg: RL learning rate for negative PE (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10)
    - decay_base: base WM recency decay rate per trial (>=0)
    - size_decay_gain: multiplicative gain of decay with set size load (>=0)
    - age_decay_gain: additional multiplicative gain of decay for older adults (>=0)

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, softmax_beta, decay_base, size_decay_gain, age_decay_gain = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last successful (rewarded) encoding time per state; initialize to "never" (-inf)
        last_enc_time = -1e9 * np.ones(nS)
        time_counter = 0

        # Build decay rate that scales with load and age
        load_factor = 1.0 + size_decay_gain * max(0, nS - 3)
        age_factor = 1.0 + age_decay_gain * age_group
        decay = decay_base * load_factor * age_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM weight from recency of last successful encoding for this state
            if last_enc_time[s] < -1e8:
                wm_weight_eff = 0.0
            else:
                elapsed = max(0.0, time_counter - last_enc_time[s])
                wm_weight_eff = np.exp(-decay * elapsed)
                wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # WM policy (from current WM store)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            q[s, a] += (alpha_pos if pe >= 0 else alpha_neg) * pe

            # WM update: encode on reward; no change otherwise (decay handled via weight)
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
                last_enc_time[s] = time_counter

            time_counter += 1

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + state-specific reliability-gated WM, with age-related lapse and load/age-reduced WM precision.

    Idea:
    - RL uses a delta rule.
    - Each state maintains a WM reliability estimate r_s (initialized at 0.5) that is updated from reward outcomes.
      WM weight is a sigmoid function of r_s relative to a threshold that increases with load and age.
    - WM precision (inverse temperature) decreases for larger set sizes and for older adults.
    - Older adults have an additional lapse probability (uniform choice mixture).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10)
    - reliab_lr: learning rate for updating state-specific WM reliability (0..1)
    - kappa: gain mapping reliability difference to WM weight via sigmoid (>0)
    - thresh_base: base threshold for reliability to engage WM (in [0,1] recommended)
    - age_lapse: additional lapse probability applied for older adults (0..1)

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, reliab_lr, kappa, thresh_base, age_lapse = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm_base = 50.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific reliability initialized at 0.5
        reliab = 0.5 * np.ones(nS)

        # WM precision reduced by load and age (no extra parameter required)
        beta_wm_eff = softmax_beta_wm_base / (1.0 + 0.5 * (nS - 3) + 0.8 * age_group)

        # Threshold raised by load and age
        threshold = np.clip(thresh_base + 0.2 * max(0, nS - 3) + 0.2 * age_group, 0.0, 1.0)

        # Age-dependent lapse
        lapse = np.clip(age_lapse * age_group, 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # WM weight from state reliability via sigmoid gate
            wm_weight_eff = 1.0 / (1.0 + np.exp(-kappa * (reliab[s] - threshold)))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture, with lapse to uniform
            p_mixed = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mixed + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: one-shot encode on reward; small fade otherwise
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # slight diffusion toward uniform to reflect uncertainty
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Update state reliability toward observed correctness (reward)
            reliab[s] += reliab_lr * (r - reliab[s])
            reliab[s] = np.clip(reliab[s], 0.0, 1.0)

        total_log_p += log_p

    return -total_log_p