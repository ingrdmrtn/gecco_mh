def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with surprise-gated arbitration, age-sensitive WM gain, and WM leak.

    Core assumptions:
    - RL learns Q-values with a single learning rate (Q-learning).
    - WM stores recent rewarded actions with leaky integration toward uniform.
    - Arbitration favors WM when WM is confident and RL is uncertain; gain of this gate is reduced or enhanced in older adults.
    - WM influence additionally scales down with larger set sizes.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age (repeated). Age group is 0 if <=45, 1 otherwise.
    model_parameters : list or array
        [lr, beta_base, wm_gain_base, age_gain_delta, wm_forget]
        - lr: RL learning rate (0..1).
        - beta_base: RL inverse temperature (scaled internally by 10).
        - wm_gain_base: baseline gain of WM-vs-RL arbitration gate.
        - age_gain_delta: change in WM gain for older adults (added if age_group=1).
        - wm_forget: WM leak rate toward uniform per visit to a state (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_gain_base, age_gain_delta, wm_forget = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted arbitration gain
        gate_gain = wm_gain_base + age_group * age_gain_delta

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Qs = q[s, :].copy()
            Qs = Qs - np.max(Qs)
            exp_rl = np.exp(softmax_beta * Qs)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM leak toward uniform, then reward-based imprint
            # Leak: move distribution toward uniform by wm_forget
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            # If rewarded, imprint chosen action (normalize to keep a distribution)
            if r > 0.0:
                imprint = np.zeros(nA)
                imprint[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * imprint  # consolidate memory
            # Ensure normalization
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # WM policy
            Ws = w[s, :]
            Ws = Ws - np.max(Ws)
            exp_wm = np.exp(softmax_beta_wm * Ws)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Confidence measures: WM separation vs RL entropy
            wm_sorted = np.sort(w[s, :])
            conf_wm = wm_sorted[-1] - wm_sorted[-2]  # larger => more confident
            rl_entropy = -np.sum(p_rl_vec * np.log(np.maximum(p_rl_vec, 1e-12))) / np.log(nA)  # normalized [0,1]
            # Arbitration: sigmoid of (WM confidence - RL entropy), scaled by set size (smaller sets -> higher weight)
            x = gate_gain * (conf_wm - rl_entropy)
            wm_weight = 1.0 / (1.0 + np.exp(-x))
            wm_weight *= (3.0 / float(nS))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL + WM with age-dependent lapse and set-size-scaled WM reliance.

    Core assumptions:
    - RL uses separate learning rates for gains and losses.
    - WM stores last rewarded action per state; when not rewarded, WM decays toward uniform
      at a rate equal to the loss learning rate (shared resource for forgetting).
    - Arbitration is a fixed set-size-based WM weight. Additionally, an age-modulated lapse
      injects uniform random choice, increasing more in larger set sizes.

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age (repeated). Age group is 0 if <=45, 1 otherwise.
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, lapse_base, lapse_old_delta]
        - alpha_pos: RL learning rate for rewards (r=1).
        - alpha_neg: RL learning rate for no-reward (r=0); also drives WM decay.
        - beta_base: RL inverse temperature (scaled internally by 10).
        - lapse_base: baseline lapse logit (before sigmoid).
        - lapse_old_delta: additive increase in lapse logit for older adults.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_base, lapse_base, lapse_old_delta = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM reliance reduces with set size
        wm_weight = np.clip(3.0 / float(nS), 0.0, 1.0)

        # Age- and set-size-dependent lapse (uniform noise)
        lapse_logit = lapse_base + age_group * lapse_old_delta
        base_lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
        # Increase lapse in larger sets (linear ramp from 3->6)
        size_factor = (float(nS) - 3.0) / 3.0  # 0 for 3, 1 for 6
        lapse_t = np.clip(base_lapse * (1.0 + size_factor), 0.0, 0.5)  # cap to keep some identifiability

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Qs = q[s, :].copy()
            Qs = Qs - np.max(Qs)
            exp_rl = np.exp(softmax_beta * Qs)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM update and policy
            if r > 0.0:
                # store last rewarded action deterministically
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # decay toward uniform at rate alpha_neg
                w[s, :] = (1.0 - alpha_neg) * w[s, :] + alpha_neg * w_0[s, :]
            # WM policy
            Ws = w[s, :]
            Ws = Ws - np.max(Ws)
            exp_wm = np.exp(softmax_beta_wm * Ws)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture before lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Lapse mixture with uniform
            p_total = (1.0 - lapse_t) * p_mix + lapse_t * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL asymmetric update
            alpha = alpha_pos if r > 0.0 else alpha_neg
            delta = r - q[s, a]
            q[s, a] += alpha * delta

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Q-decay and global choice kernel + capacity-based WM arbitration.

    Core assumptions:
    - RL Q-values decay toward uniform baseline; decay increases with larger set size and is stronger in older adults.
    - Global (state-independent) choice kernel biases repeating the recently chosen action.
    - WM capacity is slot-limited; effective WM weight equals slots/set_size, adjusted by age.
    - WM stores last rewarded action per state (no decay on errors).

    Parameters
    ----------
    states : array-like of int
        State identity on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like (single repeated value)
        Participant age (repeated). Age group is 0 if <=45, 1 otherwise.
    model_parameters : list or array
        [lr, beta_base, q_decay_base, wm_slots_base, slots_old_delta, kernel_strength]
        - lr: RL learning rate.
        - beta_base: RL inverse temperature (scaled internally by 10).
        - q_decay_base: baseline Q decay per visit (toward uniform).
        - wm_slots_base: baseline WM capacity (in slots).
        - slots_old_delta: change in capacity slots for older adults.
        - kernel_strength: strength of global choice kernel bias (and its learning rate).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, q_decay_base, wm_slots_base, slots_old_delta, kernel_strength = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global choice kernel over actions (state-independent)
        ck = np.ones(nA) / nA  # start uniform

        # Effective WM capacity and weight
        slots = wm_slots_base + age_group * slots_old_delta
        wm_weight = np.clip(slots / float(nS), 0.0, 1.0)

        # Q decay magnitude scales with set size and age
        q_decay = np.clip(q_decay_base * (float(nS) / 6.0) * (1.0 + 0.5 * age_group), 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply Q decay for this state toward uniform baseline
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * w_0[s, :]

            # RL policy with choice kernel bias
            Qs = q[s, :].copy()
            biased = Qs + kernel_strength * (ck - 1.0 / nA)  # bias toward recent choice distribution
            biased = biased - np.max(biased)
            exp_rl = np.exp(softmax_beta * biased)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM update and policy: store rewarded actions, ignore errors (no decay)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            Ws = w[s, :]
            Ws = Ws - np.max(Ws)
            exp_wm = np.exp(softmax_beta_wm * Ws)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update choice kernel toward chosen action
            ck = (1.0 - kernel_strength) * ck
            ck[a] += kernel_strength
            ck = ck / np.sum(ck)

        blocks_log_p += log_p

    return -blocks_log_p