def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic WM encoding + uncertainty-based arbitration.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM: associative table that stores rewarded action for a state with some probability,
          and decays toward uniform each trial.
    - Arbitration: trial-wise WM weight is higher when WM distribution for the state is
      more peaked (uncertainty-based), and is implicitly reduced in larger set sizes.
    - Age: older adults have reduced WM encoding probability.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group (0=young,1=old).
    model_parameters : list or array
        [alpha, beta_rl, wm_enc_base, wm_decay, beta_wm, arbitration_slope]
        - alpha: RL learning rate (0..1)
        - beta_rl: inverse temperature for RL (scaled x10 internally)
        - wm_enc_base: baseline probability to encode a rewarded association into WM
        - wm_decay: WM decay toward uniform (0..1) per trial; larger = faster decay
        - beta_wm: inverse temperature for WM policy (scaled x10 internally)
        - arbitration_slope: slope of the sigmoid mapping from WM peakedness to WM weight

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, wm_enc_base, wm_decay, beta_wm, arbitration_slope = model_parameters
    beta_rl *= 10.0
    beta_wm = 10.0 * max(1e-6, beta_wm)  # keep high, but allow estimation

    # Age group
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Size and age impact on encoding probability
        size_scale = 3.0 / float(nS)        # 1.0 for set size 3; 0.5 for set size 6
        age_scale = 0.8 if age_group == 1 else 1.0
        enc_prob = np.clip(wm_enc_base * size_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration based on WM peakedness
            peakedness = np.max(W_s) - (1.0 / nA)  # 0 when uniform, positive when peaked
            wm_weight = 1.0 / (1.0 + np.exp(-arbitration_slope * peakedness))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on reward with probability enc_prob
            if r > 0.5:
                if np.random.rand() < enc_prob:
                    w[s, :] = 0.0
                    w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce-Hall learning rate + WM with state confusion interference.

    Mechanism
    - RL: learning rate scales with unsigned prediction error (Pearce-Hall rule),
      allowing rapid learning when surprising, slower when stable.
    - WM: on reward, the selected action is stored for that state, but with a
      probability of state confusion the association is mis-bound to a different state.
      Larger set sizes and older age increase effective confusion.
    - Arbitration: fixed WM mixture weight modulated by set size and age.
    - Policies: RL and WM both softmax, then mixed.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group (0=young,1=old).
    model_parameters : list or array
        [alpha_base, beta_rl, wm_weight_base, confusion_base, vol_sensitivity, beta_wm]
        - alpha_base: baseline RL learning rate floor
        - beta_rl: RL inverse temperature (scaled x10 internally)
        - wm_weight_base: baseline WM contribution to choice
        - confusion_base: baseline probability of state confusion on WM encoding
        - vol_sensitivity: scales learning rate with unsigned prediction error (PH)
        - beta_wm: WM inverse temperature (scaled x10 internally)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta_rl, wm_weight_base, confusion_base, vol_sensitivity, beta_wm = model_parameters
    beta_rl *= 10.0
    beta_wm *= 10.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM table
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture weight: reduced for larger set sizes and for older adults
        wm_weight = wm_weight_base * (3.0 / float(nS)) * (0.85 if age_group == 1 else 1.0)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        # Effective confusion increases with set size and age
        confusion = confusion_base * (float(nS) / 3.0) * (1.3 if age_group == 1 else 1.0)
        confusion = np.clip(confusion, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mix
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall learning rate
            delta = r - Q_s[a]
            alpha_t = np.clip(alpha_base + vol_sensitivity * abs(delta), 0.0, 1.0)
            q[s, a] += alpha_t * delta

            # WM encoding only on reward; possible state confusion
            if r > 0.5:
                # Decide target state for memory: correct or confused
                if np.random.rand() < confusion:
                    # Mis-bind to a random other state
                    other_states = [ss for ss in range(nS) if ss != s]
                    if len(other_states) > 0:
                        s_mem = np.random.choice(other_states)
                    else:
                        s_mem = s
                else:
                    s_mem = s

                w[s_mem, :] = 0.0
                w[s_mem, a] = 1.0
            else:
                # Mild drift to uniform after errors (interference)
                w[s, :] = 0.75 * w[s, :] + 0.25 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay + capacity-limited WM store with eviction.

    Mechanism
    - RL: tabular Q-learning with softmax; Q-values decay toward uniform baseline each trial.
    - WM: maintains a set of at most K states (capacity) with deterministic action bindings
          for rewarded pairings; if capacity exceeded, evicts the oldest state (FIFO).
          Older adults have reduced effective capacity (age penalty).
    - Arbitration: if the current state is in WM, decisions rely mostly on WM; otherwise RL.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group (0=young,1=old).
    model_parameters : list or array
        [alpha, beta_rl, beta_wm, q_decay, wm_capacity, age_wm_penalty]
        - alpha: RL learning rate
        - beta_rl: RL inverse temperature (scaled x10 internally)
        - beta_wm: WM inverse temperature (scaled x10 internally)
        - q_decay: per-trial decay of Q-values toward uniform (0..1)
        - wm_capacity: baseline WM capacity in number of states (>=1)
        - age_wm_penalty: capacity reduction applied if older adult

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, beta_wm, q_decay, wm_capacity, age_wm_penalty = model_parameters
    beta_rl *= 10.0
    beta_wm *= 10.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        q0 = (1.0 / nA) * np.ones((nS, nA))

        # WM storage: action templates and an ordered list for eviction
        w = (1.0 / nA) * np.ones((nS, nA))
        stored_states = []  # order reflects recency of insertion

        # Effective capacity, reduced by age penalty
        K_eff = int(max(1, round(wm_capacity - (age_wm_penalty if age_group == 1 else 0.0))))
        K_eff = int(min(K_eff, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: if s is stored, W_s is peaked; else uniform
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: deterministic gating by membership in WM
            wm_weight = 1.0 if (s in stored_states) else 0.0

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += alpha * delta
            # Apply decay to current state's row (local decay for computational parsimony)
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * q0[s, :]

            # WM update: store on reward, evict if needed
            if r > 0.5:
                # Insert or refresh recency
                if s in stored_states:
                    # Move to most recent
                    stored_states.remove(s)
                stored_states.append(s)

                # Evict if over capacity
                while len(stored_states) > K_eff:
                    s_evict = stored_states.pop(0)
                    w[s_evict, :] = 1.0 / nA  # reset to uniform

                # Write deterministic binding for s
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p