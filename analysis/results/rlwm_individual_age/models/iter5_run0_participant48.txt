Below are three standalone cognitive models that follow the requested template, each returning the negative log-likelihood of the observed choices. They differ in how RL and WM interact, how load (set size) and age group affect parameters, and in the additional mechanisms introduced.

Note: Assume numpy is already imported as np.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning and age-dependent lapse and WM interference.

    Mechanism
    - RL: Q-learning with asymmetric learning rates for positive vs. negative outcomes.
    - WM: one-shot storage on rewarded trials with decay/interference toward uniform.
    - Policy: mixture of WM and RL; plus an age-dependent lapse that mixes in uniform choice.
    - Load and age:
        - WM weight is anchored at set size 3 (wm_weight_base) and scaled by 3/nS.
        - WM interference increases at set size 6 via wm_interf_6 (stronger decay toward uniform).
        - Older adults have higher lapse rate added via lapse_age.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (per block, 0..nS-1).
    actions : array-like, int
        Chosen action on each trial (0..2).
    rewards : array-like, int
        Binary reward feedback (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size per trial (3 or 6), constant within a block.
    age : array-like, int
        Participant age (same value repeated). Age group is 0 if <=45 else 1.
    model_parameters : list or array
        [lr_pos, wm_weight_base, softmax_beta, lr_neg, wm_interf_6, lapse_age]
        - lr_pos: RL learning rate for positive prediction errors
        - wm_weight_base: baseline WM mixture weight at set size 3
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - lr_neg: RL learning rate for negative prediction errors
        - wm_interf_6: additional WM interference (decay) at set size 6 relative to 3
        - lapse_age: lapse increment applied if age group is older (added to base lapse of 0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]  # lr is lr_pos, wm_weight is base, beta is RL temp
    softmax_beta *= 10.0  # higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Additional parameters
    lr_neg = model_parameters[3]
    wm_interf_6 = model_parameters[4]
    lapse_age = model_parameters[5]

    softmax_beta_wm = 50.0  # near-deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective WM weight and WM interference given load and age
        load_scale = 3.0 / nS
        wm_weight_eff = wm_weight * load_scale
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # WM interference: higher at set size 6
        base_decay = 0.05  # small passive decay toward uniform each trial
        extra_interf = wm_interf_6 if nS == 6 else 0.0
        wm_decay = np.clip(base_decay + extra_interf, 0.0, 1.0)

        # Lapse increases with age
        lapse = np.clip(lapse_age * age_group, 0.0, 0.3)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with age-dependent lapse to uniform
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM decay toward uniform (interference)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # One-shot storage of correct action on reward
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM arbitration by reliability difference; age biases arbitration toward RL.

    Mechanism
    - RL: Q-learning; inverse temperature softmax_beta governs stochasticity.
    - WM: one-shot storage on reward with decay toward uniform.
    - Arbitration: trial-wise WM weight computed via a sigmoid on (WM confidence − RL confidence).
      Age adds a negative bias, pushing older adults toward RL control.
    - Load and age:
        - WM strength is reduced by set size via wm_weight_base * (3/nS).
        - Older adults have an arbitration bias (age_bias) decreasing WM usage.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (per block).
    actions : array-like, int
        Chosen action (0..2).
    rewards : array-like, int
        Binary reward (0/1).
    blocks : array-like, int
        Block index.
    set_sizes : array-like, int
        Set size (3 or 6), constant within a block.
    age : array-like, int
        Age (repeated); age_group = 0 if <=45 else 1.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_decay, k_gain, age_bias]
        - lr: RL learning rate
        - wm_weight_base: baseline WM strength at set size 3
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_decay: WM decay toward uniform (0..1 per trial)
        - k_gain: arbitration gain; steeper sigmoid as k increases
        - age_bias: additive bias term applied when age_group=1 (older),
                    decreasing WM arbitration (negative bias pushes toward RL)

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    wm_decay = model_parameters[3]
    k_gain = model_parameters[4]
    age_bias = model_parameters[5]

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Baseline WM strength reduced by load
        wm_strength = np.clip(wm_weight * (3.0 / nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice prob for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice prob for chosen action a
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute "confidence" as spread: max - mean
            conf_wm = np.max(W_s) - np.mean(W_s)
            conf_rl = np.max(Q_s) - np.mean(Q_s)

            # Arbitration weight for WM via sigmoid, with age bias against WM
            arbi_input = k_gain * (conf_wm - conf_rl) - (age_bias * age_group)
            wm_gate = 1.0 / (1.0 + np.exp(-arbi_input))
            wm_gate = np.clip(wm_gate * wm_strength, 0.0, 1.0)

            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay and overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus and capacity-limited WM; age reduces WM capacity.

    Mechanism
    - RL: Q-learning plus directed exploration via a per-state/action uncertainty bonus
      ucb_bonus / sqrt(N_sa + 1), where N_sa counts state-action encounters in the block.
      RL inverse temperature decreases under load (beta scales by 3/nS).
    - WM: capacity-limited store of state→action mappings with near-deterministic recall.
      If state is stored in WM, policy weight is near 1; otherwise rely on RL.
    - Load and age:
        - WM capacity K_eff = max(0, K - age_capacity_shift*age_group), so older group has
          fewer reliable WM slots.
        - RL beta under load: beta_rl_eff = softmax_beta * (3/nS).
        - The parameter labeled wm_weight_base is repurposed here as RL forgetting rate (rho),
          implementing decay of Q-values toward uniform to capture load-related forgetting.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (per block).
    actions : array-like, int
        Chosen action (0..2).
    rewards : array-like, int
        Binary rewards.
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size (3 or 6) per trial, constant within block.
    age : array-like, int
        Age (repeated); age_group = 0 if <=45 else 1.
    model_parameters : list or array
        [lr, rho_forget, softmax_beta, wm_capacity_K, ucb_bonus, age_capacity_shift]
        - lr: RL learning rate
        - rho_forget: RL forgetting rate toward uniform each trial (0..1)
        - softmax_beta: base RL inverse temperature (scaled by 10 and then by 3/nS)
        - wm_capacity_K: WM capacity in number of state-action pairs at set size 3
        - ucb_bonus: directed exploration bonus magnitude
        - age_capacity_shift: capacity reduction applied if older (K_eff = K - shift)

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]  # wm_weight used as rho_forget here
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    wm_capacity_K = model_parameters[3]
    ucb_bonus = model_parameters[4]
    age_capacity_shift = model_parameters[5]

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and visitation counts for UCB-like bonus
        q = (1.0 / nA) * np.ones((nS, nA))
        visitN = np.zeros((nS, nA))

        # WM table and initial baseline
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL beta decreases with load
        beta_rl_eff = softmax_beta * (3.0 / nS)

        # RL forgetting rate rho
        rho_forget = np.clip(wm_weight, 0.0, 1.0)

        # WM capacity with age reduction
        K_eff = max(0.0, wm_capacity_K - age_capacity_shift * age_group)

        # Track which states are in WM and their recency for eviction
        in_wm = np.zeros(nS, dtype=bool)
        recency = np.zeros(nS)  # larger means more recent; for eviction we drop the least recent

        time_counter = 0.0
        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            time_counter += 1.0

            # RL with uncertainty bonus
            Q_s = q[s, :].copy()
            bonus = ucb_bonus / np.sqrt(visitN[s, :] + 1.0)
            Q_aug = Q_s + bonus
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_aug - Q_aug[a])))

            # WM policy and gating by capacity membership
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight: 1.0 if state is in WM, else 0.0
            wm_weight_t = 1.0 if in_wm[s] else 0.0
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            pe = r - q[s, a]
            q[s, a] += lr * pe
            # Forgetting for all actions in the current state toward uniform
            q[s, :] = (1.0 - rho_forget) * q[s, :] + rho_forget * (1.0 / nA)

            # Update visitation counts
            visitN[s, a] += 1.0

            # WM update and capacity management
            # If rewarded, store deterministically; if not, do nothing to WM values
            if r == 1:
                # If WM capacity exceeded, evict the least recent stored state
                if np.sum(in_wm) >= K_eff and K_eff > 0:
                    # Evict among currently stored states: smallest recency value
                    stored_indices = np.where(in_wm)[0]
                    if stored_indices.size > 0:
                        oldest = stored_indices[np.argmin(recency[stored_indices])]
                        in_wm[oldest] = False
                        w[oldest, :] = w_0[oldest, :]

                # Store current state if capacity allows
                if K_eff > 0:
                    in_wm[s] = True
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                    recency[s] = time_counter

            # Passive decay of WM for non-stored states toward uniform (keeps table bounded)
            if not in_wm[s]:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Update recency if state is in WM
            if in_wm[s]:
                recency[s] = time_counter

        blocks_log_p += log_p

    return -blocks_log_p