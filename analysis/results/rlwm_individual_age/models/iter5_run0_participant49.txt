def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM slots with age-scaled inverse temperature and capacity-limited WM gating.

    Idea
    - RL: standard Q-learning with single learning rate.
    - WM: item-in-slot model. Probability that a stateâ€™s correct action is in WM is proportional
      to a capacity parameter divided by current set size; older age reduces effective capacity.
    - Gating: WM encoding occurs on rewarded trials; WM is then used deterministically when available.
      WM engagement weight equals the current storage probability.
    - Age: reduces RL inverse temperature and WM capacity.
    - Set size: directly reduces WM availability via capacity/set-size.

    Parameters
    ----------
    model_parameters : [lr, beta_base, wm_capacity, wm_gate_thresh, age_beta_drop]
        lr : RL learning rate (0..1)
        beta_base : base inverse temperature for RL (scaled by 10 internally)
        wm_capacity : WM capacity in items (>=0)
        wm_gate_thresh : minimum number of past rewards in a state before WM encodes deterministically
                         (acts as a gating threshold; >=0, continuous allowed)
        age_beta_drop : proportional drop in RL beta for older group (0..1)
    Returns
    -------
    float : Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_capacity, wm_gate_thresh, age_beta_drop = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0 * max(0.05, (1.0 - age_beta_drop * age_group))  # keep >0
    softmax_beta_wm = 50.0  # deterministic WM when used
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM store
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        reward_counts = np.zeros(nS)  # counts rewards per state for gating

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM availability via capacity slots (reduced by age)
            eff_capacity = wm_capacity * (1.0 - 0.3 * age_group)
            p_store = min(1.0, max(0.0, eff_capacity / max(1, nS_t)))

            # Additional gating: require sufficient prior reward evidence in this state
            gate_ok = 1.0 if (reward_counts[s] >= wm_gate_thresh) else 0.0
            wm_weight_eff = np.clip(p_store * gate_ok, 0.0, 1.0)

            # WM policy (deterministic when an item is present)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: encode on reward; mild passive leak towards uniform otherwise
            leak = 0.02  # small, constant leak (does not add a parameter)
            w = (1.0 - leak) * w + leak * w_0
            if r >= 0.5:
                reward_counts[s] += 1.0
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-dependent Q-forgetting, WM mixture, and lapse.

    Idea
    - RL: Q-learning with per-trial forgetting (drift toward uniform) that increases with set size
      and age (captures difficulty maintaining value representations under higher load and aging).
    - WM: fast encoding of last rewarded action per state; WM mixture weight decreases with set size
      and age.
    - Lapse: small probability of random choice that increases with set size and age.

    Parameters
    ----------
    model_parameters : [lr, beta_base, q_forget_base, wm_weight_base, lapse_base]
        lr : RL learning rate (0..1)
        beta_base : base inverse temperature for RL (scaled by 10 internally)
        q_forget_base : base Q forgetting rate per trial (>=0), scaled by load and age
        wm_weight_base : base WM mixture weight (0..1)
        lapse_base : base lapse probability (0..1), scaled by load and age
    Returns
    -------
    float : Negative log-likelihood of the observed choices.
    """
    lr, beta_base, q_forget_base, wm_weight_base, lapse_base = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM weight decreases with load and age
            excess = max(0, nS_t - 3)
            wm_weight_eff = wm_weight_base / (1.0 + 0.6 * excess) / (1.0 + 0.5 * age_group)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse increases with load and age
            lapse = lapse_base * (1.0 + 0.5 * age_group) * (nS_t / 3.0)
            lapse = np.clip(lapse, 0.0, 0.25)

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform (scaled by load and age)
            pe = r - q[s, a]
            q[s, a] += lr * pe
            forget = q_forget_base * (1.0 + 0.5 * age_group) * (nS_t / 3.0)
            forget = np.clip(forget, 0.0, 0.5)
            q = (1.0 - forget) * q + forget * (1.0 / nA)

            # WM update: encode rewarded action; decay toward uniform otherwise
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target
            else:
                # set-size dependent WM decay
                wm_decay = 0.05 * (nS_t / 3.0)
                w = (1.0 - wm_decay) * w + wm_decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated WM/RL arbitration: mixture weight is a logistic function of WM confidence,
    RL uncertainty, set size, and age.

    Idea
    - RL: Q-learning; policy uncertainty measured by entropy of RL softmax.
    - WM: deterministic for stored items; confidence measured as low entropy of WM softmax.
    - Arbitration: mixture weight = sigmoid(wm_bias + gain_uncertainty*(WM_conf - RL_uncertainty)
      - penalty_setsize * excess_items * age_factor).
    - Age: increases the penalty of set size on WM engagement.
    - Set size: penalizes WM engagement; also induces mild WM decay.

    Parameters
    ----------
    model_parameters : [lr, beta_base, wm_bias, gain_uncertainty, penalty_setsize]
        lr : RL learning rate (0..1)
        beta_base : base inverse temperature for RL (scaled by 10 internally)
        wm_bias : intercept for WM weight (negative favors RL, positive favors WM)
        gain_uncertainty : gain on (WM_confidence - RL_uncertainty) in the logistic gate (>=0)
        penalty_setsize : penalty per item over 3 on WM weight (>=0)
    Returns
    -------
    float : Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_bias, gain_uncertainty, penalty_setsize = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0
    nA = 3
    log_nA = np.log(nA)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax and uncertainty (entropy)
            # compute normalized RL probs
            z_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = z_rl / np.sum(z_rl)
            H_rl = -np.sum(pi_rl * np.log(np.maximum(pi_rl, eps))) / log_nA  # 0..1
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax and confidence (1 - entropy)
            z_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = z_wm / np.sum(z_wm)
            H_wm = -np.sum(pi_wm * np.log(np.maximum(pi_wm, eps))) / log_nA  # 0..1
            WM_conf = 1.0 - H_wm
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration logistic
            excess = max(0, nS_t - 3)
            set_penalty = penalty_setsize * excess * (1.0 + 0.5 * age_group)
            gate_input = wm_bias + gain_uncertainty * (WM_conf - H_rl) - set_penalty
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: encode on reward; decay grows with set size
            wm_decay = 0.03 * (nS_t / 3.0)
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p