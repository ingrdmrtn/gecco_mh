def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: Uncertainty-based arbitration between RL and WM

    Description:
    - RL: Q-learning with softmax action selection (inverse temperature beta*10).
    - WM: Reward-gated memory with decay to uniform; WM policy has its own temperature (wm_beta).
    - Arbitration: Trial-wise mixture weight is a sigmoid of the entropy difference between WM and RL.
      When WM is more certain (lower entropy), arbitration favors WM. Larger set sizes reduce the
      arbitration slope. Older adults have a bias against WM via an age penalty.
    
    Parameters:
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature base (scaled by 10 internally)
    - wm_beta: WM inverse temperature base (scaled by 10 internally)
    - arb_slope: Slope controlling sensitivity of arbitration to entropy difference
    - arb_bias: Baseline bias toward WM (>0 favors WM, <0 favors RL)
    - age_arb_penalty: Additional negative bias applied if age_group==1 (older)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_beta, arb_slope, arb_bias, age_arb_penalty = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    # WM parameters (fixed decay, reward-gated update)
    wm_decay = 0.2  # decay toward uniform each trial
    wm_beta_eff_base = max(1e-3, wm_beta) * 10.0  # WM temperature

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Full RL policy for uncertainty (entropy) measure
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(rl_logits)
            p_rl_vec /= np.sum(p_rl_vec)

            # WM chosen-action probability using WM temperature
            wm_beta_eff = wm_beta_eff_base
            wm_logits = wm_beta_eff * (W_s - W_s[a])
            p_wm = 1.0 / np.sum(np.exp(wm_logits))

            # Full WM policy for uncertainty (entropy) measure
            wm_logits_full = wm_beta_eff * (W_s - np.max(W_s))
            p_wm_vec = np.exp(wm_logits_full)
            p_wm_vec /= np.sum(p_wm_vec)

            # Entropy computations
            eps = 1e-12
            H_rl = -np.sum(p_rl_vec * np.log(p_rl_vec + eps))
            H_wm = -np.sum(p_wm_vec * np.log(p_wm_vec + eps))

            # Arbitration: favor WM when H_wm < H_rl (i.e., WM is more certain)
            # Set-size reduces arbitration sensitivity via (3/nS)
            ss_scale = 3.0 / nS
            arb_input = arb_bias - age_group * abs(age_arb_penalty) + (arb_slope * ss_scale) * (H_rl - H_wm)
            wm_weight = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay to baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM update toward one-hot action template
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong reward-driven imprint; gate proportional to surprise |delta|
                gate = min(1.0, abs(delta) + 0.5)  # in [0.5, 1.0] when r>0
                w[s, :] = (1.0 - gate) * w[s, :] + gate * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: Capacity-limited WM hit probability + RL

    Description:
    - RL: Q-learning with softmax.
    - WM: Reward-gated one-hot memory trace updated with wm_alpha and decaying slightly each visit.
    - Mixture: The WM weight equals the probability that the current state is held in WM:
      hit_prob = min(1, K_eff / set_size), reduced further by an age penalty for older adults.
      This captures set-size load effects via a simple capacity account.
    
    Parameters:
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature base (scaled by 10 internally)
    - K_eff: Effective WM capacity in number of state-action bindings (>=0)
    - wm_alpha: WM learning rate toward the rewarded chosen action (0..1)
    - age_penalty: Proportional reduction in WM hit probability for older adults (0..1)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, K_eff, wm_alpha, age_penalty = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    wm_temp = 50.0  # near-deterministic WM readout

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Fixed small decay per visit
        wm_decay = 0.1

        # Compute WM hit probability for this block
        hit_prob = min(1.0, max(0.0, K_eff) / float(nS))
        if age_group == 1:
            hit_prob *= (1.0 - np.clip(age_penalty, 0.0, 1.0))
        hit_prob = np.clip(hit_prob, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen-action probability
            p_wm = 1.0 / np.sum(np.exp(wm_temp * (W_s - W_s[a])))

            # Mixture by WM hit probability
            wm_weight = hit_prob
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM update: move toward one-hot chosen action on reward
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: Asymmetric RL learning rates + PE-gated WM with age-modulated WM temperature

    Description:
    - RL: Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: Reward/PE-gated update toward chosen action, with set-size-scaled WM learning rate.
           WM policy temperature is reduced (more noise) for older adults.
    - Arbitration: Mixture weight derived from WM certainty (low entropy => higher weight),
                   with a fixed slope; no extra parameter.
    
    Parameters:
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - beta: RL inverse temperature base (scaled by 10 internally)
    - wm_lr_base: Baseline WM learning rate (0..1) scaled by 3/set_size
    - pe_gate: Threshold on positive PE to trigger strong WM update (>=0)
    - age_wm_temp: Factor increasing WM noise for older adults (>=0; larger => lower WM temperature)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta, wm_lr_base, pe_gate, age_wm_temp = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaled WM learning rate
        wm_lr = np.clip(wm_lr_base * (3.0 / nS), 0.0, 1.0)

        # WM temperature: older adults have lower effective temperature (more noise)
        wm_temp_base = 50.0
        wm_temp = wm_temp_base / (1.0 + age_group * max(0.0, age_wm_temp))

        # Small decay each visit
        wm_decay = 0.15

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen-action probability
            p_wm = 1.0 / np.sum(np.exp(wm_temp * (W_s - W_s[a])))

            # Arbitration based on WM entropy (certainty heuristic)
            wm_logits_full = wm_temp * (W_s - np.max(W_s))
            p_wm_vec = np.exp(wm_logits_full)
            p_wm_vec /= np.sum(p_wm_vec)
            eps = 1e-12
            H_wm = -np.sum(p_wm_vec * np.log(p_wm_vec + eps))
            # Normalize entropy to [0, log(nA)]; convert to certainty in [0,1]
            H_max = np.log(nA)
            wm_cert = np.clip(1.0 - H_wm / (H_max + eps), 0.0, 1.0)
            # Fixed slope mapping certainty to weight
            wm_weight = 1.0 / (1.0 + np.exp(-4.0 * (wm_cert - 0.5)))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # PE-gated WM update: only for sufficiently positive PE
            if pe > pe_gate:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p