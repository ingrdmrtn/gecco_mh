def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM capacity-gated mixture with state-wise choice-kernel perseveration.

    Idea
    - Choices are a mixture of model-free RL and WM policies.
    - WM engagement is gated by an age-dependent capacity K relative to set size nS.
      Older adults have a lower effective K than younger adults.
    - RL policy is augmented by a choice-kernel that captures perseveration within state.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, K_young, K_old, eta_ck, ck_decay]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - K_young: WM capacity for younger group (in items).
        - K_old: WM capacity for older group (in items).
        - eta_ck: weight of choice-kernel in the RL softmax (>=0).
        - ck_decay: decay of choice-kernel per trial (0..1); higher = more persistent perseveration.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, K_young, K_old, eta_ck, ck_decay = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0  # near-deterministic WM

        # Age-dependent WM capacity
        K = K_old if age_group == 1 else K_young
        # WM weight is capped by capacity K relative to set size
        wm_weight = float(np.clip(K / max(1.0, nS), 0.0, 1.0))

        # Initialize RL values, WM weights, and choice-kernel
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        ck = np.zeros((nS, nA))  # centered choice-kernel

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with choice-kernel bias
            Q_s = q[s, :]
            CK_s = ck[s, :]
            # Center CK_s to keep scale comparable
            CK_s_centered = CK_s - np.mean(CK_s)
            prefs = softmax_beta * (Q_s - np.max(Q_s)) + eta_ck * CK_s_centered
            pi_rl = np.exp(prefs - np.max(prefs))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Choice-kernel update (decay then increment chosen action)
            ck[s, :] *= ck_decay
            ck[s, a] += (1.0 - ck_decay)

            # WM update: on reward, store perfect association for state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If no reward, leave WM as is (could be wrong memory)
                pass

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with WM decay/interference and age-modulated lapse.

    Idea
    - Choices are a mixture of RL and WM policies, then passed through an epsilon-lapse.
    - WM trace decays toward uniform and suffers more interference in larger set sizes.
    - Older adults exhibit more lapses (stimulus-independent random responding).
    - RL uses a single learning rate.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_base, rho, ss_interf, age_eps_boost]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_base: baseline WM mixture weight (0..1).
        - rho: per-trial WM decay to uniform (0..1).
        - ss_interf: additional WM weakening per item above 3 (>=0).
        - age_eps_boost: added lapse probability for older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_base, rho, ss_interf, age_eps_boost = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        # Set-size dependent WM down-weighting via interference
        ss_penalty = max(0, nS - 3)
        wm_weight = wm_base * np.exp(-ss_interf * ss_penalty)
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        # Age-modulated lapse (epsilon): base derived from beta, plus age boost
        # Map beta to a small base lapse; more deterministic beta => smaller lapse
        eps_base = 1.0 / (1.0 + softmax_beta)  # in (0,1)
        epsilon = float(np.clip(eps_base + age_group * abs(age_eps_boost), 0.0, 0.5))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            pref = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(pref - np.max(pref))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture then lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform every trial (global decay)
            w = (1.0 - rho) * w + rho * w_0

            # On reward, commit one-shot memory for current state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with RL forgetting.

    Idea
    - Arbitration weight for WM is computed trial-by-trial via a logistic function
      of (WM confidence - RL uncertainty), plus biases from set size and age.
    - WM confidence is the max probability in WM for the current state.
    - RL uncertainty is the entropy of the RL softmax policy.
    - RL includes a leakage (forgetting) toward uniform each trial.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_bias, ss_w_bias, age_w_bias, q_forget]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_bias: baseline bias favoring WM in arbitration (logit space).
        - ss_w_bias: reduction in WM arbitration per item above 3 (logit slope, >=0).
        - age_w_bias: additional reduction in WM arbitration for older adults (logit, >=0).
        - q_forget: RL forgetting/leak per trial toward uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_bias, ss_w_bias, age_w_bias, q_forget = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy and entropy (uncertainty)
            Q_s = q[s, :]
            prefs = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs - np.max(prefs))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])
            H_rl = entropy(pi_rl)  # in [0, ln(nA)]

            # WM policy and confidence
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])
            wm_conf = float(np.max(W_s))  # in [1/nA, 1]

            # Arbitration: logit of WM weight
            ss_penalty = max(0, nS - 3)
            # Scale RL entropy to [0,1] by dividing by ln(nA)
            H_scaled = H_rl / np.log(nA)
            # Center WM confidence to [0,1] naturally; contrast WM confidence vs RL uncertainty
            logit_w = (wm_bias
                       - ss_w_bias * ss_penalty
                       - age_w_bias * age_group
                       + 5.0 * (wm_conf - H_scaled))  # slope 5 enhances sensitivity
            wm_weight = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA) * np.ones_like(q)

            # WM update: rewarded one-shot storage
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # No WM reset on errors; keep current noisy memory
                pass

        blocks_log_p += log_p

    return -float(blocks_log_p)