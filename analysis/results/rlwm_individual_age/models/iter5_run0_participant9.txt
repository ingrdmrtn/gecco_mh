def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying Working Memory (WM) with set-size and age-dependent decay, plus lapse.

    Mechanism
    - RL: standard Q-learning with single learning rate and softmax choice.
    - WM: per-state action distribution that becomes peaked on rewarded actions, but decays toward uniform.
    - Mixture: choices are a convex combination of WM and RL policies.
    - Set-size impact: WM decays faster in larger set sizes.
    - Age impact: Older group exhibits additional WM decay.

    Parameters (6)
    - alpha_rl: RL learning rate (0..1)
    - beta: RL inverse temperature; internally scaled by 10 for range
    - wm_weight0: base WM mixture weight (0..1)
    - k_size_decay: additional WM decay per extra item beyond 3 (>=0)
    - k_age_decay: additional WM decay if age_group==1 (>=0)
    - epsilon_lapse: lapse probability to choose uniformly at random (0..0.2 recommended)

    Inputs
    - states, actions, rewards: trial-wise arrays
    - blocks: block index per trial; Q and WM reset per block
    - set_sizes: set size per trial (constant within block)
    - age: array of a single repeated value; <=45 => 0 (young); >45 => 1 (old)
    - model_parameters: list of 6 parameters

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_rl, beta, wm_weight0, k_size_decay, k_age_decay, epsilon_lapse = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age-dependent WM decay (per trial transition)
        extra_items = max(0.0, float(nS) - 3.0)
        decay = 1.0 - (k_size_decay * (extra_items / 3.0) + k_age_decay * age_group)
        decay = max(0.0, min(1.0, decay))

        wm_mix = max(0.0, min(1.0, wm_weight0))
        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture + lapse
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe

            # WM decay toward uniform
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            # Reward-gated WM strengthening
            if r > 0.0:
                # Move distribution toward a delta on chosen action
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + uncertainty-gated WM controller, with set-size-limited precision.

    Mechanism
    - RL: Q-learning with softmax.
    - WM: stores rewarded action per state (deterministic template), but precision limited by set size.
    - Controller: WM mixture weight increases with RL policy uncertainty (entropy), decreases with set size,
      and is modulated by age.

    Parameters (5)
    - alpha_rl: RL learning rate (0..1)
    - beta: RL inverse temperature; internally scaled by 10
    - wm_bias: bias term for WM mixture (logit space)
    - k_uncert: gain from RL entropy to WM weight (logit space, >=0)
    - k_age: age penalty on WM weight in logits (>=0), applied if age_group==1

    Inputs
    - states, actions, rewards: trial-wise arrays
    - blocks: block index per trial; Q and WM reset per block
    - set_sizes: set size per trial (constant within block)
    - age: array of a single repeated value; <=45 => 0 (young); >45 => 1 (old)
    - model_parameters: list of 5 parameters

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_rl, beta, wm_bias, k_uncert, k_age = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size limits WM precision: larger set size -> more smoothing toward uniform
        precision_scale = 3.0 / float(nS)  # 1.0 for nS=3, 0.5 for nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy and entropy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)
            # entropy in nats
            entropy = -np.sum(p_rl_vec * np.log(np.maximum(p_rl_vec, 1e-12)))
            # Max entropy for 3 actions is ln(3); normalize to [0,1]
            entropy_norm = entropy / np.log(nA)

            # WM policy from current w
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Controller: logit of WM mixture weight
            # - baseline bias (wm_bias)
            # - + k_uncert * entropy_norm
            # - - log(nS) acts implicitly via precision_scale by altering WM map (see update)
            # - - k_age if older
            logit_wm = wm_bias + k_uncert * entropy_norm - k_age * age_group
            wm_mix = 1.0 / (1.0 + np.exp(-logit_wm))
            wm_mix = max(0.0, min(1.0, wm_mix))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe

            # WM update with set-size-limited precision:
            # Move current WM vector toward a one-hot on rewarded action; the step size is scaled by precision_scale
            # and only applied if reward is positive.
            # Always add mild smoothing toward uniform (1 - precision_scale).
            # First, smoothing
            w[s, :] = (1.0 - precision_scale) * w[s, :] + precision_scale * w_0[s, :] * 0.0 + w[s, :] * 0.0 + w[s, :]
            # Then, if rewarded, sharpen toward the chosen action by precision_scale
            if r > 0.0:
                w[s, :] = (1.0 - precision_scale) * w[s, :] + precision_scale * 0.0
                w[s, :] = (1.0 - precision_scale) * w[s, :]
                w[s, :] = 0.0 * w[s, :] + w[s, :]
                # Explicit sharpen to delta with strength precision_scale
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - precision_scale) * w[s, :] + precision_scale * target
            else:
                # No reward: weak drift toward uniform due to limited precision
                w[s, :] = (1.0 - precision_scale) * w[s, :] + precision_scale * w_0[s, :]

            # Renormalize to be safe
            ssum = np.sum(w[s, :])
            if ssum <= 0:
                w[s, :] = w_0[s, :]
            else:
                w[s, :] /= ssum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and WM with set-size- and age-dependent swap noise.

    Mechanism
    - RL: separate learning rates for positive and negative outcomes; softmax choice.
    - WM: on reward, stores a delta for the chosen action; BUT retrieval suffers from swap/misbinding noise
      that grows with set size and age. This reduces WM reliability by mixing with a random incorrect action.
    - Mixture: convex combination of noisy-WM policy and RL policy.

    Parameters (6)
    - alpha_pos: RL learning rate for rewards (0..1)
    - alpha_neg: RL learning rate for non-rewards (0..1)
    - beta: RL inverse temperature; internally scaled by 10
    - wm_weight: base WM mixture weight (0..1)
    - swap_base: base WM swap/misbinding rate (0..1)
    - k_age_swap: additional swap if older (>=0)

    Inputs
    - states, actions, rewards: trial-wise arrays
    - blocks: block index per trial; Q and WM reset per block
    - set_sizes: set size per trial (constant within block)
    - age: array of a single repeated value; <=45 => 0 (young); >45 => 1 (old)
    - model_parameters: list of 6 parameters

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, wm_weight, swap_base, k_age_swap = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent swap: increases with items beyond 3; age adds extra swap
        extra_items = max(0.0, float(nS) - 3.0)
        swap_rate = swap_base + (extra_items / 3.0) * swap_base + k_age_swap * age_group
        swap_rate = max(0.0, min(1.0, swap_rate))

        wm_mix = max(0.0, min(1.0, wm_weight))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM retrieval with swap noise:
            # Retrieve a near-delta from w; then corrupt by swapping to a random incorrect action
            W_s = w[s, :].copy()
            # Ideal WM policy from stored distribution
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_ideal = expW / np.sum(expW)

            # Build swapped distribution: with probability swap_rate, mass on incorrect actions uniformly
            incorrect = np.ones(nA)
            if np.argmax(p_wm_ideal) < nA:
                incorrect[int(np.argmax(p_wm_ideal))] = 0.0
            incorrect_sum = np.sum(incorrect)
            if incorrect_sum > 0:
                p_incorrect = incorrect / incorrect_sum
            else:
                p_incorrect = np.ones(nA) / nA

            p_wm_vec = (1.0 - swap_rate) * p_wm_ideal + swap_rate * p_incorrect
            p_wm_vec = np.maximum(p_wm_vec, 1e-12)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update: on reward, store perfect memory (delta on chosen action); on no-reward, light smoothing
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # gentle smoothing toward uniform to avoid pathological deltas on incorrect action
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p