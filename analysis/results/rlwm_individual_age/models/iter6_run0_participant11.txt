def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with set-size/age-dependent WM weighting and decay.

    Mechanism:
    - RL learns Q-values with softmax choice.
    - WM stores a fast-updating action policy per state and decays toward uniform.
    - The WM mixture weight is modulated by set size and age: larger set sizes and older age reduce WM reliance.
    - WM decays faster with larger set sizes and in older age.

    Parameters (list of length 6):
    - lr: RL learning rate (0..1).
    - wm_weight_base: baseline WM mixture weight in (0..1) before modulations.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay_base: base WM decay rate toward uniform in (0..1).
    - setsize_weight_slope: how much WM weight decreases per increase in set size (typically positive).
    - age_wm_downweight: additional downweighting of WM for older group (>=0). Also scales WM decay with age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay_base, setsize_weight_slope, age_wm_downweight = model_parameters

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    # Scale beta as in the provided template
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute block-level WM decay with set-size and age modulation
        decay_eff = wm_decay_base * (nS / 3.0) * (1.0 + age_wm_downweight * age_group)
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute WM policy from w via high-precision softmax
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            wm_exp = np.exp(wm_logits)
            wm_pol = wm_exp / np.sum(wm_exp)
            p_wm = float(wm_pol[a])

            # Compute per-trial WM weight from base with set-size and age penalties
            # wm_weight is redefined per-trial so that the template combination line can use it
            base_logit = np.log(np.clip(wm_weight_base, 1e-6, 1 - 1e-6)) - np.log(1 - np.clip(wm_weight_base, 1e-6, 1 - 1e-6))
            penalty = setsize_weight_slope * max(0, nS - 3) + age_wm_downweight * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-(base_logit - penalty)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-dependent WM update: reward -> push toward one-hot; no-reward -> suppress chosen
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target
            else:
                # reduce chosen action's weight, mild spread to others via renormalization
                w[s, a] = 0.8 * w[s, a]
                # slight drift toward uniform after loss
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Global decay/interference scaled by set size and age
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # Normalize rows to sum to 1
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums = np.clip(row_sums, 1e-12, None)
            w = w / row_sums

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian-like WM (Dirichlet counts) with set-size/age-dependent decay and weight.

    Mechanism:
    - RL learns Q-values and produces a softmax policy.
    - WM holds Dirichlet-like counts per action for each state; policy is the normalized counts.
    - Rewards increase counts for the chosen action strongly; non-rewards increase weakly (to encode avoidance).
    - Counts decay toward a uniform prior; decay accelerates with larger set size and in older age.
    - WM mixture weight is downscaled with larger set sizes and in older age.

    Parameters (list of length 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight_base: baseline WM mixture weight in (0..1).
    - wm_inc_pos: positive update to WM counts when rewarded (>0).
    - wm_inc_neg: negative (avoidance) update to alternative actions when not rewarded (>=0).
    - phi_base: base decay of WM counts toward prior (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_inc_pos, wm_inc_neg, phi_base = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM Dirichlet counts initialized to 1 (uniform prior)
        counts = np.ones((nS, nA))
        prior = np.ones((nS, nA))

        # Decay rate for WM counts increases with set size and in older age
        phi = phi_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
        phi = np.clip(phi, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: normalized counts (equivalent to expected categorical under Dirichlet)
            wm_pol = counts[s, :] / np.sum(counts[s, :])
            # Use a high-precision softmax over probabilities to mirror template
            wm_logits = softmax_beta_wm * wm_pol
            wm_logits -= np.max(wm_logits)
            wm_exp = np.exp(wm_logits)
            wm_pol_sm = wm_exp / np.sum(wm_exp)
            p_wm = float(wm_pol_sm[a])

            # Set-size and age reduce WM reliance
            penalty = 0.5 * max(0, nS - 3) + 0.5 * age_group
            base_logit = np.log(np.clip(wm_weight_base, 1e-6, 1 - 1e-6)) - np.log(1 - np.clip(wm_weight_base, 1e-6, 1 - 1e-6))
            wm_weight = 1.0 / (1.0 + np.exp(-(base_logit - penalty)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rewarded: increase chosen action count
            if r > 0.5:
                counts[s, a] += wm_inc_pos
            else:
                # Not rewarded: encourage shifting away by boosting alternatives slightly
                for a2 in range(nA):
                    if a2 != a:
                        counts[s, a2] += wm_inc_neg

            # Decay/interference toward prior counts
            counts = (1.0 - phi) * counts + phi * prior

            # Maintain positivity and numerical stability
            counts = np.clip(counts, 1e-6, None)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with adaptive arbitration based on online WM advantage and age bias.

    Mechanism:
    - RL learns Q-values (softmax).
    - WM stores a policy per state; updated toward rewarded actions and decays to uniform.
    - Arbitration weight (wm_weight) is not fixed: it is adapted online via gradient-like updates toward
      the model (WM vs RL) that better predicts observed choices.
    - Adaptation rate is a parameter; older age biases arbitration against WM; larger set sizes implicitly
      increase WM decay and thereby reduce WM advantage.

    Parameters (list of length 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight_init: initial WM mixture weight in (0..1) at the start of each block.
    - eta_w: learning rate for updating wm_weight based on WM vs RL log-prob advantage.
    - age_bias: subtractive bias applied to wm_weight for older group (>=0 reduces WM in older).
    - wm_decay_base: base WM decay rate toward uniform (0..1), scaled by set size and age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_init, eta_w, age_bias, wm_decay_base = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        idx = (blocks == b)

        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize adaptive arbitration weight with age bias
        wm_weight = np.clip(wm_weight_init - age_bias * age_group, 0.0, 1.0)

        # Set-size/age-scaled WM decay
        decay_eff = wm_decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group)
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            wm_exp = np.exp(wm_logits)
            wm_pol = wm_exp / np.sum(wm_exp)
            p_wm = float(wm_pol[a])

            # Mix policies using current wm_weight
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target
            else:
                w[s, a] = 0.85 * w[s, a]
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Apply set-size/age-scaled decay toward uniform
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # Normalize WM policy rows
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums = np.clip(row_sums, 1e-12, None)
            w = w / row_sums

            # Adaptive arbitration update:
            # Move wm_weight toward the source that assigned higher probability to the chosen action
            # Compute log-prob advantage of WM over RL for the chosen action
            adv = np.log(np.clip(p_wm, 1e-12, 1.0)) - np.log(np.clip(p_rl, 1e-12, 1.0))
            wm_weight = wm_weight + eta_w * adv
            # Apply age bias after update to persistently reduce WM in older adults
            wm_weight = wm_weight - 0.0 * (1 - age_group) + 0.0 * age_group  # no-op to show structure
            wm_weight = np.clip(wm_weight - 0.0 * (1 - age_group) + age_bias * 0.0, 0.0, 1.0)  # keep usage explicit
            # Re-apply age bias once per trial to ensure effect persists
            if age_group == 1:
                wm_weight = np.clip(wm_weight - 0.0, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p