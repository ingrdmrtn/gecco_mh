def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(λ) with decaying working memory subject to set-size interference.

    Mechanism
    - RL: Q-learning with an eligibility trace λ to propagate credit over time within a block.
      λ is age-modulated: young participants have higher λ than older participants.
    - WM: one-shot cache per state that stores the last rewarded action and decays toward uniform.
      WM weight decreases with larger set sizes due to interference.

    Parameters (6)
    - model_parameters:
        0) lr: RL learning rate (0..1)
        1) beta_base: base inverse temperature for RL choice (scaled by 10 internally)
        2) wm_weight_base: base mixture weight on WM policy (0..1 before transformation)
        3) wm_decay_base: base WM decay rate per trial towards uniform (0..1)
        4) size_interference_gain: scales how much larger set sizes increase WM decay and reduce WM weight (>=0)
        5) age_lambda_shift: age effect on eligibility trace λ; λ = sigmoid(age_lambda_shift + age_sign), where age_sign=+0.5 for young, -0.5 for old

    Inputs
    - states, actions, rewards: arrays of same length
    - blocks: block index per trial
    - set_sizes: set size per trial (constant within a block)
    - age: array with a single repeated value
    Returns
    - Negative log-likelihood of observed choices under the mixture policy
    """
    lr, beta_base, wm_weight_base, wm_decay_base, size_interference_gain, age_lambda_shift = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    # Age-modulated eligibility trace: young get a small positive shift, old a negative shift
    age_sign = 0.5 if age_group == 0 else -0.5
    lam = 1.0 / (1.0 + np.exp(-(age_lambda_shift + age_sign)))  # sigmoid

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        # eligibility traces per state-action
        e = np.zeros((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM parameters under set-size interference
        size_factor = max(0, nS - 3)
        wm_decay_eff = np.clip(wm_decay_base + size_interference_gain * size_factor, 0.0, 1.0)

        # Reduce WM weight with set size due to interference
        wm_weight_eff = 1.0 / (1.0 + np.exp(-(np.clip(wm_weight_base, -6, 6) - size_interference_gain * size_factor)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (soft with high precision)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(50.0 * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            # increment trace for chosen state-action
            e *= lam
            e[s, a] = 1.0  # replacing traces for chosen pair
            delta = r - Q_s[a]
            q += lr * delta * e  # update all via traces

            # WM decay towards uniform
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * (1.0 / nA)
            # If rewarded, cache the correct action deterministically at that state
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + entropy-gated WM usage with set-size and age modulation.

    Mechanism
    - RL: standard Q-learning.
    - WM: one-shot per-state cache on rewarded trials; no explicit decay parameter here.
    - Mixture weight is adaptive: greater reliance on WM when WM is confident (low entropy)
      and RL is uncertain (high entropy). Set size reduces WM usage; age shifts baseline WM usage.

    Parameters (6)
    - model_parameters:
        0) lr: RL learning rate (0..1)
        1) beta_base: base inverse temperature for RL (scaled by 10 internally)
        2) wm_weight_base: baseline WM mixture weight before adjustments (0..1 range after sigmoid)
        3) entropy_gain: how strongly entropy difference (H_rl - H_wm) increases WM reliance
        4) size_wm_drop: magnitude by which larger set sizes reduce WM reliance (>=0)
        5) age_wm_shift: additive shift to WM weight for older group (applied pre-sigmoid; young=0)

    Inputs
    - states, actions, rewards: arrays
    - blocks: block index
    - set_sizes: set size per trial
    - age: array with repeated value
    Returns
    - Negative log-likelihood
    """
    lr, beta_base, wm_weight_base, entropy_gain, size_wm_drop, age_wm_shift = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Base WM weight logit with set-size and age adjustments
        base_logit = np.clip(wm_weight_base, -6, 6)
        size_penalty = size_wm_drop * max(0, nS - 3)
        age_shift = (age_wm_shift if age_group == 1 else 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL choice prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(50.0 * (W_s - W_s[a])))

            # Entropy of RL and WM for this state
            # Compute normalized probabilities for full action set
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_probs / np.sum(rl_probs)
            wm_probs = np.exp(50.0 * (W_s - np.max(W_s)))
            wm_probs = wm_probs / np.sum(wm_probs)

            H_rl = -np.sum(np.clip(rl_probs, 1e-12, 1.0) * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            H_wm = -np.sum(np.clip(wm_probs, 1e-12, 1.0) * np.log(np.clip(wm_probs, 1e-12, 1.0)))

            # Higher (H_rl - H_wm) => rely more on WM
            logit_w = base_logit - size_penalty + age_shift + entropy_gain * (H_rl - H_wm)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit_w))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot store on reward, otherwise leave as is (imperfect persistence)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recency-based WM (response cache) with set-size penalty and age-modulated WM reliance.

    Mechanism
    - RL: standard Q-learning for values.
    - WM: stores the most recent action taken in each state (regardless of reward), implementing a recency bias
      akin to win/lose-stay. The recency learning rate is reduced with larger set sizes.
    - Mixture: WM vs RL mixture weight is baseline plus age shift (older rely less/more depending on sign).

    Parameters (6)
    - model_parameters:
        0) lr: RL learning rate (0..1)
        1) beta_base: base inverse temperature for RL (scaled by 10 internally)
        2) wm_weight_base: baseline WM mixture weight (pre-sigmoid)
        3) recency_rate_base: base WM recency learning rate (0..1), controls how quickly WM tracks last action
        4) size_recency_penalty: reduces recency_rate as set size increases (>=0)
        5) age_wm_weight_shift: additive shift to WM mixture logit for older group (young=0)

    Inputs
    - states, actions, rewards, blocks, set_sizes, age
    Returns
    - Negative log-likelihood
    """
    lr, beta_base, wm_weight_base, recency_rate_base, size_recency_penalty, age_wm_weight_shift = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM recency cache initialized uniform
        w = (1.0 / nA) * np.ones((nS, nA))

        # Set-size adjusted recency rate
        recency_rate = np.clip(recency_rate_base / (1.0 + size_recency_penalty * max(0, nS - 3)), 0.0, 1.0)

        # Age-modulated WM weight
        wm_logit_base = np.clip(wm_weight_base, -6, 6) + (age_wm_weight_shift if age_group == 1 else 0.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(50.0 * (W_s - W_s[a])))

            wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_logit_base))
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM recency update: move distribution toward the last chosen action
            w[s, :] = (1.0 - recency_rate) * w[s, :]
            w[s, a] += recency_rate
            # Normalize to maintain a proper distribution
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p