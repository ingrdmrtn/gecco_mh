def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM precision mixture.

    Idea:
    - Action selection is a mixture of an RL softmax policy and a WM softmax policy.
    - WM precision decreases with set size (higher load) and with age (older â†’ lower precision).
    - WM is subject to decay toward uniform each trial; rewarded outcomes imprint a one-shot memory trace.
    - RL is standard delta-rule.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: mixture weight for WM vs RL in [0,1]
    - softmax_beta: RL inverse temperature; scaled by 10 internally
    - wm_beta_base: base WM inverse temperature that is scaled by load and age (>=0)
    - wm_forget: WM decay rate toward uniform per trial in [0,1]
    - age_wm_drop: scales the reduction in WM precision for the older group (>=0)
    
    Age:
    - age_group = 0 if age <= 45 (younger), else 1 (older)
    - WM precision is reduced by a factor (1 - age_wm_drop * age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_beta_base, wm_forget, age_wm_drop = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective WM precision for this block based on set size and age
        load_scale = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
        wm_beta_eff_block = wm_beta_base * load_scale * (1.0 - age_wm_drop * age_group)
        wm_beta_eff_block = max(wm_beta_eff_block, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over W with precision reduced by load and age
            if wm_beta_eff_block <= 1e-8:
                # If WM precision effectively zero, it becomes uniform
                p_wm = 1.0 / nA
            else:
                denom_wm = np.sum(np.exp(wm_beta_eff_block * (W_s - W_s[a])))
                p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global WM decay toward uniform (forgetting)
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Reward-dependent imprint: a one-shot "slot-like" update scaled by precision
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Convert precision to an effective learning rate in WM (bounded)
                alpha_wm = 1.0 - np.exp(-wm_beta_eff_block / 25.0)
                alpha_wm = np.clip(alpha_wm, 0.0, 1.0)
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic WM encoding with load- and age-driven interference.

    Idea:
    - Policy is a mixture of RL softmax and WM softmax.
    - WM encoding on reward occurs probabilistically with probability p_enc that decreases with load and age.
    - WM experiences global interference that increases with load and age, pushing it toward uniform.
    - RL is a standard delta-rule.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: mixture weight for WM in [0,1]
    - softmax_beta: RL inverse temperature; scaled by 10 internally
    - p_encode_base: base probability of encoding into WM after reward in [0,1]
    - interf_rate: base interference/decay rate toward uniform per trial in [0,1]
    - age_interf_gain: multiplicative increase in interference for older adults (>=0)

    Age and load effects:
    - p_enc = p_encode_base * (3/nS) * (1 - 0.5*age_group)
    - interference = interf_rate * (nS/3) * (1 + age_interf_gain*age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, p_encode_base, interf_rate, age_interf_gain = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load- and age-dependent WM control parameters
        p_enc = p_encode_base * (3.0 / float(nS)) * (1.0 - 0.5 * age_group)
        p_enc = np.clip(p_enc, 0.0, 1.0)
        interference = interf_rate * (float(nS) / 3.0) * (1.0 + age_interf_gain * age_group)
        interference = np.clip(interference, 0.0, 1.0)

        # Use a confidence-tuned WM softmax precision proportional to encoding probability
        wm_beta_eff = 50.0 * p_enc

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            if wm_beta_eff <= 1e-8:
                p_wm = 1.0 / nA
            else:
                denom_wm = np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))
                p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global interference toward uniform
            w[s, :] = (1.0 - interference) * w[s, :] + interference * w_0[s, :]

            # Probabilistic encoding upon reward
            if r > 0.5 and np.random.rand() < p_enc:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong update when encoding occurs
                alpha_wm = 0.9
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with WM lapse and refresh dynamics.

    Idea:
    - Policy is a mixture of RL softmax and WM softmax.
    - WM policy is subject to a WM-specific lapse (mixed with uniform) that increases with load and age.
    - WM representation is refreshed each visit with a load-dependent rate; reward strengthens the trace.
    - RL is standard delta-rule.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: mixture weight for WM in [0,1]
    - softmax_beta: RL inverse temperature; scaled by 10 internally
    - wm_precision: base WM softmax precision (>=0)
    - refresh: base WM refresh rate in [0,1]
    - lapse_slope: base slope for WM-specific lapse that scales with load and age (>=0)

    Age and load effects:
    - WM lapse per trial: lapse_wm = clip(lapse_slope * (nS/3) * (1 + 0.5*age_group), 0, 0.5)
    - WM refresh per visit: refresh_eff = clip(refresh * (3/nS), 0, 1)
    - WM precision does not change across set sizes here, but lapse does; older adults have larger lapse.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_precision, refresh, lapse_slope = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load/age dependent WM lapse and refresh
        lapse_wm = lapse_slope * (float(nS) / 3.0) * (1.0 + 0.5 * age_group)
        lapse_wm = np.clip(lapse_wm, 0.0, 0.5)
        refresh_eff = refresh * (3.0 / float(nS))
        refresh_eff = np.clip(refresh_eff, 0.0, 1.0)

        wm_beta_eff = max(wm_precision, 0.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Base WM softmax
            if wm_beta_eff <= 1e-8:
                p_wm_soft = 1.0 / nA
            else:
                denom_wm = np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))
                p_wm_soft = 1.0 / max(denom_wm, 1e-12)

            # Apply WM-specific lapse (within the WM channel)
            p_wm = (1.0 - lapse_wm) * p_wm_soft + lapse_wm * (1.0 / nA)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Refresh toward a sharper trace on each visit
            # First, a small decay toward uniform
            decay_rate = 0.2 * (float(nS) / 6.0)  # more decay under higher load
            decay_rate = np.clip(decay_rate, 0.0, 1.0)
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            # Then, reward-dependent refresh towards the chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - refresh_eff) * w[s, :] + refresh_eff * one_hot

        blocks_log_p += log_p

    return -blocks_log_p