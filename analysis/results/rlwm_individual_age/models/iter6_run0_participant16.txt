def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with uncertainty-weighted arbitration and load-/age-dependent WM decay.

    Idea:
    - Choices are a mixture of an RL policy and a WM policy.
    - WM is a distribution over actions per state that decays toward uniform; positive feedback sharpens it.
    - WM decay increases with set size and for older adults; RL temperature decreases with load and for older adults.
    - Arbitration weight for WM increases with WM "confidence" (separation between best and second-best WM actions).

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1]; further modulated by confidence, set size, and age.
    - softmax_beta: base inverse temperature for RL; internally scaled by 10.
    - wm_decay_base: base WM decay logit; mapped to (0,1) and upscaled by load/age to control forgetting.
    - wm_gain: WM sharpening gain upon reward, controls how strongly WM moves to the rewarded action.
    - age_beta_drop: fractional drop in RL inverse temperature for older adults (0..1); no effect if young.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_gain, age_beta_drop = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL
    softmax_beta_wm = 50.0  # very deterministic WM
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    # helper: logistic
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load and age modulations
        load_factor = 3.0 / nS  # 1.0 for set size 3, 0.5 for set size 6
        beta_rl_eff = softmax_beta * load_factor * (1.0 - age_beta_drop * age_group)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM confidence = margin between best and second-best WM action
            sorted_W = np.sort(W_s)
            best = sorted_W[-1]
            second = sorted_W[-2] if nA > 1 else sorted_W[-1]
            conf = max(0.0, best - second)  # in [0,1]

            # Effective WM weight: base -> scaled by load and age -> boosted by confidence
            wm_base = np.clip(wm_weight, 0.0, 1.0)
            wm_eff = wm_base * load_factor * (1.0 - 0.3 * age_group)
            wm_eff = wm_eff * (0.2 + 0.8 * conf)  # confidence-weighted arbitration

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay increases with load and with age; map base to (0,1) and scale
            base_d = sigmoid(wm_decay_base)  # in (0,1)
            wm_decay = np.clip(base_d * (1.0 / load_factor) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning: reward-driven sharpening toward the chosen action
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                gain = np.clip(wm_gain * (0.5 + 0.5 * load_factor), 0.0, 1.0)
                w[s, :] = (1.0 - gain) * w[s, :] + gain * one_hot
            else:
                # Mild suppression of the chosen action on errors to encourage correction
                err_push = 0.2 * wm_gain * (1.0 - load_factor)
                if err_push > 0:
                    u = w_0[s, :]
                    w[s, a] = (1.0 - err_push) * w[s, a] + err_push * u[a]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with prediction-error-gated WM encoding.

    Idea:
    - RL updates continuously.
    - WM encodes the rewarded action for a state only when the positive prediction error (PE) exceeds a threshold.
    - The gating threshold increases with set size and for older adults, making WM encoding harder under high load and aging.
    - Arbitration weight for WM depends on base weight and instantaneous WM strength for the queried state.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1].
    - softmax_beta: base inverse temperature for RL; internally scaled by 10.
    - pe_gate_thresh: positive PE threshold for WM encoding (0..1 typical).
    - wm_learn: WM learning rate toward one-hot when gate is open (0..1).
    - age_gate_shift: additive increase to the gating threshold for older adults (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, pe_gate_thresh, wm_learn, age_gate_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM strength for arbitration: margin above uniform
            strength = max(0.0, np.max(W_s) - 1.0 / nA)
            wm_eff = np.clip(wm_weight, 0.0, 1.0) * np.clip(strength * (3.0 / nS), 0.0, 1.0)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM gated encoding by positive PE
            load_shift = 0.2 * ((nS - 3.0) / 3.0)  # ~0 for 3, ~0.2 for 6
            gate_thresh_eff = pe_gate_thresh + age_gate_shift * age_group + load_shift

            # Passive WM decay towards uniform (weak, but stronger under higher load)
            wm_decay = np.clip(0.05 + 0.15 * ((nS - 3.0) / 3.0) + 0.05 * age_group, 0.0, 0.5)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if (delta > gate_thresh_eff) and (r > 0):
                # Encode rewarded action into WM
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot
            # else: no WM strengthening; only decay applies

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration + capacity-limited WM cache.

    Idea:
    - RL selects actions via softmax over Q plus a directed exploration bonus for less-tried actions.
    - WM is a finite-capacity cache of stateâ†’action mappings for rewarded states.
    - WM capacity shrinks with age and relative set size; if full, it evicts the least-recently-rewarded state.
    - Arbitration gives higher weight to WM when the queried state is in the cache and load is low.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1].
    - softmax_beta: base RL inverse temperature; internally scaled by 10.
    - cap_base: base WM capacity (in number of states, can be fractional; will be rounded and bounded).
    - age_cap_drop: fractional capacity drop for older adults (0..1).
    - explore_bonus: directed exploration bonus magnitude added to less-visited actions (>0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, cap_base, age_cap_drop, explore_bonus = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state action visit counts for directed exploration
        visits = np.zeros((nS, nA))

        # Capacity for this block
        cap_eff = cap_base * (1.0 - age_cap_drop * age_group)
        cap_eff = int(np.clip(np.round(cap_eff), 0, nS))

        # Cache bookkeeping: boolean mask of which states are in WM and LRU order
        in_cache = np.zeros(nS, dtype=bool)
        lru_order = []  # list of state indices; most recent at the end

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # Directed exploration bonus: higher for less-visited actions
            bonus = explore_bonus / np.sqrt(np.maximum(visits[s, :] + 1.0, 1.0))
            logits_rl = softmax_beta * Q_s + bonus

            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: if in cache, use stored distribution; otherwise uniform
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: stronger WM weight if state is cached; scaled by load and age
            wm_eff = np.clip(wm_weight, 0.0, 1.0) * (1.0 if in_cache[s] else 0.2)
            wm_eff *= (3.0 / nS) * (1.0 - 0.3 * age_group)

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL updates
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            visits[s, a] += 1.0

            # WM maintenance: mild forgetting when not in cache or under high load
            wm_decay = 0.05 + 0.15 * ((nS - 3.0) / 3.0) + 0.05 * age_group
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM cache update on reward
            if r > 0:
                # Promote to cache
                if not in_cache[s]:
                    # If full, evict least-recently-rewarded state
                    if cap_eff > 0 and np.sum(in_cache) >= cap_eff:
                        evict_state = lru_order.pop(0)
                        in_cache[evict_state] = False
                        w[evict_state, :] = w_0[evict_state, :]  # reset evicted memory
                    if cap_eff > 0:
                        in_cache[s] = True
                        lru_order.append(s)
                else:
                    # Update recency (move to end)
                    if s in lru_order:
                        lru_order.remove(s)
                    lru_order.append(s)

                # Store deterministic mapping for the rewarded action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot  # blend to avoid numerical issues

        blocks_log_p += log_p

    return -blocks_log_p