def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with capacity-limited encoding gate and eligibility traces; age modulates WM gating.

    Mechanism:
    - RL: tabular Q-learning with an eligibility trace (lambda) within each state to speed up reverberation
      after rewarded actions.
    - WM: maintains a cached action distribution per state. Encoding into WM happens stochastically via a gate
      that depends on set size (harder for 6) and age (younger advantage). When encoded after reward,
      the WM for that state becomes a near-delta on the rewarded action; otherwise it stays as is.
    - Arbitration: mixture of RL and WM policies.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_weight_base, wm_gate_bias, trace_lambda, age_gate_mult]
      lr: RL learning rate in [0,1]
      softmax_beta: RL inverse temperature; internally scaled by 10
      wm_weight_base: baseline WM mixture weight in [0,1]
      wm_gate_bias: bias term for WM encoding gate (logit space)
      trace_lambda: eligibility trace parameter in [0,1] that boosts the chosen action value toward its recent outcome
      age_gate_mult: age modulation of WM gate; effective gate logit += age_gate_mult*(1 - 2*age_group)
                     (younger=0 -> positive boost, older=1 -> negative if mult>0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_gate_bias, trace_lambda, age_gate_mult = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0  # deterministic WM when item present
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight is constant across trials within block
        wm_weight_eff = float(np.clip(wm_weight_base, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with eligibility trace focused on chosen state-action
            delta = r - Q_s[a]
            e *= trace_lambda  # decay traces
            e[s, :] *= 0.0
            e[s, a] = 1.0
            q += lr * delta * e

            # WM: stochastically encode only after reward using a gate influenced by set size and age
            # Gate probability in logit space
            size_term = -1.0 if nS == 6 else 0.0  # harder gate when set size is 6
            age_term = age_gate_mult * (1 - 2 * age_group)
            gate_logit = wm_gate_bias + size_term + age_term
            gate_prob = 1.0 / (1.0 + np.exp(-gate_logit))

            if r > 0 and np.random.rand() < gate_prob:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # mild relaxation toward uniform if no encoding occurred
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size- and age-modulated temperature + WM with graded precision and partial overwriting.

    Mechanism:
    - RL: tabular Q-learning; softmax temperature decreases with set size (noisier at 6) and
      is scaled by age (younger -> higher effective beta if age_temp_scale > 0).
    - WM: distribution over actions per state updated by partial overwrite upon reward; precision (beta)
      of the WM policy decreases with set size and is age-modulated.
    - Arbitration: mixture of RL and WM policies with a fixed base weight.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_weight_base, wm_precision_base, size_noise, age_temp_scale]
      lr: RL learning rate in [0,1]
      softmax_beta: base RL inverse temperature (scaled by 10 internally)
      wm_weight_base: mixture weight for WM in [0,1]
      wm_precision_base: base WM precision (multiplies a large constant to approach deterministic)
      size_noise: scales the reduction in both RL beta and WM precision at set size 6 (>=0)
      age_temp_scale: multiplicative modulation of both RL beta and WM precision:
                      factor = 1 + age_temp_scale*(1 - 2*age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_precision_base, size_noise, age_temp_scale = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    # Age scaling factor: young (0) -> 1 + age_temp_scale; old (1) -> 1 - age_temp_scale
    age_scale = 1.0 + age_temp_scale * (1 - 2 * age_group)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling for precision: lower precision when set size is 6
        size_scale = 1.0 / (1.0 + max(size_noise, 0.0) * (1 if nS == 6 else 0))

        # Effective inverse temperatures
        beta_rl = softmax_beta * 10.0 * age_scale * size_scale
        beta_wm = max(wm_precision_base, 0.0) * 50.0 * age_scale * size_scale

        # Partial overwrite coefficient phi for WM (depend on same size_scale)
        phi = np.clip(0.7 * size_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            wm_weight_eff = float(np.clip(wm_weight_base, 0.0, 1.0))
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: partial overwrite on reward, mild forgetting otherwise
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - phi) * w[s, :] + phi * onehot
            else:
                # gentle relaxation toward uniform
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with choice stickiness and set-size-dependent WM crosstalk; age shifts WM arbitration.

    Mechanism:
    - RL: tabular Q-learning with softmax and a choice-stickiness term that biases repeating the last action
      taken in the same state.
    - WM: state-specific action memory with the same stickiness bias in policy evaluation.
      WM mixture weight is reduced by crosstalk at set size 6 and shifted by age.
    - Arbitration: mixture of biased RL and biased WM policies.

    Parameters (6):
    - model_parameters = [lr, softmax_beta, wm_weight_base, stickiness, crosstalk, age_wm_shift]
      lr: RL learning rate in [0,1]
      softmax_beta: base RL inverse temperature (scaled by 10 internally)
      wm_weight_base: baseline WM mixture weight in [0,1]
      stickiness: bias added to the last chosen action in softmax (applied to both RL and WM value vectors)
      crosstalk: reduction factor applied to WM weight when set size is 6 (wm_weight /= (1 + crosstalk))
      age_wm_shift: additive shift to WM weight by age: wm_weight += age_wm_shift*(1 - 2*age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, stickiness, crosstalk, age_wm_shift = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta_eff = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -np.ones(nS, dtype=int)

        # Effective WM weight per block with set-size crosstalk and age shift
        wm_w = wm_weight_base
        if nS == 6:
            wm_w = wm_w / (1.0 + max(crosstalk, 0.0))
        wm_w = wm_w + age_wm_shift * (1 - 2 * age_group)
        wm_w = float(np.clip(wm_w, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with stickiness
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy with stickiness on WM values
            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                W_s[last_action[s]] += stickiness
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: reward-locked caching, otherwise decay toward uniform
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p