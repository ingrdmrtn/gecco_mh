def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + reward-gated WM with decay modulated by set size and age.

    Mechanism
    - RL: standard Q-learning with softmax action selection.
    - WM: state-specific action distribution that becomes more peaked on rewarded actions
          and relaxes toward uniform otherwise. WM decays toward uniform at a rate that
          increases with set size and for older participants.
    - Arbitration: mixture of WM and RL policies. The WM weight is the product of a
          base weight parameter and a trial-wise memory strength for the current state.

    Parameters
    ----------
    states : array-like of int
        State indices for each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions (0..2).
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the current block.
    age : array-like or scalar
        Participant age; <=45 -> age_group=0, >45 -> age_group=1.
    model_parameters : list or array-like
        [lr_raw, beta_raw, wm_weight_raw, decay_base_raw, age_decay, setsize_decay]
        - lr_raw: RL learning rate (logistic-bounded to 0..1).
        - beta_raw: RL inverse temperature base, scaled by 10.
        - wm_weight_raw: base arbitration weight for WM (logistic 0..1).
        - decay_base_raw: base WM adjustment rate toward target (logistic 0..1).
        - age_decay: increases WM decay for older group (>=0).
        - setsize_decay: increases WM decay as set size grows (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_raw, beta_raw, wm_weight_raw, decay_base_raw, age_decay, setsize_decay = model_parameters

    lr = 1.0 / (1.0 + np.exp(-lr_raw))
    beta = max(1e-6, beta_raw * 10.0)
    wm_weight_base = 1.0 / (1.0 + np.exp(-wm_weight_raw))
    decay_base = 1.0 / (1.0 + np.exp(-decay_base_raw))

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nA = 3
        nS = int(block_set_sizes[0])

        setsize_level = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        # Effective WM adjustment rate increases (more decay/learning) with difficulty and age
        decay_eff = decay_base
        # Push toward more relaxation on errors and faster peaking on rewards under easier conditions
        # Convert penalties into logistic shift
        logit_decay = np.log(decay_base + eps) - np.log(1.0 - decay_base + eps) \
                      - (age_decay * age_group + setsize_decay * setsize_level)
        decay_eff = 1.0 / (1.0 + np.exp(-logit_decay))
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Memory strength for arbitration: how peaked WM is above uniform
            peak = np.max(W_s)
            uniform = 1.0 / nA
            mem_strength = (peak - uniform) / (1.0 - uniform)
            mem_strength = np.clip(mem_strength, 0.0, 1.0)

            wm_weight_t = wm_weight_base * mem_strength
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: rewarded trials sharpen to chosen action, else relax toward uniform
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * onehot
            else:
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + Dirichlet-like WM counts with interference; arbitration by WM strength.

    Mechanism
    - RL: Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: per-state Dirichlet counts (starting from 1) that increase on rewarded choices and
          decay toward the prior due to interference. Interference increases with set size and age.
          WM policy uses the normalized counts distribution.
    - Arbitration: WM weight increases with the state's WM strength (total counts above prior),
          while age and set size reduce the effective WM gain (precision).

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [alpha_pos_raw, alpha_neg_raw, beta_rl_raw, wm_gain_raw, age_wm_penalty, setsize_interference]
        - alpha_pos_raw: RL learning rate for positive PE (logistic 0..1).
        - alpha_neg_raw: RL learning rate for negative PE (logistic 0..1).
        - beta_rl_raw: RL inverse temperature (scaled by *10).
        - wm_gain_raw: base WM gain added to chosen action on reward (logistic 0..1, also scales arbitration).
        - age_wm_penalty: reduces effective WM gain in older group and increases interference (>=0).
        - setsize_interference: increases WM interference as set size grows (>=0).

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos_raw, alpha_neg_raw, beta_rl_raw, wm_gain_raw, age_wm_penalty, setsize_interference = model_parameters

    alpha_pos = 1.0 / (1.0 + np.exp(-alpha_pos_raw))
    alpha_neg = 1.0 / (1.0 + np.exp(-alpha_neg_raw))
    beta = max(1e-6, beta_rl_raw * 10.0)
    wm_gain_base = 1.0 / (1.0 + np.exp(-wm_gain_raw))

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nA = 3
        nS = int(block_set_sizes[0])

        setsize_level = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Effective WM gain is reduced by age penalty
        wm_gain_eff = 1.0 / (1.0 + np.exp(-(np.log(wm_gain_base + eps) - np.log(1 - wm_gain_base + eps) - age_wm_penalty * age_group)))
        wm_gain_eff = np.clip(wm_gain_eff, 0.0, 1.0)

        # Interference rate toward prior increases with set size and age
        inter_rate = 1.0 / (1.0 + np.exp(-( - (setsize_interference * setsize_level + age_wm_penalty * age_group))))
        inter_rate = np.clip(inter_rate, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        # Dirichlet-like counts start at 1 (uninformative symmetric prior)
        counts = np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            # Compute WM probabilities from counts
            C_s = counts[s, :]
            W_s = C_s / np.sum(C_s)

            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM strength: total mass above prior baseline (nA) normalized
            total_mass = np.sum(C_s)
            strength = (total_mass - nA) / (nA * 10.0 + eps)  # heuristic normalization (cap by ~10 updates)
            strength = np.clip(strength, 0.0, 1.0)

            wm_weight_t = np.clip(wm_gain_eff * strength, 0.0, 1.0)
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM counts interference toward prior (1's)
            counts[s, :] = (1.0 - inter_rate) * counts[s, :] + inter_rate * 1.0

            # Reward-gated increment to chosen action
            if r == 1:
                counts[s, a] += max(eps, wm_gain_eff)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + cached WM with recency-based certainty and entropy-based arbitration.

    Mechanism
    - RL: Q-learning with softmax policy.
    - WM: a cache storing the most recently rewarded action per state with a confidence m[s] in [0,1].
          Confidence decays each trial; decay is stronger with larger set size and in the older group.
          The WM action distribution is a mixture of the cached one-hot and uniform weighted by m[s].
    - Arbitration: WM weight depends on both WM confidence and RL uncertainty (entropy of Q):
          higher WM confidence and higher RL entropy shift control toward WM.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like or scalar
    model_parameters : list or array-like
        [lr_raw, beta_raw, wm_cache_raw, gamma_arbit_raw, age_recency_penalty, setsize_recency_penalty]
        - lr_raw: RL learning rate (logistic 0..1).
        - beta_raw: RL inverse temperature (scaled by *10).
        - wm_cache_raw: base WM retention (logistic 0..1) and also sets max cache contribution.
        - gamma_arbit_raw: sensitivity of arbitration to (WM confidence - RL certainty) (logistic 0..1 scaled).
        - age_recency_penalty: increases WM decay for older group (>=0).
        - setsize_recency_penalty: increases WM decay as set size grows (>=0).

    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr_raw, beta_raw, wm_cache_raw, gamma_arbit_raw, age_recency_penalty, setsize_recency_penalty = model_parameters

    lr = 1.0 / (1.0 + np.exp(-lr_raw))
    beta = max(1e-6, beta_raw * 10.0)
    cache_ret_base = 1.0 / (1.0 + np.exp(-wm_cache_raw))  # retention per trial (0..1)
    gamma_base = 1.0 / (1.0 + np.exp(-gamma_arbit_raw))   # 0..1
    gamma = 5.0 * gamma_base  # scale arbitration sensitivity

    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nA = 3
        nS = int(block_set_sizes[0])

        setsize_level = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Effective per-trial retention after penalties
        logit_ret = np.log(cache_ret_base + eps) - np.log(1.0 - cache_ret_base + eps) \
                    - (age_recency_penalty * age_group + setsize_recency_penalty * setsize_level)
        cache_ret = 1.0 / (1.0 + np.exp(-logit_ret))
        cache_ret = np.clip(cache_ret, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM cache: last rewarded action per state and its confidence
        cached_action = -np.ones(nS, dtype=int)
        m_conf = np.zeros(nS)  # confidence 0..1
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Decay all state confidences each trial
            m_conf = m_conf * cache_ret

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # Construct WM distribution for current state
            if cached_action[s] >= 0:
                onehot = np.zeros(nA)
                onehot[cached_action[s]] = 1.0
                W_s = (1.0 - m_conf[s]) * w_0[s, :] + m_conf[s] * onehot
            else:
                W_s = w_0[s, :]

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL uncertainty via normalized entropy
            P_rl = np.exp(beta * Q_s) / np.sum(np.exp(beta * Q_s))
            H_rl = -np.sum(P_rl * np.log(P_rl + eps))
            H_norm = H_rl / np.log(nA)  # 0..1

            # Arbitration: rely more on WM when its confidence is high and RL is uncertain
            signal = m_conf[s] - (1.0 - H_norm)
            wm_weight_t = 1.0 / (1.0 + np.exp(-gamma * signal))
            # Cap by base cache strength to ensure WM doesn't exceed its capacity
            wm_weight_t = np.clip(wm_weight_t, 0.0, cache_ret_base)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM cache update: on reward, cache chosen action with full confidence
            if r == 1:
                cached_action[s] = a
                m_conf[s] = 1.0  # reset to full certainty

        blocks_log_p += log_p

    return -blocks_log_p