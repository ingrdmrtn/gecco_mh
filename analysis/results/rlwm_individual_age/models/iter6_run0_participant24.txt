def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and set-size WM decay.

    Core ideas:
    - RL learns Q-values with a single learning rate and standard softmax policy (beta).
    - WM stores rewarded state-action associations as sharp traces, but decays toward uniform faster at larger set sizes.
    - Arbitration is dynamic: WM weight increases when RL is uncertain (high entropy) and decreases with larger set size.
    - Age group modulates WM reliance (young receives a WM bonus).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature, scaled by 10 internally
    - wm_base: baseline WM mixing weight before modulation (0..1)
    - k_setsize: how strongly WM weight decreases with larger set size (>=0)
    - k_uncert: how strongly WM weight increases with RL uncertainty (>=0)
    - age_wm_bonus: additive WM weight bonus for young (age_group=0), can be negative/positive

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen action indices per trial
    - rewards: array of rewards (0/1) per trial
    - blocks: array of block indices per trial
    - set_sizes: array with the block's set size repeated per trial
    - age: array with a single value repeated; age_group=0 if <=45 else 1

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, beta, wm_base, k_setsize, k_uncert, age_wm_bonus = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta *= 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM leak (0 at set size 3, 1 at set size 6)
        setsize_factor = max(nS - 3, 0) / 3.0
        wm_leak = setsize_factor  # no extra parameter: stronger leak at larger set size

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # RL uncertainty via softmax entropy (0..log(nA)); normalize to [0,1]
            entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            entropy_norm = entropy / np.log(nA)

            # Arbitration: WM weight increases with RL uncertainty, decreases with set size, age bonus for young
            wm_weight = wm_base + age_wm_bonus * (1 - age_group) + k_uncert * entropy_norm - k_setsize * setsize_factor
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform, stronger with larger set size
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM update: on reward, store sharp association
            if r > 0.5:
                w[s, :] = w0[s, :]
                w[s, a] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with global perseveration bias and load-sensitive WM strength.

    Core ideas:
    - RL learns Q-values with age-specific inverse temperature (beta_y for young, beta_o for old).
    - Global perseveration bias: tendency to repeat the most recent action (state-independent).
    - WM uses incremental strengthening toward the chosen action when rewarded; WM decays toward uniform with load.
    - Arbitration depends on WM confidence (sharpening of W_s), reduced by set size. No extra parameter for explicit mix.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_y: RL inverse temperature for young, scaled by 10 internally
    - beta_o: RL inverse temperature for old, scaled by 10 internally
    - perseveration: global choice perseveration bias added to last chosen action (in RL logits)
    - wm_eta: WM incremental learning rate on rewarded trials (0..1)
    - load_sensitivity: how strongly set size reduces WM influence and increases WM decay (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, beta_y, beta_o, perseveration, wm_eta, load_sensitivity = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = beta_y if age_group == 0 else beta_o
    beta *= 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        last_action_global = -1  # -1 indicates none yet

        setsize_factor = max(nS - 3, 0) / 3.0
        # WM decay toward uniform increases with load_sensitivity and set size
        wm_leak = np.clip(load_sensitivity * setsize_factor, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with global perseveration bias
            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_action_global >= 0:
                bias[last_action_global] += perseveration
            rl_logits = beta * (Q_s + bias - np.max(Q_s + bias))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # WM confidence: how peaked W_s is (0 if uniform, up to ~1 if sharp)
            wm_conf = np.max(W_s) - (1.0 / nA)
            wm_conf = np.clip(wm_conf / (1.0 - 1.0 / nA), 0.0, 1.0)
            # Arbitration: promote WM with its confidence and demote with set size; scale by wm_eta
            wm_weight = np.clip(0.5 + wm_eta * (wm_conf - load_sensitivity * setsize_factor), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform (load dependent)
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM incremental strengthening on rewarded trials
            if r > 0.5:
                # shrink toward zero then add mass to chosen action
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
                # renormalize to sum to 1 to keep a valid distribution
                w[s, :] = w[s, :] / np.sum(w[s, :])

            last_action_global = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with lapses, RL forgetting, and precision-controlled WM.

    Core ideas:
    - RL learns Q-values and also forgets toward uniform each trial (interference/decay), capturing load-dependent interference.
    - WM stores one-shot associations on reward with high precision; WM precision parameter controls its determinism.
    - Final action is a lapse mixture: with probability lapse choose uniformly at random; otherwise follow RL/WM mixture.
    - Lapse rate increases with set size and has an age-related boost (older -> more lapses).
    - WM/RL mixture weight is confidence-based (WM sharpness), no extra parameter.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_y: RL inverse temperature for young (scaled by 10 internally); old uses same beta but is impacted via lapse
    - lapse_base: base lapse logit (can be negative/positive)
    - age_lapse_boost: additive increase in lapse logit for old vs young (>=0 increases lapses in old)
    - rl_decay: RL forgetting rate toward uniform baseline per trial (0..1)
    - wm_precision: scales WM inverse temperature between 10 and 50

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, beta_y, lapse_base, age_lapse_boost, rl_decay, wm_precision = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = beta_y * 10.0
    # WM precision-controlled beta
    beta_wm = 10.0 + 40.0 * np.clip(wm_precision, 0.0, 1.0)

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        setsize_factor = max(nS - 3, 0) / 3.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting toward uniform each trial
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA) * np.ones_like(q)

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # WM confidence-based mixture (no extra parameter): more peaked -> more weight
            wm_conf = np.max(W_s) - (1.0 / nA)
            wm_conf = np.clip(wm_conf / (1.0 - 1.0 / nA), 0.0, 1.0)
            wm_weight = wm_conf

            # Non-lapse mixture probability
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Lapse probability depends on set size and age
            # Lapse logit = lapse_base + c_set*setsize_factor + age_lapse_boost*age_group
            c_set = 3.0  # fixed scaling to allow lapses to rise notably at larger set size
            lapse_logit = lapse_base + c_set * setsize_factor + age_lapse_boost * age_group
            lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
            lapse = np.clip(lapse, 0.0, 1.0)

            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay mild toward uniform using same rl_decay to limit parameters
            w = (1.0 - rl_decay) * w + rl_decay * w0

            # WM one-shot store on reward
            if r > 0.5:
                w[s, :] = w0[s, :]
                w[s, a] = 1.0

    return nll