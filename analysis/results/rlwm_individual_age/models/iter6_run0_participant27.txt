def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Precision-gated WM vs RL mixture with load- and age-dependent WM precision.

    Mechanism
    - RL: tabular Q-learning with softmax policy.
    - WM: associative table W that decays toward uniform; when rewarded, it overwrites the state row
          toward a one-hot vector for the chosen action (fast one-shot encoding).
    - Arbitration: the mixture weight is a deterministic function of the relative "precision"
          (inverse noise) of WM vs RL. WM precision decreases with set size (load_penalty) and
          increases for young participants (age_precision_bonus). RL precision is beta_rl.
          The WM softmax temperature equals its precision; arbitration weight is a sigmoid over
          (beta_wm - beta_rl).

    Parameters (len=6)
    - lr_rl: RL learning rate (0..1)
    - beta_base: base inverse-temperature for RL; internally scaled by 10
    - wm_precision_base: base precision for WM policy (>0); transformed into beta_wm with load/age
    - load_penalty: decrement to WM precision per additional items beyond 3 (>=0)
    - wm_decay: per-trial decay of WM rows toward uniform (0..1)
    - age_precision_bonus: amount added to WM precision if young; subtracted if old

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_rl, beta_base, wm_precision_base, load_penalty, wm_decay, age_precision_bonus = model_parameters
    beta_rl = beta_base * 10.0
    age_group = 0 if age[0] <= 45 else 1
    # Age effect on WM precision: young +bonus, old -bonus
    age_term = age_precision_bonus if age_group == 0 else -abs(age_precision_bonus)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        W_uniform = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision and arbitrated weight
        load_drop = max(0, nS - 3) * max(0.0, load_penalty)
        beta_wm = max(0.1, 10.0 * (wm_precision_base + age_term - load_drop))

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL policy
            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            p_vec_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = p_vec_rl[a]

            # WM policy
            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_vec_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = p_vec_wm[a]

            # Arbitration weight: sigmoid of precision difference
            diff = beta_wm - beta_rl
            wm_weight = 1.0 / (1.0 + np.exp(-diff))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe

            # WM decay and encoding (rewarded one-shot overwrite)
            W = (1.0 - wm_decay) * W + wm_decay * W_uniform
            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = target

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Delta-rule arbitration with interference-prone WM and asymmetric RL learning rates.

    Mechanism
    - RL: tabular Q-learning with separate learning rates for positive and negative prediction errors.
           Age effect: older adults show larger sensitivity to negative outcomes (age_neg_boost),
           implemented by scaling alpha_neg upward if age_group==1; unchanged if young.
    - WM: deterministic cache that moves state row toward a one-hot vector on reward; high-precision
           retrieval (beta_wm=50). Between trials, WM suffers interference that grows with set size.
    - Arbitration: the mixture weight is updated by a delta-rule toward recent WM "success"
           (whether WM would have predicted the chosen action and the outcome was rewarding).
           Initialization is lower under high load.

    Parameters (len=6)
    - alpha_pos: RL learning rate for positive PEs (0..1)
    - alpha_neg: RL learning rate for negative PEs (0..1); modulated by age
    - beta_rl: RL inverse temperature (>0), internally scaled by 10
    - wm_eta: learning rate that updates the WM mixture weight toward recent WM success (0..1)
    - interference: strength of WM interference per extra item beyond 3 (0..1)
    - age_neg_boost: multiplicative boost applied to alpha_neg if old; no change if young

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_rl, wm_eta, interference, age_neg_boost = model_parameters
    beta_rl = beta_rl * 10.0
    age_group = 0 if age[0] <= 45 else 1
    # Age effect on negative learning rate
    alpha_neg_eff = alpha_neg if age_group == 0 else np.clip(alpha_neg * (1.0 + abs(age_neg_boost)), 0.0, 1.0)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3
    beta_wm = 50.0  # near-deterministic WM retrieval

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        W_uniform = (1.0 / nA) * np.ones((nS, nA))

        # Initialize arbitration weight lower under higher load
        load_level = max(0, nS - 3) / 3.0  # 0 for 3, 1 for 6
        w_mix = max(0.0, 1.0 - 0.7 * load_level)  # start lower for larger set sizes

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL policy
            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            p_vec_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = p_vec_rl[a]

            # WM policy
            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_vec_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = p_vec_wm[a]

            # Mixture
            p_total = w_mix * p_wm + (1.0 - w_mix) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg_eff * pe

            # WM decay toward uniform and reward-based encoding
            # Interference increases with load: blends W toward the global mean within the block
            W_mean = np.mean(W, axis=0, keepdims=True)
            interf_strength = np.clip(interference * load_level, 0.0, 1.0)
            W = (1.0 - interf_strength) * W + interf_strength * W_mean

            # Small uniform drift keeps W bounded
            W = 0.98 * W + 0.02 * W_uniform

            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                # Move state row toward target (strong, cache-like)
                W[s, :] = 0.2 * W[s, :] + 0.8 * target

            # Arbitration delta-rule: move w_mix toward recent "WM success"
            # Define WM success as WM's MAP action equals chosen action and reward was 1
            wm_map = int(np.argmax(W_s))
            wm_success = 1.0 if (wm_map == a and r > 0.5) else 0.0
            w_mix = w_mix + wm_eta * (wm_success - w_mix)
            w_mix = float(np.clip(w_mix, 0.0, 1.0))

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration with load- and age-dependent WM noise and action stickiness.

    Mechanism
    - RL: tabular Q-learning with softmax; we also track a running estimate of RL uncertainty per state
          via an exponential average of absolute prediction errors (u_rl). Higher u_rl => less reliable RL.
    - WM: associative table with softmax policy; WM noise increases with set size (load_sens) and with age
          for older adults (age_noise_shift). For young adults, the same parameter reduces noise.
    - Arbitration: compute reliabilities as inverse uncertainties: rel = 1/(u + c). WM uncertainty is
          the entropy of the WM policy for that state (higher entropy => lower reliability). The mixture
          weight is rel_wm/(rel_wm + rel_rl).
    - Perseveration: action stickiness adds a bonus to the last action taken in the same state,
          applied to both RL and WM logits.

    Parameters (len=6)
    - lr_rl: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (>0), internally scaled by 10
    - wm_noise_base: base WM noise (>=0); higher means lower WM precision
    - load_sens: how much WM noise increases per extra item beyond 3 (>=0)
    - age_noise_shift: magnitude added to WM noise if old; subtracted if young
    - stickiness: action perseveration strength added to the previous action's logit in the same state

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_rl, beta_rl, wm_noise_base, load_sens, age_noise_shift, stickiness = model_parameters
    beta_rl = beta_rl * 10.0
    age_group = 0 if age[0] <= 45 else 1
    age_noise = -abs(age_noise_shift) if age_group == 0 else abs(age_noise_shift)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    tau_u = 0.3  # smoothing for RL uncertainty
    c_rl = 0.05  # small constant for stability in reliability
    c_wm = 0.05

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        W_uniform = (1.0 / nA) * np.ones((nS, nA))

        # RL uncertainty per state
        u_rl = 0.5 * np.ones(nS)

        # Track last action per state for stickiness
        last_act = -1 * np.ones(nS, dtype=int)

        # Compute WM beta from noise terms
        load_term = max(0, nS - 3) * max(0.0, load_sens)
        wm_noise = max(0.0, wm_noise_base + load_term + age_noise)
        beta_wm_scale = 10.0
        beta_wm = beta_wm_scale / max(wm_noise + 1e-6, 1e-6)  # larger noise -> smaller beta

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # Stickiness bonus vector
            stick_vec = np.zeros(nA)
            if last_act[s] >= 0:
                stick_vec[last_act[s]] = stickiness

            # RL policy with stickiness
            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s)) + stick_vec
            exp_rl = np.exp(logits_rl)
            p_vec_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = p_vec_rl[a]

            # WM policy with stickiness
            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s)) + stick_vec
            exp_wm = np.exp(logits_wm)
            p_vec_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = p_vec_wm[a]

            # Compute reliabilities from uncertainties
            # RL uncertainty from running abs PE
            rel_rl = 1.0 / (u_rl[s] + c_rl)
            # WM uncertainty from entropy of WM policy in this state
            p_wm_vec = np.clip(p_vec_wm, eps, 1.0)
            H_wm = -np.sum(p_wm_vec * np.log(p_wm_vec)) / np.log(nA)  # normalized [0,1]
            u_wm = H_wm
            rel_wm = 1.0 / (u_wm + c_wm)

            wm_weight = rel_wm / max(rel_wm + rel_rl, eps)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr_rl * pe

            # Update RL uncertainty (state-specific)
            u_rl[s] = (1.0 - tau_u) * u_rl[s] + tau_u * abs(pe)

            # WM decay and reward-based strengthening
            W = 0.98 * W + 0.02 * W_uniform
            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = 0.3 * W[s, :] + 0.7 * target
            else:
                # weak suppression of the chosen action on non-reward
                W[s, a] = 0.9 * W[s, a] + 0.1 * (1.0 / nA)

            # Update last action for stickiness
            last_act[s] = a

    return -float(total_log_p)