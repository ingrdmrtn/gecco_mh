def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + limited-capacity WM with probabilistic storage and load/age-dependent decay.

    Mechanisms:
    - RL system: Q-learning with softmax.
    - WM system: slot-limited store. Rewarded outcomes are stored as associations with probability
      proportional to available WM slots relative to set size; entries decay toward uniform with load and age.
    - Arbitration: mixture of RL and WM policies; WM influence scales with estimated storage probability.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: baseline weight of WM policy in [0,1]
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wm_slots: approximate number of discrete WM slots (>=0)
    - decay_rate: base WM decay rate in [0,1]
    - age_load_penalty: multiplicative penalty on WM efficacy with age (>=0)

    Age and set-size effects:
    - Storage probability p_store â‰ˆ min(1, wm_slots / nS) decreases with set size, and is further reduced by
      a factor 1 / (1 + age_load_penalty*age_group).
    - WM decay d = decay_rate * ((nS - 3)/3)+ scaled by age (1 + 0.5*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_slots, decay_rate, age_load_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # high precision WM when not decayed

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load- and age-dependent WM parameters
        p_store_base = min(1.0, max(0.0, float(wm_slots) / max(1.0, float(nS))))
        p_store_age = p_store_base / (1.0 + age_load_penalty * age_group)
        p_store_age = min(max(p_store_age, 0.0), 1.0)

        d = decay_rate * max(0.0, (nS - 3.0) / 3.0) * (1.0 + 0.5 * age_group)
        d = min(max(d, 0.0), 1.0)

        # WM mixture weight scaled by estimated storage success probability
        wm_weight_eff = wm_weight * p_store_age
        wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM associative strengths, precision reduced by decay
            W_s = w[s, :]
            beta_wm_eff = softmax_beta_wm * (1.0 - d)
            wm_logits = beta_wm_eff * (W_s - W_s.max())
            p_wm_vec = np.exp(wm_logits)
            p_wm_vec /= (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - d) * w + d * w_0

            # WM update: expected (deterministic) storage toward the correct action if rewarded
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Move the row toward onehot proportional to storage success probability
                eta = p_store_age * (1.0 - d)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with Win-Stay/Lose-Shift bias and load/age-dependent arbitration.

    Mechanisms:
    - RL system: Q-learning with softmax.
    - WM system: stores rewarded state-action associations; decays toward uniform.
    - WSLS bias: within the WM policy, choices are biased to repeat the last rewarded action in a state
      (win-stay) or to avoid the last unsuccessful action (lose-shift).
    - Arbitration: WM vs RL mixture where WM contribution declines with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: baseline WM mixture weight in [0,1]
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wsls_strength: magnitude of WSLS bias added to WM logits (>=0)
    - load_sensitivity: exponent controlling how set size reduces WM weight (>=0)
    - age_bias: multiplicative penalty on WM weight for older group (>=0)

    Age and set-size effects:
    - WM weight = wm_weight * (3/nS)^(load_sensitivity) / (1 + age_bias*age_group).
    - WM decay increases with set size and age: d = 0.2 * ((nS-3)/3)_+ * (1 + 0.5*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wsls_strength, load_sensitivity, age_bias = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action/outcome per state for WSLS bias
        last_act = -1 * np.ones(nS, dtype=int)
        last_rew = -np.ones(nS)

        # Load/age adjusted WM weight and decay
        wm_weight_eff = wm_weight * (3.0 / max(1.0, float(nS))) ** max(0.0, load_sensitivity)
        wm_weight_eff = wm_weight_eff / (1.0 + age_bias * age_group)
        wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

        d = 0.2 * max(0.0, (nS - 3.0) / 3.0) * (1.0 + 0.5 * age_group)
        d = min(max(d, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with WSLS bias
            W_s = w[s, :]
            beta_wm_eff = softmax_beta_wm * (1.0 - d)
            wm_logit = beta_wm_eff * (W_s - W_s.max())

            wsls_bias = np.zeros(nA)
            if last_act[s] >= 0:
                if last_rew[s] > 0.5:
                    # Win-Stay: add bias to repeat last rewarded action
                    wsls_bias[last_act[s]] += wsls_strength
                else:
                    # Lose-Shift: subtract bias from last failed action (equivalently add to others)
                    wsls_bias[last_act[s]] -= wsls_strength / 2.0
                    for aa in range(nA):
                        if aa != last_act[s]:
                            wsls_bias[aa] += wsls_strength / (2.0 * (nA - 1))

            wm_logits_total = wm_logit + wsls_bias
            p_wm_vec = np.exp(wm_logits_total - wm_logits_total.max())
            p_wm_vec /= (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - d) * w + d * w_0

            # WM update: store rewarded action
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                eta = 1.0 - d
                w[s, :] = (1.0 - eta) * w[s, :] + eta * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

            # Update WSLS memory
            last_act[s] = a
            last_rew[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dual learning rates + WM with load/age-dependent forgetting + lapse.

    Mechanisms:
    - RL system: Q-learning with separate learning rates for positive and negative outcomes,
      plus mild RL forgetting proportional to load/age.
    - WM system: associative memory decays toward uniform with load- and age-dependent rate,
      updated toward chosen action on reward.
    - Arbitration: mixture of RL and WM policies.
    - Lapse: small probability of random choice that increases with set size and age.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewards in [0,1]
    - lr_neg: RL learning rate for non-rewards in [0,1]
    - softmax_beta: RL inverse temperature (internally scaled by 10)
    - wm_weight: baseline WM mixture weight in [0,1]
    - wm_forget: base WM forgetting rate in [0,1]
    - lapse_base: base lapse rate in [0,1]

    Age and set-size effects:
    - WM forgetting f = wm_forget * (nS/3) * (1 + 0.5*age_group).
    - RL forgetting f_rl = 0.1 * f.
    - Lapse lambda = min(0.4, lapse_base * (nS/3) * (1 + 0.5*age_group)).
    - WM mixture weight is reduced by load: wm_weight_eff = wm_weight / (1 + 0.5*(nS-3)_+ + 0.5*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_forget, lapse_base = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        f = wm_forget * (nS / 3.0) * (1.0 + 0.5 * age_group)
        f = min(max(f, 0.0), 1.0)
        f_rl = min(1.0, 0.1 * f)

        lapse = min(0.4, lapse_base * (nS / 3.0) * (1.0 + 0.5 * age_group))

        load_term = max(0.0, (nS - 3.0))
        wm_weight_eff = wm_weight / (1.0 + 0.5 * load_term + 0.5 * age_group)
        wm_weight_eff = min(max(wm_weight_eff, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            beta_wm_eff = softmax_beta_wm * (1.0 - f)
            p_wm_vec = np.exp(beta_wm_eff * (W_s - W_s.max()))
            p_wm_vec /= (p_wm_vec.sum() + 1e-12)
            p_wm = p_wm_vec[a]

            # Mixture + lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with dual learning rates and forgetting
            pe = r - q[s, a]
            lr_eff = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_eff * pe
            # mild RL forgetting toward uniform
            q = (1.0 - f_rl) * q + f_rl * (1.0 / nA)

            # WM forgetting toward uniform
            w = (1.0 - f) * w + f * w_0

            # WM update on reward
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                eta = 1.0 - f
                w[s, :] = (1.0 - eta) * w[s, :] + eta * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

        blocks_log_p += log_p

    return -blocks_log_p