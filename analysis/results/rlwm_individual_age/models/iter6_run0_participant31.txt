def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Noisy working memory with misbinding; age- and set-size-modulated arbitration.

    Description:
    - A model-free RL controller learns Q-values with a single learning rate.
    - A WM controller maintains a probability distribution over actions per state.
      WM decays toward uniform and is subject to misbinding noise when encoding.
    - Arbitration weight for WM decreases with larger set size and with older age.
    - Age also modulates the effective RL inverse temperature.

    Parameters
    ----------
    states : array-like
        State index on each trial (0..nS-1 within a block).
    actions : array-like
        Chosen action index on each trial (0..2).
    rewards : array-like
        Reward on each trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) for the block, repeated across trials in that block.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_base, wm_mix_base, wm_forget_rate, misbind_rate, age_beta_mult]
        - lr: RL learning rate for Q updates.
        - beta_base: Base RL inverse temperature (scaled by 10 internally).
        - wm_mix_base: Baseline WM mixture weight before set-size/age modulation.
        - wm_forget_rate: Per-visit decay rate of WM toward uniform (0..1).
        - misbind_rate: Probability mass misbound to non-chosen actions on encoding.
        - age_beta_mult: Multiplier (>0) applied to beta for older adults (age group 1);
                         effective multiplier is (1 + age_group*(age_beta_mult-1)).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_mix_base, wm_forget_rate, misbind_rate, age_beta_mult = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    # Age-modulated RL temperature
    softmax_beta = beta_base * (1 + age_group * (age_beta_mult - 1.0))
    softmax_beta *= 10.0

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight declines with set size and age
        wm_weight_eff = wm_mix_base * (3.0 / nS) * (1.0 - 0.25 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # Effective WM forgetting increases with set size and age
        wm_forget_eff = np.clip(wm_forget_rate * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        # Misbinding increases with set size and age
        misbind_eff = np.clip(misbind_rate * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_forget_eff) * w[s, :] + wm_forget_eff * w_0[s, :]

            # WM encoding with misbinding after feedback
            if r > 0.0:
                # Allocate (1 - misbind) to chosen action, distribute misbind to others
                add_vec = np.zeros(nA)
                add_vec[a] = 1.0 - misbind_eff
                add_vec += (misbind_eff / (nA - 1.0)) * (1 - np.eye(nA)[a])
                # Blend with current WM and renormalize
                w[s, :] = 0.5 * w[s, :] + 0.5 * add_vec
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(+/-) with Q-value forgetting and entropy-gated arbitration with WM.

    Description:
    - RL has separate learning rates for positive and negative prediction errors.
    - Q-values undergo forgetting toward uniform to capture load-related interference.
    - WM stores rewarded actions with decay; WM reliability degrades with set size and age.
    - Arbitration is dynamic: WM weight increases when RL is uncertain (high entropy)
      and when WM is confident (low entropy). Age and set size modulate the base WM weight.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr_pos, lr_neg, beta, wm_base, q_forget_rate, arb_gain]
        - lr_pos: RL learning rate for positive PE.
        - lr_neg: RL learning rate for negative PE.
        - beta: RL inverse temperature (scaled by 10 internally).
        - wm_base: Baseline WM mixture weight before modulation.
        - q_forget_rate: Per-visit RL forgetting toward uniform (0..1).
        - arb_gain: Gain (>0) for entropy-based arbitration (how strongly entropies modulate WM weight).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta, wm_base, q_forget_rate, arb_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    # Age-modulated RL temperature (older -> a bit noisier via smaller beta factor)
    softmax_beta = beta * (1.0 - 0.2 * age_group)
    softmax_beta = max(softmax_beta, 1e-3)
    softmax_beta *= 10.0

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Base WM weight reduced by set size and age
        wm_weight_base = wm_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_weight_base = np.clip(wm_weight_base, 0.0, 1.0)

        # RL forgetting increases with set size and age
        q_forget_eff = np.clip(q_forget_rate * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute state-specific policies
            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen-action probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration (higher WM weight when RL entropy is high and WM entropy is low)
            # Compute entropies
            def softmax_probs(vec, beta_local):
                ex = np.exp(beta_local * (vec - np.max(vec)))
                return ex / np.sum(ex)

            p_rl_vec = softmax_probs(Q_s, softmax_beta / 10.0)  # scale back for entropy stability
            p_wm_vec = softmax_probs(W_s, softmax_beta_wm / 50.0)

            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))
            H_max = np.log(nA)

            wm_dyn = wm_weight_base + arb_gain * ((H_rl / H_max) - (H_wm / H_max))
            wm_dyn = np.clip(wm_dyn, 0.0, 1.0)

            p_total = wm_dyn * p_wm + (1.0 - wm_dyn) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with separate learning rates
            pe = r - q[s, a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # RL forgetting toward uniform
            q[s, :] = (1.0 - q_forget_eff) * q[s, :] + q_forget_eff * (1.0 / nA)

            # WM decay toward uniform and reinforcement on reward
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # mild generic decay each visit
            if r > 0.0:
                w[s, a] += (1.0 - w[s, a]) * 0.8
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with PE-modulated exploration and PE-gated WM encoding.

    Description:
    - RL uses a single learning rate, but its inverse temperature is dynamically
      modulated by the magnitude of the reward prediction error (|PE|): larger |PE|
      temporarily increases exploration (reduces beta).
    - WM encodes chosen actions only when PE exceeds a gating threshold (surprise),
      with the threshold modulated by set size and age. WM also decays toward uniform.
    - Arbitration mixes WM and RL with a base WM weight reduced by set size and age.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_base, beta_pe_gain, wm_weight_base, wm_pe_gate, wm_decay_base]
        - lr: RL learning rate.
        - beta_base: Base inverse temperature (scaled by 10 internally).
        - beta_pe_gain: How strongly |PE| reduces beta (exploration gain).
        - wm_weight_base: Baseline WM mixture weight before set-size/age penalties.
        - wm_pe_gate: Base PE magnitude threshold for WM encoding.
        - wm_decay_base: Baseline WM decay toward uniform per visit.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, beta_pe_gain, wm_weight_base, wm_pe_gate, wm_decay_base = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration weight: penalize with set size and age
        wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - 0.25 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # WM decay increases with set size and age
        wm_decay_eff = np.clip(wm_decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        # WM PE gate becomes stricter with larger set sizes and age
        wm_pe_gate_eff = wm_pe_gate * (nS / 3.0) * (1.0 + 0.5 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute current PE using pre-update Q
            pe = r - q[s, a]
            abs_pe = abs(pe)

            # Dynamic RL temperature: larger |PE| -> more exploration (lower beta)
            beta_dyn = beta_base / (1.0 + beta_pe_gain * abs_pe)
            beta_dyn = max(beta_dyn, 1e-3)
            softmax_beta = beta_dyn * 10.0

            # Policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            q[s, a] += lr * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # PE-gated WM encoding: encode when surprise is high enough
            if abs_pe >= wm_pe_gate_eff and r > 0.0:
                w[s, a] += (1.0 - w[s, a]) * 0.9
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p