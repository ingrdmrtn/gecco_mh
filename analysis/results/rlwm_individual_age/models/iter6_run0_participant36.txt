def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and load-modulated WM reliance and cross-state WM interference.

    Summary:
    - Choices are a mixture of RL and WM policies.
    - WM mixture weight is reduced by set size and more so for older adults.
    - WM traces decay after errors and suffer cross-state interference after rewarded storage.
    - RL updates with a single learning rate.

    Parameters (list of 6):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight; internally squashed to [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_decay: base WM decay rate toward uniform on errors (0..1).
    - xi_interf: cross-state WM interference after rewarded storage (0..1).
    - age_load_slope: how strongly larger set sizes reduce WM reliance, amplified in older adults.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: array with a single repeated value; age_group = 0 (<=45), 1 (>45).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, xi_interf, age_load_slope = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight reduced by set size and age
        wm_base = 1.0 / (1.0 + np.exp(-wm_weight))
        load_penalty = age_load_slope * max(0.0, (nS - 3) / 3.0) * (1.0 + 0.5 * age_group)
        wm_weight_eff = np.clip(wm_base * (1.0 - load_penalty), 0.0, 1.0)

        # Error-driven WM decay augmented by age and load
        wm_decay_eff = np.clip(wm_decay * (1.0 + 0.5 * age_group) * (1.0 + 0.5 * max(0, nS - 3)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as specified)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (near deterministic)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            if r > 0.0:
                # Store the rewarded mapping; interference to other states
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                if xi_interf > 0.0:
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        w[s2, :] = (1.0 - xi_interf) * w[s2, :] + xi_interf * w_0[s2, :]
            else:
                # Decay toward uniform on errors
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + surprise-gated WM with age bias.

    Summary:
    - RL controller with fixed temperature; WM controller is near-deterministic.
    - Trial-wise WM mixture weight is determined by a meta-gate responding to surprise (|r - max(Q)|),
      reduced under higher set size and in older adults.
    - WM traces are stored on reward, and decay toward uniform after non-reward.

    Parameters (list of 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_gate_base: baseline gate bias (real-valued; passed through sigmoid).
    - surprise_temp: sensitivity of the gate to surprise (>=0).
    - age_bias: subtractive bias on the gate when age_group=1 (>=0 reduces WM use in older adults).
    - wm_decay: WM decay toward uniform on errors (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_gate_base, surprise_temp, age_bias, wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent penalty on WM gating
        load_penalty = max(0.0, (nS - 3) / 3.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (as specified)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Surprise signal
            surprise = abs(r - np.max(Q_s))
            # Gate to WM (sigmoid)
            gate_input = wm_gate_base - age_bias * age_group - 2.0 * load_penalty + surprise_temp * surprise
            wm_weight_t = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Load- and age-adjusted WM decay
                decay_eff = np.clip(wm_decay * (1.0 + 0.5 * age_group) * (1.0 + 0.5 * max(0, nS - 3)), 0.0, 1.0)
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with reward sensitivity, RL forgetting, and WM noise increasing with load and age.

    Summary:
    - RL values updated with reward sensitivity rho (scaling rewards).
    - RL values undergo small forgetting toward uniform each trial, stronger with load and age.
    - WM policy is noisy: WM state vector is mixed with uniform according to a noise parameter
      that increases with set size and in older adults.
    - Mixture weight is baseline but reduced by load and age.

    Parameters (list of 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight: baseline WM mixture weight (passed through sigmoid).
    - rho_reward: scales rewards before RL update (>=0).
    - rl_forgetting: base RL forgetting rate toward uniform (0..1).
    - wm_noise: base WM policy noise (0..1), amplified by load and age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, rho_reward, rl_forgetting, wm_noise = model_parameters
    softmax_beta *= 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight reduced by load and age
        wm_base = 1.0 / (1.0 + np.exp(-wm_weight))
        wm_weight_eff = np.clip(wm_base / (1.0 + 0.6 * max(0.0, nS - 3)) * (1.0 - 0.2 * age_group), 0.0, 1.0)

        # RL forgetting amplified by load and age
        rl_forget_eff = np.clip(rl_forgetting * (1.0 + 0.5 * age_group) * (1.0 + 0.5 * max(0.0, nS - 3)), 0.0, 1.0)

        # WM noise amplified by load and age
        wm_noise_eff = np.clip(wm_noise * (1.0 + 0.7 * age_group) * (1.0 + 0.5 * max(0.0, nS - 3)), 0.0, 0.95)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply RL forgetting each trial
            q[s, :] = (1.0 - rl_forget_eff) * q[s, :] + rl_forget_eff * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as specified)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM noisy policy: mix with uniform before softmax
            W_s_noisy = (1.0 - wm_noise_eff) * W_s + wm_noise_eff * (1.0 / nA)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with reward sensitivity
            r_eff = rho_reward * r
            pe = r_eff - Q_s[a]
            q[s, a] += lr * pe

            # WM update: store on reward, mild decay otherwise (implicitly via noise at policy time)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # small decay to uniform to avoid staleness
                decay_small = 0.2 * wm_noise_eff
                w[s, :] = (1.0 - decay_small) * w[s, :] + decay_small * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set size effects:
- Model 1: WM weight reduced by set size and age; WM decay increased by set size and age; interference affects all states after rewarded storage.
- Model 2: WM gating is dynamic via surprise; gate is down-weighted for larger set size and for older adults; WM decay increases with load and age.
- Model 3: WM policy becomes noisier as set size increases and in older adults; RL forgetting is stronger with load and age; WM weight is reduced by load and age.
All parameters are used and the models return the negative log-likelihood of observed choices.