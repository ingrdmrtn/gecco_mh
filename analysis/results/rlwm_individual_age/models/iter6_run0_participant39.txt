Below are three alternative cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways. Each function:

- Follows the requested signature.
- Uses all parameters meaningfully (≤ 6 parameters).
- Fills the template’s “FILL IN” sections for the WM policy and WM updating.
- Incorporates set-size (3 vs 6) and age-group (younger=0, older=1) effects.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + fast-encoding WM with decay; age/load reduce WM contribution.

    Parameters (model_parameters):
    - lr: scalar, RL learning rate (0-1)
    - wm_weight0: scalar, base mixture weight between WM and RL (0-1)
    - softmax_beta: scalar, RL inverse temperature (will be upscaled by 10 as per template)
    - wm_lr: scalar, WM encoding rate upon reward (0-1)
    - wm_forget: scalar, per-trial WM decay toward uniform (0-1)
    - load_age_pen: scalar, multiplicative penalty strength of WM weight with set size and age (>0)
        Effective WM weight per trial: wm_weight = wm_weight0 * exp(-load_age_pen * nS * (1 + age_group))
    Notes:
    - WM policy is a softmax over a fast-updating WM value table (deterministic with high temp).
    - WM encodes only when reward is 1; otherwise only passive decay happens.
    - Older age and larger set sizes reduce the WM mixture weight exponentially.
    """
    lr, wm_weight0, softmax_beta, wm_lr, wm_forget, load_age_pen = model_parameters
    softmax_beta *= 10  # per template
    
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic WM softmax
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM table; WM weight reduced by age and load
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Effective WM mixture weight depends on set size and age
            wm_weight = wm_weight0 * np.exp(-load_age_pen * nS * (1.0 + age_group))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform each trial
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            # Fast encoding upon reward: move WM toward one-hot of chosen action
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot
            # Normalize for numerical stability
            w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with adaptive exploration + WM as episodic recall with interference; age/load modulate both.

    Parameters (model_parameters):
    - lr: scalar, RL learning rate (0-1)
    - wm_weight: scalar, base mixture weight between WM and RL (0-1)
    - softmax_beta: scalar, base RL inverse temperature (upscaled by 10 per template)
    - novelty_bonus: scalar, added to RL value for low-visit actions (encourages exploration)
    - conf_intf: scalar, WM interference strength with larger set sizes (>=0)
    - beta_gain: scalar, scales RL inverse temperature by recent reward rate (>=0)
        Effective RL beta per trial: beta_eff = softmax_beta * (1 + beta_gain * avg_reward) * (3/nS) / (1 + 0.3*age_group)
    Notes:
    - WM stores last rewarded action per state (episodic). With interference, a recalled action may be confused.
    - Interference rises with set size and age: confusion prob = 1 - conf_acc; conf_acc = 1 / (1 + conf_intf*(nS-3)*(1+0.5*age_group)).
    - RL includes a novelty bonus inversely proportional to visit count in the current state.
    """
    lr, wm_weight, softmax_beta, novelty_bonus, conf_intf, beta_gain = model_parameters
    softmax_beta *= 10  # per template

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # kept for template alignment; not used as values directly
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Episodic memory: store last rewarded action; -1 if none
        episodic = -1 * np.ones(nS, dtype=int)
        # Visit counts per state-action for novelty bonus
        visits = np.zeros((nS, nA), dtype=float)
        # Running average reward to adapt beta
        avg_reward = 0.0
        avg_alpha = 0.2  # smoothing for running reward rate

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Adaptive RL inverse temperature: higher with better recent performance; reduced by load/age
            beta_eff = softmax_beta * (1.0 + beta_gain * avg_reward)
            beta_eff *= (3.0 / max(nS, 1.0)) / (1.0 + 0.3 * age_group)

            # RL policy with novelty bonus
            bonus = novelty_bonus / (1.0 + visits[s, :])
            Q_s = q[s, :] + bonus
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Episodic recall with interference: when memory exists, choose remembered action
            if episodic[s] >= 0:
                remembered = episodic[s]
                # Accuracy of recall is reduced by interference from load and age
                conf_acc = 1.0 / (1.0 + conf_intf * max(nS - 3, 0) * (1.0 + 0.5 * age_group))
                conf_acc = np.clip(conf_acc, 0.0, 1.0)
                conf_prob = 1.0 - conf_acc
                if a == remembered:
                    p_wm = conf_acc + conf_prob * (1.0 / nA)  # if confused, becomes uniform
                else:
                    # probability of picking a specific non-remembered action under confusion -> uniform
                    p_wm = conf_prob * (1.0 / nA)
            else:
                p_wm = 1.0 / nA  # no memory yet

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Update episodic memory on reward
            if r == 1:
                episodic[s] = a

            # Template-aligned dummy WM table dynamics (light decay toward uniform)
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]
            w[s, :] /= max(np.sum(w[s, :]), eps)

            # Bookkeeping
            visits[s, a] += 1.0
            avg_reward = (1.0 - avg_alpha) * avg_reward + avg_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay + WM gating by uncertainty (entropy-based); age/load modulate gating and decay.

    Parameters (model_parameters):
    - lr: scalar, RL learning rate (0-1)
    - wm_weight0: scalar, base WM gating scale (0-1)
    - softmax_beta: scalar, RL inverse temperature (upscaled by 10 per template)
    - decay_q: scalar, per-trial RL decay toward uniform (0-1)
    - gate_k: scalar, sensitivity of gating to WM strength minus RL entropy (>0)
    - wm_temp: scalar, WM inverse temperature multiplier (applied to 50) (>0)
    Mechanism:
    - Compute RL softmax policy and its entropy; compute WM softmax policy and its strength (max prob).
    - Gate the mixture by a sigmoid(gate_k*(WM_strength - RL_entropy)), scaled by wm_weight0 and reduced by age/load.
    - RL Q-values decay toward uniform; decay is stronger for larger set size and in older adults.
    - WM updates quickly toward the chosen action on reward and otherwise drifts toward uniform.
    """
    lr, wm_weight0, softmax_beta, decay_q, gate_k, wm_temp = model_parameters
    softmax_beta *= 10  # per template

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0 * max(wm_temp, 1e-3)
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl) / max(np.sum(np.exp(logits_rl)), eps)
            p_rl = max(probs_rl[a], eps)

            # RL entropy (uncertainty)
            rl_entropy = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax on WM table
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            probs_wm = np.exp(logits_wm) / max(np.sum(np.exp(logits_wm)), eps)
            p_wm = max(probs_wm[a], eps)

            # WM strength: how peaked the WM distribution is over actions
            wm_strength = np.max(probs_wm) - (1.0 / nA)  # 0 when uniform, increases with concentration

            # Gating: more WM when WM is strong and RL is uncertain; reduced by age/load
            gate_raw = gate_k * (wm_strength - rl_entropy)
            gate = 1.0 / (1.0 + np.exp(-gate_raw))
            # Scale gating with base weight, reduced by age and load
            gate *= wm_weight0 * (3.0 / max(nS, 1.0)) * (1.0 - 0.3 * age_group)
            gate = np.clip(gate, 0.0, 1.0)

            p_total = gate * p_wm + (1.0 - gate) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Apply RL decay toward uniform; stronger with load and age
            decay_eff = decay_q * (nS / 6.0) * (1.0 + 0.5 * age_group)
            decay_eff = np.clip(decay_eff, 0.0, 1.0)
            q[s, :] = (1.0 - decay_eff) * q[s, :] + decay_eff * (1.0 / nA)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM drifts to uniform; on reward, it recenters toward chosen action
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * onehot
            # Normalize
            w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes:
- Model 1 emphasizes a fast WM store with explicit encoding and forgetting; older age and larger set sizes reduce the WM mixture weight.
- Model 2 emphasizes episodic recall with set size– and age–dependent interference; RL exploration is enhanced by a novelty bonus and adaptive temperature.
- Model 3 implements uncertainty-based gating between WM and RL, with RL decay that is stronger under load and in older adults, and a tunable WM temperature.