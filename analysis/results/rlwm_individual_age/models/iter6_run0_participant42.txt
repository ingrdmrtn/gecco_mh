Here are three distinct cognitive models that instantiate different mechanisms for arbitration between reinforcement learning (RL) and working memory (WM), while incorporating set-size and age effects. Each function returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-gated arbitration and age-noisy WM precision.

    Mechanisms
    ----------
    - RL: standard delta rule on Q(s,a).
    - WM: one-shot storage of rewarded action with decay toward uniform and small punishment for unrewarded choice.
    - Arbitration: WM weight increases with RL uncertainty (state-wise RL policy entropy) and decreases with load (set size).
    - Age: adds noise to WM precision (reduces effective WM inverse temperature).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial's block.
    age : array-like
        Single repeated value; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [lr, beta_base, wm_decay, load_interference, age_wm_noise, entropy_gate]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature, scaled internally by 10.
        - wm_decay: WM decay rate toward uniform per visit (0..1).
        - load_interference: how strongly larger set size reduces WM gating (>=0).
        - age_wm_noise: increases WM noise for older group (>=0).
        - entropy_gate: scales the influence of RL policy entropy on WM gating (>=0).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr, softmax_beta, wm_decay, load_interference, age_wm_noise, entropy_gate = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over W_s, with age-dependent noise (lower effective beta when older)
            beta_wm_eff = softmax_beta_wm / (1.0 + age_wm_noise * age_group)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: WM weight increases with RL entropy (uncertainty) and decreases with load
            # Compute RL policy over actions for entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))  # in [0, log(nA)]
            H_rl_norm = H_rl / np.log(nA)

            load = float(nS - 3)  # 0 for 3, 3 for 6
            gate_input = -load_interference * load + entropy_gate * H_rl_norm
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform; replace on reward; slight downweight on unrewarded action
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # push probability slightly away from chosen action on error
                w[s, a] = 0.5 * w[s, a]
                redistribute = (1.0 - np.sum(w[s, :])) / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL + WM win-stay/lose-switch with age- and load-dependent lapse.

    Mechanisms
    ----------
    - RL: asymmetric learning rates for positive vs negative RPE.
    - WM: win-stay/lose-switch (WSLS) implemented as a preference table W with decay.
    - Arbitration: combine RL and WM; then inject a uniform lapse that increases with set size and age.
      WM contribution within the mixture is adaptive based on state recency-of-reward.
    - Age: increases lapse probability (exploration/noise).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Reward per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block.
    age : array-like
        Single repeated value; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, wm_ws_strength, age_lapse, load_lapse]
        - alpha_pos: RL learning rate for positive RPE.
        - alpha_neg: RL learning rate for negative RPE.
        - beta_base: base RL inverse temperature, scaled internally by 10.
        - wm_ws_strength: strength (precision) of WM WSLS policy (scales WM inverse temperature).
        - age_lapse: adds lapse for older adults (>=0).
        - load_lapse: adds lapse as set size increases (>=0).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_ws_strength, age_lapse, load_lapse = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM preferences; will encode WSLS
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last rewarded action and recency for arbitration
        last_rewarded_action = -np.ones(nS, dtype=int)
        recency_since_reward = np.full(nS, 10, dtype=float)  # large initial value

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            recency_since_reward[:] = recency_since_reward + 1.0

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM WSLS policy: scale WM temperature by wm_ws_strength
            beta_wm_eff = softmax_beta_wm * max(wm_ws_strength, 1e-6)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Weight WM within the RL/WM mixture using recency of reward in this state
            rec = recency_since_reward[s]
            wm_weight = 1.0 / (1.0 + rec)  # recent reward -> higher WM reliance
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Combine RL and WM, then apply lapse that increases with age and load
            load = float(nS - 3)
            epsilon = 1.0 / (1.0 + np.exp(-(age_lapse * age_group + load_lapse * load)))  # in (0,1)
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetry
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update implementing WSLS:
            # - On reward: set strong preference for chosen action (win-stay).
            # - On no reward: suppress chosen action and relatively boost others (lose-switch).
            # Also add a mild decay toward uniform each trial.
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
                last_rewarded_action[s] = a
                recency_since_reward[s] = 0.0
            else:
                # penalize chosen action, redistribute to others
                w[s, a] *= 0.5
                remaining = 1.0 - np.sum(w[s, :])
                if remaining > 0 and nA > 1:
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] += remaining / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM with volatility-based arbitration and age-dampened sensitivity.

    Mechanisms
    ----------
    - RL: standard delta rule.
    - WM: capacity-limited one-shot storage of rewarded mappings with decay and interference;
          probability that a state is effectively in WM is proportional to K / set_size.
    - Arbitration: WM weight increases with recent absolute PE (volatility proxy), indicating RL unreliability.
      Older adults show reduced sensitivity to volatility (age dampening).
    - Set size: larger sets reduce effective WM (capacity pressure) and add interference to WM maintenance.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Reward per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block.
    age : array-like
        Single repeated value; used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, beta_base, wm_decay, pe_sensitivity, age_sensitivity_drop, wm_capacity_K]
        - alpha: RL learning rate.
        - beta_base: base RL inverse temperature, scaled internally by 10.
        - wm_decay: WM decay toward uniform (0..1).
        - pe_sensitivity: base sensitivity of arbitration to recent abs PE (>=0).
        - age_sensitivity_drop: reduction of sensitivity for older adults (>=0).
        - wm_capacity_K: WM capacity (approx. number of items; >=0).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    alpha, softmax_beta, wm_decay, pe_sens, age_sens_drop, wm_capacity_K = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track recent absolute PE per state (exponential moving average)
        ema_abs_pe = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Effective WM availability due to capacity K and set size
            p_in_wm = min(1.0, max(0.0, wm_capacity_K / max(1.0, float(nS))))

            # Arbitration weight based on recent volatility (ema of |PE|)
            sens_eff = max(0.0, pe_sens - age_sens_drop * age_group)
            wm_weight_vol = 1.0 / (1.0 + np.exp(-sens_eff * (ema_abs_pe[s] - 0.25)))  # centered around moderate volatility
            wm_weight = np.clip(p_in_wm * wm_weight_vol, 0.0, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # Update volatility estimate (state-wise EMA of |PE|)
            ema_abs_pe[s] = 0.7 * ema_abs_pe[s] + 0.3 * abs(pe)

            # WM update with decay and interference (stronger when nS > K)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # Interference: if the set size exceeds capacity, add extra decay each trial
            overload = max(0.0, float(nS) - wm_capacity_K)
            if overload > 0.0:
                extra_decay = 1.0 - 1.0 / (1.0 + overload)
                w[:, :] = (1.0 - extra_decay) * w[:, :] + extra_decay * w_0[:, :]

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes
- All three models combine an RL policy p_rl with a WM policy p_wm via a context-sensitive arbitration weight that depends on load (set size) and age group (older adults have reduced WM precision, increased lapse, or reduced arbitration sensitivity).
- WM updates follow a replace-on-reward scheme with decay and, in some cases, error-based suppression or interference when capacity is exceeded.
- Set size effects:
  - Model 1 reduces WM gating via load_interference and increases WM reliance when RL is uncertain (entropy).
  - Model 2 increases uniform lapse with load_lapse.
  - Model 3 reduces effective WM availability via K / set size and adds interference when nS > K.
- Age effects:
  - Model 1 reduces WM precision via age_wm_noise.
  - Model 2 increases lapse via age_lapse.
  - Model 3 reduces volatility sensitivity via age_sensitivity_drop, making older adults less adaptive in arbitration.