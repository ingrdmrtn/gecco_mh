def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy-driven arbitration to WM and age/load-sensitive WM leak.

    Mechanism
    - RL: standard Q-learning with softmax choice.
    - WM: one-shot cache that stores rewarded state-action mappings and leaks toward uniform.
    - Arbitration: the mixture weight for WM is dynamic and increases with RL policy entropy
      (i.e., when RL is uncertain), decreases with larger set size, and is further reduced
      in older adults.

    Parameters
    ----------
    model_parameters : list or array
        [lr, wm_weight0, softmax_beta, wm_leak, entropy_gain, age_arbitration_shift]
        - lr: RL learning rate (0..1).
        - wm_weight0: base mixture weight for WM before adjustments (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_leak: per-trial leak of WM toward uniform (0..1).
        - entropy_gain: sensitivity of WM arbitration to RL entropy (>=0).
        - age_arbitration_shift: additional down-weighting of WM in older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_leak, entropy_gain, age_arbitration_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Size-dependent baseline scaling (smaller set, more WM)
        size_scale = 1.0 if nS == 3 else 0.5
        # Age-dependent downscaling of WM influence
        age_scale = 1.0 / (1.0 + age_arbitration_shift * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice probabilities
            Q_s = q[s, :].copy()
            Qc = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(softmax_beta * Qc)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # RL entropy (higher means more uncertainty)
            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, eps, 1.0))) / np.log(nA)  # normalized to [0,1]

            # WM choice probabilities
            W_s = w[s, :].copy()
            Wc = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Wc)
            p_wm_vec /= np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            # Dynamic arbitration: base * size * age * entropy-driven gain
            wm_weight_dyn = wm_weight0 * size_scale * age_scale * (1.0 + entropy_gain * H_rl)
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM leak toward uniform
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            # WM encoding of rewarded mapping (overwrite to a near one-hot)
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row /= np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with item-interference: WM bindings suffer between-item confusion that grows with set size and age.

    Mechanism
    - RL: standard Q-learning and softmax policy.
    - WM: graded binding for rewarded mappings; between trials, interference mixes rows toward their grand mean,
      more strongly for larger set sizes and in older adults.
    - Arbitration: fixed base weight scaled down by set size (3 vs 6).

    Parameters
    ----------
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, wm_binding, interference_rate, age_interf_boost]
        - lr: RL learning rate (0..1).
        - wm_weight: base mixture weight for WM in policy (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_binding: strength of WM update toward the rewarded action (0..1).
        - interference_rate: base rate of WM inter-item interference per trial (0..1).
        - age_interf_boost: multiplicative increase of interference in older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_binding, interference_rate, age_interf_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration scaling by set size (more WM in smaller sets)
        wm_w_block = np.clip(wm_weight * (3.0 / nS), 0.0, 1.0)

        # Age and load dependent interference strength
        base_interf = interference_rate * max(0, (nS - 3) / 3.0)  # 0 for 3, interference_rate for 6
        interf_eff = base_interf * (1.0 + age_interf_boost * age_group)
        interf_eff = np.clip(interf_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            Qc = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(softmax_beta * Qc)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :].copy()
            Wc = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Wc)
            p_wm_vec /= np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM between-item interference: rows mix toward their grand mean
            row_mean = np.mean(w, axis=0, keepdims=True)  # shape (1, nA)
            w = (1.0 - interf_eff) * w + interf_eff * np.repeat(row_mean, nS, axis=0)

            # WM learning on reward: graded binding toward chosen action
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_binding) * w[s, :] + wm_binding * target

            # Renormalize each state's WM distribution to be a proper probability vector
            w = np.clip(w, eps, None)
            w /= np.sum(w, axis=1, keepdims=True)

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with capacity-limited utilization: effective WM contribution scales with an estimated capacity K,
    reduced in older adults; WM decays, RL is standard.

    Mechanism
    - RL: standard Q-learning with softmax.
    - WM: one-shot overwrite on reward with per-trial decay to uniform.
    - Arbitration: WM mixture weight = wm_weight0 * utilization, where utilization = min(1, K_eff / set_size),
      with K_eff reduced in older adults.

    Parameters
    ----------
    model_parameters : list or array
        [lr, softmax_beta, wm_weight0, wm_decay, wm_capacity_K, age_capacity_drop]
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_weight0: base WM mixture weight before utilization scaling (0..1).
        - wm_decay: per-trial decay of WM toward uniform (0..1).
        - wm_capacity_K: effective WM capacity in number of items (>=0).
        - age_capacity_drop: capacity reduction applied if older (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, wm_decay, wm_capacity_K, age_capacity_drop = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity reduced in older adults
        K_eff = max(0.0, wm_capacity_K - age_capacity_drop * age_group)
        utilization = 0.0 if nS <= 0 else min(1.0, K_eff / float(nS))
        wm_w_block = np.clip(wm_weight0 * utilization, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            Qc = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(softmax_beta * Qc)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :].copy()
            Wc = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Wc)
            p_wm_vec /= np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding on reward: overwrite to near one-hot
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row /= np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)