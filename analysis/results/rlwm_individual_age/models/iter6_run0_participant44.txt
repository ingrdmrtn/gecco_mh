Below are three alternative cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways, with explicit roles for set size (3 vs 6) and age group (younger vs older). Each function returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility traces) + WM one-shot with decay; adaptive WM gating penalized by set size and age.

    Idea:
    - RL uses eligibility traces to propagate credit over time.
    - WM is a one-shot binding of state->action after reward, but the binding decays toward uniform.
    - The mixture weight (gate) for WM is a logistic function that decreases with higher set size
      and for older adults, and increases when WM memory for the current state is sharp.

    Parameters (model_parameters):
    - alpha: RL learning rate for prediction errors (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10 internally)
    - lambda_trace: eligibility trace decay (also used as WM decay toward uniform; 0..1)
    - wm_gate_base: base WM gating bias (logit space)
    - size_gate_penalty: penalty on WM gate per extra item beyond 3 (>=0)
    - age_gate_penalty: additional gate penalty for older adults (>=0)

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, softmax_beta, lambda_trace, wm_gate_base, size_gate_penalty, age_gate_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-np.clip(x, -50, 50)))

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # WM distributions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM gate on this trial
            load_pen = size_gate_penalty * max(0, nS - 3)
            age_pen = age_gate_penalty * age_group
            # A proxy of WM sharpness: peak minus uniform baseline
            peak = np.max(W_s) - (1.0 / nA)
            gate_logit = wm_gate_base - load_pen - age_pen + 5.0 * peak
            wm_weight_eff = sigmoid(gate_logit)

            # Mixture policy and log-likelihood
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            pe = r - q[s, a]
            # decay traces
            e *= lambda_trace
            # replacing trace for chosen state-action
            e[s, a] += 1.0
            # update q
            q += alpha * pe * e

            # WM update: one-shot store on reward, global decay toward uniform
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            # decay WM toward uniform for all states
            w = (1.0 - lambda_trace) * w + lambda_trace * w_0

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration bonus + capacity-limited WM with recency eviction and size-dependent lapses.

    Idea:
    - RL learns action values and adds an uncertainty bonus to promote directed exploration.
      Uncertainty is approximated as 1/sqrt(visits) per state-action.
    - WM has a capacity in number of states it can store (K). WM stores the last rewarded action per state.
      If more than K states are rewarded, older stored states are evicted (recency-based).
    - WM mixture weight scales with K/nS. Lapses increase with set size.
    - Age is used by assigning different capacities for younger vs older participants.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10 internally)
    - bonus: directed exploration bonus strength (>=0)
    - K_young: WM capacity (in states) for younger participants (>=0)
    - K_old: WM capacity (in states) for older participants (>=0)
    - lapse_gain: additional random lapse per extra item beyond 3 (>=0), capped in computation

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, softmax_beta, bonus, K_young, K_old, lapse_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1
    K = K_young if age_group == 0 else K_old

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.zeros((nS, nA))  # for uncertainty bonus

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-limited WM store: track which states are currently in WM and recency queue
        in_wm = np.zeros(nS, dtype=bool)
        recency_queue = []  # list of state indices in order of most recent encoding

        # Mixture weight based on capacity vs load
        wm_weight_base = min(1.0, K / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with directed exploration bonus
            Q_s = q[s, :].copy()
            bonus_s = bonus / np.sqrt(counts[s, :] + 1.0)
            Q_s_bonus = Q_s + bonus_s
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s_bonus - Q_s_bonus[a])))

            # WM policy (only reliable if state is in capacity-limited store)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight: base capacity weight, but zero if state not stored
            wm_weight_eff = wm_weight_base if in_wm[s] else 0.0

            # Lapse increases with set size above 3
            eps = min(0.2, lapse_gain * max(0, nS - 3))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - eps) * p_mix + eps * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe
            counts[s, a] += 1.0

            # WM update with capacity and recency
            if r > 0:
                # encode this state-action as winner-take-all in WM
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

                # update recency and capacity membership
                if in_wm[s]:
                    # move s to most recent position
                    recency_queue = [x for x in recency_queue if x != s]
                recency_queue.insert(0, s)
                in_wm[s] = True

                # evict if over capacity
                while np.sum(in_wm) > int(np.floor(K)):
                    to_evict = recency_queue.pop()  # least recent
                    in_wm[to_evict] = False
                    # forgetting: revert to uniform for evicted state
                    w[to_evict, :] = w_0[to_evict, :]

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controlled RL+WM: adaptive WM gating from confidence vs RL uncertainty, with age-reduced beta.

    Idea:
    - RL uses standard delta rule; RL policy temperature is reduced in older adults.
    - WM stores action beliefs per state and learns toward one-hot after reward; it also decays toward
      uniform when unsupported by reward.
    - A meta-controller computes a WM gate from (WM confidence - RL uncertainty), penalized by set size
      and age. WM confidence is a running estimate of how reliable WM is for that state; RL uncertainty
      is the entropy of the RL softmax policy for the state.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta_base: base inverse temperature before scaling and age penalty (scaled by 10 internally)
    - eta_wm: WM learning/decay rate (0..1)
    - meta_temp: gain on the meta-controller (logit scale for gate) (>=0)
    - size_penalty: penalty on WM gate per extra item beyond 3 (>=0), also used for WM decay
    - age_beta_penalty: divisor factor for RL beta in older adults (>=0)

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, beta_base, eta_wm, meta_temp, size_penalty, age_beta_penalty = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    # Age reduces effective beta for RL
    beta_rl = beta_base * 10.0 / (1.0 + age_beta_penalty * age_group)
    softmax_beta_wm = 50.0

    def softmax_probs(vals, beta):
        z = vals - np.max(vals)
        ez = np.exp(beta * z)
        denom = np.sum(ez)
        return ez / max(denom, 1e-12)

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-np.clip(x, -50, 50)))

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM confidence per state (0..1)
        wm_conf = np.zeros(nS)

        # Precompute load factor
        load_factor = max(0, nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy and uncertainty (entropy)
            Q_s = q[s, :]
            # compute RL action probabilities
            p_rl_vec = softmax_probs(Q_s, beta_rl)
            p_rl = p_rl_vec[a]
            rl_unc = entropy(p_rl_vec) / np.log(nA)  # normalized entropy in [0,1]

            # WM policy
            W_s = w[s, :]
            # Convert to action probability with high beta
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Meta-control gate: compare WM confidence to RL uncertainty
            gate_signal = wm_conf[s] - rl_unc
            gate_logit = meta_temp * gate_signal - size_penalty * load_factor - 0.5 * age_group
            wm_weight_eff = sigmoid(gate_logit)

            # Final choice probability
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM update: learn toward one-hot on reward; otherwise slight decay toward uniform
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * target
                # normalize for safety
                ssum = np.sum(w[s, :])
                if ssum > 0:
                    w[s, :] /= ssum
                # increase confidence
                wm_conf[s] = wm_conf[s] + eta_wm * (1.0 - wm_conf[s])
            else:
                # decay toward uniform; stronger decay with larger set sizes
                decay = eta_wm * (1.0 + size_penalty * load_factor)
                decay = min(decay, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                # reduce confidence slightly when no reward
                wm_conf[s] *= (1.0 - 0.5 * eta_wm)

        total_log_p += log_p

    return -total_log_p