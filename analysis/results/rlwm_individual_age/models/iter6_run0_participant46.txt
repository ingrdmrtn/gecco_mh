def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age- and load-modulated WM gating and decay.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: fast look-up table that tries to store the most recently rewarded action per state.
      WM decays toward a uniform prior and only encodes on a trial with some probability.
    - Mixture: the policy is a convex combination of WM and RL, with the WM weight reduced
      under higher load (nS=6) and in older adults (age_group=1).

    Parameters (model_parameters):
    - lr: reinforcement learning rate (0..1).
    - wm_weight: baseline WM mixture weight before modulation by load/age (logit space).
      Effective WM weight = sigmoid(wm_weight - k*((nS-3) + 0.7*age_group)), k=1.
    - softmax_beta: inverse temperature for RL policy; scaled by 10 internally.
    - wm_decay: per-trial WM decay toward uniform (0..1). Effective decay increases with load/age.
    - encode_base: baseline logit probability of encoding into WM on a rewarded trial.
      Encoding prob is reduced by load and age: sigmoid(encode_base - (nS-3) - 0.5*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, encode_base = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL
    softmax_beta_wm = 50  # near-deterministic WM

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute effective WM weight and decay for this block (depends on age/load)
        # WM gating reduced by load and age (logistic transform)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-(wm_weight - ((nS - 3.0) + 0.7 * age_group))))
        # WM decay increases with load/age
        wm_decay_eff = np.clip(wm_decay + 0.1 * (nS - 3.0) + 0.1 * age_group, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of the chosen action (softmax)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM choice probability (near-deterministic softmax over WM table)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM encoding on rewarded trials with prob reduced by load/age
            p_encode = 1.0 / (1.0 + np.exp(-(encode_base - (nS - 3.0) - 0.5 * age_group)))
            if r > 0.0 and np.random.rand() < p_encode:
                # Overwrite with a one-hot memory for the rewarded action
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL + Uncertainty-gated WM with age/load-dependent forgetting.

    Mechanism:
    - RL: tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: stores the most recent correct action per state, but forgets toward uniform each trial.
    - Uncertainty gating: WM weight increases when RL is uncertain (high entropy policy),
      but is suppressed in larger set size and older adults.
    - Forgetting: WM forgetting increases with load and in older adults.

    Parameters (model_parameters):
    - lr: learning rate for positive PEs (0..1).
    - wm_weight: baseline WM gate (logit). Increased by RL uncertainty, decreased by load/age.
    - softmax_beta: inverse temperature for RL policy; scaled by 10 internally.
    - lr_neg: learning rate for negative PEs (0..1).
    - wm_forget: baseline WM forgetting rate (0..1), further increased by load/age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, lr_neg, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective forgetting increases with load and age
        forget_eff = np.clip(wm_forget + 0.15 * (nS - 3.0) + 0.15 * age_group, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy and uncertainty (entropy)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits)
            pi_rl /= max(np.sum(pi_rl), 1e-12)
            entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            # Normalize entropy to ~[0,log(3)], then center near 0.5
            entropy_max = np.log(3.0)
            u = entropy / max(entropy_max, 1e-12)  # 0..1
            # WM gate increases with uncertainty, decreases with load/age
            wm_gate_logit = wm_weight + 2.0 * (u - 0.5) - ((nS - 3.0) + age_group)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_gate_logit))

            # Choice probabilities
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM forgetting toward uniform every trial
            w[s, :] = (1.0 - forget_eff) * w[s, :] + forget_eff * w_0[s, :]

            # WM encoding on rewarded trials: store the successful action deterministically
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Heuristic WM (Win-Stay / Lose-Shift) with age/load-modulated mixture and WM leak.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: heuristic table implementing win-stay (boost chosen action on reward) and
      lose-shift (suppress chosen action, boost alternatives on no reward).
      WM values leak toward uniform; leak increases with load/age.
    - Mixture: WM weight decreases with load and in older adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (logit); reduced by load/age via a linear term inside sigmoid.
    - softmax_beta: inverse temperature for RL policy; scaled by 10 internally.
    - win_stay_bias: additive boost applied to chosen action in WM on reward (>=0).
    - age_load_scale: scales how strongly load+age suppress the WM mixture weight.
    - wm_leak: baseline WM leak toward uniform (0..1); increased by load/age.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, win_stay_bias, age_load_scale, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Mixture weight suppressed by load+age
        suppression = age_load_scale * ((nS - 3.0) + age_group)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-(wm_weight - suppression)))

        # Leak increases with load and age
        leak_eff = np.clip(wm_leak + 0.1 * (nS - 3.0) + 0.1 * age_group, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of the chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM probability of the chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM leak toward uniform
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # Heuristic WM update: win-stay / lose-shift
            if r > 0.0:
                # Boost chosen action
                w[s, a] += win_stay_bias
            else:
                # Penalize chosen and boost others
                w[s, a] -= win_stay_bias
                others = [aa for aa in range(nA) if aa != a]
                w[s, others] += win_stay_bias / (nA - 1)

            # Keep WM values non-negative and normalized (soft normalization)
            w[s, :] = np.clip(w[s, :], 0.0, None)
            if np.sum(w[s, :]) > 0:
                w[s, :] /= np.sum(w[s, :])
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set size effects in these models:
- Model 1: WM mixture weight and decay/encoding are explicitly reduced (weight) or worsened (decay, encoding probability) with set size 6 and for older adults. This captures reduced WM efficacy under load and aging.
- Model 2: WM gate increases when RL is uncertain, but this boost is counteracted by set size and age. WM forgetting is higher in larger set sizes and for older adults.
- Model 3: WM mixture weight is suppressed by a tunable age_load_scale times the combined load+age term; WM leak also increases with load and age. This yields stronger reliance on slow RL in older/high-load conditions.