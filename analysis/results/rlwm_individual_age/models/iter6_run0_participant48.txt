Below are three standalone cognitive models that implement RL+WM mechanisms tailored to the RLWM task, use age group and set size meaningfully, and return the negative log-likelihood of the observed choices. Each model uses up to six parameters, uses all parameters, and fills in the WM policy and WM value updating per the provided template logic.

Note: These functions assume numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with load- and age-modulated WM gating via a sigmoid gate; separate WM precision.

    Concept:
    - A standard model-free RL system learns Q-values with a softmax policy.
    - A one-shot WM system stores the rewarded action for a state and decays toward uniform.
    - Arbitration: a sigmoid gate determines the WM weight based on a load term (set size),
      an age penalty, and a base WM gain. Older adults have reduced WM gate; larger set size
      reduces WM gate.
    - WM has its own precision (beta_wm), separate from RL beta.

    Parameters
    ----------
    model_parameters : list or array
        [lr, beta_rl, wm_decay, wm_gain, age_penalty, beta_wm]
        - lr: RL learning rate (0..1)
        - beta_rl: RL inverse temperature (scaled by 10 internally)
        - wm_decay: rate of decay of WM toward uniform on each trial (0..1)
        - wm_gain: baseline gate gain for WM (higher -> more WM use)
        - age_penalty: penalty applied to the WM gate when age_group==1 (older adult)
        - beta_wm: WM inverse temperature (scaled by 10 internally)
    """
    lr, beta_rl, wm_decay, wm_gain, age_penalty, beta_wm = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = beta_wm * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load term decreases WM gate for larger set size (6 vs 3)
        # Using ratio 3/nS: equals 1 at nS=3 and 0.5 at nS=6; linearly scaled by wm_gain.
        load_term = (3.0 / nS) - 0.5  # centered roughly between 3 and 6
        # Age term penalizes WM gate for older adults
        age_term = -age_penalty * age_group

        # Convert total gate drive into [0,1] via sigmoid per block
        gate_drive = wm_gain * load_term + age_term
        wm_weight_block = 1.0 / (1.0 + np.exp(-gate_drive))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform, then one-shot store if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with load- and age-dependent lapse; WM decays and contributes at a fixed weight.

    Concept:
    - RL learns Q-values with a softmax policy.
    - WM stores rewarded state-action pairs with decay to uniform; WM precision is high (fixed).
    - Arbitration: use a base WM weight attenuated by load and age (linear attenuation).
    - Lapse: with probability epsilon (which increases with set size and with age), choice is random.
      This captures increased noise/interference under higher load and in older adults.

    Parameters
    ----------
    model_parameters : list or array
        [lr, beta_rl, wm_weight_base, wm_decay, epsilon_base, age_lapse_increase]
        - lr: RL learning rate (0..1)
        - beta_rl: RL inverse temperature (scaled by 10 internally)
        - wm_weight_base: baseline WM mixture weight at set size 3
        - wm_decay: rate of decay of WM toward uniform (0..1)
        - epsilon_base: base lapse at set size 3 for young adults
        - age_lapse_increase: added lapse for older adults and stronger load sensitivity
    """
    lr, beta_rl, wm_weight_base, wm_decay, epsilon_base, age_lapse_increase = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight attenuated by load and age
        load_scale = 3.0 / nS  # equals 1 at nS=3, 0.5 at nS=6
        wm_weight = wm_weight_base * load_scale
        wm_weight = max(0.0, min(1.0, wm_weight - 0.25 * age_group))  # mild age reduction

        # Lapse increases with load and age
        epsilon = epsilon_base + age_lapse_increase * age_group + (1.0 - load_scale) * (epsilon_base + age_lapse_increase * 0.5)
        epsilon = max(0.0, min(0.5, epsilon))  # cap lapse

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and one-shot store on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with dynamic arbitration based on WM confidence and PE history; age and load shift gate.

    Concept:
    - RL learns with softmax.
    - WM one-shot stores rewarded mapping and decays.
    - Arbitration gate is dynamic per trial: higher when WM is confident (difference between top
      and second-best WM action is large), and when recent absolute prediction error is small
      (suggesting stable mapping).
    - Load (set size) and age shift the gate downward (less WM use in high load and in older adults).

    Parameters
    ----------
    model_parameters : list or array
        [lr, beta_rl, wm_decay, gate_slope, gate_offset, beta_wm]
        - lr: RL learning rate (0..1)
        - beta_rl: RL inverse temperature (scaled by 10 internally)
        - wm_decay: decay of WM toward uniform (0..1)
        - gate_slope: slope scaling of the dynamic gate to WM confidence and low PE
        - gate_offset: baseline offset; will be reduced by age and load
        - beta_wm: WM inverse temperature (scaled by 10 internally)
    """
    lr, beta_rl, wm_decay, gate_slope, gate_offset, beta_wm = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = beta_wm * 10.0

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running estimate of recent absolute PE per state (for dynamic arbitration)
        pe_abs = np.zeros(nS)

        # Load and age offsets: larger set-size and older age reduce gate
        load_penalty = (nS - 3) / 3.0  # 0 at 3, 1 at 6
        age_penalty = 0.5 * age_group   # moderate downward shift when older

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy and confidence
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence: margin between best and second-best WM values (0..1)
            sorted_vals = np.sort(W_s)[::-1]
            wm_conf = sorted_vals[0] - sorted_vals[1]  # higher = more confident WM

            # Dynamic gate: prefers WM when WM is confident and recent PE is low (stable mapping)
            # Use (1 - normalized PE) so that low PE increases gate.
            pe_norm = min(1.0, pe_abs[s])  # clamp
            gate_drive = gate_offset - load_penalty - age_penalty + gate_slope * (wm_conf + (1.0 - pe_norm))
            wm_weight = 1.0 / (1.0 + np.exp(-gate_drive))
            wm_weight = max(0.0, min(1.0, wm_weight))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and update PE tracker
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            pe_abs[s] = 0.7 * pe_abs[s] + 0.3 * abs(delta)

            # WM decay and one-shot reward-based store
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p

How age and set size affect parameters/mechanisms in these models:
- Model 1: WM gate is reduced by larger set size (via 3/nS term) and by age penalty for older adults; WM has its own precision.
- Model 2: WM weight is attenuated by load and age; lapse probability increases with set size and age, capturing noisier choices under higher cognitive load and in older group.
- Model 3: Arbitration uses trial-wise WM confidence and recent prediction error, with baseline gate shifted downward by load and by age; thus, older adults and higher set size rely less on WM, especially when WM confidence is low or recent PEs are high.