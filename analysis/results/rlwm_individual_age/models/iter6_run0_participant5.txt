def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: RL + Capacity-limited WM with age-modulated capacity and decay

    Description:
    - RL: Q-learning with softmax action selection.
    - WM: A capacity-limited lookup memory of rewarded mappings (per state) that decays toward uniform.
           When rewarded, the WM for that state is overwritten to the chosen action (one-hot).
    - Arbitration: Fixed WM mixture base weight scaled by an effective capacity-based retrieval probability.
    - Set size effect: Effective WM retrieval probability scales as K_eff / set_size (capped at 1).
    - Age effect: Older group has reduced effective capacity (K_eff).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature (scaled by 10 internally)
    - wm_base: Base mixture weight for WM (0..1)
    - K_young: WM capacity for young participants (>=1)
    - age_K_shift: Reduction in capacity for older group (>=0)
    - wm_forget: Per-trial decay of WM toward uniform (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_base, K_young, age_K_shift, wm_forget = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity after age effect
        K_eff = max(1.0, K_young - age_group * max(0.0, age_K_shift))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (stable softmax probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy via near-deterministic softmax over WM vector
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-limited retrieval success scaled by set size
            rho = min(1.0, K_eff / float(nS))
            wm_weight_eff = np.clip(wm_base * rho, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay each trial
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM overwrite on reward: store the chosen action deterministically
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: RL + Surprise-gated WM with set-size scaling and age-modulated WM temperature

    Description:
    - RL: Q-learning with softmax.
    - WM: Last-rewarded action memory per state (deterministic one-hot), used via a softmax.
    - Arbitration: WM mixture weight is down-regulated by surprise via a smooth gate.
      Surprise is |r - Q_s[a]|; higher surprise favors RL over WM.
    - Set size effect: WM reliance scales with 3 / set_size.
    - Age effect: WM temperature is increased for older participants (more noise).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature (scaled by 10 internally)
    - wm_gain: Base WM mixture weight (0..1)
    - pe_slope: Slope of surprise gate (>=0)
    - pe_bias: Bias of surprise gate (threshold) (>=0)
    - age_temp_shift: Additive increase in WM temperature for older group (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_gain, pe_slope, pe_bias, age_temp_shift = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    base_wm_beta = 30.0
    wm_beta_eff = base_wm_beta / (1.0 + age_group * max(0.0, age_temp_shift))  # older -> more noise (lower beta)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: last rewarded action; start uniform
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen prob with effective beta
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Surprise gate to down-regulate WM reliance
            surprise = abs(r - Q_s[a])  # in [0,1] given rewards in {0,1}
            gate = 1.0 / (1.0 + np.exp(-max(0.0, pe_slope) * (surprise - max(0.0, pe_bias))))
            # Set size scaling decreases WM reliance
            ss_factor = 3.0 / float(nS)
            wm_weight_eff = np.clip(wm_gain * ss_factor * (1.0 - gate), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on reward, store the chosen action deterministically
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            # On no reward, keep current WM (no additional parameterized forgetting here)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: Uncertainty-based arbitration between RL and WM

    Description:
    - RL: Q-learning with softmax; also maintains a running estimate of value uncertainty per (s,a) via
          an exponential moving average of squared TD errors.
    - WM: Reward-gated memory that moves toward a one-hot after reward, and decays toward uniform.
    - Arbitration: Weight on WM vs RL is a softmax over their confidences with temperature.
        * WM confidence = (1 - normalized entropy of WM for state) scaled by 3 / set_size
        * RL confidence = 1 / (1 + mean TD-error variance for state's actions)
      The arbitration temperature is age-modulated (older -> lower arbitration sensitivity).
    - Set size effect: Directly reduces WM confidence via the 3 / set_size scaling.
    - Age effect: Arbitration temperature reduced for older group, biasing toward more averaging.

    Parameters (list):
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature (scaled by 10 internally)
    - wm_lr: WM learning rate toward one-hot after reward (0..1)
    - arb_temp: Arbitration temperature (>0) controlling sensitivity to confidence differences
    - age_arb_shift: Multiplicative reduction of arbitration temperature for older group (>=0)
    - wm_decay: Per-trial WM decay toward uniform (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_lr, arb_temp, age_arb_shift, wm_decay = model_parameters
    softmax_beta = beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # RL uncertainty (EMA of squared TD errors)
        v = np.zeros((nS, nA))

        # Effective arbitration temperature (older -> lower)
        arb_temp_eff = arb_temp / (1.0 + age_group * max(0.0, age_arb_shift))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence via entropy (scaled by set size factor)
            eps = 1e-12
            H = -np.sum(W_s * np.log(np.clip(W_s, eps, 1.0)))
            H_max = np.log(float(nA))
            H_norm = H / max(H_max, eps)
            conf_wm = (1.0 - H_norm) * (3.0 / float(nS))

            # RL confidence via inverse of TD-error variance proxy
            v_mean = np.mean(v[s, :])
            conf_rl = 1.0 / (1.0 + v_mean)

            # Arbitration weight via softmax over confidences
            z_wm = np.exp(arb_temp_eff * conf_wm)
            z_rl = np.exp(arb_temp_eff * conf_rl)
            wm_weight_eff = z_wm / (z_wm + z_rl)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and uncertainty update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            v[s, a] = (1.0 - lr) * v[s, a] + lr * (delta ** 2)

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM reward-gated update toward one-hot
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p