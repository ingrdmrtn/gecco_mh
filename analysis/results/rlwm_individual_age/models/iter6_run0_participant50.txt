def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age-dependent WM decay and age-modulated RL temperature.

    Idea
    - Choices are a mixture of model-free RL and a decaying WM store.
    - WM contribution is down-weighted by set size and subject to age-specific decay.
    - RL inverse temperature is modulated by age (older adults may be more/less stochastic).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_base, wm_decay_young, wm_decay_old, beta_age_shift]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_base: baseline WM mixture weight (0..1).
        - wm_decay_young: WM decay rate per trial for younger adults (0..1).
        - wm_decay_old: WM decay rate per trial for older adults (0..1).
        - beta_age_shift: additive shift to beta for older adults (can be negative or positive).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_base, wm_decay_young, wm_decay_old, beta_age_shift = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age-modulated RL temperature
        softmax_beta = (beta_base + beta_age_shift * age_group) * 10.0
        softmax_beta_wm = 50.0  # WM reads out near-deterministically

        # Age-specific WM decay
        wm_decay = wm_decay_old if age_group == 1 else wm_decay_young

        # Set-size scaled WM weight (smaller sets -> higher weight)
        ss_scale = np.clip(3.0 / max(1.0, nS), 0.0, 1.0)
        wm_weight = np.clip(wm_base * ss_scale, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (preserve the template's form)
            Q_s = q[s, :]
            p_rl = 1.0 / (np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))) + eps)

            # WM policy
            W_s = w[s, :]
            # near-deterministic softmax readout
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))) + eps
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then if rewarded store one-hot
            # decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # reward-based overwrite
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-gated mixture, RL forgetting, and set-size-dependent WM noise.

    Idea
    - The mixture weight depends on RL uncertainty (entropy) and an age bias.
    - RL values forget toward uniform over time.
    - WM retrieval is noisy, with noise increasing with set size.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_gain, age_gate_bias, wm_noise, rl_forget]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_gain: gain mapping uncertainty to WM reliance (>=0).
        - age_gate_bias: additive bias pushing older adults toward RL (negative) or WM (positive).
        - wm_noise: base WM noise component (>=0) scaled up with set size.
        - rl_forget: RL forgetting rate toward uniform per trial (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_gain, age_gate_bias, wm_noise, rl_forget = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM noise scale
        ss_noise_multiplier = 1.0 + ((max(3, nS) - 3.0) / 3.0)
        wm_noise_ss = wm_noise * ss_noise_multiplier

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (template form)
            Q_s = q[s, :]
            p_rl = 1.0 / (np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))) + eps)

            # WM policy with retrieval noise: convex mix with uniform
            W_s = w[s, :]
            # compute clean WM softmax
            denom_wm_clean = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))) + eps
            p_wm_clean = 1.0 / denom_wm_clean
            # mix with uniform due to noise
            p_uniform = 1.0 / nA
            # age-bias can affect retrieval indirectly via gate; noise is set-size based
            p_wm = (1.0 - np.clip(wm_noise_ss, 0.0, 1.0)) * p_wm_clean + np.clip(wm_noise_ss, 0.0, 1.0) * p_uniform

            # Uncertainty-gated mixture weight: higher RL entropy -> more WM
            # Derive RL choice probabilities for entropy
            # Using softmax explicitly to evaluate entropy (independent from the template p_rl)
            prefs = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl_full = np.exp(prefs) / (np.sum(np.exp(prefs)) + eps)
            entropy_rl = -np.sum(pi_rl_full * (np.log(pi_rl_full + eps)))
            # Normalize entropy to 0..log(nA), map via sigmoid with age bias
            norm_ent = entropy_rl / np.log(nA)
            gate_input = wm_gain * norm_ent + age_gate_bias * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot store on reward; small decay otherwise
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # light decay toward uniform when no reward
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with directed exploration bonus and precision-gated WM mixture.

    Idea
    - RL policy includes a directed exploration (uncertainty) bonus via counts.
    - WM readout influences choices proportionally to a precision gate that drops with set size and age.
    - WM is a one-shot store that decays lightly when not reinforced.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, kappa_explore, wm_boost, age_wm_drop, ss_wm_drop]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - kappa_explore: strength of directed exploration bonus (>=0).
        - wm_boost: overall WM mixture scaling (0..1).
        - age_wm_drop: drop in WM precision for older adults (0..1).
        - ss_wm_drop: drop in WM precision when set size increases from 3 to 6 (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, kappa_explore, wm_boost, age_wm_drop, ss_wm_drop = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        # WM precision gate: base = wm_boost, reduce with age and set size
        ss_factor = 0.0 if nS <= 3 else 1.0  # 0 for 3, 1 for 6
        precision = wm_boost * (1.0 - age_wm_drop * age_group) * (1.0 - ss_wm_drop * ss_factor)
        wm_weight = np.clip(precision, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # visit counts per state-action for exploration bonus
        n_counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with exploration bonus (UCB-like)
            Q_s = q[s, :]
            bonus = kappa_explore / np.sqrt(n_counts[s, :] + 1.0)
            Q_aug = Q_s + bonus
            # Template-like RL probability for chosen action using augmented values
            p_rl = 1.0 / (np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a]))) + eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))) + eps
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Increment counts after observing action
            n_counts[s, a] += 1.0

            # WM update: store correct action when rewarded; light decay otherwise
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.98 * w[s, :] + 0.02 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)