Here are three distinct cognitive models that combine reinforcement learning (RL) with working memory (WM) in different ways. Each function returns the negative log-likelihood of the observed choices. Age group is used in each model to modulate parameters or arbitration, and set size affects WM availability or interference.

Note: These functions assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with LRU gating and surprise-based arbitration.

    Mechanism
    - RL: tabular Q-learning with softmax. Beta is reduced for older participants.
    - WM: capacity-limited slots (K), dependent on age group (young vs old parameters).
      WM stores rewarded state-action pairs. If capacity is exceeded, least-recently-used (LRU) state is evicted.
      WM policy is highly deterministic (beta_wm=50) for stored states; uniform otherwise.
    - Arbitration: mixture weight equals coverage (stored_states / nS), attenuated by RL surprise |PE| via a sigmoid.
      Intuition: WM relied upon more when RL surprise is low (stable mapping), and when WM has coverage.
    - Age use: older group has its own K; inverse temperature is reduced by age; gating slope applies equally.

    Parameters
    ----------
    states : array-like
        State index per trial within each block (0..nS-1).
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary feedback (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size for the current block (3 or 6).
    age : array-like
        Participant's age repeated across trials. Age group: 0 if <=45 else 1.
    model_parameters : list/tuple
        [lr, beta_base, K_young, K_old, gate_slope, beta_age_scale]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - K_young: WM slot capacity for younger participants (>=1)
        - K_old: WM slot capacity for older participants (>=1)
        - gate_slope: sensitivity of WM arbitration to RL surprise |PE| (>=0)
        - beta_age_scale: fractional reduction of beta for older group (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, K_young, K_old, gate_slope, beta_age_scale = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0 * (1.0 - beta_age_scale * age_group)
    softmax_beta = max(softmax_beta, 1e-6)
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM values (one-hot if stored)
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM baseline when not stored

        # WM storage bookkeeping
        K = int(K_young if age_group == 0 else K_old)
        K = max(1, K)
        stored = np.zeros(nS, dtype=bool)
        recency = np.zeros(nS)  # recency counter: lower -> more recent

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # advance recency
            recency += 1.0

            # RL policy
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: if stored, use the stored one-hot; else use baseline
            if stored[s]:
                W_s = w[s, :]
            else:
                W_s = w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration weight: coverage scaled by surprise gate on RL PE magnitude
            coverage = float(np.sum(stored)) / float(nS) if nS > 0 else 0.0
            pe = r - q[s, a]
            # gate: higher PE -> reduce WM weight; sigmoid in [0,1]
            gate = 1.0 / (1.0 + np.exp(gate_slope * np.abs(pe)))
            wm_weight = np.clip(coverage * gate, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = pe
            q[s, a] += lr * delta

            # WM updating: store rewarded mappings with LRU eviction
            if r > 0.5:
                # if state not stored, ensure capacity
                if not stored[s]:
                    if np.sum(stored) >= K:
                        # evict least recently used among stored
                        idxs = np.where(stored)[0]
                        # pick the one with largest recency (oldest access)
                        evict_state = idxs[np.argmax(recency[idxs])]
                        stored[evict_state] = False
                        w[evict_state, :] = w_0[evict_state, :].copy()
                # store current state-action
                stored[s] = True
                w[s, :] = 0.0
                w[s, a] = 1.0
                recency[s] = 0.0  # most recent

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning, WM with interference-based decay, and reliability arbitration.

    Mechanism
    - RL: Q-learning with separate learning rates for positive and negative outcomes.
      Beta is age-sensitive and reduced for older group. A small lapse emerges for the older group.
    - WM: per-state action probability cache that strengthens on reward and weakens on non-reward,
      with forgetting that increases with set size and with age.
    - Arbitration: WM weight is a sigmoid of (WM confidence - RL uncertainty).
      WM confidence = max(w_s) - 1/nA. RL uncertainty = normalized entropy of RL policy.
    - Age use: beta reduced for older; WM decay effectively stronger for older; lapse added proportional to age.

    Parameters
    ----------
    states : array-like
        State index per trial within each block.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary feedback (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant's age repeated.
    model_parameters : list/tuple
        [lr_pos, lr_neg, beta_base, wm_decay0, arb_w, age_lapse]
        - lr_pos: learning rate for positive outcomes (0..1)
        - lr_neg: learning rate for negative outcomes (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - wm_decay0: base WM decay/interference rate (>=0)
        - arb_w: arbitration slope (>=0)
        - age_lapse: added lapse probability for older group (0..1), scaled by age_group

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_decay0, arb_w, age_lapse = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0 * (1.0 - 0.3 * age_group)  # age reduces beta; fixed fraction to avoid extra param
    softmax_beta = max(softmax_beta, 1e-6)
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax policy
            Q_s = q[s, :].copy()
            # compute RL action distribution for uncertainty estimation
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec /= max(np.sum(p_rl_vec), 1e-12)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy distribution from w[s,:] with deterministic softmax
            W_s = w[s, :].copy()
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(logits_wm)
            p_wm_vec /= max(np.sum(p_wm_vec), 1e-12)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Arbitration: WM confidence vs RL uncertainty (entropy)
            wm_conf = float(np.max(W_s) - (1.0 / nA))  # 0 if uniform, up to (1-1/nA)
            entropy_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            entropy_max = np.log(nA)
            rl_unc = float(entropy_rl / max(entropy_max, 1e-12))  # 0..1
            wm_weight = 1.0 / (1.0 + np.exp(-arb_w * (wm_conf - rl_unc)))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture with lapse (age-dependent)
            epsilon = age_lapse * age_group
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - q[s, a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update with interference-based decay
            # Decay rate increases with set size and age group
            decay = np.exp(-wm_decay0 * max(0.0, float(nS) - 1.0) * (1.0 + 0.5 * age_group))
            # pull w[s,:] toward uniform by (1 - decay)
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            if r > 0.5:
                # move probability mass toward chosen action on reward
                boost = 1.0 - w[s, a]
                w[s, a] += 0.5 * boost  # moderate consolidation
                # renormalize to simple convex combo with uniform to keep smoothness
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]
                w[s, a] = max(w[s, a], 1.0 / nA)
            else:
                # on error, reduce chosen action probability slightly
                w[s, a] = 0.7 * w[s, a] + 0.3 * (1.0 / nA)

            # normalize w[s,:]
            ssum = np.sum(w[s, :])
            if ssum > 1e-12:
                w[s, :] /= ssum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay and WM 'no-go' cache; set-size-driven arbitration.

    Mechanism
    - RL: Q-learning with forgetting (value decay). Age reduces beta (choice noise increases with age).
    - WM: a short-term 'avoidance' cache A[s,a] that stores negative outcomes (r=0) to discourage repeating that action.
      A decays over time with set-size-dependent rate. WM policy is to avoid cached actions via high beta.
    - Arbitration: WM weight is higher for small set sizes (exp(-gamma*(nS-1))) and reduced for larger sets.
      Young receive a small boost in WM reliance (built-in effect), while age primarily reduces RL beta.
    - Age use: beta reduction via age_beta_drop; arbitration boost for young is embedded in formula; decay depends on set size.

    Parameters
    ----------
    states : array-like
        State index per trial within each block.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary feedback (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant's age repeated.
    model_parameters : list/tuple
        [lr, beta_base, q_decay, wm_avoid_gain, gamma_set, age_beta_drop]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - q_decay: per-trial decay of Q-values toward zero (0..1)
        - wm_avoid_gain: scaling of WM 'no-go' aversion (>=0)
        - gamma_set: set-size sensitivity for WM reliance and decay (>=0)
        - age_beta_drop: fractional beta drop for older group (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, q_decay, wm_avoid_gain, gamma_set, age_beta_drop = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0 * (1.0 - age_beta_drop * age_group)
    softmax_beta = max(softmax_beta, 1e-6)
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # unused directly; keep for template consistency
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM 'no-go' avoidance cache
        A = np.zeros((nS, nA))  # higher values mean stronger avoidance of that action in that state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM 'avoid' policy: construct a value vector penalizing avoided actions
            # Convert avoidance A[s,:] into negative utilities
            W_s_vals = -wm_avoid_gain * A[s, :]
            # Map to a softmax policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_vals - W_s_vals[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration weight depends on set size (more for small sets), with a young boost
            base_wm = np.exp(-gamma_set * max(0.0, float(nS) - 1.0))
            # Young participants get a mild proportional boost in WM reliance
            wm_weight = base_wm * (1.0 + 0.2 * (1 - age_group))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay (forgetting)
            q[s, :] = (1.0 - q_decay) * q[s, :]  # decay whole state row slightly
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM 'avoid' update: strengthen avoidance on non-reward, decay otherwise
            avoid_decay = np.exp(-gamma_set * max(0.0, float(nS) - 1.0))
            A[s, :] *= avoid_decay
            if r < 0.5:
                # increase avoidance of chosen action
                A[s, a] = 1.0  # set a strong avoidance tag
            else:
                # on reward, reduce avoidance of chosen action more aggressively
                A[s, a] *= 0.3

        blocks_log_p += log_p

    return -blocks_log_p