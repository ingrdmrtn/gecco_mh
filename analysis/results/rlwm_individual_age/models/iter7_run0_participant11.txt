def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration by state-wise uncertainty with age and set-size effects.

    Mechanism:
    - RL: tabular Q-learning with softmax choice.
    - WM: fast, decaying state-action weights updated by feedback.
    - Arbitration weight is higher when WM is more confident than RL (lower entropy),
      and is penalized by larger set size and by age group.
    - WM decays toward uniform faster in larger sets and slightly faster for older adults.

    Parameters (length 6):
    - lr: RL learning rate (0..1).
    - beta_base: RL inverse temperature before scaling (scaled by 10 internally).
    - wm_decay: base WM decay rate toward uniform per trial (0..1).
    - arb_bias: baseline arbitration bias favoring WM (>0) or RL (<0).
    - arb_sensitivity: weight on confidence difference (WM minus RL) in arbitration.
    - age_arb_penalty: additional arbitration penalty for older group (>=0).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_decay, arb_bias, arb_sensitivity, age_arb_penalty = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay scales up with set size and slightly with age
        decay_eff = np.clip(wm_decay * (nS / 3.0) * (1.0 + 0.25 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            wm_exp = np.exp(wm_logits)
            wm_pol = wm_exp / np.sum(wm_exp)
            p_wm = float(wm_pol[a])

            # Compute confidence (1 - normalized entropy) for RL and WM
            # RL softmax over Q_s
            rl_logits = softmax_beta * Q_s
            rl_logits -= np.max(rl_logits)
            rl_exp = np.exp(rl_logits)
            rl_pol = rl_exp / np.sum(rl_exp)
            eps = 1e-12
            H_rl = -np.sum(rl_pol * np.log(np.clip(rl_pol, eps, 1.0))) / np.log(nA)
            conf_rl = 1.0 - H_rl

            H_wm = -np.sum(wm_pol * np.log(np.clip(wm_pol, eps, 1.0))) / np.log(nA)
            conf_wm = 1.0 - H_wm

            # Arbitration weight: logistic of bias + sensitivity*(conf_wm - conf_rl),
            # with penalties for set size (relative to 3) and age
            setsize_penalty = (max(0, nS - 3))  # 0 for 3, positive for 6
            logit = arb_bias + arb_sensitivity * (conf_wm - conf_rl) - 0.5 * setsize_penalty - age_arb_penalty * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens chosen association; non-reward weakly suppresses chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Rapid move toward one-hot with some inertia
                w[s, :] = 0.4 * w[s, :] + 0.6 * target
            else:
                # Penalize chosen action a bit
                w[s, a] = 0.8 * w[s, a] + 0.2 * w_0[s, a]

            # Global decay toward uniform
            w = (1.0 - decay_eff) * w + decay_eff * w_0

            # Row normalization
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums = np.clip(row_sums, 1e-12, None)
            w = w / row_sums

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM episodic mapping with recall strength and action stickiness.

    Mechanism:
    - RL: separate learning rates for positive and negative prediction errors; softmax choice.
    - Stickiness: bias toward repeating the last action (kappa), stronger for older adults.
    - WM: when rewarded, the state-action pair is written to memory with some probability,
      forming a near-deterministic WM policy for that state; memory strength decays with set size and age.
    - Mixture: WM recall strength for the current state mixes with RL+stickiness policy.

    Parameters (length 6):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - beta_base: RL inverse temperature base (scaled by 10 internally).
    - wm_write_prob: base probability to write to WM on reward (0..1).
    - kappa_stick: base stickiness toward previous action (>=0).
    - age_stick_gain: multiplicative increase in stickiness for older group (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_write_prob, kappa_stick, age_stick_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Memory strength per state (0..1), indicates WM recall probability
        m_strength = np.zeros(nS)

        # Effective stickiness (older adults stick more)
        kappa_eff = kappa_stick * (1.0 + age_stick_gain * age_group)

        # Previous action (for stickiness); initialize uniformly (no bias)
        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax with set-size adjusted temperature (more stochastic in larger set)
            beta_eff = softmax_beta * (3.0 / float(nS))
            Q_s = q[s, :]

            # Add stickiness bias to RL logits
            rl_logits = beta_eff * Q_s
            if prev_action is not None:
                stick_vec = np.zeros(nA)
                stick_vec[prev_action] = 1.0
                rl_logits = rl_logits + kappa_eff * stick_vec

            # Convert to probabilities and read probability of chosen action
            rl_logits -= np.max(rl_logits)
            rl_exp = np.exp(rl_logits)
            rl_pol = rl_exp / np.sum(rl_exp)
            p_rl = float(rl_pol[a])

            # WM near-deterministic policy from w[s,:]
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            wm_exp = np.exp(wm_logits)
            wm_pol = wm_exp / np.sum(wm_exp)
            p_wm = float(wm_pol[a])

            # WM mixture weight given current state's memory strength and set size
            # Stronger decay for larger set and for older adults
            base_m = m_strength[s]
            wm_weight = base_m
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM updates:
            # - On reward: write with probability scaled by set size (harder to encode in larger sets) and age
            # - On no reward: slight decay of memory strength
            write_p = wm_write_prob * (3.0 / float(nS)) * (1.0 - 0.2 * age_group)
            write_p = np.clip(write_p, 0.0, 1.0)

            if r > 0.5:
                # Stochastic write: approximate by expected update to keep differentiability of likelihood
                # Move w[s,:] toward one-hot of action a proportional to write_p
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - write_p) * w[s, :] + write_p * target
                # Increase memory strength
                m_strength[s] = np.clip(m_strength[s] + 0.5 * write_p, 0.0, 1.0)
            else:
                # Penalize chosen action slightly and decay memory strength
                w[s, a] = 0.9 * w[s, a] + 0.1 * w_0[s, a]
                m_strength[s] = np.clip(m_strength[s] * (0.85 - 0.05 * age_group), 0.0, 1.0)

            # Global decay of WM content toward uniform and memory strength decay per trial
            decay_rate = 0.1 * (nS / 3.0) * (1.0 + 0.25 * age_group)
            decay_rate = np.clip(decay_rate, 0.0, 1.0)
            w = (1.0 - decay_rate) * w + decay_rate * w_0
            m_strength = np.clip(m_strength * (1.0 - 0.2 * decay_rate), 0.0, 1.0)

            # Normalize rows of w
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums = np.clip(row_sums, 1e-12, None)
            w = w / row_sums

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and WM subject to cross-state interference.

    Mechanism:
    - RL: Q-learning with eligibility traces across within-state actions, enabling rapid credit assignment.
    - WM: fast, deterministic map updated on reward; subject to interference that grows with set size and age.
    - Mixture: fixed WM strength parameter transformed to a weight, reduced by set size.
    - Age affects interference susceptibility (more interference for older group).

    Parameters (length 6):
    - lr: RL learning rate (0..1).
    - beta_base: RL inverse temperature base (scaled by 10 internally).
    - lambda_trace: eligibility decay (0..1).
    - wm_strength: baseline WM mixture strength (0..1), transformed via logit to weight.
    - interference_base: base interference rate across states (0..1).
    - age_interf_gain: multiplicative gain on interference for older group (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, lambda_trace, wm_strength, interference_base, age_interf_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    # Helper to convert strength (0..1) to weight via logit and set-size penalty
    def wm_weight_from_strength(strength, nS):
        eps = 1e-6
        strength = np.clip(strength, eps, 1 - eps)
        base_logit = np.log(strength) - np.log(1.0 - strength)
        # Penalize larger set size
        logit = base_logit - 0.8 * max(0, nS - 3)
        wmw = 1.0 / (1.0 + np.exp(-logit))
        return np.clip(wmw, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        # Interference rate scales with set size and age
        interf = np.clip(interference_base * (nS / 3.0) * (1.0 + age_interf_gain * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            wm_exp = np.exp(wm_logits)
            wm_pol = wm_exp / np.sum(wm_exp)
            p_wm = float(wm_pol[a])

            # Mixture weight from wm_strength and set size
            wm_weight = wm_weight_from_strength(wm_strength, nS)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Increase trace for chosen (s,a); decay all traces
            e *= lambda_trace
            e[s, :] *= 0.0  # Only within the current state carry forward chosen action's trace
            e[s, a] = 1.0
            pe = r - Q_s[a]
            q += lr * pe * e

            # WM update: reinforce correct mapping on reward; slight unlearning on no reward
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.3 * w[s, :] + 0.7 * target
            else:
                w[s, a] = 0.85 * w[s, a] + 0.15 * w_0[s, a]

            # Cross-state interference: pull other states' WM weights toward uniform
            for s_other in range(nS):
                if s_other != s:
                    w[s_other, :] = (1.0 - interf) * w[s_other, :] + interf * w_0[s_other, :]

            # Normalize rows
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums = np.clip(row_sums, 1e-12, None)
            w = w / row_sums

        blocks_log_p += log_p

    return -blocks_log_p