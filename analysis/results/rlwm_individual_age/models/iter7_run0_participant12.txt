def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with confidence-contrast arbitration modulated by set size and age.

    Idea:
    - Choices arise from a mixture of RL and WM policies.
    - Arbitration weight for WM depends on a confidence contrast between WM and RL:
        c = (max_W - max_Q), promoting WM when WM seems sharper than RL.
    - WM involvement is reduced with larger set sizes and for older adults.
    - WM store is updated in a supervised manner on rewarded trials and decays to uniform after errors.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - wm_gate0: Baseline WM arbitration bias (real)
    - gate_alpha: Gain on confidence contrast (>=0)
    - size_penalty: Reduction of WM weight per item beyond 3 (>=0)
    - age_penalty: Additional WM reduction for older adults (>=0)

    Age and set-size use:
    - wm_weight = sigmoid(wm_gate0 + gate_alpha*(max_W - max_Q) - size_penalty*(nS-3) - age_penalty*age_group)
      where age_group=0 for <=45, =1 for >45.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_gate0, gate_alpha, size_penalty, age_penalty = model_parameters
    softmax_beta = softmax_beta * 10.0
    beta_wm = 50.0  # near-deterministic WM policy

    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Confidence-contrast arbitration, modulated by set size and age
            max_Q = np.max(Q_s)
            max_W = np.max(W_s)
            contrast = max_W - max_Q
            x = wm_gate0 + gate_alpha * contrast - size_penalty * max(0, nS - 3) - age_penalty * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-x))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-supervised strengthening; error-driven decay to uniform
            # Learn and decay rates are shaped by the same gains to keep params used meaningfully.
            learn_rate = 1.0 / (1.0 + np.exp(-gate_alpha))  # in (0,1)
            decay_rate = 1.0 / (1.0 + np.exp(-size_penalty))  # in (0,1)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - learn_rate) * w[s, :] + learn_rate * target
            else:
                w[s, :] = (1 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with PE-dependent meta-learning of learning rate + WM mixture with its own temperature.

    Idea:
    - RL learning rate is modulated on each trial by prediction error magnitude and task demands:
        lr_eff = lr_base * (1 + pe_gain_size*(nS-3)) * (1 - pe_gain_age*age_group) * |PE|
      capturing stronger plasticity under higher load for young adults, and reduced plasticity for older adults.
    - Choices arise from a mixture of RL and WM policies.
    - WM has its own temperature (wm_beta) and a base mixture weight reduced by set size and age.

    Parameters:
    - lr_base: Base RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - pe_gain_age: Factor reducing effective learning rate for older adults (>=0)
    - pe_gain_size: Factor increasing effective learning rate with set size (>=0)
    - wm_weight0: Baseline WM mixture weight before penalties (real, passed through sigmoid)
    - wm_beta: WM inverse temperature (policy precision)

    Age and set-size use:
    - lr_eff depends on set size and age_group as above.
    - wm_weight = sigmoid(wm_weight0 - pe_gain_size*(nS-3) - pe_gain_age*age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, softmax_beta, pe_gain_age, pe_gain_size, wm_weight0, wm_beta = model_parameters
    softmax_beta = softmax_beta * 10.0

    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute constant components for this block
        wm_weight = 1.0 / (1.0 + np.exp(-(wm_weight0 - pe_gain_size * max(0, nS - 3) - pe_gain_age * age_group)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (with its own temperature)
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with PE-dependent meta-learning rate
            pe = r - Q_s[a]
            lr_eff = lr_base * (1.0 + pe_gain_size * max(0, nS - 3)) * (1.0 - pe_gain_age * age_group) * abs(pe)
            lr_eff = float(np.clip(lr_eff, 0.0, 1.0))
            q[s, a] += lr_eff * pe

            # WM update: reward-driven attraction to chosen action; mild decay otherwise
            alpha_wm = 0.3 + 0.4 * abs(pe)  # stronger update when surprising
            alpha_wm = float(np.clip(alpha_wm, 0.0, 1.0))
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - alpha_wm) * w[s, :] + alpha_wm * target
            else:
                w[s, :] = 0.98 * w[s, :] + 0.02 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dynamic inverse temperature (meta-beta) + state-specific rewarded-action WM cache.

    Idea:
    - RL inverse temperature adapts to recent outcomes:
      - After rewards, beta increases (exploitation).
      - After non-rewards, beta decays toward baseline (exploration).
    - Parallel WM cache stores the last rewarded action per state.
      - WM usage probability decreases with set size and with age.
      - If cache exists, WM policy is near-deterministic for that state.

    Parameters:
    - lr: RL learning rate (0..1)
    - beta0: Baseline RL inverse temperature (scaled by 10 internally)
    - beta_learn: Increment to beta after reward (>=0)
    - beta_forget: Fractional decay toward beta0 after non-reward (0..1)
    - wm_recall0: Baseline WM usage bias (passed through sigmoid)
    - size_age_penalty: Penalty per unit of (set size beyond 3 + age_group) on WM usage (>=0)

    Age and set-size use:
    - p_wm_use = sigmoid(wm_recall0 - size_age_penalty * ((nS-3) + age_group))

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta0, beta_learn, beta_forget, wm_recall0, size_age_penalty = model_parameters
    beta0 = beta0 * 10.0  # RL temperature scale
    beta_wm = 50.0  # near-deterministic WM

    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise cache of last rewarded action (-1 means none)
        cache = -1 * np.ones(nS, dtype=int)

        # WM usage probability for this block
        p_wm_use = 1.0 / (1.0 + np.exp(-(wm_recall0 - size_age_penalty * (max(0, nS - 3) + age_group))))

        # Dynamic RL beta
        beta_t = beta0

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy with dynamic beta
            p_rl = 1.0 / np.sum(np.exp(beta_t * (Q_s - Q_s[a])))

            # WM policy
            if cache[s] >= 0:
                W_s = np.zeros(nA)
                W_s[cache[s]] = 1.0
            else:
                W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm_use * p_wm + (1.0 - p_wm_use) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update dynamic beta: reward increases, no-reward decays toward baseline
            if r > 0.5:
                beta_t = beta_t + beta_learn * 10.0  # keep scale consistent with beta0
            else:
                beta_t = beta0 + (1.0 - beta_forget) * (beta_t - beta0)
            beta_t = float(np.clip(beta_t, 1e-3, 500.0))

            # WM updates
            if r > 0.5:
                cache[s] = a
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * one_hot
            else:
                w[s, :] = 0.98 * w[s, :] + 0.02 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p