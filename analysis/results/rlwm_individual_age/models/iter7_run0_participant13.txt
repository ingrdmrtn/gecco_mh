Below are three standalone cognitive models that follow the requested RL+WM template logic. Each returns the negative log-likelihood of the observed choices, uses all parameters meaningfully (â‰¤ 6), and incorporates age group and set-size effects.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + WM with decay that increases with set size.
    
    Mechanism
    - RL: tabular Q-learning with eligibility traces (lambda). Traces allow credit to persist
      within a block, with e decaying after each update. Higher lambda increases recency sensitivity.
    - WM: one-shot storage of rewarded state-action pairs; WM values decay toward uniform on each trial.
      Decay increases with set size (more interference with larger lists).
    - Mixture: choice probability is a convex combination of WM and RL policies.
    
    Set-size effects
    - WM decay increases with set size: decay_eff = wm_decay_base + size_decay_gain * max(0, nS - 3).
    
    Age effects
    - WM mixture weight is reduced for older adults: wm_weight_eff = wm_weight_base * (1 - 0.3*age_group).
      (This uses age meaningfully without adding parameters; the participant here is young, so no reduction.)
    
    Parameters (model_parameters)
    - 0) lr: RL learning rate (0..1)
    - 1) beta_base: base RL inverse temperature; scaled by 10 internally
    - 2) lambda_trace: eligibility trace parameter (0..1)
    - 3) wm_weight_base: base WM mixture weight (0..1)
    - 4) wm_decay_base: base WM decay per trial toward uniform (0..1)
    - 5) size_decay_gain: how much set size > 3 increases WM decay (>=0)
    
    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, lambda_trace, wm_weight_base, wm_decay_base, size_decay_gain = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective WM parameters with set-size and age effects
        wm_decay_eff = wm_decay_base + size_decay_gain * max(0, nS - 3)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)
        wm_weight_eff = wm_weight_base * (1.0 - 0.3 * age_group)

        # Policies
        softmax_beta_wm = 50.0  # near-deterministic WM readout

        # Initialize values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute policies
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - Q_s[a]
            # Update traces: chosen SA gets 1, others decay
            e *= lambda_trace
            e[s, :] *= 0.0
            e[s, a] = 1.0
            # Q update over all SA via traces
            q += lr * delta * e

            # WM update: decay toward uniform, then store on reward
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning rates + capacity-limited WM gating by set size.
    
    Mechanism
    - RL: separate learning rates for positive and negative prediction errors (lr_pos, lr_neg).
    - WM: one-shot storage of rewarded mappings; no decay. WM influence depends on an
      effective capacity K (in slots). If set size nS > K, WM is less likely to govern choice.
    - Mixture: WM mixture weight is scaled by a capacity gate: wm_weight_eff = wm_weight_base * min(1, K_eff / nS).
    
    Set-size effects
    - Capacity depends on set size via K_eff = max(0, K_base - size_K_scale * max(0, nS - 3)).
      Larger set sizes reduce effective WM contribution by lowering K_eff/nS.
    
    Age effects
    - Older adults have 1 fewer effective slot: K_eff -= age_group*1.0 (floored at 0).
      This reduces WM contribution for older participants.
    
    Parameters (model_parameters)
    - 0) lr_pos: learning rate for positive prediction errors (0..1)
    - 1) lr_neg: learning rate for negative prediction errors (0..1)
    - 2) beta_base: base RL inverse temperature; scaled by 10 internally
    - 3) wm_weight_base: base WM mixture weight (0..1)
    - 4) K_base: base WM capacity in slots (>0)
    - 5) size_K_scale: reduction of capacity per extra item over 3 (>=0)
    
    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_weight_base, K_base, size_K_scale = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective capacity as a function of set size and age
        K_eff = K_base - size_K_scale * max(0, nS - 3)
        K_eff = max(0.0, K_eff - 1.0 * age_group)
        gate = min(1.0, K_eff / max(1.0, float(nS)))
        wm_weight_eff = wm_weight_base * gate

        # Policies
        softmax_beta_wm = 50.0  # near-deterministic WM readout

        # Initialize values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM update: store only on rewarded trials
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-directed exploration (UCB-like) + WM with set-size penalty on weight.
    
    Mechanism
    - RL: standard Q-learning for value updates. Policy uses Q augmented by an exploration bonus
      that shrinks with experience: bonus[a] = ucb_bonus / sqrt(N[s,a]+1). This encourages sampling
      of less-visited actions. The bonus affects choice probabilities but not learning updates.
    - WM: one-shot win-stay memory with soft retrieval (finite WM beta). Weight of WM decreases
      with set size (crowding/interference).
    - Mixture: convex combination of RL and WM policies.
    
    Set-size effects
    - WM weight reduced as set size increases: wm_weight_eff = wm_weight_base / (1 + size_wm_penalty * max(0, nS - 3)).
    
    Age effects
    - Older adults receive a smaller exploration bonus: ucb_bonus_eff = ucb_bonus * (1 - 0.5*age_group).
      This reduces uncertainty-driven exploration for older participants.
    
    Parameters (model_parameters)
    - 0) lr: RL learning rate (0..1)
    - 1) beta_base: base RL inverse temperature; scaled by 10 internally
    - 2) wm_weight_base: base WM mixture weight (0..1)
    - 3) wm_beta: WM inverse temperature (retrieval precision)
    - 4) size_wm_penalty: reduction scale of WM weight per extra item over 3 (>=0)
    - 5) ucb_bonus: scale of exploration bonus (>0)
    
    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_weight_base, wm_beta, size_wm_penalty, ucb_bonus = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    ucb_bonus_eff = ucb_bonus * (1.0 - 0.5 * age_group)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size effect on WM weight
        denom = 1.0 + size_wm_penalty * max(0, nS - 3)
        wm_weight_eff = wm_weight_base / max(1e-6, denom)

        # Policies
        softmax_beta_wm = wm_beta  # finite WM precision

        # Initialize values and visit counts
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # counts for UCB-like bonus

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with exploration bonus
            Q_s = q[s, :]
            bonus = ucb_bonus_eff / np.sqrt(N[s, :] + 1.0)
            Q_aug = Q_s + bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL learning update (no bonus in learning)
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Update counts after action
            N[s, a] += 1.0

            # WM update: store rewarded mapping
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p