def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + error-gated WM with load- and age-modulated leak.

    Mechanism:
    - RL: tabular Q-learning (learning rate lr), softmax with inverse temperature softmax_beta*10.
    - WM policy: softmax over a per-state associative table W with high inverse temperature (deterministic).
    - Arbitration: convex mixture of WM and RL policies with a weight that decreases with set size (load).
    - WM update: leaky toward uniform each trial; on each trial, store the chosen action more strongly when
      the unsigned RL prediction error is larger (error gating). Older age increases leak.

    Parameters (list; all used):
    - lr: RL learning rate (0..1).
    - softmax_beta: base inverse temperature for RL (scaled by 10 internally).
    - wm_weight_base: baseline WM mixture weight (0..1), modulated by load.
    - wm_leak: base WM leak toward uniform per trial (0..1).
    - wm_error_gate: sensitivity of WM storage strength to unsigned PE (higher => more error-gated storage).
    - age_leak_bonus: proportional increase in WM leak for older group (>=0).

    Inputs:
    - states: array of state indices per trial (0..set_size-1 within each block).
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set size per trial (3 or 6).
    - age: array with single repeated value; <=45 => young (0), >45 => old (1).
    - model_parameters: list as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_leak, wm_error_gate, age_leak_bonus = model_parameters
    softmax_beta *= 10.0  # higher upper bound as specified
    softmax_beta_wm = 50.0  # very deterministic WM
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by the template formula)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights with high inverse temperature
            W_shift = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * W_shift)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Mixture weight: baseline scaled by load (favor WM in smaller sets)
            load_factor = 3.0 / set_size  # 1.0 for 3, 0.5 for 6
            wm_weight = np.clip(wm_weight_base * load_factor, 0.0, 1.0)

            # Combine policies
            p_total = np.clip(wm_weight * p_wm + (1.0 - wm_weight) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Leak toward uniform, scaled by load and age
            leak_eff = np.clip(wm_leak * (set_size / 3.0) * (1.0 + age_leak_bonus * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # 2) Error-gated storage toward a one-hot of the chosen action
            # Gate increases with unsigned prediction error
            gate = 1.0 / (1.0 + np.exp(-wm_error_gate * (abs(delta) - 0.5)))
            target = np.zeros(nA)
            target[a] = 1.0
            w[s, :] = (1.0 - gate) * w[s, :] + gate * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with win-stay strengthening and load-driven interference.

    Mechanism:
    - RL: tabular Q-learning with softmax choice.
    - WM policy: deterministic softmax over W. WM encodes recent rewarded actions strongly (win-stay),
      and under non-reward, it pushes mass slightly away from the chosen action (lose-shift lite).
    - Arbitration: mixture weight decreases with load; young participants rely slightly more on WM than older.
    - WM decay/interference: leak toward uniform increases with set size (interf_gain) and age.

    Parameters (list; all used):
    - lr: RL learning rate.
    - softmax_beta: base inverse temperature for RL (scaled by 10 internally).
    - wm_weight_base: baseline WM mixture weight (0..1).
    - wm_decay_base: base WM decay/leak per trial (0..1).
    - win_stay_gain: strength of WM update toward one-hot on rewarded trials (0..1+).
    - interf_gain: scales how much leak increases from set size 3 to 6 (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, wm_decay_base, win_stay_gain, interf_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_shift = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * W_shift)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Arbitration: baseline WM weight, reduced by load; older rely a bit less on WM
            load_factor = 3.0 / set_size
            age_factor = 1.0 - 0.2 * age_group  # 20% reduction in WM reliance for older
            wm_weight = np.clip(wm_weight_base * load_factor * age_factor, 0.0, 1.0)

            p_total = np.clip(wm_weight * p_wm + (1.0 - wm_weight) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay/interference: leak grows with set size and age
            leak_scale_load = 1.0 + interf_gain * ((set_size - 3.0) / 3.0)  # 1.0 at 3, 1+interf at 6
            leak_scale_age = 1.0 + 0.5 * age_group
            leak_eff = np.clip(wm_decay_base * leak_scale_load * leak_scale_age, 0.0, 1.0)
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            # WM win-stay / lose-shift-lite update
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - win_stay_gain) * w[s, :] + win_stay_gain * target
            else:
                # push probability slightly away from the chosen (distribute to others)
                away = np.ones(nA) / (nA - 1)
                away[a] = 0.0
                lose_push = 0.25 * win_stay_gain  # smaller than win-stay
                w[s, :] = (1.0 - lose_push) * w[s, :]
                w[s, :] += lose_push * away

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and age-shifted bias.

    Mechanism:
    - RL: tabular Q-learning with softmax (beta scaled by 10).
    - WM policy: softmax over WM weights with high inverse temperature.
    - Arbitration: WM is favored when its policy is more certain (lower entropy) than RL's; a sigmoid
      transforms the entropy difference into a weight, with additional baseline, load penalty, and age bias.
    - WM update: reward-based storage toward a one-hot; leaky toward uniform with greater leak under higher load and in older age.

    Parameters (list; all used):
    - lr: RL learning rate.
    - softmax_beta: base inverse temperature for RL (scaled by 10).
    - wm_weight_base: baseline WM weight before arbitration signals (in logit space).
    - entropy_slope: gain converting (H_RL - H_WM) into WM weight (logit space).
    - wm_leak: base WM leak toward uniform per trial.
    - age_mix_bias: additive bias (logit space) that reduces WM reliance in older participants.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, entropy_slope, wm_leak, age_mix_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    def softmax_prob(vec, beta):
        v = vec - np.max(vec)
        p = np.exp(beta * v)
        return p / np.sum(p)

    def entropy(p):
        p_clip = np.clip(p, eps, 1.0)
        return -np.sum(p_clip * np.log(p_clip))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template scalar prob for chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Full RL policy vector for entropy calculation
            pi_rl = softmax_prob(Q_s, softmax_beta)

            # WM policy vector and chosen prob
            pi_wm = softmax_prob(W_s, softmax_beta_wm)
            p_wm = max(pi_wm[a], eps)

            # Entropy-based arbitration
            H_rl = entropy(pi_rl)
            H_wm = entropy(pi_wm)
            # Logit for WM weight: baseline + entropy difference + load penalty + age bias
            load_penalty = -1.0 * (set_size / 3.0 - 1.0)  # 0 at size 3, -1 at size 6
            logit_w = wm_weight_base + entropy_slope * (H_rl - H_wm) + load_penalty + age_mix_bias * age_group
            wm_weight = np.clip(sigmoid(logit_w), 0.0, 1.0)

            # Mixture
            p_total = np.clip(wm_weight * p_wm + (1.0 - wm_weight) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leak scaled by load and age, then reward-based storage
            leak_eff = np.clip(wm_leak * (set_size / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # learning strength proportional to certainty advantage of WM
                adv = sigmoid(entropy_slope * (H_rl - H_wm))
                w[s, :] = (1.0 - adv) * w[s, :] + adv * target

        blocks_log_p += log_p

    return -blocks_log_p