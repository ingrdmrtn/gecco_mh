Here are three alternative cognitive models tailored to the RL-WM task. Each adheres to the requested template, uses the participantâ€™s age group and set size meaningfully, and returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and WM with size- and age-modulated fidelity.

    Mechanism:
    - RL: tabular Q-learning with asymmetric effective learning rate for negative outcomes.
    - WM: a cache that stores the most recently rewarded action per state as a peaked distribution,
      but its fidelity suffers in larger set sizes; young participants have sharper WM focus.
    - Arbitration: convex mixture of RL and WM using a global wm_weight.

    Parameters (6):
    - model_parameters = [lr, wm_weight, softmax_beta, lr_neg_mult, size6_wm_penalty, age_wm_bonus]
      lr: baseline learning rate for RL in [0,1]
      wm_weight: mixture weight for WM (0..1)
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      lr_neg_mult: multiplier (0..1) applied to lr when reward = 0 (reduces learning from negative outcomes)
      size6_wm_penalty: (0..1) mixing toward uniform for WM when set size is 6 (interference)
      age_wm_bonus: (0..1) additional sharpening of WM for young group; reduced/none for old

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, lr_neg_mult, size6_wm_penalty, age_wm_bonus = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent interference in WM
        interf = float(size6_wm_penalty) if nS == 6 else 0.0
        interf = float(np.clip(interf, 0.0, 1.0))

        # Age-dependent sharpening of WM (young sharpen more)
        young_bonus = (1.0 - age_group) * age_wm_bonus  # age_group=0 => +bonus; age_group=1 => 0

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference and age-dependent sharpening
            W_s = w[s, :]
            # Interference from set size
            W_eff = (1.0 - interf) * W_s + interf * w_0[s, :]

            # Age sharpening: blend toward one-hot on current WM-best (if any peak exists)
            best = int(np.argmax(W_eff))
            one_hot = np.zeros(nA)
            one_hot[best] = 1.0
            W_eff = (1.0 - young_bonus) * W_eff + young_bonus * one_hot

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric negative learning
            lr_eff = lr if r > 0 else lr * np.clip(lr_neg_mult, 0.0, 1.0)
            delta = r - Q_s[a]
            q[s, a] += lr_eff * delta

            # WM updating:
            # - Rewarded: write one-shot cache for the chosen action.
            # - Unrewarded: decay slightly toward uniform (mild forgetting).
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                decay_unrewarded = 0.2 + 0.3 * interf  # stronger decay when set size = 6
                w[s, :] = (1.0 - decay_unrewarded) * w[s, :] + decay_unrewarded * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM with uncertainty-guided arbitration and WM decay; age biases arbitration.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: stores last rewarded action per state; traces decay toward uniform on every visit.
    - Arbitration: trial-wise WM weight increases when RL is uncertain (high entropy) and when set size is small.
      Age biases arbitration toward WM for younger participants.

    Parameters (6):
    - model_parameters = [lr, wm_weight, softmax_beta, arb_slope, age_arb_bias, wm_decay]
      lr: RL learning rate in [0,1]
      wm_weight: baseline WM mixture weight (0..1)
      softmax_beta: RL inverse temperature (rescaled by *10 internally)
      arb_slope: sensitivity of arbitration to RL uncertainty and set size (positive => more WM when uncertain and small set)
      age_arb_bias: additive bias toward WM for young (positive) vs old (reduced/negative)
      wm_decay: per-visit WM decay toward uniform [0,1]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, arb_slope, age_arb_bias, wm_decay = model_parameters

    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL policy and its entropy (uncertainty)
            expv = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl_vec = expv / np.sum(expv)
            p_rl = pi_rl_vec[a]
            H_rl = -np.sum(pi_rl_vec * np.log(np.clip(pi_rl_vec, eps, 1.0)))
            H_max = np.log(nA)
            rel_uncertainty = H_rl / H_max  # 0..1

            # WM policy from cached traces
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: more WM when RL is uncertain and set size is small (nS=3),
            # plus age bias favoring WM for young participants.
            size_term = (6 - nS) / 3.0  # =1 for nS=3, =0 for nS=6
            age_bias = age_arb_bias * (1.0 - age_group)  # only boosts for young
            wm_weight_trial = wm_weight + arb_slope * (rel_uncertainty * size_term) + age_bias
            wm_weight_trial = float(np.clip(wm_weight_trial, 0.0, 1.0))

            p_total = wm_weight_trial * p_wm + (1.0 - wm_weight_trial) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay on each visit, then reward-based refresh if r>0
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and episodic-like WM counts; age modulates RL temperature.

    Mechanism:
    - RL: Q-learning with eligibility traces for the currently visited state-action pair.
      Past chosen action in a state retains an eligibility that decays via lambda.
    - WM: maintains reward counts per state-action (episodic familiarity); converted to a sharp policy.
      Larger set sizes induce interference by mixing WM counts toward uniform.
    - Arbitration: fixed mixture weight between RL and WM.
    - Age: scales RL inverse temperature (older -> lower beta if age_beta_scale<1).

    Parameters (6):
    - model_parameters = [lr, wm_weight, softmax_beta, trace_lambda, size_interference, age_beta_scale]
      lr: RL learning rate in [0,1]
      wm_weight: mixture weight for WM (0..1)
      softmax_beta: baseline RL inverse temperature (rescaled by *10 internally)
      trace_lambda: eligibility decay (0..1)
      size_interference: WM interference amount applied when set size=6 (0..1)
      age_beta_scale: multiplicative factor on beta for age groups:
                      beta_eff = beta * (1 + (1 - age_group) * (age_beta_scale - 1))

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, trace_lambda, size_interference, age_beta_scale = model_parameters

    base_beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    # Age-modulated RL temperature: young get scaled by age_beta_scale, old remain at base (or vice versa depending on scale)
    beta_eff_mult = 1.0 + (1.0 - age_group) * (age_beta_scale - 1.0)
    beta_eff = base_beta * beta_eff_mult

    softmax_beta_wm = 50.0
    eps = 1e-12
    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # eligibility traces per state-action (reset each block)
        e = np.zeros((nS, nA))

        # WM episodic counts (initialize to uniform pseudo-counts)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        interf = float(size_interference) if nS == 6 else 0.0
        interf = float(np.clip(interf, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy from episodic counts with interference toward uniform
            W_s = w[s, :]
            W_eff = (1.0 - interf) * W_s + interf * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with eligibility trace:
            # Decay all eligibilities
            e *= trace_lambda
            # Set current state-action eligibility to 1 (replacing traces)
            e[s, :] *= 0.0
            e[s, a] = 1.0
            # TD error
            delta = r - Q_s[a]
            # Update Q values according to eligibility
            q += lr * delta * e

            # WM counts update: increment only if rewarded, mild decay otherwise
            if r > 0:
                # Add a unit count and renormalize to keep within [0,1] scale
                w[s, a] += 1.0
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # Mild decay toward uniform when unrewarded
                decay = 0.1 + 0.2 * interf
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size effects:
- Model 1: WM fidelity is degraded by set-size interference (size6_wm_penalty) and sharpened for young participants (age_wm_bonus). RL learning is asymmetric, reducing sensitivity to negative outcomes via lr_neg_mult.
- Model 2: Arbitration is dynamic: when RL is uncertain and the set size is small (3), the model leans more on WM. Younger participants receive an age-driven WM bias via age_arb_bias. WM decays continuously via wm_decay.
- Model 3: WM is episodic/familiarity-based and suffers interference at set size 6. RL employs eligibility traces to capture short-run perseverance within states. Age modulates RL temperature via age_beta_scale, reflecting sharper exploitation for young participants.