def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with decay.

    Idea:
    - RL: standard Rescorla-Wagner learning on Q-values.
    - WM: deterministic one-shot mapping per state that is capacity-limited.
      The effective WM contribution scales with min(1, K_eff / set_size), where
      K_eff = K_base - K_age_delta * age_group. WM traces decay toward uniform.
    - Arbitration: convex mixture between WM and RL policies with trial-invariant
      base weight scaled by the capacity ratio.

    Parameters
    ----------
    states : array-like
        State index per trial (0..set_size-1).
    actions : array-like
        Observed actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6), constant within a block.
    age : array-like
        Participant age repeated per trial. Age group: 0 if <=45, else 1.
    model_parameters : list
        [lr, softmax_beta, wm_weight_base, K_base, K_age_delta, wm_decay]
        - lr: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL (scaled x10 internally).
        - wm_weight_base: baseline arbitration weight for WM (0..1).
        - K_base: baseline WM capacity (in number of items).
        - K_age_delta: reduction of capacity for older group (>=0).
        - wm_decay: per-trial decay of WM toward uniform (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, K_base, K_age_delta, wm_decay = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity given age
        K_eff = max(0.0, K_base - K_age_delta * age_group)
        cap_ratio = min(1.0, K_eff / max(1.0, float(nS)))
        wm_weight_block = np.clip(wm_weight_base * cap_ratio, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (deterministic softmax)
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: rewarded trials create a one-shot mapping; otherwise decay
            if r >= 0.5 and pe > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # decay toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recency-gated WM.

    Idea:
    - RL: standard Q-learning.
    - WM: stores one-shot associations when rewarded; decays toward uniform.
    - Arbitration: WM influence is scaled by how recently the current state was seen.
      wm_weight_t = wm_weight_base * exp(-time_since_last_state / tau_eff),
      where tau_eff increases with age and set size (older adults and larger sets
      spread recency, thus reduce the recency penalty more slowly).

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Observed actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial. Age group: 0 if <=45, else 1.
    model_parameters : list
        [lr, softmax_beta, wm_weight_base, tau_base, age_tau_delta, setsize_tau_slope]
        - lr: RL learning rate.
        - softmax_beta: inverse temperature for RL (scaled x10).
        - wm_weight_base: baseline WM arbitration weight (0..1).
        - tau_base: base recency time constant.
        - age_tau_delta: added time constant for older adults (>=0).
        - setsize_tau_slope: added time constant per unit increase from 3 to 6.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, tau_base, age_tau_delta, setsize_tau_slope = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Recency time constant adapted by age and set size (0 for 3, 1 for 6)
        setsize_factor = max(0.0, (nS - 3) / 3.0)
        tau_eff = max(1e-3, tau_base + age_tau_delta * age_group + setsize_tau_slope * setsize_factor)

        # Track last visit time for each state
        last_visit = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Recency-based WM weight
            if last_visit[s] < 0:
                recency_weight = 1.0  # first time seen, rely more on WM if any
            else:
                dt = max(0, t - last_visit[s])
                recency_weight = np.exp(-dt / tau_eff)
            wm_weight_t = np.clip(wm_weight_base * recency_weight, 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: decay with a rate linked to tau_eff, and encode on reward
            decay = 1.0 - np.exp(-1.0 / tau_eff)
            if r >= 0.5 and pe > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Update last visit
            last_visit[s] = t

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + error-sensitivity arbitration for WM.

    Idea:
    - RL: standard Q-learning.
    - WM: one-shot mapping with error-contingent decay (faster decay after errors).
    - Arbitration: WM weight is a sigmoid of a linear combination of base bias,
      age and set-size shifts, and the state's running error trace:
        wm_weight_t = sigmoid(b0 + b_age*age_group + b_size*setsize_factor - b_err*err_trace[s])
      where err_trace is an exponentially decayed unsigned PE trace per state.
      Larger recent errors reduce WM reliance; older age and larger set sizes
      shift the sigmoid to reduce WM usage overall.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Observed actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial. Age group: 0 if <=45, else 1.
    model_parameters : list
        [lr, softmax_beta, b0, b_err, b_age, b_size]
        - lr: RL learning rate.
        - softmax_beta: inverse temperature for RL (scaled x10).
        - b0: base bias for WM usage (logit space).
        - b_err: sensitivity to recent unsigned errors (>=0).
        - b_age: shift for older adults (>=0 reduces WM if positive).
        - b_size: shift for larger set size (>=0 reduces WM if positive).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, b0, b_err, b_age, b_size = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1

    beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state running unsigned PE trace for arbitration and WM decay control
        err_trace = np.zeros(nS)
        # Error trace decay factor: larger sets maintain errors longer
        setsize_factor = max(0.0, (nS - 3) / 3.0)  # 0 for 3, 1 for 6
        gamma_err = 0.7 + 0.2 * setsize_factor  # between 0.7 and 0.9

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Update error trace for arbitration (based on last state's PE after seeing r)
            pe_unsigned = abs(r - Q_s[a])
            err_trace[s] = gamma_err * err_trace[s] + (1.0 - gamma_err) * pe_unsigned

            # Logistic arbitration
            logit = b0 - b_err * err_trace[s] + b_age * age_group + b_size * setsize_factor
            wm_weight_t = 1.0 / (1.0 + np.exp(-logit))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: encode on reward; after errors, decay faster
            if r >= 0.5 and pe > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Error-contingent decay: more recent errors -> stronger decay
                decay = 0.1 + 0.6 * err_trace[s]  # in [0.1, 0.7]
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)