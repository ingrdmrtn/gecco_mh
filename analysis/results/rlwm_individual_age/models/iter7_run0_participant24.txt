def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with load-dependent decay and age-modulated RL temperature.

    Core ideas:
    - RL uses asymmetric learning rates (positive vs negative outcomes).
    - WM stores recent correct actions per state, but decays toward uniform with set-size-dependent leak.
    - WM update is gated by a load- and age-sensitive gate parameter.
    - Arbitration uses WM confidence (sharpness of W_s).
    - Age modulates the RL inverse temperature (beta).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - beta_base: base inverse temperature for RL choice policy (scaled by 10 internally)
    - age_beta_shift: additive shift in beta for older vs younger (applied as beta_base + age_beta_shift*age_group)
    - wm_gate: WM update gate strength; higher means more likely/stronger WM updates (0..1)
    - wm_decay: baseline WM decay (leak) toward uniform; scaled up by set size (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, beta_base, age_beta_shift, wm_gate, wm_decay = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta = (beta_base + age_beta_shift * age_group) * 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size factor scales processes from 0 at size=3 to 1 at size=6
        setsize_factor = max(nS - 3, 0) / 3.0
        # WM leak increases with set size
        wm_leak = np.clip(wm_decay * setsize_factor, 0.0, 1.0)
        # Age-sensitive WM gate: older group effectively updates less
        wm_gate_eff = np.clip(wm_gate * (1.0 - 0.4 * age_group) * (1.0 - 0.5 * setsize_factor), 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # WM confidence based arbitration
            wm_conf = np.max(W_s) - (1.0 / nA)
            wm_conf = np.clip(wm_conf / (1.0 - 1.0 / nA), 0.0, 1.0)
            wm_weight = np.clip(wm_gate_eff * wm_conf, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM global decay toward uniform (interference)
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM update on rewarded trials, gated
            if r > 0.5 and wm_gate_eff > 0.0:
                # Push w[s] toward the chosen action a
                w[s, :] = (1.0 - wm_gate_eff) * w[s, :]
                w[s, a] += wm_gate_eff
                # Renormalize
                w[s, :] = w[s, :] / np.sum(w[s, :])

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and load/age-dependent WM noise.

    Core ideas:
    - RL learns Q via single learning rate and softmax with inverse temperature beta.
    - WM is a fast, reward-gated memory trace that is corrupted by interference (noise),
      which increases with set size and with age.
    - Arbitration favors WM when RL is uncertain (high entropy), and favors RL when RL is confident.
    - WM noise reduces WM precision by mixing with uniform policy.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: base RL inverse temperature (scaled by 10 internally)
    - wm_eta: WM learning rate on rewarded trials (0..1)
    - wm_noise_base: base WM noise/interference level (0..1)
    - age_noise_boost: additional WM noise for older group (>=0)
    - entropy_slope: controls how strongly RL entropy shifts arbitration toward WM (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_eta, wm_noise_base, age_noise_boost, entropy_slope = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    beta = beta_base * 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        setsize_factor = max(nS - 3, 0) / 3.0
        # WM noise increases with set size and with age
        wm_noise = np.clip(wm_noise_base + setsize_factor * wm_noise_base + age_group * age_noise_boost, 0.0, 1.0)
        # Leak proportional to noise
        wm_leak = np.clip(0.5 * wm_noise, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM base policy from W_s
            W_s_raw = w[s, :].copy()
            wm_logits = beta_wm * (W_s_raw - np.max(W_s_raw))
            exp_wm = np.exp(wm_logits)
            pi_wm_base = exp_wm / np.sum(exp_wm)

            # Apply WM noise by mixing with uniform distribution
            pi_wm = (1.0 - wm_noise) * pi_wm_base + wm_noise * (1.0 / nA)
            p_wm = max(pi_wm[a], 1e-12)

            # RL entropy as uncertainty signal
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            # Arbitration: higher RL entropy -> higher WM weight
            wm_weight = 1.0 / (1.0 + np.exp(-entropy_slope * (H_rl - np.log(nA) / 2.0)))
            # Reduce arbitration toward WM under high set size
            wm_weight = np.clip(wm_weight * (1.0 - 0.5 * setsize_factor), 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay/global interference
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM learning on reward
            if r > 0.5 and wm_eta > 0.0:
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
                w[s, :] = w[s, :] / np.sum(w[s, :])

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with meta-control gating via relative reliability and load/age-dependent lapses.

    Core ideas:
    - RL with age-specific inverse temperatures (beta_y for young, beta_o for old).
    - WM updated on reward; both systems' recent reliability is tracked online from prediction errors.
    - Arbitration weight is proportional to relative reliability of WM vs RL, then reduced by set size.
    - Final policy includes a lapse component that increases with set size and with age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_y: RL inverse temperature for young (scaled by 10 internally)
    - beta_o: RL inverse temperature for old (scaled by 10 internally)
    - wm_eta: WM learning rate on rewarded trials (0..1)
    - lapse_base: base lapse rate (0..1) mixed with uniform policy
    - load_lapse_gain: added lapse per unit set-size factor and per age group (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_y, beta_o, wm_eta, lapse_base, load_lapse_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = (beta_y if age_group == 0 else beta_o) * 10.0
    beta_wm = 50.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        setsize_factor = max(nS - 3, 0) / 3.0
        # WM leak increases with load
        wm_leak = 0.2 * setsize_factor

        # Initialize running reliabilities
        R_rl = 0.5
        R_wm = 0.5
        k_rel = 0.2  # smoothing for reliability updates

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # Arbitration via relative reliability, reduced by load
            wm_weight_raw = R_wm / (R_wm + R_rl + 1e-8)
            wm_weight = np.clip(wm_weight_raw * (1.0 - 0.5 * setsize_factor), 0.0, 1.0)

            # Mixture policy before lapse
            pi_mix_a = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            pi_mix_a = max(pi_mix_a, 1e-12)

            # Lapse increases with load and age
            lapse = np.clip(lapse_base + load_lapse_gain * (setsize_factor + 0.5 * age_group), 0.0, 1.0)
            p_total = (1.0 - lapse) * pi_mix_a + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform (interference)
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM learning on reward
            if r > 0.5 and wm_eta > 0.0:
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
                w[s, :] = w[s, :] / np.sum(w[s, :])

            # Reliability updates:
            # RL reliability increases when prediction error magnitude is small
            rel_rl_inst = 1.0 - min(abs(pe), 1.0)
            R_rl = (1.0 - k_rel) * R_rl + k_rel * rel_rl_inst

            # WM reliability increases when WM assigns high prob to chosen action and outcome matches
            # Proxy: if rewarded, high pi_wm[a] is good; if not, high (1-pi_wm[a]) is good
            rel_wm_inst = pi_wm[a] if r > 0.5 else (1.0 - pi_wm[a])
            R_wm = (1.0 - k_rel) * R_wm + k_rel * rel_wm_inst

    return nll

Notes on age and load effects:
- Model1: Age reduces RL temperature via age_beta_shift and reduces WM gate effectiveness. WM decay scales with set size.
- Model2: WM noise increases with set size and is further boosted by age; arbitration from RL entropy is dampened under load.
- Model3: Age-specific RL temperature; lapse increases with set size and age; reliability-based arbitration is weakened by load.