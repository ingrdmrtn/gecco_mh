def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated WM with shared decay and age-modulated temperature.

    Core idea:
    - RL learns with a standard delta rule.
    - WM updates are gated by surprise and reward, with stronger gating in low set size.
    - Both RL and WM traces decay toward an uninformative prior at a rate controlled by a shared decay parameter.
    - Arbitration uses the same surprise-derived gate as a dynamic WM weight.
    - Younger participants have effectively higher inverse temperature (more exploitative).

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, beta_base, wm_beta, wm_gate_surprise, decay, age_temp_shift]
        - lr: RL learning rate (0..1).
        - beta_base: baseline inverse temperature (>0), scaled by 10 internally.
        - wm_beta: WM inverse temperature (>0).
        - wm_gate_surprise: sensitivity of WM gate to surprise/reward and set size.
        - decay: shared decay toward uniform for both Q and W (0..1).
        - age_temp_shift: temperature scaling for older group (>=0). Applied as exp(-age_temp_shift*age_group).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_beta, wm_gate_surprise, decay, age_temp_shift = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = wm_beta
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective temperature reduced for older participants
        beta_eff = softmax_beta * np.exp(-age_temp_shift * age_group)
        beta_wm_eff = softmax_beta_wm  # WM is precise but parameterized

        # Initialize values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        q_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Surprise signal and WM gate (favor WM more in small set size and on rewarded trials)
            pe = abs(r - Q_s[a])  # 0..1
            size_scale = (nS - 3) / 3.0  # 0 for 3, 1 for 6
            # Gate increases with reward and surprise, decreases with larger set size and older age
            gate_input = (r - 0.5) + pe - 0.5 * size_scale - 0.2 * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-wm_gate_surprise * gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # Update RL with decay toward uniform
            delta = r - Q_s[a]
            q = (1.0 - decay) * q + decay * q_0
            q[s, a] += lr * delta

            # Update WM with gate-driven delta and shared decay
            w = (1.0 - decay) * w + decay * w_0
            w[s, a] += wm_weight * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Probabilistic WM slots with lapse and age-dependent capacity, mixed with RL.

    Core idea:
    - WM has a limited number of slots; probability that the current state's mapping is in WM
      is phi = min(1, slots_eff / nS). slots_eff is reduced in older participants.
    - Arbitration is a fixed mixture: phi * WM + (1-phi) * RL, wrapped with a lapse that increases
      with set size and age.
    - RL updates with a delta rule; WM encodes one-shot on rewarded trials and weakly adjusts on errors.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, beta, wm_beta, slots, lapse_base, age_lapse]
        - lr: RL learning rate (0..1).
        - beta: inverse temperature for RL (>0), scaled by 10 internally.
        - wm_beta: inverse temperature for WM (>0).
        - slots: WM capacity in slots (>=0).
        - lapse_base: base lapse probability at set size 3 (0..1).
        - age_lapse: multiplicative increase of lapse for older group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta, wm_beta, slots, lapse_base, age_lapse = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = wm_beta
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective WM capacity (older -> fewer effective slots)
        slots_eff = max(0.0, slots * (1.0 - 0.3 * age_group))
        phi = min(1.0, slots_eff / float(nS))  # probability state is in WM

        # Lapse increases with set size and age
        lapse = lapse_base * (nS / 3.0) * (1.0 + age_lapse * age_group)
        lapse = np.clip(lapse, 0.0, 0.5)

        # Initialize values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (prob of chosen action)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = phi * p_wm + (1.0 - phi) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot on rewarded trials, mild correction on errors
            if r > 0.5:
                # move strongly toward chosen action being correct
                w[s, :] = 0.0 * w[s, :]
                w[s, a] = 1.0
            else:
                # weakly decrease confidence in chosen action
                w[s, a] = 0.9 * w[s, a]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Entropy-based arbitration: WM vs RL weighted by relative uncertainty, with size-dependent WM decay.

    Core idea:
    - Compute RL and WM choice distributions; use their entropies to arbitrate.
      If RL is uncertain (high entropy) and WM is confident (low entropy), favor WM, and vice versa.
    - Age shifts the arbitration bias toward RL (older) or WM (younger).
    - Set size increases WM decay, reducing WM reliability for larger sets.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, beta, wm_beta, k_entropy, age_bias, size_bias]
        - lr: RL learning rate (0..1).
        - beta: RL inverse temperature (>0), scaled by 10 internally.
        - wm_beta: WM inverse temperature (>0).
        - k_entropy: slope for arbitration based on entropy difference (>=0).
        - age_bias: additive bias in arbitration toward RL for older group (>=0).
        - size_bias: controls WM decay increase with set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta, wm_beta, k_entropy, age_bias, size_bias = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = wm_beta
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Size-dependent WM decay toward uniform (stronger decay for nS=6)
        base_decay = 1.0 / (1.0 + np.exp(-size_bias * (nS - 4.5)))  # ~low for 3, high for 6
        wm_decay = np.clip(base_decay, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            # Full RL softmax distribution
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            p_rl_full = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_full[a]  # prob of chosen action

            # WM policy
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            p_wm_full = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_full[a]

            # Entropy-based arbitration
            eps = 1e-12
            H_rl = -np.sum(p_rl_full * np.log(p_rl_full + eps)) / np.log(nA)  # normalized 0..1
            H_wm = -np.sum(p_wm_full * np.log(p_wm_full + eps)) / np.log(nA)
            # Positive when WM is more certain (H_rl - H_wm > 0)
            arb_input = (H_rl - H_wm)
            # Age bias pushes older group toward RL (reduce wm_weight)
            arb_input -= age_bias * age_group
            wm_weight = 1.0 / (1.0 + np.exp(-k_entropy * arb_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and update
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            w[s, a] += (r - w[s, a])  # fast WM correction on observed outcome

        blocks_log_p += log_p

    return -blocks_log_p