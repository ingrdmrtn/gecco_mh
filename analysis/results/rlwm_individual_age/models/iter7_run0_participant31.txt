def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age- and set-sizeâ€“dependent interference.

    Description:
    - RL: Q-learning with asymmetric learning rates for positive vs negative prediction errors.
    - WM: A capacity-limited store that maintains near-deterministic action policies per state,
      but suffers interference and retrieval noise that scale with set size and age group.
    - Arbitration: Mixture of WM and RL policies. The WM mixture weight scales with the
      ratio of WM capacity to set size, and degrades more strongly with larger set sizes
      and in the older age group (via age_interf_mult).

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single repeated value). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr_pos, lr_neg, beta_base, wm_capacity, wm_retrieval_noise, age_interf_mult]
        - lr_pos: RL learning rate for positive PE.
        - lr_neg: RL learning rate for negative PE.
        - beta_base: Base inverse temperature for RL (scaled by 10 internally).
        - wm_capacity: Effective WM slots available (continuous, ~0..6).
        - wm_retrieval_noise: Scales WM temperature with load and age.
        - age_interf_mult: Scales how much age penalizes WM mixture weight with load.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_capacity, wm_retrieval_noise, age_interf_mult = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute block-level effective WM parameters
        softmax_beta = max(beta_base, 1e-6) * 10.0
        # WM retrieval temperature increases with load and age
        wm_beta = 50.0 / (1.0 + wm_retrieval_noise * (nS / 3.0) * (1.0 + 0.5 * age_group))
        wm_beta = max(wm_beta, 1.0)

        # Capacity factor and mixture weight penalized by load and age
        cap_frac = np.clip(wm_capacity / float(nS), 0.0, 1.0)
        wm_weight_eff = cap_frac * (3.0 / nS) ** (1.0 + age_interf_mult * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # WM decay increases as capacity is exceeded and with load/age
        base_decay = (1.0 - cap_frac)
        wm_decay = np.clip(base_decay * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy likelihood of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            # WM policy likelihood of chosen action
            p_wm = 1.0 / np.sum(np.exp(wm_beta * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM decay toward uniform (interference)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding: stronger move toward chosen action when rewarded
            enc_gain = 0.7 if r > 0.0 else 0.3
            w[s, :] = (1.0 - enc_gain) * w[s, :]
            w[s, a] += enc_gain
            # Normalize to maintain a distribution
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility-like WM choice kernel.

    Description:
    - RL: Standard Q-learning with a single learning rate.
    - WM: A choice kernel that accumulates evidence for recently rewarded actions
      using a decaying eligibility trace. The kernel acts like a fast, labile WM that
      highlights recently rewarded actions for each state.
    - Arbitration: Weighted mixture of WM and RL policies; WM mixture weight decreases
      with set size and is further reduced in the older group via an age shift.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single repeated value). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_base, wm_weight_base, trace_lambda, choice_kernel_gain, age_weight_shift]
        - lr: RL learning rate.
        - beta_base: Base inverse temperature for RL (scaled by 10 internally).
        - wm_weight_base: Base weight of WM in arbitration before load/age penalties.
        - trace_lambda: Decay factor for WM eligibility traces (0..1).
        - choice_kernel_gain: Increment toward chosen action on rewarded trials.
        - age_weight_shift: Reduces WM mixture weight in the older group.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_weight_base, trace_lambda, choice_kernel_gain, age_weight_shift = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM kernel represented as nonnegative strengths per action per state
        m = np.zeros((nS, nA))
        # Policies derived from normalized m
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        softmax_beta = max(beta_base, 1e-6) * 10.0
        softmax_beta_wm = 50.0

        # Arbitration weight: decreases with set size, reduced further in older group
        wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - age_weight_shift * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update WM eligibility trace globally (forgetting across trials)
            m *= np.clip(trace_lambda, 0.0, 1.0)

            # Derive WM policy distribution for current state by normalizing m
            # Add a small anchor to uniform to avoid degenerate distributions
            w[s, :] = m[s, :] + 1e-6
            w[s, :] /= np.sum(w[s, :])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM kernel update: strengthen chosen action only when rewarded
            if r > 0.0:
                # Move kernel toward chosen action in this state
                m[s, a] += choice_kernel_gain
            # Keep a minimal anchor to uniform via w_0 when kernel is very weak
            if np.sum(m[s, :]) <= 1e-8:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-gated WM and age-dependent lapse.

    Description:
    - RL: Q-learning with single learning rate and its own inverse temperature.
    - WM: Fast-updating, high-temperature policy table with decay; separate inverse
      temperature from RL to capture sharp WM-based choices.
    - Arbitration: WM mixture weight increases when RL is uncertain early in learning,
      using a simple visit-count uncertainty proxy; WM weight decreases with set size
      and age. A lapse component increases with set size and age.
    
    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single repeated value). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_rl, beta_wm, wm_weight_base, uncert_gate, age_lapse_gain]
        - lr: RL learning rate.
        - beta_rl: Inverse temperature for RL policy (scaled by 10 internally).
        - beta_wm: Inverse temperature for WM policy (scaled by 10 internally).
        - wm_weight_base: Baseline WM mixture weight before uncertainty/load/age effects.
        - uncert_gate: Strength of uncertainty (low visit count) in increasing WM weight.
        - age_lapse_gain: Adds lapse probability with age and load.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_rl, beta_wm, wm_weight_base, uncert_gate, age_lapse_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Visit counts to approximate RL uncertainty
        N = np.zeros((nS, nA))

        softmax_beta_rl = max(beta_rl, 1e-6) * 10.0
        softmax_beta_wm = max(beta_wm, 1e-6) * 10.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta_rl * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL uncertainty proxy based on visit count for the state
            visits_s = np.sum(N[s, :]) + 1e-6
            u_s = 1.0 / (1.0 + visits_s)  # high when state is rarely seen

            # Arbitration: base WM weight scaled by load and age, boosted by uncertainty
            wm_weight = wm_weight_base * (3.0 / nS) * (1.0 - 0.3 * age_group) * (1.0 + uncert_gate * u_s)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Lapse increases with load and in older group
            lapse = np.clip((0.02 + age_lapse_gain * age_group) * (nS / 3.0), 0.0, 0.49)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Add lapse to uniform policy
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)

            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform depends on load and age
            wm_decay = np.clip(0.15 * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.9)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding toward the chosen action (stronger if rewarded)
            enc = 0.6 if r > 0.0 else 0.25
            w[s, :] = (1.0 - enc) * w[s, :]
            w[s, a] += enc
            w[s, :] /= np.sum(w[s, :])

            # Update visit counts
            N[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p