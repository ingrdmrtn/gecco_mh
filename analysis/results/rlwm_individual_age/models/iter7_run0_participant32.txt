def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(α, β) with eligibility traces, mixed with capacity-limited WM recall.
    WM recall probability scales as capacity/set_size and is reduced in older adults.
    WM stores recent rewarded action (one-shot-like) and decays otherwise.
    
    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight0, capacity, age_penalty, lambda_et]
        - alpha: RL learning rate
        - softmax_beta: base inverse temperature for RL (scaled internally)
        - wm_weight0: baseline WM mixture weight
        - capacity: WM slots capacity (in items)
        - age_penalty: multiplicative penalty to WM recall for older group
        - lambda_et: eligibility trace mixing for TD(λ)-style bootstrap on chosen action
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, softmax_beta, wm_weight0, capacity, age_penalty, lambda_et = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    # Older group has lower effective recall probability via age_penalty
    age_recall_scale = 1.0 if age_group == 0 else (1.0 - np.clip(age_penalty, 0.0, 0.9))

    softmax_beta_wm = 50.0  # very deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and WM contents per state-action
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # simple eligibility that favors staying on chosen action values
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with current beta
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM recall probability based on capacity and set size, modulated by age
            p_recall = np.clip((capacity / max(1.0, nS)) * age_recall_scale, 0.0, 1.0)

            # WM policy: if recalled, use near-deterministic softmax over WM contents; else uniform
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_recalled = 1.0 / max(denom_wm, 1e-12)
            p_wm_unrecalled = 1.0 / nA
            p_wm = p_recall * p_wm_recalled + (1.0 - p_recall) * p_wm_unrecalled

            # Mixture weight (constant across trials here)
            wm_weight_eff = np.clip(wm_weight0, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with simple TD error on chosen action, eligibility concentrates on chosen pair
            delta = r - Q_s[a]
            e *= lambda_et
            e[s, :] *= 0.0
            e[s, a] += 1.0
            q += alpha * delta * e

            # WM updating: one-shot strengthening on rewarded trials, decay toward uniform otherwise
            # Consolidation strength increases when set size <= capacity
            eta_wm = np.clip(capacity / max(1.0, nS), 0.0, 1.0)
            if r > 0.0:
                # Move W_s toward one-hot for chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * one_hot
            else:
                # Mild decay toward uniform when not rewarded
                decay = 0.2 * (1.0 - eta_wm)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Normalize WM row to avoid drift
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-gated WM mixture: WM influence increases on surprising outcomes,
    WM contents decay over time and are refreshed by reward. Set-size reduces WM weight.
    
    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, beta_base, wm_weight0, surprise_gain, decay_wm, age_wm_penalty]
        - alpha: RL learning rate
        - beta_base: RL inverse temperature (scaled internally)
        - wm_weight0: baseline WM mixture weight
        - surprise_gain: scales trial-wise increase in WM weight with |PE|
        - decay_wm: WM decay rate per trial toward uniform
        - age_wm_penalty: multiplicative reduction of WM weight for older adults
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta_base, wm_weight0, surprise_gain, decay_wm, age_wm_penalty = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM reads

    # Apply age penalty to WM influence (older -> less WM)
    wm_age_scale = 1.0 if age_group == 0 else (1.0 - np.clip(age_wm_penalty, 0.0, 0.9))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy with set-size–independent beta here; set size acts via WM weight
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from current WM table
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Surprise-gated WM mixture weight
            delta = r - Q_s[a]
            surprise = abs(delta)
            # Set-size penalty: larger set -> lower WM weight
            set_penalty = 1.0 + max(0, nS - 3)
            wm_weight_eff = wm_weight0 * wm_age_scale * np.exp(np.clip(surprise_gain * surprise, -10, 10)) / set_penalty
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += alpha * delta

            # WM decay and reward-based refresh
            # Global decay toward uniform at each step (capacity-free)
            w = (1.0 - decay_wm) * w + decay_wm * w_0
            if r > 0.0:
                # Reward refresh: push chosen action up in the current state
                reinforce = 0.6  # fixed strong write
                w[s, :] = (1.0 - reinforce) * w[s, :]
                w[s, a] += reinforce
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with lapse noise and WM binding swap errors that grow with set size.
    WM policy is a softmax over a corrupted memory: a mixture of the target state's WM
    and other states' WM (swap), with swap probability increasing with set size and age.
    
    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, beta_base, wm_weight, interference_set_gain, lapse_base, age_beta_penalty]
        - alpha: RL learning rate
        - beta_base: base RL inverse temperature (scaled internally)
        - wm_weight: baseline WM mixture weight
        - interference_set_gain: scales WM swap probability with set size
        - lapse_base: base lapse probability mixed uniformly into final policy
        - age_beta_penalty: reduces RL inverse temperature in older adults
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta_base, wm_weight, interference_set_gain, lapse_base, age_beta_penalty = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    if age_group == 1:
        # Older adults show lower effective RL precision
        softmax_beta *= max(0.1, 1.0 - np.clip(age_beta_penalty, 0.0, 0.9))

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM swap/interference: observed state's WM is corrupted by mixing with others
            if nS > 1:
                others = [i for i in range(nS) if i != s]
                mean_other_w = np.mean(w[others, :], axis=0)
            else:
                mean_other_w = w[s, :]

            # Swap probability increases with set size and is higher for older adults
            load = max(0, nS - 3) / 3.0  # 0 at 3, 1 at 6
            p_swap = np.clip(interference_set_gain * load * (1.2 if age_group == 1 else 1.0), 0.0, 0.5)

            W_eff = (1.0 - p_swap) * w[s, :] + p_swap * mean_other_w
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mix WM and RL
            mix = np.clip(wm_weight, 0.0, 1.0)
            p_mix = mix * p_wm + (1.0 - mix) * p_rl

            # Lapse with set-size dependence; older adults slightly higher lapse
            lapse = np.clip(lapse_base * (1.0 + load) * (1.15 if age_group == 1 else 1.0), 0.0, 0.5)
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: deterministic strengthening on reward; mild decay otherwise
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                strengthen = 0.7
                w[s, :] = (1.0 - strengthen) * w[s, :] + strengthen * one_hot
            else:
                decay = 0.15 + 0.15 * load  # more decay under higher load
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p