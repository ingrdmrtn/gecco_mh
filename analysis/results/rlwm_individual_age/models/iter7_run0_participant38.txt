def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration and age/set-size effects on WM reliability and RL temperature.

    Mechanisms:
    - RL: vanilla Q-learning with inverse temperature scaled by age and set size.
    - WM: stores state-action associations; when rewarded, WM is sharpened; otherwise it decays toward uniform.
    - Arbitration: weight of WM increases when RL is uncertain (high entropy) and when WM reliability is high.
      WM reliability decreases with set size and for older adults.
    - Age and set size:
      - Older adults and larger set sizes reduce WM reliability.
      - Larger set sizes (and older adults modestly) reduce effective RL temperature (more noise).

    Parameters (model_parameters):
    - lr: learning rate for RL (0..1)
    - beta_base: base inverse temperature for RL; internally scaled by x10 (>=0)
    - wm_conf_base: baseline WM reliability (0..1)
    - age_wm_penalty: penalty factor applied to WM reliability for older adults (>=0)
    - entropy_slope: slope for mapping RL entropy to WM weight via a sigmoid (>=0)
    - setsize_temp_penalty: reduction in RL inverse temperature per unit increase in set size over 3 (>=0)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial (constant within block)
    - age: array with single repeated value (years)
    - Returns: negative log-likelihood of observed choices
    """
    lr, beta_base, wm_conf_base, age_wm_penalty, entropy_slope, setsize_temp_penalty = model_parameters

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])
        nA = 3

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL temperature adjusted by set size and age
        # Larger sets and older adults slightly reduce beta (i.e., increase noise)
        beta_eff = beta_base - setsize_temp_penalty * max((nS - 3), 0) / 3.0 - 0.1 * age_group
        beta_eff = max(beta_eff, 0.0) * 10.0

        # WM reliability baseline for the block (reduced by set size and age)
        # Map into [0,1]: higher is more reliable WM
        wm_reliability = wm_conf_base / (1.0 + 0.5 * max((nS - 3), 0) + age_wm_penalty * age_group)
        wm_reliability = np.clip(wm_reliability, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy: softmax probability of chosen action
            Q_s = q[s, :]
            exp_rl = np.exp(beta_eff * (Q_s - np.max(Q_s)))
            probs_rl = exp_rl / max(np.sum(exp_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)

            # WM policy: sharp softmax with very high beta
            beta_wm = 50.0
            W_s = w[s, :]
            exp_wm = np.exp(beta_wm * (W_s - np.max(W_s)))
            probs_wm = exp_wm / max(np.sum(exp_wm), 1e-12)
            p_wm = max(probs_wm[a], 1e-12)

            # RL uncertainty via entropy (0..log nA), normalize to 0..1
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, 1e-12, 1.0)))
            H_rl_norm = H_rl / np.log(nA)

            # Arbitration: more WM weight when RL is uncertain and WM is reliable
            # Use sigmoid on (entropy - (1 - wm_reliability)) as decision variable
            dv = H_rl_norm - (1.0 - wm_reliability)
            wm_weight = 1.0 / (1.0 + np.exp(-entropy_slope * dv))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy and log-likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update:
            # - If rewarded: sharpen toward chosen action (one-shot encoding)
            # - If not rewarded: decay toward uniform (forgetting)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strength of sharpening scaled by wm_reliability
                w[s, :] = (1.0 - wm_reliability) * w[s, :] + wm_reliability * one_hot
            else:
                # Decay rate increases with set size and age
                decay = 0.1 + 0.2 * max((nS - 3), 0) / 3.0 + 0.2 * age_group
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and action perseveration; WM with limited capacity/interference.
    
    Mechanisms:
    - RL: Q-learning with eligibility traces (lambda), softmax action selection.
    - Perseveration: bias toward repeating previous action in a state; strength increases with age.
    - WM: rewarded actions are stored; capacity/interference increases with set size and age (faster decay).
    - Arbitration: fixed WM weight per block derived from capacity parameter (reduced by set size and age).

    Parameters (model_parameters):
    - lr: learning rate (0..1)
    - beta: inverse temperature for RL; internally scaled by x10 (>=0)
    - lambda_tr: eligibility trace decay (0..1)
    - pers_base: base perseveration strength added to chosen action (>=0)
    - cap_strength: baseline WM influence/capacity (0..1)
    - age_cap_penalty: penalty on WM capacity and boost to perseveration for older adults (>=0)

    Inputs/Outputs:
    - Same as other models; returns negative log-likelihood.
    """
    lr, beta, lambda_tr, pers_base, cap_strength, age_cap_penalty = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])
        nA = 3

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        # State-wise last action for perseveration (initialize to None -> no bias)
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective beta
        beta_eff = max(beta, 0.0) * 10.0

        # WM capacity weight reduced by set size and age
        wm_weight = cap_strength / (1.0 + 0.5 * max((nS - 3), 0) + age_cap_penalty * age_group)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        # Perseveration strength increased with age
        pers = pers_base * (1.0 + 0.5 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias implemented as a softmax bias vector
            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += pers
            # Compute biased softmax
            logits = beta_eff * Q_s + bias
            logits -= np.max(logits)
            exp_rl = np.exp(logits)
            probs_rl = exp_rl / max(np.sum(exp_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)

            # WM policy
            beta_wm = 50.0
            W_s = w[s, :]
            exp_wm = np.exp(beta_wm * (W_s - np.max(W_s)))
            probs_wm = exp_wm / max(np.sum(exp_wm), 1e-12)
            p_wm = max(probs_wm[a], 1e-12)

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            pe = r - q[s, a]
            # Decay all traces
            e *= lambda_tr
            # Increment chosen trace
            e[s, a] += 1.0
            # Update all Q-values via traces
            q += lr * pe * e

            # WM update with capacity/interference:
            # Reward strengthens WM; non-reward induces interference-dependent decay to uniform.
            if r > 0.5:
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                # Strength of learning also limited by capacity (same factor as wm_weight)
                strength = wm_weight
                w[s, :] = (1.0 - strength) * w[s, :] + strength * one_hot
            else:
                # Interference grows with set size and age
                decay = 0.15 + 0.25 * max((nS - 3), 0) / 3.0 + 0.25 * age_group
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w0[s, :]

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learned step size from volatility; WM one-shot with decay; epsilon-lapse.

    Mechanisms:
    - RL: learning rate is adapted online by a simple volatility estimate per state.
      Volatility increases with recent absolute PE changes; higher volatility increases LR.
    - WM: one-shot learning on reward; decays each visit. Decay increases with set size and age.
    - Arbitration: weight of WM equals its memory strength for the current state
      (difference between max and min of WM weights). Stronger memory -> more WM.
    - Lapse: with probability epsilon, choice is random (applied after arbitration).
    - Age and set size:
      - Older adults and larger set sizes increase WM decay.
      - Volatility threshold for boosting LR increases with set size and age (harder to upregulate LR).

    Parameters (model_parameters):
    - lr_base: base RL learning rate (0..1)
    - beta: inverse temperature for RL; internally scaled by x10 (>=0)
    - meta_vol_sens: sensitivity of LR to volatility (>=0)
    - wm_decay_rate: base WM decay per visit (0..1)
    - age_decay_boost: multiplier on WM decay for older adults (>=0)
    - lapse: epsilon lapse rate mixing with uniform random (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, beta, meta_vol_sens, wm_decay_rate, age_decay_boost, lapse = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        nS = int(set_sizes[mask][0])
        nA = 3

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Volatility and previous PE per state for meta-learning LR
        vol = np.zeros(nS)
        prev_pe = np.zeros(nS)

        beta_eff = max(beta, 0.0) * 10.0

        # Volatility threshold to upregulate LR grows with set size and age
        vol_threshold = 0.2 + 0.2 * max((nS - 3), 0) / 3.0 + 0.2 * age_group

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            exp_rl = np.exp(beta_eff * (Q_s - np.max(Q_s)))
            probs_rl = exp_rl / max(np.sum(exp_rl), 1e-12)
            p_rl = max(probs_rl[a], 1e-12)

            # WM policy
            beta_wm = 50.0
            W_s = w[s, :]
            exp_wm = np.exp(beta_wm * (W_s - np.max(W_s)))
            probs_wm = exp_wm / max(np.sum(exp_wm), 1e-12)
            p_wm = max(probs_wm[a], 1e-12)

            # WM strength-based arbitration weight (0..1)
            wm_strength = np.max(W_s) - np.min(W_s)
            wm_weight = np.clip(wm_strength, 0.0, 1.0)

            # Mixture before lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            # Lapse mixing with uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Compute PE for RL update
            pe = r - q[s, a]

            # Update volatility (EMA of |Î”PE|)
            delta_pe = np.abs(pe - prev_pe[s])
            # Update with moderate smoothing
            vol[s] = 0.8 * vol[s] + 0.2 * delta_pe
            prev_pe[s] = pe

            # Meta-learned effective learning rate via sigmoid of (vol - threshold)
            lr_gain = 1.0 / (1.0 + np.exp(-meta_vol_sens * (vol[s] - vol_threshold)))
            lr_eff = lr_base * lr_gain
            q[s, a] += lr_eff * pe

            # WM update:
            if r > 0.5:
                one_hot = np.zeros(nA); one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                pass  # no strengthening on non-reward

            # Apply WM decay each visit; stronger decay with set size and age
            decay = wm_decay_rate * (nS / 3.0) * (1.0 + age_decay_boost * age_group)
            decay = np.clip(decay, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p