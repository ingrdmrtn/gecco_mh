def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + entropy-adaptive temperature + WM with set-size/age-dependent decay

    Mechanism:
    - RL: tabular Q-learning with forgetting toward uniform baseline.
    - Softmax temperature adapts to state-level value entropy: more certainty (low entropy) â†’ higher beta.
      Age moderates this adaptation (younger adapt more strongly).
    - WM: one-shot storage of last rewarded action per state; decays toward uniform faster with larger set size and for older group.
    - Mixture: policy = wm_weight_eff * WM + (1 - wm_weight_eff) * RL.

    Parameters (total 5):
    - lr: RL learning rate for the chosen action (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta_base: base inverse temperature for RL; internally scaled by 10.
    - beta_entropy_gain: gain controlling how much RL beta increases as entropy decreases (>=0).
    - wm_decay_base: baseline WM decay rate per trial (0..1).

    Age and set size effects:
    - Beta adaptation gain is reduced for older group: gain_eff = beta_entropy_gain * (1 - 0.5*age_group).
    - WM decay increases with set size and age: wm_decay = wm_decay_base * (1 + 0.5*age_group) * (1 + 0.2*(nS-1)).
    """
    lr, wm_weight, softmax_beta_base, beta_entropy_gain, wm_decay_base = model_parameters
    softmax_beta_base *= 10.0

    # Determine age group
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective parameters per block
        gain_eff = beta_entropy_gain * (1.0 - 0.5 * age_group)
        wm_decay = wm_decay_base * (1.0 + 0.5 * age_group) * (1.0 + 0.2 * max(0, nS - 1))
        wm_weight_eff = wm_weight / (1.0 + 0.2 * max(0, nS - 1))  # reduced WM influence in larger sets

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with entropy-adaptive temperature
            Q_s = q[s, :]
            # Normalize Q to probabilities via softmax-like to compute entropy proxy
            # Convert to a proper distribution using a fixed softmax temperature for entropy proxy
            q_shift = Q_s - np.max(Q_s)
            probs = np.exp(q_shift)
            probs = probs / np.sum(probs)
            entropy = -np.sum(probs * (np.log(probs + 1e-12)))
            entropy_norm = entropy / np.log(nA)

            beta_eff = softmax_beta_base * (1.0 + gain_eff * (1.0 - entropy_norm))
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy (deterministic readout of current WM state)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            # Forgetting for all actions in this state toward uniform baseline
            q[s, :] = (1.0 - 0.02) * q[s, :] + 0.02 * (1.0 / nA)

            # WM update: decay toward uniform, then store on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Normalize to keep numerical stability
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with differential forgetting of unchosen actions + inhibitory WM on negative feedback

    Mechanism:
    - RL: tabular Q-learning; after each trial, values of unchosen actions in the current state
      are pulled toward uniform baseline (differential forgetting), encouraging exploration.
    - WM: On positive feedback, store the chosen action deterministically (one-shot).
           On negative feedback, apply an inhibitory tag to the chosen action (reduce its WM probability),
           with age-dependent boost to inhibition. WM influence slightly downweights with larger set size.
    - Mixture: policy = wm_weight_eff * WM + (1 - wm_weight_eff) * RL.

    Parameters (total 6):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight: baseline WM mixture weight (0..1).
    - inhibit_strength: base inhibitory strength applied to WM on negative outcomes (>=0).
    - unchosen_forget: rate pulling unchosen Q-values toward uniform in the visited state (0..1).
    - age_inhib_boost: multiplicative boost of inhibition for older group (>=0).

    Age and set size effects:
    - Inhibition strength is scaled by (1 + age_inhib_boost * age_group).
    - WM weight is reduced in larger set sizes: wm_weight_eff = wm_weight / (1 + 0.3*(nS-1)).
    """
    lr, softmax_beta, wm_weight, inhibit_strength, unchosen_forget, age_inhib_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_weight / (1.0 + 0.3 * max(0, nS - 1))
        inhib_eff = inhibit_strength * (1.0 + age_inhib_boost * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            # Differential forgetting for unchosen actions in current state
            for aa in range(nA):
                if aa != a:
                    q[s, aa] = (1.0 - unchosen_forget) * q[s, aa] + unchosen_forget * (1.0 / nA)

            # WM update
            if r > 0.5:
                # Reward: store chosen action deterministically
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # No reward: apply inhibitory tag to the chosen action
                w[s, a] = max(1e-8, w[s, a] * (1.0 - inhib_eff))
                # Renormalize remaining mass to other actions
                total = np.sum(w[s, :])
                if total <= 1e-12:
                    w[s, :] = w_0[s, :]
                else:
                    w[s, :] = w[s, :] / total

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + logistic arbitration of WM by set size and age

    Mechanism:
    - RL: tabular with separate learning rates for positive and negative prediction errors.
    - WM: fast store of last rewarded action per state; no explicit decay, but overwritten on subsequent rewards.
    - Arbitration: WM mixture weight is a logistic function of set size with an age-dependent threshold shift.
      Larger set sizes reduce WM contribution depending on threshold; older group shifts the threshold.
    - Policy mixture: policy = wm_weight_eff * WM + (1 - wm_weight_eff) * RL.

    Parameters (total 6):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight0: maximum WM mixture weight at very small set sizes (0..1).
    - gate_threshold: logistic threshold on set size where WM and RL contribute equally.
    - age_gate_shift: additive shift to the threshold for older group (can be positive to make WM drop earlier in older group).

    Arbitration:
    - wm_weight_eff = wm_weight0 / (1 + exp(k*(nS - threshold_eff))), with k=1.5 and threshold_eff = gate_threshold + age_gate_shift*age_group.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight0, gate_threshold, age_gate_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    k = 1.5  # slope of logistic

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        threshold_eff = gate_threshold + age_gate_shift * age_group
        wm_weight_eff = wm_weight0 / (1.0 + np.exp(k * (nS - threshold_eff)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM update: overwrite on reward; otherwise keep prior WM (no explicit decay)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Small leakage toward uniform to avoid numerical lock-in
                w[s, :] = 0.99 * w[s, :] + 0.01 * w_0[s, :]
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p