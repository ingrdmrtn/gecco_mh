Below are three standalone cognitive models that follow the RLâ€“WM template, each using age group and set size in distinct, meaningful ways. Each function returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dual learning-rates + confidence-gated WM that decays and is weakened by age and load.

    Mechanism:
    - RL: Dual learning rates for positive/negative prediction errors; inverse temperature beta scaled up internally.
    - WM: For each state, a directional memory table w and a scalar confidence m[s] (0..1).
      Reward increases m[s] and pulls w[s] to a one-hot code of the chosen action; no-reward causes a mild diffusion.
      Confidence decays over time and is penalized by age and set size (load).
    - Arbitration: Effective WM weight is wm_weight0 * m[s] * exp(-penalty * load), where
      load = age_group + max(nS-3,0)/3. Thus older age and larger sets reduce reliance on WM.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - beta_base: Base RL inverse temperature; internally scaled by 10.
    - wm_weight0: Base arbitration weight for WM (0..1).
    - wm_decay: Per-trial WM confidence decay (0..1).
    - load_penalty: Strength of age/load penalty on WM (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_weight0, wm_decay, load_penalty = model_parameters
    softmax_beta = beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # deterministic WM readout when confidence is high
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        m = np.zeros(nS)  # WM confidence per state

        # Age/load term
        load = age_group + max(nS - 3, 0) / 3.0
        wm_load_scale = np.exp(-load_penalty * load)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy with confidence-modulated temperature
            beta_wm_eff = softmax_beta_wm * max(m[s], 1e-6)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Arbitration with confidence and load penalties
            wm_weight_eff = wm_weight0 * wm_load_scale * m[s]
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with dual learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM confidence decay (load-independent base decay every trial)
            m[s] = (1.0 - wm_decay) * m[s]
            m[s] = float(np.clip(m[s], 0.0, 1.0))

            # WM update
            if r > 0:
                # encode chosen action strongly and boost confidence
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * onehot
                row_sum = w[s, :].sum()
                if row_sum > 0:
                    w[s, :] = w[s, :] / row_sum
                # increase confidence with diminishing returns; bounded by load scaling
                m[s] = float(np.clip(m[s] + (1.0 - m[s]) * 0.5, 0.0, 1.0))
            else:
                # no reward: diffuse toward uniform
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
                row_sum = w[s, :].sum()
                if row_sum > 0:
                    w[s, :] = w[s, :] / row_sum
                # slight confidence penalty on error
                m[s] = float(np.clip(m[s] * 0.9, 0.0, 1.0))

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility-trace-inspired WM trace and dynamic arbitration via trace strength.

    Mechanism:
    - RL: Single learning rate; inverse temperature beta scaled by 10.
    - WM: A Hebbian-like trace z[s,a] (0..1) for the current state's chosen action that decays over time.
      z increases on selection (more if rewarded), and decays each trial with a load- and age-dependent factor.
      WM policy reads out from a normalized table w that follows z.
    - Arbitration: wm_weight is a logistic function of trace strength for current state, with a penalty from age/load.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - beta_base: Base RL inverse temperature; internally scaled by 10.
    - trace_gain: Increment to trace on selection (scaled by reward) (>=0).
    - trace_decay: Base decay per-trial for trace (0..1).
    - arb_slope: Slope of logistic mapping from trace to WM weight (>=0).
    - load_penalty: Age/load penalty scaling (>=0) that reduces both RL temperature and increases trace decay.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, trace_gain, trace_decay, arb_slope, load_penalty = model_parameters
    softmax_beta = beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM tables derived from a trace z
        z = np.zeros((nS, nA))  # unnormalized trace
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load operationalization
        load = age_group + max(nS - 3, 0) / 3.0

        # RL temperature becomes lower under load/age
        beta_eff = softmax_beta * np.exp(-0.5 * load_penalty * load)

        # Trace decay increases with load
        decay_eff = np.clip(trace_decay + (load_penalty * load) * 0.2, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy from normalized trace table
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Arbitration via logistic on local trace energy
            z_strength = z[s, a] - np.mean(z[s, :])
            wm_weight = 1.0 / (1.0 + np.exp(-arb_slope * z_strength))
            wm_weight *= np.exp(-load_penalty * load)
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Trace decay (global)
            z[s, :] = (1.0 - decay_eff) * z[s, :]

            # Trace increment on chosen action; reward amplifies the increment
            inc = trace_gain * (1.0 + 0.5 * r)
            z[s, a] += inc
            z[s, :] = np.clip(z[s, :], 0.0, 1.0)

            # Update WM table w to reflect trace proportions (local normalization)
            row_sum = z[s, :].sum()
            if row_sum > 1e-8:
                w[s, :] = z[s, :] / row_sum
            else:
                # fallback to slight drift to uniform when traces are empty
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                w[s, :] /= w[s, :].sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-adaptive step size + Bayesian WM with resource-limited precision and uncertainty arbitration.

    Mechanism:
    - RL: Learning rate is modulated by surprise (|PE|). Base beta scaled by 10.
    - WM: Dirichlet-like counts c[s,a] (>= prior) define a categorical distribution per state.
      A limited resource R is allocated across states; larger set size and older age reduce R, lowering WM precision.
      WM readout uses softmax with a precision proportional to kappa[s] = R/nS.
    - Arbitration: Based on relative uncertainty (entropy) between RL and WM at the current state using a sigmoid;
      larger load increases WM uncertainty and shifts arbitration to RL.

    Parameters (model_parameters):
    - lr0: Base RL learning rate (0..1).
    - beta_base: Base RL inverse temperature; internally scaled by 10.
    - wm_eta: Increment to WM counts after reward; small diffusion otherwise (>=0).
    - resource_base: Total WM resource available per block (>0).
    - resource_slope: Degree to which age/load reduces resource (>=0).
    - uncert_slope: Slope for arbitration based on entropy difference (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr0, beta_base, wm_eta, resource_base, resource_slope, uncert_slope = model_parameters
    softmax_beta = beta_base * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM counts initialized with a weak symmetric prior
        prior_count = 0.5
        c = prior_count * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Resource allocation: reduced by age and set size
        load = age_group + max(nS - 3, 0) / 3.0
        R = resource_base / (1.0 + resource_slope * load + 1e-8)
        kappa = max(R / max(nS, 1), 1e-6)  # per-state precision proxy

        # RL temperature modestly penalized by load
        beta_eff = softmax_beta * np.exp(-0.3 * resource_slope * load)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # Current WM distribution from counts
            c_row = c[s, :]
            p_row = c_row / max(c_row.sum(), 1e-8)
            w[s, :] = p_row

            # RL policy
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy with precision scaled by kappa
            beta_wm_eff = softmax_beta_wm * kappa
            denom_wm = np.sum(np.exp(beta_wm_eff * (w[s, :] - w[s, a])))
            p_wm = 1.0 / denom_wm

            # Uncertainty (entropy) for arbitration
            # RL entropy (softmax over Q with beta_eff)
            q_logits = beta_eff * (Q_s - np.max(Q_s))
            pQ = np.exp(q_logits)
            pQ /= max(pQ.sum(), 1e-8)
            H_rl = -np.sum(pQ * np.log(np.clip(pQ, 1e-12, 1.0)))

            # WM entropy from p_row
            pW = np.clip(p_row, 1e-12, 1.0)
            pW = pW / pW.sum()
            H_wm = -np.sum(pW * np.log(pW))

            # Arbitration: more WM weight when WM is more certain than RL
            diff = H_rl - H_wm
            wm_weight = 1.0 / (1.0 + np.exp(-uncert_slope * diff))
            # Additional penalty from load to favor RL under age/load
            wm_weight *= np.exp(-resource_slope * load)
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update with surprise-adaptive rate
            pe = r - Q_s[a]
            lr_eff = lr0 * (0.5 + 0.5 * min(1.0, abs(pe)))  # 0.5*lr0..lr0 depending on surprise
            q[s, a] += lr_eff * pe

            # WM update of counts
            if r > 0:
                # reinforce chosen action more strongly
                c[s, a] += wm_eta
            else:
                # mild diffusion toward uniform when not rewarded
                c[s, :] = 0.95 * c[s, :] + 0.05 * w_0[s, :]

            # Keep counts non-negative
            c[s, :] = np.maximum(c[s, :], 1e-8)

        blocks_log_p += log_p

    return -blocks_log_p