def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM arbitration with age/load-dependent WM slots and confidence-based gating.

    Idea:
    - RL learns gradually via Rescorla-Wagner.
    - WM stores one-shot rewarded mappings, but its contribution is limited by an effective capacity (slots).
    - Effective WM capacity decreases with age (older) and with larger set size; arbitration weight also depends
      on WM confidence (how peaked the WM distribution is for the current state).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr, beta_base, wm_precision, cap_intercept, cap_slope, arbitration_bias]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL policy; internally scaled by *10.
        - wm_precision: scales the gain from WM confidence into arbitration.
        - cap_intercept: baseline capacity before age/load; larger => more slots.
        - cap_slope: how strongly capacity is reduced by age/load (age_group + (nS-3)).
        - arbitration_bias: baseline bias toward WM in the mixture.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_precision, cap_intercept, cap_slope, arbitration_bias = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective WM capacity (slots) given age/load
        # K ranges roughly 0..3 depending on logistic of (cap_intercept - cap_slope*load)
        load = age_group + (nS - 3)
        K = 3.0 * (1.0 / (1.0 + np.exp(-(cap_intercept - cap_slope * load))))
        p_in_block = min(1.0, K / float(nS))  # probability the current state can be held in WM

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM confidence: how peaked is WM for this state
            conf = np.max(W_s) - (1.0 / nA)  # 0 when uniform, up to ~1 - 1/3 when one-hot
            wm_gate = 1.0 / (1.0 + np.exp(-(arbitration_bias + wm_precision * (conf * 10.0))))
            wm_weight = p_in_block * wm_gate  # capacity-limited arbitration

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM update: decay toward uniform + one-shot store if rewarded
            decay = 0.85
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dynamic exploration cooling and WM gate shaped by recent success, with age/load interference.

    Idea:
    - RL temperature cools as a function of state-specific exposure (habitization), but aging/load reheat it.
    - WM contribution is gated by a baseline propensity reduced by age/load interference and scaled by a
      success-trace indicating that WM for that state is currently reliable.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr, beta_base, wm_gate_base, interference_gain, age_penalty_beta, state_satiation]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature; internally scaled by *10.
        - wm_gate_base: baseline WM gate log-odds before success modulation.
        - interference_gain: how much age/load (age_group + (nS-3)) reduces WM gate.
        - age_penalty_beta: increases decision noise (reduces beta) with age/load.
        - state_satiation: increases beta with repeated visits to a state (habitization).

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, beta_base, wm_gate_base, interference_gain, age_penalty_beta, state_satiation = model_parameters
    softmax_beta_base = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        visit_counts = np.zeros(nS)  # to cool beta by state
        success_trace = 0.33 * np.ones(nS)  # WM reliability by state (0..1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # State-specific beta: cools with visits but is reheated by age/load
            visit_counts[s] += 1.0
            load = age_group + (nS - 3)
            beta_s = softmax_beta_base * (1.0 + state_satiation * visit_counts[s]) / (1.0 + age_penalty_beta * load)

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_s * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM gate: baseline minus interference, scaled by success trace for current state
            gate_logit = wm_gate_base - interference_gain * load
            wm_weight = (1.0 / (1.0 + np.exp(-gate_logit))) * np.clip(success_trace[s], 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM update: decay; refresh on reward
            decay = 0.85
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

            # Update success trace: approach 1 when reward, decay otherwise
            succ_alpha = 0.6
            if r > 0.0:
                success_trace[s] = success_trace[s] + succ_alpha * (1.0 - success_trace[s])
            else:
                success_trace[s] = success_trace[s] + 0.3 * (0.0 - success_trace[s])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay and WM error-suppression, with age/load reducing WM strength.

    Idea:
    - RL values decay toward uniform to capture forgetting across spaced presentations.
    - WM contributes a stable mapping when rewarded, but if an action fails, WM actively suppresses that action.
    - Aging and larger set sizes reduce the overall strength of WM in the arbitration.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of int
        Participant age repeated per trial (used to derive age group).
    model_parameters : list or array, length 6
        [lr, beta_base, q_decay, wm_strength, age_load_suppression, wm_error_supp]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature; internally scaled by *10.
        - q_decay: per-visit decay strength of RL values toward uniform (0..1).
        - wm_strength: baseline WM mixture log-odds (higher => more WM).
        - age_load_suppression: how much age/load reduces WM weight.
        - wm_error_supp: how strongly WM suppresses a recently failed action for that state (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood.
    """
    lr, beta_base, q_decay, wm_strength, age_load_suppression, wm_error_supp = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL decay toward uniform for the encountered state
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * w_0[s, :]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM weight: baseline log-odds minus age/load suppression
            load = age_group + (nS - 3)
            wm_logit = wm_strength - age_load_suppression * load
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM update
            # Decay WM slightly; on reward, store one-shot; on error, suppress chosen action in WM
            decay = 0.85
            w[s, :] = decay * w[s, :] + (1.0 - decay) * w_0[s, :]

            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
            else:
                # Suppress the failed action and renormalize partly toward others
                drop = wm_error_supp * w[s, a]
                w[s, a] = max(0.0, w[s, a] - drop)
                # Redistribute dropped mass to the other actions proportionally to their current mass
                others = [i for i in range(nA) if i != a]
                mass_others = np.sum(w[s, others])
                if mass_others > 0.0:
                    w[s, others] = w[s, others] + drop * (w[s, others] / mass_others)
                else:
                    # if all mass was on a, split evenly among others
                    w[s, others] = w[s, others] + drop / (nA - 1)

                # Keep row normalized (not strictly necessary for softmax but beneficial)
                row_sum = np.sum(w[s, :])
                if row_sum > 0.0:
                    w[s, :] = w[s, :] / row_sum

        blocks_log_p += log_p

    return -blocks_log_p