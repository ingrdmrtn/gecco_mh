def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + noisy WM with age- and load-dependent precision.

    Mechanism
    - Policy is a convex combination of RL softmax and WM softmax.
    - RL uses asymmetric learning rates for positive vs negative prediction errors.
    - WM stores rewarded mappings in a one-shot manner, then leaks toward uniform. WM precision
      (effective inverse temperature) declines with set size and further with older age.
    - WM mixture weight is reduced under higher set size (load).

    Parameters
    ----------
    states : array-like of int
        State index at each trial, zero-based within each block.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Reward feedback (0/1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like
        Participant's age (repeated). Age group: 0=young (<=45), 1=old (>45).
    model_parameters : list or array
        [lr_pos, lr_neg, wm_weight, softmax_beta, wm_leak, age_wm_noise]
        - lr_pos: RL learning rate for positive PE (0..1)
        - lr_neg: RL learning rate for negative PE (0..1)
        - wm_weight: base mixture weight for WM in the policy (0..1)
        - softmax_beta: RL inverse temperature; scaled by 10 internally
        - wm_leak: leak rate for WM toward uniform per trial (0..1)
        - age_wm_noise: multiplicative penalty on WM precision in older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_leak, age_wm_noise = model_parameters
    softmax_beta *= 10.0  # higher range for RL inverse temperature

    # Age group: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # WM is near-deterministic when precise
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables and uniform prior
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load- and age-adjusted WM mixture and precision
        wm_w_block = np.clip(wm_weight * (1.0 if nS == 3 else 0.5), 0.0, 1.0)
        # Precision declines with load and age
        wm_precision_scale = 1.0 / (1.0 + 0.5 * (nS - 3))  # 1.0 for 3, ~0.67 for 6
        wm_precision_age = 1.0 / (1.0 + age_wm_noise * age_group)
        beta_wm_eff = softmax_beta_wm * wm_precision_scale * wm_precision_age

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability for chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax probability for chosen action (analogous form)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM leak toward uniform
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            # If rewarded, encode the mapping in WM (sharpen the correct action)
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated exploration + WM persistence (episodic cache).

    Mechanism
    - Policy mixes RL softmax with a WM softmax.
    - RL inverse temperature (exploration) decreases under higher set size and further in older adults.
      This captures noisier RL choices when load is high/with aging.
    - WM serves as a persistent cache for recently rewarded mappings; it decays toward uniform with a
      persistence parameter. WM is strong when items can be maintained (nS=3), weaker when nS=6.
    - WM mixture weight is scaled by persistence and load.

    Parameters
    ----------
    states : array-like of int
        State index at each trial, zero-based within each block.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Reward feedback (0/1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like
        Participant's age (repeated). Age group: 0=young (<=45), 1=old (>45).
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, beta_load_slope, wm_persist, age_beta_boost]
        - lr: RL learning rate (0..1)
        - wm_weight: base mixture weight for WM in the policy (0..1)
        - softmax_beta: baseline RL inverse temperature; scaled by 10 internally
        - beta_load_slope: how much load (nS>3) reduces RL beta (>=0)
        - wm_persist: WM retention (0..1); higher = slower decay toward uniform
        - age_beta_boost: increases load effect on RL beta in older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, beta_load_slope, wm_persist, age_beta_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL beta is reduced by load and more so in older adults
        load_term = max(0, nS - 3)
        beta_eff = softmax_beta / (1.0 + beta_load_slope * load_term * (1.0 + age_beta_boost * age_group))

        # WM mixture: scaled by load and persistence
        wm_w_block = np.clip(wm_weight * (wm_persist if nS == 3 else wm_persist * 0.5), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability (template form)
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM softmax probability using high precision
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM persistence update: decay toward uniform, then store rewarded mapping
            w = wm_persist * w + (1.0 - wm_persist) * w_0
            if r > 0.0:
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                row = row / np.sum(row)
                w[s, :] = row

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with perseveration bias + WM implementing a Win-Stay/Lose-Shift heuristic; age boosts perseveration.

    Mechanism
    - Policy mixes RL and WM.
    - RL is augmented by a perseveration bias that favors repeating the last action in the same state.
      This bias is stronger in older adults.
    - WM approximates a WSLS heuristic: after reward, it strongly encodes staying with the chosen action;
      after no reward, it shifts probability mass to alternative actions. WM also leaks toward uniform.
    - Mixture weight is fixed but applied identically across loads; performance differences arise from
      RL and WM dynamics and age-related perseveration.

    Parameters
    ----------
    states : array-like of int
        State index at each trial, zero-based within each block.
    actions : array-like of int
        Chosen action at each trial (0..2).
    rewards : array-like of int
        Reward feedback (0/1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like
        Participant's age (repeated). Age group: 0=young (<=45), 1=old (>45).
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, perseveration, wsls_strength, age_persev_boost]
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight for WM in the policy (0..1)
        - softmax_beta: RL inverse temperature; scaled by 10 internally
        - perseveration: base bias added to last action in a state (>=0)
        - wsls_strength: how strongly WM enforces WSLS updates (0..1)
        - age_persev_boost: multiplicative increase to perseveration in older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, perseveration, wsls_strength, age_persev_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Age-boosted perseveration magnitude
        persev_eff = perseveration * (1.0 + age_persev_boost * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bias in RL for repeating last action in the same state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += persev_eff

            W_s = w[s, :].copy()

            # RL probability with biased Q (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability (softmax over WM values)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update implementing WSLS with leak toward uniform
            # Leak determined by 1 - wsls_strength (stronger WSLS -> less leak)
            leak = max(0.0, min(1.0, 1.0 - wsls_strength))
            w = (1.0 - leak) * w + leak * w_0

            # WSLS encoding at current state
            if r > 0.0:
                # Win-Stay: encode strong preference to repeat action a
                row = np.full(nA, 1e-6)
                row[a] = 1.0
                w[s, :] = row / np.sum(row)
            else:
                # Lose-Shift: reduce weight on a, redistribute to others
                row = np.full(nA, 1.0 / (nA - 1))
                row[a] = 1e-6
                w[s, :] = row / np.sum(row)

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)