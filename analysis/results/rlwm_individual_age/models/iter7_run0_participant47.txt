def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + confidence-weighted WM arbitration with age- and set-sizeâ€“dependent WM noise.

    Mechanism
    - RL: standard delta rule with single learning rate.
    - WM: graded associative store per state that is updated proportionally to reward with learning rate eta_wm.
    - Arbitration: WM weight is the product of a baseline (wm_weight_base), a size factor (favoring small sets),
      and a confidence term derived from the sharpness of the WM distribution for the current state.
    - Age and set size increase WM noise, flattening its policy.

    Parameters
    ----------
    states : array-like of int
        State on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like (single repeated value)
        Age of participant. Used to compute age group (0 young, 1 old).
    model_parameters : list or array
        [alpha, softmax_beta, eta_wm, wm_weight_base, wm_conf_temp, age_noise_scale]
        - alpha: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - eta_wm: WM learning rate toward the rewarded action (0..1).
        - wm_weight_base: baseline arbitration weight of WM (0..1).
        - wm_conf_temp: sensitivity of arbitration to WM confidence (>=0).
        - age_noise_scale: additional flattening of WM policy for older adults and larger set sizes (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, eta_wm, wm_weight_base, wm_conf_temp, age_noise_scale = model_parameters
    softmax_beta *= 10.0  # higher upper bound as per template

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic absent noise
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Size-based WM propensity (favor small sets)
        size_scale = 3.0 / float(nS)  # 1 for 3, 0.5 for 6

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with age- and size-dependent noise (flattening toward uniform)
            # The stronger the noise, the more W_s is mixed with uniform before softmax.
            noise_strength = age_noise_scale * (1.0 + 0.5 * age_group) * (nS / 3.0)
            W_s_noisy = (1.0 - np.tanh(noise_strength)) * W_s + np.tanh(noise_strength) * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))

            # Confidence from WM (sharpening = max minus mean)
            wm_confidence = max(0.0, np.max(W_s) - np.mean(W_s))
            # Map confidence to [0,1] via logistic; then scale with baseline and size
            conf_weight = 1.0 / (1.0 + np.exp(-wm_conf_temp * (wm_confidence * nA)))
            wm_weight = np.clip(wm_weight_base * size_scale * conf_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: pull distribution toward rewarded action proportional to r
            # Ensures WM learns more from rewarded outcomes.
            w[s, :] = (1.0 - eta_wm * r) * w[s, :] + (eta_wm * r) * w_0[s, :]
            if r > 0.0:
                # Add targeted boost to chosen action; renormalize
                w[s, a] += eta_wm * r
                w[s, :] = np.clip(w[s, :], 1e-6, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with probabilistic gating by surprise and set-size threshold modulation.

    Mechanism
    - RL: standard delta rule.
    - WM: one-shot encoding with probability gated by reward surprise |delta|.
    - Arbitration: WM contributes with a weight that depends on baseline, set size threshold, and age.
      WM is emphasized when set size <= theta_size (adjusted for age), otherwise down-weighted.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like (single repeated value)
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, theta_size, gate_sensitivity, wm_learn_prob]
        - alpha: RL learning rate.
        - softmax_beta: RL inverse temperature (scaled internally).
        - wm_weight_base: baseline WM arbitration weight (0..1).
        - theta_size: subjective capacity threshold around which WM weight drops (in items).
        - gate_sensitivity: how strongly surprise |delta| triggers WM encoding (>=0).
        - wm_learn_prob: max probability to encode into WM upon surprise (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, theta_size, gate_sensitivity, wm_learn_prob = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age reduces effective threshold (older adults treat smaller sets as capacity-exceeding)
    eff_theta = max(1.0, theta_size - 0.5 * age_group)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM weight via smooth threshold
        size_gap = eff_theta - nS
        size_gate = 1.0 / (1.0 + np.exp(-size_gap))  # near 1 when nS < eff_theta, near 0 when nS > eff_theta
        wm_weight_block = np.clip(wm_weight_base * size_gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice prob
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Combine
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM encoding gated by surprise |delta|
            surprise = abs(delta)
            # Probability to encode this mapping now
            p_encode = np.clip(wm_learn_prob * (1.0 - np.exp(-gate_sensitivity * surprise)), 0.0, 1.0)

            # Stochastic encoding implemented in expectation: blend toward one-hot by p_encode
            # Equivalent to soft updating without sampling.
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0 if r > 0.0 else 0.0  # only reward leads to storing a reliable mapping
            target = w_0[s, :] if r <= 0.0 else one_hot

            w[s, :] = (1.0 - p_encode) * w[s, :] + p_encode * target
            # Ensure normalized
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with cross-state interference toward globally popular rewarded actions.

    Mechanism
    - RL: standard delta rule.
    - WM: associative table per state updated toward rewarded action with learning rate eta_wm.
    - Interference: WM distribution for each state is blended with a global action popularity vector that
      tracks which actions have been rewarded across all states. Interference increases with set size and age.
    - Arbitration: fixed baseline WM weight scaled by set size (favor small sets).

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    age : array-like (single repeated value)
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight_base, eta_wm, interference_strength, gamma_pop]
        - alpha: RL learning rate.
        - softmax_beta: RL inverse temperature (scaled internally).
        - wm_weight_base: baseline WM arbitration weight (0..1).
        - eta_wm: WM learning rate toward rewarded action per state (0..1).
        - interference_strength: base strength of blending with global popularity (>=0).
        - gamma_pop: learning rate for global popularity from rewards (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, softmax_beta, wm_weight_base, eta_wm, interference_strength, gamma_pop = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global popularity vector (starts uniform)
        pop = (1.0 / nA) * np.ones(nA)

        # Arbitration weight scaled by set size (favor small sets)
        wm_weight_block = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Before policy, compute an interference-adjusted WM vector for current state
            # Interference grows with set size and age
            kappa = np.tanh(interference_strength * (nS / 3.0) * (1.0 + 0.5 * age_group))
            W_s_raw = w[s, :]
            W_s = (1.0 - kappa) * W_s_raw + kappa * pop  # blend with global popularity

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Combine
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update toward rewarded action; light decay toward uniform on no reward
            if r > 0.0:
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * w_0[s, :]
                w[s, a] += eta_wm
                w[s, :] = np.clip(w[s, :], 1e-8, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                # Gentle relaxation on errors
                relax = 0.25 * eta_wm
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

            # Update global popularity from rewards across states
            # Pop tracks which actions tend to be rewarded; decays toward uniform otherwise.
            pop = (1.0 - gamma_pop) * pop + gamma_pop * w_0[0, :]
            if r > 0.0:
                pop[a] += gamma_pop
            pop = np.clip(pop, 1e-8, None)
            pop /= np.sum(pop)

        blocks_log_p += log_p

    return -blocks_log_p