def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM mixture with uncertainty-weighted arbitration; age reduces WM precision.

    This model mixes a slow RL system with a WM system. The mixture weight placed on WM increases
    when the RL policy is uncertain (high entropy). WM precision is reduced under higher set size
    and further reduced in older adults. RL has asymmetric learning rates for gains and losses.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of int
        Binary feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6) repeated across trials within the block.
    age : array-like of int
        Participant age (same value repeated). Age group is coded as 0 if <=45, else 1.
    model_parameters : list or array
        [lr_pos, lr_neg, beta_rl, wm_weight_base, wm_precision_base, age_wm_penalty]
        - lr_pos: RL learning rate for rewards (r=1).
        - lr_neg: RL learning rate for no-reward (r=0).
        - beta_rl: RL inverse temperature (scaled internally).
        - wm_weight_base: baseline WM contribution to the policy when RL is maximally uncertain.
        - wm_precision_base: baseline WM precision scaling factor at set size 3 and young.
        - age_wm_penalty: reduction in WM precision applied if age_group=1.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_rl, wm_weight_base, wm_precision_base, age_wm_penalty = model_parameters
    softmax_beta = beta_rl * 10.0  # higher dynamic range

    # Age group: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # high precision WM; will be modulated by wm_precision
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects on WM precision and decay
        # Precision scales down with set size; older adults get an additional penalty
        wm_precision = wm_precision_base * (3.0 / nS) - age_wm_penalty * age_group
        wm_precision = np.clip(wm_precision, 0.0, 1.0)

        # WM decay per trial: more decay for larger set size (no free parameter)
        wm_decay = 1.0 - (3.0 / nS)  # 0 at nS=3, 0.5 at nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with softmax
            Q_s = q[s, :]
            # compute choice prob for observed action a via "log-sum-exp trick"
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL entropy (uncertainty) based on full policy
            logits = softmax_beta * Q_s
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)
            # entropy in nats; normalize by log(nA) to get [0,1]
            entropy = -np.sum(pi * (np.log(pi + 1e-12)))
            entropy_norm = entropy / np.log(nA)

            # WM policy (deterministic limit modulated by precision)
            W_s = w[s, :]
            beta_wm_eff = softmax_beta_wm * wm_precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Uncertainty-weighted arbitration: favor WM when RL is uncertain
            wm_weight = wm_weight_base * entropy_norm
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s, a]
            lr = lr_pos if r > 0 else lr_neg
            q[s, a] += lr * delta

            # WM update: decay toward uniform; then one-shot encode if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and gated WM use; age shifts WM gate.

    The controller probabilistically uses WM based on a logistic gate that depends on set size
    and age. WM updates on rewarded trials with a given probability and otherwise maintains
    its content; RL includes value forgetting toward uniform each trial.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of int
        Binary feedback (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    age : array-like of int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_rl_base, wm_gate_bias, wm_update_prob, age_gate_shift, rl_forgetting]
        - lr: RL learning rate (0..1).
        - beta_rl_base: base RL inverse temperature (scaled internally).
        - wm_gate_bias: baseline bias to use WM (positive = more WM).
        - wm_update_prob: probability to write rewarded action into WM.
        - age_gate_shift: reduction in WM gate for older adults (multiplied by age_group).
        - rl_forgetting: decay of Q-values toward uniform per trial (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl_base, wm_gate_bias, wm_update_prob, age_gate_shift, rl_forgetting = model_parameters
    softmax_beta = beta_rl_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set size influences both RL noise and WM gate
        beta_rl_eff = softmax_beta * (3.0 / nS)  # more noise under higher load
        load_term = - (nS - 3) / 3.0  # 0 at nS=3, -1 at nS=6

        # Constant gate within block given set size and age
        gate_logit = wm_gate_bias + load_term + (-age_gate_shift * age_group)
        wm_weight = 1.0 / (1.0 + np.exp(-gate_logit))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            q = (1.0 - rl_forgetting) * q + rl_forgetting * (1.0 / nA)

            # WM update: probabilistic write on rewarded trials
            if r == 1:
                # stochastic write without external RNG: approximate by expected update
                # Move WM toward one-hot with weight wm_update_prob
                w[s, :] = (1.0 - wm_update_prob) * w[s, :]  # shrink current content
                w[s, a] += wm_update_prob * (1.0 - w[s, a])  # push chosen toward 1

                # Renormalize to keep a distribution
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controlled RL vs. WM with reliability-driven arbitration; age biases toward RL.

    A meta-controller arbitrates between RL and WM based on their running reliabilities:
    - RL reliability increases when signed prediction errors are small.
    - WM reliability increases when WM predicts chosen actions correctly (rewarded).
    The arbitration compares reliabilities with a sensitivity parameter; older adults are
    biased toward RL, and high set size adds a bias against WM. WM is one-shot and decays.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of int
        Binary feedback (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    age : array-like of int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_rl, arbitration_k, wm_decay, age_bias, load_bias]
        - lr: RL learning rate.
        - beta_rl: RL inverse temperature (scaled internally).
        - arbitration_k: sensitivity of meta-controller to reliability difference (WM - RL).
        - wm_decay: WM decay rate per trial toward uniform (0..1).
        - age_bias: additive bias toward RL for older adults (subtracted from WM in gate).
        - load_bias: additive bias toward RL under higher set size (applied when nS=6).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl, arbitration_k, wm_decay, age_bias, load_bias = model_parameters
    softmax_beta = beta_rl * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize reliabilities (higher means more trustworthy)
        rel_rl = 0.5
        rel_wm = 0.5

        # Load bias against WM when set size is large
        load_bias_block = load_bias if nS > 3 else 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight via logistic function of reliability difference
            gate_logit = arbitration_k * (rel_wm - rel_rl) - (age_bias * age_group) - load_bias_block
            wm_weight = 1.0 / (1.0 + np.exp(-gate_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update RL reliability: higher when absolute PE is small
            pe_mag = abs(delta)
            rel_rl = 0.9 * rel_rl + 0.1 * (1.0 - pe_mag)  # running average

            # WM decay each trial toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM one-shot encode on reward
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

                # WM reliability increases when WM correctly predicts rewarded choice
                # Compute WM policy correctness for chosen action
                p_wm_chosen = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                rel_wm = 0.9 * rel_wm + 0.1 * (1.0 - (1.0 - p_wm_chosen))
            else:
                # If no reward, slightly decrease WM reliability
                rel_wm = 0.95 * rel_wm

            # Keep reliabilities in [0,1]
            rel_rl = float(np.clip(rel_rl, 0.0, 1.0))
            rel_wm = float(np.clip(rel_wm, 0.0, 1.0))

        blocks_log_p += log_p

    return -blocks_log_p