def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with dynamic WM reliability and load-/age-dependent decay.

    Idea
    - RL: single learning rate Q-learning.
    - WM: fast associative store per state updated by reward; WM reliability r_s tracks recent
      success in that state and gates the mixture weight dynamically.
    - Load/Age effects: WM decays more under higher set size and in older adults, which reduces
      effective WM impact on policy.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr_rl, beta_rl, wm_eta, beta_wm, decay_base]
        - lr_rl: RL learning rate (0..1).
        - beta_rl: base inverse temperature for RL softmax (scaled internally).
        - wm_eta: learning rate for WM reliability and WM write strength (0..1).
        - beta_wm: inverse temperature for WM softmax (higher => more deterministic).
        - decay_base: base WM decay strength modulated by set size and age (>=0).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr_rl, beta_rl, wm_eta, beta_wm, decay_base = model_parameters

    # Age group coding: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = max(1e-3, beta_wm)  # allow tuning rather than fixing to 50
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM associative values
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # State-specific WM reliability gating (tracks recent success in that state)
        r_gate = 0.0 * np.ones(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of the chosen action under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Load-/age-dependent WM decay applied to the entire WM matrix
            # Decay grows with set size beyond 3 and more for older adults.
            excess = max(0, nS_t - 3)
            decay_t = 1.0 - np.exp(-decay_base * (1.0 + 0.6 * age_group) * excess)
            # Apply small continuous decay toward uniform
            if decay_t > 0:
                w = (1.0 - decay_t) * w + decay_t * w_0

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic WM weight: product of state-specific reliability and a load/age factor
            # Reliability r_gate[s] tracks recent reward; bounded to [0,1].
            # Load/age factor reduces effective WM weight under higher load, more in older adults.
            load_factor = 1.0 / (1.0 + excess * (1.0 + 0.5 * age_group))
            wm_weight_eff = np.clip(r_gate[s] * load_factor, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr_rl * pe

            # WM update
            # - On reward, write a one-hot memory for the chosen action (fast binding).
            # - On no reward, gently depress that action in WM.
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
                # Increase reliability for this state
                r_gate[s] = (1.0 - wm_eta) * r_gate[s] + wm_eta * 1.0
            else:
                # Mild negative update: push probability mass away from chosen action
                anti = np.ones(nA) / (nA - 1)
                anti[a] = 0.0
                w[s, :] = (1.0 - 0.5 * wm_eta) * w[s, :] + 0.5 * wm_eta * anti
                # Decrease reliability
                r_gate[s] = (1.0 - wm_eta) * r_gate[s] + wm_eta * 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM slots + RL, with age-modulated capacity and WM refresh.

    Idea
    - WM: limited slots K_eff; if set size exceeds capacity, the effective WM weight scales
      as K_eff / set_size. WM stores one action per state upon reward and decays otherwise.
    - RL: standard Q-learning.
    - Age effect: older adults have reduced effective capacity (multiplicative drop).
    - Set-size effect: mixture weight automatically scales with set size via K_eff / nS.
    - WM refresh: parameter controlling how strongly correct trials refresh WM trace.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr, beta_rl, capacity, capacity_old_mult, beta_wm, wm_refresh]
        - lr: RL learning rate (0..1).
        - beta_rl: base inverse temperature for RL softmax (scaled internally).
        - capacity: baseline WM slot capacity for young participants (>=1).
        - capacity_old_mult: multiplicative factor on capacity for older adults (0..1).
        - beta_wm: inverse temperature for WM softmax.
        - wm_refresh: rate that correct feedback refreshes WM trace (0..1).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr, beta_rl, capacity, capacity_old_mult, beta_wm, wm_refresh = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = max(1e-3, beta_wm)
    eps = 1e-12
    blocks_log_p = 0.0

    # Effective capacity given age
    K_eff_base = capacity * (capacity_old_mult if age_group == 1 else 1.0)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture weight: fraction of items that can be stored in WM
            wm_weight_eff = np.clip(K_eff_base / max(1, nS_t), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM dynamics:
            # - Soft decay toward uniform each trial (limited maintenance)
            # - On reward, refresh state s strongly toward the chosen action
            decay = 0.05 * (1.0 + 0.5 * age_group) * max(1, nS_t) / 6.0
            w = (1.0 - decay) * w + decay * w_0

            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * target
            else:
                # No explicit anti-learning; rely on decay
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated WM writing + asymmetric RL learning, with load/age penalties on gating.

    Idea
    - RL: asymmetric learning rates for positive/negative PEs (older adults often show reduced
      sensitivity to negative outcomes).
    - WM: when surprise is high (|PE| large), the system is more likely to gate the current
      action-state pair into WM; gating is reduced by higher set size and in older adults.
      Stored WM traces produce a near-deterministic policy component.
    - Mixture: weighted by the instantaneous gating probability and presence of a WM trace.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action (0..2).
    rewards : array-like
        Binary reward (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age (same value repeated).
    model_parameters : list or array
        [lr_pos, lr_neg, beta_rl, gate_base, age_gate_drop, beta_wm]
        - lr_pos: RL learning rate for positive PEs (0..1).
        - lr_neg: RL learning rate for negative PEs (0..1).
        - beta_rl: base inverse temperature for RL softmax (scaled internally).
        - gate_base: baseline WM gating drive (higher => more WM usage).
        - age_gate_drop: reduction in gating for older adults and with load (>=0).
        - beta_wm: inverse temperature for WM softmax (near-deterministic when high).

    Returns
    -------
    float
        Negative log-likelihood of choices.
    """
    lr_pos, lr_neg, beta_rl, gate_base, age_gate_drop, beta_wm = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = max(1e-3, beta_wm)
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Binary availability of a strong WM trace per state (implicitly encoded in w row sharpness)
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute current PE for gating calculation (based on pre-update Q)
            pe = r - q[s, a]
            # Surprise term: absolute PE; normalized to [0,1] for binary rewards
            surprise = abs(pe)

            # Load and age penalty on gating
            load_pen = age_gate_drop * (age_group * 1.0 + 0.5 * max(0, nS_t - 3))

            # Instantaneous gating probability (squashed)
            g_raw = gate_base + 3.0 * surprise - load_pen
            wm_weight_eff = 1.0 / (1.0 + np.exp(-g_raw))  # sigmoid in [0,1]

            # Mixture probability for the chosen action
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetry
            lr_use = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_use * pe

            # WM maintenance: mild decay toward uniform, stronger in load/age
            decay = 0.03 * (1.0 + 0.7 * age_group) * (1.0 + 0.5 * max(0, nS_t - 3))
            w = (1.0 - decay) * w + decay * w_0

            # WM write if gate opens: here we sample deterministically using gating probability
            # by blending toward a one-hot trace proportional to wm_weight_eff.
            # Rewarded outcomes write stronger (binding success).
            if wm_weight_eff > 0.0:
                if r >= 0.5:
                    target = np.zeros(nA)
                    target[a] = 1.0
                    w[s, :] = (1.0 - wm_weight_eff) * w[s, :] + wm_weight_eff * target
                else:
                    # On negative outcome, reduce the chosen action in WM
                    anti = np.ones(nA) / (nA - 1)
                    anti[a] = 0.0
                    w[s, :] = (1.0 - 0.5 * wm_weight_eff) * w[s, :] + 0.5 * wm_weight_eff * anti

        blocks_log_p += log_p

    return -blocks_log_p