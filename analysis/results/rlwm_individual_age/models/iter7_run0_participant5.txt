def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: RL + WM with age- and set-size-modulated WM weight and perseveration bias.

    Description:
    - RL: Q-learning with softmax policy (given).
    - WM: Reward-gated associative memory that decays toward a uniform baseline. WM policy is near-deterministic.
    - Arbitration: Fixed WM mixture weight modulated by set size (smaller set => stronger WM) and by age (older => reduced WM).
    - Perseveration: Bias in WM policy toward repeating the previous action in the same state; stronger bias increases p_wm on repeated choices.
    
    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1) before set-size and age modulation
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - perseveration: Strength of within-state action repetition bias in WM policy (>=0)
    - age_wm_shift: Scales the reduction of WM influence for older adults (>=0). Effective multiplier = (1 - age_wm_shift*age_group).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, perseveration, age_wm_shift = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    # Keep last action per state to implement within-state perseveration
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action_by_state = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax but with perseveration bias toward repeating last action in same state
            W_aug = W_s.copy()
            if last_action_by_state[s] == a:
                # Increase the logit for the repeated action in WM space
                bias_vec = np.zeros(nA)
                bias_vec[a] = perseveration
                W_aug = W_aug + bias_vec

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_aug - W_aug[a])))

            # Effective WM weight: smaller set size and younger age -> higher WM contribution
            ss_factor = 3.0 / nS  # 1.0 for set size 3; 0.5 for set size 6
            age_factor = 1.0 - age_wm_shift * age_group
            wm_weight_eff = wm_weight * ss_factor * age_factor
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (given)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform + reward-gated imprint of chosen action
            # Decay rate increases with set size and with age (via age_wm_shift), scaled by wm_weight
            decay_rate = wm_weight * 0.1 * (nS / 3.0) * (1.0 + age_wm_shift * age_group)
            decay_rate = np.clip(decay_rate, 0.0, 1.0)
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Encoding strength increases with wm_weight and is dampened by larger set sizes
                enc = 1.0 - np.exp(-5.0 * wm_weight * (3.0 / nS))
                enc = np.clip(enc, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot

            last_action_by_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: RL + WM with uncertainty-driven arbitration and age-modulated WM precision.

    Description:
    - RL: Q-learning with softmax (given).
    - WM: Reward-based associative store. WM policy uses a high inverse temperature but is reduced by set size
      and by an age-dependent WM precision cost.
    - Arbitration: The WM mixture weight is dynamically adjusted by the relative uncertainty (entropy) of RL vs. WM:
      wm_weight_eff = sigmoid(logit(wm_weight) + arb_sensitivity * (H_rl - H_wm)),
      so WM contributes more when WM is more certain (lower entropy) than RL.
    - Set size effect: Larger set sizes reduce WM precision (effectively lowering WM inverse temperature).
    - Age effect: Older adults incur an additional precision cost in WM and stronger decay.

    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight before arbitration (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - arb_sensitivity: Sensitivity (>0) of arbitration to entropy difference (H_rl - H_wm)
    - age_wm_cost: Scales the reduction in WM precision and increases WM decay in older adults (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, arb_sensitivity, age_wm_cost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm_base = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM precision reduced by set size and age costs
            ss_penalty = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6
            wm_beta_eff = softmax_beta_wm_base / (1.0 + ss_penalty + age_wm_cost * age_group)
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Compute entropies of the two policies to arbitrate
            # We reconstruct full action probabilities up to normalizers using the same logits
            def softmax_probs(logits, beta):
                z = logits - np.max(logits)
                ex = np.exp(beta * z)
                return ex / np.sum(ex)

            p_rl_vec = softmax_probs(Q_s, softmax_beta)
            p_wm_vec = softmax_probs(W_s, wm_beta_eff)

            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))

            # Arbitration: logistic transform around base wm_weight
            def logit(x):
                x = np.clip(x, 1e-6, 1 - 1e-6)
                return np.log(x / (1 - x))

            def inv_logit(z):
                return 1.0 / (1.0 + np.exp(-z))

            z = logit(wm_weight) + arb_sensitivity * (H_rl - H_wm)
            wm_weight_eff = inv_logit(z)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (given)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay + reward-gated encoding; decay increases with set size and age_wm_cost in older adults
            decay = 0.08 * (1.0 + ss_penalty) * (1.0 + age_wm_cost * age_group)
            decay = np.clip(decay, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Encoding strength reduced by set size and age WM cost in older group
                enc = 0.8 / (1.0 + ss_penalty + 0.5 * age_wm_cost * age_group)
                enc = np.clip(enc, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: RL + WM with set-size-driven WM interference and age penalty.

    Description:
    - RL: Q-learning with softmax (given).
    - WM: Associative memory that suffers interference/noise as set size grows. We model this as
      a corruption toward uniform before sampling the WM policy and as stronger decay during updates.
    - Arbitration: Fixed WM mixture weight scaled by set size and age (older => lower WM weight).
    - Set size effect: Larger sets increase WM interference gamma, reducing WM precision.
    - Age effect: Older adults amplify interference and decay.

    Parameters (list):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_interference: Base WM interference strength (>=0), scaled by set size
    - age_wm_boost: Scales how much interference/decay grows for older adults (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_interference, age_wm_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute set-size-dependent interference factor
        ss_penalty = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6
        gamma_base = wm_interference * (1.0 + ss_penalty) * (1.0 + age_wm_boost * age_group)
        gamma_base = np.clip(gamma_base, 0.0, 1.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference: corrupt W_s toward uniform before policy computation
            gamma = np.clip(gamma_base, 0.0, 1.0)
            W_corrupt = (1.0 - gamma) * W_s + gamma * (1.0 / nA)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_corrupt - W_corrupt[a])))

            # Arbitration: downweight WM with larger set sizes and in older adults
            wm_weight_eff = wm_weight * (3.0 / nS) * (1.0 - 0.4 * age_group * np.tanh(age_wm_boost))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (given)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: stronger decay with larger interference; reward-driven imprint
            decay = np.clip(0.05 + 0.5 * gamma_base, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Encoding strength is reduced by interference
                enc = np.clip(0.9 * (1.0 - 0.7 * gamma_base), 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * onehot

        blocks_log_p += log_p

    return -blocks_log_p