def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and set-size–dependent binding noise and lapses.

    Idea
    - RL provides gradual value learning.
    - WM attempts near-deterministic recall of the last rewarded action per state.
    - Under higher set size and in older adults, WM suffers from binding noise (state-action misbinding),
      which reduces the effective WM contribution and injects noise into WM retrieval.
    - A small lapse probability mixes in uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, wm_base, softmax_beta, bind_noise, age_bind_boost, lapse]
        - lr: RL learning rate (0..1)
        - wm_base: baseline WM mixture weight (0..1) when set size is small and no noise
        - softmax_beta: RL inverse temperature; internally scaled (*10)
        - bind_noise: baseline WM binding noise strength (>=0); increases WM confusion
        - age_bind_boost: multiplicative boost of binding noise for older adults (>=0)
        - lapse: lapse probability mixing uniform random choice into the policy (0..0.2 typical)

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_base, softmax_beta, bind_noise, age_bind_boost, lapse = model_parameters
    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0  # near-deterministic WM
    eps = 1e-12

    age_group = 1 if age[0] > 45 else 0
    age_noise_mult = 1.0 + age_bind_boost * age_group

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size dependent WM weighting via binding-noise induced reduction
        # More items => more misbinding => down-weight WM exponentially with nS-3
        eff_noise = bind_noise * age_noise_mult
        wm_weight = np.clip(wm_base * np.exp(-eff_noise * max(0, nS - 3)), 0.0, 1.0)

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs_rl - np.max(prefs_rl))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy with binding noise:
            # Replace a portion of the state-specific WM vector with uniform due to misbinding
            # Binding noise increases with set size and age.
            b_noise = np.clip(eff_noise * max(0, nS - 1) / 5.0, 0.0, 1.0)
            W_s = (1.0 - b_noise) * w[s, :] + b_noise * w_0[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating rule:
            # - If rewarded, store a one-hot at the chosen action (overwrite)
            # - If not rewarded, memory drifts slightly toward uniform due to binding confusion
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Drift toward uniform proportional to binding noise
                w[s, :] = (1.0 - b_noise) * w[s, :] + b_noise * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with uncertainty-gated WM engagement modulated by age and set size.

    Idea
    - WM contribution is dynamic and increases when RL is uncertain (high entropy policy),
      reflecting strategic WM engagement under uncertainty.
    - Older adults show reduced gating sensitivity; larger set sizes also reduce baseline WM engagement.
    - WM retrieval is near-deterministic; memory stores last rewarded action.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, softmax_beta, wm0, gate_sensitivity, age_gate_scale, ss_gate_scale]
        - lr: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature; internally scaled (*10)
        - wm0: baseline WM weight at small set size (0..1)
        - gate_sensitivity: sensitivity of WM gating to RL entropy (>=0)
        - age_gate_scale: factor reducing gating sensitivity in older adults (0..1 typical)
        - ss_gate_scale: factor reducing baseline WM with larger set size (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, wm0, gate_sensitivity, age_gate_scale, ss_gate_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 1 if age[0] > 45 else 0
    gate_mult_age = 1.0 / (1.0 + age_gate_scale * age_group)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size reduces baseline WM engagement
        wm_base = wm0 / (1.0 + ss_gate_scale * max(0, nS - 3))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy and entropy (uncertainty)
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs_rl - np.max(prefs_rl))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # Entropy in [0, log nA]; normalize to [0,1]
            H = -np.sum(pi_rl * (np.log(pi_rl + eps)))
            H_norm = H / np.log(nA)

            # Uncertainty-gated WM weight for this trial
            wm_weight = np.clip(wm_base + gate_sensitivity * gate_mult_age * H_norm * (1.0 - wm_base), 0.0, 1.0)

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: rewarded => store one-hot; unrewarded => keep current trace
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # mild consolidation when WM strongly engaged even without reward
                w[s, :] = (1.0 - 0.0) * w[s, :] + 0.0 * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with WM trace learning and age- and set-size–dependent forgetting.
    WM weight is endogenously set by WM confidence (entropy of the trace).

    Idea
    - WM forms graded action-traces per state using a fast learning rate alpha_wm.
    - WM traces forget toward uniform at a rate that rises with set size and in older adults.
    - The mixture weight equals the current WM confidence (1 - normalized entropy),
      so WM contributes more when its memory is sharp, less when diffuse.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block on each trial (3 or 6).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr_rl, softmax_beta, alpha_wm, lambda_base, ss_lambda_slope, age_lambda_slope]
        - lr_rl: RL learning rate (0..1)
        - softmax_beta: RL inverse temperature; internally scaled (*10)
        - alpha_wm: WM learning rate toward one-hot on rewarded trials (0..1)
        - lambda_base: base WM forgetting rate toward uniform (0..1)
        - ss_lambda_slope: increase of forgetting per extra item beyond 3 (>=0)
        - age_lambda_slope: increase of forgetting for older adults (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_rl, softmax_beta, alpha_wm, lambda_base, ss_lambda_slope, age_lambda_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Compute block-specific forgetting rate
        lam = lambda_base
        lam += ss_lambda_slope * max(0, nS - 3)
        lam += age_lambda_slope * age_group
        lam = np.clip(lam, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs_rl - np.max(prefs_rl))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            pref_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(pref_wm - np.max(pref_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # WM confidence = 1 - normalized entropy
            H = -np.sum(W_s * np.log(W_s + eps))
            H_norm = H / np.log(nA)
            wm_weight = np.clip(1.0 - H_norm, 0.0, 1.0)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM update: learn on rewarded trials
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target

            # WM forgetting toward uniform (state-specific, every trial)
            w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)