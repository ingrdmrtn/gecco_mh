def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM cache with set-size-dependent interference and age-reduced encoding.

    Mechanism
    - RL: tabular Q-learning with softmax.
    - WM: per-state cache of the last rewarded action with strength m[s] in [0,1].
      Encoding on reward is probabilistic and reduced in older adults; strength decays faster with larger set sizes.
    - Arbitration: WM mixture weight equals current memory strength m[s]; WM policy is near-deterministic toward the cached action.

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards.
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials.
    model_parameters : list/tuple
        [lr, beta_base, wm_enc, wm_decay_base, age_wm_drop, set_decay_gain]
        - lr: RL learning rate (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - wm_enc: baseline probability to encode a rewarded action into WM (0..1)
        - wm_decay_base: base WM decay per trial (0..1)
        - age_wm_drop: fractional reduction of wm_enc for older group (0..1)
        - set_decay_gain: extra WM decay as set size increases (>0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_enc, wm_decay_base, age_wm_drop, set_decay_gain = model_parameters

    softmax_beta = beta_base * 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))      # will hold a convex mixture of one-hot cached a* and uniform
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM strengths per state
        m = np.zeros(nS)  # 0..1 strength toward cached action
        cached = np.zeros(nS, dtype=int)

        # Set-size dependent decay per trial
        extra_items = max(0.0, float(nS) - 1.0)
        wm_decay = 1.0 - np.clip(wm_decay_base * (1.0 + set_decay_gain * extra_items), 0.0, 0.99)

        # Age-dependent encoding probability
        enc_prob = np.clip(wm_enc * (1.0 - age_wm_drop * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Construct WM policy vector for current state
            W_s_vec = (1.0 - m[s]) * w_0[s, :] + m[s] * np.eye(nA)[cached[s]]
            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_vec - W_s_vec[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: weight by current memory strength
            wm_weight = m[s]
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay (global per trial) and potential encoding on reward
            m[s] = m[s] * wm_decay
            if r > 0.5:
                # Encode with probability enc_prob; implement as expected-value update of strength
                # New strength after encoding attempt: enc_prob * 1 + (1 - enc_prob) * current
                m[s] = enc_prob * 1.0 + (1.0 - enc_prob) * m[s]
                cached[s] = a

            # Update WM action distribution storage for completeness
            w[s, :] = (1.0 - m[s]) * w_0[s, :] + m[s] * np.eye(nA)[cached[s]]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and WM gating by surprise, plus set-size-driven lapse and age-tempered choice.

    Mechanism
    - RL: Q-learning with eligibility traces (per state-action) and softmax.
    - WM: cache of last rewarded action; arbitration weight increases with absolute prediction error (surprise).
    - Lapse: mixture with uniform policy increases with set size.
    - Age: reduces inverse temperature.

    Parameters
    ----------
    model_parameters : list/tuple
        [lr, beta_base, lambda_et, wm_gain_pe, age_beta_shift, lapse_set_gain]
        - lr: base learning rate (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - lambda_et: eligibility decay (0..1)
        - wm_gain_pe: gain scaling of WM weight as a function of |PE| (>=0)
        - age_beta_shift: fractional beta reduction for older group (0..1)
        - lapse_set_gain: added lapse probability per extra item over 3 (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, lambda_et, wm_gain_pe, age_beta_shift, lapse_set_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0 * (1.0 - age_beta_shift * age_group)
    softmax_beta = max(softmax_beta, 1e-3)

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces
        e = np.zeros((nS, nA))

        # WM cache: deterministic one-hot when rewarded; maintain cached action per state
        cached = np.zeros(nS, dtype=int)

        # Set-size dependent lapse
        lapse = np.clip(lapse_set_gain * max(0.0, float(nS) - 3.0), 0.0, 0.49)

        log_p = 0.0
        last_delta = 0.0  # initialize PE for first trial in block
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy from cache (if none cached, defaults to uniform)
            W_s_vec = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_vec - W_s_vec[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration weight grows with surprise |last_delta|
            wm_weight = 1.0 - np.exp(-wm_gain_pe * abs(last_delta))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mix RL and WM, then apply lapse to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Compute PE and update traces
            delta = r - q[s, a]
            # Update eligibility: decay and set current SA to 1
            e *= lambda_et
            e[s, a] = 1.0
            # Q update with traces
            q += lr * delta * e

            # Update WM cache on reward
            if r > 0.5:
                cached[s] = a
                w[s, :] = w_0[s, :] * 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded, keep prior cache; ensure default uniform if nothing cached yet
                if np.allclose(w[s, :], w_0[s, :]):
                    w[s, :] = w_0[s, :]

            last_delta = float(delta)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with volatility-adaptive learning rate and resource-rational WM capacity.

    Mechanism
    - RL: Q-learning where learning rate scales with an online volatility estimate from unsigned PE.
    - WM: capacity-limited store of rewarded actions; per-state WM weight equals K_eff / nS,
      with age reducing effective capacity. WM policy is near-deterministic toward cached action.
    - Set size reduces inverse temperature.

    Parameters
    ----------
    model_parameters : list/tuple
        [lr_base, beta_base, K_base, age_K_drop, vol_sens, beta_set_drop]
        - lr_base: base learning rate (0..1)
        - beta_base: base inverse temperature (scaled by 10 internally)
        - K_base: baseline WM capacity in slots (>=0)
        - age_K_drop: capacity reduction for older group (>=0)
        - vol_sens: sensitivity of volatility update to unsigned PE (>=0)
        - beta_set_drop: fractional beta drop per extra item over 3 (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_base, beta_base, K_base, age_K_drop, vol_sens, beta_set_drop = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_base = beta_base * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size dependent beta reduction
        beta_drop = 1.0 - beta_set_drop * max(0.0, float(nS) - 3.0)
        softmax_beta = max(softmax_beta_base * beta_drop, 1e-3)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Volatility estimate per state
        vol = np.zeros(nS)

        # WM cache and capacity-based weight
        cached = np.zeros(nS, dtype=int)
        K_eff = max(0.0, K_base - age_K_drop * age_group)
        wm_weight_const = np.clip(K_eff / max(1.0, float(nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy vector for current state
            W_s_vec = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_vec - W_s_vec[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: constant per state from capacity sharing
            wm_weight = wm_weight_const
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Volatility-adaptive learning rate for this state
            delta = r - q[s, a]
            vol[s] = vol[s] + vol_sens * (abs(delta) - vol[s])  # exponential smoothing of |PE|
            lr = np.clip(lr_base * (1.0 + vol[s]), 0.0, 1.0)
            q[s, a] += lr * delta

            # WM update on reward
            if r > 0.5:
                cached[s] = a
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded and nothing cached, keep uniform
                if not np.any(w[s, :] > (1.0 / nA) + 1e-8):
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p