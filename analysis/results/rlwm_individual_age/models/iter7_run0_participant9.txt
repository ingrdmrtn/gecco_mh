def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with entropy-gated WM reliance, age-modulated decision noise, and set-sizeâ€“scaled WM retention.

    Idea
    - RL uses a standard delta rule.
    - WM stores rewarded state-action pairs, but its retention decays toward uniform with set size.
    - Arbitration: WM weight increases when RL is confident (low entropy), reflecting a gating that favors WM when RL agrees.
    - Age group (young=0, old=1) increases decision noise (beta reduction) in older participants.

    Parameters (6)
    - alpha_rl: RL learning rate (0..1)
    - beta_base: base RL inverse temperature (scaled by 10 internally)
    - k_beta_age: age-driven reduction of RL inverse temperature (>=0). Effective beta = (beta_base - k_beta_age*age_group)*10
    - wm_weight0: base logit for WM reliance (arbitration intercept)
    - k_wm_conf: gain for RL-confidence-based WM gating (>=0). Confidence = 1 - H(p_rl)/log(3)
    - decay_base: base logit for WM retention; retention = sigmoid(decay_base)*(3/nS)

    Inputs
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size for the current block on each trial
    - age: array with a single value repeated; <=45 => young (0), >45 => old (1)
    - model_parameters: list of 6 parameters as above

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta_base, k_beta_age, wm_weight0, k_wm_conf, decay_base = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # highly deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_eff = max(0.0, beta_base - k_beta_age * age_group) * 10.0
        retention_base = sigmoid(decay_base)  # 0..1
        retention = np.clip(retention_base * (3.0 / float(nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(beta_eff * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # RL confidence via entropy (normalized 0..1)
            H = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_norm = H / np.log(nA)
            conf = 1.0 - H_norm  # higher = more confident

            # Arbitration: WM weight via logistic of intercept + confidence cue
            wm_logit = wm_weight0 + k_wm_conf * conf
            wm_mix = sigmoid(wm_logit)
            # Increase sensitivity to load by scaling down with set size
            wm_mix *= (3.0 / float(nS))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe

            # WM decay toward uniform
            w[s, :] = retention * w[s, :] + (1.0 - retention) * w_0[s, :]

            # Reward-gated WM update: store when correct/rewarded
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and forgetting, plus slot-limited WM with age-penalized capacity.

    Idea
    - RL uses separate learning rates for gains and losses, and a per-visit forgetting toward uniform.
    - WM acts as a limited-capacity store; effective capacity drives both mixture weight and retention.
    - Age group reduces effective WM slots.

    Parameters (6)
    - alpha_pos: RL learning rate for rewards (0..1)
    - alpha_neg: RL learning rate for non-rewards (0..1)
    - beta_base: RL inverse temperature (scaled by 10 internally)
    - slots_base: baseline number of WM slots (>0)
    - age_slot_pen: reduction in WM slots when age_group==1 (>=0)
    - k_forget_rl: RL forgetting rate toward uniform on visited state (0..1)

    Inputs
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size for the current block on each trial
    - age: array with a single value repeated; <=45 => young (0), >45 => old (1)
    - model_parameters: list of 6 parameters as above

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_base, slots_base, age_slot_pen, k_forget_rl = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM slots reduced by age in older group
        slots_eff = max(0.0, slots_base - age_slot_pen * age_group)
        # WM mixture and retention scale with slots/nS
        wm_scale = np.clip(slots_eff / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            wm_mix = wm_scale  # capacity-driven arbitration
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL forgetting toward uniform on the visited state
            q[s, :] = (1.0 - k_forget_rl) * q[s, :] + k_forget_rl * (1.0 / nA)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha * pe

            # WM retention toward uniform depends on capacity
            retention = wm_scale  # 0..1
            w[s, :] = retention * w[s, :] + (1.0 - retention) * w_0[s, :]

            # Reward-driven WM write; non-reward slightly suppresses chosen action
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # reduce weight on the chosen action to reflect error memory
                w[s, a] *= 0.5
                w[s, :] /= np.sum(w[s, :])  # renormalize

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Precision-weighted arbitration between RL and WM with size-driven WM interference, age WM bias, and lapse.

    Idea
    - RL uses delta rule; its reliability is estimated from the inverse normalized entropy of the RL policy.
    - WM stores rewarded items; a latent WM precision per state decays with set-size interference.
    - Arbitration uses relative precision of WM vs RL (mixture = Prec_WM / (Prec_WM + Prec_RL)).
    - Age bias adds WM precision for younger participants (greater WM reliance).
    - A small lapse probability mixes in a uniform random choice; lapse increases with set size and age.

    Parameters (6)
    - alpha_rl: RL learning rate (0..1)
    - beta_base: RL inverse temperature (scaled by 10 internally)
    - wm_precision0: initial WM precision per state (>=0)
    - k_size_interf: WM precision decay rate per extra item beyond 3 (>=0)
    - age_bias_wm: additive WM precision bias for young group (>=0)
    - lapse_base: base logit for lapse; epsilon = sigmoid(lapse_base) * (nS/6) * (1 + 0.5*age_group)

    Inputs
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size for the current block on each trial
    - age: array with a single value repeated; <=45 => young (0), >45 => old (1)
    - model_parameters: list of 6 parameters as above

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta_base, wm_precision0, k_size_interf, age_bias_wm, lapse_base = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM precision per state
        prec_w = np.ones(nS) * max(0.0, wm_precision0)
        # Lapse parameter for this block
        epsilon_base = sigmoid(lapse_base)
        epsilon_block = epsilon_base * (float(nS) / 6.0) * (1.0 + 0.5 * age_group)
        epsilon_block = np.clip(epsilon_block, 0.0, 0.5)  # keep sane

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # RL precision from inverse entropy
            H = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_norm = H / np.log(nA)
            prec_rl = np.clip(1.0 - H_norm, 0.0, 1.0)

            # WM precision with age bias (young get boost)
            prec_wm = max(0.0, prec_w[s] + (age_bias_wm * (1 - age_group)))

            # Arbitration by precision ratio
            wm_mix = prec_wm / (prec_wm + prec_rl + 1e-12)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            # Combine and add lapse
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - epsilon_block) * p_mix + epsilon_block * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += alpha_rl * pe

            # WM precision decay due to set-size interference
            decay = np.exp(-max(0.0, k_size_interf) * max(0.0, (float(nS) - 3.0) / 3.0))
            prec_w[s] *= decay

            # WM value decay toward uniform proportional to precision loss
            decay_w_val = np.clip(decay, 0.0, 1.0)
            w[s, :] = decay_w_val * w[s, :] + (1.0 - decay_w_val) * w_0[s, :]

            # Reward-driven WM write increases precision and sets deterministic policy
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
                prec_w[s] = max(prec_w[s], 1.0 + wm_precision0)

        blocks_log_p += log_p

    return -blocks_log_p