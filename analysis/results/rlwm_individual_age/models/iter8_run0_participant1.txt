def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce–Hall adaptive learning + WM with decay/interference and age-dependent WM noise.

    Rationale:
    - RL learning rate adapts to surprise (unsigned prediction error), enabling fast learning after errors.
    - WM stores state-action values with fast learning but decays over time and with larger set sizes (interference).
    - WM retrieval is noisy, with more noise for older adults.
    - Arbitration mixes WM and RL by a fixed base weight that is reduced with set size.

    Parameters (6):
    - model_parameters[0] = base_lr in [0,1]: baseline RL learning rate
    - model_parameters[1] = k_ph >= 0: Pearce–Hall gain; scales how much surprise modulates RL learning rate
    - model_parameters[2] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[3] = wm_weight_base in [0,1]: base weight of WM in arbitration at set size 3
    - model_parameters[4] = wm_decay in [0,1]: per-trial WM decay rate (higher = faster forgetting)
    - model_parameters[5] = age_wm_noise >= 0: additive WM noise scale increased by age group

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list/array of parameters

    Returns:
    - Negative log-likelihood of observed choices.
    """
    base_lr, k_ph, softmax_beta, wm_weight_base, wm_decay, age_wm_noise = model_parameters

    # Parameter sanitization
    base_lr = min(max(base_lr, 0.0), 1.0)
    k_ph = max(k_ph, 0.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_weight_base = min(max(wm_weight_base, 0.0), 1.0)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    age_wm_noise = max(age_wm_noise, 0.0)

    age_group = 0 if age[0] <= 45 else 1
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM action values
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # attractor to uniform for decay

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy with age-dependent retrieval noise and set-size interference
            # Use a softmax with high baseline beta but reduced by noise and set size
            wm_beta_base = 50.0
            wm_noise = 1.0 + age_wm_noise * age_group  # older -> larger divisor -> more noise
            setsize_interf = 1.0 + max(nS_t - 3, 0)  # more items -> more interference
            wm_beta_eff = wm_beta_base / (wm_noise * setsize_interf)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: base WM weight attenuated by set size
            # wm_weight_base is for set size 3; scale by 3/nS
            wm_weight = wm_weight_base * (3.0 / max(nS_t, 1))
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update: Pearce-Hall adaptive learning rate
            pe = r - Q_s[a]
            lr_t = min(max(base_lr + k_ph * abs(pe), 0.0), 1.0)
            q[s, a] += lr_t * pe

            # WM update: decay towards uniform and reward-driven bump on chosen action
            # Decay/interference
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reinforce chosen action proportionally to reward (fast, one-shot-like)
            # Project back to a probability-like simplex via softmax normalization
            if r > 0.0:
                bump = 1.0  # unit bump for rewarded association
                w[s, a] += bump
            # Normalize WM row to avoid blow-up; acts like soft assignment
            row = w[s, :]
            # Safe normalization: translate to positive then normalize
            row_shift = row - np.min(row)
            w[s, :] = (row_shift + eps) / np.sum(row_shift + eps)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-driven arbitration + WM delta learning with interference.

    Rationale:
    - RL uses a fixed learning rate and softmax temperature.
    - WM is learned via a delta rule with its own learning rate and decays with set size (interference).
    - Arbitration assigns more weight to WM when RL is uncertain (high action entropy),
      with a slope parameter; older adults show reduced arbitration sensitivity.
    - Set size reduces both WM strength and arbitration (harder to use WM with larger sets).

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = wm_learn in [0,1]: WM learning rate (fast encoding)
    - model_parameters[3] = entropy_slope >= 0: slope mapping RL entropy to WM weight
    - model_parameters[4] = wm_interf >= 0: WM interference gain per item over 3
    - model_parameters[5] = age_arb_drop >= 0: reduction in arbitration slope for older adults

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list/array of parameters

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_learn, entropy_slope, wm_interf, age_arb_drop = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    wm_learn = min(max(wm_learn, 0.0), 1.0)
    entropy_slope = max(entropy_slope, 0.0)
    wm_interf = max(wm_interf, 0.0)
    age_arb_drop = max(age_arb_drop, 0.0)

    age_group = 0 if age[0] <= 45 else 1
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM values
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline for decay

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy distribution for entropy
            Q_s = q[s, :]
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits)
            pi_rl /= np.sum(pi_rl)
            p_rl = max(pi_rl[a], eps)

            # RL entropy (in nats)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))

            # WM policy: softmax with fixed high beta, attenuated by interference from set size
            wm_beta_base = 50.0
            wm_beta_eff = wm_beta_base / (1.0 + wm_interf * max(nS_t - 3, 0))
            W_s = w[s, :]
            logits_wm = wm_beta_eff * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm /= np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Arbitration: WM weight increases with RL uncertainty but reduced for older adults and larger sets
            slope_eff = max(entropy_slope - age_arb_drop * age_group, 0.0)
            # Normalize entropy by max entropy log(nA)
            H_norm = H_rl / np.log(nA)
            wm_weight = slope_eff * H_norm
            # Additional set size penalty
            wm_weight *= (3.0 / max(nS_t, 1))
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: delta rule toward one-hot for chosen action when rewarded; slight move otherwise
            target = np.zeros(nA)
            if r > 0:
                target[a] = 1.0
            else:
                # Gentle push away from chosen action on errors
                target[:] = 1.0 / (nA - 1)
                target[a] = 0.0
            w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target

            # Small decay toward baseline to model forgetting/interference per trial
            decay = 0.05 * (1.0 + wm_interf * max(nS_t - 3, 0))
            decay = min(max(decay, 0.0), 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and perseveration + capacity-gated WM mixture.

    Rationale:
    - RL values decay toward uniform within blocks (forgetting), and choices have a stickiness bias
      toward the last chosen action in a state (perseveration).
    - WM stores a single best action per state with high confidence (deterministic when stored).
    - WM availability is limited by an effective capacity K_eff that decreases with set size and with age.
      Arbitration weight equals the probability that the state is within capacity.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[2] = q_forget in [0,1]: RL forgetting rate toward uniform
    - model_parameters[3] = stickiness >= 0: perseveration strength added to last chosen action in a state
    - model_parameters[4] = K_base (>0): baseline WM capacity in items
    - model_parameters[5] = age_K_drop (>=0): capacity drop for older adults (subtracted from K_base)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list/array of parameters

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, q_forget, stickiness, K_base, age_K_drop = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6) * 10.0
    q_forget = min(max(q_forget, 0.0), 1.0)
    stickiness = max(stickiness, 0.0)
    K_base = max(K_base, 1e-6)
    age_K_drop = max(age_K_drop, 0.0)

    age_group = 0 if age[0] <= 45 else 1
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM values used to derive a stored action
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -np.ones(nS, dtype=int)  # for stickiness

        # Track which states are stored in WM and an LRU counter for eviction
        stored_action = -np.ones(nS, dtype=int)
        recency = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Update recency
            recency += 1.0
            recency[s] = 0.0

            # RL policy with stickiness
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: deterministic if stored, otherwise uniform
            if stored_action[s] >= 0:
                p_wm = 1.0 if a == stored_action[s] else 0.0
            else:
                p_wm = 1.0 / nA

            # Compute effective capacity and mixture weight
            K_eff = max(K_base - age_K_drop * age_group, 0.0) / (1.0 + max(nS_t - 3, 0))
            # Probability that a given state is in WM approximated by min(1, K_eff/nS)
            wm_weight = min(1.0, K_eff / max(nS_t, 1))
            # If state not stored, reduce weight to 0 for this trial (availability gating)
            if stored_action[s] < 0:
                wm_weight = 0.0

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            pe = r - q[s, a]
            q[s, a] += lr * pe
            q[s, :] = (1.0 - q_forget) * q[s, :] + q_forget * (1.0 / nA)

            # WM update: track best recent rewarded action and manage capacity via LRU eviction
            # Strengthen chosen action on reward
            if r > 0.0:
                w[s, :] = (1.0 - 0.9) * w[s, :] + 0.9 * w_0[s, :]  # compress others toward baseline
                w[s, a] += 1.0
                # Normalize row
                row = w[s, :]
                row_shift = row - np.min(row)
                w[s, :] = (row_shift + eps) / np.sum(row_shift + eps)

                # Store action if capacity allows
                if stored_action[s] == -1:
                    # Current number of stored states
                    current_K = np.sum(stored_action >= 0)
                    K_cap = int(np.floor(K_eff + 1e-9))
                    if K_cap <= 0:
                        # No capacity: clear all
                        stored_action[:] = -1
                    else:
                        if current_K >= K_cap:
                            # Evict least recently used
                            candidates = np.where(stored_action >= 0)[0]
                            if candidates.size > 0:
                                evict = candidates[np.argmax(recency[candidates])]
                                stored_action[evict] = -1
                # Store or refresh current
                stored_action[s] = a
            else:
                # Mild decay toward baseline without reward
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Update stickiness memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p