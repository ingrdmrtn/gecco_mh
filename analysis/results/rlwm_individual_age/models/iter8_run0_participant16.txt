Below are three standalone cognitive models that implement RL–WM interactions with distinct mechanisms. Each function:

- Accepts: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Uses age group (0 = young, 1 = old) meaningfully
- Incorporates set-size effects
- Returns the negative log-likelihood of the observed choices
- Uses all parameters meaningfully
- Keeps the total number of parameters ≤ 6

All functions assume standard scientific packages (e.g., numpy as np) are already imported elsewhere.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with reward-gated WM encoding and load/age-modulated mixture.
    
    Mechanism:
    - RL: model-free Q-learning with softmax choice.
    - WM: associative table w[s, a] that is near-deterministic at choice time (high beta).
           It undergoes load-dependent interference (toward uniform) each trial.
           Writing into WM is gated by reward via a sigmoid gate that is reduced by load and age.
    - Arbitration: static WM mixture base is modulated by load and age via a sigmoid.
    
    Parameters (model_parameters):
    - lr: Q-learning rate in [0,1].
    - wm_weight_base: base bias toward WM in arbitration; higher -> more WM.
    - softmax_beta: base RL inverse temperature; internally scaled x10.
    - wm_eta: strength of WM overwrite when the gate opens (0..1).
    - gate_sens: sensitivity of the WM gate to reward; higher -> stronger gating by reward.
    - age_gate_offset: additional gate penalty applied for older adults; 0 for young if age_group==0.
    
    Returns:
    - Negative log-likelihood of the observed actions.
    """
    lr, wm_weight_base, softmax_beta, wm_eta, gate_sens, age_gate_offset = model_parameters
    softmax_beta *= 10.0  # match template scaling

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load factor: stronger WM when load is small
        load_factor = 3.0 / nS  # 1.0 for set size 3; 0.5 for set size 6
        # Arbitration weight toward WM (sigmoid bounded 0..1)
        wm_weight = 1.0 / (1.0 + np.exp(-(wm_weight_base * load_factor - age_gate_offset * age_group)))

        # Load-dependent interference toward uniform each trial
        wm_interference = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3 items; ~1 for 6 items

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (softmax)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy probability of chosen action (softmax on WM weights)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay/interference toward uniform (load-dependent)
            w[s, :] = (1.0 - wm_interference) * w[s, :] + wm_interference * w_0[s, :]

            # Reward-gated WM write: stronger when reward=1; reduced by load and age
            # Gate probability via sigmoid: gate higher for r=1; penalized by age and load
            gate_drive = gate_sens * (r - 0.5) - 0.5 * (nS - 3.0) / 3.0 - age_gate_offset * age_group
            gate_prob = 1.0 / (1.0 + np.exp(-gate_drive))

            if np.random.rand() < gate_prob:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and slot-based WM with limited capacity, age- and load-modulated.

    Mechanism:
    - RL: Q-learning with asymmetric learning rates derived from a base rate and a negative multiplier.
    - WM: capacity-limited slots that store rewarded state-action pairs; if a state is in WM,
          its policy is near-deterministic; otherwise WM policy is near-uniform.
          Capacity effectively shrinks with higher load and with age; noisy slots produce softened one-hots.
    - Arbitration: WM weight depends on base bias, load, and age.

    Parameters (model_parameters):
    - lr_base: base positive learning rate (for r=1) in [0,1].
    - neg_mult: multiplier for negative outcomes; lr_neg = min(1, lr_base * neg_mult).
    - softmax_beta: RL inverse temperature; internally scaled x10.
    - wm_weight_base: base WM arbitration bias; higher -> more WM.
    - slot_noise: noise level in WM slot representation (0=noise-free one-hot; larger = blur).
    - age_slot_penalty: penalty applied to both WM capacity and arbitration for older adults (>=0).

    Returns:
    - Negative log-likelihood of the observed actions.
    """
    lr_base, neg_mult, softmax_beta, wm_weight_base, slot_noise, age_slot_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity: shrinks with load and more so for older adults
        # Base capacity at set size 3 ~3 slots; at set size 6 reduced by load factor and age penalty
        load_scale = 3.0 / nS  # 1.0 for 3, 0.5 for 6
        cap_continuous = 3.0 * load_scale * (1.0 - 0.5 * age_slot_penalty * age_group)
        K = max(0, int(np.floor(np.clip(cap_continuous, 0.0, 3.0))))

        # Simple FIFO cache for WM slots: (state -> action)
        cache_states = []
        cache_actions = []

        # Arbitration weight with load and age modulation
        wm_weight = 1.0 / (1.0 + np.exp(-(wm_weight_base * load_scale - age_slot_penalty * age_group)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Build WM state distribution from cache
            if s in cache_states:
                idx = cache_states.index(s)
                wm_act = cache_actions[idx]
                W_s = slot_noise * (1.0 / nA) * np.ones(nA)
                W_s[wm_act] += max(0.0, 1.0 - slot_noise)
                w[s, :] = W_s.copy()  # keep an explicit w table for completeness
            else:
                W_s = (1.0 / nA) * np.ones(nA)
                w[s, :] = W_s.copy()

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetry
            pe = r - Q_s[a]
            if r > 0:
                lr = lr_base
            else:
                lr = min(1.0, lr_base * max(0.0, neg_mult))
            q[s, a] += lr * pe

            # WM cache update: store rewarded pairs; evict FIFO if over capacity
            if r > 0 and K > 0:
                if s in cache_states:
                    # refresh position (move to most recent)
                    idx = cache_states.index(s)
                    cache_states.pop(idx); cache_actions.pop(idx)
                cache_states.append(s); cache_actions.append(a)
                while len(cache_states) > K:
                    cache_states.pop(0); cache_actions.pop(0)
            # If K == 0, WM contributes near-uniform only

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Entropy-based arbitrator between RL and WM with load-dependent WM decay and age-modulated exploration.

    Mechanism:
    - RL: standard Q-learning; age reduces inverse temperature (older => more exploration).
    - WM: decays toward uniform with rate that increases with set size; learns toward the last rewarded
          action for a state with a learning rate. WM policy is near-deterministic at choice time.
    - Arbitration: dynamic, based on relative entropies of WM and RL policies:
          wm_weight = sigmoid(arb_temp * (H_rl - H_wm)), preferring the less entropic (more certain) system.

    Parameters (model_parameters):
    - lr: Q-learning rate in [0,1].
    - softmax_beta: base RL inverse temperature; internally scaled x10 and reduced for older adults.
    - wm_learn: WM learning rate toward the rewarded action (0..1).
    - wm_decay_base: base WM decay toward uniform per trial (0..1), then increased by load.
    - arb_temp: arbitration temperature; larger magnitude -> more decisive weighting by entropy difference.
    - age_beta_shift: reduces RL beta for older adults (beta := beta * exp(-age_beta_shift * age_group)).

    Returns:
    - Negative log-likelihood of the observed actions.
    """
    lr, softmax_beta, wm_learn, wm_decay_base, arb_temp, age_beta_shift = model_parameters
    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    # Age-modulated RL inverse temperature
    softmax_beta = (softmax_beta * 10.0) * np.exp(-age_beta_shift * age_group)

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay (more decay with larger set size)
        load_add = max(0.0, (nS - 3.0) / 6.0)  # 0 for 3, ~0.5 for 6
        wm_decay = np.clip(wm_decay_base + load_add, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy distribution
            logits_rl = softmax_beta * Q_s
            max_lr = np.max(logits_rl)
            prl_vec = np.exp(logits_rl - max_lr)
            prl_vec /= np.sum(prl_vec)
            p_rl = max(prl_vec[a], eps)

            # WM policy distribution
            logits_wm = softmax_beta_wm * W_s
            max_lw = np.max(logits_wm)
            pwm_vec = np.exp(logits_wm - max_lw)
            pwm_vec /= np.sum(pwm_vec)
            p_wm = max(pwm_vec[a], eps)

            # Entropy-based arbitration: prefer the less entropic controller
            H_rl = -np.sum(prl_vec * np.log(np.clip(prl_vec, eps, 1.0)))
            H_wm = -np.sum(pwm_vec * np.log(np.clip(pwm_vec, eps, 1.0)))
            wm_weight = 1.0 / (1.0 + np.exp(-arb_temp * (H_rl - H_wm)))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform, then learn toward one-hot if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size usage:
- Model 1: Age reduces both arbitration toward WM and the WM gating probability; larger set sizes increase WM interference and reduce WM arbitration.
- Model 2: Age reduces effective WM capacity and arbitration; larger set sizes reduce WM capacity. RL uses asymmetric learning derived from parameters (distinct from previously tried combinations).
- Model 3: Age decreases RL inverse temperature (more exploration for older adults). Larger set sizes increase WM decay. Arbitration uses entropy difference dynamically to weight WM vs RL.