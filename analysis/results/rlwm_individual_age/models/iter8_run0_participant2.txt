def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + global choice-kernel perseveration + capacity-limited WM recall that degrades with set size and age.

    Idea:
    - RL learns Q-values with a single learning rate and softmax choice.
    - A global (state-independent) choice-kernel biases repeating the most recent action overall.
    - WM stores state-action associations after rewarded trials and suggests the stored action with a recall probability
      that decreases with set size and with age. Arbitration is a convex mixture based on recall probability.
    
    Parameters (6):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled x10 internally)
    - beta_wm: WM inverse temperature (softmax within WM policy)
    - choice_kernel_gain: strength added to the last chosen action globally (>=0)
    - wm_hit_base: base recall probability for WM at set size 3
    - age_wm_penalty: additional recall penalty for older group (>=0); scaled by age_group
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list of parameters above
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, beta_rl, beta_wm, choice_kernel_gain, wm_hit_base, age_wm_penalty = model_parameters
    softmax_beta = beta_rl * 10.0

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = max(1e-6, beta_wm)  # WM inverse temperature
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states  = states[blocks  == b]
        block_setsizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_setsizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global choice kernel: last chosen action across all states
        last_action_global = -1  # -1 indicates none seen yet

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with global perseveration bias
            Q_s = q[s, :].copy()
            if last_action_global >= 0:
                Q_s[last_action_global] = Q_s[last_action_global] + choice_kernel_gain

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy: softmax over WM weights in current state
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration weight is the probability of successful WM recall:
            # decreases with set size and with age group penalty.
            recall = wm_hit_base * (3.0 / float(nS)) - age_wm_penalty * age_group
            wm_weight = np.clip(recall, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update:
            # Small decay toward uniform each trial to reflect interference with larger set sizes and age
            decay = np.clip(0.05 * (float(nS) / 3.0) * (1.0 + 0.3 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # If rewarded, encode strongly the chosen action in WM (normalize afterward)
            if r > 0:
                gain = 0.7
                w[s, :] *= (1.0 - gain)
                w[s, a] += gain
                w[s, :] = w[s, :] / np.sum(w[s, :])

            last_action_global = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with adaptive arbitration by surprise (prediction error) + WM win-stay cache.

    Idea:
    - RL learns Q-values; softmax choice.
    - WM implements a win-stay cache: if a state was rewarded for an action, WM recommends that action deterministically.
    - Arbitration weight toward WM increases when absolute prediction error is large (fresh information to cache)
      but declines with larger set sizes and in older adults.
    
    Parameters (6):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled x10 internally)
    - beta_wm: WM inverse temperature for cached choice vs others
    - wm_gate_base: base WM weight when |PE| is zero (in [0,1])
    - pe_gate_gain: how much WM weight increases with |PE| (>=0)
    - age_gate_penalty: penalty on WM weight for older group (>=0), scaled by set size
    
    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list of parameters above
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, beta_rl, beta_wm, wm_gate_base, pe_gate_gain, age_gate_penalty = model_parameters
    softmax_beta = beta_rl * 10.0

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = max(1e-6, beta_wm)
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states  = states[blocks  == b]
        block_setsizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_setsizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM cache: -1 means empty; otherwise stores last rewarded action for that state
        wm_cache = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy: if cache exists, bias strongly toward cached action; else uniform
            W_s = w[s, :].copy()
            if wm_cache[s] >= 0:
                cached = wm_cache[s]
                # Set a peaked distribution around the cached action within W_s
                W_s[:] = (1.0 - 0.9) / (nA - 1)
                W_s[cached] = 0.9
            # Convert W_s into softmax probability for the observed action a
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Compute PE for adaptive arbitration
            pe = r - q[s, a]
            abs_pe = np.abs(pe)

            # WM mixture weight: base + gain*|PE|, penalized by set size and age
            size_penalty = (float(nS) / 3.0 - 1.0)  # 0 at set size 3, 1 at set size 6
            wm_weight_raw = wm_gate_base + pe_gate_gain * abs_pe
            wm_weight_raw -= age_gate_penalty * age_group * (1.0 + size_penalty)
            wm_weight = np.clip(wm_weight_raw, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * pe

            # WM cache update: if rewarded, store; if not, do nothing (lose confidence mildly via diffusion)
            if r > 0:
                wm_cache[s] = a

            # WM diffusion/interference: move W_s slightly toward uniform with set-size and age
            leak = np.clip(0.03 + 0.04 * (float(nS) / 3.0) + 0.03 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-modulated learning and WM with binding errors across states.

    Idea:
    - RL learns Q-values; the effective learning rate is reduced in older adults.
    - WM stores rewarded state-action associations, but with a binding error: a fraction of the WM update
      is mis-bound to other states, and this error increases with set size and age.
    - Arbitration uses a fixed WM bias modulated by set size and age.

    Parameters (6):
    - lr_base: base RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled x10 internally)
    - beta_wm: WM inverse temperature
    - wm_binding_error_base: base fraction of WM update that spreads to other states (0..1)
    - wm_weight_base: base arbitration weight for WM at set size 3 (0..1)
    - age_effect: scales both RL learning reduction and WM binding error increase in older adults (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: list of parameters above

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_base, beta_rl, beta_wm, wm_binding_error_base, wm_weight_base, age_effect = model_parameters
    softmax_beta = beta_rl * 10.0

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    # Age-modulated RL learning rate
    lr = np.clip(lr_base * (1.0 - 0.3 * age_effect * age_group), 0.0, 1.0)

    softmax_beta_wm = max(1e-6, beta_wm)
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states  = states[blocks  == b]
        block_setsizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_setsizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration: WM weight decreases with set size and age
            wm_weight_raw = wm_weight_base * (3.0 / float(nS)) * (1.0 - 0.2 * age_effect * age_group)
            wm_weight = np.clip(wm_weight_raw, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM interference toward uniform (mild)
            base_leak = 0.02
            leak = np.clip(base_leak * (float(nS) / 3.0) * (1.0 + 0.4 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # WM binding update with mis-binding across states
            if r > 0:
                # Compute binding error that grows with set size and age
                bind_err = np.clip(wm_binding_error_base * (float(nS) / 3.0) * (1.0 + age_effect * age_group), 0.0, 1.0)
                correct_mass = (1.0 - bind_err)
                spread_mass = bind_err

                # Apply a gain to move probability toward chosen action in the correct state
                gain = 0.6
                # Correct state update
                w[s, :] *= (1.0 - gain * correct_mass)
                w[s, a] += gain * correct_mass
                w[s, :] = w[s, :] / np.sum(w[s, :])

                # Spread the mis-bound part uniformly to other states' chosen-action slots
                if nS > 1:
                    spread_per_state = spread_mass * gain / float(nS - 1)
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        # Slightly bias the same action index in other states
                        w[s_other, :] *= (1.0 - spread_per_state)
                        w[s_other, a] += spread_per_state
                        w[s_other, :] = w[s_other, :] / np.sum(w[s_other, :])

        blocks_log_p += log_p

    return -blocks_log_p