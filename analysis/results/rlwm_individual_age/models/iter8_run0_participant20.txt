def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM mixture with entropy-based arbitration, set-size-dependent WM decay, and age-driven lapse.

    Policy
    - Choice probability is a mixture of RL softmax and WM softmax.
    - Arbitration weight for WM increases with RL uncertainty (entropy) and decreases with set size.
    - A small lapse to uniform responding increases with set size and is amplified by age.

    Learning
    - RL: standard delta rule with a single learning rate.
    - WM: on rewarded trials, moves toward a one-hot association for the chosen action; otherwise decays toward uniform.
          Decay rate increases with set size.

    Age use
    - Age group (0 young, 1 old) amplifies the lapse rate, degrading performance more for older adults.
      Here the participant is young (age_group=0), so lapse is minimal.

    Parameters
    ----------
    model_parameters : list or array-like
        [lr, beta_base, wm_weight_base, wm_decay_base, lapse_age_slope]
        - lr: RL learning rate (0..1).
        - beta_base: base RL inverse temperature; internally scaled by 10.
        - wm_weight_base: base WM mixture weight (0..1).
        - wm_decay_base: base WM consolidation/decay step (0..1). Larger => faster move to target/decay.
        - lapse_age_slope: scales the increase in lapse with set size and age (>=0). Larger => more uniform lapses.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_weight_base, wm_decay_base, lapse_age_slope = model_parameters
    softmax_beta = beta_base * 10.0  # as specified
    softmax_beta_wm = 50.0  # very deterministic WM

    try:
        age_group = 0 if age[0] <= 45 else 1
    except Exception:
        age_group = 0 if age <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx].astype(int)
        block_rewards = np.asarray(rewards)[idx].astype(float)
        block_states = np.asarray(states)[idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        # Helper for entropy
        def entropy(p):
            p = np.clip(p, 1e-12, 1.0)
            return -np.sum(p * np.log(p))

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM softmax
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration: WM weight increases with RL entropy, decreases with set size
            H_rl = entropy(p_rl_vec)
            H_max = np.log(nA)
            rel_uncert = H_rl / H_max  # 0..1
            setsize_scale = 3.0 / nS   # down-weight WM in larger sets
            wm_weight = np.clip(wm_weight_base * rel_uncert * setsize_scale, 0.0, 1.0)

            # Lapse to uniform, grows with set size and age
            lapse = np.clip((nS - 3) / 3.0, 0.0, 1.0) * lapse_age_slope * (1.0 + 0.5 * age_group)
            lapse = np.clip(lapse, 0.0, 0.3)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with set-size-dependent decay
            wm_decay = 1.0 - (1.0 - wm_decay_base) * (3.0 / nS)  # larger nS -> larger decay toward uniform
            wm_decay = np.clip(wm_decay, 0.0, 1.0)

            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]

            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM mixture with slot-limited WM capacity and RL forgetting.

    Policy
    - WM mixture weight equals the probability that the current item is stored in WM,
      given a slot capacity k and set size nS: wm_weight = min(1, k_eff / nS).
    - WM and RL each produce a softmax choice distribution; the final policy is their mixture.

    Learning
    - RL: delta rule with forgetting toward the uniform prior each trial.
    - WM: on reward, stored as a one-hot association; otherwise lightly drifts toward uniform.

    Age use
    - Effective WM slots are reduced in older adults: k_eff = k_slots_base * (1 - 0.3 * age_group).

    Parameters
    ----------
    model_parameters : list or array-like
        [alpha, beta_base, k_slots_base, wm_precision, rl_forget]
        - alpha: RL learning rate (0..1).
        - beta_base: RL inverse temperature (scaled by 10 internally).
        - k_slots_base: baseline WM capacity in items (>=0).
        - wm_precision: concentration of WM policy (0..1) mapping to WM inverse temperature.
        - rl_forget: RL forgetting rate toward uniform per trial (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_base, k_slots_base, wm_precision, rl_forget = model_parameters
    softmax_beta = beta_base * 10.0
    # Map wm_precision in 0..1 to a wide precision range
    softmax_beta_wm = 5.0 + 95.0 * np.clip(wm_precision, 0.0, 1.0)

    try:
        age_group = 0 if age[0] <= 45 else 1
    except Exception:
        age_group = 0 if age <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx].astype(int)
        block_rewards = np.asarray(rewards)[idx].astype(float)
        block_states = np.asarray(states)[idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted WM capacity
        k_eff = max(0.0, k_slots_base * (1.0 - 0.3 * age_group))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax with forgetting (apply forgetting before using for choice)
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * w_0[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM softmax
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Probability item is in WM given slot capacity
            wm_weight = np.clip(k_eff / max(1, nS), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update (store rewarded associations; otherwise slight drift to uniform)
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.9 * w[s, :] + 0.1 * target
            else:
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    PE-gated WM encoding with asymmetric RL learning and age-modulated stickiness.

    Policy
    - Mixture of RL softmax and WM softmax.
    - WM mixture weight is stronger after successful learning (positive PE),
      via a PE-gating mechanism that also scales down with set size.

    Learning
    - RL: asymmetric learning rates for positive and negative prediction errors.
    - WM: encodes strongly on positive outcomes, proportional to positive PE.
          On non-reward, WM decays toward uniform.
    - Choice stickiness (tendency to repeat previous action in the same state) is added to RL logits.
      Stickiness strength increases with age.

    Age use
    - Stickiness is amplified in older adults: stickiness = stickiness_base * (1 + age_gain * age_group).

    Parameters
    ----------
    model_parameters : list or array-like
        [alpha_pos, alpha_neg, beta_base, wm_gate_slope, stickiness_base, age_gain]
        - alpha_pos: RL learning rate for positive PE (0..1).
        - alpha_neg: RL learning rate for negative PE (0..1).
        - beta_base: base RL inverse temperature (scaled by 10 internally).
        - wm_gate_slope: scales WM weight by positive PE and set-size (>=0).
        - stickiness_base: baseline choice stickiness weight added to chosen-last action (>=0).
        - age_gain: multiplicative boost of stickiness for older adults (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_base, wm_gate_slope, stickiness_base, age_gain = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    try:
        age_group = 0 if age[0] <= 45 else 1
    except Exception:
        age_group = 0 if age <= 45 else 1

    stickiness_weight = stickiness_base * (1.0 + age_gain * age_group)

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx].astype(int)
        block_rewards = np.asarray(rewards)[idx].astype(float)
        block_states = np.asarray(states)[idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax with state-specific stickiness
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            if last_action[s] >= 0:
                logits_rl[last_action[s]] += stickiness_weight
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM softmax
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Compute current PE for gating
            pe = r - Q_s[a]
            pe_pos = max(0.0, pe)

            # WM weight: increases with positive PE and shrinks with set size
            setsize_scale = 3.0 / nS
            wm_weight = np.clip(wm_gate_slope * pe_pos * setsize_scale, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            lr = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += lr * pe

            # WM update: encode on reward proportional to positive PE; otherwise decay toward uniform
            if r >= 0.5:
                encode_strength = np.clip(pe_pos, 0.0, 1.0)
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * target
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Update stickiness memory
            last_action[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)