def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with surprise-gated WM reliance and WM decay (leak).

    Mechanisms:
    - RL: tabular Q-learning with single learning rate.
    - WM: stored action template per state updated on reward (one-hot), with per-visit decay (leak) toward uniform.
    - Policy: mixture of WM and RL; WM weight is dynamically gated by surprise |r - Q(s,a)| and reduced under higher set size.
    - Age effect: RL inverse temperature (beta) is reduced for older adults by a learned factor (no reduction for young).

    Parameters (list of 6):
    - model_parameters[0] = alpha_raw: mapped via sigmoid to [0,1] RL learning rate.
    - model_parameters[1] = beta_raw: mapped to beta >= 0 via abs(beta_raw)*10 (then age-scaled).
    - model_parameters[2] = wm_gate0: baseline WM gating offset (real).
    - model_parameters[3] = surprise_gain: how strongly surprise increases WM reliance (real).
    - model_parameters[4] = wm_decay_raw: mapped via sigmoid to [0,1] leak toward uniform for WM per visit.
    - model_parameters[5] = beta_age_shift: mapped via sigmoid to [0,1], scales beta down for older group.

    Inputs:
    - states: array of state indices per trial (0..nS-1 within block)
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards (0/1)
    - blocks: array of block indices
    - set_sizes: array giving set size for the block on each trial (3 or 6)
    - age: array with single repeated participant age
    - model_parameters: parameter vector as above

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    eps = 1e-12

    alpha_raw, beta_raw, wm_gate0, surprise_gain, wm_decay_raw, beta_age_shift = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha_raw))
    wm_decay = 1.0 / (1.0 + np.exp(-wm_decay_raw))
    softmax_beta = abs(beta_raw) * 10.0
    softmax_beta_wm = 50.0

    # Age effect on beta: older -> reduce beta by a learned fraction; young unchanged
    age_group = 0 if age[0] <= 45 else 1
    age_beta_reduce = 1.0 / (1.0 + np.exp(-beta_age_shift))  # in (0,1)
    softmax_beta *= (1.0 - age_group * 0.5 * age_beta_reduce)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states  = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Compute RL policy probability of chosen action
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # Compute WM policy probability of chosen action
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Surprise-based WM gating; also scale by set size (more load -> less WM reliance)
            pe_abs = abs(r - q[s, a])
            wm_weight = 1.0 / (1.0 + np.exp(-(wm_gate0 + surprise_gain * pe_abs)))
            wm_weight *= (3.0 / float(nS))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # Update RL values
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM decay toward uniform each time the state is visited
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # On reward, store a (nearly) one-hot template for the chosen action
            if r == 1:
                w[s, :] = (1.0 / nA) * np.ones(nA)
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with competitive credit assignment and set-size-scaled temperature.

    Mechanisms:
    - RL: Q-learning with single learning rate; distributes a fraction of the prediction error with opposite sign to non-chosen actions (competitive spread).
    - WM: reward-triggered storage with limited precision (no leak); precision is controlled by a noise parameter.
    - Policy: mixture of WM and RL; WM reliance reduced under higher set size; RL temperature decreases under higher set size by a power law.
    - Age effect: older adults rely less on WM (multiplicative down-weight).

    Parameters (list of 6):
    - model_parameters[0] = alpha_raw: mapped via sigmoid to [0,1] RL learning rate.
    - model_parameters[1] = beta0_raw: base inverse temperature; beta = |beta0_raw|*10 scaled by set size.
    - model_parameters[2] = gamma_ss_raw: mapped to >=0 via softplus; controls beta scaling with set size: beta * (3/nS)^gamma.
    - model_parameters[3] = wm_base_raw: mapped via sigmoid to [0,1]; base WM weight (scaled by 3/nS).
    - model_parameters[4] = wm_noise_raw: mapped via sigmoid to [0,1]; WM noise, fidelity = 1 - noise.
    - model_parameters[5] = compete_raw: mapped via sigmoid to [0,1]; fraction of PE spread equally with opposite sign to other actions.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: parameter vector as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eps = 1e-12

    alpha_raw, beta0_raw, gamma_ss_raw, wm_base_raw, wm_noise_raw, compete_raw = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha_raw))
    beta0 = abs(beta0_raw) * 10.0
    gamma = np.log1p(np.exp(gamma_ss_raw))  # softplus >= 0
    wm_base = 1.0 / (1.0 + np.exp(-wm_base_raw))
    wm_noise = 1.0 / (1.0 + np.exp(-wm_noise_raw))
    wm_fidelity = 1.0 - wm_noise
    compete = 1.0 / (1.0 + np.exp(-compete_raw))
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states  = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Set-size effects
        beta = beta0 * (3.0 / float(nS))**gamma
        wm_weight_block = wm_base * (3.0 / float(nS))
        # Age effect on WM reliance
        wm_weight_block *= (1.0 - 0.3 * age_group)
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with competitive spread of PE
            pe = r - q[s, a]
            q[s, a] += alpha * pe
            if nA > 1:
                spread = -compete * alpha * pe / (nA - 1)
                for a_other in range(nA):
                    if a_other != a:
                        q[s, a_other] += spread

            # WM update: on reward, store a noisy one-hot; on non-reward no update
            if r == 1:
                w[s, :] = ((1.0 - wm_fidelity) / (nA - 1)) * np.ones(nA)
                w[s, a] = wm_fidelity

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with state-specific memory strength arbitration and time-based decay.

    Mechanisms:
    - RL: standard Q-learning.
    - WM: reward-locked storage (one-hot with high precision), retained until overwritten; a separate memory strength variable per state decays over visits.
    - Arbitration: WM weight is a sharp sigmoid of (memory strength - threshold), with threshold increasing with set size; older adults get a bonus toward WM gating.
    - Policy: mixture of WM and RL plus small epsilon-greedy noise.

    Parameters (list of 6):
    - model_parameters[0] = alpha_raw: mapped via sigmoid to [0,1] RL learning rate.
    - model_parameters[1] = beta_raw: RL inverse temperature = |beta_raw|*10.
    - model_parameters[2] = wm_thr_raw: mapped via sigmoid to [0,1]; baseline threshold for using WM.
    - model_parameters[3] = time_decay_raw: mapped via sigmoid to [0,1]; per-visit decay rate of memory strength for the current state.
    - model_parameters[4] = age_gate_bonus_raw: mapped via sigmoid to [0,1]; subtracts from threshold for older group (easier WM gating).
    - model_parameters[5] = epsilon_raw: mapped via sigmoid, scaled to [0,0.2]; lapse-like uniform choice mixing.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    eps = 1e-12

    alpha_raw, beta_raw, wm_thr_raw, time_decay_raw, age_gate_bonus_raw, epsilon_raw = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha_raw))
    softmax_beta = abs(beta_raw) * 10.0
    softmax_beta_wm = 50.0
    wm_thr_base = 1.0 / (1.0 + np.exp(-wm_thr_raw))
    time_decay = 1.0 / (1.0 + np.exp(-time_decay_raw))
    age_gate_bonus = 1.0 / (1.0 + np.exp(-age_gate_bonus_raw))
    epsilon = (1.0 / (1.0 + np.exp(-epsilon_raw))) * 0.2

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states  = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        mem_strength = np.zeros(nS)  # in [0,1], decays over time

        # Threshold increases with set size; older group gets a bonus toward using WM (lower threshold)
        thr_eff = wm_thr_base * (float(nS) / 3.0) - age_group * 0.5 * age_gate_bonus

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :].copy()
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :].copy()
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration based on memory strength
            gate_input = 10.0 * (mem_strength[s] - thr_eff)  # sharp transition
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL learning
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # Memory strength decay for the visited state, then reinforcement on reward
            mem_strength[s] = (1.0 - time_decay) * mem_strength[s]
            if r == 1:
                mem_strength[s] = 1.0
                # Store precise WM trace for the rewarded action
                w[s, :] = (1.0 / nA) * np.ones(nA)
                w[s, a] = 1.0
            # On non-reward, keep existing WM trace; no leak here (strength handles arbitration)

        blocks_log_p += log_p

    return -blocks_log_p