def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with asymmetric RL learning, RL forgetting, and load-sensitive WM arbitration.

    Mechanism:
    - RL: Q-learning with asymmetric learning rates for positive/negative prediction errors and global forgetting.
    - WM: fast, reward-gated, state-specific update toward the chosen action; nearly deterministic policy.
    - Arbitration: mixture weight is a logistic transform of a base WM weight penalized by load (set size)
      and by age group (older group receives an additional penalty).

    Parameters (model_parameters):
    - lr_pos: positive RL learning rate for rewarded outcomes (0..1)
    - lr_neg: negative RL learning rate for non-rewarded outcomes (0..1)
    - wm_weight_base: base reliance on WM before load/age penalties (real)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - rl_forget: per-trial RL forgetting/leak toward uniform (0..1)
    - load_penalty: how much each increment from set size 3 to 6 reduces WM reliance (>=0)

    Inputs:
    - states: array of state indices per trial (0..nS-1 within each block)
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0 or 1)
    - blocks: array of block indices per trial
    - set_sizes: array with the block's set size repeated per trial in that block (3 or 6)
    - age: array with a single repeated value (participant's age)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, rl_forget, load_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load effect on arbitration
        load_factor = max(nS - 3, 0) / 3.0
        # Effective WM weight penalized by load and age (older: stronger penalty)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-(wm_weight_base - load_penalty * load_factor - 0.5 * age_group)))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            p_rl = max(rl_probs[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = max(wm_probs[a], 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update with asymmetric learning and global forgetting
            pe = r - q[s, a]
            lr = lr_pos if pe >= 0 else lr_neg
            q *= (1.0 - rl_forget)
            q += rl_forget * (1.0 / nA)
            q[s, a] += lr * pe

            # WM update: reward-gated push toward chosen action
            if r > 0.5:
                # use fast update strength tied to RL positive learning rate
                eta_wm = lr_pos
                w[s, :] = (1.0 - eta_wm) * w[s, :]
                w[s, a] += eta_wm
                # renormalize to be a proper distribution
                w[s, :] /= np.sum(w[s, :])

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with state-specific choice stickiness and load-dependent WM decay.

    Mechanism:
    - RL: single learning rate Q-learning; action selection uses softmax with state-specific stickiness kernel.
    - WM: maintained associative weights per state decay toward uniform with set-size dependent decay.
    - Arbitration: fixed base WM weight modulated by age (older -> lower) and by trial-wise WM sharpness.
      WM sharpness is the entropy reduction of W_s relative to uniform, reducing under high load via decay.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM arbitration weight (real, transformed via logistic)
    - softmax_beta: base RL inverse temperature; scaled internally by 10
    - stickiness_base: base state-specific choice perseveration added to last chosen action (logit bias)
    - wm_decay_base: base WM decay toward uniform per trial (0..1)
    - age_stickiness_boost: how much older age increases stickiness (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, stickiness_base, wm_decay_base, age_stickiness_boost = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific last action tracker for stickiness
        last_action_state = -1 * np.ones(nS, dtype=int)

        # Load-adjusted WM decay
        load_factor = max(nS - 3, 0) / 3.0
        wm_decay = np.clip(wm_decay_base + load_factor * (1.0 - wm_decay_base), 0.0, 1.0)

        # Age-adjusted stickiness
        stickiness = stickiness_base + age_group * age_stickiness_boost

        # Age-adjusted baseline WM reliance
        wm_weight_age = 1.0 / (1.0 + np.exp(-(wm_weight_base - 0.4 * age_group)))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with state-specific stickiness
            Q_s = q[s, :].copy()
            bias = np.zeros(nA)
            if last_action_state[s] >= 0:
                bias[last_action_state[s]] += stickiness
            rl_logits = softmax_beta * (Q_s + bias - np.max(Q_s + bias))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            p_rl = max(rl_probs[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = max(wm_probs[a], 1e-12)

            # WM sharpness for arbitration (0 uniform -> 0, one-hot -> 1)
            wm_conf = (np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA)
            wm_conf = np.clip(wm_conf, 0.0, 1.0)

            wm_weight = np.clip(wm_weight_age * wm_conf, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-gated WM strengthening for current state
            if r > 0.5:
                eta_wm = lr  # tie WM learning speed to RL rate for parsimony
                w[s, :] = (1.0 - eta_wm) * w[s, :]
                w[s, a] += eta_wm
                w[s, :] /= np.sum(w[s, :])

            last_action_state[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + reactive single-shot WM cache with uncertainty gating and age-by-capacity effects.

    Mechanism:
    - RL: standard Q-learning with softmax choice.
    - WM: a reactive cache that stores the last rewarded action for each state (if any).
      If a cached action exists, WM proposes a near-deterministic policy favoring the cached action.
    - Arbitration: WM influence increases when RL is uncertain (high entropy) and when effective WM capacity is high.
      Effective capacity declines with set size and with age; a small WM lapse reduces WM influence globally.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; scaled internally by 10
    - wm_capacity_base: baseline WM capacity scalar (0..1), transformed to weight
    - age_capacity_shift: reduction in effective capacity for older group (>=0)
    - wm_lapse: probability mass shifted away from WM channel (0..1)
    - entropy_gate: sensitivity of arbitration to RL entropy (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as described above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_capacity_base, age_capacity_shift, wm_lapse, entropy_gate = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM cache: one-hot of cached action if known; start as None -> uniform row
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        has_cache = np.zeros(nS, dtype=bool)

        # Effective WM capacity weight depends on load and age
        load_factor = (nS - 3) / 3.0 if nS > 3 else 0.0
        cap = wm_capacity_base - 0.5 * load_factor - age_capacity_shift * age_group
        cap = 1.0 / (1.0 + np.exp(-cap))  # squash to (0,1)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax
            Q_s = q[s, :]
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            p_rl = max(rl_probs[a], 1e-12)

            # WM policy: if cache exists, it is near-deterministic; else uniform
            if has_cache[s]:
                W_s = w[s, :]
            else:
                W_s = w_0[s, :]
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = max(wm_probs[a], 1e-12)

            # RL uncertainty (normalized entropy in [0,1])
            ent = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            ent_norm = ent / np.log(nA)

            # Arbitration: more WM weight when RL is uncertain; scaled by effective capacity and reduced by lapse
            wm_weight = cap * (1.0 / (1.0 + np.exp(-entropy_gate * (ent_norm - 0.5))))
            wm_weight *= (1.0 - wm_lapse)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM cache update: store last rewarded action
            if r > 0.5:
                has_cache[s] = True
                w[s, :] = 0.0
                w[s, a] = 1.0

    return nll