def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with slot-based WM storage, age- and set-size-modulated gating, and stay-bias.

    Mechanism:
    - Choice probability is a mixture of RL and WM policies for the chosen action.
    - RL uses softmax with inverse temperature beta_mult*10 and a single learning rate alpha_rl.
    - WM stores a precise mapping for a state with a probability that depends on set size (capacity sharing)
      and age (older group is less likely to store/use WM).
    - A stay-bias (perseveration) increases the probability of repeating the last action in that state.
    - WM is updated to be more one-hot for rewarded actions and decays toward uniform for unrewarded actions.

    Parameters (6):
    - alpha_rl: RL learning rate (0..1)
    - beta_mult: RL inverse temperature scale (>0), multiplied by 10 internally
    - mix0: baseline WM mixture weight (0..1)
    - slot_util: WM slot utilization/capacity factor (0..1); effective WM gating scales with 3/nS
    - kappa_stay: stay-bias added to last action's value in RL/WM policy (>=0)
    - age_gamma: multiplicative age penalty on WM use (>=0), applied if age_group=1

    Age use:
    - age_group = 1 (older) reduces WM gating by factor 1/(1+age_gamma); younger leaves it unchanged.

    Set size use:
    - WM gating scales by (3/nS), i.e., 1.0 for set size 3 and 0.5 for set size 6.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_rl, beta_mult, mix0, slot_util, kappa_stay, age_gamma = model_parameters
    softmax_beta = beta_mult * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Apply stay-bias by shifting the value of the last chosen action for both policies
            if last_action[s] >= 0:
                Q_s[last_action[s]] += kappa_stay
                W_s[last_action[s]] += kappa_stay

            # RL choice probability for the chosen action (softmax trick using chosen-action normalization)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Effective WM mixture weight depends on set size and age
            ss_factor = 3.0 / float(max(3, nS))  # 1 for 3, 0.5 for 6
            age_factor = 1.0 / (1.0 + age_gamma * age_group)
            wm_weight_eff = mix0 * slot_util * ss_factor * age_factor
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha_rl * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # If rewarded, push WM for this state toward the chosen action (one-hot)
            if r > 0.0:
                eta = 0.9  # strong consolidation upon success
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - eta) * w[s, :] + eta * onehot
            else:
                # If not rewarded, decay WM toward uniform slightly
                decay = 0.2
                w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with recency-based WM gating and valence-asymmetric RL learning.

    Mechanism:
    - RL uses separate learning rates for positive vs. negative prediction errors and softmax temperature beta_scale*10.
    - WM policy is a high-precision softmax over a fast-updating w store.
    - WM mixture weight decays exponentially with the lag since the state's last visit; decay is faster when set size is larger
      and for older participants.
    - WM is updated to be more one-hot after rewards; after non-rewards, it moves toward uniform.

    Parameters (6):
    - alpha_pos: RL learning rate for positive PE (0..1)
    - alpha_neg: RL learning rate for negative PE (0..1)
    - beta_scale: RL inverse temperature scale (>0), multiplied by 10 internally
    - mix_base: baseline WM mixture weight (0..1)
    - decay_rate: base WM recency decay rate (>=0)
    - age_lambda: multiplicative factor increasing decay for older group (>=0)

    Age use:
    - Older group (age_group=1) increases WM decay via factor (1 + age_lambda).

    Set size use:
    - WM weight scaled by ss_factor = 3/nS, and the lag decay uses an additional multiplier 1/ss_factor (faster decay at nS=6).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_scale, mix_base, decay_rate, age_lambda = model_parameters
    softmax_beta = beta_scale * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        last_seen = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute lag since last seen
            if last_seen[s] < 0:
                lag = 0
            else:
                lag = (t - last_seen[s])

            ss_factor = 3.0 / float(max(3, nS))  # 1 for 3, 0.5 for 6
            age_mult = 1.0 + age_lambda * age_group
            # Faster decay for larger set sizes: divide by ss_factor (i.e., multiply by 2 at nS=6)
            eff_decay = decay_rate * age_mult / max(ss_factor, eps)
            wm_weight_eff = mix_base * ss_factor * np.exp(-eff_decay * lag)
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += lr * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                eta = 0.8
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - eta) * w[s, :] + eta * onehot
            else:
                decay = 0.25
                w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM arbitration by relative uncertainty: WM confidence vs RL volatility.

    Mechanism:
    - RL uses softmax with inverse temperature beta_gain*10 and a single learning rate alpha.
    - Track RL uncertainty per state via an exponentially smoothed squared prediction error (volatility proxy).
    - Track WM confidence per state as the entropy complement of w[s,:] (higher when distribution is peaky).
    - Arbitration weight for WM is a sigmoid of (wm_conf - rl_uncert), shifted by age and set size.
      Larger set sizes and older age shift arbitration toward RL (lower WM weight).
    - WM update: success sharpens w toward the chosen action with rate wm_learn; failure pushes toward uniform.

    Parameters (6):
    - alpha: RL learning rate (0..1)
    - beta_gain: RL inverse temperature scale (>0), multiplied by 10 internally
    - wm_learn: WM learning rate toward one-hot on reward (0..1)
    - arb_gain: gain of arbitration sigmoid (>=0)
    - age_shift: positive values shift arbitration toward RL for older group (>=0)
    - ss_shift: shift magnitude per increase in set size (>=0), applied based on 3 vs 6

    Age use:
    - Older group adds +age_shift to the arbitration bias (reduces WM weight).

    Set size use:
    - For nS=6, add +ss_shift to the arbitration bias (reduces WM weight); for nS=3, no shift.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta_gain, wm_learn, arb_gain, age_shift, ss_shift = model_parameters
    softmax_beta = beta_gain * 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        # RL uncertainty as smoothed squared PE per state
        rl_uncert = np.zeros(nS)

        # Forgetting factor for uncertainty proxy depends on age and set size without extra parameters
        # Use an implicit smoothing: gamma_uncert = 0.6 for nS=3 young, smaller for larger set size or older (more weight to new PE^2)
        base_gamma = 0.6
        gamma_uncert = base_gamma - 0.15 * (nS == 6) - 0.1 * age_group
        gamma_uncert = float(np.clip(gamma_uncert, 0.1, 0.9))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute WM confidence (1 - normalized entropy)
            pW = np.clip(W_s / max(np.sum(W_s), eps), eps, 1.0)
            entropy = -np.sum(pW * np.log(pW + eps)) / np.log(nA)  # normalized to [0,1]
            wm_conf = 1.0 - entropy  # 0 uniform, 1 one-hot

            # Arbitration: sigmoid(arb_gain * (wm_conf - rl_uncert - bias))
            bias = (age_shift * age_group) + (ss_shift if nS > 3 else 0.0)
            z = arb_gain * (wm_conf - rl_uncert[s] - bias)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-z))
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # Update RL uncertainty proxy
            rl_uncert[s] = gamma_uncert * rl_uncert[s] + (1.0 - gamma_uncert) * (pe * pe)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * onehot
            else:
                # failure increases dispersion; push toward uniform modestly
                decay = 0.3
                w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p