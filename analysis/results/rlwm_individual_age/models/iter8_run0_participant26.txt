def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM with dynamic gating driven by surprise, set size, and age.

    Mechanism
    - RL system: standard Q-learning per state-action.
    - WM system: fast, high-precision policy over a per-state action distribution w, with decay toward uniform.
    - Arbitration: trial-wise WM weight is a logistic gate that increases with surprise |PE| and decreases with set size
      and in older age.

    Parameters
    - model_parameters: [lr, softmax_beta, gate_base, gate_pe_slope, gate_size_penalty, age_gate_penalty]
        - lr: RL learning rate (0..1)
        - softmax_beta: inverse temperature for RL softmax (scaled by 10 internally)
        - gate_base: baseline WM gating bias (real)
        - gate_pe_slope: sensitivity of WM gate to |prediction error| (>=0)
        - gate_size_penalty: penalty on WM gate per extra item beyond 3 (>=0)
        - age_gate_penalty: additional penalty on WM gate for older adults (>=0)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, gate_base, gate_pe_slope, gate_size_penalty, age_gate_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Previous PE for gating (start at zero surprise)
        prev_abs_pe = 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Size- and age-dependent passive WM decay toward uniform
            decay_strength = sigmoid(gate_size_penalty * max(0, nS - 3) + age_gate_penalty * age_group) * 0.5
            w = (1.0 - decay_strength) * w + decay_strength * w_0

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Dynamic WM gate based on surprise, set size, and age
            gate_input = gate_base + gate_pe_slope * prev_abs_pe - gate_size_penalty * max(0, nS - 3) - age_gate_penalty * age_group
            wm_weight_t = sigmoid(gate_input)

            # Mixture
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: reward-gated write toward one-hot with strength tied to current gate
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                write_strength = wm_weight_t
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * target

                # Normalize to keep valid probabilities
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

            # Update surprise for next trial gating
            prev_abs_pe = abs(pe)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM slots capacity with probabilistic storage.

    Mechanism
    - RL system: Q-learning with separate learning rates for positive and negative prediction errors.
    - WM system: capacity-limited "slots" per block. If state is stored, WM generates near-deterministic policy;
      otherwise WM is uninformative (uniform). Storage is reward-gated and probabilistic when set size exceeds capacity.
    - Arbitration: WM weight is applied only to stored states; otherwise defaults to RL.

    Parameters
    - model_parameters: [lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_slots_young, wm_slots_old]
        - lr_pos: RL learning rate for positive PE (0..1)
        - lr_neg: RL learning rate for negative PE (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - wm_weight_base: baseline WM mixture weight when state is stored (0..1)
        - wm_slots_young: WM slots for younger adults (>=1)
        - wm_slots_old: WM slots for older adults (>=1)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_slots_young, wm_slots_old = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    K = wm_slots_old if age_group == 1 else wm_slots_young
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track whether a state is stored in WM
        stored = np.zeros(nS, dtype=bool)

        # If set size exceeds capacity, storage becomes probabilistic
        if nS <= K:
            p_store = 1.0
        else:
            p_store = min(1.0, max(0.0, K / float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM contribution depends on whether state is stored
            if stored[s]:
                denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = 1.0 / max(eps, denom_wm)
                wm_weight_t = wm_weight_base
            else:
                # Unstored states have uninformative WM (uniform)
                denom_wm = np.sum(np.exp(softmax_beta_wm * ((w_0[s, :]) - (w_0[s, a]))))
                p_wm = 1.0 / max(eps, denom_wm)
                wm_weight_t = 0.0  # rely fully on RL for unstored states

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetry
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0 else lr_neg
            q[s, a] += eta * pe

            # WM storage/update policy: reward-gated
            if r > 0.5:
                # Attempt to store if not already stored, with probability p_store
                if not stored[s]:
                    if np.random.rand() < p_store:
                        stored[s] = True

                if stored[s]:
                    target = np.zeros(nA)
                    target[a] = 1.0
                    # Overwrite with strong write
                    w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM with WM confidence-weighted arbitration and lapse-prone WM retrieval.

    Mechanism
    - RL: standard Q-learning with inverse temperature adjusted by age (older -> lower beta).
    - WM: high-precision but subject to lapses that increase with set size and age.
    - Arbitration: WM mixture weight scales with WM confidence (1 - normalized entropy of WM state distribution).

    Parameters
    - model_parameters: [lr, softmax_beta_base, wm_conf_weight, lapse_base, size_lapse_slope, age_beta_boost]
        - lr: RL learning rate (0..1)
        - softmax_beta_base: base inverse temperature for RL (scaled by 10 internally)
        - wm_conf_weight: max weight placed on WM as a function of WM confidence (0..1)
        - lapse_base: baseline WM lapse intercept (real; passed through sigmoid)
        - size_lapse_slope: slope for lapse increase per extra item beyond 3 (>=0)
        - age_beta_boost: reduces RL beta in older adults: beta = base / (1 + age_beta_boost*age_group) (>=0)

    Returns
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta_base, wm_conf_weight, lapse_base, size_lapse_slope, age_beta_boost = model_parameters
    softmax_beta = softmax_beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted RL beta (older -> lower)
        beta_rl = softmax_beta / (1.0 + age_beta_boost * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Gentle WM decay toward uniform that grows with set size
            decay = 0.1 * sigmoid(size_lapse_slope * max(0, nS - 3))
            w = (1.0 - decay) * w + decay * w_0

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM softmax (deterministic) followed by lapse to uniform
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_softmax = 1.0 / max(eps, denom_wm)

            # Lapse increases with set size and age
            lapse = sigmoid(lapse_base + size_lapse_slope * max(0, nS - 3) + 1.0 * age_group)
            lapse = np.clip(lapse, 0.0, 1.0)

            # Mix WM with uniform due to lapse
            p_uniform = 1.0 / nA
            p_wm = (1.0 - lapse) * p_wm_softmax + lapse * p_uniform

            # WM confidence from entropy
            H = entropy(W_s)
            H_max = np.log(nA)
            conf = 1.0 - (H / H_max)  # 0..1
            wm_weight_t = np.clip(wm_conf_weight * conf, 0.0, 1.0)

            # Arbitration
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update on reward: move toward one-hot action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Write strength tied to confidence complement (write more when uncertain)
                write_strength = 0.5 + 0.5 * (1.0 - conf)
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * target
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p