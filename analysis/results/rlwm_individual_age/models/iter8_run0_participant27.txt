def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Surprise-gated working memory with load-dependent WM precision and age-biased encoding; RL-WM entropy arbitration plus lapse.

    Mechanism
    - RL: tabular Q-learning with softmax.
    - WM: associative table storing state-action weights. Encoding into WM is gated by surprise (|PE|)
      and biased by age group (young vs old). Precision of WM retrieval degrades with set size.
    - Arbitration: WM vs RL mixed using WM entropy (lower entropy => higher WM weight).
    - Lapse: small probability to choose uniformly at random.

    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - beta_rl_base: base inverse temperature for RL; internally scaled by 10
    - beta_wm_base: base inverse temperature for WM; internally scaled by 10, reduced by set size
    - surprise_gate: slope controlling how much surprise (|PE|) increases WM encoding probability
    - age_gate_shift: additive bias on WM encoding for older vs younger (added if old, subtracted if young)
    - lapse: probability of stimulus-independent random responding on each trial (0..0.2 recommended)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl_base, beta_wm_base, surprise_gate, age_gate_shift, lapse = model_parameters
    beta_rl = beta_rl_base * 10.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12
    lapse = np.clip(lapse, 0.0, 0.5)  # keep probabilities sane

    total_log_p = 0.0
    nA = 3
    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        # Values and WM tables
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        U = (1.0 / nA) * np.ones((nS, nA))

        # WM precision degrades with load
        load_penalty = max(0, nS - 3)
        beta_wm = (beta_wm_base * 10.0) / (1.0 + 0.5 * load_penalty)

        # Per-trial decay magnitude also grows with load
        wm_decay = 0.01 + 0.03 * load_penalty

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL policy
            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec = p_rl_vec / max(np.sum(p_rl_vec), eps)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(logits_wm)
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Arbitration via WM entropy (lower entropy => higher weight)
            p_wm_vec_clip = np.clip(p_wm_vec, eps, 1.0)
            H_wm = -np.sum(p_wm_vec_clip * np.log(p_wm_vec_clip)) / np.log(nA)  # normalized 0..1
            wm_weight = np.clip(1.0 - H_wm, 0.0, 1.0)

            # Mixture with lapse
            p_choice = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            p_choice = max(p_choice, eps)
            total_log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # Surprise-gated WM encoding (biased by age)
            # Prob(encode) = sigmoid( surprise_gate * (|PE| - 0.5) + age_bias )
            age_bias = (age_gate_shift if age_group == 1 else -age_gate_shift)
            p_encode = 1.0 / (1.0 + np.exp(-(surprise_gate * (abs(pe) - 0.5) + age_bias)))
            # Soft decay toward uniform
            W = (1.0 - wm_decay) * W + wm_decay * U
            # Encode more strongly if rewarded; if not encoded, small nudge only
            if np.random.rand() < p_encode:
                if r > 0.5:
                    target = np.full(nA, 1e-6)
                    target[a] = 1.0 - (nA - 1) * 1e-6
                    W[s, :] = 0.3 * W[s, :] + 0.7 * target
                else:
                    # After negative feedback, reduce confidence in chosen action in WM
                    W[s, a] = 0.8 * W[s, a] + 0.2 * (1.0 / nA)
            else:
                # Minimal passive drift
                pass

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Interference-driven WM overwriting with load- and age-dependent interference; RL with forgetting.
    
    Mechanism
    - RL: tabular Q-learning with action values, softmax choice, and value forgetting toward uniform.
    - WM: distribution over actions per state. WM is continuously overwritten by interference that
      increases with set size and more for older adults. Positive feedback pushes WM toward the chosen action.
    - Arbitration: WM weight equals the sharpness of WM for the state (max(W_s) as a proxy).
    
    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature; internally scaled by 10
    - wm_strength: scales WM inverse temperature; internally scaled by 10
    - overwrite_base: base interference/overwrite rate per trial (0..1)
    - load_overwrite_slope: how much interference increases per extra item beyond 3 (>=0)
    - age_overwrite_shift: additive increase in interference for older adults; subtracted for young
    
    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_strength, overwrite_base, load_overwrite_slope, age_overwrite_shift = model_parameters
    beta_rl = beta_rl * 10.0
    beta_wm_scale = wm_strength * 10.0
    age_group = 0 if age[0] <= 45 else 1
    age_term = (age_overwrite_shift if age_group == 1 else -age_overwrite_shift)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3
    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        U = (1.0 / nA) * np.ones((nS, nA))

        # Interference/overwrite rate in this block
        load_term = max(0, nS - 3)
        overwrite_rate = np.clip(overwrite_base + load_overwrite_slope * load_term + age_term, 0.0, 1.0)
        beta_wm = max(1e-3, beta_wm_scale) / (1.0 + 0.3 * load_term)

        # RL forgetting toward uniform values
        q_forget = 0.01 + 0.02 * load_term  # slightly more forgetting under load

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL policy
            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec = p_rl_vec / max(np.sum(p_rl_vec), eps)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(logits_wm)
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Arbitration: WM sharpness
            wm_weight = float(np.max(W_s))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update with forgetting
            pe = r - Q[s, a]
            Q[s, a] += lr * pe
            Q = (1.0 - q_forget) * Q + q_forget * (1.0 / nA)

            # WM interference overwrite
            W = (1.0 - overwrite_rate) * W + overwrite_rate * U

            # Reward-driven WM strengthening
            if r > 0.5:
                # Move W_s closer to a peaked distribution on chosen action
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = 0.5 * W[s, :] + 0.5 * target
            else:
                # After negative feedback, reduce weight on chosen action slightly
                W[s, a] = 0.9 * W[s, a] + 0.1 * (1.0 / nA)

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Logistic arbitration by load and age with dual-temperature RL/WM and reward-triggered WM consolidation.

    Mechanism
    - RL: tabular Q-learning, softmax.
    - WM: probability table updated strongly on rewards and mildly on nonrewards; small decay each trial.
    - Arbitration: a block-constant mixture weight set by a logistic function of set size and age group.
      Larger set sizes shift weight toward RL; older age shifts away from WM.
    - Temperatures: separate inverse temperatures for RL and WM.

    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - beta_rl_base: RL inverse temperature, scaled by 10
    - beta_wm_base: WM inverse temperature, scaled by 10
    - arb_bias: bias term in the arbitration logistic (positive favors WM)
    - arb_load_slope: negative values decrease WM weight with larger set sizes
    - arb_age_shift: added if old, subtracted if young; positive reduces WM weight for older adults

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl_base, beta_wm_base, arb_bias, arb_load_slope, arb_age_shift = model_parameters
    beta_rl = beta_rl_base * 10.0
    beta_wm_base = beta_wm_base * 10.0
    age_group = 0 if age[0] <= 45 else 1
    age_term = (arb_age_shift if age_group == 1 else -arb_age_shift)

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        ba = actions[mask].astype(int)
        br = rewards[mask].astype(float)
        bs = states[mask].astype(int)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))
        U = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration weight is block-constant
        load_term = max(0, nS - 3)
        z = arb_bias + arb_load_slope * load_term + age_term
        wm_weight_block = 1.0 / (1.0 + np.exp(-z))
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        # WM temperature mildly penalized by load
        beta_wm = beta_wm_base / (1.0 + 0.2 * load_term)

        # WM decay each trial
        wm_decay = 0.02 + 0.01 * load_term

        for t in range(len(bs)):
            s = int(bs[t]); a = int(ba[t]); r = float(br[t])

            # RL policy
            Q_s = Q[s, :]
            logits_rl = beta_rl * (Q_s - np.max(Q_s))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec = p_rl_vec / max(np.sum(p_rl_vec), eps)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = W[s, :]
            logits_wm = beta_wm * (W_s - np.max(W_s))
            p_wm_vec = np.exp(logits_wm)
            p_wm_vec = p_wm_vec / max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture with fixed arbitration per block
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # WM decay
            W = (1.0 - wm_decay) * W + wm_decay * U

            # Reward-triggered WM consolidation
            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                W[s, :] = 0.4 * W[s, :] + 0.6 * target
            else:
                # Gentle correction after errors
                W[s, a] = 0.9 * W[s, a] + 0.1 * (1.0 / nA)

    return -float(total_log_p)