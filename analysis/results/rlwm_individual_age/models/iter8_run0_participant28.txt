def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + WM with surprise-gated arbitration and set-size interference
    - RL: delta-rule with single learning rate.
    - WM: fast mapping with one-shot write on reward and decay on error; set-size
      increases interference and reduces WM precision; age increases gating threshold.
    - Arbitration: WM weight increases when prediction errors are small (low surprise),
      is reduced in larger sets, and further reduced for older adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_gate_base: baseline logit for WM gating (can be negative or positive).
    - surprise_sensitivity: scales how much small absolute RPE increases WM weight (>=0).
    - ss_interference: strength of set-size-driven interference and WM weakening (>=0).
    - age_gate_shift: added penalty on WM gate and precision for older group (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_gate_base, surprise_sensitivity, ss_interference, age_gate_shift = model_parameters
    softmax_beta *= 10.0  # increase RL beta dynamic range

    # Age group coding
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # baseline WM precision
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM values
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM neutral baseline

        # Set-size and age modifiers
        ss_factor = max(0.0, nS - 3)  # 0 for set size 3, positive for 6
        gate_age_penalty = age_group * age_gate_shift

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability at chosen action (from provided template)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy: softmax over W_s with reduced effective precision under interference/age
            wm_precision_div = 1.0 + ss_interference * ss_factor + gate_age_penalty
            beta_wm_eff = softmax_beta_wm / max(0.1, wm_precision_div)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Arbitration weight: more WM when surprise (|RPE|) is low, penalized by set size and age
            rpe_abs = abs(r - Q_s[a])
            wm_logit = wm_gate_base + surprise_sensitivity * (1.0 - rpe_abs) - (ss_interference * ss_factor) - gate_age_penalty
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - On reward: one-shot write toward chosen action, slowed by interference/age
            # - On no reward: decay toward neutral baseline faster with larger set size
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                learn_strength = 1.0 / max(1.0, wm_precision_div)  # weaker with larger sets/older
                w[s, :] = (1.0 - learn_strength) * w[s, :] + learn_strength * one_hot
            else:
                decay = np.clip(0.2 * (1.0 + ss_interference * ss_factor), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL with valence-asymmetric learning + WM with error-sensitive update and lapses
    - RL: learning rate is higher after negative outcomes (valence asymmetry).
    - WM: rewarded actions are written with a strength that diminishes with set size;
      errors reduce the chosen action's WM weight (anti-Hebbian), then renormalize.
    - Arbitration: fixed WM mixture derived from current WM write strength, reduced by age.
    - Lapses: age- and set-size-dependent lapse rate mixes in uniform choice probability.

    Parameters (model_parameters):
    - lr_base: baseline RL learning rate after reward (0..1).
    - neg_lr_boost: multiplicative boost of RL learning after negative outcomes (>=0).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_strength_base: base WM write strength on reward (0..1).
    - decay_set_mult: how much larger set sizes reduce WM write strength (>=0).
    - age_noise_lapse: scales both arbitration down-weighting and lapse with age (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, neg_lr_boost, softmax_beta, wm_strength_base, decay_set_mult, age_noise_lapse = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        ss_factor = max(0, nS - 3)

        # WM write strength diminishes with set size
        wm_alpha = wm_strength_base / (1.0 + decay_set_mult * ss_factor)
        wm_alpha = np.clip(wm_alpha, 0.0, 1.0)

        # Age- and set-size-dependent lapse
        lapse = np.clip(age_group * age_noise_lapse * (nS / 6.0), 0.0, 0.4)

        # Fixed arbitration derived from WM write strength, reduced by age
        arb_logit = 5.0 * (wm_alpha - 0.5) - age_group * age_noise_lapse
        wm_weight = 1.0 / (1.0 + np.exp(-arb_logit))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability at chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM choice probability at chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture with lapse to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / 3.0)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            lr = lr_base if r > 0 else np.clip(lr_base * (1.0 + neg_lr_boost), 0.0, 1.0)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updates
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                # Penalize the chosen action within WM, soft renormalization toward baseline
                punish = 0.5 * wm_alpha
                w[s, a] = (1.0 - punish) * w[s, a]  # reduce chosen action weight
                # Renormalize by blending toward baseline to keep distribution well-formed
                renorm = 0.2 + 0.3 * (ss_factor > 0)  # slightly stronger in larger sets
                w[s, :] = (1.0 - renorm) * w[s, :] + renorm * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Capacity-limited WM slots with probabilistic access + RL
    - RL: basic delta-rule learning.
    - WM: effective access probability pi = capacity / set_size limits WM influence and
      write strength; retrieval noise reduces WM precision when a state is less likely
      to be stored. Older age reduces effective capacity.
    - Arbitration: mix weight scales with pi and a bias term.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - capacity_C_base: baseline WM capacity in "slots" (>=0).
    - age_capacity_drop: capacity reduction applied for older group (>=0).
    - wm_retrieval_noise: increases WM noise when pi is small (>=0).
    - mix_bias: additive bias on arbitration in logit space (can be +/-).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, capacity_C_base, age_capacity_drop, wm_retrieval_noise, mix_bias = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and WM access probability
        capacity = max(0.1, capacity_C_base - age_group * age_capacity_drop)
        pi = np.clip(capacity / float(nS), 0.0, 1.0)

        # Arbitration weight grows with pi
        wm_logit = mix_bias + 5.0 * (pi - 0.5)
        wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

        # Retrieval noise reduces WM beta when pi is small
        beta_wm_eff = softmax_beta_wm / (1.0 + wm_retrieval_noise * (1.0 - pi))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability at chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM choice probability at chosen action with effective precision
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Write strength proportional to storage/access probability
                write_k = pi
                w[s, :] = (1.0 - write_k) * W_s + write_k * one_hot
            else:
                # Small decay toward baseline when not rewarded; weaker when pi is high
                decay = 0.1 * (1.0 - pi)
                w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p