def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity- and decay-limited WM; age and set-size reduce WM arbitration weight.

    Mechanism:
    - RL: delta-rule value learning per state-action.
    - WM: state-specific fast store of most recent rewarded action, with decay toward uniform.
    - Arbitration: WM weight is reduced as a function of set size (relative to a capacity)
      and age group. Effective WM weight is applied per block.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM arbitration weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_decay: WM decay toward uniform each trial (0..1)
    - wm_capacity_base: nominal number of slots; effective WM weight scales with wm_capacity_base/nS
    - age_load_penalty: increases WM weight reduction with age and load (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity_base, age_load_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity/load/age adjusted WM weight for this block
        cap_scale = min(1.0, max(eps, wm_capacity_base) / max(1, nS))
        load_excess = max(0, nS - 3)
        age_load_scale = np.exp(-age_load_penalty * (age_group + load_excess))
        wm_weight_block = np.clip(wm_weight * cap_scale * age_load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy for the chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy for the chosen action (near-deterministic)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: reinforce rewarded action for the current state toward one-hot
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Fast overwrite tendency on rewarded trials
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + Uncertainty-gated WM; age and load raise uncertainty, increasing WM reliance.

    Mechanism:
    - RL: delta-rule update with per-trial forgetting toward uniform.
    - WM: rapid store of action-outcome association for rewarded actions.
    - Arbitration: compute RL policy uncertainty (entropy). WM weight increases with entropy,
      and is further boosted by age group and set size.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base WM arbitration weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - rl_forget: RL forgetting toward uniform each trial (0..1)
    - uncert_slope: slope controlling sensitivity to uncertainty in gating WM (>=0)
    - age_uncert_bias: additive bias to uncertainty from age group (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rl_forget, uncert_slope, age_uncert_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy distribution over actions for current state
            logits = softmax_beta * q[s, :]
            logits -= np.max(logits)
            prl_all = np.exp(logits)
            prl_all_sum = max(np.sum(prl_all), eps)
            prl_all /= prl_all_sum
            p_rl = prl_all[a]

            # Entropy of RL policy as uncertainty proxy (0..log nA)
            H = -np.sum(prl_all * np.log(np.maximum(prl_all, eps)))
            # Normalize load contribution to [0,1] using log base of 6 (max set size in task)
            load_term = np.log(nS) / np.log(6.0)
            # Uncertainty signal augmented by age group
            uncert = H + age_uncert_bias * age_group + load_term

            # Map uncertainty to WM arbitration weight via sigmoid and base weight
            wm_gate = 1.0 / (1.0 + np.exp(-uncert_slope * (uncert)))
            wm_weight_eff = np.clip(wm_weight * wm_gate, 0.0, 1.0)

            # WM policy for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (w[s, :] - w[s, a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with forgetting toward uniform
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay then reinforce if rewarded
            w = 0.9 * w + 0.1 * w_0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.3 * w[s, :] + 0.7 * one_hot
            else:
                # Slight forgetting on negative feedback to avoid false bindings
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with PE-driven WM arbitration and WM leak; age/load shift the PE operating point.

    Mechanism:
    - RL: delta-rule per state-action.
    - WM: associative memory that leaks to uniform each trial and is strengthened by reward.
    - Arbitration: WM weight is a continuous sigmoid function of absolute prediction error (PE).
      The PE operating point shifts rightward with load and age, reducing WM reliance for
      older/high-load conditions.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: maximum WM weight (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - pe_sensitivity: slope of sigmoid over |PE| (>=0)
    - wm_leak: WM leak toward uniform each trial (0..1)
    - age_explore_bias: shift of PE operating point with age/load (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, pe_sensitivity, wm_leak, age_explore_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Operating point shift increases with load and age
        load_excess = max(0, nS - 3)
        pe_shift = age_explore_bias * (age_group + load_excess)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy prob for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy prob for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute absolute PE prior to update
            delta_pre = r - q[s, a]
            # Continuous gating by |PE| with load/age shift
            gate_arg = pe_sensitivity * (abs(delta_pre) - pe_shift)
            wm_gate = 1.0 / (1.0 + np.exp(-gate_arg))
            wm_weight_eff = np.clip(wm_weight * wm_gate, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM leak toward uniform then reinforce on reward
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Stronger update when RL is surprised (use abs PE)
                alpha_wm = np.clip(0.3 + 0.4 * abs(delta), 0.0, 1.0)
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p