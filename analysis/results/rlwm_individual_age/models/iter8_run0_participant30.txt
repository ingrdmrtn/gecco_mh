def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Entropy-gated capacity/interference WM with age-modulated interference.

    Idea:
    - RL: delta-rule Q-learning.
    - WM: fast encoding of rewarded action, subject to proactive interference that scales with set size and is worse for older group.
    - Arbitration: WM weight increases when WM policy is sharp (low entropy); reduced in larger set sizes.
    - Age: older group suffers stronger interference (age_interf_gain), effectively reducing WM precision.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr_rl, beta_base, wm_beta, interference_base, age_interf_gain, arb_bias]
        - lr_rl: RL learning rate (0..1).
        - beta_base: baseline inverse temperature for RL; internally scaled by 10.
        - wm_beta: inverse temperature for WM policy (>0).
        - interference_base: baseline WM interference/decay toward uniform per trial (0..1).
        - age_interf_gain: multiplicative increase in interference for older group (>=0).
        - arb_bias: bias term controlling WM weight via entropy gate (real number).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_rl, beta_base, wm_beta, interference_base, age_interf_gain, arb_bias = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = wm_beta
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference factor scales with set size and age group
        size_factor = nS / 3.0  # 1 for 3, 2 for 6
        interf = interference_base * size_factor * (1.0 + age_interf_gain * age_group)
        interf = np.clip(interf, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight from entropy gate; lower entropy -> higher WM reliance
            eps = 1e-12
            prob = np.clip(W_s, eps, 1.0)
            prob = prob / prob.sum()
            entropy = -np.sum(prob * np.log(prob)) / np.log(nA)  # normalized 0..1
            # Reduce WM weight in larger set sizes
            size_penalty = (nS - 3) / 3.0  # 0 for 3, 1 for 6
            gate_input = arb_bias + (1.0 - entropy) - 0.5 * size_penalty
            wm_weight = 1.0 / (1.0 + np.exp(-5.0 * gate_input))  # fixed slope for identifiability
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr_rl * delta

            # WM interference (global drift toward uniform)
            w = (1.0 - interf) * w + interf * w_0
            # WM encoding: rewarded action strengthened; non-reward weakly repelled
            if r > 0.0:
                # Move W_s toward a one-hot on chosen action
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                w[s, a] += 0.5 * (1.0 - w[s, a])
            else:
                # Mild aversive update: push mass away from chosen action
                w[s, a] += -0.2 * (w[s, a] - 1.0 / nA)

            # Renormalize WM row to a probability simplex
            row = np.maximum(w[s, :], 1e-8)
            w[s, :] = row / row.sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Volatility-adaptive RL and WM with set-size-dependent WM decay and age-modulated temperature.

    Idea:
    - Track a running estimate of volatility via unsigned prediction errors; use it to adapt RL learning rate and arbitration.
    - WM decays faster at larger set sizes; WM is one-shot strengthened on reward.
    - Arbitration weight favors WM when volatility is low; age increases exploration (lower inverse temperature).

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr_base, beta_base, wm_beta, vol_gain, wm_decay_base, age_temp_boost]
        - lr_base: baseline RL learning rate (0..1).
        - beta_base: baseline inverse temperature for RL; scaled by 10 internally.
        - wm_beta: inverse temperature for WM policy (>0).
        - vol_gain: gain for volatility tracking and its influence (0..1).
        - wm_decay_base: base WM decay toward uniform per trial (0..1).
        - age_temp_boost: reduction of RL inverse temperature for older group (>0), applied via exp(-age_temp_boost*age_group).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_base, beta_base, wm_beta, vol_gain, wm_decay_base, age_temp_boost = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = wm_beta
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective temperature reduced for older group
        beta_eff = softmax_beta * np.exp(-age_temp_boost * age_group)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        volatility = 0.0  # running unsigned PE

        # Set-size dependent WM decay
        wm_decay = np.clip(wm_decay_base * (nS / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM trusted when volatility low; modest penalty for larger set size
            size_penalty = (nS - 3) / 3.0  # 0..1
            wm_weight = np.clip(1.0 - volatility - 0.3 * size_penalty, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL learning rate adapts with volatility
            pe = abs(r - Q_s[a])
            volatility = (1.0 - vol_gain) * volatility + vol_gain * pe
            lr_rl = np.clip(lr_base + vol_gain * (pe - 0.5), 0.0, 1.0)

            # RL update
            q[s, a] += lr_rl * (r - Q_s[a])

            # WM decay toward uniform (capacity limit)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding: one-shot strengthening on reward, gentle forgetting otherwise
            if r > 0.0:
                # Pull row toward a one-hot on chosen action
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
                w[s, a] += 0.6 * (1.0 - w[s, a])
            else:
                # Mild diffusion toward uniform on error
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Renormalize WM row
            row = np.maximum(w[s, :], 1e-8)
            w[s, :] = row / row.sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Success-tracking arbitration with set-size and age-modulated lapse.

    Idea:
    - RL: standard delta-rule Q-learning.
    - WM: fast learning with mild decay; policy via softmax.
    - Arbitration weight equals a state-specific running success estimate (m_s), downscaled at larger set sizes.
    - Lapse probability increases with set size and with age; lapses choose uniformly at random.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, beta_base, wm_beta, lapse_base, size_lapse_gain, age_lapse_gain]
        - lr: RL learning rate (0..1).
        - beta_base: baseline inverse temperature for RL; scaled by 10 internally.
        - wm_beta: inverse temperature for WM policy (>0).
        - lapse_base: baseline lapse rate (0..1).
        - size_lapse_gain: additive lapse increase when set size is 6 (>=0).
        - age_lapse_gain: additive lapse increase for older group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_beta, lapse_base, size_lapse_gain, age_lapse_gain = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = wm_beta
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        m = 0.5 * np.ones(nS)  # state-specific success estimate for arbitration

        # Lapse rate depends on set size and age
        size_is_six = 1.0 if nS == 6 else 0.0
        lapse = np.clip(lapse_base + size_lapse_gain * size_is_six + age_lapse_gain * age_group, 0.0, 0.99)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight is the current success estimate for this state,
            # discounted by set size (capacity pressure)
            size_scale = (nS - 3) / 3.0  # 0..1
            wm_weight = np.clip(m[s] * (1.0 - 0.4 * size_scale), 0.0, 1.0)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: fast strengthening on reward, mild decay otherwise
            if r > 0.0:
                w[s, :] = 0.85 * w[s, :] + 0.15 * w_0[s, :]
                w[s, a] += 0.5 * (1.0 - w[s, a])
            else:
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

            # Renormalize WM row
            row = np.maximum(w[s, :], 1e-8)
            w[s, :] = row / row.sum()

            # Update success estimate for arbitration (state-specific EMA)
            m[s] = 0.7 * m[s] + 0.3 * r

        blocks_log_p += log_p

    return -blocks_log_p