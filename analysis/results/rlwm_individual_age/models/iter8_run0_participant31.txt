def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recall-gated Working Memory (WM) with set-size and age effects.

    Description:
    - RL learns Q-values with a single learning rate and a softmax policy.
    - WM stores rewarded associations and decays toward uniform on every trial.
    - WM retrieval is probabilistic (recall). The effective recall probability
      decreases with set size and with age (older group lower recall).
    - Arbitration: convex combination of WM and RL policies, with a base WM weight
      reduced by set size and age. WM policy itself is a mixture of perfect recall
      and a uniform lapse according to recall probability.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial (constant within block).
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_base, wm_weight_base, wm_recall_base, wm_decay_base, age_recall_penalty]
        - lr: RL learning rate (0..1).
        - beta_base: Base inverse-temperature for RL; internally scaled by 10.
        - wm_weight_base: Baseline arbitration weight on WM before set-size/age scaling.
        - wm_recall_base: Baseline probability of retrieving WM (per probe).
        - wm_decay_base: Baseline decay of WM toward uniform per trial visit.
        - age_recall_penalty: Multiplicative recall penalty applied if age group is old.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_weight_base, wm_recall_base, wm_decay_base, age_recall_penalty = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM retrieval
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))       # RL Q-values
        w = (1.0 / nA) * np.ones((nS, nA))       # WM weights
        w_0 = (1.0 / nA) * np.ones((nS, nA))     # Uniform prior for decay anchor

        # Set-size and age adjustments
        wm_weight_eff = wm_weight_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        recall_penalty = (age_recall_penalty if age_group == 1 else 1.0)
        wm_recall_eff = np.clip(wm_recall_base * (3.0 / nS) / recall_penalty, 0.0, 1.0)

        wm_decay_eff = np.clip(wm_decay_base * (nS / 3.0) * (1.0 + 0.4 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            softmax_beta = max(beta_base, 1e-6) * 10.0
            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM retrieval policy with recall lapse to uniform
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = wm_recall_eff * p_wm_det + (1.0 - wm_recall_eff) * (1.0 / nA)

            # Arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM encoding on rewarded outcomes: push toward chosen action
            if r > 0.0:
                # Strong encoding towards one-hot for chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite with strong pull; then renormalize
                w[s, :] = 0.1 * w[s, :] + 0.9 * one_hot
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL + WM with WSLS bias and uncertainty-weighted arbitration.

    Description:
    - RL uses separate learning rates for positive and negative prediction errors,
      with a softmax policy.
    - WM stores rewarded actions and decays; its influence is reduced in larger set
      sizes and in older adults.
    - A win-stay/lose-shift (WSLS) transient bias is implemented within the WM
      policy: after a win in a state, WM boosts the last action; after a loss, it
      depresses the last action (encouraging shift).
    - Arbitration: base WM weight scaled by set size and age, further modulated by
      state-uncertainty (higher RL uncertainty -> more WM). RL uncertainty is
      approximated via the visitation count per state.

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr_pos, lr_neg, beta_rl, wm_weight_base, wsls_bias, age_wm_shift]
        - lr_pos: Learning rate for positive PE.
        - lr_neg: Learning rate for negative PE.
        - beta_rl: Base inverse temperature for RL (scaled by 10 internally).
        - wm_weight_base: Baseline WM mixture weight.
        - wsls_bias: Magnitude of WSLS logit bias added to WM policy.
        - age_wm_shift: Additional reduction of WM weight applied to older adults.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, beta_rl, wm_weight_base, wsls_bias, age_wm_shift = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and last outcome per state for WSLS
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        visits = np.zeros(nS)  # count visits per state for uncertainty

        # Base WM weight scaled by set size and age
        wm_weight_eff_base = wm_weight_base * (3.0 / nS) * (1.0 - (0.2 + age_wm_shift) * age_group)
        wm_weight_eff_base = np.clip(wm_weight_eff_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            visits[s] += 1.0

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # Inject WSLS bias into WM logits for current state
            if last_action[s] >= 0:
                if last_reward[s] > 0.0:
                    # Win-stay: boost last chosen action
                    W_s[last_action[s]] += wsls_bias
                else:
                    # Lose-shift: depress last chosen action
                    W_s[last_action[s]] -= wsls_bias

            # Compute policies
            beta = max(beta_rl, 1e-6) * 10.0
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-weighted arbitration: rely more on WM when RL is uncertain (low visits)
            # reliability term in [0,1], decreasing with visits
            reli = 1.0 / np.sqrt(1.0 + visits[s])
            wm_weight_eff = np.clip(wm_weight_eff_base * (1.0 + reli), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr * pe

            # WM decay and encoding
            w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot
                w[s, :] /= np.sum(w[s, :])

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with misbinding across states and reliability-based arbitration.

    Description:
    - RL updates standard Q-values and uses a softmax policy.
    - WM stores rewarded actions but suffers from two imperfections:
        (1) decay toward uniform, stronger at larger set sizes and in older adults;
        (2) misbinding: a fraction of the WM update "leaks" to other states, which
            increases with set size and age (feature-to-state binding errors).
    - Arbitration weight on WM is scaled by set size and age and is further
      reduced as RL becomes more certain (more visits to the current state).

    Parameters
    ----------
    states : array-like
        State index per trial.
    actions : array-like
        Chosen action index per trial (0..2).
    rewards : array-like
        Reward per trial (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size (3 or 6) per trial.
    age : array-like
        Participant age (single value repeated). Age group: 0 if <=45, 1 if >45.
    model_parameters : list or tuple
        [lr, beta_base, wm_weight_base, wm_misbind_base, wm_decay_base, age_misbind_gain]
        - lr: RL learning rate.
        - beta_base: Base inverse-temperature for RL (scaled by 10 internally).
        - wm_weight_base: Baseline WM mixture weight.
        - wm_misbind_base: Baseline fraction of WM update that leaks to other states.
        - wm_decay_base: Baseline WM decay toward uniform per visit.
        - age_misbind_gain: Multiplicative increase in misbinding for older adults.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_weight_base, wm_misbind_base, wm_decay_base, age_misbind_gain = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        visits = np.zeros(nS)

        # Effective parameters with set-size and age modulation
        wm_weight_base_eff = wm_weight_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_weight_base_eff = np.clip(wm_weight_base_eff, 0.0, 1.0)

        wm_decay_eff = np.clip(wm_decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        misbind_eff = wm_misbind_base * (nS / 3.0) * (1.0 + age_misbind_gain * age_group)
        misbind_eff = np.clip(misbind_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            visits[s] += 1.0

            Q_s = q[s, :]
            W_s = w[s, :]

            beta = max(beta_base, 1e-6) * 10.0
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Reliability-based arbitration: rely relatively more on WM at low RL exposure
            reli = 1.0 / (1.0 + visits[s])  # in (0,1]
            wm_weight_eff = np.clip(wm_weight_base_eff * (1.0 + reli), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM encoding with misbinding on rewarded outcomes
            if r > 0.0:
                # Correct-state encoding pull toward chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot
                w[s, :] /= np.sum(w[s, :])

                # Misbinding leak: distribute a fraction of the encoding to other states
                if nS > 1 and misbind_eff > 0.0:
                    leak = misbind_eff
                    per_other = leak / (nS - 1)
                    for j in range(nS):
                        if j == s:
                            continue
                        # Nudge other states toward the same action a by a small amount
                        w[j, a] = (1.0 - per_other) * w[j, a] + per_other * 1.0
                        # Renormalize each affected other state row
                        row_sum = np.sum(w[j, :])
                        if row_sum > 0:
                            w[j, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p