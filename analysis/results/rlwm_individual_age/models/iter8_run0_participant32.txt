def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility) + WM with rehearsal and set-size–dependent interference.

    Mechanism
    - RL: standard delta rule with eligibility trace per state to boost recent learning.
    - WM: a fast memory map per state that is rehearsed on every visit and strengthened on reward.
    - Arbitration: fixed wm_weight mixes WM and RL policies.
    - Set-size effect: more interference/decay of WM at larger set sizes; rehearsal counters interference.
    - Age effect: older group reduces decision precision and WM retention via age_noise.

    Parameters
    ----------
    states : array-like of int
        State per trial (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight, trace, wm_rehearsal, age_noise]
        - alpha: RL learning rate
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_weight: mixing weight for WM in arbitration (0..1)
        - trace: eligibility trace persistence (0..1)
        - wm_rehearsal: rehearsal gain per state visit (0..1)
        - age_noise: scales age-related reduction in precision/WM retention

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, softmax_beta, wm_weight, trace, wm_rehearsal, age_noise = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age reduces RL precision and WM retention
    beta_age_factor = 1.0 - 0.25 * age_noise * age_group
    wm_retention_age = 1.0 - 0.3 * age_noise * age_group
    softmax_beta *= max(beta_age_factor, 0.1)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and state eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # WM distributions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size–dependent RL precision (mild effect)
            beta_eff = softmax_beta / (1.0 + 0.2 * max(0, nS - 3))

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mix policies
            wm_w_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            # Decay all eligibilities and set current to 1 for chosen action at current state
            e *= trace
            e[s, :] *= 0.0
            e[s, a] = 1.0

            delta = r - Q_s[a]
            q += alpha * delta * e

            # WM update: decay/interference + rehearsal + reward-based boost
            # Interference grows with set size; age reduces retention
            forget = (0.05 + 0.25 * max(0, nS - 3) / 3.0)
            forget *= (1.0 / max(wm_retention_age, 1e-3))  # more forgetting if older
            forget = np.clip(forget, 0.0, 0.9)

            # Drift toward uniform due to interference
            w = (1.0 - forget) * w + forget * w_0

            # Rehearsal on visited state s: sharpen toward chosen action
            reh = np.clip(wm_rehearsal, 0.0, 1.0)
            w[s, :] = (1.0 - reh) * w[s, :]
            w[s, a] += reh
            # Reward-based consolidation (one-shot-like)
            if r > 0.0:
                cons = 0.5  # fixed strength (kept within param budget)
                w[s, :] = (1.0 - cons) * w[s, :]
                w[s, a] += cons

            # Renormalize WM distributions
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Confidence-gated arbitration: WM-RL mixture modulated by WM entropy and set size.

    Mechanism
    - RL: delta rule with single learning rate.
    - WM: fast map updated toward chosen action, with decay toward uniform under load.
    - Arbitration: trial-wise WM weight is a sigmoid of negative WM entropy (higher confidence -> more WM).
    - Set-size effect: increases WM decay and reduces RL precision.
    - Age effect: shifts arbitration toward RL for older adults (age_arbitration_shift).

    Parameters
    ----------
    states : array-like of int
        State per trial (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial's block.
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight0, wm_conf_gain, beta_conf_gain, age_arbitration_shift]
        - alpha: RL learning rate
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_weight0: base mixing weight (bias)
        - wm_conf_gain: scales influence of (-entropy) on WM weight
        - beta_conf_gain: increases RL precision when WM entropy is high (uncertain)
        - age_arbitration_shift: reduces WM use for older adults

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, softmax_beta, wm_weight0, wm_conf_gain, beta_conf_gain, age_arbitration_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age shifts arbitration away from WM
    wm_bias_age = wm_weight0 - age_group * np.clip(age_arbitration_shift, 0.0, 1.0)
    wm_bias_age = np.clip(wm_bias_age, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Set-size effects
            beta_eff = softmax_beta / (1.0 + 0.25 * max(0, nS - 3))
            wm_forget = 0.08 + 0.28 * max(0, nS - 3) / 3.0  # 0.08 at 3, ~0.36 at 6
            wm_forget = np.clip(wm_forget, 0.0, 0.9)

            Q_s = q[s, :]
            W_s = w[s, :]

            # WM entropy as confidence proxy
            eps = 1e-12
            H = -np.sum(W_s * np.log(W_s + eps))  # [0, ln 3]
            H_norm = H / np.log(3.0)  # 0..1
            conf = 1.0 - H_norm  # 1=high confidence

            # Confidence-gated arbitration
            wm_w = wm_bias_age + wm_conf_gain * (conf - 0.5)
            wm_w = np.clip(wm_w, 0.0, 1.0)

            # RL precision boosted by uncertainty (compensatory)
            beta_comp = beta_eff * (1.0 + beta_conf_gain * H_norm)

            # Policies
            p_rl = 1.0 / np.sum(np.exp(beta_comp * (Q_s - Q_s[a])))
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform (interference)
            w = (1.0 - wm_forget) * w + wm_forget * w_0
            # State-specific sharpening toward chosen action; reward adds extra consolidation
            sharpen = 0.25 + 0.5 * r  # 0.25 if no reward, 0.75 if reward
            w[s, :] = (1.0 - sharpen) * w[s, :]
            w[s, a] += sharpen
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay and choice stickiness + WM with lateral inhibition.

    Mechanism
    - RL: delta rule with value decay (forgetting) each trial; choice stickiness biases repeats.
    - WM: updated toward chosen action; non-chosen actions at the visited state are inhibited more strongly.
    - Arbitration: fixed wm_weight to combine WM and RL.
    - Set-size effect: increases RL decay and WM decay due to interference.
    - Age effect: older adults show stronger WM decay (age_wm_decay).

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of int
        Participant age (repeated). Age group: 0 if <=45 else 1.
    model_parameters : list or array
        [alpha, softmax_beta, wm_weight0, rl_decay, stickiness, age_wm_decay]
        - alpha: RL learning rate
        - softmax_beta: base inverse temperature for RL (scaled internally by 10)
        - wm_weight0: mixing weight for WM
        - rl_decay: per-trial decay rate applied to all Q-values
        - stickiness: bias added to the last chosen action
        - age_wm_decay: additive WM decay increase for older adults

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, softmax_beta, wm_weight0, rl_decay, stickiness, age_wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    wm_weight = np.clip(wm_weight0, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Decay both RL and WM depending on set size and age
            rl_decay_eff = np.clip(rl_decay + 0.05 * max(0, nS - 3) / 3.0, 0.0, 0.5)
            q = (1.0 - rl_decay_eff) * q + rl_decay_eff * (1.0 / nA)

            wm_decay = 0.06 + 0.22 * max(0, nS - 3) / 3.0 + age_group * np.clip(age_wm_decay, 0.0, 0.5)
            wm_decay = np.clip(wm_decay, 0.0, 0.9)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with stickiness
            sticky_bias = np.zeros(nA)
            if last_action is not None:
                sticky_bias[last_action] = stickiness
            Q_eff = Q_s + sticky_bias

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update with lateral inhibition at current state
            inhibit = 0.5  # non-parametric, within budget
            excite = 0.5 if r > 0.0 else 0.25
            # Inhibit non-chosen actions
            for aa in range(nA):
                if aa != a:
                    w[s, aa] *= (1.0 - inhibit)
            # Excite chosen
            w[s, a] = (1.0 - excite) * w[s, a] + excite
            # Renormalize
            w[s, :] = w[s, :] / np.sum(w[s, :])

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p