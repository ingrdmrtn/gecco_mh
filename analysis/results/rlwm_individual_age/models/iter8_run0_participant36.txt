def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + gated WM with selective maintenance and state-wise perseveration.

    Summary:
    - Two-learning-rate RL updates Q-values.
    - A WM store w holds one-hot action mappings when confident; otherwise decays.
    - A gating policy determines whether WM is used, decreasing with set size and in older adults.
    - State-wise perseveration bias favors repeating the last action in the same state.
    - RL temperature decreases under higher load; older adults are more affected.

    Parameters (list of 6):
    - lr_pos: learning rate for positive prediction errors.
    - lr_neg: learning rate for negative prediction errors.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - wm_gate_base: baseline WM gate logit; higher -> more WM use.
    - wm_selective: probability of writing a one-hot (deterministic) WM trace after reward; else partial update.
    - perseveration_state: additive bias to repeat last action within the same state.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: 1D array with a single repeated value; age_group = 0 (<=45), 1 (>45).
    - model_parameters: list of 6 parameters as defined above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_base, wm_gate_base, wm_selective, perseveration_state = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise last action for perseveration
        last_action_state = -1 * np.ones(nS, dtype=int)

        # Effective RL temperature reduced by load and age
        softmax_beta_eff = softmax_beta / (1.0 + 0.5 * (nS - 3.0) + 0.4 * age_group)

        # WM gate probability as a logistic function of base, reduced by load and age
        gate_logit = wm_gate_base - 0.8 * max(0, nS - 3) - 0.7 * age_group
        p_gate = 1.0 / (1.0 + np.exp(-gate_logit))
        p_gate = np.clip(p_gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy with state-wise perseveration bias
            logits_rl = softmax_beta_eff * Q_s
            if last_action_state[s] >= 0:
                logits_rl[last_action_state[s]] += perseveration_state

            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy (deterministic softmax over WM values)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Mixture policy using gate probability
            p_total = p_gate * p_wm + (1.0 - p_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr * pe

            # WM updating: rewarded trials store more deterministic one-hot with probability wm_selective
            if r > 0.0:
                if np.random.rand() < np.clip(wm_selective, 0.0, 1.0):
                    one_hot = np.zeros(nA)
                    one_hot[a] = 1.0
                    w[s, :] = one_hot
                else:
                    # partial sharpening toward one-hot
                    target = np.zeros(nA)
                    target[a] = 1.0
                    w[s, :] = 0.5 * w[s, :] + 0.5 * target
            else:
                # unrewarded: gentle decay toward uniform, stronger with load and age
                decay = np.clip(0.1 * (1.0 + 0.5 * age_group + 0.5 * max(0, nS - 3)), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            last_action_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility trace + WM precision with load/age scaling + lapse.

    Summary:
    - RL uses an eligibility trace (for bandit-like states it reduces to reinforcing the visited state more strongly).
    - WM provides a sharp policy; its precision decreases when set size exceeds capacity and in older adults.
    - Lapse probability mixes in uniform responding; lapses increase with age and load.
    - Final policy is a three-way mixture: (1 - lapse) * [alpha_wm * WM + (1 - alpha_wm) * RL] + lapse * uniform.

    Parameters (list of 6):
    - lr: RL learning rate.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - wm_precision_base: base WM precision scaling (higher -> more deterministic WM).
    - capacity: WM capacity in number of states (where precision starts to drop).
    - trace: eligibility trace factor that boosts the chosen state-action update.
    - age_lapse_gain: scales lapse increase with age group.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: 1D array with a single repeated value; age_group = 0 (<=45), 1 (>45).
    - model_parameters: list of 6 parameters as defined above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_precision_base, capacity, trace, age_lapse_gain = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL temperature reduced under load and age
        beta_rl = softmax_beta / (1.0 + 0.3 * (nS - 3.0) + 0.3 * age_group)

        # WM precision declines when nS exceeds capacity and in older adults
        overload = max(0.0, nS - capacity)
        beta_wm = max(1.0, wm_precision_base * 25.0 / (1.0 + 0.8 * overload + 0.6 * age_group))

        # WM weighting as a function of precision (mapped to [0,1])
        alpha_wm = 1.0 - 1.0 / (1.0 + 0.1 * beta_wm)
        alpha_wm = np.clip(alpha_wm, 0.0, 1.0)

        # Lapse probability increases with age and load
        lapse = np.clip(0.02 + age_lapse_gain * (0.05 + 0.05 * age_group + 0.03 * max(0, nS - 3)), 0.0, 0.3)

        log_p = 0.0
        # Initialize eligibility for each state-action (replacing trace for chosen pair)
        e = np.zeros_like(q)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Mixture with lapse
            p_mix = alpha_wm * p_wm + (1.0 - alpha_wm) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility trace (replacing)
            e *= trace
            e[s, :] *= 0.0
            e[s, a] = 1.0
            pe = r - Q_s[a]
            q += lr * pe * e

            # WM update: reward-driven one-hot; else decay to uniform with load/age-scaled decay
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                decay = np.clip(0.1 + 0.05 * age_group + 0.05 * max(0, nS - capacity), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + adaptive WM weighting driven by recent prediction errors.

    Summary:
    - RL controller learns Q-values; temperature is reduced by load and further in older adults.
    - WM stores rewarded mappings; unrewarded trials decay WM toward uniform.
    - The WM weight is adapted on each trial based on recent absolute prediction error:
      lower PE (stable mapping learned) increases WM reliance; high PE shifts to RL.
    - Age and set size penalize WM weight multiplicatively.

    Parameters (list of 6):
    - lr: RL learning rate.
    - beta_base: base inverse temperature for RL (scaled by 10 internally).
    - wm_weight_base: baseline WM mixture weight (in logit space).
    - pe_sensitivity: sensitivity of WM weight to recent absolute PE (negative effect).
    - decay_unrewarded: WM decay rate on unrewarded trials toward uniform.
    - age_load_penalty: multiplicative penalty coefficient applied to WM weight by age/load.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays of equal length.
    - age: 1D array with a single repeated value; age_group = 0 (<=45), 1 (>45).
    - model_parameters: list of 6 parameters as defined above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_weight_base, pe_sensitivity, decay_unrewarded, age_load_penalty = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50.0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL temperature reduced by load and age
        beta_rl = softmax_beta / (1.0 + 0.4 * (nS - 3.0) + 0.4 * age_group)

        # Initialize running absolute PE (per state) for WM gating
        avg_abs_pe = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Compute policies
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-300)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-300)

            # Adaptive WM weight from base logit, penalized by recent abs PE and age/load
            base_logit = wm_weight_base
            penalty = pe_sensitivity * avg_abs_pe[s]  # higher abs PE -> lower weight if pe_sensitivity > 0
            load_age_pen = age_load_penalty * (age_group + max(0, nS - 3))
            wm_weight_logit = base_logit - penalty - load_age_pen
            wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture probability
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Update running absolute PE (EWMA, faster in small set size)
            k = 0.3 / (1.0 + 0.3 * max(0, nS - 3))
            avg_abs_pe[s] = (1.0 - k) * avg_abs_pe[s] + k * abs(pe)

            # WM update: reward -> store one-hot; no reward -> decay toward uniform with specified rate amplified by load/age
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                decay = np.clip(decay_unrewarded * (1.0 + 0.5 * age_group + 0.5 * max(0, nS - 3)), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p