def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM Model 1: Dual-learning-rate RL + capacity-limited WM with interference.

    Mechanism:
    - RL: Q-learning with separate learning rates for positive vs negative prediction errors.
    - WM: a fast, correct-response cache (one-hot) subject to interference/decay each trial.
    - Policy: mixture of a high-gain WM softmax and an RL softmax.
    - Age and set size effects: WM weight is scaled by an effective capacity term (wm_capacity/nS)
      and reduced in older adults.

    Parameters (model_parameters; all used):
    - alpha_pos: RL learning rate for positive prediction errors (0..1).
    - alpha_neg: RL learning rate for negative prediction errors (0..1).
    - beta_base: base inverse temperature for RL; internally scaled by 10.
    - wm_capacity: nominal WM capacity (in "slots"); determines WM reliance as wm_capacity/nS.
    - interference_rate: per-trial interference/decay rate moving WM toward uniform (0..1).
    - wm_weight_base: baseline WM mixture weight before age and set-size scaling (0..1).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta_base, wm_capacity, interference_rate, wm_weight_base = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture scaling by capacity, set size and age
        capacity_scale = np.clip(wm_capacity / float(nS), 0.0, 1.0)
        age_scale = (1.0 - 0.30 * age_group)  # older rely less on WM
        wm_weight_eff = np.clip(wm_weight_base * capacity_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action via denominator trick
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: near-deterministic softmax over WM weights
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with dual learning rates
            pe = r - q[s, a]
            lr_use = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += lr_use * pe

            # WM interference/decay toward uniform
            w = (1.0 - interference_rate) * w + interference_rate * w0

            # WM update: store correct association on reward
            if r > 0.5:
                # Overwrite current state's memory with a strong one-hot trace
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM Model 2: Pearce–Hall attention in RL + surprise-gated WM reliance.

    Mechanism:
    - RL: Q-learning with state-specific dynamic learning rate following a Pearce–Hall rule:
      alpha_s,t = clip(alpha0_base + surprise_gain * |PE_{s,t-1}|, 0, 1).
    - WM: fast cache with decay toward uniform; correct responses refresh WM.
    - Policy: mixture; WM weight is reduced by surprise (when RL needs to adjust more)
      and by set size; older adults have lower WM reliance.
    - Set-size scaling: (3/nS)^size_sensitivity penalizes WM in larger sets.

    Parameters (model_parameters; all used):
    - alpha0_base: base RL learning rate (0..1).
    - beta_base: base inverse temperature for RL; internally scaled by 10.
    - wm_base: baseline WM mixture weight (0..1).
    - surprise_gain: scales both RL attention increase and WM down-weighting by surprise.
    - size_sensitivity: exponent controlling how strongly WM weight drops with set size.
    - decay_wm: per-trial decay of WM toward uniform (0..1).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha0_base, beta_base, wm_base, surprise_gain, size_sensitivity, decay_wm = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last unsigned PE per state for attention
        last_abs_pe = np.zeros(nS)

        # Base WM reliance after set-size and age scaling
        size_scale = (3.0 / float(nS)) ** np.clip(size_sensitivity, 0.0, None)
        age_scale = (1.0 - 0.25 * age_group)
        wm_weight_base = np.clip(wm_base * size_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Pearce–Hall dynamic learning rate for this state
            alpha_s = np.clip(alpha0_base + surprise_gain * last_abs_pe[s], 0.0, 1.0)

            # Surprise-gated WM reliance (more surprise -> rely less on WM)
            wm_gate = 1.0 / (1.0 + surprise_gain * last_abs_pe[s])
            wm_weight_eff = np.clip(wm_weight_base * wm_gate, 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha_s * pe
            last_abs_pe[s] = abs(pe)

            # WM decay toward uniform
            w = (1.0 - decay_wm) * w + decay_wm * w0

            # WM update: reward refreshes memory; errors slightly push away chosen action
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            else:
                # small corrective nudge away from the chosen wrong action
                w[s, a] = max(w0[s, a], w[s, a] * (1.0 - 0.1))

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM Model 3: Optimistic RL initialization + WM retention with age- and set-size-dependent lapses.

    Mechanism:
    - RL: Q-learning with a single learning rate, but optimistic initial Q values (v_init).
    - WM: fast cache with retention controlled by a time constant tau_wm_retention (decay per trial).
    - Policy: mixture of WM and RL; in addition, a lapse process chooses uniformly at random.
    - Lapse probability increases with age group and with set size (harder condition).

    Parameters (model_parameters; all used):
    - lr: RL learning rate (0..1).
    - beta_base: base inverse temperature for RL; internally scaled by 10.
    - wm_weight0: baseline WM mixture weight (0..1).
    - tau_wm_retention: retention time constant; larger -> slower WM decay (>0).
    - v_init: optimistic initial Q value added to uniform initialization.
    - lapse_gain: controls increase of lapse with age and set size.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_weight0, tau_wm_retention, v_init, lapse_gain = model_parameters
    softmax_beta = beta_base * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0
    eps = 1e-12

    # Convert tau to per-trial decay rate in (0,1)
    decay_rate = 1.0 / max(tau_wm_retention, 1e-6)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL optimistic initialization around uniform
        q = (1.0 / nA + v_init) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight scaled by set size and age
        size_scale = 3.0 / float(nS)
        age_scale = (1.0 - 0.35 * age_group)
        wm_weight_eff = np.clip(wm_weight0 * size_scale * age_scale, 0.0, 1.0)

        # Lapse probability depends on difficulty (set size) and age
        difficulty = age_group + (nS - 3.0) / 3.0  # 0 for young-3, up to ~2 for old-6
        p_lapse = 1.0 / (1.0 + np.exp(-lapse_gain * difficulty))  # sigmoid in (0,1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_action = (1.0 - p_lapse) * p_mix + p_lapse * (1.0 / nA)
            log_p += np.log(max(p_action, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM retention/decay toward uniform
            w = (1.0 - decay_rate) * w + decay_rate * w0

            # WM update: correct outcomes refresh memory
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p