Below are three standalone cognitive models that follow the provided template structure. Each model blends RL and WM, uses age group (older vs younger) and set size (3 vs 6) meaningfully, and returns the negative log-likelihood of the observed choices. All parameters are used and each model has at most six parameters.

--------------------------------------------------------------------------------
def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, age- and load-modulated WM with decay.

    Idea:
    - RL learns Q-values with a single learning rate and softmax action selection.
    - WM stores a near-deterministic mapping from state to action, but is capacity-limited and decays.
    - Effective WM encoding rate is proportional to an effective capacity K_eff divided by set size (nS).
    - Older age reduces K_eff (age_decline) and larger set size dilutes WM efficacy (via K_eff/nS).
    - WM also decays towards uniform with a load- and age-modulated decay parameter.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: baseline mixture weight between WM and RL (0..1)
    - softmax_beta: RL inverse temperature before upscaling (template multiplies by 10)
    - decay_base: base WM decay rate toward uniform (0..1)
    - K_base: baseline WM capacity in slots (e.g., around 3)
    - age_decline: fractional reduction in capacity for older adults (0..1). K_eff = K_base * (1 - age_decline*age_group)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, decay_base, K_base, age_decline = model_parameters
    softmax_beta *= 10  # per template scaling

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # near-deterministic WM policy
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM weights (prob-like)
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform attractor

        # Effective WM capacity and decay (both age and load dependent)
        K_eff = max(0.0, K_base * (1.0 - age_decline * age_group))
        wm_encode_rate = np.clip(K_eff / max(nS, 1.0), 0.0, 1.0)  # scaled by load
        decay_eff = np.clip(decay_base * (nS / 6.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy probability of the chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy probability of the chosen action (softmax over w)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = np.clip(wm_weight0, 0.0, 1.0) * p_wm + (1.0 - np.clip(wm_weight0, 0.0, 1.0)) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]
            # WM encoding on reward: move toward a one-hot on the rewarded action
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_encode_rate) * w[s, :] + wm_encode_rate * target
            # Renormalize WM row to keep it probability-like
            w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p


--------------------------------------------------------------------------------
def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM gated by surprise (RPE magnitude) and age/load effects.

    Idea:
    - RL uses separate learning rates for positive/negative prediction errors.
    - WM encodes the rewarded action when a gate opens; gate probability increases with |RPE|.
    - Gate probability also decreases with age and larger set size; WM also decays implicitly with load and age.
    - WM policy is softmax over w with a WM temperature that is higher (more deterministic) for small set size and younger age.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PEs (0..1)
    - lr_neg: RL learning rate for negative PEs (0..1)
    - wm_weight: static mixture weight between WM and RL (0..1)
    - softmax_beta: RL inverse temperature before upscaling (template multiplies by 10)
    - gate_sensitivity: sensitivity of WM encoding gate to |RPE| (higher -> more encoding)
    - wm_beta_base: baseline WM inverse temperature (before state/load/age scaling)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gate_sensitivity, wm_beta_base = model_parameters
    softmax_beta *= 10  # per template

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM weights
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # uniform attractor

        # WM temperature becomes more deterministic for small nS and younger age
        wm_beta = wm_beta_base * (3.0 / max(nS, 1.0)) * (1.0 + 0.5 * (1 - age_group))
        wm_beta = np.clip(wm_beta, 1.0, 100.0)

        # Implicit WM decay factor (no extra parameter): faster decay with larger nS and older age
        decay_factor = np.clip(0.15 * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.9)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(wm_beta * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            w_mix = np.clip(wm_weight, 0.0, 1.0)
            p_total = w_mix * p_wm + (1.0 - w_mix) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_use * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - decay_factor) * w[s, :] + decay_factor * w_0[s, :]

            # WM encoding gate based on |RPE|, attenuated by age and load
            # Gate uses a smooth sigmoid around |pe| ~ 0.5
            gate_drive = gate_sensitivity * (abs(pe) - 0.5)
            p_gate = 1.0 / (1.0 + np.exp(-gate_drive))
            p_gate *= (3.0 / max(nS, 1.0))  # lower at high load
            p_gate *= (1.0 - 0.3 * age_group)  # lower for older
            p_gate = np.clip(p_gate, 0.0, 1.0)

            if r == 1 and p_gate > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - p_gate) * w[s, :] + p_gate * target

            # Renormalize WM row
            w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p


--------------------------------------------------------------------------------
def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + entropy-based arbitration with WM cache and age/load-modulated decay.

    Idea:
    - RL learns Q-values; WM stores a cached mapping like a probability over actions.
    - Arbitration adjusts the WM-RL mixture weight dynamically based on policy uncertainty:
      more WM use when RL is uncertain and WM is confident, and vice versa.
    - Arbitration is modulated by age and set size: older adults rely less on WM; larger set
      sizes reduce the impact of WM (via both decay and arbitration).
    - WM decays toward uniform with a parameter; WM is updated toward the rewarded action.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: baseline WM mixture weight (0..1) before arbitration
    - softmax_beta: RL inverse temperature before upscaling (template multiplies by 10)
    - rl_uncertainty_gain: scales RL entropy's contribution to arbitration (>=0)
    - wm_decay: WM decay rate toward uniform (0..1)
    - age_arbitration: shifts arbitration toward RL for older adults (>=0 reduces WM use when age_group=1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, rl_uncertainty_gain, wm_decay, age_arbitration = model_parameters
    softmax_beta *= 10  # per template

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy distribution
            Q_s = q[s, :]
            probs_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs_rl = probs_rl / max(np.sum(probs_rl), eps)
            p_rl = max(probs_rl[a], eps)

            # WM policy distribution
            W_s = w[s, :]
            probs_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            probs_wm = probs_wm / max(np.sum(probs_wm), eps)
            p_wm = max(probs_wm[a], eps)

            # Entropy-based arbitration (higher RL entropy -> shift weight toward WM)
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))
            H_wm = -np.sum(probs_wm * np.log(np.clip(probs_wm, eps, 1.0)))
            # Normalize entropies to [0, log(nA)] range; difference drives arbitration
            entropy_signal = rl_uncertainty_gain * (H_rl - H_wm)

            # Set-size penalty and age-driven reduction of WM reliance
            load_term = -np.log(max(nS / 3.0, 1.0))  # <= 0, penalizes larger sets
            age_term = -age_arbitration * age_group   # <= 0 for older (reduces WM weight)

            arb_input = entropy_signal + load_term + age_term
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-(arb_input)))  # sigmoid in [0,1]
            # Blend with baseline mixture wm_weight0
            wm_weight_eff = np.clip(0.5 * wm_weight0 + 0.5 * wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay depends on set size and age
            decay_eff = np.clip(wm_decay * (nS / 6.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            # WM supervised update on reward
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
                # Learning rate toward target scales down with load and age implicitly via decay already applied
                eta = np.clip(0.6 * (3.0 / max(nS, 1.0)) * (1.0 - 0.2 * age_group), 0.0, 1.0)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target

            # Normalize
            w[s, :] = w[s, :] / max(np.sum(w[s, :]), eps)

        blocks_log_p += log_p

    return -blocks_log_p

--------------------------------------------------------------------------------

Notes on age and load effects:
- Model 1: WM capacity K is reduced for older adults (age_decline) and diluted by set size (K_eff/nS). WM decay increases with set size and age.
- Model 2: WM encoding gate based on |RPE| is reduced by age and larger set sizes; WM becomes hotter (less deterministic) with larger set sizes and for older adults; decay increases with set size and age.
- Model 3: Arbitration shifts weight toward WM when RL is uncertain relative to WM; older age and larger set sizes reduce the WM influence; WM decay increases with set size and age.