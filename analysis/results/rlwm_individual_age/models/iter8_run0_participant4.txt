def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + probabilistic-gating WM with set-size interference and age-amplified noise.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: fast, precise store over actions per state, updated probabilistically after reward.
           WM traces decay toward uniform via interference that grows with set size and is amplified by age.
    - Mixture: policy is a convex combination of WM and RL action probabilities.

    Age and set-size influences:
    - Interference/decay of WM increases with set size and is amplified by age.
    - Probability of storing into WM decreases with set size.

    Parameters (list; 6 total):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL; internally scaled by 10.
    - wm_store_prob: baseline probability to store a rewarded action into WM (0..1).
    - wm_noise_age_scale: multiplicative increase of WM interference for older group (>=0). Young group uses 0 add-on.
    - setsize_interference: how much WM interference grows with set size (>=0).

    Inputs:
    - states: array of stimulus/state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0/1).
    - blocks: block index per trial.
    - set_sizes: array of set sizes per trial (constant within a block; 3 or 6).
    - age: array with a single repeated value for participant age.
    - model_parameters: list of parameter values.

    Output:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_store_prob, wm_noise_age_scale, setsize_interference = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM interference/decay grows with set size and age
        interference = setsize_interference * max(0, nS - 1)
        interference *= (1.0 + age_group * wm_noise_age_scale)
        # Keep in [0,1] for stability
        interference = np.clip(interference, 0.0, 1.0)

        # Probability to store into WM decreases with set size
        # e.g., effective gate = wm_store_prob / (1 + (nS-1))
        gate_den = 1.0 + max(0, nS - 1)
        wm_store_eff = wm_store_prob / gate_den
        wm_store_eff = np.clip(wm_store_eff, 0.0, 1.0)

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay/interference towards uniform
            w[s, :] = (1.0 - interference) * w[s, :] + interference * w_0[s, :]

            # WM gated storage upon reward: move w[s] toward one-hot of action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_store_eff) * w[s, :] + wm_store_eff * one_hot

            # Normalize for numerical stability
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with lapse noise.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate.
    - WM: stores the last rewarded action per state (deterministic store), with mild leak to uniform.
    - Mixture: WM and RL policies combined; then a lapse probability mixes in uniform random choice.

    Age and set-size influences:
    - Effective WM mixture weight decreases as set size exceeds a capacity threshold.
      wm_weight_eff = wm_weight * sigmoid(capacity_C - nS * (1 + age_group * age_capacity_shift))
      Thus, larger set sizes and older age reduce WM reliance.
    - Lapse (uniform) noise applies equally across ages but is estimated separately.

    Parameters (list; 6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight: baseline WM weight before capacity modulation (0..1).
    - capacity_C: WM capacity threshold controlling set-size drop-off (real).
    - age_capacity_shift: multiplier for how much age increases effective set size burden (>=0).
    - lapse_rate: probability of random responding (0..1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters: as specified.

    Output:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, capacity_C, age_capacity_shift, lapse_rate = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight via capacity-limited sigmoid
        eff_load = nS * (1.0 + age_group * age_capacity_shift)
        wm_scale = 1.0 / (1.0 + np.exp(-(capacity_C - eff_load)))  # sigmoid in (0,1)
        # We want WM to decrease with higher load: multiply baseline by (1 - wm_scale) or similar.
        # Here: higher eff_load -> smaller (capacity_C - eff_load) -> wm_scale closer to 0 -> less downweight.
        # To ensure monotonic decrease with load, use wm_weight_eff = wm_weight * (1 - wm_scale).
        wm_weight_eff = wm_weight * (1.0 - wm_scale)
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        # Mild WM leak
        wm_leak = 0.10

        # Lapse
        lapse = np.clip(lapse_rate, 0.0, 1.0)
        uni = 1.0 / nA

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture then lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * uni
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM leak toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # Rewarded storage: set to one-hot of chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size- and age-adjusted temperature + uncertainty-gated WM, with WM decay.

    Mechanism:
    - RL: tabular Q-learning with inverse temperature reduced by set size and further reduced for older adults.
    - WM: fast store with decay; on reward, WM shifts toward the chosen action.
    - Mixture: WM weight increases when RL is uncertain (high entropy) and decreases when RL is confident.

    Age and set-size influences:
    - RL temperature: beta_eff = beta / (1 + beta_setsize_sens*(nS-1)) / (1 + age_group*age_beta_penalty).
      Larger set size and older age reduce RL determinism.
    - WM decay increases with age (via multiplicative factor below).

    Parameters (list; 6 total):
    - lr: RL learning rate (0..1).
    - wm_weight0: baseline WM weight scaling (0..1), modulated by RL uncertainty.
    - softmax_beta: RL inverse temperature baseline; internally scaled by 10.
    - beta_setsize_sens: how strongly set size reduces RL beta (>=0).
    - age_beta_penalty: multiplicative penalty to RL beta for older group (>=0).
    - wm_decay: baseline WM decay toward uniform each trial (0..1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters: as specified.

    Output:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, beta_setsize_sens, age_beta_penalty, wm_decay = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective RL beta adjusted by set size and age
        beta_eff = softmax_beta / (1.0 + beta_setsize_sens * max(0, nS - 1))
        beta_eff = beta_eff / (1.0 + age_group * age_beta_penalty)

        # WM decay adjusted by age (older -> more decay/leak)
        wm_decay_eff = wm_decay * (1.0 + 0.5 * age_group)
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # Compute RL uncertainty via softmax entropy
            # First get full RL softmax over actions for state s
            prefs = beta_eff * Q_s
            prefs = prefs - np.max(prefs)  # stability
            probs = np.exp(prefs)
            probs = probs / np.sum(probs)
            entropy = -np.sum(probs * np.log(np.clip(probs, 1e-12, 1.0)))
            entropy_norm = entropy / np.log(nA)  # normalize to [0,1]

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Uncertainty-gated mixture: rely on WM when RL is uncertain
            wm_weight_eff = np.clip(wm_weight0 * entropy_norm, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Reward-based strengthening toward one-hot of chosen action
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Move w toward one-hot by the same decay factor (acts like learning rate for WM)
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * one_hot

            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p