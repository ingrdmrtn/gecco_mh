def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-biased learning and capacity-based WM arbitration that degrades with set size and age.

    Mechanisms
    - RL: learning rate is asymmetric via a multiplicative factor for negative prediction errors.
    - WM: item-specific one-shot storage after reward; contents decay toward uniform with an interference-driven decay.
    - Arbitration: WM weight is a logistic function of an effective capacity that decreases with larger set size and for older adults.

    Parameters
    - model_parameters: [lr_base, neg_mult, beta_rl, wm_cap_base, k_age_cap, gate_temp]
        lr_base      : base RL learning rate for positive PEs (0..1).
        neg_mult     : multiplicative factor on lr_base for negative PEs (>=0).
        beta_rl      : RL inverse temperature (scaled internally by 10).
        wm_cap_base  : baseline WM capacity proxy (in arbitrary units).
        k_age_cap    : reduction of WM capacity for older adults (>=0).
        gate_temp    : gain of the logistic arbitration over capacity (>=0).

    Returns negative log-likelihood of choices.
    """
    lr_base, neg_mult, softmax_beta, wm_cap_base, k_age_cap, gate_temp = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity proxy decreases with load and age
        # load term: 0 for set=3, 3 for set=6
        load = float(nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (probability of the chosen action)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy (probability of the chosen action)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration weight based on effective capacity
            cap_eff = wm_cap_base - (0.7 * load) - (k_age_cap * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_temp * (cap_eff - 1.0)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence bias
            pe = r - Q_s[a]
            lr = lr_base if pe >= 0.0 else lr_base * neg_mult
            q[s, a] += lr * pe

            # WM updating: decay + refresh on reward
            # Interference-driven decay increases with load and with age
            decay = np.clip(0.1 + 0.15 * load + 0.1 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-driven arbitration and interference-based WM decay sensitive to age and load.

    Mechanisms
    - RL: standard delta-rule learning.
    - WM: one-shot storage on reward; exponential decay toward uniform with rate depending on set size and age.
    - Arbitration: weight on WM is a logistic function of RL-policy entropy (less RL certainty -> more WM reliance),
                   modulated by load and age.

    Parameters
    - model_parameters: [lr, beta_rl, wm_decay, xi_interf, tau_ent, age_scale]
        lr        : RL learning rate (0..1).
        beta_rl   : RL inverse temperature (scaled internally by 10).
        wm_decay  : base WM decay rate toward uniform (0..1).
        xi_interf : additional decay per unit load (>=0).
        tau_ent   : sensitivity of the gate to RL entropy (>=0).
        age_scale : age-related bias to use WM (can be +/-; positive increases WM in older adults).

    Returns negative log-likelihood of choices.
    """
    lr, softmax_beta, wm_decay, xi_interf, tau_ent, age_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load = float(nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability for chosen action
            logits_rl = softmax_beta * (Q_s - Q_s[a])
            denom_rl = np.sum(np.exp(logits_rl))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # RL full policy for entropy calculation
            logits_full = softmax_beta * (Q_s - np.max(Q_s))
            probs_full = np.exp(logits_full)
            probs_full /= max(np.sum(probs_full), 1e-12)
            # Entropy in nats
            H_rl = -np.sum(probs_full * np.log(np.clip(probs_full, 1e-12, 1.0)))

            # WM policy probability for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: more WM when RL is uncertain (high entropy), but reduced by load; age bias applied
            gate_input = (tau_ent * H_rl) - (xi_interf * load) + (age_scale * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: decay increases with load and age; refresh on reward
            decay = np.clip(wm_decay + xi_interf * load + 0.1 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learning of choice temperature and logistic WM gate shaped by load and age.

    Mechanisms
    - RL: standard delta-rule; inverse temperature increases with recent reward rate (meta-learning),
          but is reduced for older adults.
    - WM: one-shot storage on reward; moderate decay; arbitration via logistic gate driven by load and age.
    - The WM store probability is implicit (always store on reward), which interacts with load-dependent gate.

    Parameters
    - model_parameters: [lr, beta0, k_meta, wm_gate0, k_load, k_age]
        lr       : RL learning rate (0..1).
        beta0    : base RL inverse temperature (scaled internally by 10).
        k_meta   : meta-gain: increase of beta with running average reward (>=0).
        wm_gate0 : baseline bias of WM gate (can be +/-).
        k_load   : slope of WM gate decrease with load (>=0).
        k_age    : reduction of RL temperature per age group and gate bias shift (>=0).

    Returns negative log-likelihood of choices.
    """
    lr, beta0, k_meta, wm_gate0, k_load, k_age = model_parameters
    # scale RL base temperature
    beta0 *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load = float(nS - 3)
        # running average reward for meta-temperature
        avg_r = 0.5

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update meta temperature from running reward average
            beta_rl_t = beta0 + 10.0 * (k_meta * avg_r) - (10.0 * k_age * age_group)
            beta_rl_t = max(beta_rl_t, 0.0)

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action with meta beta
            denom_rl = np.sum(np.exp(beta_rl_t * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy probability for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM gate: baseline bias, reduced by load, extra age penalty on gate (same k_age)
            gate_input = wm_gate0 - (k_load * load) - (k_age * age_group)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: modest decay every trial and store on reward
            decay = np.clip(0.15 + 0.1 * load + 0.1 * age_group, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update running average reward for meta-learning of beta
            avg_r = 0.8 * avg_r + 0.2 * r

        blocks_log_p += log_p

    return -blocks_log_p