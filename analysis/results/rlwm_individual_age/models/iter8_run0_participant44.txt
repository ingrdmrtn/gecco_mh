Below are three alternative cognitive models that combine reinforcement learning (RL) and working memory (WM) with different arbitration and memory dynamics. Each function follows the requested interface and returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported in the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and interference-driven WM decay.

    Idea:
    - RL updates state-action values with a single learning rate.
    - WM encodes one-shot associations after reward and is consulted via a precise policy.
    - Arbitration favors WM when its policy is more certain than RL (lower entropy).
    - WM strength decays due to interference: every time a different state is encountered,
      the memory for the current state's binding weakens. Interference scales with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10 internally)
    - wm_gain: gain that converts entropy difference into WM weight via a sigmoid
    - interference_base: base interference per encounter with other states (>=0)
    - size_interf_gain: scales interference with set size load (>=0)
    - age_interf_gain: additional interference for older adults (>=0)

    Inputs:
    - states: array of state indices per trial
    - actions: array of chosen actions per trial (0..2)
    - rewards: array of rewards per trial (0/1)
    - blocks: array of block indices
    - set_sizes: array of set sizes per trial (3 or 6)
    - age: array-like with a single repeated age value
    - model_parameters: list of 6 parameters described above

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, wm_gain, interference_base, size_interf_gain, age_interf_gain = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0  # very deterministic WM policy

    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM memory strength per state (decays with interference)
        mem_strength = np.zeros(nS)  # 0 means no memory; 1 means fully encoded
        last_state = None

        # Precompute interference factor given set size and age
        load_factor = 1.0 + size_interf_gain * max(0, nS - 3)
        age_factor = 1.0 + age_interf_gain * age_group
        interference = interference_base * load_factor * age_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Apply interference when switching between states
            if last_state is not None and s != last_state:
                # All states' memory degrade a bit when other states are presented
                mem_strength = mem_strength * np.exp(-interference)
                # Small floor to avoid negative or NaN
                mem_strength = np.clip(mem_strength, 0.0, 1.0)
            last_state = s

            # RL policy (full softmax vector for entropy; chosen-action prob for likelihood)
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl /= np.sum(pi_rl) if np.sum(pi_rl) > 0 else 1.0
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm /= np.sum(pi_wm) if np.sum(logits_wm) > 0 else 1.0
            p_wm = max(pi_wm[a], 1e-12)

            # Entropy-based arbitration: lower entropy -> more weight
            # H = -sum p log p
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, 1e-12, 1.0)))
            # base arbitration favoring lower entropy (WM gets more weight if H_rl > H_wm)
            raw = wm_gain * (H_rl - H_wm)
            # Convert to [0,1] via sigmoid, scale by memory strength for this state
            wm_weight_eff = 1.0 / (1.0 + np.exp(-raw))
            wm_weight_eff *= mem_strength[s]
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture policy and log-likelihood
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update:
            # - On reward, encode one-shot association for this state (set to one-hot)
            # - On no reward, let WM drift slightly toward uniform via mem_strength decay (already handled through arbitration; we additionally reduce mem_strength a bit)
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
                mem_strength[s] = 1.0
            else:
                # No explicit change to w[s], but reduce memory strength slightly to reflect uncertainty
                mem_strength[s] = mem_strength[s] * np.exp(-0.5 * interference)
                mem_strength[s] = np.clip(mem_strength[s], 0.0, 1.0)

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and reward-prediction-error-gated WM encoding.

    Idea:
    - RL uses an eligibility trace to diffuse credit across recently visited state-action pairs.
    - WM encodes a one-shot mapping when there is a sufficiently surprising positive outcome:
      gating probability increases with |PE|, but is penalized by set size and by age.
    - WM policy is precise. Arbitration is simply whether WM has a stored mapping; if not, RL governs.
      Mixing weight equals the current WM presence probability for the state.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL policy (scaled by 10 internally)
    - lambda_elig: eligibility trace decay parameter (0..1)
    - gate_base: base sensitivity converting |PE| to WM gating via sigmoid
    - size_gate_penalty: penalty on gating with set size load (>=0)
    - age_gate_penalty: additional gating penalty for older adults (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters: as specified.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, lambda_elig, gate_base, size_gate_penalty, age_gate_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        # Track whether WM has an entry for a state (probabilistic presence)
        wm_presence = np.zeros(nS)

        # Compute gating penalty factors from load and age
        load_penalty = 1.0 + size_gate_penalty * max(0, nS - 3)
        age_penalty = 1.0 + age_gate_penalty * age_group

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy (chosen-action probability)
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl /= np.sum(pi_rl) if np.sum(pi_rl) > 0 else 1.0
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy for current state
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm /= np.sum(pi_wm) if np.sum(pi_wm) > 0 else 1.0
            p_wm = max(pi_wm[a], 1e-12)

            # Arbitration: WM weight equals presence probability for that state
            wm_weight_eff = np.clip(wm_presence[s], 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Decay traces, then increment current state-action
            e *= lambda_elig
            e[s, a] += 1.0
            pe = r - q[s, a]
            q += lr * pe * e

            # WM gating based on surprise: stronger when |PE| is large and positive reward
            if r > 0:
                # Surprise magnitude, modulated by penalties
                surprise = abs(pe)
                gate_input = gate_base * surprise / (load_penalty * age_penalty)
                gate_prob = 1.0 / (1.0 + np.exp(-gate_input))
                # Stochastic encoding approximated deterministically: update presence toward gate_prob
                wm_presence[s] = 1.0 - (1.0 - wm_presence[s]) * (1.0 - gate_prob)
                # If encoded, set one-hot WM map
                if wm_presence[s] > 0.5:
                    w[s, :] = w_0[s, :].copy()
                    w[s, a] = 1.0
                    w[s, :] /= np.sum(w[s, :])
            else:
                # No reward: gently relax presence toward zero, stronger under load and in older age
                decay_rate = 0.05 * load_penalty * age_penalty
                wm_presence[s] = max(0.0, wm_presence[s] * (1.0 - decay_rate))

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with load/age-modulated inverse temperature and a choice stickiness bias.

    Idea:
    - RL uses a standard learning rate. Decision temperature (beta) is reduced by load and age.
    - WM stores one-shot rewarded mappings and is mixed with RL via a base WM weight penalized
      by load and age.
    - A choice stickiness bias (per state) adds a fixed bonus to the previously chosen action
      in that state, capturing perseveration often seen in older adults.
    - WM policy remains precise; RL policy includes stickiness in its preferences.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: base inverse temperature for RL (scaled by 10 internally)
    - wm_weight_base: base WM mixture weight (0..1)
    - stickiness: bonus added to the last chosen action's preference in the same state (>=0)
    - size_beta_penalty: penalty that reduces both beta and WM weight with set size (>=0)
    - age_beta_penalty: additional penalty for older adults that reduces both beta and WM weight (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters: as specified.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, beta_base, wm_weight_base, stickiness, size_beta_penalty, age_beta_penalty = model_parameters
    beta_base *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness (initialize with -1 = none)
        last_choice = -1 * np.ones(nS, dtype=int)

        # Compute penalties for this block
        load_penalty = 1.0 + size_beta_penalty * max(0, nS - 3)
        age_pen = 1.0 + age_beta_penalty * age_group

        # Effective beta and WM weight after penalties
        beta_eff = beta_base / (load_penalty * age_pen)
        wm_weight_eff_base = wm_weight_base / (load_penalty * age_pen)
        wm_weight_eff_base = np.clip(wm_weight_eff_base, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with stickiness
            Q_s = q[s, :].copy()
            if last_choice[s] >= 0:
                Q_s[last_choice[s]] += stickiness

            logits_rl = beta_eff * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl /= np.sum(pi_rl) if np.sum(pi_rl) > 0 else 1.0
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm /= np.sum(pi_wm) if np.sum(pi_wm) > 0 else 1.0
            p_wm = max(pi_wm[a], 1e-12)

            # WM weight: base amount, but only effective if WM has an entry for this state
            has_entry = float(np.max(W_s) > (1.0 / nA))
            wm_weight_eff = wm_weight_eff_base * has_entry

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: encode one-shot on reward
            if r > 0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

            # Update last choice for stickiness
            last_choice[s] = a

        total_log_p += log_p

    return -total_log_p

Notes on set size and age usage:
- Model 1: WM interference (decay) increases with set size and in older adults; WM arbitration depends on relative entropy of RL and WM policies.
- Model 2: WM encoding gate decreases with set size and age; RL uses eligibility traces to better propagate outcomes in longer sequences, potentially compensating for weaker WM under higher load/older age.
- Model 3: Both RL temperature and WM mixture weight are reduced by set size and age penalties; a stickiness bias captures perseveration, often more impactful when effective beta is reduced.