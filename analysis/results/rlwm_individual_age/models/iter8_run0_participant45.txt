def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with age- and load-dependent WM noise and leak.

    Idea:
    - RL: single learning rate with softmax choice.
    - WM: a state-specific cache that stores the last rewarded action but is noisy and leaky.
      On rewarded trials, the cache is set to a mixture of one-hot and uniform, where the mixing
      noise increases with age group and set size (6 > 3). On non-rewarded trials, the cache
      leaks toward uniform at a rate that also increases with age and set size.
    - Arbitration: fixed mixture between RL and WM using wm_weight (from parameters). Thus age and
      set-size effects operate by degrading WM precision and accelerating WM leak.

    Parameters
    ----------
    states : array-like
        State identity per trial (0..nS-1 within each block).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size of the block on each trial (3 or 6).
    age : array-like
        Participant age (repeated). Age group: 0 if <=45, 1 otherwise.
    model_parameters : list or array
        [lr, wm_weight, softmax_beta, wm_noise_base, wm_noise_age_add, wm_noise_ss6_add]
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_noise_base: baseline WM noise (0..1), used both as reward-time imprecision and leak rate.
        - wm_noise_age_add: additional WM noise for older adults (>=0).
        - wm_noise_ss6_add: additional WM noise for set size 6 (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_noise_base, wm_noise_age_add, wm_noise_ss6_add = model_parameters
    softmax_beta *= 10  # RL beta scaling

    # Age group: 0 (young) or 1 (old)
    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # deterministic WM readout of the cache
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent noise increment
        ss6 = 1 if nS == 6 else 0
        # Trial-invariant (per block) noise/leak parameters that depend on age and set size
        wm_noise_reward = np.clip(wm_noise_base + age_group * wm_noise_age_add + ss6 * wm_noise_ss6_add, 0.0, 1.0)
        wm_leak = wm_noise_reward  # use the same source for leak to keep parsimony

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given by template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over cache probabilities W_s with a high precision.
            # Numerical stability: center by max
            Ws_center = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Ws_center)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]
            # [END FILL]

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rewarded: set a noisy one-hot cache favoring the chosen action.
            # Non-rewarded: leak the cache toward uniform.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_noise_reward) * one_hot + wm_noise_reward * w_0[s, :]
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            # Renormalize to avoid drift
            w[s, :] = w[s, :] / np.maximum(np.sum(w[s, :]), 1e-12)
            # [END FILL]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with set-size modulated rehearsal (anti-leak) and error-driven inhibition.

    Idea:
    - RL: single learning rate with softmax choice.
    - WM: a recency-weighted cache per state that is strengthened ("rehearsed") more in small sets
      than in large sets (because smaller set size affords more rehearsal), with an age penalty on
      rehearsal. Errors trigger an action-specific inhibition in WM for the chosen action.
    - Arbitration: fixed wm_weight mixing.

    Parameters
    ----------
    model_parameters : [lr, wm_weight, softmax_beta, q_decay_base, wm_rehearsal_base, error_inhibit]
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - q_decay_base: decay toward uniform for Q-values per trial visit (interference; 0..1).
                        We increase decay in larger set sizes via an internal factor.
        - wm_rehearsal_base: baseline WM rehearsal gain (0..1); increased for small sets, reduced by age.
        - error_inhibit: on non-reward, reduce WM probability mass of the chosen action by this factor (0..1).

    Age and set-size usage
    - Age group (old=1) reduces WM rehearsal gain.
    - Set size 6 both increases RL Q decay and reduces WM rehearsal gain relative to nS=3.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, q_decay_base, wm_rehearsal_base, error_inhibit = model_parameters
    softmax_beta *= 10

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        ss6 = 1 if nS == 6 else 0

        # Set-size dependent Q decay: more decay (interference) in larger sets.
        q_decay = np.clip(q_decay_base * (1.0 + 0.5 * ss6), 0.0, 1.0)
        # WM rehearsal gain: more in small set, less in old age.
        wm_rehearsal = np.clip(wm_rehearsal_base * (1.0 + 0.5 * (1 - ss6)) * (1.0 - 0.5 * age_group), 0.0, 1.0)
        err_inh = np.clip(error_inhibit, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply Q decay (toward uniform) before choice to simulate ongoing interference
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            Ws_center = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Ws_center)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]
            # [END FILL]

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                # Rehearsal: move WM toward one-hot for the chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_rehearsal) * w[s, :] + wm_rehearsal * one_hot
            else:
                # Error-driven inhibition: suppress chosen action's trace, renormalize
                w[s, a] = (1.0 - err_inh) * w[s, a]
                # Mild diffusion to avoid zeroing out completely: move slightly toward uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            # Renormalize
            w[s, :] = w[s, :] / np.maximum(np.sum(w[s, :]), 1e-12)
            # [END FILL]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with binding swap errors (mis-binding) that increase with age and set size.

    Idea:
    - RL: single learning rate with softmax choice.
    - WM: When reward arrives, an intended one-hot binding for the chosen action is stored, but
      with some probability the binding is swapped to a wrong action (mis-binding). The swap rate
      increases in older adults and in set size 6. On non-rewarded trials, WM drifts toward
      uniform at a small rate.
    - Arbitration: fixed wm_weight mixing.

    Parameters
    ----------
    model_parameters : [lr, wm_weight, softmax_beta, wm_swap_base, wm_swap_age_add, wm_swap_ss6_add]
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_swap_base: baseline mis-binding probability on rewarded updates (0..1).
        - wm_swap_age_add: additional swap probability for older adults (>=0).
        - wm_swap_ss6_add: additional swap probability for set size 6 (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_swap_base, wm_swap_age_add, wm_swap_ss6_add = model_parameters
    softmax_beta *= 10

    age_group = 1 if age[0] > 45 else 0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        ss6 = 1 if nS == 6 else 0

        # Mis-binding probability
        swap_p = np.clip(wm_swap_base + age_group * wm_swap_age_add + ss6 * wm_swap_ss6_add, 0.0, 1.0)
        # Drift/leak toward uniform on non-reward
        wm_leak = np.clip(0.1 + 0.2 * age_group + 0.2 * ss6, 0.0, 1.0)  # derived from age/load, not a free param

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            Ws_center = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Ws_center)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]
            # [END FILL]

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.0:
                # Construct a mis-bound one-hot: with prob (1 - swap_p) store chosen,
                # with prob swap_p distribute mass to the other two actions equally.
                correct = (1.0 - swap_p)
                incorrect = swap_p
                one_hot = np.zeros(nA)
                one_hot[a] = correct
                other_idx = [i for i in range(nA) if i != a]
                for i in other_idx:
                    one_hot[i] = incorrect / (nA - 1)
                # Set WM state to this distribution (deterministic cache of a noisy binding)
                w[s, :] = one_hot
            else:
                # Leak toward uniform on non-reward
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            # Renormalize
            w[s, :] = w[s, :] / np.maximum(np.sum(w[s, :]), 1e-12)
            # [END FILL]

        blocks_log_p += log_p

    return -blocks_log_p