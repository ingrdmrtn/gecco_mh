Below are three alternative cognitive models that combine reinforcement learning (RL) with working memory (WM) in different, testable ways. Each function returns the negative log-likelihood of the observed choices. All parameters are used, age group is incorporated meaningfully, and set size (3 vs 6) affects WM contributions.

Note: Assume numpy (as np) and other standard packages are already imported by the caller. Do not add imports inside these functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Surprise-gated WM with forgetting and age-modulated gating.

    Mechanism
    - RL: standard delta rule with softmax choice (inverse temperature scaled by 10).
    - WM: near-deterministic policy (high beta_wm) over a per-state item that is stored
      when outcomes are surprising (|prediction error| high). The probability of storing
      is controlled by a logistic "gate" driven by |PE| and set size. WM decays toward
      uniform each trial.
    - Arbitration: fixed WM mixture weight per block that declines with larger set sizes.
    - Age: older adults gate less strongly (higher effective threshold) and forget faster.

    Parameters
    ----------
    model_parameters : list or array
        [alpha, beta_rl, wm_weight_base, gate_sensitivity, wm_forget, beta_wm]
        - alpha (0..1): RL learning rate.
        - beta_rl (>0): RL inverse temperature; internally scaled by 10.
        - wm_weight_base (0..1): baseline WM mixture weight before set-size scaling.
        - gate_sensitivity (>0): slope of logistic gating by |PE|.
        - wm_forget (0..1): WM forgetting rate toward uniform per trial (base).
        - beta_wm (>0): WM inverse temperature (not scaled).
    Other inputs
    ------------
    - states, actions, rewards: trial-wise arrays.
    - blocks: block index per trial.
    - set_sizes: set size per trial; determines number of states in the block.
    - age: array with a single value; age_group = 1 if age > 45 else 0.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, wm_weight_base, gate_sensitivity, wm_forget, beta_wm = model_parameters
    softmax_beta = beta_rl * 10.0

    age_group = 1 if age[0] > 45 else 0
    # Age effects: older adults forget faster and have reduced effective gating
    age_forget_mult = 1.3 if age_group == 1 else 1.0
    age_gate_bias = -0.5 if age_group == 1 else 0.0  # lowers probability of gating for older adults

    softmax_beta_wm = max(beta_wm, 1e-6)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture weight declines with larger set sizes
        wm_weight_block = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)

        # Forgetting (age-adjusted)
        phi_forget = np.clip(wm_forget * age_forget_mult, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL choice prob for chosen action (as in provided template)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM choice prob for chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - phi_forget) * w[s, :] + phi_forget * w_0[s, :]

            # Surprise-gated WM encoding (based on |PE|)
            # Gate probability increases with |delta|, reduced by set size (harder when larger)
            size_scale = 3.0 / float(nS)  # 1 at size 3, 0.5 at size 6
            gate_input = gate_sensitivity * abs(delta) * size_scale + age_gate_bias
            # Logistic
            p_gate = 1.0 / (1.0 + np.exp(-gate_input))

            if np.random.rand() < p_gate and r > 0.5:
                # On successful gate and reward, store chosen action deterministically
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Entropy-based arbitration with leaky WM (age- and load-dependent).

    Mechanism
    - RL: delta rule with softmax (beta_rl*10). We use the chosen-action prob as in the template,
      but also derive full softmax to compute policy entropy.
    - WM: leaky memory that can store rewarded action deterministically; evolves with leak to uniform.
      WM policy uses its own beta_wm.
    - Arbitration: the WM weight is a logistic function of the relative uncertainty:
        w_mix = wm_weight_base * sigmoid(entropy_temp * (H_RL - H_WM))
      so WM dominates when RL is more uncertain than WM. This is further scaled down by set size.
    - Age: older adults exhibit stronger WM leak (forgetting) and reduced effective WM contribution.

    Parameters
    ----------
    model_parameters : list or array
        [alpha, beta_rl, beta_wm, wm_weight_base, entropy_temp, age_wm_leak]
        - alpha (0..1): RL learning rate.
        - beta_rl (>0): RL inverse temperature (scaled by 10 internally).
        - beta_wm (>0): WM inverse temperature.
        - wm_weight_base (0..1): baseline WM mixture weight.
        - entropy_temp (>0): sensitivity of arbitration to entropy difference (H_RL - H_WM).
        - age_wm_leak (0..1): base WM leak rate that is amplified by age and set size.
    Other inputs
    ------------
    - states, actions, rewards, blocks, set_sizes, age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, beta_wm, wm_weight_base, entropy_temp, age_wm_leak = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = max(beta_wm, 1e-6)

    age_group = 1 if age[0] > 45 else 0
    # Age reduces effective arbitration strength and increases leak
    age_leak_mult = 1.4 if age_group == 1 else 1.0
    age_weight_scale = 0.85 if age_group == 1 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM contribution reduces with set size; also age scales it
        size_scale = 3.0 / float(nS)
        wm_weight_cap = np.clip(wm_weight_base * size_scale * age_weight_scale, 0.0, 1.0)

        # Leak increases with set size and age
        leak = np.clip(age_wm_leak * (6.0 / float(nS)) * age_leak_mult, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL prob of chosen action (template form)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # Also compute full RL policy for entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs) if np.sum(rl_probs) > 0 else np.ones_like(rl_probs) / len(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))

            # WM chosen-action prob
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # WM entropy
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs) if np.sum(wm_probs) > 0 else np.ones_like(wm_probs) / len(wm_probs)
            H_wm = -np.sum(wm_probs * np.log(np.clip(wm_probs, 1e-12, 1.0)))

            # Entropy-based arbitration: more WM when RL more uncertain than WM
            # Normalize entropy difference via logistic with entropy_temp
            ent_input = entropy_temp * (H_rl - H_wm)
            ent_weight = 1.0 / (1.0 + np.exp(-ent_input))
            wm_weight = np.clip(wm_weight_cap * ent_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Update WM on reward (store the correct action deterministically)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay + WM delta-learning (graded encoding) and age-modulated RL control.

    Mechanism
    - RL: delta rule plus value decay toward uniform each trial (q_decay). Softmax with beta_rl*10.
      Older adults experience a stronger effective decay (reduced retention).
    - WM: instead of one-shot storage, WM learns via a delta rule towards a one-hot code for the
      chosen action when rewarded; otherwise drifts toward uniform. WM uses its own beta_wm.
    - Arbitration: fixed per block WM weight scaled by set size.
    - Age: reduces RL stability (stronger q_decay_age) and modestly reduces WM contribution.

    Parameters
    ----------
    model_parameters : list or array
        [alpha_rl, beta_rl, beta_wm, q_decay, wm_alpha, age_rl_penalty]
        - alpha_rl (0..1): RL learning rate.
        - beta_rl (>0): RL inverse temperature (scaled by 10 internally).
        - beta_wm (>0): WM inverse temperature.
        - q_decay (0..1): base RL value decay toward uniform per visit.
        - wm_alpha (0..1): WM delta learning rate towards one-hot (on reward) or uniform (on no reward).
        - age_rl_penalty (>=0): scales up RL decay for older adults (higher -> more decay).
    Other inputs
    ------------
    - states, actions, rewards, blocks, set_sizes, age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_rl, beta_rl, beta_wm, q_decay, wm_alpha, age_rl_penalty = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = max(beta_wm, 1e-6)

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effect on WM arbitration; age reduces WM influence slightly
        size_scale = 3.0 / float(nS)
        age_wm_scale = 0.9 if age_group == 1 else 1.0
        wm_weight_block = np.clip(0.6 * size_scale * age_wm_scale, 0.0, 1.0)  # fixed base inside model to use all params elsewhere

        # Age-modulated RL decay
        q_decay_age = np.clip(q_decay * (1.0 + age_rl_penalty * (1 if age_group == 1 else 0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL chosen-action probability (template form)
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM chosen-action probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL decay toward uniform before/after update on this state
            # Decay only applied to visited state to keep comparability with WM updates
            q[s, :] = (1.0 - q_decay_age) * q[s, :] + q_decay_age * (1.0 / nA)

            # RL delta update
            delta = r - Q_s[a]
            q[s, a] += alpha_rl * delta

            # WM delta-learning:
            # If reward: move WM toward one-hot for chosen action
            # If no reward: move WM toward uniform (erodes incorrect binding)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = np.ones(nA) / nA
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target

        blocks_log_p += log_p

    return -blocks_log_p

Modeling notes
- Model 1 emphasizes a PE-driven encoding gate for WM with age-reduced gating and stronger forgetting for older adults, consistent with hypothesized declines in gating and maintenance.
- Model 2 arbitrates using relative uncertainty (entropy) between RL and WM, with older adults showing greater leak and reduced WM influence; load reduces WM weight.
- Model 3 replaces one-shot WM with a graded delta-learning WM, adds RL value decay that is stronger in older adults, and scales WM influence by set size.

All three models produce a likelihood over the observed actions via a WM-RL mixture and should be suitable for parameter estimation on the provided participant data.