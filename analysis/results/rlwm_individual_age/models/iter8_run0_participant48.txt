def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with WM interference and age-dependent WM down-weighting.

    Idea:
    - Behavior is a mixture of a model-free RL system and a working-memory (WM) system.
    - WM is precise when a state-action pair was recently rewarded, but suffers interference
      that increases with set size and age. WM strength (mixture weight) is reduced in older adults
      and under higher load.
    - RL learns with a single learning rate and no asymmetry. RL temperature is fixed within a block.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 for the current block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary feedback (0 or 1) per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (constant within a block, 3 or 6).
    age : array-like of int
        Participant age, repeated per trial. Age group is coded as 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_rl, wm_weight_base, wm_decay, wm_interference, age_wm_penalty]
        - lr: RL learning rate (0..1).
        - beta_rl: RL inverse temperature (scaled up internally).
        - wm_weight_base: baseline WM mixture weight at set size 3 for younger adults.
        - wm_decay: per-trial decay of WM values toward uniform (0..1).
        - wm_interference: amount of WM value corruption toward uniform; increases with load.
        - age_wm_penalty: additional reduction of WM weight and additional interference in older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_decay, wm_interference, age_wm_penalty = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # deterministic WM policy baseline
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load scaling from 3 to 6: 1.0 at 3, 0.5 at 6
        load_scale = 3.0 / nS

        # WM mixture weight: down-weighted by larger set size and by age
        wm_weight = wm_weight_base * load_scale - age_wm_penalty * age_group
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        # WM interference magnitude: grows with load and with age
        # 0 at set size 3 (if parameterized that way), increases toward wm_interference at set size 6
        load_interf = wm_interference * (1.0 - load_scale)
        age_interf = age_wm_penalty * age_group
        wm_interf_eff = np.clip(load_interf + age_interf, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL choice probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference: convex combination with uniform
            W_s_clean = w[s, :]
            W_s = (1.0 - wm_interf_eff) * W_s_clean + wm_interf_eff * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform, then one-shot overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and load-modulated learning rate.

    Idea:
    - RL learning rate is reduced under higher set size and further reduced in older adults,
      capturing slower integration/updating under cognitive load and aging.
    - WM contributes via a mixture; WM contents decay but are otherwise precise when rewarded.
    - RL temperature is fixed; WM policy is near-deterministic.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 for the current block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary feedback (0 or 1) per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (constant within a block, 3 or 6).
    age : array-like of int
        Participant age, repeated per trial. Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr_base, beta_rl, wm_weight_base, wm_decay, age_lr_drop, load_lr_drop]
        - lr_base: baseline RL learning rate at set size 3 for younger adults.
        - beta_rl: RL inverse temperature (scaled up internally).
        - wm_weight_base: baseline WM mixture weight at set size 3 for younger adults.
        - wm_decay: per-trial WM decay toward uniform (0..1).
        - age_lr_drop: reduction of RL learning rate for older adults.
        - load_lr_drop: reduction of RL learning rate when moving from set size 3 to 6.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_base, softmax_beta, wm_weight_base, wm_decay, age_lr_drop, load_lr_drop = model_parameters
    softmax_beta *= 10.0

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute effective learning rate per block
        load_factor = (nS - 3.0) / 3.0  # 0 at 3, 1 at 6
        lr_eff = lr_base - age_lr_drop * age_group - load_lr_drop * load_factor
        lr_eff = np.clip(lr_eff, 0.0, 1.0)

        # WM mixture weight decreases with load (fewer reliable WM items under higher set size)
        wm_weight = wm_weight_base * (3.0 / nS)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with effective learning rate
            delta = r - q[s, a]
            q[s, a] += lr_eff * delta

            # WM decay and overwrite on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with gated WM updates and age-modulated decision precision.

    Idea:
    - RL uses a base inverse temperature that is reduced in older adults (more decision noise).
      Load additionally reduces RL precision. Learning rate is standard.
    - WM updates are probabilistic (gated): after a rewarded trial, WM encodes the correct action
      with probability p_update that declines with load and is further reduced in older adults.
      We implement this deterministically as an expected update (soft overwrite).
    - WM has its own effective precision (beta_wm) that decreases with load.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 for the current block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary feedback (0 or 1) per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (constant within a block, 3 or 6).
    age : array-like of int
        Participant age, repeated per trial. Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, beta_rl_base, wm_weight_base, beta_wm_base, wm_update_prob, age_beta_shift]
        - lr: RL learning rate (0..1).
        - beta_rl_base: base RL inverse temperature (scaled up internally).
        - wm_weight_base: baseline WM mixture weight at set size 3.
        - beta_wm_base: base multiplier on WM inverse temperature (scales the fixed WM beta).
        - wm_update_prob: baseline probability (at set size 3, young) that WM stores the rewarded action.
        - age_beta_shift: reduction in RL precision for older adults; also reduces WM precision and WM update probability.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl_base, wm_weight_base, beta_wm_base, wm_update_prob, age_beta_shift = model_parameters
    softmax_beta = beta_rl_base * 10.0  # base RL precision

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    base_softmax_beta_wm = 50.0  # will be scaled by beta_wm_eff below
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load factor: 0 at 3, 1 at 6
        load_factor = (nS - 3.0) / 3.0

        # Effective RL precision reduced by load and age
        beta_rl_eff = softmax_beta * (1.0 - 0.5 * load_factor) * (1.0 - age_beta_shift * age_group)
        beta_rl_eff = max(beta_rl_eff, 1e-3)

        # WM precision reduced by load and age
        beta_wm_eff = beta_wm_base * (1.0 - 0.5 * load_factor) * (1.0 - age_beta_shift * age_group)
        beta_wm_eff = max(beta_wm_eff, 1e-3)
        softmax_beta_wm = base_softmax_beta_wm * beta_wm_eff

        # WM mixture weight also reduced by load
        wm_weight = wm_weight_base * (3.0 / nS)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        # WM update probability reduced by load and age; deterministic expected overwrite
        p_upd = wm_update_prob * (1.0 - 0.5 * load_factor) * (1.0 - age_beta_shift * age_group)
        p_upd = np.clip(p_upd, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM policy with adjustable precision
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # mild baseline decay each trial
            # Expected overwrite on reward with probability p_upd
            if r == 1:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_upd) * w[s, :] + p_upd * one_hot

        blocks_log_p += log_p

    return -blocks_log_p