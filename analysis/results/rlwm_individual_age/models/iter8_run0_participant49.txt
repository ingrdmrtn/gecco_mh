def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Confidence-gated WM mixture with load- and age-dependent WM decay.

    Core ideas
    - RL: standard Rescorla-Wagner update with softmax policy (beta scaled by 10).
    - WM: a tabular store that is moved toward one-hot on rewarded actions and away from
      chosen action on non-reward; WM decays toward uniform with a decay that increases with
      set size and in older adults.
    - Arbitration: mixture weight increases with WM "confidence" (action discriminability in WM)
      and decreases with set size and in older adults.

    Parameters
    ----------
    model_parameters : list or array
        [lr, wm_base, beta_rl, conf_slope, age_load_w, wm_decay_base]
        - lr: RL learning rate (0..1).
        - wm_base: baseline WM mixture weight (logit-space intercept).
        - beta_rl: base inverse temperature for RL (scaled by 10 internally).
        - conf_slope: how strongly WM confidence boosts WM weight.
        - age_load_w: how strongly age group and set size reduce WM weight and increase WM decay.
        - wm_decay_base: base WM decay toward uniform per trial.

    Age use
    - Older group (age >= 45) has increased decay and reduced WM mixture via age_load_w.
    Load use
    - Larger set size increases WM decay and reduces WM mixture.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_base, beta_rl, conf_slope, age_load_w, wm_decay_base = model_parameters

    # Age group coding
    age_group = 0 if age[0] <= 45 else 1

    # RL softmax beta scaled up (template)
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50  # very deterministic WM (template)
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action (template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM table
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence from WM: separation between best and average WM value
            conf = np.max(W_s) - np.mean(W_s)

            # Load/age penalty term
            load_age = (age_group * 1.0) + max(0, nS_t - 3) / 3.0

            # Effective WM weight (sigmoid/logit link)
            wm_logit = wm_base + conf_slope * conf - age_load_w * load_age
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (template)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay increases with load and age
            decay = wm_decay_base * (1.0 + age_load_w * load_age)
            decay = min(max(decay, 0.0), 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # WM supervised-like update:
            # - reward: pull toward one-hot of chosen action
            # - no reward: push away chosen action toward alternatives
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target
            else:
                anti = np.ones(nA) / (nA - 1)
                anti[a] = 0.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Noisy-gating WM with set-size/age-modulated RL temperature and lapses.

    Core ideas
    - RL: standard Rescorla-Wagner with softmax; beta drops with load and in older adults.
    - WM: a simple cache that stores rewarded action per state; gating to WM decreases with load;
      if not gated, WM remains diffuse. WM policy is softmax over the WM row.
    - Lapse: with probability that rises with load, choice is uniform random.
    - Mixture: WM and RL are mixed by the (noisy) gate; then lapse mixing applied.

    Parameters
    ----------
    model_parameters : list or array
        [lr, beta_base, wm_gate0, q_decay, beta_age_w, lapse0]
        - lr: RL learning rate (0..1).
        - beta_base: baseline RL inverse temperature (scaled by 10 internally).
        - wm_gate0: baseline WM gate strength (higher => more WM usage).
        - q_decay: small decay of RL values toward uniform (captures forgetting/interference).
        - beta_age_w: degree to which age and load reduce RL beta (exploration with load/age).
        - lapse0: baseline lapse that is amplified by set size.

    Age use
    - Older group has lower effective RL beta and higher lapse via load-age interactions.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_gate0, q_decay, beta_age_w, lapse0 = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    eps = 1e-12
    softmax_beta_wm = 50  # deterministic WM (template)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Effective RL beta drops with age and set size
            load_age = (age_group * 1.0) + max(0, nS_t - 3) / 3.0
            softmax_beta = (beta_base * 10.0) / (1.0 + beta_age_w * load_age)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from cache
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Gate decreases with load (set size) and noise; age increases penalty
            gate = wm_gate0 / (1.0 + 1.0 * max(0, nS_t - 3) + 0.5 * age_group)
            gate = 1.0 / (1.0 + np.exp(-gate))  # squash to [0,1]

            p_mix = gate * p_wm + (1.0 - gate) * p_rl

            # Lapse increases with load and age
            lapse = lapse0 * (1.0 + 0.5 * max(0, nS_t - 3)) * (1.0 + 0.5 * age_group)
            lapse = min(max(lapse, 0.0), 0.5)  # keep reasonable

            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (template)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Small RL forgetting to capture load interference
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA)

            # WM update: cache rewarded action strongly; otherwise, slight anti-boost
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.3 * w[s, :] + 0.7 * target
            else:
                anti = np.ones(nA) / (nA - 1)
                anti[a] = 0.0
                w[s, :] = 0.9 * w[s, :] + 0.1 * anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM with age/load-dependent capacity and mixture; standard RL.

    Core ideas
    - RL: standard Rescorla-Wagner with softmax; beta scaled by 10.
    - WM: only a limited number of states (capacity K) are stored with high fidelity.
      K decreases with set size and in older adults. States beyond capacity remain near-uniform.
    - Arbitration: WM mixture weight scales with whether the state is within capacity and
      a global WM base weight; WM policy is softmax over WM table.

    Parameters
    ----------
    model_parameters : list or array
        [alpha0, beta0, wm_base, cap0, pen_load_age, wm_temp]
        - alpha0: RL learning rate (0..1).
        - beta0: baseline RL inverse temperature (scaled by 10 internally).
        - wm_base: baseline WM mixture weight (0..1 after squashing).
        - cap0: baseline WM capacity in number of states (approx range 1..6).
        - pen_load_age: reduction in WM capacity with load and age; also mildly reduces WM weight.
        - wm_temp: scales WM softmax determinism by modulating its temperature indirectly via W.

    Age use
    - Older group reduces effective capacity and mixture weight via pen_load_age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, beta0, wm_base, cap0, pen_load_age, wm_temp = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta = beta0 * 10.0  # RL beta (template scaling)
    softmax_beta_wm = 50  # WM beta (template)
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track which states are currently within capacity (by order of first encounter)
        seen_order = []
        in_capacity = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Determine capacity for this block/trial
            cap_eff = cap0 - pen_load_age * ((age_group * 1.0) + max(0, nS_t - 3) / 3.0)
            cap_eff = max(0.0, min(float(nS), cap_eff))
            K = int(np.floor(cap_eff + 1e-9))

            # Update capacity membership on first visits
            if s not in seen_order:
                seen_order.append(s)
                if len(seen_order) <= K:
                    in_capacity[s] = True

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action (template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM mixture weight: base weight squashed, scaled by capacity membership and load/age
            wm_base_sig = 1.0 / (1.0 + np.exp(-wm_base))
            load_age_pen = 1.0 / (1.0 + pen_load_age * ((age_group * 1.0) + max(0, nS_t - 3) / 3.0))
            wm_weight = wm_base_sig * load_age_pen * (1.0 if in_capacity[s] else 0.2)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (template)
            delta = r - Q_s[a]
            q[s, a] += alpha0 * delta

            # WM update depends on capacity membership
            if in_capacity[s]:
                # Strong, low-noise update when rewarded; weaker when not
                if r >= 0.5:
                    target = np.zeros(nA)
                    target[a] = 1.0
                    # wm_temp increases sharpening: blend strength scales with sigmoid(wm_temp)
                    gain = 0.5 + 0.4 * (1.0 / (1.0 + np.exp(-wm_temp)))
                    w[s, :] = (1.0 - gain) * w[s, :] + gain * target
                else:
                    anti = np.ones(nA) / (nA - 1)
                    anti[a] = 0.0
                    w[s, :] = 0.85 * w[s, :] + 0.15 * anti
            else:
                # Out-of-capacity states drift toward uniform quickly
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p