def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: Uncertainty-weighted arbitration between RL and WM

    Description:
    - RL: Q-learning with softmax action selection.
    - WM: Reward-gated associative table W with decay toward uniform; near-deterministic softmax policy.
    - Arbitration: Trial-wise WM mixture weight increases when WM is confident and RL is uncertain.
      Confidence is measured as the inverse entropy margin of WM; RL uncertainty via policy entropy.
      Set size reduces effective WM weight; older age biases arbitration away from WM.
    
    Parameters:
    - lr: RL learning rate (0..1)
    - beta_rl: Inverse temperature for RL (scaled up internally by 10)
    - wm_eta: Strength of WM encoding after reward (0..1)
    - wm_decay: Decay of WM toward uniform each trial (0..1)
    - arb_kappa: Slope controlling sensitivity of arbitration to (WM confidence - RL uncertainty) (>0)
    - age_wm_bias: Additive bias to WM weight for young=0 vs old=1 (negative values reduce WM for older)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_eta, wm_decay, arb_kappa, age_wm_bias = model_parameters
    softmax_beta = beta_rl * 10.0  # beta has a higher upper bound
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM choice probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM confidence vs RL uncertainty
            # RL uncertainty: entropy of RL policy at state s
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_probs / np.sum(rl_probs)
            rl_entropy = -np.sum(rl_probs * (np.log(rl_probs + 1e-12)))

            # WM confidence: margin between top-2 WM values, squashed
            sorted_w = np.sort(W_s)[::-1]
            wm_margin = sorted_w[0] - sorted_w[1]
            wm_conf = 1.0 / (1.0 + np.exp(-10.0 * (wm_margin - 0.1)))  # near 0 if margin small, ~1 if large

            # Set size penalty on WM contribution
            ss_factor = 3.0 / float(nS)  # 1 for set size 3, 0.5 for set size 6

            # Combine into a dynamic WM weight via sigmoid arbitration
            arb_signal = (wm_conf - rl_entropy) * arb_kappa + age_wm_bias * (1 if age_group == 1 else 0)
            wm_weight = ss_factor * (1.0 / (1.0 + np.exp(-arb_signal)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay towards uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM encoding toward the chosen action (one-hot)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: Set-size scaled WM precision and age-modulated RL exploration

    Description:
    - RL: Q-learning with softmax; RL temperature increases (more exploratory) in older group.
    - WM: Reward-gated memory trace with strength wm_trace; WM policy uses fixed beta but we scale
      the effective W_s before softmax, yielding a set size- and age-dependent WM precision.
    - Mixture: Fixed convex combination where WM weight equals its instantaneous confidence
      from the scaled W (data-driven and set-size dependent).

    Parameters:
    - lr: RL learning rate (0..1)
    - beta_base: Base inverse temperature for RL (scaled by 10 internally)
    - wm_trace: Magnitude of WM update toward one-hot pattern on reward (0..1)
    - wm_beta_base: Base gain scaling applied to W before WM softmax (>=0)
    - ss_wm_drop: Exponent controlling how much larger set size reduces WM precision (>=0)
    - age_beta_shift: Reduction in RL beta for older group (>=0); RL beta_young = beta_base, RL beta_old = beta_base/(1+age_beta_shift)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_base, wm_trace, wm_beta_base, ss_wm_drop, age_beta_shift = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    # Age-modulated RL temperature
    beta_rl = beta_base / (1.0 + age_beta_shift * age_group)
    softmax_beta = beta_rl * 10.0  # beta has a higher upper bound

    softmax_beta_wm = 50.0  # very deterministic (we modulate W scaling instead)
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set size scaling for WM precision (alpha multiplies W before softmax)
        ss_factor = (3.0 / float(nS)) ** max(ss_wm_drop, 0.0)
        # Age reduces WM precision multiplicatively
        age_wm_mult = 1.0 - 0.3 * age_group
        wm_alpha = wm_beta_base * ss_factor * age_wm_mult

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: scale W_s by wm_alpha before the fixed high-temperature softmax
            scaled_W = wm_alpha * W_s + (1.0 - wm_alpha) * (np.ones_like(W_s) / nA)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (scaled_W - scaled_W[a])))

            # Data-driven WM weight from current WM confidence
            sorted_w = np.sort(scaled_W)[::-1]
            wm_margin = sorted_w[0] - sorted_w[1]
            wm_weight = np.clip(wm_margin, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform and reward-driven strengthening
            w[s, :] = 0.2 * w_0[s, :] + 0.8 * w[s, :]
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_trace) * w[s, :] + wm_trace * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: Meta-learned WM mixture weight via prediction error

    Description:
    - RL: Q-learning with softmax.
    - WM: Reward-gated table with decay; WM policy via near-deterministic softmax.
    - Meta-arbitration: A block-level WM mixture weight starts at wm_weight0 and is adapted online by a
      meta-learning rule driven by absolute prediction error. Larger PEs push arbitration toward RL.
      Set size adds a negative bias to WM weight; older age adds an additional negative bias.

    Parameters:
    - lr: RL learning rate (0..1)
    - beta: RL inverse temperature (scaled up by 10 internally)
    - wm_weight0: Initial WM mixture weight at the start of each block (0..1)
    - wm_meta_lr: Learning rate for adapting the WM mixture weight (0..1)
    - ss_bias: Subtractive bias to WM weight when set size is 6 vs 3 (>=0; applied as ss_bias*(nS-3)/3)
    - age_meta_bias: Additional subtractive bias applied if older group (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_weight0, wm_meta_lr, ss_bias, age_meta_bias = model_parameters
    softmax_beta = beta * 10.0  # beta has a higher upper bound
    
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialize WM mixture weight with set size and age penalties
        ss_penalty = ss_bias * max((nS - 3), 0) / 3.0  # 0 for nS=3, ss_bias for nS=6
        age_penalty = age_meta_bias * age_group
        wm_weight = np.clip(wm_weight0 - ss_penalty - age_penalty, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Meta-learning of WM weight based on absolute PE
            wm_weight += -wm_meta_lr * abs(delta)  # large errors reduce WM reliance
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # WM update: decay and reward-gated encoding
            w[s, :] = 0.2 * w_0[s, :] + 0.8 * w[s, :]
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = 0.6 * w[s, :] + 0.4 * onehot  # moderate encoding strength

        blocks_log_p += log_p

    return -blocks_log_p