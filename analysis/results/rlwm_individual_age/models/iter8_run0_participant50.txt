def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and set-sizeâ€“dependent decay in both systems.

    Idea
    - Choices are a mixture of model-free RL and a fast working-memory (WM) policy.
    - Both RL and WM traces decay, and decay strength increases with set size (higher load).
    - Older adults suffer stronger decay (especially in WM), reducing WM contribution.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6, constant within block).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_weight0, wm_decay_base, q_decay_base, age_decay_boost]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_weight0: base weight on WM (0..1) before capacity/decay gating.
        - wm_decay_base: base WM decay per trial toward uniform (0..1).
        - q_decay_base: base RL Q-value decay per trial toward uniform (0..1).
        - age_decay_boost: multiplicative boost (>0) on decays for older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, beta_base, wm_weight0, wm_decay_base, q_decay_base, age_decay_boost = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0  # near-deterministic WM

        # Age- and load-modulated decays
        ss_factor = 1.0 + max(0, nS - 3)  # 1 for 3A3S, 4 for 6A6S; amplifies load
        age_boost = (age_decay_boost if age_group == 1 else 1.0)
        wm_decay = np.clip(wm_decay_base * ss_factor * age_boost, 0.0, 1.0)
        q_decay = np.clip(q_decay_base * (0.5 + 0.5 * ss_factor) * (1.0 + 0.2 * age_group), 0.0, 1.0)

        # Capacity/weight gating: heavier load reduces WM reliance
        load_gate = 3.0 / float(nS)  # 1.0 for nS=3; 0.5 for nS=6
        wm_weight = np.clip(wm_weight0 * load_gate / (1.0 + 0.2 * age_group), 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))  # RL Q-values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM policy logits (as a distribution)
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline uniform for decay

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs_rl - np.max(prefs_rl))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy (distribution stored in w)
            W_s = w[s, :]
            prefs_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(prefs_wm - np.max(prefs_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * (1.0 / nA)

            # WM update: reinforce correct mapping, else decay toward uniform
            if r > 0.5:
                w[s, :] = (1e-6)  # near-one-hot after softmax
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-gated WM engagement and age-dependent lapses.

    Idea
    - RL provides a softmax policy with base beta.
    - WM stores last rewarded action per state; retrieval is noisy.
    - WM engagement is gated by a sigmoid of WM certainty (how peaked W_s is),
      penalized by set size and age.
    - Older adults exhibit higher lapse (random choice) probability.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6, constant within block).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr, beta_base, wm_gain, ss_sensitivity, age_lapse_old, wm_noise]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_gain: sensitivity of WM weight to certainty signal (higher => more WM when confident).
        - ss_sensitivity: penalty of set size on WM engagement (>=0).
        - age_lapse_old: lapse probability for older adults (0..0.5); young has smaller derived lapse.
        - wm_noise: retrieval noise mixing WM with uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, beta_base, wm_gain, ss_sensitivity, age_lapse_old, wm_noise = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        # Lapse (older higher, younger fraction of that)
        lapse = age_lapse_old if age_group == 1 else 0.2 * age_lapse_old

        # Initialize RL and WM memory
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs_rl - np.max(prefs_rl))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM retrieval with noise: smooth toward uniform
            W_raw = w[s, :]
            W_s = (1.0 - wm_noise) * W_raw + wm_noise * w_0[s, :]
            prefs_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(prefs_wm - np.max(prefs_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # WM certainty signal: peakiness above uniform
            peak = np.max(W_s)
            certainty = peak - (1.0 / nA)  # in [0, 1-1/nA]
            # Gate: sigmoid of certainty minus load and age penalties
            load_penalty = ss_sensitivity * max(0, nS - 3)  # 0 for 3, >0 for 6
            age_penalty = 0.5 * age_group  # small fixed penalty for older
            wm_weight = sigmoid(wm_gain * certainty - load_penalty - age_penalty)

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store last rewarded action deterministically on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On non-reward, keep current uncertain representation (no change)
                pass

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + capacity-limited WM with interference.

    Idea
    - RL uses separate learning rates for positive and negative outcomes.
    - WM contributes when set size is within an effective capacity K_eff,
      which is lower for older adults. WM traces also suffer interference that
      grows with set size relative to capacity.
    - Mixture of RL and WM policies weighted by capacity ratio.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..nS-1).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Observed reward on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6, constant within block).
    age : array-like
        Participant age (constant array).
    model_parameters : list or array
        [lr_pos, lr_neg, beta_base, K_all, age_K_drop, interf_base]
        - lr_pos: RL learning rate after reward (0..1).
        - lr_neg: RL learning rate after no reward (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - K_all: baseline WM capacity in items (>=1).
        - age_K_drop: capacity reduction for older adults (>=0).
        - interf_base: baseline WM interference/decay per trial (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, beta_base, K_all, age_K_drop, interf_base = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0

        # Effective capacity by age
        K_eff = max(1.0, K_all - age_group * age_K_drop)
        wm_weight = float(np.clip(K_eff / float(nS), 0.0, 1.0))

        # Interference: grows when load exceeds capacity
        overload = max(0.0, (nS / max(1.0, K_eff)) - 1.0)
        wm_interf = np.clip(interf_base * (1.0 + overload), 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            prefs_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(prefs_rl - np.max(prefs_rl))
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)
            p_rl = float(pi_rl[a])

            # WM policy
            W_s = w[s, :]
            prefs_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(prefs_wm - np.max(prefs_wm))
            pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            p_wm = float(pi_wm[a])

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (asymmetric)
            lr_use = lr_pos if r > 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr_use * delta

            # WM update with interference
            if r > 0.5:
                # Strengthen correct mapping
                w[s, :] = (1e-6)
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            # Global interference toward uniform (affects all states mildly)
            w = (1.0 - wm_interf) * w + wm_interf * w_0

        blocks_log_p += log_p

    return -float(blocks_log_p)