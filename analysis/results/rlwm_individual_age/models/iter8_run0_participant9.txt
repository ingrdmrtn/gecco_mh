def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying WM; age modulates exploration.

    Mechanism
    - RL: standard delta rule with inverse temperature (softmax). Older adults explore more (lower beta).
    - WM: quasi-deterministic store of rewarded S-A associations; decays toward uniform with set-size-dependent interference.
    - Mixture: action probabilities are a convex combination of WM and RL policies.

    Parameters (6)
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature, scaled by *10 internally
    - wm_weight0: baseline WM mixture weight in [0,1]
    - wm_decay0: base WM decay toward uniform per trial (0..1)
    - size_interf: additional WM decay contribution per extra item beyond 3 (>=0)
    - age_explore_pen: multiplicative penalty on RL beta for older adults (>=0). Effective beta_old = beta * exp(-age_explore_pen)

    Inputs
    - states, actions, rewards: arrays per trial (int/int/float)
    - blocks: block index per trial
    - set_sizes: block set size per trial
    - age: array with a single value repeated; <=45 => 0 (young), >45 => 1 (old)
    - model_parameters: list/tuple of 6 floats as specified

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay0, size_interf, age_explore_pen = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    # Age modulation: older -> more exploration (lower beta)
    if age_group == 1:
        softmax_beta = softmax_beta * np.exp(-max(0.0, age_explore_pen))

    softmax_beta_wm = 50.0  # near-deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size reduces effective reliance on WM
        wm_mix = np.clip(wm_weight0, 0.0, 1.0) * (3.0 / float(nS))
        # WM decay increases with set size
        extra = max(0.0, (float(nS) - 3.0) / 3.0)
        wm_decay = np.clip(wm_decay0 + max(0.0, size_interf) * extra, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-gated WM overwrite with one-hot
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with UCB exploration bonus + probabilistic-capacity WM encoding.

    Mechanism
    - RL: delta rule on Q; action selection uses Q plus an uncertainty bonus (UCB) based on visit counts.
    - WM: on reward, encode the chosen action as a near one-hot trace, but the strength of encoding
      scales with a capacity parameter C relative to set size (expected encoding).
    - Mixture of WM and RL policies.
    - Age: older participants have reduced inverse temperature (more exploration).

    Parameters (6)
    - lr: RL learning rate in [0,1]
    - beta_base: base RL inverse temperature; scaled by *10 internally
    - wm_weight: baseline WM mixture weight in [0,1]
    - capacity_C: WM capacity (in items). Effective encoding weight p_enc = clip(C / nS, 0..1)
    - c_explore: UCB exploration gain (>=0); bonus = c_explore / sqrt(N_sa + 1)
    - age_beta_drop: fractional drop of beta for older adults (0..1). beta_old = beta * (1 - age_beta_drop)

    Returns negative log-likelihood.
    """
    lr, beta_base, wm_weight, capacity_C, c_explore, age_beta_drop = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    if age_group == 1:
        softmax_beta = softmax_beta * max(0.0, 1.0 - np.clip(age_beta_drop, 0.0, 1.0))

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for UCB
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size reduces WM reliance and encoding strength
        wm_mix = np.clip(wm_weight, 0.0, 1.0) * (3.0 / float(nS))
        p_enc = np.clip(capacity_C / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with UCB bonus
            bonus = max(0.0, c_explore) / np.sqrt(N[s, :] + 1.0)
            Qeff = q[s, :] + bonus
            Q_shift = Qeff - np.max(Qeff)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and count
            pe = r - q[s, a]
            q[s, a] += lr * pe
            N[s, a] += 1.0

            # Expected WM encoding on reward: blend current with one-hot using p_enc
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * one_hot
            else:
                # Without reward, no new encoding; softly relax toward uniform (weak decay aligned with limited capacity)
                decay = 0.05 * (float(nS) - 3.0) / 3.0 if nS > 3 else 0.0
                decay = np.clip(decay, 0.0, 0.2)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay + PE-gated WM encoding; age shifts the PE gate.

    Mechanism
    - RL: delta rule with within-state value decay toward uniform after each trial (prevents overcommitment).
    - WM: encodes a one-hot mapping only when the absolute prediction error exceeds a threshold (PE gate).
    - Gate depends on set size (more items -> higher gate) and age (older -> higher gate).
    - Mixture of WM and RL policies.

    Parameters (6)
    - lr: RL learning rate in [0,1]
    - softmax_beta: RL inverse temperature; scaled by *10 internally
    - wm_weight_base: baseline WM mixture weight in [0,1]
    - decay_rl: RL forgetting toward uniform for the visited state per trial (0..1)
    - pe_gate: base PE gating threshold (>0). Effective threshold = pe_gate * (nS/3) + age_gate_shift*age_group
    - age_gate_shift: additional threshold added if old (>=0)

    Returns negative log-likelihood.
    """
    lr, softmax_beta, wm_weight_base, decay_rl, pe_gate, age_gate_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture reduced by set size
        wm_mix = np.clip(wm_weight_base, 0.0, 1.0) * (3.0 / float(nS))
        # PE gate increases with set size and if older
        gate = max(0.0, pe_gate) * (float(nS) / 3.0) + max(0.0, age_gate_shift) * age_group
        gate = max(0.0, gate)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = max(p_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = max(p_wm_vec[a], 1e-12)

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # RL decay toward uniform on the visited state
            q[s, :] = (1.0 - decay_rl) * q[s, :] + decay_rl * (1.0 / nA)

            # WM: PE-gated encoding; larger sets and older age require larger surprise to store
            if abs(pe) >= gate:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not gated in, allow mild drift toward uniform with set-size-dependent decay
                decay_wm = np.clip((float(nS) - 3.0) / 15.0, 0.0, 0.2)
                w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p