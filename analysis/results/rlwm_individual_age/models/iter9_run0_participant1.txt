def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying associative WM (set-size and age dependent) with fixed arbitration weight.

    Mechanism:
    - RL: tabular Q-learning with softmax policy.
    - WM: associative matrix w[s,a] that encodes the most recently reinforced action via supervised encoding.
      WM decays toward a uniform baseline each trial; decay increases with set size and with older age.
    - Arbitration: a fixed base wm_weight is used, but can be re-assigned per trial to reflect set size and age.
      Here we keep it fixed to satisfy the template line, and let decay handle load/age effects on WM reliability.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = wm_weight in [0,1]: mixture weight for WM in arbitration
    - model_parameters[2] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[3] = wm_encode in [0,1]: WM encoding strength on rewarded trials
    - model_parameters[4] = wm_decay_base in [0,1]: baseline WM decay rate per trial
    - model_parameters[5] = decay_gain (>=0): additional decay per unit of load and per age group (adds for both)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_encode, wm_decay_base, decay_gain = model_parameters

    # Parameter hygiene
    lr = min(max(lr, 0.0), 1.0)
    wm_weight = min(max(wm_weight, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6)
    wm_encode = min(max(wm_encode, 0.0), 1.0)
    wm_decay_base = min(max(wm_decay_base, 0.0), 1.0)
    decay_gain = max(decay_gain, 0.0)

    softmax_beta *= 10.0  # follows the template guidance

    # Age group coding: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM baseline (uniform)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM decay increases with set size and age group
            # d in [0,1]
            d = wm_decay_base + decay_gain * (max(nS_t - 3, 0) + age_group)
            d = min(max(d, 0.0), 1.0)
            # Apply decay toward baseline
            w = (1.0 - d) * w + d * w_0

            # WM policy from associative memory
            W_s = w[s, :]
            # High-beta softmax approximates argmax/one-hot retrieval
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy (template line)
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: supervised encoding on rewarded trials
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_encode) * w[s, :] + wm_encode * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM with binding errors (load- and age-dependent).

    Mechanism:
    - RL: tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: stores the last rewarded action per state. Retrieval is noisy due to "binding errors":
      with probability (1 - be) WM returns the stored action; with probability be, WM confuses and
      responds uniformly at random among all actions.
      The binding error be increases with set size and with older age.
    - Arbitration: fixed wm_weight between WM and RL per the template.

    Parameters (6):
    - model_parameters[0] = lr_pos in [0,1]: RL learning rate for positive PE
    - model_parameters[1] = wm_weight in [0,1]: mixture weight for WM in arbitration
    - model_parameters[2] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[3] = lr_neg in [0,1]: RL learning rate for negative PE
    - model_parameters[4] = bind_setsize_gain (>=0): binding error increase per extra item over 3
    - model_parameters[5] = bind_age_gain (>=0): additional binding error for older adults

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, wm_weight, softmax_beta, lr_neg, bind_setsize_gain, bind_age_gain = model_parameters

    lr_pos = min(max(lr_pos, 0.0), 1.0)
    wm_weight = min(max(wm_weight, 0.0), 1.0)
    softmax_beta = max(softmax_beta, 1e-6)
    lr_neg = min(max(lr_neg, 0.0), 1.0)
    bind_setsize_gain = max(bind_setsize_gain, 0.0)
    bind_age_gain = max(bind_age_gain, 0.0)

    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # will mirror stored action for documentation
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Discrete WM store: -1 means nothing stored; otherwise stores the action index
        stored_action = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM binding error as a function of load and age
            be = bind_setsize_gain * max(nS_t - 3, 0) + bind_age_gain * age_group
            be = min(max(be, 0.0), 1.0)

            # WM policy
            if stored_action[s] >= 0:
                # With prob 1-be choose stored action; otherwise random among all actions
                wm_policy = np.ones(nA) * (be / nA)
                wm_policy[stored_action[s]] += (1.0 - be)
                p_wm = wm_policy[a]
            else:
                p_wm = 1.0 / nA  # no memory yet

            # For completeness, keep w aligned with stored_action (not strictly needed for policy)
            if stored_action[s] >= 0:
                w[s, :] = w_0[s, :]
                w[s, stored_action[s]] = 1.0

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: store action on rewarded trials
            if r > 0.0:
                stored_action[s] = a
                # Update w matrix to reflect storage
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with interference-driven decay and uncertainty-based arbitration (age and load modulate arbitration indirectly).

    Mechanism:
    - RL: tabular Q-learning with softmax policy.
    - WM: associative matrix with supervised encoding on reward, and interference-based decay each trial.
      Interference grows with set size and is stronger for older adults.
    - Arbitration: we transform the base wm_weight via a logistic function of (WM confidence - RL entropy),
      penalized by interference; we assign this back to wm_weight before the mixture line.
      This keeps the template intact while making arbitration state-, load-, and age-dependent.

    Parameters (6):
    - model_parameters[0] = lr in [0,1]: RL learning rate
    - model_parameters[1] = wm_weight in (0,1): base arbitration weight (converted to logit internally)
    - model_parameters[2] = softmax_beta (>0, scaled by 10): RL inverse temperature
    - model_parameters[3] = arb_uncert_slope (>=0): sensitivity of arbitration to (WM confidence - RL entropy)
    - model_parameters[4] = interf_setsize_gain (>=0): WM interference increase per extra item over 3
    - model_parameters[5] = age_interf_gain (>=0): additional interference for older adults

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, arb_uncert_slope, interf_setsize_gain, age_interf_gain = model_parameters

    lr = min(max(lr, 0.0), 1.0)
    wm_weight = min(max(wm_weight, 1e-6), 1.0 - 1e-6)  # keep in (0,1) for logit
    softmax_beta = max(softmax_beta, 1e-6)
    arb_uncert_slope = max(arb_uncert_slope, 0.0)
    interf_setsize_gain = max(interf_setsize_gain, 0.0)
    age_interf_gain = max(age_interf_gain, 0.0)

    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    nA = 3

    # helper: entropy of a softmax distribution
    def entropy_from_logits(logits):
        # logits already centered as Q_s - Q_s[a] in p_rl part, but here we compute full distribution
        probs = np.exp(logits - np.max(logits))
        probs /= max(np.sum(probs), eps)
        probs = np.clip(probs, eps, 1.0)
        return -np.sum(probs * np.log(probs)), probs

    # base arbitration in logit space
    base_logit = np.log(wm_weight / (1.0 - wm_weight))

    for b in np.unique(blocks):
        mask = (blocks == b)

        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM interference-driven decay
            interf = interf_setsize_gain * max(nS_t - 3, 0) + age_interf_gain * age_group
            interf = min(max(interf, 0.0), 1.0)
            w = (1.0 - interf) * w + interf * w_0

            # WM policy and confidence (max probability under WM)
            W_s = w[s, :]
            wm_logits = softmax_beta_wm * W_s
            # compute p_wm for chosen action and WM confidence
            max_W = np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)
            wm_conf = max_W  # in [1/nA, 1], serves as a proxy for WM certainty

            # RL entropy at state s (lower entropy = more certain)
            rl_H, _ = entropy_from_logits(softmax_beta * Q_s)

            # Dynamic arbitration: overwrite wm_weight via a logistic transform
            # Higher wm_conf and lower RL entropy push weight to WM; interference penalizes it.
            logit_w = base_logit + arb_uncert_slope * (wm_conf - rl_H) - 5.0 * interf
            wm_weight = 1.0 / (1.0 + np.exp(-logit_w))
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            # Mixture per template
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: supervised encoding on reward
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong encoding when rewarded
                w[s, :] = 0.0 * w[s, :] + onehot

        blocks_log_p += log_p

    return -blocks_log_p