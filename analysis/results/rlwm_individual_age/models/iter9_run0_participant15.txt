def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-weighted arbitration, cross-state WM interference, and age-modulated exploration.

    Policy:
    - Mixture of RL and WM: p_total = (1 - lapse_eff) * [w_eff * p_wm + (1 - w_eff) * p_rl] + lapse_eff * (1/nA)
    - WM arbitration weight w_eff scales with set size (3/nS), WM certainty (1 - entropy), and age (older -> less WM).
    - RL inverse temperature is reduced for younger participants (more exploration) via age_explore.

    RL dynamics:
    - Single learning rate (lr). Standard delta rule.

    WM dynamics:
    - On reward, move W[s,:] toward one-hot of chosen action (fast encoding).
    - Global interference: every trial, all WM rows decay multiplicatively toward uniform by a factor
      that increases with set size (more items => more interference).
    - Noisy WM readout via a high softmax temperature (softmax_beta_wm).

    Age and set-size dependence:
    - WM arbitration reduced for larger set sizes and for older adults.
    - Lapse increases with set size and age.
    - RL exploration increased for younger adults (age_explore reduces beta for age_group=0).

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: base RL inverse temperature (scaled by 10 internally)
    - wm_priority: base WM mixture weight in [0,1]
    - interference: WM cross-state interference rate in [0,1]
    - age_explore: exploration boost for younger adults in [0,1]; reduces beta_rl if young
    - lapse_base: base lapse probability in [0,1]
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_priority, interference, age_explore, lapse_base = model_parameters

    softmax_beta = beta_rl * 10.0
    age_group = 0 if age[0] <= 45 else 1

    # Younger adults explore more: reduce RL beta
    softmax_beta *= (1.0 - 0.4 * age_explore * (1 - age_group))
    softmax_beta = max(softmax_beta, 1e-3)

    softmax_beta_wm = 50.0  # deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM certainty via normalized entropy (0..1), convert to certainty (1 - H_norm)
            eps = 1e-12
            H = -np.sum(W_s * np.log(W_s + eps))
            H_max = np.log(nA)
            wm_certainty = 1.0 - (H / max(H_max, eps))

            # Arbitration weight: base priority * load scaling * certainty * age effect
            load_scale = 3.0 / float(nS)  # 1.0 for 3, 0.5 for 6
            age_scale = (1.0 - 0.3 * age_group)  # older down-weight WM
            w_eff = np.clip(wm_priority * load_scale * age_scale * (0.5 + 0.5 * wm_certainty), 0.0, 1.0)

            # Lapse increases with set size and age
            lapse_eff = np.clip(lapse_base * (1.0 + 0.2 * (nS == 6) + 0.4 * age_group), 0.0, 0.5)

            # Mixture policy
            p_mix = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # Cross-state WM interference: decay all rows toward uniform; more with larger set size
            interf = np.clip(interference * (nS / 3.0), 0.0, 1.0)
            w = (1.0 - interf) * w + interf * w_0

            # Reward-driven WM encoding (overwrite toward chosen action)
            if r > 0.5:
                alpha_wm = np.clip(0.8 * (0.7 + 0.3 * load_scale), 0.0, 1.0)  # slightly weaker under higher load
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM gating by state-wise success streak and set-size-dependent WM precision.

    Policy:
    - Mixture of RL softmax and WM softmax:
        p_total = (1 - lapse_eff) * [w_eff * p_wm + (1 - w_eff) * p_rl] + lapse_eff * (1/nA)
      where w_eff depends on a state-wise success streak and set size, reduced in older adults.

    RL dynamics:
    - Separate learning rates for positive vs negative prediction errors (lr_pos, lr_neg).

    WM dynamics:
    - WM decays toward uniform at rate wm_decay each trial.
    - WM precision (beta_wm) decreases with set size; base value beta_wm_base scaled by (3/nS).
    - Upon reward, encode chosen action into WM (one-shot update).
    - WM influence weight increases with recent success streak for that state.

    Age and set-size dependence:
    - Older adults have lower WM mixture weight.
    - Lapse increases with set size and age.
    - WM precision lower in larger set sizes.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PEs in [0,1]
    - lr_neg: RL learning rate for negative PEs in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - wm_gate_base: base WM mixture weight in [0,1]
    - wm_decay: WM decay toward uniform per trial in [0,1]
    - beta_wm_base: base WM inverse temperature for set size 3 (scaled to set size)
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_rl, wm_gate_base, wm_decay, beta_wm_base = model_parameters

    softmax_beta = beta_rl * 10.0
    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track per-state success streak for WM gating
        streak = np.zeros(nS)

        # Set-size dependent WM precision
        softmax_beta_wm = max(beta_wm_base * (3.0 / float(nS)), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM mixture weight depends on success streak (sigmoid over streak), set size, and age
            load_scale = 3.0 / float(nS)
            age_scale = (1.0 - 0.35 * age_group)
            streak_gate = 1.0 / (1.0 + np.exp(-streak[s]))  # 0.5 at 0, increases with success
            w_eff = np.clip(wm_gate_base * load_scale * age_scale * streak_gate, 0.0, 1.0)

            # Lapse increases with set size and age
            lapse_eff = np.clip(0.05 + 0.15 * (nS == 6) + 0.15 * age_group, 0.0, 0.5)

            p_mix = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] = Q_s[a] + eta * pe

            # WM decay each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM rewarded encoding and streak update
            if r > 0.5:
                alpha_wm = np.clip(0.85 * load_scale, 0.0, 1.0)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot
                streak[s] += 1.0
            else:
                streak[s] -= 0.5  # mild penalty on failure

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility trace + WM overwrite and set-size/age-modulated choice stickiness.

    Policy:
    - Mixture of RL and WM with stickiness bias:
        p_total âˆ exp(beta_rl * Q[s,a] + kappa * I[a == a_prev_s] + beta_wm * W[s,a]),
      implemented via mixture:
        p_total = (1 - lapse_eff) * [w_eff * p_wm + (1 - w_eff) * p_rl_sticky] + lapse_eff * (1/nA)

    RL dynamics:
    - Delta rule with eligibility trace (lambda_et): maintains E[s,a] for recency-weighted credit assignment.

    WM dynamics:
    - On reward, overwrite WM for that state toward chosen action with strength wm_overwrite.
    - Global small decay toward uniform each trial proportional to set size.
    
    Stickiness:
    - Bias to repeat last action within the same state; strength kappa depends on set size and age
      (older and larger set sizes -> more stickiness).

    Age and set-size dependence:
    - WM mixture weight reduced for larger set sizes and older adults.
    - Stickiness increases with set size and age.
    - Lapse increases with set size and age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - lambda_et: eligibility trace decay in [0,1]
    - wm_overwrite: WM overwrite strength on reward in [0,1]
    - stickiness_base: base stickiness coefficient (>=0)
    - lapse_base: base lapse probability in [0,1]
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, lambda_et, wm_overwrite, stickiness_base, lapse_base = model_parameters

    softmax_beta = beta_rl * 10.0
    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces per state-action
        E = np.zeros((nS, nA))

        # Last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with stickiness bias via softmax reweighting
            # We fold stickiness into policy by adjusting Q with kappa on the last action
            kappa = stickiness_base * (1.0 + 0.5 * (nS == 6) + 0.5 * age_group)
            Q_adj = Q_s.copy()
            if last_action[s] >= 0:
                Q_adj[last_action[s]] += kappa / max(softmax_beta, 1e-6)

            denom_rl = np.sum(np.exp(softmax_beta * (Q_adj - Q_adj[a])))
            p_rl_sticky = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: WM weight drops with load and age
            load_scale = 3.0 / float(nS)
            age_scale = (1.0 - 0.3 * age_group)
            w_eff = np.clip(0.6 * load_scale * age_scale, 0.0, 1.0)

            # Lapse increases with set size and age
            lapse_eff = np.clip(lapse_base * (1.0 + 0.25 * (nS == 6) + 0.4 * age_group), 0.0, 0.5)

            p_mix = w_eff * p_wm + (1.0 - w_eff) * p_rl_sticky
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Update last action for stickiness
            last_action[s] = a

            # RL update with eligibility traces
            # Decay all traces
            E *= lambda_et
            # Increment chosen trace at current state
            E[s, a] += 1.0
            pe = r - Q_s[a]
            q += lr * pe * E  # distribute PE via eligibility

            # WM decay proportional to set size (more load -> stronger decay)
            decay = np.clip(0.05 * (nS / 3.0), 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # WM overwrite on reward
            if r > 0.5:
                alpha_wm = np.clip(wm_overwrite * (0.8 * load_scale + 0.2), 0.0, 1.0)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p