def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Associative WM with capacity-ratio arbitration and age-adjusted effective capacity.

    Idea:
    - Choices are a mixture of model-free RL and a WM associative policy.
    - WM maintains an action-probability distribution per state (w), updated by a WM learning rate
      and leaked toward uniform by a WM leak parameter.
    - Arbitration weight is proportional to an effective capacity ratio (K_eff / set_size), where
      K_eff is age-modulated (lower for older adults). Thus, larger sets reduce WM influence.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy; internally scaled by x10.
    - wm_alpha: WM learning rate (how strongly WM shifts toward rewarded action) in [0,1].
    - wm_leak: WM leak toward uniform each trial in [0,1] (higher = faster forgetting).
    - K_base: baseline WM capacity (in slots; e.g., around 3â€“4).
    - age_K_drop: proportional reduction of K_base for older adults (0..1). No effect if young.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_alpha, wm_leak, K_base, age_K_drop = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity and arbitration weight (constant within block)
        K_eff = max(0.0, K_base * (1.0 - age_K_drop * age_group))
        wm_weight = np.clip(K_eff / float(nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (deterministic softmax over WM weights)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # WM associative update toward chosen action if rewarded
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with surprise-gated WM and load/age-modulated gating.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores the most recent action tendency via a probability vector, but the WM write strength
      is gated by the magnitude of the RL prediction error (surprise). Larger surprise => stronger WM write.
    - Gating is further modulated by set size (lower for larger sets) and age (reduced if older).
    - Arbitration weight equals the current WM gate strength (trial-by-trial).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors in [0,1].
    - lr_neg: RL learning rate for negative prediction errors in [0,1].
    - softmax_beta: inverse temperature for RL policy; internally scaled by x10.
    - surprise_scale: scales the impact of |prediction error| on WM gating (higher = stronger gating).
    - age_gate_shift: additive boost to gating for young adults; reduced by age (multiplied by 1-age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, surprise_scale, age_gate_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Surprise-gated arbitration
            pe = r - Q_s[a]
            # Load and age modulation: higher load (nS=6) reduces gate; young get an additive shift
            load_term = (3.0 / float(nS))  # 1 for nS=3, 0.5 for nS=6
            gate_raw = surprise_scale * abs(pe) * load_term + age_gate_shift * (1.0 - age_group)
            # Squash to (0,1)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_raw))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetry
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM decay depends on load (more interference at larger set sizes)
            leak = max(0.0, 1.0 - 3.0 / float(nS))  # 0 for 3, ~0.5 for 6
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # WM write strength equals the same gate: move toward the chosen action if rewarded
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_weight) * w[s, :] + wm_weight * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with WM-weighted arbitration and WM-dampened perseveration; age modulates stickiness.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - A perseveration bias (stickiness) adds to RL logits but is dampened by WM confidence:
      when WM is confident, reliance on stickiness is reduced.
    - WM confidence is the max probability in the WM state vector.
    - WM weight decreases with set size; older adults exhibit stronger stickiness.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy; internally scaled by x10.
    - wm_weight_base: base WM mixture weight (scaled by 3/set_size).
    - kappa_base: base stickiness strength added to the last action in RL policy.
    - age_stick_boost: multiplicative increase in stickiness for older adults (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, kappa_base, age_stick_boost = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -1 * np.ones(nS, dtype=int)

        # Base WM weight adjusted by load
        wm_weight_block = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)
        # Stickiness with age modulation
        kappa_eff = kappa_base * (1.0 + age_stick_boost * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute WM confidence to dampen stickiness
            wm_conf = np.max(W_s)  # in [1/3,1]
            stick_vec = np.zeros(nA)
            if last_action[s] >= 0:
                stick_vec[last_action[s]] = 1.0
            # Dampen stickiness when WM is confident
            kappa_damped = kappa_eff * (1.0 - wm_conf)

            # RL policy with dampened stickiness
            logits_rl = softmax_beta * Q_s + kappa_damped * stick_vec
            denom_rl = np.sum(np.exp(logits_rl - logits_rl[a]))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration (block-level) WM weight
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM gentle decay toward uniform and overwrite on reward
            decay = 0.2 * max(0.0, (nS - 3.0) / 3.0)  # stronger decay for larger set size
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p