def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, reward-gated Working Memory (WM) mixture.

    Mechanism:
    - RL: tabular Q-learning with softmax policy (beta scaled by 10 per template).
    - WM: stores rewarded associations as one-hot vectors with strength wm_store, and decays toward uniform with wm_decay.
    - Capacity: WM is capacity-limited (K_base); when set size exceeds effective capacity, WM becomes less reliable.
    - Mixture: Trial-by-trial mixture between RL and WM; WM weight decreases with load and with age.
    
    Parameters (list; total <= 6):
    - lr: Q-learning rate for RL.
    - wm_weight_base: baseline mixture weight for WM (0..1).
    - softmax_beta: base RL inverse temperature; multiplied by 10 internally.
    - wm_store: WM write strength on rewarded trials (0..1).
    - wm_decay: WM decay toward uniform each trial (0..1).
    - K_base: nominal WM capacity in number of items (can be fractional; effective capacity reduced by age).

    Inputs:
    - states: array of state indices per trial within each block (0..nS-1).
    - actions: array of chosen actions (0..2).
    - rewards: array of rewards (0/1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set size per trial (3 or 6).
    - age: array with a single repeated value indicating participant age.
    - model_parameters: list of parameters described above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_store, wm_decay, K_base = model_parameters
    softmax_beta *= 10.0  # as specified
    softmax_beta_wm = 50.0  # deterministic WM softmax per template
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1  # 0 young, 1 old

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob for chosen action (template form retained)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (deterministic temperature)
            # Capacity effect: when set_size > K_eff, WM becomes less diagnostic.
            K_eff = max(1.0, K_base * (1.0 - 0.3 * age_group))  # older => reduced effective capacity
            load_factor = np.clip(K_eff / max(1.0, set_size), 0.0, 1.0)  # <= 1
            # Sharpen WM state if within capacity; otherwise approach uniform
            W_centered = W_s - np.mean(W_s)
            W_scaled = load_factor * W_centered + (1.0 - load_factor) * (w_0[s, :] - np.mean(w_0[s, :]))
            wm_logits = W_scaled
            wm_logits = wm_logits - np.max(wm_logits)
            pi_wm = np.exp(softmax_beta_wm * wm_logits)
            pi_wm /= np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Mixture weight depends on load and age
            wm_weight = np.clip(wm_weight_base * load_factor * (1.0 - 0.2 * age_group), 0.0, 1.0)

            p_total = np.clip(p_wm * wm_weight + (1.0 - wm_weight) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated write with decay toward uniform
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_store) * w[s, :] + wm_store * target

            # Decay increases with load relative to capacity and with age
            decay_eff = np.clip(wm_decay * (set_size / max(1.0, K_eff)) * (1.0 + 0.3 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + WM with load-dependent temperature; mixture shifted by age.

    Mechanism:
    - RL: asymmetric learning rates for positive and negative prediction errors.
    - WM: stores rewarded associations; WM choice noise increases with load (via effective temperature scaling).
    - Mixture: WM weight increases under low load, and is shifted by age (age_mix_shift).
    
    Parameters (list; total <= 6):
    - lr: positive learning rate (alpha_pos) for RL.
    - wm_weight_base: baseline WM mixture weight.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - alpha_neg: negative learning rate for RL.
    - wm_temp_base: scales WM determinism (higher => sharper WM, lower => noisier).
    - age_mix_shift: additive shift to WM weight for young vs old (applied as (1 - age_group)).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters.

    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, alpha_neg, wm_temp_base, age_mix_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base; we will modulate via wm_temp_base and load
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-dependent effective temperature
            # Effective WM inverse temperature decreases with load; scale by wm_temp_base
            load_scale = 3.0 / max(3.0, float(set_size))  # 1 for set_size=3, 0.5 for 6
            beta_wm_eff = max(0.0, softmax_beta_wm * wm_temp_base * load_scale)
            wm_logits = W_s - np.max(W_s)
            pi_wm = np.exp(beta_wm_eff * wm_logits)
            pi_wm /= np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # Mixture: base weight increased under low load, with age-dependent shift
            wm_weight = wm_weight_base * load_scale
            wm_weight += age_mix_shift * (1 - age_group)  # young get a shift; 0 for old
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = np.clip(p_wm * wm_weight + (1.0 - wm_weight) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update: reward-gated storage; mild decay
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # store strongly when low load (i.e., more certain context)
                store_strength = np.clip(0.7 * wm_temp_base * load_scale, 0.0, 1.0)
                w[s, :] = (1.0 - store_strength) * w[s, :] + store_strength * target

            decay = np.clip(0.1 * (1.0 / max(1.0, wm_temp_base)) * (set_size / 3.0) * (1.0 + 0.2 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with novelty bonus + leaky WM; mixture weighted by WM confidence and load.

    Mechanism:
    - RL: Q-learning augmented with an uncertainty-driven novelty bonus on less-experienced state-action pairs.
    - WM: rewarded one-hot associations that leak toward uniform; leak increases with load and age.
    - Mixture: WM weight scales with load and with WM confidence (1 - normalized entropy); young receive larger novelty emphasis.

    Parameters (list; total <= 6):
    - lr: Q-learning rate.
    - wm_weight_base: baseline WM mixture weight.
    - softmax_beta: base RL inverse temperature (scaled by 10 internally).
    - novelty_bonus: bonus added to RL value for uncertain state-action pairs.
    - wm_leak_base: baseline WM leak rate (0..1).
    - age_novelty_shift: multiplicative boost to novelty bonus for young participants.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age, model_parameters.

    Returns:
    - Negative log-likelihood.
    """
    lr, wm_weight_base, softmax_beta, novelty_bonus, wm_leak_base, age_novelty_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track visit counts for novelty/uncertainty
        counts = np.zeros((nS, nA)) + 1e-6  # avoid divide by zero

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            set_size = int(block_set_sizes[t])

            Q_s = q[s, :].copy()

            # RL policy with novelty bonus applied to values
            total_counts_s = np.sum(counts[s, :])
            uncertainty = 1.0 - (counts[s, :] / (total_counts_s + eps))  # higher for rarely chosen actions
            novelty_eff = novelty_bonus * (1.0 + age_novelty_shift * (1 - age_group))  # boost for young
            Q_aug = Q_s + novelty_eff * uncertainty

            # Use template p_rl structure by substituting augmented Q
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy: softmax over WM with fixed high beta
            W_s = w[s, :]
            wm_logits = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * wm_logits)
            pi_wm /= np.sum(pi_wm)
            p_wm = max(pi_wm[a], eps)

            # WM confidence via normalized entropy (0 peaked -> high confidence)
            probs_wm = np.clip(pi_wm, eps, 1.0)
            entropy = -np.sum(probs_wm * np.log(probs_wm))
            entropy_norm = entropy / np.log(nA)
            wm_confidence = 1.0 - entropy_norm  # 0..1

            # Mixture weight: base scaled by load and WM confidence; small age penalty
            load_scale = 3.0 / max(3.0, float(set_size))
            wm_weight = wm_weight_base * load_scale * (0.8 + 0.4 * wm_confidence) * (1.0 - 0.1 * age_group)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = np.clip(wm_weight * p_wm + (1.0 - wm_weight) * p_rl, eps, 1.0 - eps)
            log_p += np.log(p_total)

            # RL update (without novelty in learning rule; novelty affects policy only)
            delta = r - q[s, a]
            q[s, a] += lr * delta
            counts[s, a] += 1.0

            # WM update: reward-based storage and leak to uniform
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * target  # moderate write

            leak_eff = np.clip(wm_leak_base * (set_size / 3.0) * (1.0 + 0.4 * age_group), 0.0, 1.0)
            w[s, :] = (1.0 - leak_eff) * w[s, :] + leak_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and load:
- All models treat young participants (age_group=0; this participant is 20) as having stronger WM influence and, in model3, a larger novelty bonus.
- WM reliability and mixture weights decrease with set size 6 vs 3.
- Parameterizations are distinct from the combinations tried so far (e.g., capacity K with reward-gated write in model1; asymmetric RL + WM temperature in model2; novelty bonus + entropy-weighted mixing in model3).