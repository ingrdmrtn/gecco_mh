def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM with age- and load-modulated WM precision.

    Mechanism:
    - RL: tabular Q-learning with softmax.
    - WM: per-state categorical cache that becomes more precise after reward, but decays toward uniform.
          WM precision is reduced at set size 6 (interference) and boosted for younger adults.
    - Arbitration: fixed mixture weight between WM and RL.

    Parameters (6):
    - model_parameters = [lr, wm_weight, softmax_beta, wm_decay, size6_noise, age_wm_boost]
      lr: RL learning rate in [0,1]
      wm_weight: mixture weight for WM in [0,1]
      softmax_beta: RL inverse temperature (scaled by *10 internally)
      wm_decay: per-visit decay of WM traces toward uniform in [0,1]
      size6_noise: multiplicative reduction of WM precision at set size 6 in [0,1]
      age_wm_boost: multiplicative boost of WM precision for young vs old:
                    wm_beta_mult = 1 + age_wm_boost*(1 - 2*age_group),
                    so young (0) -> boost; old (1) -> reduction.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, size6_noise, age_wm_boost = model_parameters

    # Age group: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    # Scale RL temperature; fixed, high WM temperature base
    softmax_beta *= 10
    softmax_beta_wm = 50  # base WM precision (very deterministic)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Trial-wise log-likelihood
        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL choice prob for chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM precision modulation:
            # - reduced at set size 6 (interference)
            # - boosted in young adults (age effect)
            size_penalty = (1.0 - size6_noise) if nS == 6 else 1.0
            age_mult = 1.0 + age_wm_boost * (1 - 2*age_group)
            wm_beta_eff = max(1e-6, softmax_beta_wm * size_penalty * age_mult)

            p_wm = 1 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Strengthen memory toward the rewarded action; weaker strengthening under high load
            if r > 0:
                strengthen = size_penalty  # less strengthening for set size 6
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - strengthen) * w[s, :] + strengthen * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning + WM win-stay/lose-decay; age modulates RL temperature.

    Mechanism:
    - RL: Q-learning with separate effective learning rates for positive vs negative outcomes.
          Implemented by setting lr dynamically before each update.
    - WM: per-state "last action" trace that becomes highly confident after reward (win-stay)
          and decays toward uniform otherwise (lose-decay). Decay is stronger at set size 6.
    - Arbitration: fixed mixture between WM and RL.
    - Age: modulates RL softmax temperature (younger -> higher beta).

    Parameters (6):
    - model_parameters = [lr_base, wm_weight, softmax_beta, lr_neg_mult, wm_forget_size6, age_temp_scale]
      lr_base: baseline RL learning rate for positive outcomes
      wm_weight: mixture weight for WM in [0,1]
      softmax_beta: RL inverse temperature (scaled by *10 internally)
      lr_neg_mult: multiplier for RL learning rate when r=0 (effective lr = lr_base * lr_neg_mult)
      wm_forget_size6: WM decay rate when set size is 6 (for set size 3, we use wm_forget_size6/2)
      age_temp_scale: multiplicative change of beta by age group:
                      beta_eff = beta * (1 + age_temp_scale*(1 - 2*age_group))

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_base, wm_weight, softmax_beta, lr_neg_mult, wm_forget_size6, age_temp_scale = model_parameters

    # Age group: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    # RL temperature with age modulation
    softmax_beta *= 10
    softmax_beta = softmax_beta * (1.0 + age_temp_scale * (1 - 2*age_group))
    softmax_beta = max(1e-6, softmax_beta)

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM win-stay / lose-decay:
            # - After reward: W becomes one-hot on the chosen action (deterministic memory).
            # - Otherwise: W decays toward uniform; stronger decay at set size 6.
            # Policy is softmax over W (high precision).
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            # RL update with valence-asymmetric learning rates
            lr = lr_base if r > 0 else lr_base * lr_neg_mult
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0:
                # Win-stay: set a sharp memory
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Lose: decay toward uniform; heavier forgetting at higher set size
                forget = wm_forget_size6 if nS == 6 else wm_forget_size6 / 2.0
                forget = np.clip(forget, 0.0, 1.0)
                w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM gating with age-dependent capacity and encoding lapse.

    Mechanism:
    - RL: standard Q-learning with softmax.
    - WM: per-state cache of rewarded action; when WM encodes the state, policy is near-deterministic,
          otherwise it falls back to uniform choice. The probability that a state is encoded is
          capacity-limited: p(cap) = min(1, K / set_size). K depends on age group.
    - Arbitration: mixture of WM and RL at the policy level (WM contributes only when encoded).

    Parameters (6):
    - model_parameters = [lr, wm_weight, softmax_beta, K_young, K_old, wm_lapse]
      lr: RL learning rate in [0,1]
      wm_weight: mixture weight for WM in [0,1]
      softmax_beta: RL inverse temperature (scaled by *10 internally)
      K_young: effective WM capacity for younger adults (in slots)
      K_old: effective WM capacity for older adults (in slots)
      wm_lapse: probability mass that leaks to uniform during WM encoding/maintenance in [0,1]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, K_young, K_old, wm_lapse = model_parameters

    # Age group: 0 = young (<=45), 1 = old (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta *= 10
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity (age-dependent)
        K = K_young if age_group == 0 else K_old
        cap_prob = min(1.0, max(0.0, K / float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: with probability cap_prob, use WM (softmax on W); else, uniform.
            p_wm_cached = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = cap_prob * p_wm_cached + (1.0 - cap_prob) * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Encoding with lapse: some probability mass leaks to uniform
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lapse) * onehot + wm_lapse * w_0[s, :]
            else:
                # Maintenance lapse toward uniform when not rewarded
                w[s, :] = (1.0 - wm_lapse) * w[s, :] + wm_lapse * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p