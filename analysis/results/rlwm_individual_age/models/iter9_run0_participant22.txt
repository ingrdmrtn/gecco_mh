def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + error-likelihood-gated working memory (WM).

    Idea:
    - RL: standard delta-rule learning.
    - WM: one-shot storage of rewarded pairs; non-reward relaxes toward uniform.
    - Arbitration: WM weight increases when the estimated error likelihood for the state is high.
      The error likelihood is tracked per state with an exponential moving average and its
      influence is modulated by age group and set size.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_weight_base, err_sensitivity, age_err_delta, size_err_delta]
        - lr: RL learning rate (0..1).
        - softmax_beta: inverse temperature for RL action selection; internally scaled x10.
        - wm_weight_base: baseline weight of WM in the mixture (0..1 via logistic transform).
        - err_sensitivity: slope linking (estimated) state error rate to WM engagement (>0).
        - age_err_delta: increase in sensitivity for older adults (age_group=1).
        - size_err_delta: increase in sensitivity when set size increases (from 3 to 6).
          Implemented proportionally to setsize_factor in [0,1].

    Notes
    -----
    - Age group: 0 if age <= 45, else 1; used to modulate the error sensitivity.
    - Returns the negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, err_sensitivity, age_err_delta, size_err_delta = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    beta_wm = 50.0  # near-deterministic WM when a mapping is stored
    eps = 1e-12

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM table
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track per-state error likelihood with EMA
        err_hat = 0.5 * np.ones(nS)  # initial uncertainty
        err_alpha = 0.3  # fixed smoothing within a block

        # Precompute set-size factor (0 for 3, 1 for 6)
        setsize_factor = max(0.0, (nS - 3) / 3.0)

        # Transform base WM weight to [0,1] via logistic; keep as baseline gain
        base_w = 1.0 / (1.0 + np.exp(-wm_weight_base))

        # Sensitivity including age and size
        sens = err_sensitivity + age_err_delta * age_group + size_err_delta * setsize_factor

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: WM weight increases with expected error likelihood in this state
            wm_weight = base_w * (1.0 / (1.0 + np.exp(-sens * (err_hat[s] - 0.5))))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture probability for the chosen action
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL learning
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update error likelihood estimate (1 - reward)
            err_hat[s] = (1.0 - err_alpha) * err_hat[s] + err_alpha * (1.0 - r)

            # WM updating: reward-gated one-shot with relaxation when not rewarded
            if r >= 0.5 and pe > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                relax = 0.2
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + interference-driven decaying WM.

    Idea:
    - RL: standard delta-rule.
    - WM: one-shot encoding when rewarded; all WM rows decay toward uniform at each trial,
      with a decay parameter. In addition, the arbitration weight for WM decreases as more
      unique states are encountered within the block (interference).
    - Age and set size: older adults experience stronger interference; set size increases
      the interference slope.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_weight_base, interference_slope, age_interf_delta, wm_decay]
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled x10 internally).
        - wm_weight_base: baseline WM contribution (0..1 via logistic).
        - interference_slope: how strongly unique-state count reduces WM weight.
        - age_interf_delta: additional slope added for older adults.
        - wm_decay: per-trial decay of WM rows toward uniform (0..1).

    Notes
    -----
    - Unique-state count is tracked within each block and normalized by set size.
    - Returns the negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, interference_slope, age_interf_delta, wm_decay = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Unique states seen so far within the block
        seen = np.zeros(nS, dtype=bool)

        # Base WM mixture weight
        base_w = 1.0 / (1.0 + np.exp(-wm_weight_base))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Update seen states
            seen[s] = True
            interference_level = np.sum(seen) / float(nS)  # 0..1

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute interference-modulated WM weight
            slope = interference_slope + age_interf_delta * age_group
            wm_weight = base_w * np.exp(-slope * interference_level)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL learning
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM global decay toward uniform (interference/forgetting)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-gated WM write
            if r >= 0.5 and pe > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-modulated WM encoding and age/set-size dependent WM precision.

    Idea:
    - RL: standard delta-rule.
    - WM: encodes the chosen action proportionally to the unsigned prediction error (surprise).
      Uses the RL learning rate as the WM write-rate to couple learning across systems.
    - WM precision (its softmax temperature) depends on age group and set size:
      older adults and larger set sizes reduce WM inverse temperature (i.e., more noise).
    - Arbitration: fixed baseline WM mixture weight.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, wm_beta_base, wm_beta_age_penalty, wm_beta_size_bonus, wm_weight_base]
        - lr: RL learning rate (0..1). Also used as WM write rate for surprise-driven update.
        - softmax_beta: RL inverse temperature (scaled x10).
        - wm_beta_base: base inverse temperature for WM policy.
        - wm_beta_age_penalty: reduction applied to WM beta for older adults (>=0).
        - wm_beta_size_bonus: bonus for smaller set size; applied as * (1 - setsize_factor).
          Effective WM beta = wm_beta_base - wm_beta_age_penalty*age_group
                             + wm_beta_size_bonus*(1 - setsize_factor).
        - wm_weight_base: baseline WM mixture weight (0..1 via logistic).

    Notes
    -----
    - setsize_factor: 0 for size 3, 1 for size 6.
    - Returns the negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_beta_base, wm_beta_age_penalty, wm_beta_size_bonus, wm_weight_base = model_parameters
    beta = softmax_beta * 10.0
    age_group = 0 if age[0] <= 45 else 1
    eps = 1e-12

    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        setsize_factor = max(0.0, (nS - 3) / 3.0)

        # WM beta incorporating age and set size
        beta_wm = wm_beta_base - wm_beta_age_penalty * age_group + wm_beta_size_bonus * (1.0 - setsize_factor)
        beta_wm = max(1e-3, beta_wm)  # keep positive

        # Fixed WM mixture weight transformed to [0,1]
        wm_weight = 1.0 / (1.0 + np.exp(-wm_weight_base))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy with beta_wm
            denom_wm = np.sum(np.exp(beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Surprise-modulated WM encoding using RL learning rate as write strength
            # Move the WM row toward a one-hot on the chosen action by |PE|
            surprise = abs(pe)
            onehot = np.zeros(3)
            onehot[a] = 1.0
            write_strength = lr * surprise
            w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * onehot

            # Mild relaxation toward uniform if no reward to avoid latching on errors
            if r < 0.5:
                relax = 0.05
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

        blocks_log_p += log_p

    return -float(blocks_log_p)