def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + slot-based WM with capacity-limited arbitration, load-dependent WM decay, and lapses.

    Rationale:
    - RL learns slowly and acts stochastically via softmax.
    - WM stores the most recently learned correct action per state (on rewarded trials).
    - Arbitration weight equals the probability that the current state is within WM capacity: K/nS.
    - WM decays toward uniform under cognitive load (larger set size).
    - A small lapse probability mixes in uniform responding.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_y: inverse temperature for young participants (scaled by 10 internally)
    - beta_o: inverse temperature for older participants (scaled by 10 internally)
    - K_base: baseline number of WM slots (controls WM arbitration weight as K/nS)
    - wm_decay: base WM decay rate toward uniform per trial (scaled up by load)
    - lapse: lapse probability mixing uniform policy into the final choice (0..0.2 typical)

    Age and set-size effects:
    - Age enters via age-specific softmax temperature (beta_y vs beta_o).
    - Set size reduces WM arbitration weight via p_inWM = clip(K_base / nS, 0..1),
      and increases WM decay via a load factor proportional to (nS-3)/3.
    """
    lr, beta_y, beta_o, K_base, wm_decay, lapse = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = (beta_y if age_group == 0 else beta_o) * 10.0
    beta_wm = 50.0  # deterministic WM policy

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM leak toward uniform
        load_factor = max(nS - 3, 0) / 3.0
        wm_leak = np.clip(wm_decay * load_factor, 0.0, 1.0)

        # Arbitration weight based on capacity and set size
        wm_weight_block = np.clip(K_base / max(nS, 1), 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy (deterministic-like)
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # Mix RL and WM via capacity-based arbitration and add lapses
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay toward uniform
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM write on positive feedback: store the action for the state (one-hot)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and state-specific choice kernel + uncertainty-gated WM.

    Rationale:
    - RL uses separate learning rates for positive and negative outcomes.
    - A state-specific choice kernel captures recent repetition in that state (habit),
      with decay tied to set size (more load -> faster decay).
    - WM increments toward the chosen action when rewarded; arbitration depends on WM certainty
      (1 - normalized entropy) scaled down by set size.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewarded trials (0..1)
    - lr_neg: RL learning rate for unrewarded trials (0..1)
    - beta_base: base RL inverse temperature (scaled by 10 internally)
    - age_beta_mult: multiplicative factor on beta for older vs young (old beta = beta_base*age_beta_mult)
    - wm_eta_base: WM learning rate on rewarded trials (0..1)
    - age_wm_mult: multiplicative factor on wm_eta for older vs young (old wm_eta = wm_eta_base*age_wm_mult)

    Age and set-size effects:
    - Age changes the RL inverse temperature and WM learning strength via the multiplicative factors.
    - Set size reduces WM arbitration via a factor 3/nS and increases choice-kernel decay with load.
    """
    lr_pos, lr_neg, beta_base, age_beta_mult, wm_eta_base, age_wm_mult = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = beta_base * (1.0 if age_group == 0 else age_beta_mult)
    beta *= 10.0
    beta_wm = 50.0

    wm_eta = wm_eta_base * (1.0 if age_group == 0 else age_wm_mult)

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific choice kernel (habit), decays faster under load
        ck = np.zeros((nS, nA))
        load_factor = max(nS - 3, 0) / 3.0
        # Larger set size -> faster decay; bound to [0,1)
        ck_decay = np.clip(0.2 + 0.6 * load_factor, 0.0, 0.95)
        # Kernel strength scales down with set size
        ck_weight = 1.0 / nS

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL + choice kernel policy
            Q_s = q[s, :].copy()
            Q_s_bias = Q_s + ck_weight * ck[s, :]
            rl_logits = beta * (Q_s_bias - np.max(Q_s_bias))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # WM arbitration by certainty (1 - normalized entropy), scaled by 3/nS
            eps = 1e-12
            entropy = -np.sum(W_s * np.log(W_s + eps))
            max_entropy = np.log(nA)
            wm_conf = np.clip(1.0 - entropy / max_entropy, 0.0, 1.0)
            wm_weight = wm_conf * (3.0 / max(nS, 1))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update with asymmetric learning rates
            if r > 0.5:
                q[s, a] += lr_pos * (1.0 - q[s, a])
            else:
                q[s, a] += lr_neg * (0.0 - q[s, a])

            # Update choice kernel: decay then increment chosen action for this state
            ck[s, :] *= (1.0 - ck_decay)
            ck[s, a] += 1.0

            # WM diffusion toward uniform (mild) with load; and reward-based strengthening
            w = (1.0 - 0.1 * load_factor) * w + 0.1 * load_factor * w0
            if r > 0.5:
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
                w[s, :] /= np.sum(w[s, :]) + 1e-12

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load-dependent forgetting + WM as limited slots (probabilistic availability).

    Rationale:
    - RL values forget toward uniform under higher load.
    - WM stores one-hot correct actions on rewarded trials; availability is the probability
      that the current state is within the limited slots.
    - Arbitration weight = probability that WM is available for the state, reduced by load.
    - WM also leaks toward uniform under load.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - beta_base: RL inverse temperature for all participants (scaled by 10 internally)
    - slots_y: number of WM slots if young
    - slots_o: number of WM slots if old
    - rl_forget_base: base RL forgetting rate toward uniform per trial (scaled by load)
    - load_slope: increases WM availability drop and WM leak with load

    Age and set-size effects:
    - Age determines the effective number of WM slots (slots_y vs slots_o).
    - Set size increases RL forgetting and WM leak with a factor proportional to (nS-3)/3,
      and reduces WM availability via a load_slope-scaled penalty.
    """
    lr, beta_base, slots_y, slots_o, rl_forget_base, load_slope = model_parameters

    age_group = 0 if age[0] <= 45 else 1
    beta = beta_base * 10.0
    beta_wm = 50.0

    slots = slots_y if age_group == 0 else slots_o

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        load_factor = max(nS - 3, 0) / 3.0

        # RL forgetting toward uniform increases with load
        rl_forget = np.clip(rl_forget_base * load_factor, 0.0, 1.0)

        # WM availability and leak under load
        base_avail = np.clip(slots / max(nS, 1), 0.0, 1.0)
        wm_avail = np.clip(base_avail - load_slope * load_factor, 0.0, 1.0)
        wm_leak = np.clip(load_slope * load_factor, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            rl_logits = beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(rl_logits)
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = max(pi_rl[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            wm_logits = beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = max(pi_wm[a], 1e-12)

            # Arbitration by WM availability
            wm_weight = wm_avail
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update + forgetting toward uniform
            pe = r - q[s, a]
            q[s, a] += lr * pe
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            # WM leak toward uniform
            w = (1.0 - wm_leak) * w + wm_leak * w0

            # WM write on reward: store perfect association for that state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

    return nll