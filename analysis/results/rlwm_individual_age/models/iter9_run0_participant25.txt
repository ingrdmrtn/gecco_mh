def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM mixture with age-dependent lapses and asymmetric RL learning.

    Mechanism:
    - Action choice is a mixture of RL and WM policies plus a small lapse probability.
    - RL uses a softmax with inverse temperature softmax_beta*10 and asymmetric learning rates
      for positive vs. negative prediction errors.
    - WM policy is high-precision softmax over a WM table. WM has a slot-capacity C_slots:
      effective WM contribution scales with min(1, C_slots / set_size).
    - Age-dependent lapses: lapse increases multiplicatively for older adults.
    - WM update: reward-consistent, one-hot attraction on correct trials; gentle recency-based decay.

    Parameters (6):
    - lr_pos: RL learning rate for positive PE (0..1)
    - lr_neg: RL learning rate for negative PE (0..1)
    - beta_scale: RL inverse temperature scale (>0), multiplied by 10 internally
    - wm_gate0: baseline WM mixture weight (0..1)
    - C_slots: WM capacity in slots (>=1)
    - age_lapse_mult: multiplicative factor on lapses for older adults (>=1)

    Age use:
    - Older group has higher lapse via age_lapse_mult.

    Set size use:
    - WM weight is reduced by capacity limit: wm_weight_eff = wm_gate0 * min(1, C_slots/nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, beta_scale, wm_gate0, C_slots, age_lapse_mult = model_parameters
    softmax_beta = beta_scale * 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Lapse rate: small baseline for young; older increases via multiplier
        lapse_base_young = 0.02
        lapse = lapse_base_young * (1.0 + (age_lapse_mult - 1.0) * age_group)
        lapse = float(np.clip(lapse, 0.0, 0.2))

        # Capacity-adjusted WM weight
        cap_ratio = float(np.clip(C_slots / max(1.0, nS), 0.0, 1.0))
        wm_weight_eff = float(np.clip(wm_gate0 * cap_ratio, 0.0, 1.0))

        # Gentle WM decay per visit (stronger with larger set sizes)
        wm_decay = 0.05 + 0.10 * max(0, nS - 3) / 3.0
        wm_decay = float(np.clip(wm_decay, 0.0, 0.3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM decay toward uniform due to interference
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * (1.0 / nA)

            # Reward-consistent WM strengthening (one-hot attraction)
            if r > 0.0:
                k = 0.7  # strong attraction on success
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k) * w[s, :] + k * onehot
            else:
                # On error, slight anti-attraction away from chosen action
                k_err = 0.15
                anti = np.ones(nA) / (nA - 1.0)
                anti[a] = 0.0
                w[s, :] = (1.0 - k_err) * w[s, :] + k_err * anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Streak-gated WM with decay and RL temperature: WM gating increases with recent correctness streak.

    Mechanism:
    - Action probability is a mixture of RL and WM policies.
    - RL: single learning rate and inverse temperature beta_rl*10.
    - WM: high-precision softmax. WM decays each visit; decay increases with set size and age.
    - WM gating depends on per-state correctness streak: more recent correct responses in state s
      increase the WM mixture weight for that state.
    - Age reduces streak gain and increases decay.
    
    Parameters (6):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature scale (>0), multiplied by 10 internally
    - wm_beta: WM inverse temperature baseline added to 50 via scaling factor (>0)
    - gate_base: baseline WM mixture weight (0..1)
    - streak_gain: gain mapping from streak length to additional WM weight (>=0)
    - decay_ss: base WM decay per visit that scales with set size (>=0)

    Age use:
    - Older group: reduced effective streak_gain and increased decay.

    Set size use:
    - Larger set size increases decay and reduces effective WM beta via noise.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_beta, gate_base, streak_gain, decay_ss = model_parameters
    softmax_beta = beta_rl * 10.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        # Track per-state correctness streaks
        streak = np.zeros(nS, dtype=float)

        # WM temperature adjusted by set size (more noise for larger sets)
        ss_noise = max(0.0, nS - 3) / 3.0  # 0 for 3, 1 for 6
        softmax_beta_wm = 50.0 / (1.0 + wm_beta * ss_noise)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective WM gating from streak
            # Age reduces the effect of streak
            gain_eff = streak_gain * (1.0 - 0.5 * age_group)
            wm_weight_eff = gate_base + gain_eff * streak[s]
            # Compress into [0,1] with a saturating function
            wm_weight_eff = float(np.clip(wm_weight_eff / (1.0 + abs(wm_weight_eff)), 0.0, 1.0))

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # Update per-state streak (increase on reward, decay otherwise)
            if r > 0.0:
                streak[s] += 1.0
            else:
                streak[s] = max(0.0, streak[s] - 0.5)

            # WM decay increases with set size and age
            decay = decay_ss * (1.0 + ss_noise) * (1.0 + 0.5 * age_group)
            decay = float(np.clip(decay, 0.0, 1.0))
            w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)

            # WM attraction to chosen action depends on reward and streak
            if r > 0.0:
                k = 0.4 + 0.1 * np.tanh(streak[s])
                k = float(np.clip(k, 0.0, 1.0))
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k) * w[s, :] + k * onehot
            else:
                # Mild repulsion on error
                k_err = 0.1 + 0.05 * ss_noise
                anti = np.ones(nA) / (nA - 1.0)
                anti[a] = 0.0
                w[s, :] = (1.0 - k_err) * w[s, :] + k_err * anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-based arbitration between RL and WM with set-size noise and age bias.

    Mechanism:
    - Action probability is a mixture based on relative uncertainty: higher RL entropy relative
      to WM increases WM weight, and vice versa.
    - RL uses softmax with inverse temperature beta_scale*10 and a single learning rate.
    - WM uses a high-precision softmax but suffers set-size-dependent noise that increases entropy.
    - Arbitration: wm_weight_eff = sigmoid(uncert_temp * (H_rl - H_wm) + ss_term + age_bias),
      where H_rl and H_wm are per-state policy entropies.
    - Age bias shifts arbitration toward RL in older adults (reduced WM reliance).

    Parameters (6):
    - lr: RL learning rate (0..1)
    - beta_scale: RL inverse temperature scale (>0), multiplied by 10 internally
    - wm_weight0: baseline WM weight added after sigmoid (0..1), controls mixture offset
    - uncert_temp: arbitration sensitivity to entropy difference (>0)
    - wm_noise_ss: strength of WM noise increase with set size (>=0)
    - age_bias_wm: bias term added for young (positive) vs. old (negative) toward WM

    Age use:
    - wm_bias = age_bias_wm * (1 - 2*age_group): positive for young, negative for old.

    Set size use:
    - WM entropy increases with set size via wm_noise_ss; arbitration term ss_term penalizes WM for larger sets.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_scale, wm_weight0, uncert_temp, wm_noise_ss, age_bias_wm = model_parameters
    softmax_beta = beta_scale * 10.0
    softmax_beta_wm_base = 50.0

    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        ss_scale = max(0.0, nS - 3) / 3.0  # 0 for 3, 1 for 6
        # WM temperature reduced by noise for larger sets (more entropy)
        softmax_beta_wm = softmax_beta_wm_base / (1.0 + wm_noise_ss * ss_scale)

        # Age bias term toward WM for young (positive) or away for old (negative)
        age_bias = age_bias_wm * (1 - 2 * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute full action distributions for entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits) / np.maximum(np.sum(np.exp(rl_logits)), eps)
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits) / np.maximum(np.sum(np.exp(wm_logits)), eps)

            # Choice probabilities for observed action (as in template)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Entropy (base e)
            H_rl = -np.sum(rl_probs * np.log(np.maximum(rl_probs, eps)))
            H_wm = -np.sum(wm_probs * np.log(np.maximum(wm_probs, eps)))

            # Set-size penalty term (discourage WM when set size is large)
            ss_term = -ss_scale

            # Arbitration via sigmoid of entropy difference + biases, then blended with wm_weight0
            arb = 1.0 / (1.0 + np.exp(-uncert_temp * (H_rl - H_wm) + ss_term + age_bias))
            wm_weight_eff = float(np.clip(0.5 * wm_weight0 + (1.0 - 0.5 * wm_weight0) * arb, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: decay then reward-aligned sharpening
            decay = 0.05 + 0.15 * ss_scale
            w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)

            if r > 0.0:
                k = 0.6
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - k) * w[s, :] + k * onehot
            else:
                # Mild flattening on negative outcome
                k_flat = 0.1 + 0.1 * ss_scale
                w[s, :] = (1.0 - k_flat) * w[s, :] + k_flat * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p