Below are three distinct cognitive models for the RL-WM task. Each model:

- Implements tabular RL (q) and a working memory store (w).
- Mixes RL and WM action probabilities through an arbitration mechanism that depends on age group and set size.
- Returns the negative log-likelihood of the observed choices.
- Uses all provided parameters meaningfully (â‰¤ 6 per model).
- Only fills in the WM policy and WM updating inside the indicated template regions.

Note: Assumes numpy as np is already imported by the environment.

------------------------------------------------------------

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    PE-gated WM with load- and age-modulated WM weight and decay.

    Mechanism
    - RL: tabular Q-learning with softmax choice (as in template).
    - WM: a table of action probabilities w, softmaxed with a high inverse temperature (deterministic).
           WM decays toward uniform each trial; decay increases with set size and (for older group) age.
           On reward, WM is updated toward a one-hot for the rewarded action; on non-reward, WM softly
           demotes the chosen action toward uniform.
    - Arbitration: trial-wise WM weight is a logistic function of a base term, set size (load), and age group,
      further gated by the magnitude of the current RL prediction error (|PE|); high surprise shifts weight to RL.

    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; multiplied by 10 internally
    - wm_weight_base: baseline bias for WM weight before load/age adjustments (logit space)
    - pe_gate_slope: strength of gating WM by |PE|; higher => more shift to RL on large |PE|
    - load_decay_slope: how much WM decay increases with set size (nS-3)
    - age_wm_shift: additive term to WM weight (negative values reduce WM reliance for older group)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, pe_gate_slope, load_decay_slope, age_wm_shift = model_parameters
    softmax_beta *= 10.0  # RL beta scaled up
    softmax_beta_wm = 50.0  # near-deterministic WM
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Block-level modifiers
        load_term = max(0, nS - 3)  # 0 for set size 3, positive for 6
        # WM decay increases with load and for older group
        base_decay = 0.02
        wm_decay = np.clip(base_decay + load_decay_slope * load_term + 0.02 * age_group, 0.0, 0.5)

        # Base WM weight on load and age (logit space)
        wm_logit = wm_weight_base + (-0.6 * load_term) + (age_wm_shift * (1 if age_group == 1 else -1))
        wm_weight_block = 1.0 / (1.0 + np.exp(-wm_logit))
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (probability of chosen action a)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy via softmax on WM table
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # PE-based gating: large |PE| reduces reliance on WM for this trial
            pe_now = r - Q_s[a]
            gate = 1.0 - (1.0 / (1.0 + np.exp(-pe_gate_slope * np.abs(pe_now))))
            # gate in [0,1]; near 1 when |PE| small, near 0 when |PE| large
            wm_weight_trial = np.clip(wm_weight_block * gate, 0.0, 1.0)

            p_total = wm_weight_trial * p_wm + (1.0 - wm_weight_trial) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global WM decay toward uniform (interference increases with load and age)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-dependent consolidation into WM
            if r > 0.5:
                # Move W_s toward one-hot on chosen action
                eta_pos = 0.6  # strong consolidation on reward
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                w[s, :] = (1.0 - eta_pos) * w[s, :] + eta_pos * target
            else:
                # Mild demotion of the chosen action toward uniform when not rewarded
                eta_neg = np.clip(0.1 + 0.1 * np.abs(delta), 0.0, 0.3)
                w[s, a] = (1.0 - eta_neg) * w[s, a] + eta_neg * (1.0 / nA)

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Priority-based WM capacity with state-specific arbitration.

    Mechanism
    - RL: tabular Q-learning with softmax choice (template).
    - WM: a probability table updated strongly on rewards. WM decays slightly each trial.
    - State priority and capacity: each state gets a dynamic priority (increases with rewards).
      Only the top-K states (per block) receive high WM weight; others get a small WM contribution.
      K depends on set size and age. This models limited WM slots and strategic allocation.
    - Arbitration: per-trial WM weight depends on whether the current state is among the top-K priority states.

    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; multiplied by 10 internally
    - wm_weight_high: WM mixture weight for states inside the top-K set
    - K_base: baseline capacity parameter (logit-space, transformed to K via sigmoid times nS)
    - load_K_slope: how much increasing set size reduces effective capacity
    - age_K_shift: age-dependent shift of capacity (positive -> larger K in older group, negative -> smaller)

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_high, K_base, load_K_slope, age_K_shift = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State priority scores (higher => more likely to be in WM)
        priority = np.zeros(nS)

        # Capacity K as a function of load and age
        load_term = max(0, nS - 3)
        cap_logit = K_base + (-load_K_slope * load_term) + (age_K_shift if age_group == 1 else -age_K_shift)
        cap_frac = 1.0 / (1.0 + np.exp(-cap_logit))  # fraction of nS
        K_eff = int(np.clip(np.round(cap_frac * nS), 1, nS))

        # WM decay small and load-sensitive
        wm_decay = np.clip(0.02 + 0.01 * load_term + 0.01 * age_group, 0.0, 0.3)

        # Low WM weight for out-of-capacity states
        wm_weight_low = np.clip(0.1 * wm_weight_high, 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Determine top-K states by priority
            topK_idx = np.argsort(-priority)[:K_eff]
            in_capacity = (s in topK_idx)

            # WM policy via softmax, arbitration weight based on capacity membership
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            wm_weight = wm_weight_high if in_capacity else wm_weight_low

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global mild WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-driven WM update for current state
            if r > 0.5:
                eta = 0.6
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target
                priority[s] += 1.0  # increase priority on successful outcome
            else:
                # Slight penalty to chosen action and small decay of priority
                w[s, a] = 0.9 * w[s, a] + 0.1 * (1.0 / nA)
                priority[s] = max(0.0, priority[s] - 0.2)

            # Small global priority decay to keep ranking dynamic
            priority *= (1.0 - 0.02)

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Lapse-augmented arbitration with load-sensitive WM precision and age-modulated lapse.

    Mechanism
    - RL: tabular Q-learning with softmax choice (template).
    - WM: probability table w, updated toward one-hot on rewards; decays to uniform each trial.
           WM precision/learning strength reduced by load; older age increases lapse.
    - Arbitration: mixture of RL and WM further blended with a lapse to random choice.
      Lapse grows with set size and with age group.

    Parameters (len=6)
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature; multiplied by 10 internally
    - wm_bias: controls both WM reliance and WM learning strength via a sigmoid transform
    - lapse_base: baseline lapse (logit space)
    - age_lapse_shift: additive shift to lapse logit for older group (positive => more lapse)
    - load_wm_penalty: how much increasing set size reduces WM reliance and learning strength

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_bias, lapse_base, age_lapse_shift, load_wm_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_group = 0 if age[0] <= 45 else 1

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load_term = max(0, nS - 3)

        # WM reliance and learning strength are derived from wm_bias and reduced by load
        wm_strength = 1.0 / (1.0 + np.exp(-(wm_bias - load_wm_penalty * load_term - 0.3 * age_group)))
        wm_weight_block = np.clip(wm_strength, 0.0, 1.0)

        # Lapse probability depends on load and age in logit space
        lapse_logit = lapse_base + 0.7 * load_term + (age_lapse_shift if age_group == 1 else -age_lapse_shift)
        lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
        lapse = np.clip(lapse, 0.0, 0.4)

        # WM decay: mild but increases with load
        wm_decay = np.clip(0.02 + 0.015 * load_term + 0.01 * age_group, 0.0, 0.4)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy via softmax; mixture with RL, then blend with lapse to uniform
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            mix_no_lapse = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * mix_no_lapse + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM learning strength tied to wm_strength (reduced by load/age)
            eta_wm = 0.2 + 0.6 * wm_strength  # in [0.2, 0.8] approximately
            if r > 0.5:
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * target
            else:
                # small corrective move toward uniform for the chosen action
                w[s, a] = 0.95 * w[s, a] + 0.05 * (1.0 / nA)

        blocks_log_p += log_p

    return -float(blocks_log_p)