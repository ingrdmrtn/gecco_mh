def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + WM with prediction-error-gated arbitration and set-size/age penalty on WM
    - RL: delta-rule with a single learning rate.
    - WM: fast one-shot associative store for rewarded actions; errors cause decay toward uniform.
    - Arbitration: WM weight is reduced by (a) large unsigned prediction errors (PE gate),
      and (b) higher set size and older age (penalty).
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight0: baseline WM mixture weight in [0,1] (transformed via logit internally).
    - softmax_beta: RL inverse temperature (scaled internally by 10 for a higher upper bound).
    - pe_gate: sensitivity of WM-vs-RL arbitration to unsigned prediction error (>=0).
    - ss_age_penalty: penalty factor reducing WM contribution with set size and age (>=0).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, pe_gate, ss_age_penalty = model_parameters
    softmax_beta *= 10.0  # higher upper bound for RL temperature
    
    # Age group coding: 0 = young, 1 = old
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0  # very deterministic WM baseline
    blocks_log_p = 0.0
    eps = 1e-12
    
    # Helper: logit and sigmoid
    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))
    
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Running unsigned PE (for gating); initialize small
        last_abs_pe = 0.0
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            # RL policy: p(a|s) under RL
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)
            
            # WM policy: softmax over W with an effective precision that is penalized by set size and age
            penalty = 1.0 + ss_age_penalty * (max(0, nS - 3) + age_group)
            beta_wm_eff = softmax_beta_wm / max(0.1, penalty)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)
            
            # Arbitration: WM weight down-regulated by unsigned PE and by set size/age penalty
            base_logit = logit(wm_weight0)
            wm_gate = sigmoid(base_logit - pe_gate * last_abs_pe - ss_age_penalty * (max(0, nS - 3) + age_group))
            wm_weight = wm_gate  # use the name expected by the template combination
            
            # Mixture and log-likelihood
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update: reward -> one-shot write; no reward -> decay toward uniform (strength tied to pe_gate and penalty)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                learn_strength = 1.0 / max(1.0, penalty)  # reduced with higher penalty
                w[s, :] = (1.0 - learn_strength) * w[s, :] + learn_strength * one_hot
            else:
                decay = np.clip(pe_gate / max(1.0, penalty), 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            
            # Update gating signal
            last_abs_pe = abs(delta)
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: RL with asymmetric learning rates + WM with capacity-limited precision
    - RL: separate learning rates for positive and negative outcomes.
    - WM: stores rewarded action per state; precision decreases with set size and with age.
    - Arbitration: fixed WM mixture weight, but WM policy precision is reduced by set size and age.
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive reward prediction errors.
    - lr_neg: RL learning rate for negative reward prediction errors.
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_weight: mixture weight for WM in [0,1].
    - capacity_slope: how much WM precision is degraded by set size and age (>=0).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, capacity_slope = model_parameters
    softmax_beta *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Effective WM precision penalty due to set size and age
        penalty = 1.0 + capacity_slope * (max(0, nS - 3) + age_group)
        beta_wm_eff = softmax_beta_wm / max(0.1, penalty)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)
            
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)
            
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_eff = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr_eff * delta
            
            # WM update: reward strengthens the chosen action; small global decay per step scaled by penalty
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                learn_strength = 1.0 / max(1.0, penalty)
                w[s, :] = (1.0 - learn_strength) * w[s, :] + learn_strength * one_hot
            # Apply interference/decay toward uniform each trial (stronger with higher penalty)
            global_decay = np.clip((capacity_slope / 5.0) / max(1.0, penalty), 0.0, 0.5)
            w = (1.0 - global_decay) * w + global_decay * w_0
        
        blocks_log_p += log_p
    
    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Visit-based gating of WM with age/set-size shift and WM decay
    - RL: delta-rule with single learning rate.
    - WM: one-shot learning on reward; decays toward uniform otherwise.
    - Arbitration: WM weight decreases as a state is encountered more often (visitation-based),
      and is further reduced by larger set size and older age.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight0: baseline WM mixture weight in [0,1] (used in a logistic gate).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - visit_slope: rate at which increased state visits reduce WM reliance (>=0).
    - age_setsize_shift: how much set size and age shift the gate toward RL (>=0).
    - wm_decay: decay rate of WM toward uniform after non-reward (0..1).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, visit_slope, age_setsize_shift, wm_decay = model_parameters
    softmax_beta *= 10.0
    
    age_group = 0 if age[0] <= 45 else 1
    
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    
    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))
    
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        
        # Track per-state visit counts within the block
        visits = np.zeros(nS)
        
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]
            
            Q_s = q[s, :]
            W_s = w[s, :]
            
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)
            
            # WM precision does not change, but gating does; still compute WM policy via softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)
            
            # Visit-based gate: more visits -> more RL; also shifted by set size and age
            gate_shift = visit_slope * visits[s] + age_setsize_shift * (max(0, nS - 3) + age_group)
            wm_weight = sigmoid(logit(wm_weight0) - gate_shift)
            
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)
            
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            
            # WM update: reward -> one-shot write; else decay toward uniform with rate wm_decay
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - 1.0) * w[s, :] + 1.0 * one_hot  # full overwrite on reward
            else:
                decay = np.clip(wm_decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            
            # Update visit count for the state
            visits[s] += 1
        
        blocks_log_p += log_p
    
    return -blocks_log_p