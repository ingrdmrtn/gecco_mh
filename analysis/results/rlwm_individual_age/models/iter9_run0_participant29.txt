Below are three standalone cognitive models that combine reinforcement learning (RL) with working memory (WM) in different ways. Each returns the negative log-likelihood of the observed choices, uses age group (0=young, 1=old) and set size (3 vs 6) meaningfully, and uses all parameters. None of the parameter combinations duplicate the tried sets you listed.

Note: Assume numpy (np) is already imported. No imports are included.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Capacity-limited WM with decay and load/age-gated arbitration.

    Mechanism:
    - RL: standard delta-rule with softmax choice.
    - WM: per-state "slot" storing of the last rewarded action, with decay toward uniform.
      The WM policy is near-deterministic when a state has a strong WM trace.
    - Arbitration: WM weight is a gate that decreases when set size exceeds effective
      capacity and when age/load increase. Gate = sigmoid(gate_phi_base - age_size_gate_gain * mismatch),
      where mismatch = max(0, set_size - effective_capacity). Effective capacity is reduced in
      older age and with larger set size.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay: WM decay toward uniform each trial (0..1); larger means faster decay
    - k_slots_base: baseline WM capacity in slots (>=1)
    - gate_phi_base: base gating intercept for WM usage (real)
    - age_size_gate_gain: gain for capacity reduction and gating penalty from load/age (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial
    - age: array with a single repeated value; age_group=0 if <=45 else 1
    - model_parameters: list/array of the six parameters above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_decay, k_slots_base, gate_phi_base, age_size_gate_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM probabilities per state

        # Effective capacity reduced by age and load
        extra_items = max(0, nS - 3)
        k_eff = max(1.0, k_slots_base - age_size_gate_gain * (age_group + 0.5 * extra_items))

        # Gate depends on how much the set size exceeds the effective capacity
        mismatch = max(0.0, nS - k_eff)
        gate = 1.0 / (1.0 + np.exp(-(gate_phi_base - age_size_gate_gain * mismatch)))
        gate = np.clip(gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            logits_rl = softmax_beta * q[s, :]
            logits_rl -= np.max(logits_rl)
            prl_all = np.exp(logits_rl)
            prl_all_sum = max(np.sum(prl_all), eps)
            prl_all /= prl_all_sum
            p_rl = prl_all[a]

            # WM policy (near-deterministic over WM weights)
            logits_wm = softmax_beta_wm * w[s, :]
            logits_wm -= np.max(logits_wm)
            pwm_all = np.exp(logits_wm)
            pwm_all_sum = max(np.sum(pwm_all), eps)
            pwm_all /= pwm_all_sum
            p_wm = pwm_all[a]

            # Arbitration: mix WM and RL with gate
            p_total = gate * p_wm + (1.0 - gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * (1.0 / nA)

            # WM storage on rewarded trials for the current state
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strengthen the stored action at state s
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-weighted prediction errors + leaky familiarity WM + load/age-based WM usage and stickiness.

    Mechanism:
    - RL: single learning rate, but the prediction error is scaled by kappa_pos vs kappa_neg
      depending on reward valence, allowing asymmetric sensitivity without changing lr itself.
    - WM: "familiarity" counts per state-action updated by leaky integration of chosen actions.
      WM policy favors recently chosen actions within a state.
    - Arbitration: WM weight decreases with age and set size via a logistic transform of
      -(size_age_lapse)*(age_group + extra_items). Thus older age and larger set size reduce WM usage.
    - Stickiness: adds an action perseveration bias into the RL policy via a stickiness term.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - kappa_pos: multiplier for PE on rewarded trials (>=0)
    - kappa_neg: multiplier for PE on non-reward trials (>=0)
    - stickiness: bias added to the last chosen action's logit in RL (real, could be +/-)
    - size_age_lapse: controls how quickly WM usage declines with age/load (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial
    - age: array with a single repeated value; age_group=0 if <=45 else 1
    - model_parameters: list/array of the six parameters above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, kappa_pos, kappa_neg, stickiness, size_age_lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # Familiarity (WM) per state-action
        f = np.ones((nS, nA)) / nA

        # WM usage decreases with age and load
        extra_items = max(0, nS - 3)
        wm_weight = 1.0 / (1.0 + np.exp(size_age_lapse * (age_group + extra_items)))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        last_action = None  # for stickiness
        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with stickiness
            logits_rl = softmax_beta * q[s, :]
            if last_action is not None:
                logits_rl[last_action] += stickiness
            logits_rl -= np.max(logits_rl)
            prl_all = np.exp(logits_rl)
            prl_all_sum = max(np.sum(prl_all), eps)
            prl_all /= prl_all_sum
            p_rl = prl_all[a]

            # WM policy from familiarity
            logits_wm = softmax_beta_wm * f[s, :]
            logits_wm -= np.max(logits_wm)
            pwm_all = np.exp(logits_wm)
            pwm_all_sum = max(np.sum(pwm_all), eps)
            pwm_all /= pwm_all_sum
            p_wm = pwm_all[a]

            # Arbitration
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with valence-weighted PE
            pe = r - q[s, a]
            kappa = kappa_pos if r > 0.5 else kappa_neg
            q[s, a] += lr * kappa * pe

            # WM familiarity update: leaky integrate toward chosen action at this state
            # Use size_age_lapse to govern leak (larger -> more leak)
            leak = 1.0 / (1.0 + size_age_lapse)  # bounded in (0,1]
            f[s, :] = (1.0 - leak) * f[s, :]
            f[s, a] += leak
            # Renormalize to a distribution
            row_sum = max(np.sum(f[s, :]), eps)
            f[s, :] /= row_sum

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-control between RL and WSLS heuristic, driven by recent accuracy with load/age penalty.

    Mechanism:
    - RL: standard delta-rule with softmax choice.
    - WSLS (WM-like heuristic): per-state memory of last action and outcome;
      if last outcome was reward, prefer staying; else prefer shifting away.
      The strength of this heuristic is controlled by wsls_bias and a leak on memory certainty.
    - Arbitration: weight on WSLS increases when recent accuracy (EMA of rewards) is high,
      but is penalized by age and set size (load). Weight = sigmoid(eta_accuracy * acc_ema
      - load_age_penalty * (age_group + extra_items)).
    - WM leak: decays the certainty that the last state memory is valid.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wsls_bias: scale of WSLS preference in the WM softmax (>=0)
    - eta_accuracy: sensitivity of arbitration to recent accuracy (real)
    - load_age_penalty: penalty on WM arbitration from age/load (>=0)
    - wm_leak: leak (decay) of WSLS memory certainty per trial (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial
    - age: array with a single repeated value; age_group=0 if <=45 else 1
    - model_parameters: list/array of the six parameters above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wsls_bias, eta_accuracy, load_age_penalty, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_states = states[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WSLS state memory: last action and last reward per state
        last_a = -1 * np.ones(nS, dtype=int)
        last_r = np.zeros(nS)
        mem_conf = np.zeros(nS)  # memory certainty per state

        extra_items = max(0, nS - 3)

        # EMA of recent accuracy within the block
        acc_ema = 0.5  # start neutral
        ema_alpha = 0.3

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            logits_rl = softmax_beta * q[s, :]
            logits_rl -= np.max(logits_rl)
            prl_all = np.exp(logits_rl)
            prl_all_sum = max(np.sum(prl_all), eps)
            prl_all /= prl_all_sum
            p_rl = prl_all[a]

            # WSLS policy construction
            pref = np.zeros(nA)
            if last_a[s] >= 0:
                if last_r[s] > 0.5:
                    # Win: prefer staying with last action
                    pref[last_a[s]] = 1.0
                else:
                    # Lose: prefer shifting away from last action
                    pref[last_a[s]] = -1.0
            # Scale by WSLS bias and memory confidence
            logits_wm = softmax_beta_wm * wsls_bias * mem_conf[s] * pref
            logits_wm -= np.max(logits_wm)
            pwm_all = np.exp(logits_wm)
            pwm_all_sum = max(np.sum(pwm_all), eps)
            pwm_all /= pwm_all_sum
            p_wm = pwm_all[a]

            # Arbitration weight from accuracy with load/age penalty
            gating_input = eta_accuracy * acc_ema - load_age_penalty * (age_group + extra_items)
            wm_weight = 1.0 / (1.0 + np.exp(-gating_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update WSLS memory for this state with leak on confidence
            mem_conf *= (1.0 - wm_leak)
            last_a[s] = a
            last_r[s] = r
            # Increase confidence on update for this state
            mem_conf[s] = min(1.0, mem_conf[s] + (1.0 - wm_leak))

            # Update accuracy EMA
            acc_ema = (1.0 - ema_alpha) * acc_ema + ema_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p