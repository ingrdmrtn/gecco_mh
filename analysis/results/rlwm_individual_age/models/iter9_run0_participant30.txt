def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM with interference and age-modulated temperature.

    Idea:
    - RL learns action values with a delta rule.
    - WM stores stimulus-action values with near one-shot updating, but suffers global interference (implemented as decay toward uniform).
    - Arbitration weight for WM depends on an effective capacity vs. current set size.
    - Older group is modeled as more stochastic in RL (lower inverse temperature).

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, K_eff, interference, age_temp_factor]
        - lr: RL learning rate (0..1).
        - wm_weight_base: baseline WM arbitration weight (0..1).
        - softmax_beta: baseline inverse temperature (>0), scaled by 10 internally.
        - K_eff: effective WM capacity in number of items (positive real).
        - interference: controls both set-size slope in arbitration and WM decay (real, mapped internally).
        - age_temp_factor: scales reduction in beta for older group (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, K_eff, interference, age_temp_factor = model_parameters
    softmax_beta *= 10.0  # higher dynamic range
    age_group = 0 if age[0] <= 45 else 1

    # WM policy precision (kept high/deterministic)
    softmax_beta_wm = 50.0

    # Map interference to [0,1] for decay; also use as arbitration slope
    decay = 1.0 / (1.0 + np.exp(-interference))  # 0..1
    slope = interference  # can be negative/positive; used in arbitration

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age-modulated RL temperature
        beta_eff = softmax_beta * np.exp(-age_temp_factor * age_group)

        # Value tables initialized to uniform
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute logit of base WM weight
        eps = 1e-8
        base_logit = np.log(np.clip(wm_weight_base, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight_base, eps, 1 - eps))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy
            U_rl = beta_eff * Q_s
            p_rl = 1.0 / np.sum(np.exp(U_rl - U_rl[a]))

            # WM policy
            W_s = w[s, :]
            U_wm = softmax_beta_wm * W_s
            p_wm = 1.0 / np.sum(np.exp(U_wm - U_wm[a]))

            # Arbitration: capacity vs set size with slope "slope"
            K_eff_age = K_eff - 0.5 * age_group  # slightly lower capacity assumed for older group
            gate_input = base_logit + slope * (K_eff_age - nS)
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM interference/decay then near one-shot update
            w = (1.0 - decay) * w + decay * w_0
            w[s, a] += (r - w[s, a])  # push chosen item toward observed outcome

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-driven arbitration with surprise-gated WM learning.

    Idea:
    - RL updates with delta rule.
    - WM updates with its own learning rate.
    - Arbitration weight depends on trialwise surprise and set size: more surprise -> trust WM more,
      larger set size -> trust WM less. Age adds a bias against WM use in older group.
    - This implements a dynamic gate combining a base weight and a sensitivity to surprise.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_lr, gate_sensitivity, age_gate_bias]
        - lr: RL learning rate (0..1).
        - wm_weight_base: baseline WM arbitration weight (0..1).
        - softmax_beta: baseline inverse temperature (>0), scaled by 10 internally.
        - wm_lr: WM learning rate (0..1).
        - gate_sensitivity: strength of surprise and set size in the WM gate (>0).
        - age_gate_bias: bias against WM in older group (>=0), subtracted from gate input.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_lr, gate_sensitivity, age_gate_bias = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # WM is precise

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            U_rl = softmax_beta * Q_s
            p_rl = 1.0 / np.sum(np.exp(U_rl - U_rl[a]))

            W_s = w[s, :]
            U_wm = softmax_beta_wm * W_s
            p_wm = 1.0 / np.sum(np.exp(U_wm - U_wm[a]))

            # Surprise (absolute PE) and set size penalty
            pe = abs(r - Q_s[a])  # 0..1
            size_pen = (nS - 3) / 3.0  # 0 for 3, 1 for 6

            # Gate combines base weight (in logit) + surprise - size - age bias
            eps = 1e-8
            base_logit = np.log(np.clip(wm_weight_base, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight_base, eps, 1 - eps))
            gate_input = base_logit + gate_sensitivity * (pe - size_pen) - age_gate_bias * age_group
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_dyn = np.clip(wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: gated by surprise (bigger PE -> stronger update)
            w[s, a] += wm_lr * pe * (r - w[s, a])

            # Mild global stabilization toward uniform to prevent runaway
            w = 0.98 * w + 0.02 * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with age- and load-modulated perseveration and decaying WM.

    Idea:
    - RL uses separate learning rates for positive and negative outcomes.
    - WM decays toward uniform and updates the chosen entry; its contribution is weighted by set size and age.
    - Action perseveration (stickiness) biases both RL and WM policies toward the previous action,
      and this bias grows with set size and with age.

    Parameters
    ----------
    states : array-like
        State index per trial within block (0..nS-1).
    actions : array-like
        Chosen action per trial (0..2).
    rewards : array-like
        Reward per trial (0 or 1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (3 or 6).
    age : array-like
        Participant age repeated per trial.
    model_parameters : list or array
        [alpha_pos, alpha_neg, softmax_beta, wm_weight_base, wm_decay, stickiness_age_scale]
        - alpha_pos: RL learning rate for rewards (0..1).
        - alpha_neg: RL learning rate for non-rewards (0..1).
        - softmax_beta: inverse temperature (>0), scaled by 10 internally.
        - wm_weight_base: baseline WM weight (0..1).
        - wm_decay: WM decay toward uniform each trial (0..1).
        - stickiness_age_scale: base perseveration strength scaled by age group and set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, wm_decay, stickiness_age_scale = model_parameters
    softmax_beta *= 10.0
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        prev_action = None
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Perseveration bias scaled by age and set size
            stickiness = stickiness_age_scale * (1.0 + 0.5 * age_group) * (nS / 6.0)
            bias_vec = np.zeros(nA)
            if prev_action is not None:
                bias_vec[prev_action] = stickiness

            # RL policy with stickiness
            Q_s = q[s, :]
            U_rl = softmax_beta * Q_s + bias_vec
            p_rl = 1.0 / np.sum(np.exp(U_rl - U_rl[a]))

            # WM policy with stickiness
            W_s = w[s, :]
            U_wm = softmax_beta_wm * W_s + bias_vec
            p_wm = 1.0 / np.sum(np.exp(U_wm - U_wm[a]))

            # Arbitration: WM de-emphasized with larger set size and in older group
            wm_weight_dyn = wm_weight_base * (3.0 / nS) * (1.0 - 0.3 * age_group)
            wm_weight_dyn = float(np.clip(wm_weight_dyn, 0.0, 1.0))

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM decay toward uniform then update chosen entry (learning strength tied to remaining mass)
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            w[s, a] += (1.0 - wm_decay) * (r - w[s, a])

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p