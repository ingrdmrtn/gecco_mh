def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with optimistic Q-initialization, age- and set-size-modulated WM strength and decay.

    Description:
    - RL uses a single learning rate and softmax temperature. Q-values are initialized optimistically,
      which speeds acquisition; optimism is reduced in older adults.
    - WM stores a near-deterministic action template per state that decays toward uniform.
      WM weight is reduced by larger set sizes (3 -> 6) and by older age; WM decay increases
      with set size and age (more interference/maintenance difficulty).
    - Arbitration is a convex mixture of WM and RL policies.

    Parameters
    ----------
    model_parameters : [lr, beta_base, wm_weight_base, q_init, wm_decay_base]
        - lr: RL learning rate.
        - beta_base: Base inverse temperature for RL softmax (scaled by 10 internally).
        - wm_weight_base: Baseline WM contribution weight in arbitration.
        - q_init: Optimistic initial Q-value for all state-actions; reduced in older group.
        - wm_decay_base: Baseline WM decay per visit toward uniform (increases with set size and age).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, beta_base, wm_weight_base, q_init, wm_decay_base = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM policy
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values with optimistic initialization (age-modulated)
        q_init_eff = q_init * (1.0 - 0.3 * age_group)
        q = q_init_eff * np.ones((nS, nA))

        # WM store with uniform prior
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective parameters modulated by set size and age
        softmax_beta = beta_base * 10.0
        wm_weight_eff = np.clip(wm_weight_base * (3.0 / nS) * (1.0 - 0.2 * age_group), 0.0, 1.0)
        wm_decay_eff = np.clip(wm_decay_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action probability (probability of chosen action a)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM action probability (deterministic template subject to decay)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and encoding (encode rewarded action strongly)
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]
            if r > 0.0:
                # move distribution toward a one-hot at chosen action
                gain = 0.9
                w[s, a] += (1.0 - w[s, a]) * gain
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size/age-adaptive learning rate + WM with probabilistic encoding.

    Description:
    - RL uses a learning rate that decreases with larger set sizes (more to learn) and with older age,
      modeling slower integration. A single softmax temperature governs RL choice.
    - WM encodes rewarded associations with a strength determined by a base encoding gain pushed
      through a logistic nonlinearity; this encoding strength is reduced by larger set sizes and
      older age (fewer effective resources). WM decays modestly between visits via implicit
      renormalization.
    - Arbitration mixes WM and RL; WM weight is reduced by set size and age.

    Parameters
    ----------
    model_parameters : [lr_base, lr_size_pen, lr_age_pen, beta_base, wm_weight_base, wm_encode_sensitivity]
        - lr_base: Baseline RL learning rate.
        - lr_size_pen: Penalty factor on learning rate due to set size (>=0).
        - lr_age_pen: Additional learning-rate penalty for older age group (>=0).
        - beta_base: Base inverse temperature for RL softmax (scaled by 10 internally).
        - wm_weight_base: Baseline WM mixture weight before penalties.
        - wm_encode_sensitivity: Controls WM encoding strength via logistic; reduced with set size/age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_base, lr_size_pen, lr_age_pen, beta_base, wm_weight_base, wm_encode_sensitivity = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM initial states
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective RL learning rate
        size_factor = (nS / 3.0) - 1.0  # 0 for size=3, 1 for size=6
        lr_eff = lr_base / (1.0 + lr_size_pen * max(0.0, size_factor) + lr_age_pen * age_group)
        lr_eff = np.clip(lr_eff, 0.0, 1.0)

        # Arbitration weight reduced by set size and age
        wm_weight_eff = np.clip(wm_weight_base * (3.0 / nS) * (1.0 - 0.25 * age_group), 0.0, 1.0)

        # WM encoding strength (logistic of base sensitivity) with penalties
        base_encode = 1.0 / (1.0 + np.exp(-wm_encode_sensitivity))
        wm_encode_eff = base_encode * (3.0 / nS) * (1.0 - 0.3 * age_group)
        wm_encode_eff = np.clip(wm_encode_eff, 0.0, 1.0)

        softmax_beta = beta_base * 10.0

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM choice probabilities
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr_eff * pe

            # WM decay and probabilistic encoding (stronger for rewarded outcomes)
            # Decay toward uniform slightly each visit (implicit via partial overwrite + renorm)
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]  # light decay common across groups

            if r > 0.0:
                # move a fraction wm_encode_eff toward one-hot on chosen action
                delta_w = np.zeros_like(W_s)
                delta_w[a] = 1.0
                delta_w = delta_w - W_s
                w[s, :] = W_s + wm_encode_eff * delta_w
                # renormalize
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Conflict-based arbitration between RL and WM with separate temperatures.

    Description:
    - RL updates with a single learning rate and has its own inverse temperature (beta_rl).
    - WM forms a near-deterministic per-state template but with its own temperature (beta_wm).
    - Arbitration weight for WM decreases when the RL and WM policies disagree (conflict).
      Conflict is computed as 1 - overlap between the two distributions. Older age increases
      conflict impact; larger set sizes also reduce WM weight.

    Parameters
    ----------
    model_parameters : [lr, beta_rl, beta_wm, wm_weight_base, conflict_gain, age_conflict_shift]
        - lr: RL learning rate.
        - beta_rl: RL inverse temperature (scaled by 10 internally).
        - beta_wm: WM inverse temperature (scaled by 10 internally).
        - wm_weight_base: Baseline WM weight before conflict, set-size, and age penalties.
        - conflict_gain: How strongly policy conflict reduces WM weight.
        - age_conflict_shift: Multiplier increasing conflict effect for older group.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_rl, beta_wm, wm_weight_base, conflict_gain, age_conflict_shift = model_parameters

    age_group = 0 if age[0] <= 45 else 1

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_rl_eff = max(beta_rl, 1e-3) * 10.0
        beta_wm_eff = max(beta_wm, 1e-3) * 10.0

        # Base WM weight reduced by larger set sizes and age
        wm_weight_base_eff = np.clip(wm_weight_base * (3.0 / nS) * (1.0 - 0.2 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Full RL and WM distributions
            rl_logits = beta_rl_eff * (Q_s - np.max(Q_s))
            wm_logits = beta_wm_eff * (W_s - np.max(W_s))
            p_rl_vec = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            p_wm_vec = np.exp(wm_logits) / np.sum(np.exp(wm_logits))

            # Scalar probabilities for chosen action (as in template)
            p_rl = p_rl_vec[a]
            p_wm = p_wm_vec[a]

            # Conflict-based arbitration weight for WM
            overlap = np.sum(np.minimum(p_rl_vec, p_wm_vec))
            conflict = 1.0 - overlap  # 0 => agree, 1 => disjoint
            conflict_penalty = 1.0 + conflict_gain * conflict * (1.0 + age_conflict_shift * age_group)
            wm_weight = wm_weight_base_eff / conflict_penalty
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM: mild decay and encode rewarded associations
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            if r > 0.0:
                push = 0.85
                w[s, a] += (1.0 - w[s, a]) * push
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p