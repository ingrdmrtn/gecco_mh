def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with load-dependent WM decay and age-modulated decisional precision.

    Mechanism:
    - RL: single learning rate alpha with softmax choice (beta scaled).
    - WM: near-deterministic policy over a one-shot memory matrix that decays each trial.
    - Arbitration: convex mixture of WM and RL, with WM weight reduced by interference from set size.
    - Age use: age parameter scales inverse temperature (beta) differently across age groups.

    Parameters
    ----------
    states : array-like of int
        State per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like of int
        Participant age (repeated).
    model_parameters : list or array
        [alpha, wm_anchor, beta_base, decay_base, interference_gain, age_beta_scale]
        - alpha: RL learning rate.
        - wm_anchor: baseline arbitration weight for WM (0..1).
        - beta_base: base inverse temperature for RL (scaled by 10 internally).
        - decay_base: base WM decay per trial toward uniform (0..1).
        - interference_gain: extra WM interference/weakening per added item beyond 3.
        - age_beta_scale: scales decisional precision by age group
            (young: beta *= age_beta_scale; old: beta *= 2 - age_beta_scale).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, wm_anchor, beta_base, decay_base, interference_gain, age_beta_scale = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    # Age-dependent precision scaling applied for both groups (but differently)
    if age_group == 0:
        softmax_beta *= age_beta_scale
    else:
        softmax_beta *= (2.0 - age_beta_scale)

    softmax_beta_wm = 50.0  # near-deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay coefficient
        load = max(0, nS - 3)
        wm_decay = np.clip(decay_base * (1.0 + interference_gain * load), 0.0, 1.0)
        # Arbitration weight reduced under load
        wm_weight_base = wm_anchor / (1.0 + interference_gain * load)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (ratio trick to get p(a))
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (near-deterministic softmax on WM values)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Rewarded trials consolidate the chosen mapping
            if r > 0.0:
                # One-shot-like consolidation (strong write-in, normalized)
                consolidate = 0.7
                w[s, :] = (1.0 - consolidate) * w[s, :]
                w[s, a] += consolidate
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-specific learning and surprise-driven arbitration with WM.

    Mechanism:
    - RL learns with separate learning rates for rewards vs non-rewards.
    - WM stores state-action after rewards; decays modestly each trial.
    - Arbitration weight for WM increases when RL surprise is low (confidence high)
      and decreases when surprise is high; WM also weakened under larger set size.
    - Age use: shifts arbitration toward WM for young and away from WM for old.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of int
        Participant age (repeated). 0=young, 1=old (derived internally).
    model_parameters : list or array
        [alpha_pos, alpha_neg, beta_base, wm_base, surprise_sensitivity, age_wm_shift]
        - alpha_pos: RL learning rate after reward.
        - alpha_neg: RL learning rate after no reward.
        - beta_base: base inverse temperature for RL (scaled by 10 internally).
        - wm_base: baseline WM mixture weight.
        - surprise_sensitivity: how strongly arbitration responds to RL surprise
          (higher => shift away from WM when surprise is high).
        - age_wm_shift: adds to WM weight if young, subtracts if old.

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos, alpha_neg, beta_base, wm_base, surprise_sensitivity, age_wm_shift = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size penalty on WM contribution
        load = max(0, nS - 3)
        wm_load_divisor = 1.0 + 0.5 * load  # 1 for 3, 1.5 for 6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Surprise from RL: |PE| computed w.r.t. chosen action value
            pe = r - Q_s[a]
            # Map surprise into [0,1) using a squashing nonlinearity
            surprise = 1.0 - np.exp(-np.abs(pe))
            # Arbitration: push WM down when surprise is high
            wm_weight_eff = wm_base - surprise_sensitivity * surprise
            # Age shift: young toward WM, old away from WM
            wm_weight_eff += (1 - age_group) * age_wm_shift - age_group * age_wm_shift
            # Reduce with load
            wm_weight_eff /= wm_load_divisor
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning
            alpha = alpha_pos if pe > 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM: mild decay each trial and write on reward
            decay = 0.1 + 0.1 * load  # 0.1 at 3, 0.2 at 6
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            if r > 0.0:
                write = 0.6
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with action stickiness and WM mis-encoding under load.

    Mechanism:
    - RL: single alpha and softmax choice; includes within-state perseveration by
      boosting the last chosen action's value (stickiness).
    - WM: near-deterministic, but can mis-encode the chosen action on write with
      some probability; mis-encoding increases with set size and age_penalty.
    - Arbitration: fixed WM weight reduced modestly with load.
    - Age use: increases mis-encoding rate and reduces decision temperature in older adults.

    Parameters
    ----------
    states : array-like of int
        State per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of int
        Participant age (repeated). 0=young, 1=old (derived internally).
    model_parameters : list or array
        [alpha, beta_base, wm_weight, stickiness, wm_misencode, age_noise_gain]
        - alpha: RL learning rate.
        - beta_base: base inverse temperature for RL (scaled by 10 internally).
        - wm_weight: baseline arbitration weight for WM.
        - stickiness: value bonus added to last chosen action in the same state.
        - wm_misencode: base probability of mis-encoding the action into WM.
        - age_noise_gain: increases mis-encoding with set size and age; also reduces beta in old.

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta_base, wm_weight, stickiness, wm_misencode, age_noise_gain = model_parameters
    softmax_beta = beta_base * 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    # Age reduces decisional precision and increases mis-encoding
    if age_group == 1:
        softmax_beta /= (1.0 + 0.25 * age_noise_gain)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -np.ones(nS, dtype=int)

        load = max(0, nS - 3)
        # WM arbitration weakens with load
        wm_weight_eff_block = wm_weight / (1.0 + 0.5 * load)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Apply stickiness to RL values (within-state)
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            wm_weight_eff = np.clip(wm_weight_eff_block, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (note: update base q without stickiness)
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # WM decay toward uniform each visit
            decay = 0.15 + 0.05 * load  # 0.15 at 3, 0.20 at 6
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM write with possible mis-encoding on rewarded trials
            if r > 0.0:
                # Effective mis-encoding increases with load and age
                mis = wm_misencode * (1.0 + age_noise_gain * (load / 3.0 + 0.5 * age_group))
                mis = np.clip(mis, 0.0, 0.9)
                write = 0.7

                # Target distribution: mostly on chosen action, but some mass spread to others if mis-encoded
                target = np.ones(nA) * (mis / (nA - 1))
                target[a] = 1.0 - mis

                # Blend current WM with target and renormalize
                w[s, :] = (1.0 - write) * w[s, :] + write * target
                w[s, :] /= np.sum(w[s, :])

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p