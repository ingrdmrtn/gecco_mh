def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age- and set-size-scaled access probability.

    Mechanism:
    - RL: Q-learning with single learning rate and softmax choice.
    - WM store: a simple associative table that stores the last rewarded action per state
      (deterministic one-hot), with global noise/decay.
    - Capacity: WM has limited effective capacity; WM access probability scales with capacity
      relative to set size. Older age reduces effective capacity.
    - Mixture policy: Weighted mixture of WM and RL policies. WM weight is the product of
      a base weight and the probability that the current state is effectively in WM.

    Parameters (model_parameters; all used):
    - lr: RL learning rate in [0,1].
    - beta: RL inverse temperature (scaled internally by 10).
    - wm_prior: baseline WM contribution in [0,1] before capacity scaling.
    - cap_scale: scales WM capacity (K = 3*cap_scale, then adjusted by age).
    - age_wm_drop: proportional reduction in capacity for older group (age_group=1).
    - wm_noise: WM global decay/noise per trial in [0,1] toward uniform.

    Age and set-size effects:
    - Effective WM capacity: K_eff = max(1, 3*cap_scale*(1 - age_wm_drop*age_group)).
    - WM access probability on a trial: p_in = min(1, K_eff / nS). Larger set size reduces p_in.
    - WM mixture weight = wm_prior * p_in, capped to [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_prior, cap_scale, age_wm_drop, wm_noise = model_parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity and access probability
        K_eff = max(1.0, 3.0 * cap_scale * (1.0 - age_wm_drop * age_group))
        p_in = min(1.0, K_eff / float(nS))
        wm_weight_base = np.clip(wm_prior * p_in, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action a
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy: softmax with high beta over W_s
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture
            p_mix = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl
            p_total = max(p_mix, 1e-12)
            log_p += np.log(p_total)

            # RL learning update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM updating:
            # - Global decay toward uniform (wm_noise)
            # - If reward, encode deterministic mapping for this state
            w = (1.0 - wm_noise) * w + wm_noise * w0
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-rate RL + confidence-weighted WM from prediction errors.

    Mechanism:
    - RL: Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: associative table updated proportionally to the magnitude of the (signed) prediction error:
      larger surprise strengthens encoding. WM policy uses high-beta softmax.
    - Confidence gating: WM mixture weight scales with a running confidence per state derived from
      the exponentially weighted magnitude of past PEs for that state.
    - Age effect: older group has reduced inverse temperature (noisier RL policy) and reduced WM base.
    - Set size effect: WM weight scales by 3/nS.

    Parameters (model_parameters; all used):
    - alpha_pos: RL learning rate for positive PEs.
    - alpha_neg: RL learning rate for negative PEs.
    - beta: base RL inverse temperature (scaled internally by 10).
    - wm_base: baseline WM mixture weight.
    - conf_gain: gain controlling how PE magnitude maps to confidence (sigmoid).
    - beta_age_drop: proportional drop in inverse temperature for older group.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, wm_base, conf_gain, beta_age_drop = model_parameters
    age_group = 1 if age[0] > 45 else 0

    # Age reduces effective beta
    softmax_beta = beta * 10.0 * (1.0 - beta_age_drop * age_group)
    softmax_beta = max(1e-3, softmax_beta)  # keep positive
    softmax_beta_wm = 50.0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific running confidence (EWMA of |PE|)
        conf = np.zeros(nS)

        size_scale = 3.0 / float(nS)
        wm_base_eff = np.clip(wm_base * size_scale * (1.0 - 0.3 * age_group), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Confidence from previous step (before updating with current PE)
            # Sigmoid of centered confidence
            gate = 1.0 / (1.0 + np.exp(-conf_gain * (conf[s] - 0.25)))
            wm_weight_eff = np.clip(wm_base_eff * gate, 0.0, 1.0)

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_mix, 1e-12)
            log_p += np.log(p_total)

            # Compute PE and RL update with dual rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += alpha_pos * pe
            else:
                q[s, a] += alpha_neg * pe

            # WM updating proportional to |PE|
            # Larger |PE| -> stronger rewrite toward one-hot of chosen action if rewarded,
            # and away from chosen action if not rewarded.
            k = np.clip(abs(pe), 0.0, 1.0)
            if r > 0.5:
                # Move toward one-hot for action a
                w[s, :] = (1.0 - k) * w[s, :] + k * w0[s, :]
                w[s, a] = (1.0 - k) * w[s, a] + k * 1.0
            else:
                # Weaken confidence in chosen action and diffuse toward uniform
                w[s, :] = (1.0 - 0.5 * k) * w[s, :] + (0.5 * k) * w0[s, :]
                w[s, a] = max(w0[s, a], w[s, a] - 0.5 * k)

            # Update confidence as EWMA of |PE|
            conf[s] = 0.7 * conf[s] + 0.3 * abs(pe)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-driven meta-control and lapse.

    Mechanism:
    - RL: Q-learning with single learning rate and softmax.
    - WM: associative store that gets refreshed on rewards and decays otherwise.
    - Meta-control: estimate RL uncertainty via entropy of the RL policy over actions; when
      uncertainty is high, rely more on WM. Also scaled by set size (3/nS) and age (older reduce WM reliance).
    - Lapse: a set-size- and age-dependent lapse probability that produces uniform random choices.

    Parameters (model_parameters; all used):
    - lr: RL learning rate.
    - beta: base RL inverse temperature (scaled internally by 10).
    - wm_weight0: baseline WM weight before meta-control.
    - meta_sensitivity: scales the impact of RL uncertainty (entropy) on WM weight.
    - lapse_base: baseline lapse logit; converted to probability per trial.
    - age_temp_mod: proportional reduction of beta for older group.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_weight0, meta_sensitivity, lapse_base, age_temp_mod = model_parameters

    age_group = 1 if age[0] > 45 else 0
    softmax_beta = beta * 10.0 * (1.0 - age_temp_mod * age_group)
    softmax_beta = max(1e-3, softmax_beta)
    softmax_beta_wm = 50.0

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))

        size_scale = 3.0 / float(nS)  # smaller in 6-set
        wm_base_eff = np.clip(wm_weight0 * size_scale * (1.0 - 0.25 * age_group), 0.0, 1.0)

        # Lapse probability per trial via a logit combined with age and set size
        # p_lapse = sigmoid(lapse_base + 0.5*age_group + 0.5*(nS==6))
        lapse_logit = lapse_base + 0.5 * age_group + (0.5 if nS > 3 else 0.0)
        p_lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
        p_uniform = 1.0 / nA

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy vector and chosen prob
            Q_s = q[s, :]
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl_vec = np.exp(logits)
            pi_rl_vec = pi_rl_vec / np.sum(pi_rl_vec)
            p_rl = max(pi_rl_vec[a], 1e-12)

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm_vec = np.exp(logits_wm)
            pi_wm_vec = pi_wm_vec / np.sum(pi_wm_vec)
            p_wm = max(pi_wm_vec[a], 1e-12)

            # RL uncertainty via entropy (normalized between 0 and 1)
            entropy = -np.sum(pi_rl_vec * (np.log(pi_rl_vec + 1e-12)))
            max_entropy = np.log(nA)
            uncert = float(entropy / max_entropy)

            wm_weight_eff = np.clip(wm_base_eff * (1.0 + meta_sensitivity * uncert), 0.0, 1.0)

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - p_lapse) * p_mix + p_lapse * p_uniform
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM update: reward refresh and mild decay otherwise
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0
            else:
                # decay slightly toward uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p