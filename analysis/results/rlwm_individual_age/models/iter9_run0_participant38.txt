def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age and set-size modulation; lapse noise.

    Mechanisms:
    - RL: Q-learning with single learning rate and softmax policy.
    - WM: stores last rewarded action per state (one-hot) and decays toward uniform when no reward.
    - Arbitration: WM weight is a logistic function of a base weight reduced by cognitive load (set size)
      and by age group (older adults rely less on WM).
    - Lapse: with small probability, choices are uniform random (undirected noise).
    - Age and set-size effects:
      - WM weight decreases as set size increases and for older adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_base: base WM weight before modulation (real; passed through sigmoid)
    - softmax_beta: RL inverse temperature (scaled x10 internally)
    - cap_slope: how strongly set size reduces WM weight (>=0)
    - age_drop: additional WM reduction for older adults (>=0)
    - lapse: lapse probability mixing with uniform policy (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, cap_slope, age_drop, lapse = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # compute block-level WM weight given set size and age
        # logistic transform of wm_base, reduced by capacity cost and age
        cap_cost = cap_slope * ((nS - 3) / 3.0)  # 0 at nS=3, rises at nS=6
        wm_weight_block = 1.0 / (1.0 + np.exp(-(wm_base - cap_cost - age_drop * age_group)))
        wm_weight_block = np.clip(wm_weight_block, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy for chosen action a
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # POLICY FOR THE WORKING MEMORY: softmax over WM weights (sharp)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration and lapse
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # VALUE UPDATING FOR THE WORKING MEMORY
            # If rewarded, store chosen action as one-hot; if not, decay towards uniform baseline
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.7 * w[s, :] + 0.3 * one_hot
            else:
                # decay faster under higher set size (more interference)
                decay = 0.10 + 0.10 * ((nS - 3) / 3.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based reliability arbitration and age/set-size interference.

    Mechanisms:
    - RL: Q-learning; softmax policy.
    - WM: stores rewarded actions with learning rate; suffers interference that increases with set size
      and more so for older adults.
    - Arbitration: WM weight depends on the relative reliability of WM vs RL, approximated by 1 - normalized
      entropy of each policy distribution. A bias term shifts arbitration toward/away from WM.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_bias: bias added in logit space to favor WM (>0) or RL (<0)
    - softmax_beta: RL inverse temperature (scaled x10 internally)
    - wm_learn: WM learning rate toward one-hot on reward (0..1)
    - interference: base interference/decay toward uniform per trial (0..1)
    - age_interf_boost: multiplicative boost of interference for older adults (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_bias, softmax_beta, wm_learn, interference, age_interf_boost = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute interference strength for this block given age and set size
        interf_block = interference * (nS / 3.0) * (1.0 + age_group * age_interf_boost)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy for chosen action a
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # Full RL policy distribution to compute entropy
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= max(np.sum(rl_probs), 1e-12)

            # POLICY FOR THE WORKING MEMORY: softmax over WM weights (sharp)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= max(np.sum(wm_probs), 1e-12)

            # Reliability via normalized entropy (lower entropy => higher reliability)
            def norm_entropy(p):
                hp = -np.sum(p * np.log(np.clip(p, 1e-12, 1.0)))
                return hp / np.log(nA)

            rl_conf = 1.0 - norm_entropy(rl_probs)
            wm_conf = 1.0 - norm_entropy(wm_probs)

            # Convert confidences into WM weight with bias in logit space
            base_w = wm_conf / max((wm_conf + rl_conf), 1e-12)
            base_w = np.clip(base_w, 1e-6, 1.0 - 1e-6)
            logit = np.log(base_w) - np.log(1.0 - base_w)
            wm_weight = 1.0 / (1.0 + np.exp(-(logit + wm_bias)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # VALUE UPDATING FOR THE WORKING MEMORY
            # Reward-driven consolidation plus ongoing interference toward uniform
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * one_hot
            # Apply interference every trial
            w[s, :] = (1.0 - interf_block) * w[s, :] + interf_block * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-adaptive temperature + WM gate by running reward-rate; age/set-size modulate gate.

    Mechanisms:
    - RL: Q-learning; inverse temperature increases when state value uncertainty is low (larger Q-spread).
    - WM: stores last rewarded action; mild decay otherwise.
    - Arbitration: WM weight increases when recent reward rate is high relative to 0.5 (task learnability),
      but decreases with set size and for older adults (harder to rely on WM under load).
    - Age and set-size effects:
      - Gate hardness increases with set size and for older adults.
      - RL temperature boosted by an uncertainty term independent of age (meta-optimizing exploration).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_base: base gate bias (real; passed through sigmoid)
    - softmax_beta: base RL inverse temperature (scaled x10 internally)
    - rr_tau: time constant for running reward rate (>=1; larger => slower updates)
    - age_hard: strength of age-by-set-size penalty on WM gate (>=0)
    - unc_gain: gain on RL temperature by local uncertainty (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, rr_tau, age_hard, unc_gain = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # running reward rate (initialized at chance .33)
        rr = 1.0 / nA
        alpha_rr = 1.0 / max(rr_tau, 1.0)

        # Gate penalty from age and set size
        gate_penalty = age_hard * (nS / 3.0) * age_group

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Uncertainty of RL at state s via Q-spread; larger spread => higher confidence => higher beta
            q_spread = np.max(Q_s) - np.min(Q_s)
            beta_eff = softmax_beta * (1.0 + unc_gain * q_spread)

            # RL policy for chosen action a
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # POLICY FOR THE WORKING MEMORY: softmax over WM weights (sharp)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Gate by reward rate: higher rr => more WM
            # wm_base is transformed via sigmoid, then shifted by rr-0.5 and penalized by age*set size
            base_logit = wm_base + 2.0 * (rr - 0.5) - gate_penalty
            wm_weight = 1.0 / (1.0 + np.exp(-base_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # VALUE UPDATING FOR THE WORKING MEMORY
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.6 * w[s, :] + 0.4 * one_hot
            else:
                # mild decay; slightly stronger for larger set size
                decay = 0.08 + 0.04 * ((nS - 3) / 3.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Update running reward rate
            rr = (1.0 - alpha_rr) * rr + alpha_rr * r

        blocks_log_p += log_p

    return -blocks_log_p