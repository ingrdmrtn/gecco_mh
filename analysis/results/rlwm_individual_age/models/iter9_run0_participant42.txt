def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + noisy WM recall; arbitration weakens with load and age.

    Mechanisms
    ----------
    - RL: two learning rates (positive/negative PE) with softmax policy.
    - WM: one-shot storage after reward, softmax readout with high inverse temperature.
          Imperfect recall: with probability p_recall use WM, otherwise uniform.
    - Arbitration: wm_weight(t) = wm_base * exp(-k_load*(set_size-3)) * exp(-k_age*age_group).
      This dynamically mixes WM and RL policies per trial.

    Parameters
    ----------
    model_parameters : [lr_pos, lr_neg, wm_base, softmax_beta, k_load, k_age]
        - lr_pos: learning rate for positive prediction errors (0..1).
        - lr_neg: learning rate for negative prediction errors (0..1).
        - wm_base: baseline WM reliance (0..1) when set size is 3 and young.
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - k_load: load sensitivity reducing WM arbitration with set size.
        - k_age: additional WM arbitration reduction for older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_base, softmax_beta, k_load, k_age = model_parameters
    softmax_beta *= 10.0  # higher upper bound for RL temperature

    # Age group: 0=young, 1=old
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM readout (before recall noise)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM weights
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM table
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # baseline (uniform)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM deterministic policy (softmax with high beta)
            denom_wm_det = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(denom_wm_det, 1e-12)

            # Imperfect recall within WM: blend with uniform according to p_recall
            # Recall probability decreases with load and age
            load = float(nS - 3)
            p_recall = wm_base * np.exp(-k_load * load) * np.exp(-k_age * age_group)
            p_recall = float(np.clip(p_recall, 0.0, 1.0))
            p_wm = p_recall * p_wm_det + (1.0 - p_recall) * (1.0 / nA)

            # Arbitration weight: same modulation as recall (interpreted as WM availability)
            wm_weight = p_recall

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] = Q_s[a] + alpha * pe

            # WM update:
            # - Rewarded: store the association (one-hot)
            # - Unrewarded: gentle decay toward baseline (no overwriting)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # small decay toward uniform to reflect uncertainty after errors
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy-driven WM arbitration + age/load-dependent WM precision and decay.

    Mechanisms
    ----------
    - RL: single learning rate, softmax policy.
    - WM: fast storage after reward; continuous decay toward uniform each trial.
          WM readout precision (beta_wm) is reduced by set size and age.
    - Arbitration: base wm_weight scaled up when RL policy is uncertain (high entropy),
      but down with larger set sizes and older age.

    Parameters
    ----------
    model_parameters : [lr, wm_weight_base, softmax_beta, k_entropy, wm_decay, age_wm_noise]
        - lr: RL learning rate (0..1).
        - wm_weight_base: baseline WM arbitration weight at set size 3 for young.
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - k_entropy: sensitivity to RL choice entropy for increasing WM reliance.
        - wm_decay: WM decay rate toward uniform per trial (0..1).
        - age_wm_noise: increases WM noise for older adults (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, k_entropy, wm_decay, age_wm_noise = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm_nominal = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # RL entropy (in bits) for arbitration
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl)
            probs_rl /= np.sum(probs_rl)
            entropy_rl = -np.sum(probs_rl * np.log2(np.maximum(probs_rl, 1e-12)))

            # WM precision decreases with load and age
            load = float(nS - 3)
            wm_beta_scale = 1.0 + 0.5 * load + age_wm_noise * age_group
            softmax_beta_wm = softmax_beta_wm_nominal / wm_beta_scale

            # WM policy with current precision
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration:
            # - Start from base
            # - Increase with RL entropy
            # - Decrease with load and age
            wm_weight = wm_weight_base
            wm_weight += k_entropy * (entropy_rl / np.log2(nA))  # normalized 0..1
            wm_weight *= np.exp(-0.6 * load) * np.exp(-0.6 * age_group)
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Rewarded trials refresh the mapping for that state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value forgetting + capacity-based WM gating that declines with load and age.

    Mechanisms
    ----------
    - RL: single learning rate with softmax; Q-values leak toward uniform baseline each trial.
    - WM: slot-like storage after reward; global decay toward uniform.
    - Arbitration: WM weight is a sigmoid function of effective capacity
      cap_eff = cap_base - cap_load*(set_size-3) - cap_age*age_group.

    Parameters
    ----------
    model_parameters : [lr, softmax_beta, cap_base, cap_load, cap_age, q_forget]
        - lr: RL learning rate (0..1).
        - softmax_beta: RL inverse temperature (scaled internally by 10).
        - cap_base: baseline WM capacity proxy at set size 3 for young.
        - cap_load: capacity decrement per extra item beyond 3.
        - cap_age: additional capacity decrement for older adults.
        - q_forget: RL forgetting rate toward uniform baseline per trial (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, cap_base, cap_load, cap_age, q_forget = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Capacity-based arbitration
            load = float(nS - 3)
            cap_eff = cap_base - cap_load * load - cap_age * age_group
            # Map capacity proxy to [0,1] weight via sigmoid
            wm_weight = 1.0 / (1.0 + np.exp(-2.0 * (cap_eff - 1.5)))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with forgetting to uniform baseline
            pe = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * pe
            # Leak all Q toward uniform
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA)

            # WM update: store after reward; otherwise mild decay
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p