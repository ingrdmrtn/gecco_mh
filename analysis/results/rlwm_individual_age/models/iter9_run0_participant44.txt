def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and age/load-driven interference.

    Idea:
    - RL uses a standard delta rule with softmax choice.
    - WM stores a graded action distribution per state (delta-rule), and passively decays toward
      uniform due to interference that increases with set size and with age group.
    - Arbitration uses the WM state's entropy: lower entropy (sharper WM) yields more WM influence.
      A noise/interference term (growing with set size and age) increases effective entropy, thus
      down-weighting WM under high load and in older adults.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL (scaled by 10 internally)
    - wm_lr: WM learning rate for rewarded associations (0..1)
    - k_entropy: sensitivity of arbitration to WM entropy (>=0). Larger increases WM/RL differentiation.
    - size_noise_gain: additive increase in WM interference with set size load (>=0)
    - age_noise_gain: additional additive interference for older adults (>=0)

    Age/Set-size effects:
    - WM interference sigma = (1 + size_noise_gain*(nS-3)) + age_noise_gain*age_group
      increases with larger set size (6 vs 3) and for older adults, which reduces WM weight.
    """
    alpha, softmax_beta, wm_lr, k_entropy, size_noise_gain, age_noise_gain = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    total_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference level due to set size and age; larger => more decay toward uniform
        sigma = 1.0 + size_noise_gain * max(0, nS - 3) + age_noise_gain * age_group
        # Convert to per-trial decay rate in [0,1)
        decay = 1.0 - np.exp(-sigma)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration based on entropy of WM distribution (lower H => higher WM weight)
            W_safe = np.clip(W_s, eps, 1.0)
            H = -np.sum(W_safe * np.log(W_safe))  # in nats, max ~ log(3)
            # Effective entropy augmented by interference; higher => reduce WM weight
            H_eff = H + sigma
            wm_weight_eff = np.exp(-k_entropy * H_eff)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM decay toward uniform (interference)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM learning: reward-locked strengthening toward chosen action
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot
                # Renormalize to valid distribution
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        total_log_p += log_p

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM implementing a win-stay/lose-reset strategy with load- and age-sensitive gating.

    Idea:
    - RL uses a standard delta rule and softmax.
    - WM approximates a WSLS heuristic by storing the last rewarded action for each state;
      after rewards, WM concentrates mass on that action; after losses, WM partially resets
      toward uniform. This captures a common heuristic in aging.
    - The WM/RL mixture weight is a linear gate reduced by set size and by age group.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL (scaled by 10 internally)
    - wsls_strength: base WM influence (0..1); higher means stronger WM reliance
    - size_penalty: reduction of WM weight per additional items beyond 3 (>=0)
    - age_penalty: additional reduction of WM weight for older adults (>=0)
    - wm_decay: how strongly WM resets toward uniform after losses (0..1)

    Age/Set-size effects:
    - Effective WM weight: wsls_strength - size_penalty*(nS-3) - age_penalty*age_group,
      thus lower in 6-item blocks and for older adults.
    """
    alpha, softmax_beta, wsls_strength, size_penalty, age_penalty, wm_decay = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute a fixed gate for this block given its load and age
        gate = wsls_strength - size_penalty * max(0, nS - 3) - age_penalty * age_group
        gate = np.clip(gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / max(np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))), eps)

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / max(np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))), eps)

            p_total = gate * p_wm + (1.0 - gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # WM update: win-stay (concentrate on a); lose-reset (decay toward uniform)
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # strong concentration on the rewarded action
                w[s, :] = 0.0 * w[s, :] + onehot
            else:
                # partial reset toward uniform after a loss
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
                # also reduce the probability of the just-failed action a bit more
                reduce = 0.1 * wm_decay
                w[s, a] = max(w[s, a] - reduce, 0.0)
                # renormalize
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        total_log_p += log_p

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with volatility-adaptive temperature + WM gated by recent success streak, penalized by age/load.

    Idea:
    - RL updates Q-values with a delta rule. The inverse temperature adapts to recent outcome
      volatility (unsigned prediction error): higher volatility -> more exploration (lower beta).
    - WM stores rewarded bindings with a delta-rule; its mixture weight depends on the length of
      recent success streak for the current state. Longer streaks -> stronger WM.
    - Both WM gating and effective RL temperature are impacted by set size and age:
      older age and larger set sizes reduce WM gating; RL temperature is driven by volatility only.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - softmax_beta: base RL inverse temperature (scaled by 10 internally)
    - wm_lr: WM learning rate for rewarded bindings (0..1)
    - beta_vol_gain: how strongly volatility reduces RL beta (>=0)
    - wm_streak_gain: how strongly success streak increases WM weight (>=0)
    - age_size_penalty: penalty applied to WM gate per unit of load (nS-3) and per age group (>=0)

    Age/Set-size effects:
    - WM weight = sigmoid(wm_streak_gain * streak_s - age_size_penalty * ((nS-3) + age_group))
      so older adults and larger set sizes have lower WM influence for a given streak.
    """
    alpha, softmax_beta, wm_lr, beta_vol_gain, wm_streak_gain, age_size_penalty = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    eps = 1e-12

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Volatility tracker (unsigned PE), and success streak per state
        vol = 0.0
        vol_alpha = 0.1  # fixed smoothing for volatility
        streak = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with volatility-adaptive beta
            Q_s = q[s, :]
            beta_eff = softmax_beta * max(0.0, 1.0 - beta_vol_gain * vol)
            beta_eff = max(beta_eff, 1e-3)  # keep positive
            p_rl = 1.0 / max(np.sum(np.exp(beta_eff * (Q_s - Q_s[a]))), eps)

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / max(np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))), eps)

            # WM gate based on success streak, penalized by age and load
            load = max(0, nS - 3) + age_group
            gate_arg = wm_streak_gain * streak[s] - age_size_penalty * load
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_arg))  # sigmoid
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += alpha * pe

            # Update volatility (unsigned PE)
            vol = (1.0 - vol_alpha) * vol + vol_alpha * abs(pe)

            # Update streak and WM
            if r > 0:
                streak[s] += 1.0
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot
            else:
                streak[s] = 0.0
                # mild decay toward uniform on errors
                decay = 0.2
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Renormalize WM row
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] /= np.sum(w[s, :])

        total_log_p += log_p

    return -total_log_p