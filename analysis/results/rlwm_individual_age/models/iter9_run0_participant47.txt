Below are three standalone cognitive models that implement RL-WM hybrids for the rlwm task. Each function:

- Takes inputs: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Uses an RL system and a WM system, mixes their policies, and returns negative log-likelihood
- Uses age group (0=young, 1=old) and set size in a meaningful way
- Uses all parameters (≤ 6) and avoids previously tried parameter combinations
- Fills in the policy and update rules for the WM system as requested

Note: Assume numpy is already imported as np.


def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with softmax + capacity-limited, recall-based WM + age-dependent lapses.
    - WM acts as a one-shot associative store of rewarded actions per state.
    - The effective WM contribution is the probability of recall, which depends on
      set size (capacity-limited) and age (older adults recall less).
    - Includes an age-modulated lapse that mixes in uniform random choice.
    - WM decays toward uniform when capacity is exceeded (no extra parameter; derived from set size vs capacity).

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group (0=young, 1=old).
    model_parameters : list or array
        [alpha, beta_rl, wm_recall_base, wm_capacity_slots, age_lapse]
        - alpha: RL learning rate (0..1)
        - beta_rl: inverse temperature for RL (scaled internally by 10)
        - wm_recall_base: baseline WM recall probability (0..1)
        - wm_capacity_slots: WM slot capacity (e.g., around 3–4); limits recall as nS increases
        - age_lapse: baseline lapse probability mixed with uniform; amplified in older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, wm_recall_base, wm_capacity_slots, age_lapse = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM recall probability depends on capacity and age
        capacity_factor = min(1.0, float(wm_capacity_slots) / max(1.0, float(nS)))
        age_factor = 0.8 if age_group == 1 else 1.0  # older adults recall less
        wm_recall_prob = np.clip(wm_recall_base * capacity_factor * age_factor, 0.0, 1.0)

        # Age-modulated lapse
        lapse = np.clip(age_lapse * (1.0 + 0.5 * age_group), 0.0, 1.0)

        # Capacity-driven WM decay toward uniform if overloaded (no extra parameter)
        overload = max(0.0, float(nS) - float(wm_capacity_slots))
        phi_overload = np.clip(overload / max(1.0, float(nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy (deterministic if a rewarded association is stored)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mixture with recall probability and lapse
            p_mix = wm_recall_prob * p_wm + (1.0 - wm_recall_prob) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay due to overload
            w[s, :] = (1.0 - phi_overload) * w[s, :] + phi_overload * w_0[s, :]

            # WM update: store rewarded action as one-hot, else leave as is
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value decay + Hebbian WM with interference.
    - RL values decay toward uniform; decay increases with set size and more in older adults.
    - WM uses a graded associative matrix updated Hebbian-style when rewarded, with negative update on non-reward.
    - WM suffers interference (decay toward uniform) that scales with set size and age.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group.
    model_parameters : list or array
        [alpha, beta_rl, q_decay_base, wm_bind_prob, interference_rate]
        - alpha: RL learning rate (0..1)
        - beta_rl: RL inverse temperature (scaled internally by 10)
        - q_decay_base: baseline RL forgetting rate toward uniform per trial (0..1)
        - wm_bind_prob: baseline WM binding strength on reward (0..1)
        - interference_rate: WM interference/decay toward uniform per trial (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, q_decay_base, wm_bind_prob, interference_rate = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL decay increases with set size and is higher in older adults
        q_decay = np.clip(q_decay_base * (float(nS) / 6.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        # WM interference increases with set size and age
        gamma_int = np.clip(interference_rate * (float(nS) / 6.0) * (1.0 + 0.5 * age_group), 0.0, 1.0)

        # Reward-dependent WM binding probability, reduced by set size and age
        bind_strength = np.clip(wm_bind_prob * (3.0 / float(nS)) * (1.0 - 0.2 * age_group), 0.0, 1.0)
        # Non-reward suppression is weaker than binding
        suppress_strength = 0.5 * bind_strength

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM softmax
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Mix using a simple average, with more confidence in WM when it is sharp.
            # Here, use the norm of W_s deviation from uniform as a proxy for WM confidence.
            wm_conf = np.clip(0.5 + 0.5 * (np.max(W_s) - 1.0 / nA) / (1.0 - 1.0 / nA + 1e-12), 0.0, 1.0)
            p_total = wm_conf * p_wm + (1.0 - wm_conf) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += alpha * delta
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA) * np.ones_like(q)

            # WM interference toward uniform
            w = (1.0 - gamma_int) * w + gamma_int * w_0

            # WM Hebbian update
            if r > 0.5:
                # Move row s toward one-hot for action a
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - bind_strength) * w[s, :] + bind_strength * target
            else:
                # Suppress the chosen action slightly when incorrect
                w[s, a] = (1.0 - suppress_strength) * w[s, a]
                # Renormalize row softly by adding back to uniform
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces + conflict-driven arbitration with WM.
    - RL uses eligibility traces (state-action credit assignment persists across trials).
    - WM stores the last rewarded action per state (one-shot) and decays implicitly with set size.
    - Arbitration: when RL is uncertain (small value gap) rely more on WM; effect amplified in older adults.
    - Includes a separate temperature for WM policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action per trial.
    rewards : array-like of {0,1}
        Binary feedback.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size of the block for each trial.
    age : array-like of float
        Age of participant (single repeated value). Used to derive age group.
    model_parameters : list or array
        [alpha, beta_rl, lambda_et, beta_wm, wm_weight_base, age_conf_sensitivity]
        - alpha: RL learning rate (0..1)
        - beta_rl: inverse temperature for RL (scaled internally by 10)
        - lambda_et: eligibility-trace decay (0..1)
        - beta_wm: WM inverse temperature (scaled internally by 10)
        - wm_weight_base: baseline WM arbitration weight
        - age_conf_sensitivity: scales how strongly age amplifies WM reliance under conflict

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_rl, lambda_et, beta_wm, wm_weight_base, age_conf_sensitivity = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = beta_wm * 10.0

    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = blocks == b
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility trace
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay derived from set size (larger sets => more decay per visit)
        wm_forget = np.clip((float(nS) - 3.0) / 6.0, 0.0, 1.0)  # 0 for nS=3, ~0.5 for nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1e-12

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1e-12

            # Conflict-driven arbitration based on RL value gap
            sorted_Q = np.sort(Q_s)[::-1]
            gap = sorted_Q[0] - sorted_Q[1] if len(sorted_Q) > 1 else 0.0
            # Map gap to [0,1] conflict weight (larger gap => more confident RL => less WM)
            conf = 1.0 / (1.0 + np.exp(10.0 * gap - 2.0))  # sigmoid; gap≈0 => ~0.88; gap large => ~0
            size_scale = 3.0 / float(nS)  # more WM for smaller sets
            age_amp = 1.0 + age_conf_sensitivity * age_group
            wm_weight = np.clip(wm_weight_base * size_scale * age_amp * conf, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL eligibility-trace update
            # Decay all traces
            e *= lambda_et
            # Set current state-action eligibility to 1
            e[s, a] = 1.0
            delta = r - Q_s[a]
            q += alpha * delta * e  # update all state-actions proportionally to eligibility

            # WM decay toward uniform (set-size dependent)
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM update: store rewarded action one-shot
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p