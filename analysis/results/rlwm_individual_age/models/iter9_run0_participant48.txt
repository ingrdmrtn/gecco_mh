def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with error-driven WM encoding and nonlinear PE; age reduces WM contribution.

    Core ideas
    - RL system updates Q-values with a nonlinear, signed prediction error (|PE|^pe_exponent * sign(PE)).
    - WM system stores rewarded state-action associations with a graded, error-driven encoding strength.
    - Mixture between WM and RL is reduced by load (nS=6 vs 3) and further reduced by age (older group).

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..nS-1 within each block).
    actions : array-like, int
        Chosen action per trial (0..2).
    rewards : array-like, int
        Binary feedback (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size for each block (3 or 6), repeated within block.
    age : array-like, int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, wm_update_prob_base, pe_exponent, age_wm_penalty]
        - lr: RL learning rate (0..1)
        - wm_weight_base: baseline WM mixture weight at set size 3
        - softmax_beta: RL inverse temperature (scaled up internally)
        - wm_update_prob_base: base WM encoding strength at set size 3 (0..1)
        - pe_exponent: exponent for nonlinear prediction error (>=1 increases emphasis on big errors)
        - age_wm_penalty: reduction in WM weight for older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_update_prob_base, pe_exponent, age_wm_penalty = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM values (action-probability-like)
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline WM (uniform)

        # Load and age adjustments:
        load_scale = 3.0 / nS  # smaller at higher load
        wm_weight_eff = np.clip(wm_weight * load_scale - age_wm_penalty * age_group, 0.0, 1.0)
        wm_update_eff = np.clip(wm_update_prob_base * load_scale - 0.5 * age_wm_penalty * age_group, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with nonlinear PE
            pe = r - q[s, a]
            pe_nl = np.sign(pe) * (np.abs(pe) ** pe_exponent)
            q[s, a] += lr * pe_nl

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # First, mild relaxation toward uniform baseline (prevents stale traces)
            w[s, :] = (1.0 - wm_update_eff) * w[s, :] + wm_update_eff * w_0[s, :]
            # Error-driven strengthening for rewarded responses (graded one-hot push)
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_update_eff) * w[s, :] + wm_update_eff * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM + RL with lapse; age reduces effective capacity and increases lapses under load.

    Core ideas
    - WM contribution is determined by a capacity K relative to set size (wm_weight ~ min(1, K/nS)).
    - Older adults have a reduced effective K (by 1 slot), operationalizing age-related WM decline.
    - RL updates standard delta-rule; WM decays toward uniform and is sharpened on reward.
    - Choice has a load- and age-sensitive lapse component that randomizes responding.

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..nS-1 within each block).
    actions : array-like, int
        Chosen action per trial (0..2).
    rewards : array-like, int
        Binary feedback (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size for each block (3 or 6), repeated within block.
    age : array-like, int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, wm_weight_base, softmax_beta, capacity_K, wm_decay, lapse_base]
        - lr: RL learning rate (0..1)
        - wm_weight_base: scales the capacity-derived WM contribution
        - softmax_beta: RL inverse temperature (scaled up internally)
        - capacity_K: WM slots available (0..6)
        - wm_decay: WM decay per state visit toward uniform (0..1)
        - lapse_base: base lapse probability at set size 3

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_K, wm_decay, lapse_base = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity reduced for older adults by 1 slot (bounded at 0)
        K_eff = max(0.0, capacity_K - 1.0 * age_group)
        wm_capacity_weight = min(1.0, K_eff / max(1, nS))
        wm_weight_eff = np.clip(wm_weight * wm_capacity_weight, 0.0, 1.0)

        # Lapse increases with load and age
        lapse = np.clip(lapse_base * (nS / 3.0) * (1.0 + 0.5 * age_group), 0.0, 0.99)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-triggered sharpening toward the chosen action
            if r == 1:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
                # renormalize to sum to 1 for stability (optional but safe)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Confidence-based arbitration between RL and WM; age biases arbitration away from WM.

    Core ideas
    - Compute each system's choice confidence (max action probability).
    - Mixture weight is a sigmoid of the confidence difference (WM minus RL), plus bias terms.
    - Age shifts arbitration bias away from WM, and higher load weakens WM via stronger decay.
    - RL uses standard delta-rule; WM decays to uniform and is set to one-hot after rewards.

    Parameters
    ----------
    states : array-like, int
        State index per trial (0..nS-1 within each block).
    actions : array-like, int
        Chosen action per trial (0..2).
    rewards : array-like, int
        Binary feedback (0 or 1).
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size for each block (3 or 6), repeated within block.
    age : array-like, int
        Participant age (same value repeated). Age group is 0 if <=45, else 1.
    model_parameters : list or array
        [lr, wm_weight_bias, softmax_beta, wm_decay, conf_slope, age_conf_bias]
        - lr: RL learning rate (0..1)
        - wm_weight_bias: baseline bias toward WM in arbitration (logit units)
        - softmax_beta: RL inverse temperature (scaled up internally)
        - wm_decay: WM decay toward uniform (0..1 per visit)
        - conf_slope: sensitivity of arbitration to confidence difference
        - age_conf_bias: additional logit bias against WM for older adults

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, conf_slope, age_conf_bias = model_parameters
    softmax_beta *= 10.0  # higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50.0  # deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay amplification: higher load -> effectively more decay
        decay_eff = 1.0 - (1.0 - wm_decay) ** (nS / 3.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy and confidence
            Q_s = q[s, :]
            exp_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = exp_rl / np.sum(exp_rl)
            p_rl = pi_rl[a]
            conf_rl = np.max(pi_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            exp_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pi_wm = exp_wm / np.sum(exp_wm)
            p_wm = pi_wm[a]
            conf_wm = np.max(pi_wm)

            # Confidence-based arbitration (logistic)
            logit = wm_weight + conf_slope * (conf_wm - conf_rl) - age_conf_bias * age_group - np.log(nS / 3.0)
            wm_mix = 1.0 / (1.0 + np.exp(-logit))
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform with load-amplified decay
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]
            # Reward-driven one-hot update
            if r == 1:
                w[s, :] = (1.0 - decay_eff) * w[s, :]
                w[s, a] += decay_eff
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p