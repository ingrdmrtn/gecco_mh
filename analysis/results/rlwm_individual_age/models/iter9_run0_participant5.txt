Here are three standalone cognitive models that follow the RL+WM template, incorporate age and set-size effects meaningfully, and return the negative log-likelihood of the observed choices. Each model uses no more than 6 parameters and explores distinct mechanisms relative to the parameterizations youâ€™ve already tried.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 1: Entropy-weighted arbitration between RL and a decaying WM trace.

    Description:
    - RL: Q-learning with softmax policy.
    - WM: Decaying memory trace per state that drifts toward uniform; policy via near-deterministic softmax.
    - Arbitration: Weight given to WM increases as RL policy becomes more uncertain (high entropy) and as WM
      becomes more certain (low entropy). The WM weight decreases with set size and with older age.
    - Set-size effect: WM contribution is down-weighted for larger sets via a multiplicative factor 3/nS.
    - Age effect: Older group has a larger WM down-weight (age_drop).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - beta_rl: RL inverse temperature (scaled by 10 internally)
    - wm_base: Base WM mixture weight (0..1), modulated by entropies, set-size, and age
    - wm_decay: Per-trial WM decay toward uniform (0..1)
    - entropy_temp: Temperature controlling sharpness of entropy-based arbitration (>0)
    - age_drop: Proportional drop in WM weight for the older group (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_rl, wm_base, wm_decay, entropy_temp, age_drop = model_parameters
    beta_rl *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # near-deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute entropies (base-e)
            def softmax(x, beta):
                z = x - np.max(x)
                p = np.exp(beta * z)
                p /= np.sum(p)
                return p

            p_rl_vec = softmax(Q_s, beta_rl)
            p_wm_vec = softmax(W_s, softmax_beta_wm)

            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, 1e-12, 1.0)))
            H_wm = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, 1e-12, 1.0)))

            # Entropy-based arbitration: prefer WM when RL is uncertain and WM is certain
            # Map entropies to a [0,1] preference via a logistic on (H_rl - H_wm)
            diff = H_rl - H_wm
            arb = 1.0 / (1.0 + np.exp(-entropy_temp * diff))

            # Set-size and age scaling of WM weight
            ss_scale = 3.0 / float(nS)  # 1.0 at set size 3, 0.5 at set size 6
            wm_weight_eff = wm_base * arb * ss_scale * (1.0 - age_drop * age_group)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-driven sharpening of WM for chosen action (toward one-hot)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Use the same lr to sharpen WM on rewarded trials (couples memory to learning pace)
                w[s, :] = (1.0 - lr) * w[s, :] + lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 2: Capacity-limited WM with interference and confidence-weighted arbitration.

    Description:
    - RL: Q-learning with softmax policy.
    - WM: Capacity-limited store; each state holds a mapping vector that is sharpened by reward and
      subject to interference and decay. A "confidence" signal is the max entry of W_s.
    - Arbitration: WM weight scales with confidence and the probability that the state is within
      capacity (K_eff / nS). Capacity is reduced for older group.
    - Set-size effect: Probability of being represented in WM is K_eff / nS (capped at 1).
    - Age effect: Reduced effective capacity (age_capacity_shift).

    Parameters (list):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_capacity: Base WM capacity in number of items (>=0)
    - wm_confidence: Scales how strongly confidence maps to arbitration weight (0..1)
    - interference_rate: Per-trial WM interference toward uniform (0..1)
    - age_capacity_shift: Capacity reduction for older group (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_capacity, wm_confidence, interference_rate, age_capacity_shift = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity accounting for age
        K_eff = max(0.0, wm_capacity - age_capacity_shift * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence from WM for this state (0..1)
            conf = float(np.max(W_s))

            # Probability that state is represented in capacity-limited WM
            p_in_wm = min(1.0, K_eff / float(nS)) if nS > 0 else 0.0

            # Arbitration: WM weight is confidence-weighted and capacity-weighted
            wm_weight_eff = wm_confidence * conf * p_in_wm
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM interference/decay toward uniform for all states each time the state appears
            w[s, :] = (1.0 - interference_rate) * w[s, :] + interference_rate * w_0[s, :]

            # Reward-driven sharpening of WM for current state-action
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Use confidence as a gate to how much WM sharpens (self-consistency)
                sharpen = min(1.0, 0.5 + 0.5 * conf)
                w[s, :] = (1.0 - sharpen) * w[s, :] + sharpen * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Cognitive Model 3: Surprise-gated arbitration with WM fast-learning and choice stickiness.

    Description:
    - RL: Q-learning with softmax policy.
    - WM: Fast-learning one-hot-like trace that decays toward uniform; policy via near-deterministic softmax.
    - Arbitration: WM weight increases when unsigned reward prediction error (|PE|) is small (i.e., low surprise),
      decreases with set size, and is reduced in the older group.
    - Choice stickiness: Adds a choice kernel bias toward repeating the previous action; older group can have
      stronger stickiness via an age gain.
    - Set-size effect: WM contribution is scaled by 3/nS.
    - Age effect: WM weight reduced and stickiness increased for the older group.

    Parameters (list):
    - lr: RL learning rate (0..1)
    - beta: Base inverse temperature for RL (scaled by 10 internally) used for softmax over Q
    - wm_lr: WM learning rate toward one-hot (0..1)
    - stickiness: Choice stickiness weight added to the chosen action preference (>=0)
    - age_stickiness_gain: Multiplicative gain to stickiness for the older group (>=0)
    - ss_wm_slope: Exponent controlling set-size penalty on WM weight (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta, wm_lr, stickiness, age_stickiness_gain, ss_wm_slope = model_parameters
    beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Choice kernel for stickiness (per state)
        k = np.zeros((nS, nA))

        log_p = 0.0
        prev_a_by_state = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add stickiness bias to RL preferences for previous action in the same state
            eff_stickiness = stickiness * (1.0 + age_stickiness_gain * age_group)
            if prev_a_by_state[s] >= 0:
                k_s = np.zeros(nA)
                k_s[prev_a_by_state[s]] = eff_stickiness
                Q_s = Q_s + k_s  # bias RL logits

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise (unsigned PE) from un-biased Q-values (without stickiness)
            pe = abs(r - q[s, a])

            # Map surprise to WM weight: higher WM reliance when surprise is low
            wm_weight_base = 1.0 / (1.0 + pe)  # in (0.5..1] if q[s,a] in [0,1]
            # Set-size scaling via power-law
            ss_scale = (3.0 / float(nS)) ** max(0.0, ss_wm_slope)
            # Age scaling: older group has reduced WM reliance
            wm_weight_eff = wm_weight_base * ss_scale * (1.0 - 0.3 * age_group)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (without stickiness in the learning rule)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = 0.2 * w_0[s, :] + 0.8 * w[s, :]

            # WM fast learning toward one-hot for any feedback (rewarded or not)
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot if r > 0.0 else w[s, :]

            # Update previous action for stickiness
            prev_a_by_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p

Notes on age and set-size effects:
- Model 1: WM weight increases when RL is uncertain and WM is confident, but is scaled down by 3/nS and reduced for older participants by age_drop.
- Model 2: WM contribution depends on effective capacity K_eff relative to set size (K_eff/nS); K_eff is reduced for older participants by age_capacity_shift.
- Model 3: WM weight is higher when surprise (|PE|) is small, scaled by (3/nS)^ss_wm_slope, and reduced in the older group. Older participants also show stronger choice stickiness via age_stickiness_gain.