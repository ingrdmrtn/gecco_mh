def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with age- and set-size–dependent WM reliability and decay interference.

    Idea
    - Choices are a mixture of an RL policy and a WM policy.
    - WM reliability (mixture weight) is penalized by both age and set size.
    - WM contents decay each trial, with stronger decay when set size is larger.
    - RL uses a single learning rate; action selection via softmax.
    - WM encodes rewarded stimulus–action bindings as a sharp one-hot; on non-reward, it weakens that binding.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like
        Participant age (constant array); age_group = 1 if > 45 else 0.
    model_parameters : list or array
        [lr, beta_base, wm_reliab0, wm_decay0, age_wm_penalty, ss_wm_penalty]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_reliab0: baseline WM mixture weight (0..1).
        - wm_decay0: baseline WM decay per trial (0..1), higher = faster forgetting.
        - age_wm_penalty: additive penalty to WM reliability if older (>=0).
        - ss_wm_penalty: penalty per “extra item proportion” beyond 3; applied as wm_reliab -= ss_wm_penalty*(nS-3)/3.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_reliab0, wm_decay0, age_wm_penalty, ss_wm_penalty = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0  # near-deterministic WM

        # WM weight penalized by age and set size
        wm_weight = wm_reliab0
        wm_weight -= age_wm_penalty * age_group
        wm_weight -= ss_wm_penalty * max(0.0, (nS - 3) / 3.0)
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        # WM decay increases with set size (more interference)
        wm_decay = wm_decay0 * (nS / 3.0)
        wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL softmax probability of chosen action a; matches template form
            p_rl = 1.0 / (np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))) + eps)

            # WORKING MEMORY policy: softmax over WM weights (nearly deterministic)
            W_s = w[s, :]
            p_wm = 1.0 / (np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))) + eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WORKING MEMORY updating
            # Decay/forgetting toward uniform (w_0) for all states each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Then apply state-specific update:
            if r > 0.5:
                # Strengthen the chosen association to one-hot
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On error, mildly suppress the chosen action probability within the state
                # while renormalizing to stay a distribution.
                suppress = 0.2
                w[s, a] = max(w[s, a] - suppress, 0.0)
                rem = np.sum(w[s, :])
                if rem <= eps:
                    w[s, :] = w_0[s, :]
                else:
                    w[s, :] /= rem

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning + WM with error-tagging and age-dependent exploration.

    Idea
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM encodes correct bindings on reward; on non-reward, it tags the chosen action as incorrect
      (shifting WM mass to the other actions) to accelerate elimination.
    - Mixture weight scales with 3/nS (capacity dilution) and a baseline; age increases exploration
      by reducing RL inverse temperature.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age (constant array); age_group = 1 if > 45 else 0.
    model_parameters : list or array
        [lr_pos, lr_neg, beta_base, wm_base, wm_err_tag, age_explore_penalty]
        - lr_pos: RL learning rate for positive PE (0..1).
        - lr_neg: RL learning rate for negative PE (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_base: baseline WM mixture weight (0..1), scaled by (3/nS).
        - wm_err_tag: amount of WM mass moved away from chosen action on error (0..1).
        - age_explore_penalty: reduces inverse temperature if older: beta *= (1 - age_explore_penalty).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, lr_neg, beta_base, wm_base, wm_err_tag, age_explore_penalty = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL temperature with age-dependent exploration boost (older -> lower beta)
        softmax_beta = beta_base * 10.0
        if age_group == 1:
            softmax_beta *= max(0.0, 1.0 - age_explore_penalty)

        softmax_beta_wm = 50.0  # near-deterministic WM

        # WM mixture weight capacity dilution
        wm_weight = wm_base * (3.0 / max(1.0, nS))
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy probability of chosen action a
            p_rl = 1.0 / (np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))) + eps)

            # WM policy probability of chosen action a
            W_s = w[s, :]
            p_wm = 1.0 / (np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))) + eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            if pe >= 0.0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update:
            if r > 0.5:
                # Encode the correct mapping deterministically
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Tag the chosen action as incorrect by moving mass to the other actions
                move = min(wm_err_tag, w[s, a])
                w[s, a] -= move
                others = [x for x in range(nA) if x != a]
                w[s, others] += move / (nA - 1)
                # ensure normalization
                total = np.sum(w[s, :])
                if total <= eps:
                    w[s, :] = w_0[s, :]
                else:
                    w[s, :] /= total

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated arbitration: WM engaged more when RL is uncertain, modulated by age.

    Idea
    - Arbitration weight is dynamic per trial: a saturating function of RL policy entropy.
      Higher RL uncertainty -> greater WM weight, but only if set size is small.
    - Age reduces gate sensitivity, leading to weaker WM engagement for older adults.
    - WM traces leak toward uniform each trial (leak parameter).
    - RL uses single learning rate and softmax.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like
        Participant age (constant array); age_group = 1 if > 45 else 0.
    model_parameters : list or array
        [lr, beta_base, wm_max, gate_k, age_gate_shift, wm_leak]
        - lr: RL learning rate (0..1).
        - beta_base: base inverse temperature for RL; internally scaled (*10).
        - wm_max: maximum WM mixture weight cap (0..1).
        - gate_k: sensitivity of WM gate to RL entropy (>=0).
        - age_gate_shift: subtractive shift to gate input if older (>=0).
        - wm_leak: per-trial leak of WM toward uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, beta_base, wm_max, gate_k, age_gate_shift, wm_leak = model_parameters
    eps = 1e-12
    age_group = 1 if age[0] > 45 else 0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        softmax_beta = beta_base * 10.0
        softmax_beta_wm = 50.0  # near-deterministic WM

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Helper: maximum entropy for nA actions
        H_max = np.log(nA)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL softmax distribution
            denom = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s)))) + eps
            pi_rl_vec = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / denom
            p_rl = float(pi_rl_vec[a])

            # RL uncertainty via normalized entropy
            H = -np.sum(pi_rl_vec * (np.log(pi_rl_vec + eps)))
            H_norm = float(np.clip(H / H_max, 0.0, 1.0))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / (np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a]))) + eps)

            # Gate: WM weight increases with uncertainty when set size is small
            # Base drive = gate_k * H_norm * (3/nS)
            gate_drive = gate_k * H_norm * (3.0 / max(1.0, nS))
            # Age reduces effective gate drive
            gate_drive = gate_drive - age_gate_shift * age_group
            wm_weight = float(np.clip(wm_max * (1.0 / (1.0 + np.exp(-gate_drive)) ), 0.0, wm_max))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leak toward uniform, then store rewarded one-hot
            w = (1.0 - wm_leak) * w + wm_leak * w_0
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # No explicit negative tagging; rely on leak and arbitration
                pass

        blocks_log_p += log_p

    return -float(blocks_log_p)