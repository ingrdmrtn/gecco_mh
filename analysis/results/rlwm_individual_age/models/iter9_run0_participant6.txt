def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM with capacity-limited WM precision and load/age-dependent lapses.

    Mechanism:
    - RL: delta rule with softmax (template).
    - WM: softmax over WM values with precision that scales with effective capacity K_eff = 3 * wm_precision_base / set_size.
           WM forgetting toward uniform also increases with set size and age.
    - Arbitration: mixture between RL and WM with baseline wm_weight; final decision incorporates a lapse to uniform
                   that grows with set size and age.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_precision_base: baseline WM precision factor at set size 3 (>=0).
    - lapse_base: baseline lapse probability at set size 3 (0..1), also used as WM forgetting strength base.
    - age_lapse_increase: multiplicative increase of lapse/forgetting for older group (>=0); applied as (1 + age_lapse_increase*age_group).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_precision_base, lapse_base, age_lapse_increase = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic when precision is high
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-/age-dependent lapse and WM forgetting scaling
        load_factor = nS / 3.0
        age_mult = (1.0 + age_lapse_increase * age_group)
        lapse = np.clip(lapse_base * load_factor * age_mult, 0.0, 0.5)  # cap lapses
        wm_forget = np.clip(lapse_base * load_factor * age_mult, 0.0, 0.9)

        # Effective WM precision scales inversely with load
        wm_precision = wm_precision_base * (3.0 / nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for the observed action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with precision that falls with load
            logits_wm = (softmax_beta_wm * wm_precision) * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = float(np.clip(pi_wm[a], eps, 1.0))

            # Mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Global lapse to uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-driven write with forgetting toward uniform
            if r > 0.5:
                # Write stronger when reward is present
                w[s, :] *= (1.0 - wm_precision)  # precision also as write strength proxy
                w[s, a] += wm_precision
            # Normalize and apply forgetting
            w[s, :] = np.clip(w[s, :], 0, None)
            w[s, :] /= np.sum(w[s, :])
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM with WM interference from competing items and age-modulated WM weight.

    Mechanism:
    - RL: delta rule with softmax (template).
    - WM: near-deterministic policy, but WM representations are corrupted each trial by
          load-dependent interference that redistributes probability mass to non-target actions.
    - Arbitration: baseline wm_weight down-weighted by load and by an age factor.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay_base: baseline WM decay toward uniform at set size 3 (0..1).
    - interference_rate: mass per trial shifted from current WM distribution to the two non-chosen actions (0..1).
    - age_wm_scaler: multiplicative factor applied to wm_weight for older group (<=1 typically, but free).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, interference_rate, age_wm_scaler = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load- and age-dependent WM parameters
        load_factor = nS / 3.0
        wm_decay = np.clip(wm_decay_base * load_factor, 0.0, 0.95)
        wm_interf = np.clip(interference_rate * (load_factor - 1.0 + 1e-9), 0.0, 0.9)  # zero at set size 3, grows with load
        wm_weight_eff = wm_weight * (3.0 / nS) * (1.0 - (1.0 - age_wm_scaler) * age_group)
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic readout from W with high beta
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = float(np.clip(pi_wm[a], eps, 1.0))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-driven one-hot write, followed by interference and decay
            if r > 0.5:
                # Write: push probability toward chosen action
                w[s, :] *= 0.0
                w[s, a] = 1.0
            # Interference: leak some mass to the two unchosen actions (only meaningful under load)
            if wm_interf > 0.0:
                leak = wm_interf * w[s, :]
                w[s, :] -= leak
                # Evenly distribute leaked mass to other actions
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += np.sum(leak) / (nA - 1)
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Normalize
            w[s, :] = np.clip(w[s, :], 0, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLWM with entropy-gated arbitration and error-triggered WM clearing.

    Mechanism:
    - RL: delta rule with softmax (template).
    - WM policy: deterministic readout; WM is cleared on negative feedback to reduce perseveration.
                 A mild load-dependent forgetting is applied each trial.
    - Arbitration: WM weight is dynamically adjusted by RL policy entropy. High RL entropy increases reliance on WM.
                   Age reduces the sensitivity of the gate to entropy; load also reduces the effective WM contribution.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - gate_gain: sensitivity of WM weight to RL entropy (>=0).
    - entropy_temp: temperature used to compute RL policy for entropy estimation (>=0).
    - age_gate_penalty: factor in [0,1] reducing gate_gain for older group (effective gain *= (1 - age_gate_penalty*age_group)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_gain, entropy_temp, age_gate_penalty = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    softmax_beta_wm = 50  # very deterministic for WM
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay per trial
        wm_forget = np.clip(0.1 * (nS / 3.0), 0.0, 0.5)

        # Gate parameters with age penalty
        eff_gain = gate_gain * (1.0 - age_gate_penalty * age_group)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for observed action (template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL policy for entropy calculation (use entropy_temp as inverse temperature)
            beta_ent = max(entropy_temp, 0.0)
            logits_rl = beta_ent * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0))) / np.log(nA)  # normalized to [0,1]

            # WM policy: deterministic readout
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = float(np.clip(pi_wm[a], eps, 1.0))

            # Entropy-gated WM weight, reduced by load
            gate = np.clip(wm_weight + eff_gain * (entropy - 0.5), 0.0, 1.0)
            wm_weight_eff = gate * (3.0 / nS)
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            if r > 0.5:
                # On reward: encode chosen action strongly
                w[s, :] *= 0.0
                w[s, a] = 1.0
            else:
                # On error: clear WM for that state to avoid perseveration
                w[s, :] = w_0[s, :]

            # Apply mild forgetting toward uniform each trial (load-dependent)
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            w[s, :] = np.clip(w[s, :], 0, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p