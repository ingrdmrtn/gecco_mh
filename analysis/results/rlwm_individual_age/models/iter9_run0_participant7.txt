def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with load- and age-modulated WM weight and decay.

    Key ideas:
    - RL learns Q-values with softmax choice.
    - WM stores recent correct associations and acts via a near-deterministic softmax.
    - Arbitration is a fixed-base wm_weight that is adjusted by set size and age.
      Larger set sizes reduce WM contribution; young age boosts it.
    - WM traces decay toward uniform at a rate that increases with set size and age.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: base arbitration weight on WM (0..1), converted to logit internally
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_decay_load: sensitivity of WM decay to load (>=0)
    - age_wm_bonus: additional WM bias for young relative to old (can be negative or positive)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay_load, age_wm_bonus = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound

    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax policy
            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            # Effective wm_weight adjusted by load and age (young bonus; load penalty)
            # Convert base weight to logit, apply adjustments, then back to probability.
            wm_weight = np.clip(wm_weight, 1e-6, 1 - 1e-6)
            logit_base = np.log(wm_weight) - np.log(1 - wm_weight)
            load_penalty = -wm_decay_load * ((nS - 3) / 3.0)
            age_bonus = (1 - age_group) * age_wm_bonus  # only applies to young
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(logit_base + load_penalty + age_bonus)))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # On rewarded trials, pull WM toward one-hot for chosen action; otherwise mild attraction to uniform.
            target = np.ones(nA) / nA
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            # Learning rate fixed here via decay-driven attraction strength
            # First, attraction toward target
            eta_learn = 0.5  # implicit rate for storing; bounded via normalization
            w[s, :] += eta_learn * (target - W_s)

            # Then, decay to uniform modulated by load and age
            decay_rate = 1.0 - np.exp(-wm_decay_load * ((nS / 3.0) + 0.5 * age_group))
            w[s, :] = (1 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]
            # Renormalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with load-driven WM interference and age-modulated RL temperature.

    Key ideas:
    - RL learns Q-values; RL temperature is reduced by age and load (more noise with age/load).
    - WM produces a near-deterministic policy but is corrupted by interference that increases with load and age.
    - Arbitration uses fixed wm_weight.
    - WM updates toward one-hot on reward, and away from the chosen action on non-reward.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: arbitration weight on WM (0..1)
    - softmax_beta: RL inverse temperature base (scaled by 10 internally), then divided by an age-load factor
    - alpha_wm: WM learning rate toward target (0..1)
    - interference: WM interference gain with load (>=0)
    - age_temp_shift: additional RL temperature penalty for older adults (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_wm, interference, age_temp_shift = model_parameters
    softmax_beta *= 10.0  # base scaling

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Age- and load-modulated RL temperature (more noise with age and load)
        rl_temp_penalty = 1.0 + (age_temp_shift * age_group) + (interference * max(0, nS - 3) / 3.0)
        softmax_beta_eff = softmax_beta / rl_temp_penalty

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with effective beta
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta_eff * Qc))
            p_rl = np.exp(softmax_beta_eff * Qc[a]) / max(1e-12, denom_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic WM policy corrupted by interference-based lapse
            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm_det = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            # Interference increases with set size; age adds extra noise
            xi = 1.0 - np.exp(-interference * (1.0 + max(0, nS - 3))) + 0.1 * age_group
            xi = float(np.clip(xi, 0.0, 0.9))  # cap lapse
            p_wm = (1.0 - xi) * p_wm_det + xi * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward: attract to one-hot; No reward: push mass away from chosen action toward others
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] += alpha_wm * (target - W_s)
            else:
                # Move a fraction alpha_wm of chosen action's weight to others uniformly
                move = alpha_wm * W_s[a]
                w[s, a] = W_s[a] - move
                w[s, :] += (move / (nA - 1)) * (np.ones(nA) - np.eye(1, nA, a).flatten())

            # Small pull to uniform baseline prevents degeneracy
            w[s, :] = 0.98 * w[s, :] + 0.02 * w_0[s, :]
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with confidence- and surprise-based dynamic arbitration.

    Key ideas:
    - WM policy is near-deterministic softmax over WM weights.
    - Arbitration weight wm_weight is dynamically adjusted each trial based on:
        - WM confidence (margin between top two WM actions; higher -> more WM)
        - RL surprise (|R - Q|; higher -> more WM updating and more WM reliance if confidence is decent)
        - Set size and age reduce the impact of WM confidence on arbitration
    - WM learning rate is modulated by RL surprise (greater update when RL is surprised).

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: base arbitration bias toward WM (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - alpha_wm: base WM learning rate (0..1)
    - meta_arbitration: gain on confidence-surprise signal for arbitration (>=0)
    - age_load_slope: penalty on confidence per extra items and for age (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_wm, meta_arbitration, age_load_slope = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            Qc = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Qc))
            p_rl = np.exp(softmax_beta * Qc[a]) / max(1e-12, denom_rl)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy
            Lc = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * Lc))
            p_wm = np.exp(softmax_beta_wm * Lc[a]) / max(1e-12, denom_wm)

            # Compute WM confidence (top-2 margin) and RL surprise
            sorted_W = np.sort(W_s)[::-1]
            conf = sorted_W[0] - (sorted_W[1] if nA > 1 else 0.0)
            surprise = np.abs(r - Q_s[a])

            # Scale confidence by load and age penalty
            conf_eff = conf - age_load_slope * ((nS - 3) / 3.0 + 0.5 * age_group)

            # Dynamic arbitration: adjust wm_weight via logit with meta_arbitration * (conf_eff + surprise)
            wm_weight = np.clip(wm_weight, 1e-6, 1 - 1e-6)
            logit_base = np.log(wm_weight) - np.log(1 - wm_weight)
            wm_logit = logit_base + meta_arbitration * (conf_eff + 0.5 * surprise)
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = p_wm * wm_weight_dyn + (1.0 - wm_weight_dyn) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Surprise-gated WM learning: larger updates when RL is surprised
            alpha_eff = np.clip(alpha_wm * (0.05 + surprise), 0.0, 1.0)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] += alpha_eff * (target - W_s)
            else:
                # On non-reward, partial decay toward uniform plus slight suppression of chosen action
                decay = 0.5 * alpha_eff
                w[s, :] = (1 - decay) * W_s + decay * w_0[s, :]
                w[s, a] *= (1 - 0.25 * alpha_eff)

            # Renormalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p