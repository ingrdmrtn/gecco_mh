def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with interference and age-biased gating.

    Mechanism
    - RL: tabular Q-learning with softmax choice.
    - WM: per-state one-shot binding of rewarded action, stored as a sharp distribution that
      passively decays toward uniform with set-size-dependent interference.
    - Arbitration: mixture weight is the base wm_weight scaled by an effective capacity factor
      (min(1, cap_slots / set_size)) and an age-dependent gating bias (reduced if older).

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials.
    model_parameters : list/tuple
        [lr, wm_weight, softmax_beta, cap_slots, gate_age_bias, wm_forget]
        - lr: RL learning rate (0..1)
        - wm_weight: base arbitration weight for WM (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10 internally)
        - cap_slots: WM capacity in slots (>=1), scales down WM at larger set sizes
        - gate_age_bias: fractional reduction of WM weight in older group (0..1)
        - wm_forget: WM decay/interference rate per trial, scaled by set size (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, cap_slots, gate_age_bias, wm_forget = model_parameters
    softmax_beta *= 10.0

    # Age group coding: 0 young (<=45), 1 old (>45)
    age_group = 0 if age[0] <= 45 else 1

    softmax_beta_wm = 50.0  # deterministic WM readout

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))    # WM current beliefs per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM baseline (uniform)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL choice probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Read WM distribution for this state and compute its softmax choice prob
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Dynamic arbitration weight: capacity- and age-gated
            cap_factor = min(1.0, max(1e-6, float(cap_slots)) / max(1.0, float(nS)))
            age_gate = (1.0 - gate_age_bias * age_group)
            wm_weight = np.clip(wm_weight * cap_factor * age_gate, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Set-size-dependent global interference/decay toward uniform
            decay = 1.0 - np.exp(-wm_forget * max(1.0, float(nS)))
            decay = np.clip(decay, 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # Rewarded binding: if rewarded, set a sharp memory for the chosen action in that state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM "NoGo" suppression cache with age-modulated exploration and set-size decay.

    Mechanism
    - RL: Q-learning with age-modulated inverse temperature (older => softer choices).
    - WM: caches suppression weights for actions that produced 0 reward (NoGo memory).
      Policy favors actions with lower suppression. Suppression decays faster with larger set sizes.
    - Arbitration: fixed base wm_weight scaled down by set size (proportional to 3/nS).

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials.
    model_parameters : list/tuple
        [lr, wm_weight, softmax_beta, nogo_gain, age_beta_mix, decay_set]
        - lr: RL learning rate (0..1)
        - wm_weight: base WM arbitration weight (0..1)
        - softmax_beta: base RL inverse temperature (scaled by 10)
        - nogo_gain: strength of WM suppression (>=0)
        - age_beta_mix: fractional reduction of beta for older group (0..1)
        - decay_set: base decay rate of NoGo suppression per trial per item (>=0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, nogo_gain, age_beta_mix, decay_set = model_parameters
    # Scale beta and modulate by age
    softmax_beta = softmax_beta * 10.0 * (1.0 - age_beta_mix * (0 if age[0] <= 45 else 1))
    softmax_beta = max(softmax_beta, 1e-3)

    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM holds suppression weights; larger is more suppressed
        w = np.zeros((nS, nA))
        w_0 = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Convert suppression into utilities (lower suppression => higher utility)
            sup = w[s, :]
            U_wm = -nogo_gain * sup
            denom_wm = np.sum(np.exp(softmax_beta_wm * (U_wm - U_wm[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: scale WM weight down with set size (reference set size = 3)
            wm_weight = np.clip(wm_weight * (3.0 / max(3.0, float(nS))), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Set-size dependent decay of suppression toward 0
            decay = 1.0 - np.exp(-decay_set * max(1.0, float(nS)))
            decay = np.clip(decay, 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # If feedback is 0, increase suppression for the chosen action in that state
            if r < 0.5:
                w[s, a] += 1.0  # add one unit of suppression; scale already controlled by nogo_gain

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-adaptive learning + WM recency trace with age-scaled strength.

    Mechanism
    - RL: learning rate is modulated by surprise |delta| with sensitivity param; larger surprise -> larger update.
    - WM: per-state recency distribution updated toward the last chosen action, weighted by reward.
      The recency trace decays each trial; decay increases with set size.
    - Arbitration: base wm_weight multiplied by the current WM "confidence" (norm deviation from uniform),
      scaled by an age factor (older => weaker WM influence).

    Parameters
    ----------
    states : array-like
        State indices per trial.
    actions : array-like
        Chosen actions (0..2).
    rewards : array-like
        Binary rewards (0/1).
    blocks : array-like
        Block index per trial.
    set_sizes : array-like
        Set size per trial (constant within block).
    age : array-like
        Participant age repeated across trials.
    model_parameters : list/tuple
        [lr, wm_weight, softmax_beta, surprise_sens, recency_rate, age_wm_scale]
        - lr: base RL learning rate (0..1)
        - wm_weight: base WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (scaled by 10)
        - surprise_sens: gain converting |PE| to LR boost (>=0)
        - recency_rate: WM recency learning rate per update (0..1)
        - age_wm_scale: multiplicative WM scaling drop for older group (0..1; applied as 1 - age_wm_scale)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, surprise_sens, recency_rate, age_wm_scale = model_parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM recency trace per state (probability vector over actions)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM confidence: deviation from uniform; scaled to [0,1]
            conf = 0.5 * np.sum(np.abs(W_s - w_0[s, :]))  # max 1.0 when one-hot
            # Age scales down WM influence for older participants
            age_scale = (1.0 - age_wm_scale * age_group)
            wm_weight_eff = np.clip(wm_weight * conf * age_scale, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with surprise-adaptive learning rate
            pe = r - Q_s[a]
            lr_eff = np.clip(lr + surprise_sens * abs(pe), 0.0, 1.0)
            q[s, a] += lr_eff * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Set-size dependent decay toward uniform
            decay = 1.0 - np.exp(-recency_rate * max(1.0, float(nS)))
            decay = np.clip(decay, 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # Reward-weighted recency update toward the chosen action
            one_hot = np.zeros(3)
            one_hot[a] = 1.0
            # Move the state's WM distribution toward the chosen action; scaled by reward
            w[s, :] = (1.0 - recency_rate * r) * w[s, :] + (recency_rate * r) * one_hot

        blocks_log_p += log_p

    return -blocks_log_p