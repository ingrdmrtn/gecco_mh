def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards from the RL + outcome-gated WM mixture with lapse and global WM decay.

    Arguments:
    - stimulus: sequence of stimuli (states) for each trial (array of ints)
    - blocks: block index for each trial (array)
    - set_sizes: block-dependent set sizes on each trial (array)
    - correct_answer: correct action on each trial (array of ints)
    - age: participant's age (array, single repeated value)
    - parameters: [lr, softmax_beta, wm_weight_base, wm_conf_boost, wm_decay, lapse]

    Returns:
    - simulated_actions: simulated action choices (array of ints)
    - simulated_rewards: simulated binary rewards (array of ints)
    """
    import numpy as np

    lr, softmax_beta, wm_weight_base, wm_conf_boost, wm_decay, lapse = parameters

    # Match scaling used in the fitting code
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM

    age_group = 0 if age[0] <= 45 else 1

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        # Trials in this block
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices].astype(int)
        block_correct = correct_answer[block_indices].astype(int)
        block_set_sizes = set_sizes[block_indices]

        nA = 3
        nS = int(block_set_sizes[0])

        # Map each state to its correct action within the block
        unique_states = np.unique(block_states)
        # Ensure states are 0..nS-1
        # If states are already 0..nS-1, this is direct; otherwise map them
        state_map = {st: i for i, st in enumerate(unique_states)}
        inv_state_map = {i: st for st, i in state_map.items()}
        nS = len(unique_states)

        correct_actions = np.zeros(nS, dtype=int)
        for st_i in range(nS):
            original_state_val = inv_state_map[st_i]
            # find first occurrence in the block to get its correct action
            idx = np.where(block_states == original_state_val)[0][0]
            correct_actions[st_i] = block_correct[idx]

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s_orig = int(block_states[t])
            s = state_map[s_orig]  # remap to 0..nS-1

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax policy
            rl_logits = softmax_beta * Q_s
            rl_logits -= np.max(rl_logits)
            p_rl = np.exp(rl_logits)
            p_rl /= np.sum(p_rl)

            # WM softmax policy (high beta)
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            p_wm = np.exp(wm_logits)
            p_wm /= np.sum(p_wm)

            # Effective WM weight with age and load effects
            wm_weight_eff = wm_weight_base * (1.0 - 0.4 * age_group)
            load_scale = 3.0 / float(nS)

            # Outcome-gated mixture weight applied per action (based on whether action would be correct)
            correct_act = int(correct_actions[s])
            wm_weight_per_action = np.zeros(nA)
            for a in range(nA):
                r_a = 1.0 if a == correct_act else 0.0
                outcome_boost = (1.0 + wm_conf_boost * r_a) / (1.0 + wm_conf_boost)
                wm_weight_trial = wm_weight_eff * load_scale * outcome_boost
                wm_weight_per_action[a] = np.clip(wm_weight_trial, 0.0, 1.0)

            # Mixture distribution (normalize to form a valid distribution)
            p_mix = wm_weight_per_action * p_wm + (1.0 - wm_weight_per_action) * p_rl
            p_mix = p_mix / np.sum(p_mix)

            # Lapse depends on set size and age
            lapse_age = lapse * (1.0 + 0.2 * (nS == 6) + 0.5 * age_group)
            lapse_age = np.clip(lapse_age, 0.0, 0.5)

            # Final policy with lapse
            p_total = (1.0 - lapse_age) * p_mix + lapse_age * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(np.arange(nA), p=p_total))
            simulated_actions[block_indices[t]] = a

            # Reward based on correctness
            r = 1 if a == correct_act else 0
            simulated_rewards[block_indices[t]] = r

            # RL update
            delta = r - Q_s[a]
            q[s, a] = Q_s[a] + lr * delta

            # WM decay toward uniform prior
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, one-shot WM boost to the chosen action
            if r > 0.5:
                outcome_boost_r1 = (1.0 + wm_conf_boost * 1.0) / (1.0 + wm_conf_boost)
                alpha_wm = 0.8 * (1.0 + 0.5 * (outcome_boost_r1 - 0.5))
                alpha_wm = np.clip(alpha_wm, 0.0, 1.0)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * one_hot

            tr += 1

    return simulated_actions, simulated_rewards