def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL+WM mixture with set-size- and age-modulated WM precision and choice perseveration.

    Arguments:
    - stimulus: sequence of stimuli for each trial (array of ints: state IDs within a block)
    - blocks: block index for each trial (array)
    - set_sizes: block-dependent set sizes on each trial (array; first value per block used)
    - correct_answer: correct action on each trial (array of ints in [0, nA-1])
    - age: participant's age (array, single repeated value)
    - parameters: [lr, softmax_beta, kappa_base, wm_prec_base, age_wm_drop]

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """
    import numpy as np

    lr, softmax_beta, kappa_base, wm_prec_base, age_wm_drop = parameters
    softmax_beta *= 10.0

    age_group = 0 if age[0] <= 45 else 1

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        # Keep original order within the block
        block_indices.sort()
        block_states = stimulus[block_indices]
        block_correct = correct_answer[block_indices]

        nA = 3
        # Use provided set size per block (consistent with fitting code)
        nS = int(set_sizes[block_indices][0])

        # Map from local state IDs (0..nS-1) to correct action using first occurrence
        unique_states = np.unique(block_states)
        # If states are already 0..nS-1, this is direct; otherwise remap robustly
        state_to_local = {st: i for i, st in enumerate(unique_states)}
        local_states = np.array([state_to_local[st] for st in block_states], dtype=int)
        correct_actions = np.zeros(nS, dtype=int)
        for st in unique_states:
            st_local = state_to_local[st]
            first_idx = np.where(block_states == st)[0][0]
            correct_actions[st_local] = int(block_correct[first_idx])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Block-level parameters depending on set size and age
        load_factor = 3.0 / nS
        wm_driver = wm_prec_base * load_factor * (1.0 - age_wm_drop * age_group)
        wm_weight = 1.0 / (1.0 + np.exp(-wm_driver))
        softmax_beta_wm = 5.0 + 45.0 * max(0.0, wm_driver)
        kappa_eff = kappa_base * (1.0 + 0.5 * age_group)
        wm_interference = max(0.0, (nS - 3.0) / 3.0)

        for t, tr in enumerate(block_indices):
            s = local_states[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Choice perseveration bias (sticky choice) on RL policy
            if last_action[s] >= 0:
                stick_vec = (np.arange(nA) == last_action[s]).astype(float)
            else:
                stick_vec = np.zeros(nA)

            # RL policy
            logits_rl = softmax_beta * Q_s + kappa_eff * stick_vec
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl /= np.sum(p_rl)

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            p_wm = np.exp(logits_wm)
            p_wm /= np.sum(p_wm)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            p_total /= np.sum(p_total)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[tr] = a

            # Reward from environment (correct action yields reward 1)
            r = 1 if a == int(correct_actions[s]) else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay/interference toward baseline for this state
            w[s, :] = (1.0 - wm_interference) * w[s, :] + wm_interference * w_0[s, :]

            # WM update on reward: blend toward one-hot of chosen action
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot

            # Update perseveration memory
            last_action[s] = a

    return simulated_actions, simulated_rewards