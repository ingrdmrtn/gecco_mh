def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards for:
    RL with eligibility trace + gated WM by prediction-error magnitude.

    Inputs:
    - stimulus: state index per trial (array of ints)
    - blocks: block index per trial (array of ints)
    - set_sizes: set size per trial (array of ints; constant within block)
    - correct_answer: correct action per trial (array of ints 0..2)
    - age: participant's age repeated per trial (array, single repeated value)
    - parameters: [lr, softmax_beta, lambda_et, wm_gate_base, age_gate_delta, setsize_gate_delta]

    Returns:
    - simulated_actions: simulated action choices (array of ints)
    - simulated_rewards: simulated binary rewards (array of ints 0/1)
    """
    lr, softmax_beta, lambda_et, wm_gate_base, age_gate_delta, setsize_gate_delta = parameters
    beta = softmax_beta * 10.0
    beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    k = 5.0  # slope for WM gate
    relax = 0.1  # WM relaxation toward uniform

    tr_global = 0
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)
        block_setsize = int(set_sizes[block_idx][0])

        nA = 3
        nS = block_setsize

        # Derive per-state correct action mapping within block
        correct_actions = np.zeros(nS, dtype=int)
        for st in range(nS):
            # take the first occurrence's correct action for this state
            st_mask = (block_states == st)
            first_idx = np.where(st_mask)[0][0]
            correct_actions[st] = block_correct[first_idx]

        # Initialize RL and WM structures
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # Gate threshold for this block
        setsize_factor = max(0.0, (nS - 3) / 3.0)  # 0 for 3, 1 for 6
        gate_theta = wm_gate_base + age_gate_delta * age_group + setsize_gate_delta * setsize_factor

        # Store last unsigned PE per state to drive gate on the next visit
        pe_last = gate_theta * np.ones(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])

            # Policies
            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            rl_logits = beta * (Q_s - np.max(Q_s))
            p_rl = np.exp(rl_logits)
            p_rl /= np.sum(p_rl)

            # WM softmax (near-deterministic)
            wm_logits = beta_wm * (W_s - np.max(W_s))
            p_wm = np.exp(wm_logits)
            p_wm /= np.sum(p_wm)

            # Arbitration using last PE for this state
            wm_weight = 1.0 / (1.0 + np.exp(-k * (pe_last[s] - gate_theta)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = p_total / np.sum(p_total)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[block_idx[t]] = a

            # Outcome
            r = 1 if a == int(correct_actions[s]) else 0
            simulated_rewards[block_idx[t]] = r

            # Store Q before update for PE used to update pe_last
            q_sa_before = q[s, a]

            # Eligibility trace update
            e[s, :] *= lambda_et
            e[s, a] += 1.0

            # RL update with traces
            pe = r - q_sa_before
            q[s, :] += lr * pe * e[s, :]

            # WM update: one-shot write on positive reward and positive PE, else relax
            if (r >= 0.5) and (pe > 0):
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - relax) * w[s, :] + relax * w_0[s, :]

            # Update PE cache for next visit to this state (unsigned PE before Q update)
            pe_last[s] = abs(r - q_sa_before)

            tr_global += 1

    return simulated_actions, simulated_rewards