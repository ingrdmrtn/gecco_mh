import numpy as np

def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL + Capacity-limited WM with load- and age-modulated gating and decay.

    Arguments:
    - stimulus: sequence of stimuli for each trial (array of ints)
    - blocks: block index for each trial (array)
    - set_sizes: block dependent set sizes on each trial (array)
    - correct_answer: correct answer on each trial (array of ints in 0..2)
    - age: participant's age (array or scalar)
    - parameters: [lr_raw, beta_raw, wm_conf_raw, decay_raw, age_wm_cost, load_wm_cost]

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """
    lr_raw, beta_raw, wm_conf_raw, decay_raw, age_wm_cost, load_wm_cost = parameters

    # Parameter transforms
    lr = 1.0 / (1.0 + np.exp(-lr_raw))
    beta = max(1e-6, beta_raw * 10.0)
    wm_conf_base = 1.0 / (1.0 + np.exp(-wm_conf_raw))
    decay_base = 1.0 / (1.0 + np.exp(-decay_raw))

    # Age group
    age_val = age[0] if hasattr(age, "__len__") else age
    age_group = 0 if age_val <= 45 else 1

    softmax_beta_wm = 50.0
    eps = 1e-12

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        mask = (blocks == b)
        idx = np.where(mask)[0]

        block_states = stimulus[mask].astype(int)
        block_correct = correct_answer[mask].astype(int)
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Build deterministic correct action mapping per state
        # Assume within a block, each state maps to a single correct action
        states_in_block = np.unique(block_states)
        state_to_correct = {}
        for s in states_in_block:
            first_idx = np.where(block_states == s)[0][0]
            state_to_correct[s] = int(block_correct[first_idx])

        # Load level for this block
        load_level = (nS - 3) / 3.0

        # Effective WM gate and decay for this block
        wm_conf_logit = np.log(wm_conf_base + eps) - np.log(1.0 - wm_conf_base + eps)
        wm_gate_eff = 1.0 / (1.0 + np.exp(-(wm_conf_logit - age_wm_cost * age_group - load_wm_cost * load_level)))
        wm_gate_eff = np.clip(wm_gate_eff, 0.0, 1.0)

        decay_penalty = 1.0 + age_wm_cost * age_group + load_wm_cost * load_level
        decay_eff = 1.0 - (1.0 - decay_base) ** max(1e-6, decay_penalty)
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        # Initialize values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = block_states[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            q_shift = Q_s - np.max(Q_s)
            p_rl = np.exp(beta * q_shift)
            p_rl /= np.sum(p_rl)

            # WM softmax (high inverse temperature)
            w_shift = W_s - np.max(W_s)
            p_wm = np.exp(softmax_beta_wm * w_shift)
            p_wm /= np.sum(p_wm)

            # Mixture policy
            p_total = wm_gate_eff * p_wm + (1.0 - wm_gate_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            p_total /= np.sum(p_total)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[idx[t]] = a

            # Simulate reward (deterministic mapping)
            correct_a = state_to_correct[s]
            r = 1 if a == correct_a else 0
            simulated_rewards[idx[t]] = r

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: push toward last rewarded action, else decay toward uniform
            if r == 1:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_gate_eff) * w[s, :] + wm_gate_eff * onehot

            # Decay toward uniform each trial
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            tr += 1

    return simulated_actions, simulated_rewards