def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using Model 1: RL + WM with entropy-based arbitration and set-size/age-scaled WM precision.

    Arguments:
    - stimulus: sequence of stimuli for each trial (array of ints, state indices)
    - blocks: block index for each trial (array)
    - set_sizes: set size for each trial (array; assumed constant within block)
    - correct_answer: correct action for each trial (array of ints)
    - age: participant's age (array, single repeated value)
    - parameters: [lr, softmax_beta, wm_bias, ss_scale, age_wm_mult, neg_error_decay]

    Returns:
    - simulated_actions: simulated action choices (array of ints)
    - simulated_rewards: simulated binary rewards (array of ints)
    """
    lr, softmax_beta, wm_bias, ss_scale, age_wm_mult, neg_error_decay = parameters
    softmax_beta *= 10.0  # higher upper bound for RL temperature
    softmax_beta_wm_base = 50.0  # very deterministic WM baseline

    if age[0] <= 45:
        age_group = 0
    else:
        age_group = 1

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    eps = 1e-12
    tr = 0

    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices]
        block_correct = correct_answer[block_indices]
        block_set_sizes = set_sizes[block_indices]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size and age scaling for WM precision
        ss_factor = 1.0 + ss_scale * max(0, nS - 3)
        age_factor = 1.0 + age_group * age_wm_mult
        wm_precision_divisor = max(0.1, ss_factor * age_factor)

        for t in range(len(block_states)):
            s = block_states[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            z_rl = Q_s - np.max(Q_s)
            rl_exp = np.exp(softmax_beta * z_rl)
            rl_probs = rl_exp / np.clip(np.sum(rl_exp), eps, None)

            # RL entropy for arbitration
            rl_entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))

            # WM policy (precision reduced by set size and age)
            beta_wm_eff = softmax_beta_wm_base / wm_precision_divisor
            z_wm = W_s - np.max(W_s)
            wm_exp = np.exp(beta_wm_eff * z_wm)
            wm_probs = wm_exp / np.clip(np.sum(wm_exp), eps, None)

            # Arbitration: mixture weight based on RL uncertainty (entropy) and bias
            wm_mix_logit = wm_bias + rl_entropy
            wm_mix = 1.0 / (1.0 + np.exp(-wm_mix_logit))

            # Combined policy
            p_total = wm_mix * wm_probs + (1.0 - wm_mix) * rl_probs
            p_total = np.clip(p_total, eps, 1.0)
            p_total = p_total / np.sum(p_total)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[block_indices[t]] = a

            # Outcome
            r = 1 if a == block_correct[t] else 0
            simulated_rewards[block_indices[t]] = r

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                learn_strength = 1.0 / wm_precision_divisor
                w[s, :] = (1.0 - learn_strength) * w[s, :] + learn_strength * one_hot
            else:
                decay = np.clip(neg_error_decay / wm_precision_divisor, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            tr += 1

    return simulated_actions, simulated_rewards