def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL with choice-kernel and capacity-limited WM slots, with age/load-modulated access and lapses.

    Arguments:
    - stimulus: sequence of stimuli (state indices) per trial (array)
    - blocks: block index per trial (array)
    - set_sizes: block-dependent set sizes per trial (array)
    - correct_answer: correct action per trial (array)
    - age: participant's age (array, single repeated value)
    - parameters: [lr, beta_rl, kernel_strength, wm_cap, age_pen, slip]

    Returns:
    - simulated_actions: simulated action choices (array, int)
    - simulated_rewards: simulated binary rewards (array, int)
    """
    lr, beta_rl, kernel_strength, wm_cap, age_pen, slip = parameters
    beta_rl *= 10.0
    softmax_beta_wm = 50.0

    age_group = 0 if age[0] <= 45 else 1

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices].astype(int)
        block_correct_actions = correct_answer[block_indices].astype(int)
        block_set_sizes = set_sizes[block_indices]

        nA = 3
        # Use provided set size for the block (assumed constant within block as in fitting code)
        nS = int(block_set_sizes[0])

        # Build a mapping from state to its correct action in this block
        unique_states = np.unique(block_states)
        # Map states to 0..nS-1 if they aren't already; assume states are 0..nS-1 as in fitting code
        # Create per-state correct action list
        correct_actions_by_state = {}
        for st in unique_states:
            # take the first occurrence's correct action for this state in the block
            correct_actions_by_state[int(st)] = int(block_correct_actions[block_states == st][0])

        # Initialize RL and WM values
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM access probability and lapse for this block
        p_access = min(1.0, max(0.0, wm_cap / max(1.0, float(nS))))
        p_access *= (1.0 - 0.5 * age_pen * age_group)
        p_access = min(max(p_access, 0.0), 1.0)

        lapse = min(0.5, slip * (nS / 3.0) * (1.0 + 0.5 * age_pen * age_group))

        # Choice kernel initialization
        kernel_decay = 0.2 + 0.2 * max(0.0, (nS - 3.0) / 3.0)  # faster decay at higher load
        kernel_bias = np.zeros(nA)

        for t in range(len(block_states)):
            s = int(block_states[t])

            # RL policy with choice kernel
            Q_s = q[s, :].copy()
            Q_pref = Q_s + kernel_strength * kernel_bias
            Q_pref = Q_pref - np.max(Q_pref)
            exp_rl = np.exp(beta_rl * Q_pref)
            p_rl_vec = exp_rl / (np.sum(exp_rl) + 1e-12)

            # WM policy
            W_s = w[s, :].copy()
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(wm_logits)
            p_wm_vec = exp_wm / (np.sum(exp_wm) + 1e-12)

            # Mixture and lapse
            p_mix_vec = p_access * p_wm_vec + (1.0 - p_access) * p_rl_vec
            p_total_vec = (1.0 - lapse) * p_mix_vec + lapse * (1.0 / nA)
            p_total_vec = np.clip(p_total_vec, 1e-12, 1.0)
            p_total_vec = p_total_vec / p_total_vec.sum()

            # Sample action
            a = int(np.random.choice(nA, p=p_total_vec))
            simulated_actions[block_indices[t]] = a

            # Reward based on correct action for this state
            correct_a = correct_actions_by_state.get(s, 0)
            r = 1 if a == correct_a else 0
            simulated_rewards[block_indices[t]] = r

            # Update choice kernel
            kernel_bias *= (1.0 - kernel_decay)
            kernel_bias = np.maximum(kernel_bias, 0.0)
            kernel_bias[a] += 1.0

            # RL update
            delta = float(r) - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward baseline
            d = 0.05 * (nS / 3.0) * (1.0 + 0.5 * age_pen * age_group)
            d = min(max(d, 0.0), 1.0)
            w = (1.0 - d) * w + d * w_0

            # WM storage on rewarded trials
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                eta_wm = 0.8
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * onehot
                w[s, :] /= (w[s, :].sum() + 1e-12)

            tr += 1

    return simulated_actions, simulated_rewards