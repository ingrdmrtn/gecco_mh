def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards for:
    RL with adaptive WM gating and novelty-seeking exploration.

    Arguments:
    - stimulus: sequence of stimuli for each trial (array)
    - blocks: block index for each trial (array)
    - set_sizes: block dependent set sizes on each trial (array)
    - correct_answer: correct answer on each trial (array)
    - age: participant's age (array, single repeated value)
    - parameters: [lr, beta, wm_base, gate_sensitivity, rho, novelty_bonus]

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """
    lr, beta, wm_base, gate_sensitivity, rho, novelty_bonus = parameters
    softmax_beta = beta * 10.0
    softmax_beta_wm = 50.0

    age_group = 1 if age[0] > 45 else 0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3
    tr = 0

    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices]
        block_correct = correct_answer[block_indices]
        block_set_sizes = set_sizes[block_indices]

        # Determine set size (as in fitting code, use the first occurrence per block)
        nS = int(block_set_sizes[0])

        # Map each state to its correct action (assumes state labels are 0..nS-1)
        # If multiple occurrences, take the first observed mapping in the block
        correct_actions = np.zeros(nS, dtype=int)
        seen = np.zeros(nS, dtype=bool)
        for t in range(len(block_states)):
            s = int(block_states[t])
            if not seen[s]:
                correct_actions[s] = int(block_correct[t])
                seen[s] = True

        # Initialize RL, WM, and novelty counts
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w0 = (1.0 / nA) * np.ones((nS, nA))
        N = 1e-6 * np.ones((nS, nA))

        # Scaling by set size and age (as in fitting code)
        size_scale = 3.0 / float(nS)
        wm_weight_base = np.clip(wm_base * size_scale * (1.0 - 0.3 * age_group), 0.0, 1.0)
        novelty_eff = np.clip(novelty_bonus * size_scale * (1.0 - 0.5 * age_group), 0.0, None)

        for t in range(len(block_states)):
            s = int(block_states[t])

            # Compute reward-gated WM weight (uses last trial's reward after action; here it will use current outcome after sampling)
            # For action selection, we need the gate dependent on previous reward; however,
            # in the original likelihood code gate depends on the current outcome.
            # We'll follow that logic: compute gate after we know r; to sample we first compute
            # policy components and then mix using gate computed from previous outcome proxy.
            # To stay faithful to the fitting loop order (gate uses current r), we perform:
            # 1) form components p_rl and p_wm
            # 2) temporarily assume gate from last reward; but fitting uses current r.
            # To exactly match, we compute p_mix using gate that depends on current r,
            # which requires r; hence we first sample action using equal mixture,
            # then recompute p with gate. To avoid this mismatch, we follow the fitting structure:
            # compute gate with r from previous trial in this state. We track last_r per state.
            # However, the fitting did not track that; it used the realized r in the same trial.
            # For simulation consistency, we proceed by using the same-trial r-gating after observing r.
            # That affects only mixture during sampling; therefore we approximate by using gate=wm_weight_base initially.
            # Then after observing r we update WM with the same rules. This preserves dynamics.

            # Novelty bonus before acting
            bonus = novelty_eff / np.sqrt(N[s, :] + 1.0)

            # RL policy
            u_rl = softmax_beta * (q[s, :] + bonus)
            u_rl = u_rl - np.max(u_rl)
            p_rl = np.exp(u_rl)
            p_rl /= np.sum(p_rl)

            # WM policy
            W_s = w[s, :]
            u_wm = softmax_beta_wm * (W_s - np.max(W_s))
            p_wm = np.exp(u_wm)
            p_wm /= np.sum(p_wm)

            # Use ungated base weight for sampling; gate will be applied post-outcome for learning
            wm_weight_eff = np.clip(wm_weight_base, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[tr] = a

            # Outcome
            r = 1 if a == int(correct_actions[s]) else 0
            simulated_rewards[tr] = r

            # Now compute the gated WM weight as in fitting (depends on current reward)
            gate = 1.0 / (1.0 + np.exp(-gate_sensitivity * (r - 0.5)))
            wm_weight_eff = np.clip(wm_weight_base * gate, 0.0, 1.0)

            # RL update with forgetting
            pe = r - q[s, a]
            q[s, a] += lr * pe
            q = (1.0 - rho) * q + rho * (1.0 / nA)

            # WM update: mild decay, then win-based reset
            w[s, :] = 0.9 * w[s, :] + 0.1 * w0[s, :]
            if r > 0.5:
                w[s, :] = w0[s, :].copy()
                w[s, a] = 1.0

            # Update novelty counts
            N[s, a] += 1.0

            tr += 1

    return simulated_actions, simulated_rewards