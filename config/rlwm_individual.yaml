loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1



task:
  name: "rlwm"
  description: |
    In this task a participant is presented with a state on each trial, and asked to select one of the 3 possible actions. For each state there is a fixed correct action. A state can be a part of a smaller set (3) or larger set (6).
    Following the action selection, participants observed feedback (0 or 1). Possible states reset at the start of each block (that is, there were no overlapping states between the blocks).
    The objective of the experiment was to understand the effect of cognitive load conditions on performance (3 vs 6 state-action pairs).

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python functions:
    {model_names}

data:
  max_prompt_trials: 5
  max_prompt_blocks: 3
  path: "data/rlwm.csv"
  id_column: "participant"
  input_columns: ["stimulus", "actions", "rewards", "blocks", "set_sizes"]
  simulation_columns: ["stimulus","blocks","set_sizes","correct_answer"]
  simulation_return: ["simulated_actions", "simulated_rewards"]
  data2text_function: "narrative"

  narrative_template: |
    
    Block: {blocks}, Set size:{set_sizes}, Trial: {trial}, State: {stimulus}, Chosen action: {actions}, Reward: {rewards}'


  value_mappings:
#    choice_1:
#      "0": "A"
#      "1": "U"
#    state:
#      "0": "X"
#      "1": "Y"

  splits: # [start:end-1] index-based splits with non-inclusion of last index
    prompt: "[1:4]"
    eval: "[4:14]"
    test: "[14:]"

metadata:
  flag: true # set to false to exclude metadata from prompt
  description: |
    Participant's age was recorded as a part of the experiment. 
    Participant over the age of 45 were considered a part of the older group, and participants under the age of 45 were a part of the younger group.
  narrative_template: |
    This participant's age was {age}.

llm:
  provider: "openai"
  base_model: "gpt-5"
  temperature: 0.1
  reasoning_effort: "low"
  max_tokens: None
  max_output_tokens: None
  text_verbosity: None
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a reinforcement learning - working memory task.
    Your job is to propose cognitive models, expressed as Python functions,
    that explain how participant make their decisions over time.

  models_per_iteration: 3
  include_feedback: true # ⬅️ whether to include feedback in later iterations
  guardrails:

    - "Each model must be a standalone Python function."
    - "Function names must be `cognitive_model1`, `cognitive_model2`, etc."
    - "Take as input: `states, actions, rewards, blocks, set_sizes, model_parameters`."
    - "Return the **negative log-likelihood** of observed choices."
    - "Use all parameters meaningfully (no unused params)."
    - "Include a clear docstring for the model and each parameter."
    - "Do NOT include any package imports inside the code you write. Assume all packages are already imported."
    - "Use the information about age, and the respective age group meaningfully when constructing the model."
    - "You should fill out the template function - that is, do NOT modify what is already specified in the template, but fill in the equations in the FILL IN commented areas."
    - "You may add additional parameters other than the ones that are specified, but the total number of parameters, including alpha and beta, shouldn't be over 5)."

  template_model: |
    def cognitive_model(stimulus, actions, rewards, blocks, set_sizes, parameters):
    
        lr, wm_weight, softmax_beta = parameters
    
        blocks_log_p = 0
        for b in np.unique(blocks):
    
            block_actions = actions[blocks == b]
            block_rewards = rewards[blocks == b]
            block_states = states[blocks == b]
            block_set_sizes = set_sizes[blocks == b]
    
            nA = 3
            nS = block_set_sizes[0]
    
            q = (1 / nA) * np.ones((nS, nA))
            w = (1 / nA) * np.ones((nS, nA))
            w_0 = (1 / nA) * np.ones((nS, nA))
    
            log_p = 0
            for t in range(len(block_states)):
    
                a = block_actions[t]
                s = block_states[t]
                r = block_rewards[t]
    
                Q_s = q[s,:]
                W_s = w[s,:]
                p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))
    
                # [FILL IN THE POLICY FOR THE WORKING MEMORY]
                p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
                log_p += np.log(p_total)
          
                delta = r-Q_s[a]
                q[s][a] += lr*delta
                # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]

            blocks_log_p += log_p
    
        return -blocks_log_p


  simulation_template: |
    def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    
        """    
        Parameters:
        - states: sequence of state indices for each trial
        - blocks: block index for each trial
        - correct_actions_dict: dict mapping state index to correct action
        - parameters: [lr_low, lr_high, forgetting, wm_weight, softmax_beta]
        - seed: random seed for reproducibility
    
        Returns:
        - actions: simulated action choices
        - rewards: simulated binary rewards
        """
    
        lr, wm_weight, softmax_beta = parameters
    
        n_trials = len(stimulus)
        simulated_actions = np.zeros(n_trials, dtype=int)
        simulated_rewards = np.zeros(n_trials, dtype=int)
        set_sizes = np.zeros(n_trials, dtype=int)
        tr = 0
    
    
        for b in np.unique(blocks):
            block_indices = np.where(blocks == b)[0]
            block_states = stimulus[block_indices]
            block_correct_actions = correct_answers[block_indices]
    
            nA = 3
            nS = len(np.unique(block_states))
            correct_actions = []
            for st in range(nS):
                correct_actions.append(block_correct_actions[block_states==st][0])
            # Initialize RL and WM values
            q = (1 / nA) * np.ones((nS, nA))
            w = (1 / nA) * np.ones((nS, nA))
            w_0 = (1 / nA) * np.ones((nS, nA))
    
            for t in range(len(block_states)):
    
    
                s = block_states[t]
                set_sizes[tr] = nS
                Q_s = q[s, :]
                W_s = w[s, :]
    
                # Compute probabilities
                p_rl = np.exp(softmax_beta * Q_s)
                p_rl /= np.sum(p_rl)
    
                p_wm = np.exp(softmax_beta * W_s)
                p_wm /= np.sum(p_wm)
    
                p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
    
                # Sample action
                a = np.random.choice(nA, p=p_total)
                simulated_actions[tr] = a
    
                # Simulate reward
                correct_action =  correct_actions[s]
                r = 1 if a == correct_action else 0
                simulated_rewards[tr] = r
    
                # RL update
                delta = r - Q_s[a]
                if nS == 3:  # low load
                    q[s][a] += lr_low * delta
                else:  # high load
                    q[s][a] += lr_high * delta
    
                # WM update
                w += forgetting * (w_0 - w)
                if r > 0:
                    w[s][a] = 2 * r
                else:
                    w[s][a] += lr_low * (0 - w[s][a])  # using lr_low for WM forgetting
                tr+=1
    
        return simulated_actions, simulated_rewards


evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "individual"

feedback:
  type: "manual" # or "llm"


