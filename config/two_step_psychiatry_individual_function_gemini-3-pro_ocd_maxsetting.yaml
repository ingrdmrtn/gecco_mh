loop:
  early_stopping: "False"
  max_iterations: 10
  max_independent_runs: 1

task:
  name: "two_step_psychiatry_individual_function_ocibalanced"
  description: |
    Participant played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred. 
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participant asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python functions:
    {model_names}

individual_difference:
  individual_feature: None
  description:

data:
  max_prompt_trials: 200
  path: "data/two_step_gillan_2016_ocibalanced.csv"
  id_column: "participant"
  input_columns: ["choice_1", "state", "choice_2", "reward"]
  simulation_columns:
    ["n_trials", "parameters", "drift1", "drift2", "drift3", "drift4"]
  data2text_function: "narrative"
  simulation_return: ["stage1_choice", "state2", "stage2_choice", "reward"]

  narrative_template: |
    In trial {trial}, participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

  splits:
    prompt: "[1:3]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.7
  reasoning_effort: "high"
  max_tokens: None
  max_output_tokens: None
  text_verbosity: None
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task.
    Your job is to propose cognitive models, expressed as Python functions,
    that explain how participants make their decisions over time.

  models_per_iteration: 3
  include_feedback: true # ⬅️ whether to include feedback in later iterations
  guardrails:
    - "Each model must be a standalone Python function."
    - "Function names must be `cognitive_model1`, `cognitive_model2`, etc."
    - "Take as input: `action_1, state, action_2, reward, model_parameters`."
    - "Return the **negative log-likelihood** of observed choices."
    - "Study the patterns in participant behavior provided in the participant data section carefully and use that to inform your model generation."
    - "Use all parameters meaningfully (no unused params)."
    - "Include a clear docstring for the model and each parameter."
    - "Parameter bounds: [0,1] for most; [0,10] for softmax `beta` (inverse temperature). Please refer to the template for how these are defined."
    - "Do NOT include any package imports inside the code you write. Assume all packages are already imported."
    - "You should fill out the template function - that is, do NOT modify what is already specified in the template, but fill in the equations in the FILL IN commented areas."
    - "You may add additional parameters other than alpha and beta, but try to be parsimonious; the total number of parameters, including alpha and beta, should not be over 8, which is the number of parameters in the baseline."

  template_model: |
    def cognitive_model(action_1, state, action_2, reward, model_parameters):
        """Example model illustrating format only (do not reuse logic).
        Bounds:
        learning_rate: [0,1]
        beta: [0,10]
        """
        learning_rate, beta = model_parameters
        n_trials = len(action_1)
      
        transition_matrix = np.array([[0.7, 0.3], [0.3, 0.7]])
        p_choice_1 = np.zeros(n_trials)
        p_choice_2 = np.zeros(n_trials)
        q_stage1_mf = np.zeros(2)
        q_stage2_mf = np.zeros((2, 2))

        for trial in range(n_trials):

            # policy for the first choice
            max_q_stage2 = np.max(q_stage2_mf, axis=1)
            q_stage1_mb = transition_matrix @ max_q_stage2
            exp_q1 = np.exp(beta * q_stage1_mb)
            probs_1 = exp_q1 / np.sum(exp_q1)
            p_choice_1[trial] = probs_1[action_1[trial]]
            state_idx = state[trial]

            # policy for the second choice
            # [FILL IN THE POLICY FOR THE SECOND CHOICE]
            
            # p_choice_2[trial] ... 
      
            delta_stage1 = q_stage2_mf[state_idx, action_2[trial]] - q_stage1_mf[action_1[trial]]
            # [FILL IN ACTION VALUE UPDATING FOR CHOICE 1]
            delta_stage2 = reward[trial] - q_stage2_mf[state_idx, action_2[trial]]
            # [FILL IN ACTION VALUE UPDATING FOR CHOICE 2]
            

        eps = 1e-10
        log_loss = -(np.sum(np.log(p_choice_1 + eps)) + np.sum(np.log(p_choice_2 + eps)))
        return log_loss

  simulation_template: |
    def simulate_model(
        n_trials,
        parameters,
        drift1,
        drift2,
        drift3,
        drift4):
        """
        Simulates choices and rewards using the hybrid model-based/model-free 2-step model
        with eligibility traces and perseveration.

        Parameters:
            n_trials (int): Number of trials to simulate.
            parameters (list): [learning_rate, learning_rate_2, beta, beta_2, w, lambd, perseveration]
            drift1, drift2, drift3, drift4 (np.ndarray): Trial-wise reward probabilities.
            seed (int or None): Random seed for reproducibility.

        Returns:
            stage1_choice (np.ndarray): First-stage choices (0 or 1).
            state2 (np.ndarray): Second-stage states (0 or 1).
            stage2_choice (np.ndarray): Second-stage choices (0 or 1).
            reward (np.ndarray): Rewards (0 or 1).
        """

        learning_rate, learning_rate_2, beta, beta_2, w, lambd, perseveration = parameters
        transition_matrix = np.array([[0.7, 0.3],
                                      [0.3, 0.7]])
        q_mf = np.zeros((3, 2))
        pers_array = np.zeros(2)

        stage1_choice = np.zeros(n_trials, dtype=int)
        state2 = np.zeros(n_trials, dtype=int)
        stage2_choice = np.zeros(n_trials, dtype=int)
        reward = np.zeros(n_trials, dtype=int)

        for t in range(n_trials):
            # Drift reward probs
            reward_probs = [[drift1[t],drift2[t]],[drift3[t],drift4[t]]]

            max_q_stage2 = np.max(q_mf[1:], axis=1)
            q_mb = transition_matrix @ max_q_stage2
            q_net = w * q_mb + (1 - w) * q_mf[0] + perseveration * pers_array

            p_stage1 = np.exp(beta * q_net) / np.sum(np.exp(beta * q_net))
            a1 = rng.choice([0, 1], p=p_stage1)

            s2 = rng.choice([0, 1], p=transition_matrix[a1])
            state_idx = s2 + 1

            q2 = q_mf[state_idx]
            p_stage2 = np.exp(beta_2 * q2) / np.sum(np.exp(beta_2 * q2))
            a2 = rng.choice([0, 1], p=p_stage2)

            r = int(rng.random() < reward_probs[s2][a2])

            delta1 = q_mf[state_idx, a2] - q_mf[0, a1]
            q_mf[0, a1] += learning_rate * delta1

            delta2 = r - q_mf[state_idx, a2]
            q_mf[state_idx, a2] += learning_rate_2 * delta2
            q_mf[0, a1] += lambd * learning_rate * delta2

            pers_array.fill(0)
            pers_array[a1] = 1

            stage1_choice[t] = a1
            state2[t] = s2
            stage2_choice[t] = a2
            reward[t] = r

        return stage1_choice, state2, stage2_choice, reward

metadata:
  flag: false # set to false to exclude metadata from prompt

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "individual"

feedback:
  type: "manual" # or "llm"

