loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1



task:
  name: "two_step_psychiatry_individual_stai_class"
  description: |
    Participant played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred. 
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participant asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python functions:
    {model_names}

individual_difference:
  individual_feature: stai
  description: |
    This participant scored {individual_feature} on an anxiety questionnaire (stai score).
    Participants who score under .31 have low anxiety.
    Participants who score between .31 and .51 have medium anxiety.
    Participants who score over .51 have high anxiety.

#  instructions: "Participants choose between two spaceships and interact with aliens for rewards."

data:
  max_prompt_trials: 50
  path: "data/two_step_gillan_2016.csv"
  id_column: "participant"
  input_columns: ["choice_1", "state", "choice_2", "reward","stai"]
  simulation_columns: ["reward_p_s0_0","reward_p_s0_1","reward_p_s1_0","reward_p_s1_1"]
  data2text_function: "narrative"

  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.1
  reasoning_effort: "low"
  max_tokens: None
  max_output_tokens: None
  text_verbosity: None
  api_key: null
  do_simulation: "False"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task.
    Your job is to propose cognitive models, expressed as Python functions,
    that explain how participants make their decisions over time.

  models_per_iteration: 3
  include_feedback: true # ⬅️ whether to include feedback in later iterations
  guardrails:
    - "Each model must inherit from `CognitiveModelBase`."
    - "Class names must be `ParticipantModel1`, `ParticipantModel2`, etc."
    - "Each class must end with: `cognitive_modelX = make_cognitive_model(ParticipantModelX)`"
    - "Take as input via constructor: `n_trials, stai, model_parameters`."
    - "Return the **negative log-likelihood** via `run_model()`."
    - "Use all parameters meaningfully (no unused params)."
    - "Include a docstring with cognitive hypothesis and parameter bounds"
    - "Parameter bounds: [0,1] for most; [0,10] for softmax `beta`."
    - "Each model MUST use `self.stai` to modulate at least one aspect of behavior"
    - "Do NOT include `import` statements - numpy is available as `np`"
    - "Override only the methods you need to customize."

  abstract_base_model: |
    ``` python
    from abc import ABC, abstractmethod
    class CognitiveModelBase(ABC):
        """
        Base class for cognitive models in a two-step task.
        
        Override methods to implement participant-specific cognitive strategies.
        """

        def __init__(self, n_trials: int, stai: float, model_parameters: tuple):
            # Task structure
            self.n_trials = n_trials
            self.n_choices = 2
            self.n_states = 2
            self.stai = stai
            
            # Transition matrix
            self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            
            # Choice probability sequences
            self.p_choice_1 = np.zeros(n_trials)
            self.p_choice_2 = np.zeros(n_trials)
            
            # Value representations
            self.q_stage1 = np.zeros(self.n_choices)
            self.q_stage2 = np.zeros((self.n_states, self.n_choices))
            
            # Trial tracking
            self.trial = 0
            self.last_action1 = None
            self.last_action2 = None
            self.last_state = None
            self.last_reward = None
            
            # Initialize
            self.unpack_parameters(model_parameters)
            self.init_model()

        @abstractmethod
        def unpack_parameters(self, model_parameters: tuple) -> None:
            """Unpack model_parameters into named attributes."""
            pass

        def init_model(self) -> None:
            """Initialize model state. Override to set up additional variables."""
            pass

        def policy_stage1(self) -> np.ndarray:
            """Compute stage-1 action probabilities. Override to customize."""
            return self.softmax(self.q_stage1, self.beta)

        def policy_stage2(self, state: int) -> np.ndarray:
            """Compute stage-2 action probabilities. Override to customize."""
            return self.softmax(self.q_stage2[state], self.beta)

        def value_update(self, action_1: int, state: int, action_2: int, reward: float) -> None:
            """Update values after observing outcome. Override to customize."""
            delta_2 = reward - self.q_stage2[state, action_2]
            self.q_stage2[state, action_2] += self.alpha * delta_2
            
            delta_1 = self.q_stage2[state, action_2] - self.q_stage1[action_1]
            self.q_stage1[action_1] += self.alpha * delta_1

        def pre_trial(self) -> None:
            """Called before each trial. Override to add computations."""
            pass

        def post_trial(self, action_1: int, state: int, action_2: int, reward: float) -> None:
            """Called after each trial. Override to add computations."""
            self.last_action1 = action_1
            self.last_action2 = action_2
            self.last_state = state
            self.last_reward = reward

        def run_model(self, action_1, state, action_2, reward) -> float:
            """Run model over all trials. Usually don't override."""
            for self.trial in range(self.n_trials):
                a1, s = int(action_1[self.trial]), int(state[self.trial])
                a2, r = int(action_2[self.trial]), float(reward[self.trial])
                
                self.pre_trial()
                self.p_choice_1[self.trial] = self.policy_stage1()[a1]
                self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
                self.value_update(a1, s, a2, r)
                self.post_trial(a1, s, a2, r)
            
            return self.compute_nll()
        
        def compute_nll(self) -> float:
            """Compute negative log-likelihood."""
            eps = 1e-12
            return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))
        
        def softmax(self, values: np.ndarray, beta: float) -> np.ndarray:
            """Softmax action selection."""
            centered = values - np.max(values)
            exp_vals = np.exp(beta * centered)
            return exp_vals / np.sum(exp_vals)

    def make_cognitive_model(ModelClass):
        """Create function interface from model class."""
        def cognitive_model(action_1, state, action_2, reward, stai, model_parameters):
            n_trials = len(action_1)
            stai_val = float(stai[0]) if hasattr(stai, '__len__') else float(stai)
            model = ModelClass(n_trials, stai_val, model_parameters)
            return model.run_model(action_1, state, action_2, reward)
        return cognitive_model
    ```
      
  template_model: |
    ```python
    class ParticipantModel1(CognitiveModelBase):
    """
    [HYPOTHESIS: Describe what cognitive strategy this model captures]
    
    Parameter Bounds:
    -----------------
    alpha: [0, 1]
    beta: [0, 10]
    [Add upto two to three parameters with their bounds if needed]
    """
    
    def unpack_parameters(self, model_parameters: tuple) -> None:
        self.alpha, self.beta = model_parameters  # Modify as needed
    
    # Override any of these methods to implement your hypothesis:
    # - init_model()      : Initialize additional state variables
    # - policy_stage1()   : Stage-1 action selection (default: softmax over q_stage1)
    # - policy_stage2()   : Stage-2 action selection (default: softmax over q_stage2[state])
    # - value_update()    : Learning rule (default: TD learning)
    # - pre_trial()       : Computations before each trial
    # - post_trial()      : Computations after each trial
    #
    # Available state: self.q_stage1, self.q_stage2, self.T, self.stai,
    #                  self.last_action1, self.last_state, self.last_reward, self.trial

    cognitive_model1 = make_cognitive_model(ParticipantModel1)
    ```

  diversity_requirement:
    - "Each of your {models_per_iteration} models should test a **different hypothesis** about how this participant makes decisions. Consider varying:"
    - "How stage-1 values are computed"
    - "How stage-2 values are computed"
    - "How learning occurs both stages"
    - "How the participant's anxiety score (stai) influences any of the above computations (e.g., multiplicative, additive, threshold-based, or selectively affecting different mechanisms)"
    - "What information from previous trials affects current choices"

metadata:
  flag: false # set to false to exclude metadata from prompt

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "individual"

feedback:
  type: "manual" # or "llm"
