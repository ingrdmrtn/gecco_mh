loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1

task:
  name: "two_step_study2_ddm_free"
  description: |
    Participants played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred.
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participants asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.
    Reaction times (in milliseconds) were recorded at both decision stages.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python classes:
    {model_names}

metadata:
  flag: true
  description: |
    Participants were assessed on a large battery of psychiatric questionnaires.
    Three transdiagnostic symptom factors were derived via factor analysis:
      - AD  (Anxious-Depression): elevated scores reflect anxiety and depressive symptoms
      - CIT (Compulsive/Intrusive Thoughts): elevated scores reflect obsessive-compulsive and intrusive thought symptoms
      - SW  (Social Withdrawal): elevated scores reflect social avoidance and apathy
    Scores are standardized (mean=0, SD≈1).
  narrative_template: |
    Participant {participant_id} — AD: {factor1:.2f}, CIT: {factor2:.2f}, SW: {factor3:.2f}

data:
  max_prompt_trials: 50
  path: "data_g_2019/twostep_study2_preprocessed.csv"  # placeholder
  id_column: "participant"
  input_columns:
    ["choice_1", "state", "choice_2", "reward",
     "stage_1_rt", "stage_2_rt",
     "factor1", "factor2", "factor3"]
  simulation_columns:
    ["ModelClass", "n_trials", "factor1", "factor2", "factor3",
     "model_parameters", "drift1", "drift2", "drift3", "drift4"]
  data2text_function: "narrative"
  simulation_return: ["stage1_choice", "state2", "stage2_choice", "reward"]

  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.
    Stage 1 RT: {stage_1_rt} ms. Stage 2 RT: {stage_2_rt} ms.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

  splits:
    prompt: "[1:4]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.7
  reasoning_effort: "low"
  max_tokens: null
  max_output_tokens: null
  text_verbosity: null
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task along with
    three transdiagnostic psychiatric symptom scores and reaction times.
    Your task is to propose models where:
    (1) choices are governed by reinforcement learning (Q-values, MB/MF weighting, etc.)
    (2) stage-1 reaction times are governed by a shifted Wald (inverse Gaussian) accumulator
    (3) the RT drift rate v1 is a FREE PARAMETER — it captures each participant's overall
        speed of evidence accumulation independent of their Q-value state.
    (4) psychiatric symptom scores modulate the RL component, the DDM parameters, or both.
    This design tests whether RT carries information BEYOND what is captured by choice alone,
    and whether psychiatric dimensions predict individual differences in decision speed.

  models_per_iteration: 3
  include_feedback: true

  guardrails:
    - "Each model must inherit from `CognitiveModelBase`."
    - "Class names must be `ParticipantModel1`, `ParticipantModel2`, etc."
    - "Each class must end with: `cognitive_modelX = make_cognitive_model(ParticipantModelX)`"
    - "Constructor receives: `n_trials, factor1, factor2, factor3, model_parameters`."
    - "The NLL combines choice NLL and stage-1 RT NLL via `compute_nll()` — do not override it."
    - "v1 is a CONSTANT free parameter (not trial-varying). It represents the participant's baseline evidence accumulation speed."
    - "v1 bounds: [0.1, 5.0]. a_bound bounds: [0.5, 5.0]. t0 bounds: [0.05, 0.5] seconds."
    - "Psychiatric factors may modulate v1, a_bound, or t0, but the result MUST remain positive. Use multiplicative scaling (e.g. v1 * exp(gamma * self.cit)) or additive with a floor (e.g. max(0.1, v1 + gamma * self.cit)). Never allow v1, a_bound, or t0 to reach zero or go negative."
    - "Use at least one of `self.ad`, `self.cit`, `self.sw` to modulate the RL or DDM component."
    - "Do NOT include `import` statements — numpy is available as `np`."
    - "Total free parameters must not exceed 7."

  diversity_requirement:
    - "Each of your {models_per_iteration} models must differ in HOW psychiatric factors modulate decision speed. Consider:"
    - "AD modulating a_bound — e.g. a_bound * exp(gamma * self.ad) — more anxious/depressed sets higher evidence threshold"
    - "CIT modulating v1 — e.g. v1 * exp(gamma * self.cit) — higher compulsivity associated with slower accumulation"
    - "SW modulating t0 — e.g. t0 * exp(gamma * self.sw) — social withdrawal affects non-decision processing time"
    - "A factor modulating v1 AND alpha jointly (correlated speed-learning hypothesis)"
    - "Interaction: one factor modulates a_bound (caution), another modulates v1 (speed), capturing dissociable psychiatric effects"
    - "Consider also whether the RL component (alpha, beta, MB/MF weight) should be modulated independently of the DDM component"

  abstract_base_model: |
    ``` python
    from abc import ABC, abstractmethod
    class CognitiveModelBase(ABC):
        def __init__(self, n_trials, factor1, factor2, factor3, model_parameters):
            self.n_trials  = n_trials
            self.n_choices = 2
            self.n_states  = 2
            self.ad  = factor1   # Anxious-Depression score
            self.cit = factor2   # Compulsive/Intrusive Thoughts score
            self.sw  = factor3   # Social Withdrawal score
            self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            self.p_choice_1  = np.zeros(n_trials)
            self.p_choice_2  = np.zeros(n_trials)
            self.q_stage1    = np.zeros(self.n_choices)
            self.q_stage2    = np.zeros((self.n_states, self.n_choices))
            self.trial = 0
            self.rt1 = None
            self.rt2 = None
            self.rt1_array = None
            self.rt2_array = None
            self.last_action1 = self.last_action2 = None
            self.last_state   = self.last_reward  = None
            self.unpack_parameters(model_parameters)
            self.init_model()

        @abstractmethod
        def unpack_parameters(self, model_parameters): pass

        def init_model(self): pass
        def policy_stage1(self): return self.softmax(self.q_stage1, self.beta)
        def policy_stage2(self, state): return self.softmax(self.q_stage2[state], self.beta)

        def value_update(self, a1, state, a2, reward):
            d2 = reward - self.q_stage2[state, a2]
            self.q_stage2[state, a2] += self.alpha * d2
            d1 = self.q_stage2[state, a2] - self.q_stage1[a1]
            self.q_stage1[a1] += self.alpha * d1

        def compute_rt_nll(self):
            """Shifted Wald NLL using constant participant-level drift v1."""
            if self.rt1_array is None:
                return 0.0
            dt = np.maximum(self.rt1_array / 1000.0 - self.t0, 1e-5)
            # v1 is constant across trials — participant-level speed parameter
            v  = max(self.v1, 1e-3)
            # Wald: f(t; v, a) = a/sqrt(2π t³) * exp(-(a - v*t)²/(2t))
            log_lik = (np.log(self.a_bound)
                       - 0.5 * np.log(2.0 * np.pi)
                       - 1.5 * np.log(dt)
                       - (self.a_bound - v * dt) ** 2 / (2.0 * dt))
            return -np.sum(log_lik)

        def pre_trial(self): pass
        def post_trial(self, a1, state, a2, reward):
            self.last_action1 = a1;  self.last_action2 = a2
            self.last_state   = state; self.last_reward = reward

        def run_model(self, action_1, state, action_2, reward, rt1=None, rt2=None):
            self.rt1_array = rt1
            self.rt2_array = rt2
            for self.trial in range(self.n_trials):
                a1 = int(action_1[self.trial]); s = int(state[self.trial])
                a2 = int(action_2[self.trial]); r = float(reward[self.trial])
                self.rt1 = float(rt1[self.trial]) if rt1 is not None else None
                self.rt2 = float(rt2[self.trial]) if rt2 is not None else None
                self.pre_trial()
                self.p_choice_1[self.trial] = self.policy_stage1()[a1]
                self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
                self.value_update(a1, s, a2, r)
                self.post_trial(a1, s, a2, r)
            return self.compute_nll()

        def compute_nll(self):
            eps = 1e-12
            nll_choice = -(np.sum(np.log(self.p_choice_1 + eps))
                           + np.sum(np.log(self.p_choice_2 + eps)))
            nll_rt = self.compute_rt_nll()
            return nll_choice + nll_rt

        def softmax(self, values, beta):
            c = values - np.max(values)
            e = np.exp(beta * c)
            return e / np.sum(e)

    def make_cognitive_model(ModelClass):
        def cognitive_model(action_1, state, action_2, reward,
                            stage_1_rt, stage_2_rt,
                            factor1, factor2, factor3, model_parameters):
            n_trials = len(action_1)
            f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
            f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
            f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
            model = ModelClass(n_trials, f1, f2, f3, model_parameters)
            return model.run_model(action_1, state, action_2, reward, stage_1_rt, stage_2_rt)
        return cognitive_model
    ```

  template_model: |
    ```python
    class ParticipantModel1(CognitiveModelBase):
        """
        [HYPOTHESIS: describe the RL component and how psychiatric factors modulate
        the constant drift rate v1, boundary a_bound, or non-decision time t0]

        The drift rate v1 is a CONSTANT participant-level parameter — it does not vary
        trial-to-trial with Q-values. Psychiatric factors may scale v1, a_bound, or t0.

        Parameter Bounds:
        -----------------
        alpha:    [0, 1]      — learning rate
        beta:     [0, 10]     — softmax inverse temperature
        v1:       [0.1, 5.0]  — participant-level stage-1 evidence accumulation speed
        a_bound:  [0.5, 5.0]  — DDM boundary separation (caution / speed-accuracy tradeoff)
        t0:       [0.05, 0.5] — non-decision time (seconds)
        [Add up to 2 more parameters with bounds if needed]
        """
        def unpack_parameters(self, model_parameters):
            self.alpha, self.beta, self.v1, self.a_bound, self.t0 = model_parameters

        # Override compute_rt_nll() to modulate DDM parameters with psychiatric factors.
        # The result of any modulation MUST be positive — use multiplicative scaling:
        #
        #   Safe:   effective_v = self.v1 * np.exp(self.gamma * self.cit)
        #           effective_a = self.a_bound * np.exp(self.gamma * self.ad)
        #
        #   Unsafe: self.v1 * (1 - 0.3 * self.cit)  ← CAN GO NEGATIVE
        #
        # Using exp() keeps the parameter positive for any real value of gamma and factor score.
        # gamma can be positive or negative (exp(negative) → shrinks but never reaches zero).

        # Override any combination of:
        # - init_model()    : additional state variables
        # - policy_stage1() : stage-1 choice policy
        # - policy_stage2() : stage-2 choice policy
        # - value_update()  : learning rule
        # - pre_trial()     : per-trial setup
        # - post_trial()    : per-trial bookkeeping

    cognitive_model1 = make_cognitive_model(ParticipantModel1)
    ```

  simulation_template: |
    # RT is not simulated — simulation generates choice trajectories only.
    def simulate_model(ModelClass, n_trials, factor1, factor2, factor3,
                       model_parameters, drift1, drift2, drift3, drift4, seed=None):
        rng = np.random.default_rng(seed)
        f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
        f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
        f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
        model = ModelClass(n_trials, f1, f2, f3, model_parameters)
        action_1 = np.zeros(n_trials, dtype=int)
        state    = np.zeros(n_trials, dtype=int)
        action_2 = np.zeros(n_trials, dtype=int)
        reward   = np.zeros(n_trials, dtype=int)
        for t in range(n_trials):
            model.trial = t
            model.rt1 = None; model.rt2 = None
            model.pre_trial()
            p1 = np.clip(model.policy_stage1(), 1e-12, 1.0); p1 /= p1.sum()
            a1 = int(rng.choice([0, 1], p=p1))
            pT = np.clip(model.T[a1], 1e-12, 1.0); pT /= pT.sum()
            s  = int(rng.choice([0, 1], p=pT))
            p2 = np.clip(model.policy_stage2(s), 1e-12, 1.0); p2 /= p2.sum()
            a2 = int(rng.choice([0, 1], p=p2))
            if s == 0:
                pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
            else:
                pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
            r = int(rng.random() < pr)
            model.p_choice_1[t] = p1[a1]; model.p_choice_2[t] = p2[a2]
            model.value_update(a1, s, a2, float(r))
            model.post_trial(a1, s, a2, float(r))
            action_1[t] = a1; state[t] = s; action_2[t] = a2; reward[t] = r
        return action_1, state, action_2, reward

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "group"

feedback:
  type: "manual"
