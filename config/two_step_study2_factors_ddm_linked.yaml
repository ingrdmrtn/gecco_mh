loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1

task:
  name: "two_step_study2_ddm_linked"
  description: |
    Participants played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred.
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participants asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.
    Reaction times (in milliseconds) were recorded at both decision stages.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python classes:
    {model_names}

metadata:
  flag: true
  description: |
    Participants were assessed on a large battery of psychiatric questionnaires.
    Three transdiagnostic symptom factors were derived via factor analysis:
      - AD  (Anxious-Depression): elevated scores reflect anxiety and depressive symptoms
      - CIT (Compulsive/Intrusive Thoughts): elevated scores reflect obsessive-compulsive and intrusive thought symptoms
      - SW  (Social Withdrawal): elevated scores reflect social avoidance and apathy
    Scores are standardized (mean=0, SD≈1).
  narrative_template: |
    Participant {participant_id} — AD: {factor1:.2f}, CIT: {factor2:.2f}, SW: {factor3:.2f}

data:
  max_prompt_trials: 50
  path: "data_g_2019/twostep_study2_preprocessed.csv"  # placeholder
  id_column: "participant"
  input_columns:
    ["choice_1", "state", "choice_2", "reward",
     "stage_1_rt", "stage_2_rt",
     "factor1", "factor2", "factor3"]
  simulation_columns:
    ["ModelClass", "n_trials", "factor1", "factor2", "factor3",
     "model_parameters", "drift1", "drift2", "drift3", "drift4"]
  data2text_function: "narrative"
  simulation_return: ["stage1_choice", "state2", "stage2_choice", "reward"]

  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.
    Stage 1 RT: {stage_1_rt} ms. Stage 2 RT: {stage_2_rt} ms.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

  splits:
    prompt: "[1:4]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.7
  reasoning_effort: "low"
  max_tokens: null
  max_output_tokens: null
  text_verbosity: null
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task along with
    three transdiagnostic psychiatric symptom scores and reaction times.
    Your task is to propose models where:
    (1) choices are governed by reinforcement learning (Q-values, MB/MF weighting, etc.)
    (2) reaction times are governed by a shifted Wald (inverse Gaussian) accumulator
    (3) the RT drift rate v is derived directly from the Q-value difference —
        the SAME value signal that drives choice also determines how quickly evidence accumulates.
    (4) psychiatric symptom scores modulate either the RL or the DDM component.

  models_per_iteration: 3
  include_feedback: true

  guardrails:
    - "Each model must inherit from `CognitiveModelBase`."
    - "Class names must be `ParticipantModel1`, `ParticipantModel2`, etc."
    - "Each class must end with: `cognitive_modelX = make_cognitive_model(ParticipantModelX)`"
    - "Constructor receives: `n_trials, factor1, factor2, factor3, model_parameters`."
    - "The NLL combines choice NLL and stage-1 RT NLL via `compute_nll()` — do not override `compute_nll` directly."
    - "Override `compute_drift_stage1()` to specify how Q-values map to the drift rate v. Must return a positive scalar."
    - "The drift rate MUST always be positive. Use the form: v_t = v_base + v_scale * |Q_diff|. v_base ensures a non-zero drift even when Q-values are equal (i.e. when the participant is uncertain). This is required — never let drift reach zero."
    - "v_base bounds: [0.1, 5.0]. v_scale bounds: [0.01, 10.0]. The sum v_base + v_scale * |Q_diff| must always be > 0."
    - "Override `compute_rt_nll()` only if you want to customize beyond the default shifted Wald."
    - "a_bound (boundary separation) and t0 (non-decision time, seconds) must be free parameters."
    - "t0 bounds: [0.05, 0.5] seconds (50–500 ms). a_bound bounds: [0.5, 5.0]."
    - "Use at least one of `self.ad`, `self.cit`, `self.sw` to modulate the RL or DDM component."
    - "Do NOT include `import` statements — numpy is available as `np`."
    - "Total free parameters must not exceed 7. Note: v_base + v_scale already count as 2 DDM params alongside a_bound and t0, leaving 3 slots for alpha, beta, and one additional parameter."

  diversity_requirement:
    - "Each of your {models_per_iteration} models must differ in HOW Q-values map to drift rate. Consider:"
    - "Pure MF: v ∝ |Q_stage1[a1] - Q_stage1[1-a1]| — only model-free Q-difference drives RT"
    - "Pure MB: v ∝ |Q_MB[a1] - Q_MB[1-a1]| where Q_MB = transition_matrix @ max(Q_stage2)"
    - "Hybrid: v ∝ w·v_MB + (1-w)·v_MF — same w that weights MB/MF in choice also weights RT"
    - "Consider also how a psychiatric factor modulates the drift-choice link:"
    - "e.g. high CIT → reduced v_scale (slower deliberation independent of value certainty)"
    - "e.g. high AD → larger a_bound (more cautious, sets higher evidence threshold)"
    - "e.g. high SW → lower t0 (less processing time before decision)"

  abstract_base_model: |
    ``` python
    from abc import ABC, abstractmethod
    class CognitiveModelBase(ABC):
        def __init__(self, n_trials, factor1, factor2, factor3, model_parameters):
            self.n_trials  = n_trials
            self.n_choices = 2
            self.n_states  = 2
            self.ad  = factor1   # Anxious-Depression score
            self.cit = factor2   # Compulsive/Intrusive Thoughts score
            self.sw  = factor3   # Social Withdrawal score
            self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            self.p_choice_1  = np.zeros(n_trials)
            self.p_choice_2  = np.zeros(n_trials)
            self.v1_array    = np.zeros(n_trials)  # trial-wise stage-1 drift rates
            self.q_stage1    = np.zeros(self.n_choices)
            self.q_stage2    = np.zeros((self.n_states, self.n_choices))
            self.trial = 0
            self.rt1 = None   # stage-1 RT for current trial (ms)
            self.rt2 = None   # stage-2 RT for current trial (ms)
            self.rt1_array = None
            self.rt2_array = None
            self.last_action1 = self.last_action2 = None
            self.last_state   = self.last_reward  = None
            self.unpack_parameters(model_parameters)
            self.init_model()

        @abstractmethod
        def unpack_parameters(self, model_parameters): pass

        def init_model(self): pass
        def policy_stage1(self): return self.softmax(self.q_stage1, self.beta)
        def policy_stage2(self, state): return self.softmax(self.q_stage2[state], self.beta)

        def value_update(self, a1, state, a2, reward):
            d2 = reward - self.q_stage2[state, a2]
            self.q_stage2[state, a2] += self.alpha * d2
            d1 = self.q_stage2[state, a2] - self.q_stage1[a1]
            self.q_stage1[a1] += self.alpha * d1

        def compute_drift_stage1(self):
            """Compute stage-1 drift rate from current Q-values. Override this.
            MUST return a value > 0. Use v_base + v_scale * |Q_diff| as the base form.
            v_base guarantees a non-zero drift even when Q-values are equal."""
            q_diff = self.q_stage1[0] - self.q_stage1[1]
            return self.v_base + self.v_scale * abs(q_diff)

        def compute_rt_nll(self):
            """Shifted Wald (inverse Gaussian) NLL for stage-1 RT."""
            if self.rt1_array is None:
                return 0.0
            dt = np.maximum(self.rt1_array / 1000.0 - self.t0, 1e-5)
            v  = np.maximum(self.v1_array, 1e-3)
            # Wald: f(t; v, a) = a/sqrt(2π t³) * exp(-(a - v*t)²/(2t))
            log_lik = (np.log(self.a_bound)
                       - 0.5 * np.log(2.0 * np.pi)
                       - 1.5 * np.log(dt)
                       - (self.a_bound - v * dt) ** 2 / (2.0 * dt))
            return -np.sum(log_lik)

        def pre_trial(self): pass
        def post_trial(self, a1, state, a2, reward):
            self.last_action1 = a1;  self.last_action2 = a2
            self.last_state   = state; self.last_reward = reward

        def run_model(self, action_1, state, action_2, reward, rt1=None, rt2=None):
            self.rt1_array = rt1
            self.rt2_array = rt2
            for self.trial in range(self.n_trials):
                a1 = int(action_1[self.trial]); s = int(state[self.trial])
                a2 = int(action_2[self.trial]); r = float(reward[self.trial])
                self.rt1 = float(rt1[self.trial]) if rt1 is not None else None
                self.rt2 = float(rt2[self.trial]) if rt2 is not None else None
                self.pre_trial()
                # Store drift BEFORE choice — v computed from pre-decision Q-values
                self.v1_array[self.trial] = self.compute_drift_stage1()
                self.p_choice_1[self.trial] = self.policy_stage1()[a1]
                self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
                self.value_update(a1, s, a2, r)
                self.post_trial(a1, s, a2, r)
            return self.compute_nll()

        def compute_nll(self):
            eps = 1e-12
            nll_choice = -(np.sum(np.log(self.p_choice_1 + eps))
                           + np.sum(np.log(self.p_choice_2 + eps)))
            nll_rt = self.compute_rt_nll()
            return nll_choice + nll_rt

        def softmax(self, values, beta):
            c = values - np.max(values)
            e = np.exp(beta * c)
            return e / np.sum(e)

    def make_cognitive_model(ModelClass):
        def cognitive_model(action_1, state, action_2, reward,
                            stage_1_rt, stage_2_rt,
                            factor1, factor2, factor3, model_parameters):
            n_trials = len(action_1)
            f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
            f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
            f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
            model = ModelClass(n_trials, f1, f2, f3, model_parameters)
            return model.run_model(action_1, state, action_2, reward, stage_1_rt, stage_2_rt)
        return cognitive_model
    ```

  template_model: |
    ```python
    class ParticipantModel1(CognitiveModelBase):
        """
        [HYPOTHESIS: describe the RL mechanism, how Q-values map to drift rate, and how a
        psychiatric factor modulates the RL or DDM component]

        The drift rate v is derived from Q-values each trial via compute_drift_stage1().
        The same value signal that determines choice probability also determines RT speed.

        Parameter Bounds:
        -----------------
        alpha:    [0, 1]      — learning rate
        beta:     [0, 10]     — softmax inverse temperature
        v_base:   [0.1, 5.0]  — baseline drift when Q-values are equal (must be > 0)
        v_scale:  [0.01, 10]  — scales Q-diff to add on top of v_base
        a_bound:  [0.5, 5.0]  — DDM boundary separation (caution)
        t0:       [0.05, 0.5] — non-decision time (seconds)
        [Add 1 more parameter with bounds if needed]
        """
        def unpack_parameters(self, model_parameters):
            self.alpha, self.beta, self.v_base, self.v_scale, self.a_bound, self.t0 = model_parameters

        def compute_drift_stage1(self):
            # Override to change how Q-values map to drift rate.
            # MUST return a positive scalar — always v_base + v_scale * |something|.
            # Available: self.q_stage1, self.q_stage2, self.T, self.ad, self.cit, self.sw
            #
            # Example — MF drift:
            #   q_diff = self.q_stage1[0] - self.q_stage1[1]
            #   return self.v_base + self.v_scale * abs(q_diff)
            #
            # Example — hybrid MB+MF drift (same w as choice policy):
            #   q_mb = self.T @ np.max(self.q_stage2, axis=1)
            #   v_mb = abs(q_mb[0] - q_mb[1])
            #   v_mf = abs(self.q_stage1[0] - self.q_stage1[1])
            #   return self.v_base + self.v_scale * (self.w * v_mb + (1 - self.w) * v_mf)
            pass

        # Override any combination of:
        # - init_model()    : additional state variables
        # - policy_stage1() : stage-1 choice policy
        # - policy_stage2() : stage-2 choice policy
        # - value_update()  : learning rule
        # - pre_trial()     : per-trial setup
        # - post_trial()    : per-trial bookkeeping
        # Do NOT override compute_nll() or compute_rt_nll() unless extending the RT model.

    cognitive_model1 = make_cognitive_model(ParticipantModel1)
    ```

  simulation_template: |
    # RT is not simulated — simulation generates choice trajectories only.
    def simulate_model(ModelClass, n_trials, factor1, factor2, factor3,
                       model_parameters, drift1, drift2, drift3, drift4, seed=None):
        rng = np.random.default_rng(seed)
        f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
        f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
        f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
        model = ModelClass(n_trials, f1, f2, f3, model_parameters)
        action_1 = np.zeros(n_trials, dtype=int)
        state    = np.zeros(n_trials, dtype=int)
        action_2 = np.zeros(n_trials, dtype=int)
        reward   = np.zeros(n_trials, dtype=int)
        for t in range(n_trials):
            model.trial = t
            model.rt1 = None; model.rt2 = None
            model.pre_trial()
            p1 = np.clip(model.policy_stage1(), 1e-12, 1.0); p1 /= p1.sum()
            a1 = int(rng.choice([0, 1], p=p1))
            pT = np.clip(model.T[a1], 1e-12, 1.0); pT /= pT.sum()
            s  = int(rng.choice([0, 1], p=pT))
            p2 = np.clip(model.policy_stage2(s), 1e-12, 1.0); p2 /= p2.sum()
            a2 = int(rng.choice([0, 1], p=p2))
            if s == 0:
                pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
            else:
                pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
            r = int(rng.random() < pr)
            model.p_choice_1[t] = p1[a1]; model.p_choice_2[t] = p2[a2]
            model.value_update(a1, s, a2, float(r))
            model.post_trial(a1, s, a2, float(r))
            action_1[t] = a1; state[t] = s; action_2[t] = a2; reward[t] = r
        return action_1, state, action_2, reward

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "group"

feedback:
  type: "manual"
