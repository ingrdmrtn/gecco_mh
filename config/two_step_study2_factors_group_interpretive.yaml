loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1

task:
  name: "two_step_study2_transdiagnostic_interpretive"
  description: |
    Participants played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred.
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participants asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python classes:
    {model_names}

metadata:
  flag: true
  description: |
    Participants completed 9 psychiatric questionnaires: OCI (obsessive-compulsive), STAI (anxiety),
    SDS (depression), LSAS (social anxiety), BIS (impulsivity), SCZ (schizotypy),
    AES (apathy), EAT (eating attitudes), AUDIT (alcohol use).
    Three latent symptom dimensions were extracted via factor analysis across all questionnaire items.
    These factors are NOT pre-labeled — their psychological interpretation is for you to infer
    from the task data and your knowledge of the underlying questionnaires.
    Scores are standardized (mean=0, SD≈1).
  narrative_template: |
    Participant {participant_id} — F1: {factor1:.2f}, F2: {factor2:.2f}, F3: {factor3:.2f}

data:
  max_prompt_trials: 50
  path: "data_g_2019/twostep_study2_preprocessed.csv"  # placeholder — update when Study 2 task data is collected
  id_column: "participant"
  input_columns: ["choice_1", "state", "choice_2", "reward", "factor1", "factor2", "factor3"]
  simulation_columns:
    ["ModelClass", "n_trials", "factor1", "factor2", "factor3",
     "model_parameters", "drift1", "drift2", "drift3", "drift4"]
  data2text_function: "narrative"
  simulation_return: ["stage1_choice", "state2", "stage2_choice", "reward"]

  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

  splits:
    prompt: "[1:4]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.7
  reasoning_effort: "low"
  max_tokens: null
  max_output_tokens: null
  text_verbosity: null
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task along with
    three unlabeled psychiatric symptom dimensions derived from factor analysis.
    Your job is to (1) interpret what each factor likely represents based on the questionnaires
    it was derived from, and (2) propose cognitive models that explain how those symptom
    dimensions mechanistically shape decision-making behavior.

  models_per_iteration: 3
  include_feedback: true

  guardrails:
    - "Each model must inherit from `CognitiveModelBase`."
    - "Class names must be `ParticipantModel1`, `ParticipantModel2`, etc."
    - "Each class must end with: `cognitive_modelX = make_cognitive_model(ParticipantModelX)`"
    - "Constructor receives: `n_trials, factor1, factor2, factor3, model_parameters`."
    - "Return the **negative log-likelihood** of choices via `run_model()`."
    - "Use all free parameters meaningfully (no unused params)."
    - "The model docstring MUST include: (a) your interpretation of each factor used (e.g. 'F1 likely reflects compulsivity based on high OCI loading'), and (b) the cognitive hypothesis linking that factor to the mechanism."
    - "Parameter bounds: [0,1] for most; [0,10] for softmax `beta` (inverse temperature)."
    - "Each model MUST use at least one of `self.f1`, `self.f2`, `self.f3` to modulate behavior."
    - "Do NOT include `import` statements — numpy is available as `np`."
    - "Override only the methods you need to customize."
    - "Total free parameters (including alpha and beta) must not exceed 6."

  diversity_requirement:
    - "Each of your {models_per_iteration} models must: (1) use a **different factor** as the primary modulator, and (2) propose a **different interpretation** of that factor's psychological content."
    - "Consider what each factor might represent given the questionnaire battery (OCI, STAI, SDS, LSAS, BIS, SCZ, AES, EAT, AUDIT) and how that construct could plausibly affect:"
    - "Model-based vs model-free weighting (goal-directed vs habitual control)"
    - "Learning rates for rewarded vs unrewarded outcomes (asymmetric updating)"
    - "Inverse temperature / exploration-exploitation tradeoff"
    - "Perseveration or switching tendencies"
    - "Variance in the chosen mechanism (e.g. stochastic vs deterministic updating)"

  abstract_base_model: |
    ``` python
    from abc import ABC, abstractmethod
    class CognitiveModelBase(ABC):
        def __init__(self, n_trials, factor1, factor2, factor3, model_parameters):
            self.n_trials  = n_trials
            self.n_choices = 2
            self.n_states  = 2
            self.f1 = factor1   # Unlabeled symptom dimension 1 — interpret in your docstring
            self.f2 = factor2   # Unlabeled symptom dimension 2 — interpret in your docstring
            self.f3 = factor3   # Unlabeled symptom dimension 3 — interpret in your docstring
            self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            self.p_choice_1 = np.zeros(n_trials)
            self.p_choice_2 = np.zeros(n_trials)
            self.q_stage1   = np.zeros(self.n_choices)
            self.q_stage2   = np.zeros((self.n_states, self.n_choices))
            self.trial = 0
            self.last_action1 = self.last_action2 = None
            self.last_state   = self.last_reward  = None
            self.unpack_parameters(model_parameters)
            self.init_model()

        @abstractmethod
        def unpack_parameters(self, model_parameters): pass

        def init_model(self): pass
        def policy_stage1(self): return self.softmax(self.q_stage1, self.beta)
        def policy_stage2(self, state): return self.softmax(self.q_stage2[state], self.beta)

        def value_update(self, a1, state, a2, reward):
            d2 = reward - self.q_stage2[state, a2]
            self.q_stage2[state, a2] += self.alpha * d2
            d1 = self.q_stage2[state, a2] - self.q_stage1[a1]
            self.q_stage1[a1] += self.alpha * d1

        def pre_trial(self): pass
        def post_trial(self, a1, state, a2, reward):
            self.last_action1 = a1;  self.last_action2 = a2
            self.last_state   = state; self.last_reward = reward

        def run_model(self, action_1, state, action_2, reward):
            for self.trial in range(self.n_trials):
                a1 = int(action_1[self.trial]); s = int(state[self.trial])
                a2 = int(action_2[self.trial]); r = float(reward[self.trial])
                self.pre_trial()
                self.p_choice_1[self.trial] = self.policy_stage1()[a1]
                self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
                self.value_update(a1, s, a2, r)
                self.post_trial(a1, s, a2, r)
            return self.compute_nll()

        def compute_nll(self):
            eps = 1e-12
            return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))

        def softmax(self, values, beta):
            c = values - np.max(values)
            e = np.exp(beta * c)
            return e / np.sum(e)

    def make_cognitive_model(ModelClass):
        def cognitive_model(action_1, state, action_2, reward,
                            factor1, factor2, factor3, model_parameters):
            n_trials = len(action_1)
            f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
            f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
            f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
            model = ModelClass(n_trials, f1, f2, f3, model_parameters)
            return model.run_model(action_1, state, action_2, reward)
        return cognitive_model
    ```

  template_model: |
    ```python
    class ParticipantModel1(CognitiveModelBase):
        """
        Factor interpretation:
          F1: [YOUR INTERPRETATION — e.g. 'likely reflects compulsivity given high OCI/SCZ loading']
          F2: [YOUR INTERPRETATION — e.g. 'likely reflects anxious-depression given high STAI/SDS loading']
          F3: [YOUR INTERPRETATION — e.g. 'likely reflects social withdrawal given high LSAS/AES loading']

        Cognitive hypothesis:
          [State which factor modulates which mechanism, and the direction/form of the effect]

        Parameter Bounds:
        -----------------
        alpha: [0, 1]   — base learning rate
        beta:  [0, 10]  — inverse temperature / exploitation
        [Add up to 4 more parameters with bounds if needed]
        """
        def unpack_parameters(self, model_parameters):
            self.alpha, self.beta = model_parameters  # extend as needed

        # Available: self.f1, self.f2, self.f3 (standardized, ~N(0,1))
        # Override any combination of:
        # - init_model()     : additional state variables
        # - policy_stage1()  : stage-1 action selection
        # - policy_stage2()  : stage-2 action selection
        # - value_update()   : learning rule
        # - pre_trial()      : computations before each trial
        # - post_trial()     : computations after each trial

    cognitive_model1 = make_cognitive_model(ParticipantModel1)
    ```

  simulation_template: |
    def simulate_model(ModelClass, n_trials, factor1, factor2, factor3,
                       model_parameters, drift1, drift2, drift3, drift4, seed=None):
        rng = np.random.default_rng(seed)
        f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
        f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
        f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
        model = ModelClass(n_trials, f1, f2, f3, model_parameters)
        action_1 = np.zeros(n_trials, dtype=int)
        state    = np.zeros(n_trials, dtype=int)
        action_2 = np.zeros(n_trials, dtype=int)
        reward   = np.zeros(n_trials, dtype=int)
        for t in range(n_trials):
            model.trial = t
            model.pre_trial()
            p1 = np.clip(model.policy_stage1(), 1e-12, 1.0); p1 /= p1.sum()
            a1 = int(rng.choice([0, 1], p=p1))
            pT = np.clip(model.T[a1], 1e-12, 1.0); pT /= pT.sum()
            s  = int(rng.choice([0, 1], p=pT))
            p2 = np.clip(model.policy_stage2(s), 1e-12, 1.0); p2 /= p2.sum()
            a2 = int(rng.choice([0, 1], p=p2))
            if s == 0:
                pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
            else:
                pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
            r = int(rng.random() < pr)
            model.p_choice_1[t] = p1[a1]; model.p_choice_2[t] = p2[a2]
            model.value_update(a1, s, a2, float(r))
            model.post_trial(a1, s, a2, float(r))
            action_1[t] = a1; state[t] = s; action_2[t] = a2; reward[t] = r
        return action_1, state, action_2, reward

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "group"

feedback:
  type: "manual"
