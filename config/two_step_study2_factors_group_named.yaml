loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1

task:
  name: "two_step_study2_transdiagnostic_named"
  description: |
    Participants played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred.
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participants asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python classes:
    {model_names}

metadata:
  flag: true
  description: |
    Participants were assessed on a large battery of psychiatric questionnaires.
    Three transdiagnostic symptom factors were derived via factor analysis:
      - AD  (Anxious-Depression): elevated scores reflect anxiety and depressive symptoms
      - CIT (Compulsive/Intrusive Thoughts): elevated scores reflect obsessive-compulsive and intrusive thought symptoms
      - SW  (Social Withdrawal): elevated scores reflect social avoidance and apathy
    Scores are standardized (mean=0, SD≈1). Positive scores are above average, negative scores are below.
  narrative_template: |
    Participant {participant_id} — AD: {factor1:.2f}, CIT: {factor2:.2f}, SW: {factor3:.2f}

data:
  max_prompt_trials: 50
  path: "data_g_2019/twostep_study2_preprocessed.csv"  # placeholder — update when Study 2 task data is collected
  id_column: "participant"
  input_columns: ["choice_1", "state", "choice_2", "reward", "factor1", "factor2", "factor3"]
  simulation_columns:
    ["ModelClass", "n_trials", "factor1", "factor2", "factor3",
     "model_parameters", "drift1", "drift2", "drift3", "drift4"]
  data2text_function: "narrative"
  simulation_return: ["stage1_choice", "state2", "stage2_choice", "reward"]

  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

  splits:
    prompt: "[1:4]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.7
  reasoning_effort: "low"
  max_tokens: null
  max_output_tokens: null
  text_verbosity: null
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task along with
    three transdiagnostic psychiatric symptom scores: Anxious-Depression (AD),
    Compulsive/Intrusive Thoughts (CIT), and Social Withdrawal (SW).
    Your job is to propose cognitive models that explain how participants make decisions
    and how their symptom profile mechanistically shapes those decisions.

  models_per_iteration: 3
  include_feedback: true

  guardrails:
    - "Each model must inherit from `CognitiveModelBase`."
    - "Class names must be `ParticipantModel1`, `ParticipantModel2`, etc."
    - "Each class must end with: `cognitive_modelX = make_cognitive_model(ParticipantModelX)`"
    - "Constructor receives: `n_trials, factor1, factor2, factor3, model_parameters`."
    - "Return the **negative log-likelihood** of choices via `run_model()`."
    - "Use all free parameters meaningfully (no unused params)."
    - "Include a docstring stating the cognitive hypothesis and all parameter bounds."
    - "Parameter bounds: [0,1] for most; [0,10] for softmax `beta` (inverse temperature)."
    - "Each model MUST use at least one of `self.ad`, `self.cit`, `self.sw` to modulate behavior."
    - "Do NOT include `import` statements — numpy is available as `np`."
    - "Override only the methods you need to customize."
    - "Total free parameters (including alpha and beta) must not exceed 6."

  diversity_requirement:
    - "Each of your {models_per_iteration} models must test a **different hypothesis** about how a specific symptom dimension shapes a specific decision mechanism. Consider:"
    - "CIT (compulsivity) modulating model-based vs model-free weighting — higher CIT → less goal-directed control"
    - "AD (anxiety/depression) modulating learning rate — higher AD → faster updating from negative outcomes"
    - "SW (social withdrawal) modulating exploration vs exploitation (beta) or perseveration"
    - "Whether a factor acts additively, multiplicatively, or as a sigmoid-scaled gate on a mechanism"
    - "Whether different factors affect stage 1 and stage 2 independently"
    - "Interactions between factors (e.g. combined AD×CIT effect on learning asymmetry)"

  abstract_base_model: |
    ``` python
    from abc import ABC, abstractmethod
    class CognitiveModelBase(ABC):
        def __init__(self, n_trials, factor1, factor2, factor3, model_parameters):
            self.n_trials  = n_trials
            self.n_choices = 2
            self.n_states  = 2
            self.ad  = factor1   # Anxious-Depression score
            self.cit = factor2   # Compulsive/Intrusive Thoughts score
            self.sw  = factor3   # Social Withdrawal score
            self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            self.p_choice_1 = np.zeros(n_trials)
            self.p_choice_2 = np.zeros(n_trials)
            self.q_stage1   = np.zeros(self.n_choices)
            self.q_stage2   = np.zeros((self.n_states, self.n_choices))
            self.trial = 0
            self.last_action1 = self.last_action2 = None
            self.last_state   = self.last_reward  = None
            self.unpack_parameters(model_parameters)
            self.init_model()

        @abstractmethod
        def unpack_parameters(self, model_parameters): pass

        def init_model(self): pass
        def policy_stage1(self): return self.softmax(self.q_stage1, self.beta)
        def policy_stage2(self, state): return self.softmax(self.q_stage2[state], self.beta)

        def value_update(self, a1, state, a2, reward):
            d2 = reward - self.q_stage2[state, a2]
            self.q_stage2[state, a2] += self.alpha * d2
            d1 = self.q_stage2[state, a2] - self.q_stage1[a1]
            self.q_stage1[a1] += self.alpha * d1

        def pre_trial(self): pass
        def post_trial(self, a1, state, a2, reward):
            self.last_action1 = a1;  self.last_action2 = a2
            self.last_state   = state; self.last_reward = reward

        def run_model(self, action_1, state, action_2, reward):
            for self.trial in range(self.n_trials):
                a1 = int(action_1[self.trial]); s = int(state[self.trial])
                a2 = int(action_2[self.trial]); r = float(reward[self.trial])
                self.pre_trial()
                self.p_choice_1[self.trial] = self.policy_stage1()[a1]
                self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
                self.value_update(a1, s, a2, r)
                self.post_trial(a1, s, a2, r)
            return self.compute_nll()

        def compute_nll(self):
            eps = 1e-12
            return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))

        def softmax(self, values, beta):
            c = values - np.max(values)
            e = np.exp(beta * c)
            return e / np.sum(e)

    def make_cognitive_model(ModelClass):
        def cognitive_model(action_1, state, action_2, reward,
                            factor1, factor2, factor3, model_parameters):
            n_trials = len(action_1)
            f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
            f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
            f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
            model = ModelClass(n_trials, f1, f2, f3, model_parameters)
            return model.run_model(action_1, state, action_2, reward)
        return cognitive_model
    ```

  template_model: |
    ```python
    class ParticipantModel1(CognitiveModelBase):
        """
        [HYPOTHESIS: State which symptom dimension (AD/CIT/SW) modulates which mechanism, and why]

        Parameter Bounds:
        -----------------
        alpha: [0, 1]   — base learning rate
        beta:  [0, 10]  — inverse temperature / exploitation
        [Add up to 4 more parameters with bounds if needed]

        Available attributes: self.ad, self.cit, self.sw (standardized factor scores)
        """
        def unpack_parameters(self, model_parameters):
            self.alpha, self.beta = model_parameters  # extend as needed

        # Override any combination of:
        # - init_model()     : additional state variables (e.g. self.w = ..., self.pers = 0)
        # - policy_stage1()  : stage-1 action selection
        # - policy_stage2()  : stage-2 action selection
        # - value_update()   : learning rule
        # - pre_trial()      : computations before each trial
        # - post_trial()     : computations after each trial (last_action1/2, last_state, last_reward available)

    cognitive_model1 = make_cognitive_model(ParticipantModel1)
    ```

  simulation_template: |
    def simulate_model(ModelClass, n_trials, factor1, factor2, factor3,
                       model_parameters, drift1, drift2, drift3, drift4, seed=None):
        rng = np.random.default_rng(seed)
        f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
        f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
        f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
        model = ModelClass(n_trials, f1, f2, f3, model_parameters)
        action_1 = np.zeros(n_trials, dtype=int)
        state    = np.zeros(n_trials, dtype=int)
        action_2 = np.zeros(n_trials, dtype=int)
        reward   = np.zeros(n_trials, dtype=int)
        for t in range(n_trials):
            model.trial = t
            model.pre_trial()
            p1 = np.clip(model.policy_stage1(), 1e-12, 1.0); p1 /= p1.sum()
            a1 = int(rng.choice([0, 1], p=p1))
            pT = np.clip(model.T[a1], 1e-12, 1.0); pT /= pT.sum()
            s  = int(rng.choice([0, 1], p=pT))
            p2 = np.clip(model.policy_stage2(s), 1e-12, 1.0); p2 /= p2.sum()
            a2 = int(rng.choice([0, 1], p=p2))
            if s == 0:
                pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
            else:
                pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
            r = int(rng.random() < pr)
            model.p_choice_1[t] = p1[a1]; model.p_choice_2[t] = p2[a2]
            model.value_update(a1, s, a2, float(r))
            model.post_trial(a1, s, a2, float(r))
            action_1[t] = a1; state[t] = s; action_2[t] = a2; reward[t] = r
        return action_1, state, action_2, reward

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "group"

feedback:
  type: "manual"
