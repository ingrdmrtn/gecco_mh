loop:
  early_stopping: "True"
  max_iterations: 10
  max_independent_runs: 1

task:
  name: "two_step_study2_transdiagnostic_named_rt"
  description: |
    Participants played a two-step decision task to collect as many gold coins as possible.
    On each trial, they chose between two spaceships (A or U),
    each probabilistically traveling to one of two planets (X or Y).
    Spaceship A commonly traveled to planet X, and spaceship U commonly traveled to planet Y. Sometimes, rare transitions occurred.
    Planet X had aliens W and S; Planet Y had aliens P and H.
    Once on a planet, participants asked one alien for gold; different aliens returned coins with varying probabilities which changed slowly over trials.
    Reaction times (in milliseconds) were recorded at both decision stages.

  goal: |
    Your task: Propose **{models_per_iteration} new cognitive models** as Python classes:
    {model_names}

metadata:
  flag: true
  description: |
    Participants were assessed on a large battery of psychiatric questionnaires.
    Three transdiagnostic symptom factors were derived via factor analysis:
      - AD  (Anxious-Depression): elevated scores reflect anxiety and depressive symptoms
      - CIT (Compulsive/Intrusive Thoughts): elevated scores reflect obsessive-compulsive and intrusive thought symptoms
      - SW  (Social Withdrawal): elevated scores reflect social avoidance and apathy
    Scores are standardized (mean=0, SD≈1). Positive scores are above average, negative scores are below.
  narrative_template: |
    Participant {participant_id} — AD: {factor1:.2f}, CIT: {factor2:.2f}, SW: {factor3:.2f}

data:
  max_prompt_trials: 50
  path: "data_g_2019/twostep_study2_preprocessed.csv"  # placeholder — update when Study 2 task data is collected
  id_column: "participant"
  input_columns:
    ["choice_1", "state", "choice_2", "reward",
     "stage_1_rt", "stage_2_rt",
     "factor1", "factor2", "factor3"]
  simulation_columns:
    ["ModelClass", "n_trials", "factor1", "factor2", "factor3",
     "model_parameters", "drift1", "drift2", "drift3", "drift4"]
  data2text_function: "narrative"
  # RT is an input to the cognitive model but is NOT simulated generatively.
  # Simulation returns only choice outcomes.
  simulation_return: ["stage1_choice", "state2", "stage2_choice", "reward"]

  narrative_template: |
    The participant chose spaceship {choice_1}, traveled to planet {state},
    asked alien {choice_2}, and received {reward} coins.
    Stage 1 RT: {stage_1_rt} ms. Stage 2 RT: {stage_2_rt} ms.

  value_mappings:
    choice_1:
      "0": "A"
      "1": "U"
    state:
      "0": "X"
      "1": "Y"
    choice_2:
      "0": "W"
      "1": "S"

  splits:
    prompt: "[1:4]"
    eval: "[4:14]"
    test: "[14:]"

llm:
  provider: "gemini"
  base_model: "gemini-3-pro-preview"
  temperature: 0.7
  reasoning_effort: "low"
  max_tokens: null
  max_output_tokens: null
  text_verbosity: null
  api_key: null
  do_simulation: "True"
  system_prompt: |
    You are a renowned cognitive scientist and expert Python programmer.
    You will be given participant data from a two-step decision-making task along with
    three transdiagnostic psychiatric symptom scores: Anxious-Depression (AD),
    Compulsive/Intrusive Thoughts (CIT), and Social Withdrawal (SW).
    Reaction times are provided at both decision stages as a trial-level signal.
    Your job is to propose cognitive models that explain how participants make decisions
    and how their symptom profile and deliberation speed mechanistically shape those decisions.
    Note: reaction time is provided as context — the NLL is computed over choices only.

  models_per_iteration: 3
  include_feedback: true

  guardrails:
    - "Each model must inherit from `CognitiveModelBase`."
    - "Class names must be `ParticipantModel1`, `ParticipantModel2`, etc."
    - "Each class must end with: `cognitive_modelX = make_cognitive_model(ParticipantModelX)`"
    - "Constructor receives: `n_trials, factor1, factor2, factor3, model_parameters`."
    - "Return the **negative log-likelihood** of choices via `run_model()` — NLL is over choices only, not RT."
    - "Use all free parameters meaningfully (no unused params)."
    - "Include a docstring stating the cognitive hypothesis and all parameter bounds."
    - "Parameter bounds: [0,1] for most; [0,10] for softmax `beta` (inverse temperature)."
    - "Each model MUST use at least one of `self.ad`, `self.cit`, `self.sw` to modulate behavior."
    - "RT is available each trial as `self.rt1` (stage 1, ms) and `self.rt2` (stage 2, ms). It is optional to use."
    - "If using RT, normalize it first (e.g. divide by mean RT, or use log transform) to avoid large-magnitude effects."
    - "Do NOT include `import` statements — numpy is available as `np`."
    - "Override only the methods you need to customize."
    - "Total free parameters (including alpha and beta) must not exceed 6."

  diversity_requirement:
    - "Each of your {models_per_iteration} models must test a **different hypothesis** about how symptom dimensions and/or reaction time shape decision mechanisms. Consider:"
    - "CIT modulating model-based vs model-free weighting — higher compulsivity → less goal-directed control"
    - "AD modulating learning rate — higher anxiety/depression → faster updating from negative outcomes"
    - "SW modulating perseveration or exploration"
    - "Stage 1 RT as a trial-level deliberation signal — slow RT trials may reflect more model-based computation"
    - "RT interacting with a psychiatric factor — e.g. high CIT participants show less RT-dependent modulation of MB control"
    - "RT modulating learning: reward prediction errors weighted differently on fast vs slow trials"

  abstract_base_model: |
    ``` python
    from abc import ABC, abstractmethod
    class CognitiveModelBase(ABC):
        def __init__(self, n_trials, factor1, factor2, factor3, model_parameters):
            self.n_trials  = n_trials
            self.n_choices = 2
            self.n_states  = 2
            self.ad  = factor1   # Anxious-Depression score
            self.cit = factor2   # Compulsive/Intrusive Thoughts score
            self.sw  = factor3   # Social Withdrawal score
            self.T = np.array([[0.7, 0.3], [0.3, 0.7]])
            self.p_choice_1 = np.zeros(n_trials)
            self.p_choice_2 = np.zeros(n_trials)
            self.q_stage1   = np.zeros(self.n_choices)
            self.q_stage2   = np.zeros((self.n_states, self.n_choices))
            self.trial = 0
            self.rt1 = None   # stage 1 RT for current trial (ms) — set by run_model
            self.rt2 = None   # stage 2 RT for current trial (ms) — set by run_model
            self.last_action1 = self.last_action2 = None
            self.last_state   = self.last_reward  = None
            self.unpack_parameters(model_parameters)
            self.init_model()

        @abstractmethod
        def unpack_parameters(self, model_parameters): pass

        def init_model(self): pass
        def policy_stage1(self): return self.softmax(self.q_stage1, self.beta)
        def policy_stage2(self, state): return self.softmax(self.q_stage2[state], self.beta)

        def value_update(self, a1, state, a2, reward):
            d2 = reward - self.q_stage2[state, a2]
            self.q_stage2[state, a2] += self.alpha * d2
            d1 = self.q_stage2[state, a2] - self.q_stage1[a1]
            self.q_stage1[a1] += self.alpha * d1

        def pre_trial(self): pass
        def post_trial(self, a1, state, a2, reward):
            self.last_action1 = a1;  self.last_action2 = a2
            self.last_state   = state; self.last_reward = reward

        def run_model(self, action_1, state, action_2, reward, rt1=None, rt2=None):
            for self.trial in range(self.n_trials):
                a1 = int(action_1[self.trial]); s = int(state[self.trial])
                a2 = int(action_2[self.trial]); r = float(reward[self.trial])
                self.rt1 = float(rt1[self.trial]) if rt1 is not None else None
                self.rt2 = float(rt2[self.trial]) if rt2 is not None else None
                self.pre_trial()
                self.p_choice_1[self.trial] = self.policy_stage1()[a1]
                self.p_choice_2[self.trial] = self.policy_stage2(s)[a2]
                self.value_update(a1, s, a2, r)
                self.post_trial(a1, s, a2, r)
            return self.compute_nll()

        def compute_nll(self):
            # NLL is over choices only — RT is an input signal, not a fitted outcome
            eps = 1e-12
            return -(np.sum(np.log(self.p_choice_1 + eps)) + np.sum(np.log(self.p_choice_2 + eps)))

        def softmax(self, values, beta):
            c = values - np.max(values)
            e = np.exp(beta * c)
            return e / np.sum(e)

    def make_cognitive_model(ModelClass):
        def cognitive_model(action_1, state, action_2, reward,
                            stage_1_rt, stage_2_rt,
                            factor1, factor2, factor3, model_parameters):
            n_trials = len(action_1)
            f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
            f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
            f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
            model = ModelClass(n_trials, f1, f2, f3, model_parameters)
            return model.run_model(action_1, state, action_2, reward, stage_1_rt, stage_2_rt)
        return cognitive_model
    ```

  template_model: |
    ```python
    class ParticipantModel1(CognitiveModelBase):
        """
        [HYPOTHESIS: State which symptom dimension (AD/CIT/SW) and/or RT modulates which mechanism]

        RT note: self.rt1 and self.rt2 are in milliseconds — normalize before use
        (e.g. self.rt1 / mean_rt, or np.log(self.rt1))

        Parameter Bounds:
        -----------------
        alpha: [0, 1]   — base learning rate
        beta:  [0, 10]  — inverse temperature / exploitation
        [Add up to 4 more parameters with bounds if needed]
        """
        def unpack_parameters(self, model_parameters):
            self.alpha, self.beta = model_parameters  # extend as needed

        # Available each trial: self.rt1 (stage 1 RT, ms), self.rt2 (stage 2 RT, ms)
        # Available always: self.ad, self.cit, self.sw (standardized factor scores)
        # Override any combination of:
        # - init_model()     : additional state variables (e.g. self.mean_rt for normalization)
        # - policy_stage1()  : stage-1 action selection
        # - policy_stage2()  : stage-2 action selection
        # - value_update()   : learning rule (self.rt1/rt2 available here)
        # - pre_trial()      : per-trial setup (e.g. compute normalized RT)
        # - post_trial()     : per-trial bookkeeping

    cognitive_model1 = make_cognitive_model(ParticipantModel1)
    ```

  simulation_template: |
    # NOTE: Simulation does not generate RT — RT is an observed input signal only.
    # The simulate_model function generates choice trajectories without RT.
    def simulate_model(ModelClass, n_trials, factor1, factor2, factor3,
                       model_parameters, drift1, drift2, drift3, drift4, seed=None):
        rng = np.random.default_rng(seed)
        f1 = float(factor1[0]) if hasattr(factor1, '__len__') else float(factor1)
        f2 = float(factor2[0]) if hasattr(factor2, '__len__') else float(factor2)
        f3 = float(factor3[0]) if hasattr(factor3, '__len__') else float(factor3)
        model = ModelClass(n_trials, f1, f2, f3, model_parameters)
        action_1 = np.zeros(n_trials, dtype=int)
        state    = np.zeros(n_trials, dtype=int)
        action_2 = np.zeros(n_trials, dtype=int)
        reward   = np.zeros(n_trials, dtype=int)
        for t in range(n_trials):
            model.trial = t
            model.rt1 = None; model.rt2 = None  # RT unavailable during simulation
            model.pre_trial()
            p1 = np.clip(model.policy_stage1(), 1e-12, 1.0); p1 /= p1.sum()
            a1 = int(rng.choice([0, 1], p=p1))
            pT = np.clip(model.T[a1], 1e-12, 1.0); pT /= pT.sum()
            s  = int(rng.choice([0, 1], p=pT))
            p2 = np.clip(model.policy_stage2(s), 1e-12, 1.0); p2 /= p2.sum()
            a2 = int(rng.choice([0, 1], p=p2))
            if s == 0:
                pr = float(drift1[t]) if a2 == 0 else float(drift2[t])
            else:
                pr = float(drift3[t]) if a2 == 0 else float(drift4[t])
            r = int(rng.random() < pr)
            model.p_choice_1[t] = p1[a1]; model.p_choice_2[t] = p2[a2]
            model.value_update(a1, s, a2, float(r))
            model.post_trial(a1, s, a2, float(r))
            action_1[t] = a1; state[t] = s; action_2[t] = a2; reward[t] = r
        return action_1, state, action_2, reward

evaluation:
  metric: "bic"
  optimizer: "L-BFGS-B"
  fit_type: "group"

feedback:
  type: "manual"
