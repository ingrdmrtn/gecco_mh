def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Working-Memory mixture with load-dependent WM weight and memory decay.

    Policy:
    - RL system: softmax over Q-values with inverse temperature beta.
    - WM system: one-shot memory that stores the last rewarded action for each state with a graded strength.
      The WM policy is a convex combination of a one-hot distribution over the memorized action and a uniform
      distribution, mixed by the current memory strength.
    - Final choice probability: mixture of RL and WM policies with a load-dependent WM weight.

    Learning and Memory:
    - RL: Rescorla-Wagner update with learning rate alpha.
    - WM: On rewarded trials (r == 1) store the chosen action for that state and set memory strength to 1.
          On every visit to a state, memory strength decays multiplicatively by (1 - wm_decay), bounded to [0,1].

    Parameters (model_parameters):
    - alpha (float): RL learning rate (0..1).
    - beta (float): Inverse temperature for the RL softmax (>0).
    - wm_weight_small (float): Mixture weight for WM in small set-size blocks (e.g., 3), in [0..1].
    - wm_weight_large (float): Mixture weight for WM in large set-size blocks (e.g., 6), in [0..1].
    - wm_decay (float): Per-visit decay of WM strength for the current state (0..1).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, wm_weight_small, wm_weight_large, wm_decay]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_weight_small, wm_weight_large, wm_decay = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        wm_w = wm_weight_small if nS == 3 else wm_weight_large

        Q = np.zeros((nS, nA))
        wm_action = -1 * np.ones(nS, dtype=int)        # stored action if any
        wm_strength = np.zeros(nS, dtype=float)        # strength in [0,1]

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)


            wm_strength[s] = max(0.0, min(1.0, wm_strength[s] * (1.0 - wm_decay)))

            p_wm = np.full(nA, 1.0 / nA)
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                one_hot = np.zeros(nA)
                one_hot[wm_action[s]] = 1.0

                p_wm = wm_strength[s] * one_hot + (1.0 - wm_strength[s]) * (1.0 / nA) * np.ones(nA)

            p = wm_w * p_wm + (1.0 - wm_w) * p_rl
            total_loglik += np.log(max(eps, p[a]))

            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            if r >= 1.0:
                wm_action[s] = a
                wm_strength[s] = 1.0

    return -float(total_loglik)