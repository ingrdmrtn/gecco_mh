def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, time-decaying working memory (WM) mixture model.

    Policy:
    - A mixture between model-free RL softmax and a WM policy.
    - WM has limited capacity (relative to set size) and decays over time.
    - When WM "remembers" a state, it suggests a noisy one-shot policy that favors the stored action.

    Parameters (model_parameters):
    - alpha (float): Learning rate for RL value updates in [0,1].
    - beta (float): Inverse temperature for RL softmax (choice stochasticity).
    - capacity (float): Effective WM capacity (in number of state-action pairs).
                         The WM weight scales as min(1, capacity / set_size).
    - decay (float): Time-based decay for WM strength each trial (0=no decay, 1=full forgetting).
    - lapse (float): Lapse/noise in WM policy; when WM is used, it chooses the stored action with prob 1-lapse,
                     and distributes lapse across the other actions.

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, capacity, decay, lapse]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, capacity, decay, lapse = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: action memory and strength per state
        wm_action = -1 * np.ones(nS, dtype=int)    # -1 means no memory for that state
        wm_strength = np.zeros(nS, dtype=float)    # in [0,1], decays over time

        # Capacity-based baseline WM weight for this block
        base_wm_weight = min(1.0, max(0.0, capacity / max(1, nS)))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            # Skip invalid trials (ensure indices and rewards make sense)
            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                # Still apply decay to WM even if skipping, to keep temporal meaning of 'decay'
                wm_strength *= (1.0 - decay)
                continue

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy for this state
            if wm_action[s] >= 0 and wm_strength[s] > 0:
                p_wm = np.full(nA, lapse / max(1, (nA - 1)))
                p_wm[wm_action[s]] = 1.0 - lapse
            else:
                p_wm = np.full(nA, 1.0 / nA)

            # Compute current WM weight (capacity scaled, modulated by current strength)
            w_s = base_wm_weight * wm_strength[s]
            w_s = min(1.0, max(0.0, w_s))

            # Mixture policy
            p_mix = w_s * p_wm + (1.0 - w_s) * p_rl
            p_a = max(eps, p_mix[a])
            total_loglik += np.log(p_a)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Time-based WM decay (global each trial)
            wm_strength *= (1.0 - decay)

            # WM update: encode rewarded action with full strength; otherwise do not overwrite,
            # but decay already applied above reduces influence of old memories.
            if r >= 0.5:  # treat as a correct association
                wm_action[s] = a
                wm_strength[s] = 1.0

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-specific learning rates, perseveration bias, and value forgetting.

    Policy:
    - Softmax over Q-values with inverse temperature beta.
    - Additive perseveration bias to the logit of the previously chosen action in the same state.

    Learning:
    - Learning rate depends on set size: alpha_small for set size 3, alpha_large for set size 6.
    - Per-trial value forgetting (shrinkage towards zero) with rate rho_forget.

    Parameters (model_parameters):
    - alpha_small (float): Learning rate used in 3-item blocks (0..1).
    - alpha_large (float): Learning rate used in 6-item blocks (0..1).
    - beta (float): Inverse temperature for softmax.
    - persev (float): Additive bias towards repeating the last action taken in the same state.
    - rho_forget (float): Per-trial forgetting rate applied to all Q-values of the current state (0..1).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha_small, alpha_large, beta, persev, rho_forget]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha_small, alpha_large, beta, persev, rho_forget = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        alpha = alpha_small if nS == 3 else alpha_large

        Q = np.zeros((nS, nA))
        prev_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            # Build logits with perseveration bias
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if prev_action[s] >= 0:
                logits[prev_action[s]] += persev

            # Softmax policy
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)
            total_loglik += np.log(max(eps, p[a]))

            # Forgetting/shrinkage towards zero for current state's Qs
            Q[s, :] *= (1.0 - rho_forget)

            # RL update on chosen action
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update perseveration memory
            prev_action[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian-like WM counts with uncertainty-gated arbitration.

    Policy:
    - Two controllers:
        1) RL: Softmax over Q-values with inverse temperature beta.
        2) WM: Predictive distribution derived from decaying counts for each state-action.
    - Arbitration weight is a sigmoid function of (capacity_k / set_size - uncertainty),
      where uncertainty is the entropy of WM's predictive distribution for the current state.

    Learning/Memory:
    - RL is updated with learning rate alpha.
    - WM counts decay each trial (for the visited state) with rate tied to wm_noise,
      then are incremented for rewarded actions.

    Parameters (model_parameters):
    - alpha (float): Learning rate for RL (0..1).
    - beta (float): Inverse temperature for RL softmax.
    - capacity_k (float): Capacity parameter controlling preference for WM in small set sizes.
    - gate_slope (float): Slope of the sigmoid gating (higher => sharper arbitration).
    - wm_noise (float): Controls both WM stochasticity and decay (0 => deterministic/non-decaying; larger => noisier/faster decay).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, capacity_k, gate_slope, wm_noise]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, capacity_k, gate_slope, wm_noise = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    def softmax(x):
        z = x - np.max(x)
        p = np.exp(z)
        return p / (np.sum(p) + eps)

    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM counts with symmetric Dirichlet(1) prior to avoid degeneracy
        counts = np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            # RL policy
            p_rl = softmax(beta * Q[s, :])

            # WM predictive distribution from counts with temperature tied to wm_noise
            # Lower wm_noise => sharper distribution. Use inverse temperature = 1 / (wm_noise + small)
            inv_temp = 1.0 / max(wm_noise + 1e-6, 1e-6)
            p_count = counts[s, :] / (np.sum(counts[s, :]) + eps)
            p_wm = softmax(inv_temp * np.log(np.clip(p_count, eps, 1.0)))

            # WM uncertainty and gating
            H = entropy(p_count)  # in [0, ln(nA)]
            # Capacity term favors WM in small set sizes; subtract uncertainty to favor confident WM
            gate_input = capacity_k / max(1, nS) - H
            w = sigmoid(gate_slope * gate_input)
            w = min(1.0, max(0.0, w))

            # Mixture policy
            p_mix = w * p_wm + (1.0 - w) * p_rl
            p_a = max(eps, p_mix[a])
            total_loglik += np.log(p_a)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM decay for the visited state: higher wm_noise => faster decay
            decay_rate = np.tanh(max(0.0, wm_noise))  # maps wm_noise to (0,1)
            counts[s, :] = (1.0 - decay_rate) * counts[s, :] + decay_rate * 1.0  # small pull towards prior=1

            # WM reinforcement: increment evidence for rewarded chosen action
            if r >= 0.5:
                counts[s, a] += 1.0

    return -float(total_loglik)