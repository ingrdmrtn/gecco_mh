def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Q-learning with set-size-modulated exploration bonus (UCB-style) and lapse.

    Policy:
    - Softmax over augmented action values: Q[s,a] + exploration_bonus[s,a]
    - Exploration bonus is larger for actions with fewer visits, and scales with set size.

    Learning:
    - Standard Rescorla-Wagner update on chosen action.

    Parameters (model_parameters):
    - alpha (float): Learning rate for Q-learning (0..1).
    - beta (float): Inverse temperature for softmax (>0).
    - kappa (float): Base magnitude of the exploration bonus (>=0).
    - omega_load (float): Set-size modulation of exploration; bonus is multiplied by (1 + omega_load * load),
                          where load = (set_size - 3) / 3 so that no-load baseline is 3.
                          Positive values increase exploration in larger sets.
    - lapse (float): Lapse probability added as uniform mixing with softmax probabilities (0..0.2 recommended).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen actions (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards (e.g., 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, kappa, omega_load, lapse]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, kappa, omega_load, lapse = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        load = (nS - 3) / 3.0  # 0 for set size 3, 1 for set size 6
        bonus_scale = max(0.0, 1.0 + omega_load * load)

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            # Skip invalid trials (e.g., negative actions/rewards in corrupted data)
            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            # UCB-style exploration bonus: larger for rarely chosen actions
            bonus = np.zeros(nA)
            for aa in range(nA):
                bonus[aa] = kappa * bonus_scale / np.sqrt(N[s, aa] + 1.0)

            logits = beta * (Q[s, :] + bonus - np.max(Q[s, :] + bonus))
            p_soft = np.exp(logits)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Lapse: mix with uniform
            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)
            total_loglik += np.log(max(eps, p[a]))

            # Update
            N[s, a] += 1.0
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Q-learning with lag-dependent perseveration and within-state novelty bonus.

    Policy:
    - Softmax over Q-values with two additives to logits:
      1) Perseveration bias toward repeating the last action in the same state; the bias decays
         exponentially with the number of trials since the last visit to that state.
      2) Novelty bonus that favors actions within a state that have been sampled less often.

    Learning:
    - Standard Rescorla-Wagner update on chosen action.

    Parameters (model_parameters):
    - alpha (float): Learning rate for Q-learning (0..1).
    - beta (float): Inverse temperature for softmax (>0).
    - persev0 (float): Base perseveration strength added to the previous action's logit (>0 favors repetition).
    - lambda_decay (float): Exponential decay rate of perseveration with inter-visit lag (>=0).
    - eta_nov (float): Novelty bonus magnitude; bonus is eta_nov / sqrt(1 + n_visits_state_action) (>=0).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen actions (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards (e.g., 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, persev0, lambda_decay, eta_nov]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, persev0, lambda_decay, eta_nov = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        visits = np.zeros((nS, nA))  # count per state-action for novelty
        prev_action = -1 * np.ones(nS, dtype=int)
        last_visit_time = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            # Compute lag since last visit to this state
            lag = 0 if last_visit_time[s] < 0 else (t - last_visit_time[s])

            # Perseveration bias decays with lag
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if prev_action[s] >= 0:
                bias = persev0 * np.exp(-lambda_decay * lag)
                logits[prev_action[s]] += bias

            # Novelty bonus within state: prefer less-sampled actions
            novelty = np.zeros(nA)
            for aa in range(nA):
                novelty[aa] = eta_nov / np.sqrt(1.0 + visits[s, aa])
            logits += novelty

            p = np.exp(logits)
            p = p / (np.sum(p) + eps)
            total_loglik += np.log(max(eps, p[a]))

            # Learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Book-keeping
            visits[s, a] += 1.0
            prev_action[s] = a
            last_visit_time[s] = t

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Arbitration between working-memory (WM) and RL based on set size and memory decay.

    Policy:
    - Mixture policy p = w_t * p_WM + (1 - w_t) * p_RL
      where:
        p_RL: softmax over Q-values with inverse temperature beta.
        p_WM: if a rewarded action is stored for the current state, it selects that action with prob (1 - epsilon),
              distributing epsilon uniformly over other actions; otherwise, uniform over actions.
      The arbitration weight w_t depends on set size (lower in larger sets) and recency of WM information:
        w_t = sigmoid(w0 + w_load * load) * exp(-decay * lag_since_store)
        where load = (set_size - 3) / 3 and lag_since_store is trials since WM was last updated for this state.

    Learning:
    - Q-learning update on chosen action.
    - WM store is updated to the chosen action only when reward is 1.

    Parameters (model_parameters):
    - alpha (float): Learning rate for Q-learning (0..1).
    - beta (float): Inverse temperature for RL softmax (>0).
    - w0 (float): Baseline arbitration logit (higher increases WM reliance at set size 3).
    - w_load (float): Effect of set size on arbitration logit (typically negative to reduce WM in larger sets).
    - decay (float): Exponential decay rate of WM strength with time since last rewarded store (>=0).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen actions (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards (e.g., 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, w0, w_load, decay]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, w0, w_load, decay = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    epsilon_wm = 1e-3  # small noise so WM is not perfectly deterministic

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        load = (nS - 3) / 3.0

        Q = np.zeros((nS, nA))

        # WM store: last rewarded action per state and the time it was stored
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_store_time = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy
            p_wm = np.ones(nA) / nA
            if wm_action[s] >= 0:
                p_wm = np.ones(nA) * (epsilon_wm / (nA - 1))
                p_wm[wm_action[s]] = 1.0 - epsilon_wm

            # Arbitration weight with set-size effect and decay by lag since last store
            base_w = sigmoid(w0 + w_load * load)
            lag = 0 if wm_store_time[s] < 0 else (t - wm_store_time[s])
            w_t = base_w * np.exp(-decay * lag)

            p = w_t * p_wm + (1.0 - w_t) * p_rl
            total_loglik += np.log(max(eps, p[a]))

            # Learning
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update only on rewarded trials
            if r >= 1.0 - eps:
                wm_action[s] = a
                wm_store_time[s] = t

    return -float(total_loglik)