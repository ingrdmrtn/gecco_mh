def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Working-Memory mixture with load-dependent WM weight and memory decay.

    Policy:
    - RL system: softmax over Q-values with inverse temperature beta.
    - WM system: one-shot memory that stores the last rewarded action for each state with a graded strength.
      The WM policy is a convex combination of a one-hot distribution over the memorized action and a uniform
      distribution, mixed by the current memory strength.
    - Final choice probability: mixture of RL and WM policies with a load-dependent WM weight.

    Learning and Memory:
    - RL: Rescorla-Wagner update with learning rate alpha.
    - WM: On rewarded trials (r == 1) store the chosen action for that state and set memory strength to 1.
          On every visit to a state, memory strength decays multiplicatively by (1 - wm_decay), bounded to [0,1].

    Parameters (model_parameters):
    - alpha (float): RL learning rate (0..1).
    - beta (float): Inverse temperature for the RL softmax (>0).
    - wm_weight_small (float): Mixture weight for WM in small set-size blocks (e.g., 3), in [0..1].
    - wm_weight_large (float): Mixture weight for WM in large set-size blocks (e.g., 6), in [0..1].
    - wm_decay (float): Per-visit decay of WM strength for the current state (0..1).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, wm_weight_small, wm_weight_large, wm_decay]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_weight_small, wm_weight_large, wm_decay = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        wm_w = wm_weight_small if nS == 3 else wm_weight_large

        # Initialize RL Q-values and WM memory per state
        Q = np.zeros((nS, nA))
        wm_action = -1 * np.ones(nS, dtype=int)        # stored action if any
        wm_strength = np.zeros(nS, dtype=float)        # strength in [0,1]

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            # Skip invalid trials
            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                continue

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy for this state
            # Decay memory strength on each visit
            wm_strength[s] = max(0.0, min(1.0, wm_strength[s] * (1.0 - wm_decay)))

            p_wm = np.full(nA, 1.0 / nA)
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                one_hot = np.zeros(nA)
                one_hot[wm_action[s]] = 1.0
                # Strength-weighted mixture between one-hot and uniform
                p_wm = wm_strength[s] * one_hot + (1.0 - wm_strength[s]) * (1.0 / nA) * np.ones(nA)

            # Mixture policy
            p = wm_w * p_wm + (1.0 - wm_w) * p_rl
            total_loglik += np.log(max(eps, p[a]))

            # RL learning
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM learning: on rewarded trials, store the action and reset strength to 1
            if r >= 1.0:
                wm_action[s] = a
                wm_strength[s] = 1.0

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with confidence-weighted temperature, load-dependent temperature scaling, and global stickiness.

    Policy:
    - Softmax over Q-values with an effective inverse temperature that adapts to state-specific "confidence":
      beta_eff = beta_base * (1 + bonus_scale * conf_s), where conf_s = max(Q_s) - min(Q_s) in [0,1].
      Higher confidence sharpens the policy.
    - Load-dependent scaling of temperature: for set size 3, multiply by (1 + load_temp_scale);
      for set size 6, divide by (1 + load_temp_scale). This models reduced precision under higher load.
    - Additive global stickiness bias: adds 'stick' to the logit of the action chosen on the previous trial
      irrespective of state, capturing motor/choice inertia.

    Learning:
    - Standard RL with learning rate alpha.

    Parameters (model_parameters):
    - alpha (float): RL learning rate (0..1).
    - beta_base (float): Baseline inverse temperature (>0).
    - bonus_scale (float): Scales the confidence-dependent sharpening of temperature (>=0).
    - load_temp_scale (float): Scales temperature by load: higher values reduce precision in large set sizes (>=0).
    - stick (float): Additive bias to the last chosen action's logit across trials (can be positive or negative).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta_base, bonus_scale, load_temp_scale, stick]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta_base, bonus_scale, load_temp_scale, stick = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        last_action_global = -1  # global stickiness across states

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < 0 or r > 1:
                # reset stickiness if invalid? we simply skip likelihood and learning
                continue

            q_s = Q[s, :]
            # Confidence from Q spread (bounded between 0 and 1 given reward scale)
            conf_s = float(np.max(q_s) - np.min(q_s))
            beta_eff = beta_base * (1.0 + bonus_scale * conf_s)
            # Load-dependent scaling
            scale = (1.0 + load_temp_scale)
            beta_eff = beta_eff * scale if nS == 3 else beta_eff / scale

            logits = beta_eff * (q_s - np.max(q_s))
            if last_action_global >= 0:
                logits[last_action_global] += stick

            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            total_loglik += np.log(max(eps, p[a]))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            last_action_global = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with within-state eligibility traces and load-dependent lapses.

    Policy:
    - Softmax over Q-values with inverse temperature beta to produce a model-based choice distribution.
    - Load-dependent lapse: with probability lapse_small (set size 3) or lapse_large (set size 6),
      the choice is assumed to be uniformly random over actions; otherwise, it follows the softmax policy.
      This captures increased lapses under higher cognitive load.

    Learning:
    - Rescorla-Wagner TD error on the chosen action.
    - Eligibility traces within the current state: a replacing-accumulating trace over actions is maintained
      for the current state. On each visit to state s:
        E[s,:] *= lambda_et
        E[s,a] += 1
      Q updates spread over actions in that state: Q[s,:] += alpha * delta * E[s,:]
      This captures spillover credit assignment to recently favored actions in the same state.

    Parameters (model_parameters):
    - alpha (float): Learning rate (0..1).
    - beta (float): Inverse temperature for softmax (>0).
    - lambda_et (float): Eligibility trace decay (0..1); higher means longer memory within the state.
    - lapse_small (float): Lapse probability in small set sizes (0..1).
    - lapse_large (float): Lapse probability in large set sizes (0..1).

    Inputs:
    - states (array-like of int): State indices per trial.
    - actions (array-like of int): Chosen action indices per trial (0..2). Trials with invalid actions are skipped.
    - rewards (array-like of float/int): Rewards per trial (typically 0 or 1). Trials with invalid rewards are skipped.
    - blocks (array-like of int): Block index per trial.
    - set_sizes (array-like of int): Set size per trial (constant within a block).
    - model_parameters (sequence): [alpha, beta, lambda_et, lapse_small, lapse_large]

    Returns:
    - float: Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, lambda_et, lapse_small, lapse_large = model_parameters

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        epsilon = lapse_small if nS == 3 else lapse_large

        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if s < 0 or s >= nS or a < 0 or a >= nA or r < -1 or r > 1:
                # Skip invalid trials
                continue

            # Softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_soft = np.exp(logits)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Lapse mixture
            p = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)
            total_loglik += np.log(max(eps, p[a]))

            # Eligibility traces within state s
            E[s, :] *= lambda_et
            E[s, a] += 1.0

            # TD update with traces
            delta = r - Q[s, a]
            Q[s, :] += alpha * delta * E[s, :]

    return -float(total_loglik)