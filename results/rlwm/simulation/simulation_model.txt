def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    RL + Working-Memory mixture with load-dependent WM weight and memory decay.

    Parameters:
    - stimulus: array-like, state index per trial (may be any labels; remapped per block to 0..nS-1)
    - blocks: array-like, block index per trial
    - set_sizes: array-like, set size per trial (assumed constant within a block)
    - correct_answer: array-like, correct action (0..2) per trial
    - parameters: [alpha, beta, wm_weight_small, wm_weight_large, wm_decay]

    Returns:
    - simulated_actions: array of chosen actions (ints 0..2) per trial
    - simulated_rewards: array of rewards (0/1) per trial
    """
    import numpy as np

    alpha, beta, wm_weight_small, wm_weight_large, wm_decay = parameters

    n_trials = len(stimulus)
    nA = 3
    eps = 1e-12

    simulated_actions = -1 * np.ones(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Process block by block to reset latent states each block
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        # Preserve within-block order
        block_states_raw = np.asarray(stimulus)[block_idx]
        block_correct = np.asarray(correct_answer)[block_idx]
        block_set_sizes = np.asarray(set_sizes)[block_idx]

        # Determine set size and corresponding WM weight
        nS = int(block_set_sizes[0])
        wm_w = wm_weight_small if nS == 3 else wm_weight_large

        # Remap any arbitrary state labels in this block to 0..nS-1
        unique_states = np.unique(block_states_raw)
        if len(unique_states) != nS:
            # fall back to the number of unique states if mismatch
            nS = len(unique_states)
            wm_w = wm_weight_small if nS == 3 else wm_weight_large
        state_map = {s_label: i for i, s_label in enumerate(unique_states)}
        block_states = np.array([state_map[s] for s in block_states_raw], dtype=int)

        # Initialize RL and WM memory
        Q = np.zeros((nS, nA), dtype=float)
        wm_action = -1 * np.ones(nS, dtype=int)   # stored action per state
        wm_strength = np.zeros(nS, dtype=float)   # strength in [0,1]

        # Iterate through trials within the block in order
        for local_t, tr in enumerate(block_idx):
            s = int(block_states[local_t])
            correct_a = int(block_correct[local_t])

            # Compute RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Decay WM for current state (per-visit)
            wm_strength[s] = max(0.0, min(1.0, wm_strength[s] * (1.0 - wm_decay)))

            # Compute WM policy
            p_wm = np.full(nA, 1.0 / nA)
            if wm_action[s] >= 0 and wm_strength[s] > 0.0:
                one_hot = np.zeros(nA)
                one_hot[wm_action[s]] = 1.0
                p_wm = wm_strength[s] * one_hot + (1.0 - wm_strength[s]) * (1.0 / nA) * np.ones(nA)

            # Mixture policy
            p = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p = p / (np.sum(p) + eps)

            # Sample action
            a = int(np.random.choice(nA, p=p))
            simulated_actions[tr] = a

            # Determine reward from correct answer
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update on rewarded trials
            if r >= 1:
                wm_action[s] = a
                wm_strength[s] = 1.0

    return simulated_actions, simulated_rewards