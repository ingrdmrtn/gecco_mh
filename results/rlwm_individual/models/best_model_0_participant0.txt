def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-learning-rate RL with set-size-dependent exploration and age-dependent perseveration.
    Policy includes a stickiness term favoring repetition of the last action in the same state.
    Older adults are modeled to have different perseveration strength.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback 0/1 on each trial.
    blocks : array-like of int
        Block index for each trial; used to reset values at block boundaries.
    set_sizes : array-like of int
        Set size for each trial; constant within block; modulates inverse temperature.
    age : array-like (length 1 or broadcastable)
        Participant's age in years; used to determine age group for stickiness.
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg, beta_small, beta_large, kappa_young, kappa_old)
        - alpha_pos: learning rate for positive prediction errors (0..1).
        - alpha_neg: learning rate for negative prediction errors (0..1).
        - beta_small: inverse temperature for set size = 3, scaled internally.
        - beta_large: inverse temperature for set size = 6, scaled internally.
        - kappa_young: perseveration strength (softmax bias) for younger group.
        - kappa_old: perseveration strength (softmax bias) for older group.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, beta_small, beta_large, kappa_young, kappa_old = model_parameters
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    beta_small = max(1e-6, beta_small) * 10.0
    beta_large = max(1e-6, beta_large) * 10.0
    is_old = 1 if (np.asarray(age)[0] >= 45) else 0
    kappa = kappa_old if is_old else kappa_young

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.ones((nS, nA)) / nA

        last_action_per_state = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_block = int(block_set_sizes[t])

            beta = beta_small if nS_block <= 3 else beta_large

            pref = beta * Q[s, :].copy()
            if last_action_per_state[s] >= 0:
                pref[last_action_per_state[s]] += kappa

            pref -= np.max(pref)
            expv = np.exp(pref)
            p_vec = expv / np.sum(expv)
            p = np.clip(p_vec[a], 1e-12, 1.0)
            total_log_p += np.log(p)

            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            last_action_per_state[s] = a

    return -float(total_log_p)