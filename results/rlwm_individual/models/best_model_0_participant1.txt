def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with novelty bonus and simple recall, with age- and load-modulated exploration.

    Concept
    - Asymmetric RL updates (alpha_pos for positive PE, alpha_neg for negative PE).
    - Novelty/exploration bonus favors less-tried actions within a state.
    - A simple recall mechanism: if a state has a cached last rewarded action, with some probability
      (decaying with load and age), the model biases toward that action (direct access).
    - Age modulates both the effective inverse temperature and the novelty drive.
    - Set size reduces effective inverse temperature (more exploration under load).

    Parameters (model_parameters)
    - alpha_pos: learning rate for positive prediction errors in [0,1].
    - alpha_neg: learning rate for negative prediction errors in [0,1].
    - beta_base: base inverse temperature (>0).
    - novelty_bonus: scale of novelty bonus and recall propensity (>0).
    - age_explore_penalty: reduces inverse temperature and novelty when older (>=45) (>=0).
    - load_temp_penalty: reduces inverse temperature as set size increases (>=0).

    Inputs
    - states: array of int, state index per trial.
    - actions: array of int, chosen action per trial (0..2).
    - rewards: array of float/int, feedback (0/1).
    - blocks: array of int, block index per trial.
    - set_sizes: array of int, set size for each trial's block.
    - age: array-like of length 1, participant age in years.
    - model_parameters: tuple/list of 6 floats as described above.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_base, novelty_bonus, age_explore_penalty, load_temp_penalty = model_parameters

    nA = 3
    age_group = 1.0 if age[0] >= 45 else 0.0

    beta_base = max(beta_base, 1e-6) * 3.0
    novelty_bonus = max(novelty_bonus, 1e-6)
    age_explore_penalty = max(0.0, age_explore_penalty)
    load_temp_penalty = max(0.0, load_temp_penalty)

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA))  # for novelty
        last_rewarded = -1 * np.ones(nS, dtype=int)  # last rewarded action per state; -1 if none

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            load = (max(nS, 3) - 3) / 3.0

            beta_eff = beta_base
            beta_eff *= np.exp(-load_temp_penalty * load)
            beta_eff *= np.exp(-age_group * age_explore_penalty)
            beta_eff = max(beta_eff, 1e-6) * 3.0


            novelty_scale = novelty_bonus * (1.0 - 0.5 * age_group)  # younger preserve full novelty
            bon = novelty_scale / (1.0 + visits[s, :])

            logits = beta_eff * (Q[s, :] + bon)
            logits = logits - np.max(logits)
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)


            if last_rewarded[s] >= 0:
                recall_base = 1.0 / (1.0 + np.exp(-novelty_scale))  # sigmoid of novelty scale
                recall_load = np.exp(-2.0 * load)  # more load, less recall
                recall_age = 1.0 / (1.0 + age_group * age_explore_penalty)  # older, less recall
                p_recall = np.clip(recall_base * recall_load * recall_age, 0.0, 1.0)
                p_recall_vec = np.ones(nA) / nA
                p_recall_vec[last_rewarded[s]] = max(1.0 - (nA - 1) * 1e-6, 1.0)  # near-point mass

                p_mix = p_recall * p_recall_vec + (1.0 - p_recall) * p_rl
            else:
                p_mix = p_rl

            p_choice = np.clip(p_mix[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            visits[s, a] += 1.0

            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            if r > 0.5:
                last_rewarded[s] = a

    return -float(total_loglik)