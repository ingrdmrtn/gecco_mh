def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size-specific learning rates, state-wise perseveration, and age-dependent exploration.

    The policy is a softmax over action preferences composed of:
      - Q-values (learned with different learning rates for small vs large set sizes),
      - A perseveration bias that favors repeating the last action taken in the same state.

    Older adults are modeled as having lower inverse temperature (more exploratory choices).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed chosen action on each trial (0..nA-1).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6 here).
    age : array-like (length 1)
        Participant age in years. Younger: age < 45, Older: age >= 45.
    model_parameters : list/tuple of float
        [alpha_small, alpha_large, beta_base, perseveration_kappa, age_beta_bonus, decay_base]
        - alpha_small: RL learning rate for small set size blocks (e.g., 3).
        - alpha_large: RL learning rate for large set size blocks (e.g., 6).
        - beta_base: baseline inverse temperature (>0), scaled internally by 10.
        - perseveration_kappa: additive bias for repeating the last action in a state.
        - age_beta_bonus: proportional reduction to beta for older adults (0=no reduction; 1=100% reduction).
        - decay_base: value decay toward zero that grows with set size: effective_decay = decay_base * ((nS-3)/3).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    alpha_small, alpha_large, beta_base, kappa, age_beta_bonus, decay_base = model_parameters
    nA = 3
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        alpha = alpha_small if nS <= 3 else alpha_large

        beta = max(1e-6, beta_base) * 10.0
        beta = beta * (1.0 - is_older * np.clip(age_beta_bonus, 0.0, 1.0))
        beta = max(1e-6, beta)

        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # -1 means no previous action in this state yet

        eff_decay = np.clip(decay_base, 0.0, 1.0) * max(0.0, (nS - 3) / 3.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            pref = Q[s, :].copy()
            if last_action[s] >= 0:
                pref[last_action[s]] += kappa

            pref_centered = pref - np.max(pref)
            expp = np.exp(beta * pref_centered)
            p_all = expp / np.sum(expp)
            p = max(1e-12, p_all[a])
            total_logp += np.log(p)

            Q[s, a] += alpha * (r - Q[s, a])

            Q[s, :] *= (1.0 - eff_decay)

            last_action[s] = a

    return -float(total_logp)