def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and decay-limited working memory, modulated by set size and age.
    
    Model summary:
    - Two systems contribute to choice: model-free RL (Q-learning) and a capacity-limited WM store.
    - WM weight decreases as set size exceeds capacity and decays over trials within a block.
    - Younger adults contribute more WM weight (age effect), especially at small set sizes.
    - Choices are produced by a mixture of RL softmax policy and WM policy, with a small lapse/choice noise.
    
    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1]
    - beta: Inverse temperature for RL softmax (scaled up internally)
    - C: WM capacity (in items). Effective WM weight scales as min(1, C / set_size).
    - rho: WM decay toward uniform on every trial (0=no decay, 1=full decay to uniform each step)
    - w0: Baseline WM weight at/below capacity (0..1), age- and set-size-modulated
    - eps: Lapse probability (0..1). With probability eps, action is random uniform (1/3)
    
    Inputs:
    - states: np.array of state indices (0..nS-1 within a block)
    - actions: np.array of chosen actions (int; expected 0,1,2; if outside, treated as lapse/unmodeled and no updates)
    - rewards: np.array of scalar rewards (0/1; values are clipped into [0,1])
    - blocks: np.array of block indices (int), resets learning across blocks
    - set_sizes: np.array giving the set size on each trial (3 or 6)
    - age: iterable/array with participant age; age[0] used
    - model_parameters: iterable of parameters as defined above
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, C, rho, w0, eps = model_parameters
    beta = max(1e-6, beta) * 10.0
    eps = np.clip(eps, 0.0, 0.5)
    alpha = np.clip(alpha, 0.0, 1.0)
    rho = np.clip(rho, 0.0, 1.0)
    C = max(1e-6, C)
    w0 = np.clip(w0, 0.0, 1.0)

    age_val = age[0] if hasattr(age, "__len__") else age
    younger = 1 if age_val < 45 else 0
    age_wm_boost = 1.20 if younger else 0.85  # multiplicative factor on WM contribution

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3

        nS = int(np.max(block_states)) + 1
        if len(block_set_sizes) > 0:
            nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))  # start neutral; could start at zeros
        W = np.ones((nS, nA)) / nA  # uniform WM at block start

        last_seen_trial = -np.ones(nS, dtype=int)  # for optional per-state decay control

        prev_state = None
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            r = float(np.clip(r, 0.0, 1.0))
            ss = int(block_set_sizes[t])


            W[s, :] = (1 - rho) * W[s, :] + rho * (1.0 / nA)

            Qs = Q[s, :]
            Qs_centered = Qs - np.max(Qs)  # numerical stability
            pi_rl = np.exp(beta * Qs_centered)
            pi_rl_sum = np.sum(pi_rl)
            if pi_rl_sum <= 0:
                pi_rl = np.ones(nA) / nA
            else:
                pi_rl /= pi_rl_sum

            pi_wm = W[s, :].copy()
            pi_wm = np.maximum(pi_wm, 1e-8)
            pi_wm /= np.sum(pi_wm)

            cap_factor = min(1.0, C / max(1, ss))
            w_eff = np.clip(w0 * cap_factor * age_wm_boost, 0.0, 1.0)

            pi_mix = (1.0 - w_eff) * pi_rl + w_eff * pi_wm
            pi_mix = (1.0 - eps) * pi_mix + eps * (1.0 / nA)

            if 0 <= a < nA:
                p_a = float(np.clip(pi_mix[a], 1e-12, 1.0))
                nll -= np.log(p_a)
            else:

                nll -= np.log(1.0 / nA)
                continue


            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            if r >= 0.5:
                W[s, :] = (1 - rho) * (np.zeros(nA))
                W[s, a] = (1 - rho) * 1.0 + rho * (1.0 / nA)

                W[s, :] = np.maximum(W[s, :], 1e-8)
                W[s, :] /= np.sum(W[s, :])

            prev_state = s

    return nll