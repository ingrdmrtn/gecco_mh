def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with separate Go/NoGo action channels, age-modulated NoGo learning,
    set-size-scaled learning, and choice inertia bias.

    The policy is based on an action preference A = Go - NoGo + bias, where Go and
    NoGo values are learned from a critic's prediction error. Positive PE strengthens
    Go for chosen action; negative PE strengthens NoGo. Older adults (>=45) have
    amplified NoGo learning. Learning rates are reduced under higher set sizes.
    A perseverative bias favors repeating the last action in the same state.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Binary reward (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block corresponding to each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define group (older >= 45).
    model_parameters : tuple/list of floats
        (alpha, beta, nogo_age_gain, size_lr_scale, inertia_bias, lapse)
        - alpha: base learning rate for actor and critic (squashed 0..1)
        - beta: inverse temperature for softmax over preferences (scaled internally)
        - nogo_age_gain: multiplicative gain on NoGo learning in older group (>=45)
        - size_lr_scale: exponent controlling load scaling of learning via (3/nS)^size_lr_scale
        - inertia_bias: strength of bias to repeat the last action in the same state
                        (adds to chosen action preference; can be positive/negative)
        - lapse: lapse probability mixed with uniform policy (squashed to 0..0.3)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, nogo_age_gain, size_lr_scale, inertia_bias, lapse = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3
    size_lr_scale = size_lr_scale  # free
    nogo_age_gain = np.exp(nogo_age_gain)  # positive multiplicative factor
    inertia_bias = inertia_bias  # can be positive or negative

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        G = np.zeros((nS, nA))
        N = np.zeros((nS, nA))
        Q = np.zeros((nS, nA))

        last_action = -np.ones(nS, dtype=int)

        lr_scale = (3.0 / float(nS)) ** size_lr_scale
        alpha_eff = np.clip(alpha * lr_scale, 0.0, 1.0)
        nogo_gain_eff = 1.0 + (nogo_age_gain - 1.0) * older

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            pref = G[s, :] - N[s, :]
            if last_action[s] >= 0:
                pref[last_action[s]] += inertia_bias

            pref = pref - np.max(pref)
            pi = np.exp(beta * pref)
            pi = pi / (np.sum(pi) + eps)

            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            pe = r - Q[s, a]
            Q[s, a] += alpha_eff * pe

            if pe >= 0.0:

                G[s, a] += alpha_eff * pe
            else:

                N[s, a] += alpha_eff * (-pe) * nogo_gain_eff

            last_action[s] = a

    return nll