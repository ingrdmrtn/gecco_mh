def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with confidence-adaptive WM influence.
    
    This model blends RL with a WM store but adjusts the effective WM weight within each state
    based on WM confidence (how peaked the WM distribution is) and set size. WM is updated on all trials,
    with stronger updates after rewards and weaker after non-rewards, and decays over time.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1). Modulated by WM confidence and set size.
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        base_wm_scale = wm_weight * min(1.0, 3.0 / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))


            spread = np.max(W_s) - np.min(W_s)  # in [0,1]
            wm_conf = spread

            eff_wm_weight = base_wm_scale * wm_conf

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta


            decay = 0.10
            w = (1.0 - decay) * w + decay * w_0

            alpha_pos = 1.0   # full overwrite when rewarded
            alpha_neg = 0.25  # partial update when not rewarded
            if r > 0.0:
                w[s, :] = (1.0 - alpha_pos) * w[s, :]  # which zeroes if alpha_pos=1
                w[s, a] += alpha_pos
            else:

                w[s, :] = (1.0 - alpha_neg) * w[s, :]
                w[s, a] += alpha_neg

            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p