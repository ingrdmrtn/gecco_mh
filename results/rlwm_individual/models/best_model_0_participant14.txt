def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity- and decay-modulated working memory.
    
    On each trial, choices are generated by a mixture of:
    - RL policy (softmax over Q-values) updated with a single learning rate.
    - WM policy (softmax with very high inverse temperature over W-values) that stores
      the last rewarded action per state, but suffers from decay toward uniform and
      reduced availability when set size exceeds capacity.
    
    The effective WM contribution is scaled by an availability factor that depends
    on set size relative to a WM capacity parameter K.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1, where nS is the set size in the current block).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial. Value changes denote block boundaries (no overlap of states).
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    model_parameters : iterable of floats
        [lr, wm_weight, softmax_beta, K, wm_decay]
        - lr: RL learning rate (0..1)
        - wm_weight: baseline mixture weight of WM (0..1)
        - softmax_beta: RL inverse temperature (>0); internally scaled up
        - K: WM capacity in number of items (>0, typically ~3-4)
        - wm_decay: per-trial decay rate of WM toward uniform (0..1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))


            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            p_wm_a = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            nS_curr = float(block_set_sizes[t])
            avail = min(1.0, max(0.0, K / nS_curr))
            wm_eff = wm_weight * avail

            p_total = wm_eff * p_wm_a + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]  # keep general decay already applied
                w[s, :] = (1.0 / nA) * np.ones(nA)  # reset row before strong write
                w[s, a] = 1.0 - (nA - 1) * 1e-6     # nearly one-hot
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = 1e-6


        blocks_log_p += log_p

    return -blocks_log_p