def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and set-size–modulated working-memory weight and perseveration.

    This model assumes choices are generated by a mixture of a reinforcement-learning (RL) system
    and a working-memory (WM) system. The WM system stores recently rewarded stimulus–action
    associations with decay. The arbitration weight given to WM is reduced as set size increases
    and further modulated by age group (younger vs older). A choice perseveration (stickiness)
    bias promotes repeating the previous action.

    Parameters
    ----------
    states : array-like of int
        State identity at each trial (0-indexed within a block).
    actions : array-like of int
        Chosen action at each trial (0, 1, or 2).
    rewards : array-like of float
        Binary feedback (0 or 1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of distinct states in that block) for each trial.
    age : array-like of float
        Age of the participant (single-element array). Used to set age group.
    model_parameters : sequence of 6 floats
        alpha: RL learning rate in [0,1].
        beta: inverse temperature for softmax; internally scaled by 10.
        wm_weight_base: base weight for WM contribution in [0,1].
        wm_decay: decay of WM values toward uniform per trial in [0,1].
        stickiness: perseveration weight added to the last chosen action’s preference.
        gamma_setsize: exponent governing how WM weight decreases with set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np  # assumed available per instructions
    alpha, beta, wm_weight_base, wm_decay, stickiness, gamma_setsize = model_parameters
    beta *= 10.0
    age_val = float(age[0])
    is_younger = 1.0 if age_val < 45 else 0.0


    age_wm_factor = 1.0 + 0.3 * (is_younger - (1.0 - is_younger))  # +0.3 if younger, -0.3 if older

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))

        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            ss_factor = (3.0 / ss) ** max(0.0, gamma_setsize)
            wm_weight = np.clip(wm_weight_base * age_wm_factor * ss_factor, 0.0, 1.0)

            prefs_rl = beta * Q[s, :].copy()
            if prev_action is not None:
                prefs_rl[prev_action] += stickiness
            max_pref_rl = np.max(prefs_rl)
            pi_rl = np.exp(prefs_rl - max_pref_rl)
            pi_rl /= np.sum(pi_rl)

            beta_wm = 2.0 * beta
            prefs_wm = beta_wm * W[s, :].copy()
            if prev_action is not None:
                prefs_wm[prev_action] += stickiness
            max_pref_wm = np.max(prefs_wm)
            pi_wm = np.exp(prefs_wm - max_pref_wm)
            pi_wm /= np.sum(pi_wm)

            pi = wm_weight * pi_wm + (1.0 - wm_weight) * pi_rl
            p_a = np.clip(pi[a], eps, 1.0)
            nll -= np.log(p_a)

            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)

            if r > 0.5:
                W[s, :] = (0.0)
                W[s, a] = 1.0

            row_sum = np.sum(W[s, :])
            if row_sum > 0:
                W[s, :] /= row_sum

            prev_action = a

    return nll