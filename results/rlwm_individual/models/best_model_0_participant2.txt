def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Count-based uncertainty bonus with age-modulated directed exploration and load-dependent interference.

    The model uses:
    - Standard Q-learning for values.
    - A directed exploration bonus proportional to epistemic uncertainty U(s,a) = 1/sqrt(N+1),
      where N is the visit count per state-action. The bonus weight is modulated by age group.
    - Load-dependent interference that decays Q-values upon each state visit, simulating memory load.
    - Choice perseveration that weakens under higher load.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block (0..nS-1 inside block).
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age; used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta0, bonus0, age_bonus_shift, size_forget, perseveration]
        - alpha: learning rate for Q-learning.
        - beta0: baseline inverse temperature; scaled by 10 internally.
        - bonus0: baseline directed-exploration bonus weight.
        - age_bonus_shift: modulation of the bonus by age group; positive favors younger.
                           bonus_eff = bonus0 + age_bonus_shift*(0.5 - age_group).
        - size_forget: amount of Q decay per state-visit that increases with set size.
                       decay_rate = clip(size_forget * load_level, 0, 0.5).
        - perseveration: baseline choice stickiness that weakens with load.

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    alpha, beta0, bonus0, age_bonus_shift, size_forget, perseveration = model_parameters
    nA = 3
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    total_logp = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts
        last_act = -1 * np.ones(nS, dtype=int)

        beta_eff = 10.0 * beta0
        load_level = (nS - 3) / 3.0  # 0 or 1
        bonus_eff = bonus0 + age_bonus_shift * (0.5 - age_group)
        pers_eff = perseveration * (1.0 - 0.5 * load_level)
        decay_rate = np.clip(size_forget * load_level, 0.0, 0.5)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            Q[s, :] *= (1.0 - decay_rate)

            U = 1.0 / np.sqrt(N[s, :] + 1.0)

            logits = beta_eff * Q[s, :] + bonus_eff * U
            if last_act[s] >= 0:
                logits[last_act[s]] += pers_eff
            logits = logits - np.max(logits)
            pol = np.exp(logits)
            pol = pol / np.sum(pol)

            p_a = np.clip(pol[a], 1e-12, 1.0)
            total_logp += np.log(p_a)

            N[s, a] += 1.0
            Q[s, a] += alpha * (r - Q[s, a])

            last_act[s] = a

    return -float(total_logp)