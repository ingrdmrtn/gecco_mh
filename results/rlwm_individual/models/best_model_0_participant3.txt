def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetrical learning rates and age/load-modulated perseveration bias.

    Idea:
    - Choices follow a softmax over Q-values with an added perseveration bonus
      for repeating the previous action. This bias is stronger under higher load
      (larger set size) and for older adults.
    - Learning uses separate learning rates for positive vs. negative prediction errors.
    - Lapse process mixes in uniform responding.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 in a block).
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2; invalid values handled as lapses).
    rewards : array-like of float
        Obtained reward on each trial (e.g., 0/1; can be negative for punishments).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6 here).
    age : array-like of float
        Participant age (first element used). Age group: older >=45.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, kappa_base, kappa_age_slope, lapse)
        - alpha_pos: learning rate for positive prediction errors (0..1).
        - alpha_neg: learning rate for negative prediction errors (0..1).
        - beta: softmax inverse temperature (>0).
        - kappa_base: base perseveration strength (added to chosen action value next trial).
        - kappa_age_slope: additional perseveration for being older (applied additively);
                           load modulation multiplies by set_size/6.
        - lapse: lapse rate mixing with uniform responding (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, beta, kappa_base, kappa_age_slope, lapse = model_parameters
    beta = max(1e-6, beta) * 5.0
    alpha_pos = min(max(alpha_pos, 0.0), 1.0)
    alpha_neg = min(max(alpha_neg, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    age_val = age[0] if hasattr(age, "__len__") else float(age)
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            load_factor = float(block_set_sizes[t]) / 6.0  # 0.5 for 3, 1.0 for 6

            kappa = kappa_base + kappa_age_slope * older
            kappa_eff = kappa * load_factor
            pers = np.zeros(nA)
            if prev_action is not None and 0 <= prev_action < nA:
                pers[prev_action] = kappa_eff

            vals = Q[s] + pers
            m = np.max(beta * vals)
            expv = np.exp(beta * vals - m)
            pi = expv / np.sum(expv)
            pi = (1.0 - lapse) * pi + lapse * (np.ones(nA) / nA)

            if 0 <= a < nA:
                p_a = max(pi[a], eps)
            else:
                p_a = max(lapse / nA, eps)

            total_logp += np.log(p_a)

            if 0 <= a < nA:
                pe = r - Q[s, a]
                lr = alpha_pos if pe >= 0 else alpha_neg
                Q[s, a] += lr * pe
                prev_action = a
            else:


                prev_action = None

    return -float(total_logp)