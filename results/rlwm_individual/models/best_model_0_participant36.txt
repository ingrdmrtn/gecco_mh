def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + rewarded-episode retrieval with set-size and age-modulated retrieval probability and decay.
    
    Mechanism:
      - Model-free Q-learning within blocks.
      - Parallel "episodic success memory" M that stores action strengths for actions that previously
        yielded reward = 1 in a given state. M decays via interference that grows with set size.
      - On each trial, the decision policy mixes an episodic retrieval policy and the RL policy.
      - The retrieval weight (rho) is higher for smaller set sizes and is reduced in older adults,
        but this is counteracted (or amplified) by an age-specific shift parameter.
    
    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Base inverse temperature (>0); internally scaled for numerical stability.
    decay_base : float
        Base decay rate for episodic memory per trial in [0,1].
    retr_base : float
        Baseline retrieval log-odds; higher -> more reliance on episodic memory.
    age_retr_shift : float
        Additive shift on retrieval log-odds if older (>=45). Positive increases retrieval in older adults.
    ss_retr_slope : float
        Slope by which retrieval log-odds decreases with set size (relative to 3). Positive -> less retrieval as set size grows.
    
    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial. Values indicate trials belonging to the same block.
    set_sizes : array-like (T,)
        Set size for each trial in its block (typically 3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, decay_base, retr_base, age_retr_shift, ss_retr_slope = model_parameters
    beta = 5.0 * max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        M = np.zeros((nS, nA))  # episodic success strengths


        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        decay = np.clip(decay_base * (1.0 + 0.5 * ss_factor) * (1.0 + 0.25 * is_older), 0.0, 1.0)


        logit_rho = retr_base - ss_retr_slope * ss_factor + age_retr_shift * is_older
        rho = 1.0 / (1.0 + np.exp(-logit_rho))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            if decay > 0:
                M *= (1.0 - decay)

            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)  # stabilize
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            m_s = M[s, :].copy()
            m_s -= np.max(m_s)
            p_ep = np.exp(beta * m_s)
            p_ep = p_ep / (np.sum(p_ep) + eps)

            p = rho * p_ep + (1.0 - rho) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            pe = r - Q[s, a]
            Q[s, a] += alpha * pe


            if r > 0.5:

                M[s, :] *= 0.5  # partial suppression of competitors
                M[s, a] = 1.0   # mark successful action strongly

    return -float(total_logp)