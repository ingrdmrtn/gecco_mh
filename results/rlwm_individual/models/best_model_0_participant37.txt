def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Bayesian RL (Beta-Bernoulli) with WM win-stay/lose-shift heuristic and capacity gating.

    Model idea:
    - RL system maintains Beta(a,b) posterior per state-action on Bernoulli rewards. The expected value is a/(a+b).
      Choice is softmax over expected values.
    - WM system implements a fast win-stay / lose-shift heuristic for each state:
        after reward: peak at last chosen action
        after no reward: distribute mass over the other actions
      A lapse parameter smooths the WM policy.
    - Mixture weight depends on set size via capacity K: wm_weight_eff = base * min(1, K / nS)

    Parameters (model_parameters):
    - prior_a: scalar > 0. Prior success count for Beta posterior.
    - prior_b: scalar > 0. Prior failure count for Beta posterior.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - wm_base: scalar in [0,1]. Baseline WM mixture weight.
    - lapse: scalar in [0,1). Lapse/epsilon for WM policy; smooths away determinism.
    - capacity_K: scalar > 0. WM capacity modulating set-size effect.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    prior_a, prior_b, softmax_beta, wm_base, lapse, capacity_K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        a_post = np.full((nS, nA), prior_a, dtype=float)
        b_post = np.full((nS, nA), prior_b, dtype=float)

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        base_gate = wm_base * min(1.0, capacity_K / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q = a_post / (a_post + b_post)
            Q_s = Q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            if r >= 1.0 - 1e-12:

                wm_pref = np.full(nA, lapse / (nA - 1))
                wm_pref[a] = 1.0 - lapse
            else:

                wm_pref = np.full(nA, (1.0 - lapse) / (nA - 1))
                wm_pref[a] = lapse

            w[s, :] = 0.5 * w[s, :] + 0.5 * wm_pref  # partial overwrite for stability
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = base_gate * p_wm + (1.0 - base_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            a_post[s, a] += r
            b_post[s, a] += (1.0 - r)

        blocks_log_p += log_p

    return -blocks_log_p