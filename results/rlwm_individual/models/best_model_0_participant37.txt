def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and set-size-depressed precision + WM with set-size-driven interference.

    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta_eff,
      where beta_eff = beta * exp(-size_beta_drop * (nS - 3)).
    - WM channel: softmax over WM weights with high inverse temperature.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    RL dynamics:
    - Asymmetric learning rates for positive and negative prediction errors:
        alpha_pos for PE > 0, alpha_neg for PE < 0.

    WM dynamics:
    - Rewarded overwriting: if reward=1, store a one-hot vector at chosen action.
    - Interference toward uniform that increases with set size:
        gamma_eff = 1 - (1 - wm_interference)^(1 + (nS - 3)),
        w[s,:] <- (1 - gamma_eff) * w[s,:] + gamma_eff * uniform.

    Set-size effects:
    - RL precision is reduced at larger set sizes via beta_eff above.
    - WM interference increases with set size via gamma_eff above.

    Parameters (tuple):
    - alpha_pos: RL learning rate for positive PE (0..1).
    - alpha_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: Baseline inverse temperature for RL (scaled by 10 internally).
    - wm_weight: Constant WM mixture weight (0..1).
    - wm_interference: Baseline interference toward uniform (0..1).
    - size_beta_drop: Strength of set-size penalty on RL precision (>=0).
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight, wm_interference, size_beta_drop = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_eff = softmax_beta * np.exp(-max(0, nS - 3) * size_beta_drop)
        gamma_eff = 1.0 - (1.0 - wm_interference) ** (1 + max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            alpha_t = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha_t * pe

            w[s, :] = (1.0 - gamma_eff) * w[s, :] + gamma_eff * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p