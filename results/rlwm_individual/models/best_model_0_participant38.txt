def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+Capacity-limited working memory mixture with age-modulated WM capacity and lapses.

    Idea:
    - Choices are a mixture of a slow RL system and a capacity-limited WM system.
    - WM stores the last rewarded action for each state when feedback=1; otherwise it's uninformative.
    - WM influence is attenuated as set size exceeds a capacity parameter, and further reduced for older adults.
    - A lapse parameter accounts for action omissions/invalid responses and occasional random choices.

    Parameters (model_parameters):
    - alpha_rl: learning rate for RL (0..1)
    - beta: inverse temperature for RL softmax; internally scaled up by 10
    - wm_weight_base: baseline mixture weight on WM (0..1)
    - K_wm: WM capacity (in number of state-action pairs that can be maintained); interacts with set size
    - lapse: lapse probability for random choice and invalid choices (0..1)

    Inputs:
    - states: array-like, state index on each trial (0..set_size-1 within block)
    - actions: array-like, chosen action on each trial; valid actions are {0,1,2}; invalid like -2 are treated as lapses
    - rewards: array-like, feedback per trial; valid feedback in {0,1}; negative (e.g., -1) indicates no feedback/omission
    - blocks: array-like, block index per trial; values group trials into blocks
    - set_sizes: array-like, set size per trial (3 or 6 here)
    - age: array-like with a single value; age[0] in years
    - model_parameters: list or array of parameters as defined above

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_rl, beta, wm_weight_base, K_wm, lapse = model_parameters
    beta *= 10.0
    age_val = float(age[0])
    is_older = age_val >= 45.0

    age_capacity_factor = 0.6 if is_older else 1.0

    alpha_rl_eff = alpha_rl * (0.8 if is_older else 1.0)

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))  # RL values


        W = (1.0 / nA) * np.ones((nS, nA))

        wm_has = np.zeros(nS, dtype=bool)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_curr = int(block_set_sizes[t])

            K_eff = max(0.0, K_wm * age_capacity_factor)
            wm_weight = wm_weight_base * min(1.0, K_eff / max(1.0, float(nS_curr)))

            q_s = Q[s, :]

            pref = beta * (q_s - np.max(q_s))
            exp_pref = np.exp(pref)
            p_rl_vec = exp_pref / np.sum(exp_pref)

            w_s = W[s, :]
            p_wm_vec = w_s

            p_mix_vec = wm_weight * p_wm_vec + (1.0 - wm_weight) * p_rl_vec
            p_final_vec = (1.0 - lapse) * p_mix_vec + lapse * (1.0 / nA)

            if a in [0, 1, 2]:
                p_a = float(p_final_vec[a])
                nll -= np.log(max(p_a, eps))
            else:

                p_lapse_only = lapse + (1.0 - lapse) * (1.0 / nA)  # probability mass on random responding
                nll -= np.log(max(p_lapse_only, eps))

                continue

            if r >= 0.0:

                delta = r - q_s[a]
                Q[s, a] = q_s[a] + alpha_rl_eff * delta

                if r > 0.5:

                    W[s, :] = 0.0
                    W[s, a] = 1.0
                    wm_has[s] = True
                else:

                    W[s, :] = (1.0 / nA)
                    wm_has[s] = False

    return nll