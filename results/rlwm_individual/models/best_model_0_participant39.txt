def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Model-free RL with action-generalization across states, age-modulated generalization, 
             and load-sensitive forgetting.

    Rationale:
      - Standard Q-learning within each state.
      - After each trial, the same action in other states is also adjusted (generalization),
        scaled by set size (more dilution in larger sets) and amplified for older adults.
      - Q-values also decay toward 0 each trial, more so in larger set sizes (load-sensitive forgetting).

    Parameters (model_parameters; total 5):
      - alpha: float in (0,1), primary learning rate for the experienced state-action.
      - beta: float > 0, inverse temperature for softmax (rescaled internally by 10).
      - gen_gain: float >= 0, base gain for action-generalization to other states.
      - age_gen_boost: float >= 0, multiplicative boost to generalization for older adults.
      - rho_forget: float in [0,1), base forgetting rate per trial; scaled by set size.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha, beta, gen_gain, age_gen_boost, rho_forget = model_parameters
    nA = 3
    beta_eff = max(1e-6, beta) * 10.0
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            q_s = Q[s, :].copy()
            logits = beta_eff * (q_s - np.max(q_s))
            p = np.exp(logits)
            p /= np.sum(p)

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 1e-6, 1.0) * pe


            g_eff = max(0.0, gen_gain) * (1.0 + is_older * max(0.0, age_gen_boost)) / max(1.0, ss)
            if nS > 1:
                for s2 in range(nS):
                    if s2 == s:
                        continue

                    Q[s2, a] += g_eff * pe


            forget_rate = np.clip(rho_forget * (1.0 + max(0.0, ss - 3.0) / 3.0), 0.0, 0.999)
            Q *= (1.0 - forget_rate)

    return float(neg_loglik)