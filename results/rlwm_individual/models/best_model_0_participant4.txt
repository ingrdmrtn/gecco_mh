def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-dependent directed exploration bonus, set-size-dependent forgetting, and lapse.

    Idea:
    - Standard model-free RL for values.
    - Directed exploration via a count-based bonus added to preferences:
        bonus_group / sqrt(N[s,a]) where N[s,a] is the action visit count in state s.
      The bonus magnitude depends on age group (younger vs older).
    - Larger set sizes induce stronger value decay (forgetting), scaling the decay parameter.
    - Lapse probability mixes in uniform random choice.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial.
    actions : 1D array-like of int
        Observed action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Reward feedback.
    blocks : 1D array-like of int
        Block index.
    set_sizes : 1D array-like of int
        Set size on each trial (3 or 6).
    age : 1D array-like (single value)
        Participant age; used to choose bonus magnitude.
    model_parameters : list/array of 6 floats
        [alpha, beta, bonus_young, bonus_old, decay, lapse]
        - alpha in [0,1]: RL learning rate.
        - beta > 0: inverse temperature (scaled by 10 internally).
        - bonus_young >= 0: exploration bonus magnitude for younger participants.
        - bonus_old >= 0: exploration bonus magnitude for older participants.
        - decay in [0,1]: base forgetting rate for RL values; effective decay scales with set size.
                          Effective per-trial forgetting factor for the visited state:
                          Q[s,:] *= (1 - decay * ((set_size - 3)/3)).
        - lapse in [0,1]: lapse rate mixing in uniform random choice.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    alpha, beta, bonus_y, bonus_o, decay, lapse = model_parameters
    alpha = max(0.0, min(1.0, alpha))
    beta = max(eps, beta) * 10.0
    bonus_y = max(0.0, bonus_y)
    bonus_o = max(0.0, bonus_o)
    decay = max(0.0, min(1.0, decay))
    lapse = max(0.0, min(1.0, lapse))
    is_older = age[0] >= 45
    bonus_group = bonus_o if is_older else bonus_y
    nA = 3

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        N = np.ones((nS, nA))  # start at 1 to avoid div by zero and bias extreme bonuses

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            sz = float(block_set_sizes[t])

            decay_eff = decay * ((max(3.0, sz) - 3.0) / 3.0)
            Q[s, :] *= (1.0 - decay_eff)

            bonus_vec = bonus_group / np.sqrt(N[s, :])

            prefs = Q[s, :] + bonus_vec
            prefs -= np.max(prefs)
            p_soft = np.exp(beta * prefs)
            p_soft /= (np.sum(p_soft) + eps)

            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)
            pa = max(p[a], eps)
            nll -= np.log(pa)

            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            N[s, a] += 1.0

    return nll