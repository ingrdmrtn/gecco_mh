def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with meta-learned inverse temperature + WM that encodes only after confirmation.

    Mechanism
    - RL: standard delta-rule learning; inverse temperature is adapted online via
      meta-learning from signed prediction error (increase beta after rewards,
      decrease after non-rewards), improving exploitation after success.
    - RL win-stay bias: on trials following a rewarded choice in the same state,
      add a bonus to that action's logit (distinct from generic stickiness).
    - WM: only commits (strongly encodes) when receiving a reward that confirms
      the same action was also rewarded last time this state appeared ("confirmation").
      Otherwise WM softly decays to uniform.
    - Arbitration: WM weight shrinks with set size.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), downscaled by 3/nS.
        softmax_beta : float
            Initial RL inverse temperature; internally scaled by 10; adapted online.
        meta_beta_lr : float
            Step size for adapting beta based on signed prediction error (0-1).
        win_stay_bias : float
            Additive logit bias for previously rewarded action in same state (>=0).
        wm_confirm_gain : float
            Strength of WM consolidation on confirmation (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, meta_beta_lr, win_stay_bias, wm_confirm_gain = model_parameters
    softmax_beta *= 10 # base beta
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    nA = 3
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_rew_action = -np.ones(nS, dtype=int)
        last_rew_was_positive = np.zeros(nS, dtype=bool)

        beta_t = softmax_beta
        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:].copy()

            logits_Q = beta_t * (Q_s - np.max(Q_s))
            if last_rew_was_positive[s] and last_rew_action[s] >= 0:
                logits_Q[last_rew_action[s]] += win_stay_bias

            exp_rl = np.exp(logits_Q - np.max(logits_Q))
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            W_s = w[s,:]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(wm_weight * size_scale, 0.0, 1.0)

            p_total = p_wm*eff_wm_weight + (1-eff_wm_weight)*p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            delta = r - q[s][a]
            q[s][a] += lr*delta

            beta_t = np.maximum(1e-6, beta_t + meta_beta_lr * (r - 0.5))  # bounded below

            w[s,:] = 0.9*w[s,:] + 0.1*w_0[s,:]


            confirm = (r > 0) and last_rew_was_positive[s] and (last_rew_action[s] == a)
            if confirm:
                target = np.zeros(nA); target[a] = 1.0
                w[s,:] = (1 - wm_confirm_gain)*w[s,:] + wm_confirm_gain*target

            if r > 0:
                last_rew_action[s] = a
                last_rew_was_positive[s] = True
            else:
                last_rew_was_positive[s] = False

        blocks_log_p += log_p

    return -blocks_log_p