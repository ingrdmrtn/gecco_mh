def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory mixture with age- and load-dependent weighting.

    This model mixes a standard RL softmax policy with a working memory (WM) policy.
    WM has a limited effective capacity and decays over time; its influence decreases with set size
    and decreases further for older adults.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial within a block (0..nS-1 for that block).
    actions : array-like of int
        Chosen actions on each trial. Valid actions are 0,1,2. Invalid actions are treated as lapses.
    rewards : array-like of float
        Observed feedback on each trial. Will be clipped to {0,1}.
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size (number of distinct states in the current block) for each trial.
    age : array-like or scalar
        Participant age; used to determine age group (older >= 45).
    model_parameters : sequence (length = 5)
        alpha : RL learning rate (0..1).
        beta : Inverse temperature for RL (positive). Internally scaled.
        k_wm : Nominal WM capacity (in items, positive).
        wm_decay : WM decay per trial (0..1).
        lapse : Lapse probability (0..0.5), adds uniform random responding.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, k_wm, wm_decay, lapse = model_parameters

    beta_eff_base = max(1e-6, beta) * 5.0  # scale beta to a higher effective range
    k_wm = max(1e-6, k_wm)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    age_val = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if age_val >= 45 else 0.0


    k_wm_age = k_wm * (1.0 - 0.3 * is_older)
    beta_age_scale = 1.0 - 0.2 * is_older
    beta_eff_base *= max(0.1, beta_age_scale)

    nA = 3
    eps = 1e-12
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA), dtype=float)

        W = np.ones((nS, nA), dtype=float) / nA

        last_correct = -np.ones(nS, dtype=int)  # -1 means unknown

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = 1.0 if block_rewards[t] > 0 else 0.0
            set_sz = float(block_set_sizes[t])


            wm_load = min(1.0, max(0.0, k_wm_age / max(1.0, set_sz)))


            overload = max(0.0, (set_sz - k_wm_age) / max(1.0, set_sz))
            wm_weight = wm_load * (1.0 - 0.25 * is_older * overload)
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            Qs = Q[s, :]

            beta_eff = beta_eff_base
            Qs_center = Qs - np.max(Qs)
            expQ = np.exp(beta_eff * Qs_center)
            p_rl = expQ / np.sum(expQ)


            W[s, :] = (1.0 - wm_decay) * W[s, :]

            if r > 0.5:


                eta = 0.7
                target = np.zeros(nA); target[a] = 1.0
                W[s, :] = (1 - eta) * W[s, :] + eta * target
                last_correct[s] = a
            else:

                eta_neg = 0.2
                supp = np.zeros(nA); supp[a] = 1.0
                W[s, :] = (1 - eta_neg) * W[s, :] + eta_neg * (1.0 / nA) * np.ones(nA)

            beta_wm = 10.0 * beta_eff  # WM assumed sharp when active
            Ws_center = W[s, :] - np.max(W[s, :])
            p_wm = np.exp(beta_wm * Ws_center)
            p_wm = p_wm / np.sum(p_wm)

            if last_correct[s] >= 0:
                lc = last_correct[s]
                spike = np.zeros(nA); spike[lc] = 1.0
                p_wm = 0.8 * p_wm + 0.2 * spike

            if a < 0 or a >= nA:

                p_choice = 1.0 / nA
                total_logp += np.log(max(eps, p_choice))

                continue

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = (1.0 - lapse) * p_mix[a] + lapse * (1.0 / nA)
            total_logp += np.log(max(eps, p_choice))

            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return -float(total_logp)