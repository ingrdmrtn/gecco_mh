def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid with surprise-gated WM, WM precision boost for small sets, and choice perseveration.

    Idea:
    - RL: standard delta-rule.
    - WM: one-hot storage with decay; WM precision (inverse temperature) increases when set size is small
      via a chunking/precision advantage.
    - Arbitration: WM reliance decreases with surprise (|PE|) and with set size; increases with WM strength.
    - Choice perseveration: bias to repeat previous action, applied to both WM and RL channels.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - base_beta: RL inverse temperature baseline; scaled by 10 internally.
    - wm_chunk_adv: boosts WM inverse temperature when set size is small (>=0).
    - surprise_sens: sensitivity of WM gating to absolute prediction error (>=0).
    - wm_decay: WM decay rate per trial toward uniform (0..1).
    - choice_bias: positive favors repeating previous action; negative favors switching.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, base_beta, wm_chunk_adv, surprise_sens, wm_decay, choice_bias = model_parameters
    softmax_beta = base_beta * 10.0
    softmax_beta_wm_base = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_pe_abs = np.ones(nS)  # start uncertain
        last_action = -1  # for perseveration

        lam = np.clip(wm_decay, 0.0, 1.0)

        size_factor = 3.0 / max(3.0, float(nS))
        beta_wm_eff = softmax_beta_wm_base * (1.0 + wm_chunk_adv * (size_factor - 0.5))
        beta_wm_eff = max(1.0, beta_wm_eff)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            bias_chosen = choice_bias if (a == last_action) else 0.0
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - (Q_s[a] + bias_chosen))))

            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - (W_s[a] + bias_chosen))))

            wm_strength = np.max(W_s) - 1.0 / nA
            strength_scale = np.clip(wm_strength / max(1e-6, 1.0 - 1.0 / nA), 0.0, 1.0)
            pe_gate = 1.0 / (1.0 + surprise_sens * last_pe_abs[s])
            wm_avail = np.clip(size_factor * pe_gate * strength_scale, 0.0, 1.0)

            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            delta = r - q[s, a]
            q[s, a] += lr * delta
            last_pe_abs[s] = abs(delta)

            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

            w = (1.0 - lam) * w + lam * w_0

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p