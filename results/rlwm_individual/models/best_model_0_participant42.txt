def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying working-memory mixture model with age- and load-dependent WM weighting.
    
    The model mixes a standard model-free RL controller with a simple working-memory (WM)
    store that remembers the last rewarded action per state, with decay toward uniform.
    The weight on WM decreases as set size exceeds capacity and with older age.
    A small lapse probability accounts for random choices and invalid trials.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..set_size-1).
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2). Invalid actions are handled via lapse.
    rewards : array-like of int
        Binary reward feedback (0 or 1). Invalid rewards (<0) trigger lapse handling.
    blocks : array-like of int
        Block index for each trial. Values group trials into independent blocks.
    set_sizes : array-like of int
        The set size for each trial (3 or 6 here). First value within a block defines nS.
    age : array-like of float
        Participant age (single-element array). Used to adjust WM effectiveness (older -> lower).
    model_parameters : sequence of floats
        Five parameters:
          - alpha: RL learning rate (0..1)
          - beta: inverse temperature for RL softmax (scaled internally by 10)
          - wm_capacity: effective WM capacity in number of state-action mappings
          - wm_decay: per-trial decay of WM towards uniform (0..1)
          - lapse: probability of random responding (0..1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_capacity, wm_decay, lapse = model_parameters
    beta = 10.0 * beta  # increase dynamic range
    alpha = np.clip(alpha, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 1.0)
    age_val = float(age[0])

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        W = np.ones((nS, nA)) / nA  # WM distribution over actions per state


        age_penalty = max(0.0, (age_val - 45.0) / 30.0)  # 0 at 45 or below, ~1 at 75
        k_eff = max(0.0, wm_capacity - age_penalty)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]
            ss = int(b_set_sizes[t])

            invalid = (a < 0 or a >= nA or r < 0)
            if invalid:
                nll -= np.log(1.0 / nA + eps)

                W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)
                continue

            q_s = Q[s, :]
            z = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(z)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            p_wm_vec = W[s, :]
            p_wm = p_wm_vec[a]


            wm_weight = np.clip(k_eff / max(1, ss), 0.0, 1.0)

            p_choice = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_final = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            nll -= np.log(np.clip(p_final, eps, 1.0))


            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)

            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0

    return nll