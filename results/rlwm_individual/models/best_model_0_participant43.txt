def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + gated retrieval model with age- and load-dependent gating and noisy retrieval.

    Idea:
    - Choices come from a mixture of a slow RL system (Q-values) and a lightweight "retrieval" system
      that stores the last rewarded action for a state (a sparse WM-like cache).
    - A gate determines whether retrieval is attempted on a trial; the gate closes with higher load
      (set size 6 vs 3) and for older adults.
    - Retrieval itself is noisy: even when the cached action is available, selection is softened and
      mixed with a uniform policy (to capture retrieval noise), more so for older adults.

    Parameters (model_parameters): 5 total
    - alpha: RL learning rate in (0,1]
    - beta: base inverse temperature (>0) used for both RL and retrieval softmax
    - gate_base: baseline gating log-odds (unconstrained real). Higher => more retrieval usage
    - retrieve_noise: [0,1] mixing weight with uniform during retrieval. Higher => noisier retrieval
    - age_sensitivity: >=0 strength with which older age reduces gate and increases retrieval noise

    Inputs:
    - states: np.array of state indices per trial (int)
    - actions: np.array of chosen actions per trial (0..2)
    - rewards: np.array of rewards per trial (0/1)
    - blocks: np.array of block indices per trial
    - set_sizes: np.array of set size per trial (3 or 6)
    - age: np.array with single entry (participant age in years)
    - model_parameters: list/tuple of 5 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, gate_base, retrieve_noise, age_sensitivity = model_parameters

    nll = 0.0
    alpha = min(max(alpha, 1e-6), 1.0)
    beta = max(beta, 1e-6) * 5.0
    retrieve_noise = min(max(retrieve_noise, 0.0), 1.0)
    age_sensitivity = max(0.0, age_sensitivity)

    is_older = 1.0 if age[0] >= 45 else 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx]
        b_states = states[idx].astype(int)
        b_setsize = int(set_sizes[idx][0])

        nA = 3
        nS = b_setsize

        Q = np.zeros((nS, nA))

        M = np.ones((nS, nA)) / nA  # start as uniform (no cache yet)

        load_penalty = 0.0 if b_setsize <= 3 else 1.0

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            exp_rl = np.exp(logits_rl)
            p_rl = exp_rl / np.sum(exp_rl)

            m_s = M[s, :]
            logits_ret = beta * (m_s - np.max(m_s))
            exp_ret = np.exp(logits_ret)
            p_ret_clean = exp_ret / np.sum(exp_ret)

            noise = min(1.0, retrieve_noise * (1.0 + 0.5 * age_sensitivity * is_older))
            p_ret = (1.0 - noise) * p_ret_clean + noise * (1.0 / nA)

            gate_logit = gate_base - (0.75 * load_penalty) - (age_sensitivity * is_older)
            gate = 1.0 / (1.0 + np.exp(-gate_logit))
            gate = min(max(gate, 0.0), 1.0)

            p = gate * p_ret + (1.0 - gate) * p_rl

            pa = max(1e-12, p[a])
            nll -= np.log(pa)

            pe = r - Q[s, a]
            Q[s, a] += alpha * pe



            relax = 0.10 + 0.10 * load_penalty + 0.10 * is_older  # more relaxation with load/age
            relax = min(max(relax, 0.0), 1.0)
            M[s, :] = (1.0 - relax) * M[s, :] + relax * (np.ones(nA) / nA)

            if r > 0.5:

                target = np.zeros(nA)
                target[a] = 1.0
                M[s, :] = 0.2 * M[s, :] + 0.8 * target  # rapid imprinting on reward

    return nll