def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with load- and age-modulated eligibility traces and lapse.

    Idea:
    - Policy (actor) uses softmax over action preferences per state.
    - Value function (critic) learns state values; TD-error updates both actor and critic.
    - Eligibility traces accelerate credit assignment; traces decay faster with higher set size
      and in older adults (reduced maintenance of recent information).
    - A lapse component increases with load and age, mixing in a uniform response probability.

    Parameters (model_parameters, 6 total):
    - alpha_actor: learning rate for actor preferences.
    - alpha_critic: learning rate for state values.
    - beta: inverse temperature for softmax policy; internally scaled by 10.
    - lambda_base: baseline eligibility decay (0..1) at set size 3.
    - load_decay: increase in decay per extra item beyond 3 (reduces effective lambda).
    - age_lapse: added lapse probability for older adults; lapse also increases with load.

    Inputs:
    - states: array of state indices per trial (0..set_size-1).
    - actions: array of chosen actions per trial (0,1,2); values outside [0,2] are treated as lapses (uniform).
    - rewards: array of rewards per trial (0/1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha_actor, alpha_critic, beta, lambda_base, load_decay, age_lapse].

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha_actor, alpha_critic, beta, lambda_base, load_decay, age_lapse = model_parameters
    nA = 3
    eps = 1e-12
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        pref = np.zeros((nS, nA))
        V = np.zeros(nS)

        e_actor = np.zeros((nS, nA))
        e_critic = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            if s < 0 or s >= nS:
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            load = max(0, int(block_set_sizes[t]) - 3)
            lambda_eff = lambda_base * max(0.0, 1.0 - load_decay * load) * (1.0 - 0.3 * is_older)
            lambda_eff = min(max(lambda_eff, 0.0), 1.0)

            beta_eff = beta * 10.0
            logits = beta_eff * pref[s, :]
            logits = logits - np.max(logits)
            p_vec = np.exp(logits)
            p_vec = p_vec / np.sum(p_vec)

            lapse_eff = np.clip(0.02 * load + age_lapse * is_older, 0.0, 0.5)
            p_mix = (1.0 - lapse_eff) * p_vec + lapse_eff * (np.ones(nA) / nA)

            if a >= 0 and a < nA:
                nll -= np.log(max(eps, p_mix[a]))
            else:
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            v_s = V[s]
            v_next = 0.0  # episodic within trial; no next-state provided; treat as 0 baseline
            delta = r + v_next - v_s

            e_actor *= lambda_eff
            e_critic *= lambda_eff

            grad = -p_vec
            if a >= 0 and a < nA:
                grad[a] += 1.0
            e_actor[s, :] += grad
            e_critic[s] += 1.0

            pref += alpha_actor * delta * e_actor
            V += alpha_critic * delta * e_critic

    return nll