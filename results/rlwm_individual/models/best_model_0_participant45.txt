def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and surprise-gated WM use modulated by age and load.

    Idea:
    - RL updates with learning rate and per-visit decay (forgetting) toward uniform, capturing memory loss.
    - WM stores recent rewarded associations and is deterministic, but decays toward uniform over time.
    - Arbitration weight for WM is a logistic function of RL surprise (|prediction error|), penalized by
      higher load (log(nS)) and by age (older adults show lower WM gating).
    - This creates greater reliance on WM after surprising outcomes when load is low and in younger adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - rl_decay: Per-visit RL decay toward uniform for the current state (0..1).
    - gate_base: Baseline bias for using WM in arbitration (can be negative or positive).
    - surprise_gain: Sensitivity of WM gating to absolute RL surprise |PE| (>=0).
    - age_gate_penalty: Reduction in WM gating for older adults (>=0; applied if age>=45).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, rl_decay, gate_base, surprise_gain, age_gate_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_decay = np.clip(0.5 * rl_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            pe = r - Q_s[a]
            surprise = abs(pe)

            load_penalty = np.log(max(2.0, float(nS)))  # stronger penalty at larger nS
            gate_input = gate_base + surprise_gain * surprise - age_gate_penalty * is_older - load_penalty
            p_use_wm = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_t = np.clip(p_use_wm, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)
            q[s, a] += lr * (r - q[s, a])

            w = (1.0 - wm_decay) * w + wm_decay * w_0

            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0

                enc = np.clip(0.5 + 0.5 * surprise, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:

                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p