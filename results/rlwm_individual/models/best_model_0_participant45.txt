def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age-adjusted capacity and decay.
    
    The model assumes two systems:
      - Model-free RL learns Q-values via a Rescorla-Wagner update and selects with softmax.
      - A capacity-limited WM stores (state->action) associations when feedback is correct,
        but decays over time. Retrieval probability scales with effective capacity and set size.
      - Arbitration: policy is a mixture between WM and RL, with retrieval probability as the WM weight.
      - A small lapse rate mixes in uniform choice on every trial.
    
    Inputs:
      - states: np.array of ints, state index on each trial (0..nS-1 within block)
      - actions: np.array of ints, chosen action on each trial (0,1,2). Out-of-range treated as invalid and skipped.
      - rewards: np.array of floats/ints, feedback per trial (1 or 0). -1 indicates invalid/missing and is skipped.
      - blocks: np.array of ints, block id per trial
      - set_sizes: np.array of ints, set size per trial (3 or 6 for this task); constant within block
      - age: np.array with a single scalar age
      - model_parameters: tuple/list with up to 6 parameters:
          alpha       : RL learning rate in [0,1]
          beta        : inverse temperature (>0), scaled internally
          K_base      : baseline WM capacity (in items)
          k_age       : capacity reduction applied if older adult (>=45); K_eff = max(0, K_base - k_age*older)
          wm_decay    : per-trial WM decay in [0,1]
          lapse       : lapse mixing with uniform choice in [0,1]
    
    Returns:
      - negative log-likelihood of the observed choices under the model
    """
    alpha, beta, K_base, k_age, wm_decay, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0  # scale beta to allow sharper policies
    lapse = np.clip(lapse, 0.0, 1.0)
    alpha = np.clip(alpha, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    older = 1.0 if age_val >= 45.0 else 0.0
    
    nll = 0.0
    nA = 3
    eps = 1e-12
    
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        W = np.zeros((nS, nA))

        K_eff = max(0.0, K_base - k_age * older)
        p_retrieve_block = min(1.0, K_eff / max(1.0, float(nS)))
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]

            if a < 0 or a >= nA or r < 0:

                W *= (1.0 - wm_decay)
                continue

            prefs_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            exp_rl = np.exp(prefs_rl)
            pi_rl = exp_rl / np.sum(exp_rl)

            w_s = W[s, :].copy()
            if np.sum(w_s) <= 0:
                pi_wm = np.ones(nA) / nA
            else:
                pi_wm = w_s / np.sum(w_s)

            w_wm = p_retrieve_block

            pi_mix = w_wm * pi_wm + (1.0 - w_wm) * pi_rl
            pi = (1.0 - lapse) * pi_mix + lapse * (1.0 / nA)
            pa = max(eps, pi[a])
            nll -= np.log(pa)

            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            if r >= 0.5:

                W[s, :] *= (1.0 - wm_decay)  # decay others
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0

                W[s, :] = np.maximum(W[s, :], one_hot)
            else:

                W *= (1.0 - wm_decay)
    
    return nll