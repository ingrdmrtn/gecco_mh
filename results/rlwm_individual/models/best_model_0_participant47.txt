def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Dual-rate RL + WM mixture with set-sizeâ€“specific WM weight and WM leak.
    - RL: Q-learning with asymmetric learning rates for positive/negative prediction errors.
    - WM: decays toward uniform within-state; rewarded actions are stored/sharpened.
    - Arbitration: WM weight explicitly depends on set size via two separate weights
      for low load (set size 3) and high load (set size 6).

    Parameters:
      lr_pos:        RL learning rate for positive PE (>=0, <=1)
      lr_neg:        RL learning rate for negative PE (>=0, <=1)
      softmax_beta:  RL inverse temperature (scaled internally by 10; >=0)
      wm_w_small:    WM mixture weight when set size is small (3) (0..1)
      wm_w_large:    WM mixture weight when set size is large (6) (0..1)
      wm_decay:      WM decay toward uniform within-state each encounter (0..1);
                     also sets storage strength via store_strength = clip(2*wm_decay,0,1)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_w_small, wm_w_large, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        if nS <= 3:
            wm_weight_eff = np.clip(wm_w_small, 0.0, 1.0)
        elif nS >= 6:
            wm_weight_eff = np.clip(wm_w_large, 0.0, 1.0)
        else:

            alpha = (nS - 3) / max(1.0, (6 - 3))
            wm_weight_eff = np.clip((1 - alpha) * wm_w_small + alpha * wm_w_large, 0.0, 1.0)

        store_strength = np.clip(2.0 * max(0.0, wm_decay), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:

                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength

                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p