def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Gated WM-RL hybrid with entropy-based WM reliance, modulated by age and set size.

    The policy is a convex combination of a model-free RL softmax and a WM-driven policy.
    WM stores a single most-likely action per state when rewarded (one-shot-like), with
    a reliability that changes with feedback and decays with noise. The gate favoring WM
    increases when WM for that state is confident (low entropy), decreases for larger
    set sizes and in older adults.

    Parameters
    ----------
    states : array-like, int
        State indices on each trial within block (0..nS-1 for that block).
    actions : array-like, int
        Chosen action indices (0..2).
    rewards : array-like, float or int
        Binary rewards (0 or 1).
    blocks : array-like, int
        Block indices corresponding to each trial.
    set_sizes : array-like, int
        Set size of the presented stimuli on each trial (constant within a block).
    age : array-like, float or int
        Participant's age; first element is used to determine age group.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, wm_gate_base, wm_reliance_gain, noise]
        - alpha: RL learning rate in [0,1].
        - beta: base inverse temperature for RL softmax (scaled internally).
        - wm_gate_base: baseline gating bias toward WM (logit space).
        - wm_reliance_gain: gain on (1 - entropy) to weight WM when it's confident.
        - noise: WM noise parameter; lower is sharper WM policy; age increases effective noise.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_gate_base, wm_reliance_gain, noise = model_parameters
    beta *= 10.0

    a = age[0]
    is_older = 1.0 if a >= 45 else 0.0

    noise_eff = max(1e-3, noise * (1.2 * is_older + 0.8 * (1.0 - is_older)))
    beta_wm = 5.0 / noise_eff  # higher => sharper WM policy

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        WM = (1.0 / nA) * np.ones((nS, nA))

        R = np.zeros(nS)

        size_term = -0.6 * (nS - 3)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a_ch = int(block_actions[t])
            r = float(block_rewards[t])

            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / np.sum(p_rl)


            wm_centered = WM[s, :] - (1.0 / nA)
            logits_wm = beta_wm * (wm_centered - np.max(wm_centered))
            p_wm = np.exp(logits_wm)
            p_wm = p_wm / np.sum(p_wm)

            wm_dist = WM[s, :]
            ent = -np.sum(wm_dist * np.log(wm_dist + eps)) / np.log(nA)

            age_term = -0.6 if is_older else 0.6
            gate_logit = wm_gate_base + wm_reliance_gain * (1.0 - ent) + age_term + size_term
            gate = 1.0 / (1.0 + np.exp(-gate_logit))
            gate = min(max(gate, 0.0), 1.0)

            p = gate * p_wm + (1.0 - gate) * p_rl
            nll -= np.log(max(p[a_ch], eps))

            pe = r - Q[s, a_ch]
            Q[s, a_ch] += alpha * pe

            if r > 0.5:

                WM[s, :] = eps
                WM[s, a_ch] = 1.0 - (nA - 1) * eps
                R[s] = min(1.0, R[s] + 0.5)
            else:

                R[s] = max(0.0, R[s] - 0.2)
                WM[s, :] = 0.9 * WM[s, :] + 0.1 * (1.0 / nA)

            WM[s, :] = (1.0 - 0.1 * noise_eff) * WM[s, :] + (0.1 * noise_eff) * (1.0 / nA)
            WM[s, :] = np.maximum(WM[s, :], eps)
            WM[s, :] = WM[s, :] / np.sum(WM[s, :])

    return nll