def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity limits, age-dependent WM weight, and perseveration bias.

    The model blends a model-free RL system with a capacity-limited working-memory (WM)
    system. WM stores the last rewarded action for each state within a block, and its
    influence depends on the block set size relative to WM capacity. Older adults (age>=45)
    are modeled with a reduced effective WM influence and increased perseveration.

    Parameters
    ----------
    states : array-like of int
        State index at each trial within its block (0..nS-1).
    actions : array-like of int
        Observed actions (0..2). Trials with out-of-range actions are ignored in likelihood and updates.
    rewards : array-like of float/int
        Observed rewards (typically 0/1). Values are clipped to [0,1] for learning.
    blocks : array-like of int
        Block index per trial. Learning resets across blocks.
    set_sizes : array-like of int
        Set size (3 or 6) for the current trial/block.
    age : array-like or scalar
        Participant age. Age>=45 -> older group.
    model_parameters : tuple/list of floats
        (alpha, beta, wm_weight, wm_capacity, perseveration, lapse)
        - alpha: RL learning rate in [0,1]
        - beta: inverse temperature (>0); internally scaled by 10
        - wm_weight: baseline WM mixture weight in [0,1]
        - wm_capacity: WM capacity (e.g., around 3-6), used to scale WM influence by set size
        - perseveration: choice stickiness weight added to last chosen action
        - lapse: lapse probability in [0,0.2] mixed with uniform to avoid overconfidence

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_weight, wm_capacity, perseveration, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0
    lapse = min(max(lapse, 0.0), 0.2)
    nA = 3
    age_val = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if age_val >= 45 else 0.0

    wm_weight_eff_scale = 1.0 - 0.3 * is_older
    pers_eff_scale = 1.0 + 0.3 * is_older

    total_loglik = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]
        nS = int(block_set_sizes[0])

        Q = np.ones((nS, nA)) / nA

        WM_store = -1 * np.ones(nS, dtype=int)
        last_action = -1  # for perseveration

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            r = 0.0 if r < 0 else r  # clip negative to 0
            r = 1.0 if r > 1 else r  # clip above 1 to 1

            if s < 0 or s >= nS:
                continue  # skip invalid state entries

            Q_s = Q[s].copy()
            bias = np.zeros(nA)
            if last_action >= 0 and last_action < nA:
                bias[last_action] = perseveration * pers_eff_scale
            logits = beta * Q_s + bias
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_rl = exp_logits / np.sum(exp_logits)


            set_size = int(block_set_sizes[t])
            cap_ratio = min(1.0, max(0.0, wm_capacity / max(1.0, float(set_size))))
            wm_w = wm_weight * wm_weight_eff_scale * cap_ratio

            p_wm = np.ones(nA) / nA
            if 0 <= WM_store[s] < nA:
                stored_a = int(WM_store[s])

                high = 0.9
                low = (1.0 - high) / (nA - 1)
                p_wm = low * np.ones(nA)
                p_wm[stored_a] = high

            p_mix = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (np.ones(nA) / nA)

            if a >= 0 and a < nA:
                p_a = max(1e-12, p_final[a])
                total_loglik += np.log(p_a)

                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                if r >= 0.5:
                    WM_store[s] = a
                else:


                    if WM_store[s] == a:
                        WM_store[s] = -1

                last_action = a
            else:

                continue

    return -float(total_loglik)