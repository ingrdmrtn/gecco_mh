def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-control between WM-like recent-outcome policy and RL, with asymmetric learning and stickiness.
    Gating depends on load (set size) and age; older age reduces WM gating and/or increases reliance on RL.

    Mechanism:
    - RL with separate learning rates for positive and negative prediction errors.
    - Choice stickiness (choice kernel) added to action preferences.
    - A WM-like table W stores the most recent outcome for the chosen action in a state.
      The WM policy favors the most recently rewarded action in that state via softmax.
    - A logistic gate selects how much to rely on WM vs RL; the gate increases at low load,
      and is reduced by older age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) reduces WM gate.
    model_parameters : sequence of 6 floats
        alpha_pos      : Positive PE learning rate
        alpha_neg      : Negative PE learning rate
        beta           : Inverse temperature for both systems (scaled by 10 internally)
        stickiness     : Additive bias for repeating the previous action
        gate_intercept : Intercept of WM gate logistic
        gamma_load     : Sensitivity of WM gate to load (scaled by 3 - set_size)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, beta, stickiness, gate_intercept, gamma_load = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        W = np.zeros((nS, nA))  # last outcome memory (0..1)
        last_action = -1  # for stickiness

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            prefs_rl = Q[s, :].copy()
            if last_action >= 0:
                prefs_rl[last_action] += stickiness
            prefs_rl -= np.max(prefs_rl)
            p_rl = np.exp(beta * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            prefs_wm = W[s, :].copy()
            if last_action >= 0:
                prefs_wm[last_action] += stickiness
            prefs_wm -= np.max(prefs_wm)
            p_wm = np.exp(beta * prefs_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)



            load_term = gamma_load * (3.0 - float(nS))  # 0 for 3, -3 for 6 if gamma_load>0
            age_term = -0.5 * is_older  # fixed reduction for older adults
            z = gate_intercept + load_term + age_term
            gate = 1.0 / (1.0 + np.exp(-z))
            gate = min(max(gate, 0.0), 1.0)

            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            p_a = p_mix[a]
            total_log_p += np.log(max(p_a, eps))

            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            W[s, :] *= 0.9
            W[s, a] = r

            last_action = a

    return -float(total_log_p)