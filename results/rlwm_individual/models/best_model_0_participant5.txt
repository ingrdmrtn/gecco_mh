def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited working memory, decay, and age-modulated WM reliance and decay.
    
    Negative log-likelihood of observed actions under a mixture of:
    - Model-free RL (Q-learning with softmax)
    - Working memory (fast, one-shot caching of correct actions with decay)
    
    Age is used to modulate WM reliance and decay (younger adults rely slightly more on WM and forget less).
    Set size gates WM contribution via a capacity parameter.
    
    Parameters
    ----------
    states : 1D array of int
        State index on each trial (0..set_size-1 for the block).
    actions : 1D array of int
        Chosen action on each trial (assumed 0,1,2 are valid; other values are treated as invalid/missing and skipped).
    rewards : 1D array of int
        Reward on each trial (0/1; other values are treated as invalid/missing and skipped).
    blocks : 1D array of int
        Block index for each trial.
    set_sizes : 1D array of int
        Set size for each trial (3 or 6).
    age : 1D array with one numeric element
        Participant age in years; used to compute age group (<45 younger, >=45 older).
    model_parameters : tuple/list of 6 floats
        (alpha, beta, wm_weight_base, wm_capacity_k, wm_decay, lapse)
        - alpha: RL learning rate in [0,1].
        - beta: inverse temperature (scaled by 10 internally).
        - wm_weight_base: baseline WM mixture weight in [0,1].
        - wm_capacity_k: WM capacity (roughly number of items fully supported by WM).
        - wm_decay: WM decay per trial (toward uniform) in [0,1].
        - lapse: lapse probability mixing with uniform policy in [0,0.2] (recommended).
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, wm_weight_base, wm_capacity_k, wm_decay, lapse = model_parameters
    beta = 10.0 * beta
    wm_capacity_k = max(0.1, wm_capacity_k)  # avoid divide-by-zero
    lapse = np.clip(lapse, 0.0, 0.49)

    ag = age[0]
    is_older = 1.0 if ag >= 45 else 0.0

    wm_weight_mult = 0.8 if is_older else 1.1
    decay_mult = 1.3 if is_older else 0.9

    nll = 0.0
    for b in np.unique(blocks):
        mask_b = (blocks == b)
        block_actions = actions[mask_b]
        block_rewards = rewards[mask_b]
        block_states = states[mask_b]
        block_set_sizes = set_sizes[mask_b]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        W = (1.0 / nA) * np.ones((nS, nA))

        wm_w_eff_base = np.clip(wm_weight_base * wm_weight_mult, 0.0, 1.0)
        wm_decay_eff = np.clip(wm_decay * decay_mult, 0.0, 1.0)

        prev_seen_states = set()

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = int(block_rewards[t])
            s = int(block_states[t])

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1):


                if 0 <= s < nS:
                    W[s, :] = (1 - wm_decay_eff) * W[s, :] + wm_decay_eff * (1.0 / nA)
                continue

            phi = min(1.0, wm_capacity_k / float(nS))
            wm_mix = np.clip(wm_w_eff_base * phi, 0.0, 1.0)

            q_s = Q[s, :]
            q_s_center = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_center)
            p_rl = exp_q / np.sum(exp_q)

            w_s = W[s, :]
            w_s = np.clip(w_s, 1e-12, None)
            w_s = w_s / np.sum(w_s)
            p_wm = w_s

            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            delta = r - q_s[a]
            Q[s, a] += alpha * delta


            W[s, :] = (1 - wm_decay_eff) * W[s, :] + wm_decay_eff * (1.0 / nA)

            if r == 1:
                W[s, :] = (1e-6) * np.ones(nA)
                W[s, a] = 1.0

    return nll