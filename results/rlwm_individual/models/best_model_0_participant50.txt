def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates, perseveration, age- and load-modulated exploration, and lapse-to-nonresponse.
    
    This model is a pure RL account augmented with:
      - Asymmetric learning rates for positive vs. non-positive outcomes
      - Action perseveration (stickiness) within each state
      - Age and load dependent reduction of inverse temperature (more noise for older adults under high load)
      - Lapses that lead to -2 actions

    Negative log-likelihood of the observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block.
    actions : array-like of int
        Action chosen per trial; -2 indicates a lapse/nonresponse.
    rewards : array-like of float or int
        Feedback; positive (>0) is treated as correct.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float or int
        Participant age; older group is age >= 45.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, kappa, age_noise_scale, lapse)
        - alpha_pos: learning rate for positive outcomes (0..1)
        - alpha_neg: learning rate for non-positive outcomes (0..1)
        - beta: base inverse temperature (>0), scaled by load and age
        - kappa: perseveration weight added to the last taken action in a state
        - age_noise_scale: scales how much beta is reduced for older adults under higher set size
        - lapse: probability of lapse resulting in -2 response
    
    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, beta, kappa, age_noise_scale, lapse = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))

        last_a = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            load_ratio = max(1.0, nS_t) / 3.0  # 1 for 3, 2 for 6
            beta_eff = beta * 10.0 / (1.0 + is_old * age_noise_scale * (load_ratio - 1.0))

            Q_s = q[s, :].copy()
            if last_a[s] in [0, 1, 2]:
                Q_s[last_a[s]] += kappa  # bias toward repeating last action in this state

            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(beta_eff * Qc)
            p_task_vec = expQ / np.sum(expQ)

            if a == -2:
                p_total = max(1e-12, min(1.0, lapse))
            else:
                a_clip = int(np.clip(a, 0, nA - 1))
                p_total = max(1e-12, (1.0 - lapse) * p_task_vec[a_clip])

            log_p += np.log(p_total)

            if a in [0, 1, 2]:
                r_pos = 1.0 if r > 0 else 0.0
                alpha_use = alpha_pos if r_pos > 0.5 else alpha_neg
                delta = r_pos - q[s, a]
                q[s, a] = q[s, a] + alpha_use * delta
                last_a[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)