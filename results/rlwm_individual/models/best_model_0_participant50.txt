def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with WM decay.

    Policy:
    - RL: softmax over Q-values with inverse-temperature softmax_beta*10.
    - WM: sharp softmax (beta=50) over WM weights, but diluted by a capacity factor:
        p_slot = min(1, capacity / set_size).
        Effective WM policy = p_slot * softmax(W) + (1 - p_slot) * uniform.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Learning/Updating:
    - RL: delta-rule with learning rate lr.
    - WM: global decay toward a uniform baseline w_0 with wm_decay each trial,
           then state-specific update toward a target vector t_s:
           t_s = w_0[s,:]; t_s[a] = r
           w[s,:] <- (1 - wm_lr) * w[s,:] + wm_lr * t_s

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: mixture weight of WM vs RL (0..1).
    - softmax_beta: base inverse-temperature; internally scaled by 10 for RL.
    - wm_lr: WM learning rate toward target trace (0..1).
    - wm_decay: per-trial WM decay toward uniform (0..1).
    - capacity: WM capacity in number of items (e.g., 1..6); shapes dilution with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_soft_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_slot = min(1.0, float(capacity) / float(nS))
            p_wm = p_slot * p_soft_wm + (1.0 - p_slot) * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            w = (1.0 - wm_decay) * w + wm_decay * w_0

            t_vec = np.copy(w_0[s, :])
            t_vec[a] = r
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * t_vec

        blocks_log_p += log_p

    return -blocks_log_p