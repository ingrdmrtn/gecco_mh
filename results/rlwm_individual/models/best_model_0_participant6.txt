def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with global forgetting and a 'rule' system, with age- and set-size-modulated rule engagement.

    The model mixes:
      - RL system: softmax over Q-values with per-trial global forgetting toward 0.
      - Rule system: once a rewarded action is observed for a state, it is stored as the 'rule'
        and chosen deterministically when the rule system is engaged.

    Rule engagement weight depends on set size and age group:
      - w_rule = clip((rule_base * (1 + age_sign * age_effect)) / set_size, 0, 1),
        where age_sign=+1 for younger (<45) and -1 for older (>=45). Thus, with positive age_effect,
        younger rely more on rules, and larger set sizes reduce rule engagement.

    RL forgetting:
      - On each trial, all Q-values decay by factor (1 - phi_eff), with
        phi_eff = phi * (set_size / 6.0) * (1 + 0.5 * (age_sign < 0) * age_effect),
        i.e., more forgetting for larger set sizes and for older participants when age_effect>0.

    A lapse probability epsilon mixes in uniform choice.

    Parameters (model_parameters):
      - alpha: RL learning rate in (0,1).
      - beta: softmax inverse temperature (scaled up internally).
      - phi: baseline forgetting rate per trial in (0,1).
      - rule_base: baseline rule engagement numerator (capacity-like).
      - age_effect: scales age modulation of rule engagement and forgetting.
      - epsilon: lapse probability.

    Inputs:
      - states: array of state indices (0..nS-1 within block).
      - actions: array of chosen actions (0..2).
      - rewards: array of rewards (0 or 1).
      - blocks: array of block indices.
      - set_sizes: array of set sizes per trial (3 or 6).
      - age: array-like of length 1 or scalar with participant age.
      - model_parameters: list/tuple of parameters as above.

    Returns:
      - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, phi, rule_base, age_effect, epsilon = model_parameters
    beta = beta * 10.0
    epsilon = np.clip(epsilon, 1e-6, 0.2)
    alpha = np.clip(alpha, 0.0, 1.0)
    phi = np.clip(phi, 0.0, 1.0)

    if isinstance(age, (list, tuple, np.ndarray)):
        age_val = float(age[0])
    else:
        age_val = float(age)
    age_sign = 1.0 if age_val < 45 else -1.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        rule_action = -1 * np.ones(nS, dtype=int)  # -1 if no rule learned yet

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            older = 1.0 if age_sign < 0 else 0.0
            phi_eff = phi * (set_size / 6.0) * (1.0 + 0.5 * older * max(age_effect, 0.0))
            phi_eff = np.clip(phi_eff, 0.0, 1.0)

            Q *= (1.0 - phi_eff)

            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s)
            prl = exp_q[a] / (np.sum(exp_q) + 1e-12)

            w_rule = np.clip((rule_base * (1.0 + age_sign * age_effect)) / max(set_size, 1.0), 0.0, 1.0)

            if rule_action[s] >= 0:
                p_rule = 1.0 if a == rule_action[s] else 0.0
            else:
                p_rule = 1.0 / nA

            p_choice = (1.0 - epsilon) * (w_rule * p_rule + (1.0 - w_rule) * prl) + epsilon * (1.0 / nA)
            total_log_p += np.log(max(p_choice, 1e-12))

            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            if r > 0.5:
                rule_action[s] = a

    return -total_log_p