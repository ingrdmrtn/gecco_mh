def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with success-memory boost and age-modulated exploration bonus (UCB-like).

    Idea:
    - Model-free RL provides baseline Q-values.
    - If a state-action has been observed to produce reward in the past, a WM-like
      "success boost" is added to that action's preference for that state (captures
      rapid learning in small sets).
    - An exploration bonus is added to under-sampled actions using a 1/sqrt(N) form.
      Younger adults receive a larger exploration bonus (if age_explore_shift > 0).
      Larger set sizes increase the effective bonus.

    Parameters (model_parameters): [alpha, beta, wm_success_boost, explore_bonus_base, age_explore_shift]
    - alpha: (0,1) RL learning rate.
    - beta: >0 inverse temperature for softmax (internally scaled).
    - wm_success_boost: >=0 additive boost to the preference of previously-successful action in a state.
    - explore_bonus_base: >=0 baseline exploration bonus magnitude.
    - age_explore_shift: real, additive shift to exploration bonus by age group (younger=+1, older=-1).

    Inputs:
    - states: array of state indices (T,)
    - actions: array of chosen actions (T,) in {0,1,2}
    - rewards: array of rewards (T,) in {0,1}
    - blocks: array of block indices (T,)
    - set_sizes: array of set sizes per trial (T,), in {3,6}
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple with the 5 parameters above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_success_boost, explore_bonus_base, age_explore_shift = model_parameters
    beta = max(1e-6, beta) * 5.0
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = np.zeros((nS, nA))

        has_success = np.zeros((nS, nA), dtype=bool)

        N = np.zeros((nS, nA)) + 1e-6  # small offset to avoid div by zero

        load_scale = float(nS) / 3.0  # 1 for nS=3, 2 for nS=6
        explore_bonus = max(0.0, explore_bonus_base + age_explore_shift * age_group) * load_scale

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            pref = Q[s, :].copy()

            if has_success[s, :].any():
                boost_vec = np.zeros(nA)
                boost_vec[has_success[s, :]] = wm_success_boost
                pref = pref + boost_vec

            bonus = explore_bonus / np.sqrt(N[s, :] + 0.0)
            pref = pref + bonus

            logits = beta * (pref - np.max(pref))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            p = max(eps, float(pi[a]))
            total_log_p += np.log(p)

            N[s, a] += 1.0
            Q[s, a] += alpha * (r - Q[s, a])

            if r > 0.0:
                has_success[s, a] = True

    return -total_log_p