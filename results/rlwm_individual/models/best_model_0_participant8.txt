def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Valence-asymmetric RL with set-sizeâ€“dependent forgetting and choice stickiness,
    modulated by age group.

    This model is purely RL-based but includes:
    - Separate learning rates for positive and negative prediction errors.
    - Per-trial decay (forgetting) towards a neutral prior that increases with set size.
    - Choice stickiness that biases repeating the previous action within a block.
    - Older adults exhibit stronger forgetting (decay) than younger adults.

    Returns the negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..nS-1 within each block).
    actions : array-like of int
        Action chosen at each trial (valid 0,1,2; invalid/missed trials may be -2).
    rewards : array-like of float
        Reward observed at each trial (1 or 0; invalid trials may be -1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial; constant within a block.
    age : array-like of int or float
        Age of the participant, provided as a one-element array.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, decay_base, stickiness, age_forget)
        - alpha_pos: Learning rate for positive prediction errors (0..1).
        - alpha_neg: Learning rate for negative prediction errors (0..1).
        - beta: Softmax inverse temperature (positive).
        - decay_base: Baseline forgetting rate toward uniform (0..1 per trial).
        - stickiness: Bias added to the last chosen action (can be positive or negative).
        - age_forget: Proportional increase of decay if age >= 45 (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, decay_base, stickiness, age_forget = model_parameters
    beta = max(1e-6, beta) * 10.0
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    decay_base = np.clip(decay_base, 0.0, 1.0)
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    total_loglik = 0.0
    nA = 3
    prior = np.ones(nA) / nA

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.array(actions)[idx]
        block_rewards = np.array(rewards)[idx]
        block_states = np.array(states)[idx]
        block_set_sizes = np.array(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = np.ones((nS, nA)) / nA
        prev_action = -np.ones(nS, dtype=int)

        set_scale = (float(nS) / 6.0)  # more forgetting when more items to keep track of
        decay_eff = np.clip(decay_base * (1.0 + age_forget * is_older) * set_scale, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            if (a < 0) or (r < 0):

                continue

            Q[s, :] = (1.0 - decay_eff) * Q[s, :] + decay_eff * prior

            stick_vec = np.zeros(nA)
            if prev_action[s] >= 0:
                stick_vec[prev_action[s]] = stickiness

            logits = beta * (Q[s, :] - np.max(Q[s, :])) + stick_vec
            p = np.exp(logits)
            p = p / np.sum(p)

            p_a = max(1e-12, p[a])
            total_loglik += np.log(p_a)

            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            prev_action[s] = a

    return -float(total_loglik)