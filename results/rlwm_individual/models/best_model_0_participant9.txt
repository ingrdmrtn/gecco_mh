def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL with state-local forgetting and directed exploration, modulated by set size and age.

    Mechanism:
    - Model-free RL updates Q(s,a) with a single learning rate.
    - When a state is visited, non-chosen action values in that state decay toward 0 (forgetting),
      capturing load- and age-related maintenance limits.
    - Directed exploration bonus encourages sampling less-visited actions using a count-based bonus.
      The magnitude of the bonus scales with set size and age group.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial in {0,1,2}.
    rewards : array-like of int
        Binary reward per trial in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; e.g., 3 or 6).
    age : array-like of float
        Age in years. Used to define age group (younger <45, older >=45).
    model_parameters : iterable of 5 floats
        alpha          : RL learning rate in [0,1].
        beta           : inverse temperature base, scaled internally by 10.
        decay_base     : base decay strength mapped by logistic to (0,1).
        age_effect     : nonnegative factor scaling age-group differences on decay and exploration.
        explore_bonus  : base scale of directed exploration bonus.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay_base, age_effect, explore_bonus = model_parameters
    beta = beta * 10.0

    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        N = np.zeros((nS, nA))

        base_decay = 1.0 / (1.0 + np.exp(-decay_base))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            eff_decay = base_decay * (set_size / 6.0) * (1.0 + age_effect * is_older)
            eff_decay = min(max(eff_decay, 0.0), 1.0)


            bonus_scale = (set_size / 6.0) * (1.0 - age_effect * is_older)
            bonus_scale = max(0.0, bonus_scale)
            bonus = np.zeros(nA)
            for aa in range(nA):
                bonus[aa] = explore_bonus * bonus_scale / np.sqrt(1.0 + N[s, aa])

            prefs = Q[s, :] + bonus
            prefs_centered = prefs - np.max(prefs)
            expv = np.exp(beta * prefs_centered)
            p = expv / np.sum(expv)

            pa = max(p[a], 1e-12)
            nll -= np.log(pa)

            N[s, a] += 1.0
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            for aa in range(nA):
                if aa != a:
                    Q[s, aa] *= (1.0 - eff_decay)

    return nll