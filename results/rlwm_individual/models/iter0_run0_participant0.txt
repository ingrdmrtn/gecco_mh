def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM) mixture.
    The model mixes a model-free RL policy with a WM policy whose influence depends on set size,
    a capacity parameter, and decay. Age modulates WM capacity (older adults have reduced effective capacity).

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback 0/1 on each trial.
    blocks : array-like of int
        Block index for each trial; used to reset values at block boundaries.
    set_sizes : array-like of int
        Set size for each trial; constant within block; used for WM capacity scaling.
    age : array-like (length 1 or broadcastable)
        Participant's age in years; used to compute age group.
    model_parameters : tuple/list of floats
        (alpha, beta, K, decay, age_effect, wm_temp)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0), scaled internally for numerical range.
        - K: WM capacity (0..3 ideally), effective capacity decreases with set size.
        - decay: WM decay per trial (0..1), applied to WM contents across all states.
        - age_effect: capacity reduction applied if participant is in the older group (>=45).
        - wm_temp: inverse temperature for WM softmax (>0); larger => more deterministic WM choices.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, K, decay, age_effect, wm_temp = model_parameters
    # Scale inverse temperatures to typical RL/WM ranges
    beta = max(1e-6, beta) * 10.0
    wm_temp = max(1e-6, wm_temp) * 10.0
    decay = np.clip(decay, 0.0, 1.0)
    alpha = np.clip(alpha, 0.0, 1.0)
    K = np.clip(K, 0.0, 3.0)
    is_old = 1 if (np.asarray(age)[0] >= 45) else 0
    # Age reduces effective capacity (younger: no reduction; older: subtract age_effect)
    K_eff_base = np.clip(K - is_old * max(0.0, age_effect), 0.0, 3.0)

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM table
        Q = np.ones((nS, nA)) / nA
        W = np.ones((nS, nA)) / nA  # WM action weights per state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_block = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = np.clip(p_rl_vec[a], 1e-12, 1.0)

            # WM policy with capacity and decay
            # Apply per-trial decay to all WM contents
            W = (1.0 - decay) * W + decay * (1.0 / nA)

            w_s = W[s, :]
            logits_wm = wm_temp * (w_s - np.max(w_s))
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = np.clip(p_wm_vec[a], 1e-12, 1.0)

            # Capacity-based WM mixture weight: down-weights WM when set size exceeds capacity
            K_eff = min(K_eff_base, 3.0)
            wm_weight = np.clip(K_eff / float(nS_block), 0.0, 1.0)

            # Mixture policy
            p_total = np.clip(wm_weight * p_wm + (1.0 - wm_weight) * p_rl, 1e-12, 1.0)
            total_log_p += np.log(p_total)

            # RL update
            pe = r - q_s[a]
            Q[s, a] += alpha * pe

            # WM update: write rewarded associations more strongly (one-hot toward chosen action)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite toward one-hot; the decay already softened memory globally
                W[s, :] = one_hot

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-learning-rate RL with set-size-dependent exploration and age-dependent perseveration.
    Policy includes a stickiness term favoring repetition of the last action in the same state.
    Older adults are modeled to have different perseveration strength.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback 0/1 on each trial.
    blocks : array-like of int
        Block index for each trial; used to reset values at block boundaries.
    set_sizes : array-like of int
        Set size for each trial; constant within block; modulates inverse temperature.
    age : array-like (length 1 or broadcastable)
        Participant's age in years; used to determine age group for stickiness.
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg, beta_small, beta_large, kappa_young, kappa_old)
        - alpha_pos: learning rate for positive prediction errors (0..1).
        - alpha_neg: learning rate for negative prediction errors (0..1).
        - beta_small: inverse temperature for set size = 3, scaled internally.
        - beta_large: inverse temperature for set size = 6, scaled internally.
        - kappa_young: perseveration strength (softmax bias) for younger group.
        - kappa_old: perseveration strength (softmax bias) for older group.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, beta_small, beta_large, kappa_young, kappa_old = model_parameters
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    beta_small = max(1e-6, beta_small) * 10.0
    beta_large = max(1e-6, beta_large) * 10.0
    is_old = 1 if (np.asarray(age)[0] >= 45) else 0
    kappa = kappa_old if is_old else kappa_young

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize Q-values
        Q = np.ones((nS, nA)) / nA
        # Track last action per state for perseveration
        last_action_per_state = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_block = int(block_set_sizes[t])

            beta = beta_small if nS_block <= 3 else beta_large

            # Softmax with stickiness bias on the last action in this state
            pref = beta * Q[s, :].copy()
            if last_action_per_state[s] >= 0:
                pref[last_action_per_state[s]] += kappa

            pref -= np.max(pref)
            expv = np.exp(pref)
            p_vec = expv / np.sum(expv)
            p = np.clip(p_vec[a], 1e-12, 1.0)
            total_log_p += np.log(p)

            # RL update with dual learning rates
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Update perseveration memory
            last_action_per_state[s] = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated RL-WM mixture with age- and set-size-dependent gating and lapse.
    WM stores the most recent rewarded action per state. A gating function combines WM and RL policies.
    Gating increases for smaller set sizes and for younger adults, and scales with WM confidence.
    A small lapse rate mixes in uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback 0/1 on each trial.
    blocks : array-like of int
        Block index for each trial; used to reset values at block boundaries.
    set_sizes : array-like of int
        Set size for each trial; constant within block; used in gating.
    age : array-like (length 1 or broadcastable)
        Participant's age in years; used to determine age group that shifts the WM gate.
    model_parameters : tuple/list of floats
        (alpha, beta, gate_base, gate_setsize_slope, age_gate_shift, lapse)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0), scaled internally.
        - gate_base: baseline WM gate intercept; higher favors WM.
        - gate_setsize_slope: slope for (3 - set_size); positive => more WM weight at smaller set sizes.
        - age_gate_shift: additive shift to gate for age group (+ reduces WM for older if negative sign applied below).
        - lapse: lapse rate (0..0.2), mixes in uniform random responding.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, gate_base, gate_setsize_slope, age_gate_shift, lapse = model_parameters
    alpha = np.clip(alpha, 0.0, 1.0)
    beta = max(1e-6, beta) * 10.0
    lapse = np.clip(lapse, 0.0, 0.49)  # keep under 0.5 for identifiability
    is_old = 1 if (np.asarray(age)[0] >= 45) else 0
    # Age code: younger = -1, older = +1 so positive age_gate_shift reduces WM for older if negative coefficient inside sigmoid
    age_code = 1 if is_old else -1

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM stores
        Q = np.ones((nS, nA)) / nA
        W = np.ones((nS, nA)) / nA  # WM belief over actions per state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_block = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            rl_logits = beta * (q_s - np.max(q_s))
            exp_rl = np.exp(rl_logits)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = np.clip(p_rl_vec[a], 1e-12, 1.0)

            # WM policy from stored belief
            w_s = W[s, :]
            wm_logits = 20.0 * (w_s - np.max(w_s))  # deterministic WM without extra parameter
            exp_wm = np.exp(wm_logits)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = np.clip(p_wm_vec[a], 1e-12, 1.0)

            # WM confidence signal (0..1): how peaked the WM distribution is
            sorted_w = np.sort(w_s)[::-1]
            top = sorted_w[0]
            second = sorted_w[1] if len(sorted_w) > 1 else 0.0
            wm_conf = np.clip(top - second, 0.0, 1.0)

            # Gate depends on set size and age; higher for smaller sets and younger adults
            gate_linear = gate_base + gate_setsize_slope * (3 - nS_block) - age_gate_shift * age_code
            g = sigmoid(gate_linear)
            wm_weight = np.clip(g * wm_conf, 0.0, 1.0)

            # Lapse-augmented mixture policy
            p_mix = wm_weight * p_wm_vec + (1.0 - wm_weight) * p_rl_vec
            p_final_vec = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p = np.clip(p_final_vec[a], 1e-12, 1.0)
            total_log_p += np.log(p)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: overwrite with most recent rewarded action; attenuate on non-reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                W[s, :] = one_hot
            else:
                # Slight diffusion toward uniform when non-rewarded
                W[s, :] = 0.5 * W[s, :] + 0.5 * (1.0 / nA)

    return -float(total_log_p)