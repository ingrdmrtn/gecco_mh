def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited working memory (WM) mixture model with WM decay.

    The choice probability is a convex combination of an RL softmax policy and a WM softmax policy.
    WM has limited effectiveness as set size grows, and its contents decay toward a uniform prior.

    Parameters (model_parameters):
    - lr: scalar in [0,1], learning rate for RL Q-values
    - wm_weight: base mixing weight for WM (scaled down by set size)
    - softmax_beta: inverse temperature for RL softmax (scaled by 10 internally)
    - wm_decay: per-trial decay of WM toward uniform (also used as WM learning step)
    - wm_capacity: positive scalar controlling how WM weight scales with set size (effective_w = wm_weight * clip(wm_capacity / nS, 0, 1))

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixture weight scales with set size (capacity limit)
        wm_weight_eff = wm_weight * np.clip(wm_capacity / max(nS, 1), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over WM values with high beta
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning: on rewarded trials, write a strong one-hot memory toward chosen action
            if r > 0.5:
                # Move mass toward a; redistribute mass from others
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture model with a sigmoid capacity gate and asymmetric WM write rule.

    The WM contribution is dynamically gated by set size via a logistic function, capturing
    sharp drops in WM reliance as the number of states exceeds a participant-specific capacity.
    WM decays toward a uniform prior and writes stronger memories after positive feedback,
    weaker after negative feedback (to capture misbinding/uncertainty).

    Parameters (model_parameters):
    - lr: scalar in [0,1], learning rate for RL Q-values
    - wm_weight: base WM weight (maximal reliance when nS << wm_capacity)
    - softmax_beta: inverse temperature for RL softmax (scaled by 10 internally)
    - k_slope: positive slope of the capacity gate (steepness of the drop around capacity)
    - wm_capacity: set-size at which WM reliance is halved (logistic midpoint)
    - wm_decay: per-trial WM decay toward uniform and step size for WM updates

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, k_slope, wm_capacity, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Logistic capacity gate: higher nS -> smaller gate value
        gate = 1.0 / (1.0 + np.exp(k_slope * (nS - wm_capacity)))
        wm_weight_eff = wm_weight * gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (for the current state row)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Asymmetric WM write: stronger consolidation after reward, weaker after no reward
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
            else:
                # Mild move toward the chosen action even without reward (to capture noisy binding)
                weak_step = 0.5 * wm_decay
                w[s, :] = (1.0 - weak_step) * w[s, :]
                w[s, a] += weak_step

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM gating with interference and lapse.

    WM aims to store one-hot correct actions for each state upon reward, but:
    - It forgets over time.
    - It suffers interference as set size increases (reducing WM weight by a power law).
    - A small lapse probability mixes in uniform random choice.

    Parameters (model_parameters):
    - lr: RL learning rate
    - wm_weight: base WM weight (scaled down by nS via a power law)
    - softmax_beta: RL softmax inverse temperature (scaled by 10 internally)
    - wm_forgetting: per-trial forgetting in WM toward uniform
    - wm_interference: interference strength applied to non-current states when writing WM
    - lapse: lapse rate adding uniform random choice to the final policy

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_forgetting, wm_interference, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Power-law reduction in WM weight with set size; cap in [0,1]
        wm_weight_eff = wm_weight * (1.0 / max(nS, 1))**0.5
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting on the current state
            w[s, :] = (1.0 - wm_forgetting) * w[s, :] + wm_forgetting * w_0[s, :]

            # WM write on reward: store one-hot and induce interference on other states
            if r > 0.5:
                # Strengthen memory for current state/action
                w[s, :] = (1.0 - wm_forgetting) * w[s, :]
                w[s, a] += wm_forgetting

                # Interference: nudge other states toward uniform (eroding their specificity)
                if wm_interference > 0.0:
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        w[s_other, :] = (1.0 - wm_interference) * w[s_other, :] + wm_interference * w_0[s_other, :]

        blocks_log_p += log_p

    return -blocks_log_p