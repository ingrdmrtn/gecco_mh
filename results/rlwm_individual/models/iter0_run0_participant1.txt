def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited working memory with age- and set-size-dependent WM weighting.

    Parameters
    ----------
    states : array-like of int
        State indices for each trial within block. Values are in [0, nS-1] for that block.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial. Values group trials into separate blocks; states reset per block.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like (length 1)
        Participant age in years. Used to define age group: younger (<45) vs older (>=45).
    model_parameters : list or tuple
        Model parameters (max 6):
        - alpha: RL learning rate in [0,1]
        - beta: inverse temperature for RL softmax (>0)
        - K: working-memory capacity (effective items) in [0,6]
        - wm_decay: trial-wise decay of WM strengths in [0,1]
        - age_effect: fractional reduction of WM capacity if older in [0,1]
        - lapse: lapse probability, action-independent noise in [0,0.2]

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, K, wm_decay, age_effect, lapse = model_parameters
    beta = max(1e-6, beta) * 5.0  # scale to meaningful range
    lapse = np.clip(lapse, 0.0, 0.49)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        # Initialize RL Q-values and WM memory strengths
        Q = (1.0 / nA) * np.ones((nS, nA))
        M = np.zeros((nS, nA))  # WM strengths in [0,1]

        # Effective WM capacity adjusted by age
        K_eff = max(0.0, K * (1.0 - age_effect * age_group))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            exp_rl = np.exp(beta * q_s)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy: softmax over WM strengths; if all zero => uniform
            m_s = M[s, :]
            if np.all(m_s <= 1e-12):
                p_wm_vec = np.ones(nA) / nA
            else:
                # Emphasize WM reads; stronger strengths yield higher prob
                beta_wm = 2.0 * beta
                ms = m_s - np.max(m_s)
                exp_wm = np.exp(beta_wm * ms)
                p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Set-size and age modulate WM weight
            w_wm = np.clip(K_eff / float(nS), 0.0, 1.0)

            # Lapse-adjusted mixture
            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_final = np.clip(p_final, 1e-12, 1.0)
            total_loglik += np.log(p_final)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM decay and update:
            # - decay all WM strengths in the current state
            M[s, :] *= (1.0 - wm_decay)
            # - if rewarded, set a strong memory for the chosen action
            if r > 0.5:
                M[s, :] = 0.0
                M[s, a] = 1.0

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning rates, perseveration, and age- and load-dependent exploration.

    Parameters
    ----------
    states : array-like of int
        State indices for each trial within block. Values are in [0, nS-1] for that block.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial. Values group trials into separate blocks; states reset per block.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like (length 1)
        Participant age in years. Used to define age group: younger (<45) vs older (>=45).
    model_parameters : list or tuple
        Model parameters (max 6):
        - alpha_pos: learning rate for rewards (r=1) in [0,1]
        - alpha_neg: learning rate for non-rewards (r=0) in [0,1]
        - beta_base: baseline inverse temperature (>0)
        - persev: perseveration bias added to last action's logit (stickiness)
        - age_explore: fractional reduction of beta if older in [0,1]
        - load_beta_scale: reduction of beta with set size; beta /= (1 + load_beta_scale * load), load in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, beta_base, persev, age_explore, load_beta_scale = model_parameters
    beta_base = max(1e-6, beta_base) * 5.0
    persev = float(persev)
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize Q-values and perseveration memory per state
        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action_state = -1 * np.ones(nS, dtype=int)

        # Effective beta adjusts for age (older => more exploration) and set size (higher load)
        load = (max(nS, 3) - 3) / 3.0  # 0 for set size 3, 1 for set size 6
        beta_eff = beta_base * (1.0 - age_explore * age_group)
        beta_eff = beta_eff / (1.0 + load_beta_scale * load)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Construct logits: beta*Q + perseveration bonus for repeating the last action in this state
            logits = beta_eff * Q[s, :].copy()
            la = int(last_action_state[s])
            if la >= 0:
                logits[la] += persev

            # Softmax policy and likelihood of chosen action
            logits = logits - np.max(logits)
            probs = np.exp(logits)
            probs = probs / np.sum(probs)
            p = np.clip(probs[a], 1e-12, 1.0)
            total_loglik += np.log(p)

            # RL update with valence asymmetry
            if r > 0.5:
                Q[s, a] += alpha_pos * (1.0 - Q[s, a])
            else:
                Q[s, a] += alpha_neg * (0.0 - Q[s, a])

            # Update perseveration memory
            last_action_state[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic WM of last rewarded action with interference/decay, age-dependent forgetting, and global stickiness.

    Parameters
    ----------
    states : array-like of int
        State indices for each trial within block. Values are in [0, nS-1] for that block.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Binary feedback on each trial (0 or 1).
    blocks : array-like of int
        Block index for each trial. Values group trials into separate blocks; states reset per block.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6).
    age : array-like (length 1)
        Participant age in years. Used to define age group: younger (<45) vs older (>=45).
    model_parameters : list or tuple
        Model parameters (max 6):
        - alpha: RL learning rate in [0,1]
        - beta_rl: inverse temperature for RL softmax (>0)
        - beta_wm: inverse temperature for WM softmax (>0)
        - decay_interference: per-trial WM confidence decay scaled by set size in [0,1]
        - age_forgetting: added decay if older in [0,1]
        - stickiness: global perseveration bias to repeat last action (state-independent)

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta_rl, beta_wm, decay_interference, age_forgetting, stickiness = model_parameters
    beta_rl = max(1e-6, beta_rl) * 5.0
    beta_wm = max(1e-6, beta_wm) * 5.0
    decay_interference = np.clip(decay_interference, 0.0, 1.0)
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: last rewarded action per state and its confidence c in [0,1]
        stored_action = -1 * np.ones(nS, dtype=int)
        conf = np.zeros(nS)  # confidence that stored action is correct

        # Global last action for stickiness
        last_action_global = -1

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute current decay factor for WM confidence based on load and age
            load = (max(nS, 3) - 3) / 3.0  # 0 for 3, 1 for 6
            decay = np.clip(decay_interference * (0.5 + 0.5 * load) + age_forgetting * age_group, 0.0, 1.0)
            # Apply decay to all confidences each trial (global interference)
            conf *= (1.0 - decay)

            # RL policy logits
            logits_rl = beta_rl * (Q[s, :].copy())

            # Add global stickiness
            if last_action_global >= 0:
                logits_rl[last_action_global] += stickiness

            # WM policy logits: if there is a stored action, favor it proportionally to confidence
            logits_wm = np.zeros(nA)
            sa = int(stored_action[s])
            if sa >= 0 and conf[s] > 1e-12:
                logits_wm[sa] = beta_wm * conf[s]
            # Softmax for RL and WM
            # RL
            lr_logits = logits_rl - np.max(logits_rl)
            p_rl_vec = np.exp(lr_logits)
            p_rl_vec = p_rl_vec / np.sum(p_rl_vec)
            # WM
            wm_logits = logits_wm - np.max(logits_wm)
            p_wm_vec = np.exp(wm_logits)
            p_wm_vec = p_wm_vec / np.sum(p_wm_vec)

            # Mixture weight equals the current confidence for this state
            w = np.clip(conf[s], 0.0, 1.0)
            p_mix_vec = w * p_wm_vec + (1.0 - w) * p_rl_vec
            p = np.clip(p_mix_vec[a], 1e-12, 1.0)
            total_loglik += np.log(p)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM update: if rewarded, store this action with full confidence
            if r > 0.5:
                stored_action[s] = a
                conf[s] = 1.0

            # Update global stickiness memory
            last_action_global = a

    return -float(total_loglik)