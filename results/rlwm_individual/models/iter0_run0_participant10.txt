def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working memory (WM) mixture model with age-sensitive WM utilization.

    On each trial, choice probability is a convex combination of:
      - An RL softmax policy over Q-values.
      - A WM policy based on a decaying associative store that is refreshed by rewarded outcomes.

    The WM contribution scales down with larger set sizes and with older age.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed chosen action on each trial (0..nA-1). nA is fixed to 3.
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial. State sets do not overlap across blocks.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6 here).
    age : array-like (length 1)
        Participant age in years. Used to compute age group.
        Younger group: age < 45; Older group: age >= 45.
    model_parameters : list/tuple of float
        [alpha, beta, C, wm_decay, wm_eta, age_wm_scale]
        - alpha: RL learning rate in [0,1].
        - beta: inverse temperature (>0) for RL softmax; internally scaled by 10.
        - C: WM capacity (e.g., between 1 and 6). Scales WM weight as min(1, C/set_size).
        - wm_decay: per-visit decay of WM row toward uniform in [0,1].
        - wm_eta: encoding strength on rewarded trials in [0,1].
        - age_wm_scale: multiplicative reduction in WM weight for older adults (0=no reduction, 1=full reduction).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    alpha, beta, C, wm_decay, wm_eta, age_wm_scale = model_parameters
    beta = max(1e-6, beta) * 10.0
    nA = 3
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0
    # Age factor reduces WM utilization for older adults; no change for younger.
    age_wm_factor = 1.0 - is_older * np.clip(age_wm_scale, 0.0, 1.0)

    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_setsize = int(set_sizes[mask][0])
        nS = b_setsize

        # Initialize RL and WM stores
        Q = np.zeros((nS, nA))  # value-learning starts neutral
        W = np.ones((nS, nA)) / nA  # WM distribution per state starts uniform

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # RL policy
            q = Q[s, :]
            q_centered = q - np.max(q)
            expq = np.exp(beta * q_centered)
            p_rl_all = expq / np.sum(expq)
            p_rl = max(1e-12, p_rl_all[a])

            # WM policy: use the WM distribution for this state
            w_row = W[s, :]
            # Ensure it is a valid distribution (numerical safety)
            w_row = np.clip(w_row, 1e-12, 1.0)
            w_row = w_row / np.sum(w_row)
            p_wm = max(1e-12, w_row[a])

            # Compute a state- and set-size-dependent WM mixture weight
            # - Scales with capacity relative to set size
            # - Scales with WM certainty in this state
            # - Reduced for older age
            cap_scale = min(1.0, max(0.0, C) / max(1.0, float(nS)))
            # WM certainty: how peaked is the distribution (0 at uniform, 1 at delta)
            peak = np.max(w_row)
            uniform = 1.0 / nA
            wm_certainty = (peak - uniform) / max(1e-12, (1.0 - uniform))
            wm_weight = np.clip(cap_scale * wm_certainty * age_wm_factor, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, p_total)
            total_logp += np.log(p_total)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM decay on visited state row toward uniform
            W[s, :] = (1.0 - wm_decay) * W[s, :] + wm_decay * (1.0 / nA)

            # WM encoding on rewarded trials (refresh the correct association)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                W[s, :] = (1.0 - wm_eta) * W[s, :] + wm_eta * one_hot

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size-specific learning rates, state-wise perseveration, and age-dependent exploration.

    The policy is a softmax over action preferences composed of:
      - Q-values (learned with different learning rates for small vs large set sizes),
      - A perseveration bias that favors repeating the last action taken in the same state.

    Older adults are modeled as having lower inverse temperature (more exploratory choices).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed chosen action on each trial (0..nA-1).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6 here).
    age : array-like (length 1)
        Participant age in years. Younger: age < 45, Older: age >= 45.
    model_parameters : list/tuple of float
        [alpha_small, alpha_large, beta_base, perseveration_kappa, age_beta_bonus, decay_base]
        - alpha_small: RL learning rate for small set size blocks (e.g., 3).
        - alpha_large: RL learning rate for large set size blocks (e.g., 6).
        - beta_base: baseline inverse temperature (>0), scaled internally by 10.
        - perseveration_kappa: additive bias for repeating the last action in a state.
        - age_beta_bonus: proportional reduction to beta for older adults (0=no reduction; 1=100% reduction).
        - decay_base: value decay toward zero that grows with set size: effective_decay = decay_base * ((nS-3)/3).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    alpha_small, alpha_large, beta_base, kappa, age_beta_bonus, decay_base = model_parameters
    nA = 3
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Select learning rate based on set size
        alpha = alpha_small if nS <= 3 else alpha_large

        # Age-adjusted inverse temperature: older => lower beta
        beta = max(1e-6, beta_base) * 10.0
        beta = beta * (1.0 - is_older * np.clip(age_beta_bonus, 0.0, 1.0))
        beta = max(1e-6, beta)

        # State-action values and last-action memory for perseveration
        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # -1 means no previous action in this state yet

        # Set-size-dependent value decay toward zero (more decay in larger sets)
        eff_decay = np.clip(decay_base, 0.0, 1.0) * max(0.0, (nS - 3) / 3.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Preference includes perseveration bias toward repeating last action in this state
            pref = Q[s, :].copy()
            if last_action[s] >= 0:
                pref[last_action[s]] += kappa

            # Softmax
            pref_centered = pref - np.max(pref)
            expp = np.exp(beta * pref_centered)
            p_all = expp / np.sum(expp)
            p = max(1e-12, p_all[a])
            total_logp += np.log(p)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # Optional decay towards zero, scaled by set size
            Q[s, :] *= (1.0 - eff_decay)

            # Update perseveration memory
            last_action[s] = a

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted Bayesian RL combined with item-specific WM gating, and age-dependent WM noise.

    RL component:
      - Maintains Beta-Bernoulli (Dirichlet over binary outcomes) counts per state-action.
      - Choice values are posterior means of reward probabilities.

    WM component:
      - Stores the last rewarded action per state (if any).
      - When available, proposes that action with high probability (noisy recall).
      - Weight of WM contribution scales with capacity relative to set size.

    Age effect:
      - Older adults have noisier WM recall (larger epsilon), implemented as higher lapse probability
        in the WM channel.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Observed chosen action on each trial (0..nA-1).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (3 or 6 here).
    age : array-like (length 1)
        Participant age in years; Younger: age < 45; Older: age >= 45.
    model_parameters : list/tuple of float
        [alpha_prior, beta, C, wm_conf, eps_base, age_eps_bonus]
        - alpha_prior: symmetric Beta prior strength for success and failure counts (>0).
        - beta: inverse temperature for softmax in the RL channel; internally scaled by 10.
        - C: WM capacity scaling the WM weight as min(1, C/set_size).
        - wm_conf: base WM mixture weight when an item is in WM (0..1).
        - eps_base: base WM recall noise (lapse) in (0, 0.5).
        - age_eps_bonus: multiplicative increase to epsilon for older adults (0=no change; 1=+100%).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    alpha_prior, beta, C, wm_conf, eps_base, age_eps_bonus = model_parameters
    beta = max(1e-6, beta) * 10.0
    nA = 3
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    # Age-adjusted WM noise
    epsilon = np.clip(eps_base * (1.0 + is_older * np.clip(age_eps_bonus, 0.0, 5.0)), 1e-6, 0.49)

    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # RL counts: successes and failures per state-action
        succ = np.ones((nS, nA)) * max(1e-6, alpha_prior)
        fail = np.ones((nS, nA)) * max(1e-6, alpha_prior)

        # WM store: -1 if none stored; otherwise store action index last rewarded
        wm_store = -np.ones(nS, dtype=int)

        # WM weight scales down with set size (capacity-limited)
        wm_weight_base = np.clip(wm_conf, 0.0, 1.0) * min(1.0, max(0.0, C) / max(1.0, float(nS)))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # RL posterior mean Q
            Q = succ[s, :] / (succ[s, :] + fail[s, :])
            Qc = Q - np.max(Q)
            expq = np.exp(beta * Qc)
            p_rl_all = expq / np.sum(expq)
            p_rl = max(1e-12, p_rl_all[a])

            # WM policy (if an item is stored for this state)
            if wm_store[s] >= 0:
                a_star = int(wm_store[s])
                if a == a_star:
                    p_wm = 1.0 - epsilon
                else:
                    p_wm = epsilon / (nA - 1)
                w_wm = wm_weight_base
            else:
                # No WM information; effectively no WM contribution
                # Keep p_wm uniform but set weight to 0 so it doesn't matter
                p_wm = 1.0 / nA
                w_wm = 0.0

            p_total = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_total = max(1e-12, p_total)
            total_logp += np.log(p_total)

            # RL count update
            succ[s, a] += r
            fail[s, a] += (1.0 - r)

            # WM update: store the rewarded action for this state
            if r > 0.5:
                wm_store[s] = a

    return -float(total_logp)