Below are three standalone cognitive models that combine reinforcement learning (RL) with working memory (WM) in different ways. Each function follows the provided template structure, returns the negative log-likelihood of observed choices, and uses all parameters meaningfully. All models allow WM contribution to vary with set size (load).

Note: Assumes numpy as np is available and no imports are included in the code.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with decay

    Description:
    - Combines a standard Q-learning policy with a working-memory (WM) policy that rapidly stores
      action-outcome information but suffers decay and limited capacity.
    - WM effective weight scales down with set size: wm_mix = wm_weight * min(1, K / nS).
      Thus, WM dominates in small set-size blocks and diminishes as load increases.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Base mixture weight of WM (0..1), scaled by capacity vs set size
    - softmax_beta: Inverse temperature for RL; internally scaled by 10 for a higher upper bound
    - wm_lr: WM learning rate (0..1); one-shot-ish learning in WM after feedback
    - wm_decay: Decay toward uniform baseline in WM per trial (0..1)
    - wm_capacity: Effective WM capacity (e.g., 1..6), scales WM contribution as min(1, K / nS)

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, wm_capacity = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM retrieval is highly deterministic
    eps = 1e-12  # numerical stability

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM matrices
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mixing weight decreases with load
        wm_mix = wm_weight * min(1.0, float(wm_capacity) / max(1, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy probability of chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform + associative update using outcome
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Outcome-dependent strengthening/suppression
            target = np.zeros(nA)
            target[a] = r  # strengthen chosen action if rewarded
            w[s, :] += wm_lr * (target - w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with perseveration and exploration; WM efficacy reduced by interference (set size)

    Description:
    - RL component uses a softmax over Q-values combined with a decaying choice kernel
      to capture perseveration (sticky choices).
    - WM component stores recent rewarded actions with decay; its effective weight is reduced
      by interference that scales with set size: wm_mix = wm_weight / (1 + interference*(nS - 1)).
    - Final policy mixes RL and WM and includes an epsilon-greedy exploration component.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture (0..1)
    - softmax_beta: Inverse temperature for RL; internally scaled by 10
    - epsilon: Lapse/exploration probability (0..1), adds uniform choice noise
    - kappa: Perseveration strength (>=0), weights the choice kernel in RL policy
    - decay: Shared decay for WM values and choice kernel (0..1)

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, epsilon, kappa, decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values, WM store, baseline, and choice kernel
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        ck = np.zeros((nS, nA))  # choice kernel (perseveration)

        # WM effective weight reduced by interference with set size
        # interference factor increases with nS - 1; letting 'decay' serve as interference coefficient
        interference = decay
        wm_mix = wm_weight / (1.0 + interference * max(0, nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with perseveration bias via choice kernel
            Qeff = q[s, :] + kappa * ck[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Qeff - Qeff[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture with epsilon-greedy lapses
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform + reward-based adjustment
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            target = np.zeros(nA)
            target[a] = r
            # small WM learning rate tied to decay to avoid extra parameter; ensures parameter use
            w_lr = decay
            w[s, :] += w_lr * (target - w[s, :])

            # Choice kernel update (perseveration): decay then increment chosen action
            ck[s, :] *= (1.0 - decay)
            ck[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + gated WM (logistic load gating) and one-shot caching

    Description:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM acts as a one-shot cache: after a rewarded action, WM stores a near-deterministic
      mapping for that state; it slowly decays otherwise.
    - WM gating depends smoothly on set size via a logistic function:
        wm_mix = wm_weight * sigmoid(gamma * (K50 - nS)),
      producing high WM reliance for small nS and reduced reliance for large nS.

    Parameters (tuple):
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - wm_weight: Base WM mixture (0..1), modulated by logistic gating with set size
    - softmax_beta: Inverse temperature for RL; internally scaled by 10
    - gamma: Slope of the logistic gating function (>0 makes transition sharper)
    - K50: Set size at which WM contribution is half-max (e.g., between 1 and 6)

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gamma, K50 = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_idx = (blocks == b)
        block_actions = actions[block_idx]
        block_rewards = rewards[block_idx]
        block_states = states[block_idx]
        block_set_sizes = set_sizes[block_idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM maps
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Logistic WM gating by set size
        wm_mix = wm_weight * sigmoid(gamma * (float(K50) - float(nS)))

        # WM decay rate tied to gating (smaller gating -> faster forgetting)
        wm_decay = 1.0 - sigmoid(gamma * (float(K50) - float(nS)))  # more decay when nS > K50

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability with asymmetric learning
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM update:
            # - decay toward uniform baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # - one-shot caching upon reward: push WM toward a one-hot map for chosen action
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                # fast consolidation when rewarded
                wm_lr = 1.0
                w[s, :] += wm_lr * (target - w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p