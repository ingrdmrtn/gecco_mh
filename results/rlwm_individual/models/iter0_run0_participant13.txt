def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-scaled WM contribution.
    
    On each trial, the choice policy is a mixture of:
    - Model-free RL (Q-learning with softmax).
    - A fast, one-shot Working Memory (WM) store that encodes the last rewarded action for a state.
    
    WM influence is scaled down as set size increases (capacity pressure), approximated by a simple
    capacity ratio (3/nS) that prioritizes smaller set sizes. WM traces also decay slightly over time.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1). Effective weight is scaled by set size per block.
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10.0  # higher dynamic range for RL choice
    softmax_beta_wm = 50.0  # near-deterministic WM choice
    blocks_log_p = 0.0
    eps = 1e-12  # numerical stability
    
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM weight: stronger in smaller set sizes
        # Reference capacity is 3; scale down linearly.
        wm_weight_block = wm_weight * min(1.0, 3.0 / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob for chosen action a (softmax trick in template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: mild decay towards uniform + one-shot write on rewarded trials
            # Decay
            decay = 0.10
            w = (1.0 - decay) * w + decay * w_0
            # Reward-gated storage (one-shot)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with confidence-adaptive WM influence.
    
    This model blends RL with a WM store but adjusts the effective WM weight within each state
    based on WM confidence (how peaked the WM distribution is) and set size. WM is updated on all trials,
    with stronger updates after rewards and weaker after non-rewards, and decays over time.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1). Modulated by WM confidence and set size.
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Baseline set-size modulation (same intuition as model1)
        base_wm_scale = wm_weight * min(1.0, 3.0 / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM confidence: how peaked is W_s?
            # Use the max-min spread as a simple confidence proxy in [0,1].
            spread = np.max(W_s) - np.min(W_s)  # in [0,1]
            wm_conf = spread

            # Effective WM weight incorporates state-specific confidence
            eff_wm_weight = base_wm_scale * wm_conf

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay + graded update by outcome
            # Decay
            decay = 0.10
            w = (1.0 - decay) * w + decay * w_0
            # Graded write: strong toward chosen action on reward; weak on non-reward
            alpha_pos = 1.0   # full overwrite when rewarded
            alpha_neg = 0.25  # partial update when not rewarded
            if r > 0.0:
                w[s, :] = (1.0 - alpha_pos) * w[s, :]  # which zeroes if alpha_pos=1
                w[s, a] += alpha_pos
            else:
                # Slight bias toward the attempted action even when not rewarded (error monitoring)
                w[s, :] = (1.0 - alpha_neg) * w[s, :]
                w[s, a] += alpha_neg

            # Renormalize W_s to keep it a probability distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM reliability arbitration.
    
    Rather than a fixed mixture, this model arbitrates between RL and WM based on their
    momentary reliabilities. WM reliability increases when WM is peaked; RL reliability
    increases when the RL policy is decisive (peaked Q softmax).
    
    The base arbitration is governed by wm_weight, with set-size and confidence determining
    how much weight actually goes to WM vs RL on each trial. The final policy is a
    reliability-weighted average of the two policies.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 3
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base arbitration bias toward WM (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Baseline bias to WM reduces with larger set sizes
        base_wm_bias = wm_weight * min(1.0, 3.0 / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Full policies (not just chosen-action probabilities)
            # RL softmax
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)

            # WM softmax
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)

            # Reliability estimates: use entropy-based confidence
            def entropy(p):
                p_safe = np.clip(p, eps, 1.0)
                return -np.sum(p_safe * np.log(p_safe))
            H_rl = entropy(rl_probs)
            H_wm = entropy(wm_probs)
            # Convert entropy to confidence in [0,1] by comparing to max entropy (log nA)
            H_max = np.log(nA)
            conf_rl = 1.0 - (H_rl / H_max)
            conf_wm = 1.0 - (H_wm / H_max)

            # Arbitration weights
            w_wm = base_wm_bias * conf_wm
            w_rl = (1.0 - base_wm_bias) * conf_rl
            if (w_wm + w_rl) <= 0:
                alpha = base_wm_bias
            else:
                alpha = w_wm / (w_wm + w_rl)

            mix_probs = alpha * wm_probs + (1.0 - alpha) * rl_probs

            p_total = max(mix_probs[a], eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay + reward-gated one-shot write (as in model1)
            decay = 0.10
            w = (1.0 - decay) * w + decay * w_0
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p