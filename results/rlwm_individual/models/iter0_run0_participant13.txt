def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM mixture with age-dependent capacity and WM decay.

    The model mixes a standard RL policy with a working-memory (WM) policy.
    The WM contribution is capacity-limited and depends on set size and age group (younger vs older).
    WM contents update rapidly and decay toward uniform when feedback is not instructive.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial (states reset across blocks).
    set_sizes : array-like of int
        Set size for the block of each trial (e.g., 3 or 6).
    age : array-like of float
        Participant age; age[0] is used to define age group.
    model_parameters : tuple/list of floats
        (alpha, beta, K_young, K_old, wm_weight, phi)
        - alpha: RL learning rate (0..1 after squashing)
        - beta: inverse temperature for RL softmax (scaled internally)
        - K_young: WM capacity for younger group
        - K_old: WM capacity for older group
        - wm_weight: base mixture weight of WM vs RL (0..1 after squashing)
        - phi: WM update fraction toward target each trial (0..1 after squashing)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    # Unpack and squash parameters to valid ranges
    alpha, beta, K_y, K_o, wm_weight, phi = model_parameters
    # Squash into valid ranges
    alpha = 1 / (1 + np.exp(-alpha))
    wm_weight = 1 / (1 + np.exp(-wm_weight))
    phi = 1 / (1 + np.exp(-phi))
    beta = (1 / (1 + np.exp(-beta))) * 15.0  # map to (0,15) range for softmax sharpness

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0
    K = K_o if older else K_y

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM weights (probabilities over actions per state)
        Q = np.zeros((nS, nA))
        W = np.ones((nS, nA)) / nA  # WM distribution per state starts uniform

        # Probability that the state is stored in WM given capacity and set size
        p_in = min(1.0, max(0.0, K / float(nS)))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s_centered = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_centered)
            pi_rl = exp_q / (np.sum(exp_q) + eps)
            p_rl = pi_rl[a]

            # WM policy (softmax over WM distribution with a high precision)
            w_s = W[s, :]
            # Avoid zero entries; add small epsilon before softmax
            w_s_centered = w_s - np.max(w_s)
            exp_w = np.exp(50.0 * w_s_centered)  # near-argmax behavior for WM
            pi_wm = exp_w / (np.sum(exp_w) + eps)
            p_wm = pi_wm[a]

            # Mixture weighting: WM contributes proportional to capacity coverage
            mix = wm_weight * p_in
            p_total = mix * p_wm + (1.0 - mix) * p_rl
            nll -= np.log(max(p_total, eps))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update:
            # - If reward=1: move WM distribution toward one-hot on chosen action (fast encoding)
            # - If reward=0: decay WM back toward uniform (uncertainty)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                W[s, :] = (1.0 - phi) * W[s, :] + phi * target
            else:
                uniform = np.ones(nA) / nA
                W[s, :] = (1.0 - phi) * W[s, :] + phi * uniform

            # Normalize WM row to be a proper distribution
            row_sum = np.sum(W[s, :])
            if row_sum > 0:
                W[s, :] /= row_sum
            else:
                W[s, :] = np.ones(nA) / nA

    return nll



def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pure RL with age- and load-dependent learning rate, action perseveration, and lapse.

    The model uses a single RL system whose learning rate varies with age group and set size.
    A perseveration bias (stickiness) adds value to repeating the last action in the same state.
    A small lapse mixes the policy with a uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group.
    model_parameters : tuple/list of floats
        (alpha_base, alpha_age_slope, alpha_size_penalty, beta, perseveration, lapse)
        - alpha_base: base learning rate before modulations (squashed to 0..1)
        - alpha_age_slope: additive effect of age group on alpha (older=1, younger=0)
        - alpha_size_penalty: reduction in alpha for larger set size (applied when nS==6)
        - beta: inverse temperature (scaled internally)
        - perseveration: bias added to previously chosen action in same state
        - lapse: lapse rate mixing with uniform policy (squashed to 0..0.5)
          Note: all parameters are used to compute trial-by-trial likelihood.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    alpha_base, alpha_age_slope, alpha_size_penalty, beta, perseveration, lapse = model_parameters
    # Squash/scale params
    beta = (1 / (1 + np.exp(-beta))) * 15.0
    lapse = (1 / (1 + np.exp(-lapse))) * 0.5  # cap lapse at 0.5
    # alpha will be computed per trial based on age group and set size; use sigmoid to keep 0..1
    # perseveration used directly as bias weight

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))
        # Track last action per state for perseveration bias; -1 means none
        last_action = -np.ones(nS, dtype=int)

        # Compute effective learning rate for this block given age group and load
        raw_alpha = alpha_base + alpha_age_slope * older - alpha_size_penalty * (1 if nS >= 6 else 0)
        alpha_eff = 1 / (1 + np.exp(-raw_alpha))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Preferences = Q plus perseveration bias on repeating last action in same state
            prefs = Q[s, :].copy()
            if last_action[s] >= 0:
                prefs[last_action[s]] += perseveration

            # Softmax with lapse
            prefs_centered = prefs - np.max(prefs)
            exp_p = np.exp(beta * prefs_centered)
            pi = exp_p / (np.sum(exp_p) + eps)
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = pi[a]
            nll -= np.log(max(p, eps))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha_eff * delta

            # Update last action for perseveration
            last_action[s] = a

    return nll



def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and uncertainty-based arbitration with a volatile WM cache.

    The model combines:
    - RL with separate learning rates for positive and negative prediction errors.
    - A lightweight WM cache that stores the last rewarded action per state with a strength that decays.
    - Arbitration weight depends on set size (capacity limit) and age group (older group has more decay).

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) observed each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group.
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg, beta, wm_gate, capacity_K, age_noise)
        - alpha_pos: learning rate for positive PEs (squashed to 0..1)
        - alpha_neg: learning rate for negative PEs (squashed to 0..1)
        - beta: inverse temperature for RL softmax (scaled internally)
        - wm_gate: base arbitration weight for WM contribution (0..1 after squashing)
        - capacity_K: WM capacity controlling p_in = min(1, K/nS)
        - age_noise: increases WM decay and reduces WM contribution for older group

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    import numpy as np

    alpha_pos, alpha_neg, beta, wm_gate, capacity_K, age_noise = model_parameters
    # Squash/scale parameters
    alpha_pos = 1 / (1 + np.exp(-alpha_pos))
    alpha_neg = 1 / (1 + np.exp(-alpha_neg))
    wm_gate = 1 / (1 + np.exp(-wm_gate))
    beta = (1 / (1 + np.exp(-beta))) * 15.0
    age_noise = max(0.0, age_noise)  # ensure non-negative noise scaling

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))

        # WM cache: for each state, store last rewarded action and its strength m in [0,1]
        wm_action = -np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS)

        # Capacity-dependent inclusion prob
        p_in = min(1.0, max(0.0, capacity_K / float(nS)))
        # Older age reduces effective WM participation and increases decay
        age_factor = np.exp(-age_noise * older)  # in (0,1] when older=1

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s_centered = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_centered)
            pi_rl = exp_q / (np.sum(exp_q) + eps)
            p_rl = pi_rl[a]

            # WM policy: if we have a cached rewarded action, choose it; otherwise uniform
            if wm_action[s] >= 0 and wm_strength[s] > 0:
                wm_probs = np.ones(nA) * ((1.0 - 0.999) / (nA - 1))
                wm_probs[wm_action[s]] = 0.999  # near-deterministic toward cached action
            else:
                wm_probs = np.ones(nA) / nA
            p_wm = wm_probs[a]

            # Arbitration weight
            w = wm_gate * p_in * age_factor * wm_strength[s]
            p_total = w * p_wm + (1.0 - w) * p_rl
            nll -= np.log(max(p_total, eps))

            # RL update with asymmetric learning
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # WM update:
            # - If reward=1: store this action with full strength
            # - If reward=0: decay strength
            if r > 0.5:
                wm_action[s] = a
                wm_strength[s] = 1.0
            else:
                # Decay depends on capacity/load and age (faster decay for older or larger set size)
                decay = (0.5 + 0.5 * (nS / max(1.0, capacity_K)))  # more decay when nS > K
                decay *= (1.0 + age_noise * older)                 # extra decay for older group
                wm_strength[s] = max(0.0, wm_strength[s] * np.exp(-decay))

    return nll