def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited working memory and reward-tagged storage.

    Idea:
    - Choices are a convex combination of RL softmax policy and a WM policy.
    - WM is more effective in smaller set sizes via a capacity parameter K (effective WM weight scales with K / nS).
    - WM decays toward uniform on every encounter of a state, and is updated strongly on rewarded trials (replacement).
    - RL updates via a standard delta rule.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight: scalar in [0,1], base mixture weight for WM policy (before capacity scaling).
    - softmax_beta: scalar >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - K: positive scalar, WM capacity (number of items that can be maintained effectively).
    - wm_decay: scalar in [0,1], decay of WM values toward uniform when a state is visited.
    - wm_replace: scalar in [0,1], strength of WM replacement on rewarded trials toward a one-hot mapping for the chosen action.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, wm_weight, softmax_beta, K, wm_decay, wm_replace = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # capacity scaling for WM effect
        capacity_factor = min(1.0, max(0.0, K / max(1.0, nS)))
        wm_weight_eff = wm_weight * capacity_factor

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM values (high beta -> near-deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # mixture of WM and RL
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)  # numerical guard

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # decay WM toward uniform for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # on rewarded trials, strengthen a one-hot memory for the chosen action
            if r > 0:
                w[s, :] = (1.0 - wm_replace) * w[s, :]
                w[s, a] += wm_replace

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with adaptive arbitration and lapses:
    - WM influence scales with set size (capacity K) and with RL uncertainty (entropy-based arbitration).
    - RL learning uses separate learning rates for positive vs. negative outcomes.
    - Occasional lapses produce random choices.

    Parameters (model_parameters):
    - lr_pos: scalar in [0,1], RL learning rate on rewarded trials.
    - lr_neg: scalar in [0,1], RL learning rate on non-rewarded trials.
    - base_wm_weight: scalar in [0,1], baseline WM mixture weight (before arbitration).
    - softmax_beta: scalar >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - K: positive scalar, WM capacity controlling set-size dependence of WM.
    - lapse: scalar in [0,0.2], lapse probability of emitting a random response.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr_pos, lr_neg, base_wm_weight, softmax_beta, K, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    lapse = np.clip(lapse, 0.0, 0.5)  # safety
    blocks_log_p = 0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        capacity_factor = min(1.0, max(0.0, K / max(1.0, nS)))

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy chosen-action probability (template-compatible form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # For arbitration: compute full RL softmax to get entropy
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Qc)
            pvec_rl = expQ / np.sum(expQ)
            entropy_rl = -np.sum(pvec_rl * np.log(pvec_rl + 1e-12))  # 0..log(3)

            # WM policy (chosen-action probability)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Adaptive arbitration:
            # weight rises with RL uncertainty (higher entropy -> rely more on WM) and falls with set size.
            # Using a fixed slope around entropy ~ 0.5.
            arb = 1.0 / (1.0 + np.exp(-5.0 * (entropy_rl - 0.5)))  # sigmoid in [0,1]
            wm_weight_eff = base_wm_weight * capacity_factor * arb

            mixture = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * mixture + lapse * (1.0 / nA)
            log_p += np.log(p_total + 1e-12)

            # RL update with valence-dependent learning rate
            lr = lr_pos if r > 0 else lr_neg
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform, replace on reward
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]  # mild generic decay per encounter
            if r > 0:
                # strong overwrite for rewarded mapping
                w[s, :] = 0.2 * w[s, :] + 0.8 * w_0[s, :]
                w[s, a] = 1.0  # push toward a one-hot memory
                # renormalize softly to keep values in [0,1]
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with time-based WM decay and set-size dependent recall probability.

    Idea:
    - WM stores recent correct mappings with a strength that decays with time since last encounter,
      and decays faster under higher set sizes (via K).
    - Effective use of WM on each trial is weighted by a recall probability that scales with K/nS.
    - RL runs in parallel and learns via a delta rule.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight: scalar in [0,1], base weight of the WM policy when recalled.
    - softmax_beta: scalar >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - K: positive scalar, WM capacity determining both recall probability and decay rate scaling.
    - wm_decay: scalar in [0,1], baseline per-trial WM decay rate used to compute time-based forgetting.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last seen time for each state within the block (for time-based decay)
        last_seen = -1 * np.ones(nS, dtype=int)

        # Recall probability scales with capacity and set size
        p_recall = min(1.0, max(0.0, K / max(1.0, nS)))
        wm_weight_eff = wm_weight * p_recall

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Apply time-based decay to WM for state s since last seen
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
                # Effective decay increases with set size (smaller K or larger nS => faster forgetting)
                decay_eff = 1.0 - (1.0 - wm_decay) ** max(1, int(np.ceil(nS / max(K, 1e-6))))
                decay_factor = (1.0 - decay_eff) ** dt
                w[s, :] = decay_factor * w[s, :] + (1.0 - decay_factor) * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM chosen-action probabilities
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with recall-weighted WM
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: if rewarded, store a strong one-hot trace; otherwise, mild reversion to baseline
            if r > 0:
                w[s, :] = 0.0 * w[s, :] + 0.0 * w_0[s, :]  # clear previous trace
                w[s, a] = 1.0  # store deterministic mapping
            else:
                # mild drift toward uniform if not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # update last seen time
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p