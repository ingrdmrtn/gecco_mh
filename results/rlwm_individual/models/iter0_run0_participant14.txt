def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and set-size–modulated working-memory weight and perseveration.

    This model assumes choices are generated by a mixture of a reinforcement-learning (RL) system
    and a working-memory (WM) system. The WM system stores recently rewarded stimulus–action
    associations with decay. The arbitration weight given to WM is reduced as set size increases
    and further modulated by age group (younger vs older). A choice perseveration (stickiness)
    bias promotes repeating the previous action.

    Parameters
    ----------
    states : array-like of int
        State identity at each trial (0-indexed within a block).
    actions : array-like of int
        Chosen action at each trial (0, 1, or 2).
    rewards : array-like of float
        Binary feedback (0 or 1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of distinct states in that block) for each trial.
    age : array-like of float
        Age of the participant (single-element array). Used to set age group.
    model_parameters : sequence of 6 floats
        alpha: RL learning rate in [0,1].
        beta: inverse temperature for softmax; internally scaled by 10.
        wm_weight_base: base weight for WM contribution in [0,1].
        wm_decay: decay of WM values toward uniform per trial in [0,1].
        stickiness: perseveration weight added to the last chosen action’s preference.
        gamma_setsize: exponent governing how WM weight decreases with set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np  # assumed available per instructions
    alpha, beta, wm_weight_base, wm_decay, stickiness, gamma_setsize = model_parameters
    beta *= 10.0
    age_val = float(age[0])
    is_younger = 1.0 if age_val < 45 else 0.0

    # Age effect on WM: younger participants get boosted WM weight; older reduced
    # Map to multiplicative factor in [~0.7, ~1.3]
    age_wm_factor = 1.0 + 0.3 * (is_younger - (1.0 - is_younger))  # +0.3 if younger, -0.3 if older

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM map
        Q = np.zeros((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))

        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Set-size dependent WM weight; larger set size reduces WM contribution
            ss_factor = (3.0 / ss) ** max(0.0, gamma_setsize)
            wm_weight = np.clip(wm_weight_base * age_wm_factor * ss_factor, 0.0, 1.0)

            # RL policy with stickiness
            prefs_rl = beta * Q[s, :].copy()
            if prev_action is not None:
                prefs_rl[prev_action] += stickiness
            max_pref_rl = np.max(prefs_rl)
            pi_rl = np.exp(prefs_rl - max_pref_rl)
            pi_rl /= np.sum(pi_rl)

            # WM policy: softmax on W with higher precision; also add stickiness
            beta_wm = 2.0 * beta
            prefs_wm = beta_wm * W[s, :].copy()
            if prev_action is not None:
                prefs_wm[prev_action] += stickiness
            max_pref_wm = np.max(prefs_wm)
            pi_wm = np.exp(prefs_wm - max_pref_wm)
            pi_wm /= np.sum(pi_wm)

            # Mixture
            pi = wm_weight * pi_wm + (1.0 - wm_weight) * pi_rl
            p_a = np.clip(pi[a], eps, 1.0)
            nll -= np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay to uniform
            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)
            # WM storage on rewarded trials: store deterministic mapping when r=1
            if r > 0.5:
                W[s, :] = (0.0)
                W[s, a] = 1.0
            # Normalize W row for numerical safety
            row_sum = np.sum(W[s, :])
            if row_sum > 0:
                W[s, :] /= row_sum

            prev_action = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with decay, set-size–dependent exploration, age-dependent inverse temperature,
    and perseveration.

    This model is a single-system RL that features:
    - Asymmetric learning rates for positive vs negative prediction errors.
    - Value decay toward zero to capture forgetting under cognitive load.
    - Inverse temperature scaled down when set size is larger.
    - Age-dependent scaling of inverse temperature (older adults more exploratory).
    - Perseveration bias for repeating the previous action.

    Parameters
    ----------
    states : array-like of int
        State identity at each trial (0-indexed within a block).
    actions : array-like of int
        Chosen action at each trial (0, 1, or 2).
    rewards : array-like of float
        Binary feedback (0 or 1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of distinct states in that block) for each trial.
    age : array-like of float
        Age of the participant (single-element array). Used to set age group.
    model_parameters : sequence of 6 floats
        alpha_pos: learning rate for positive PE in [0,1].
        alpha_neg_factor: multiplicative factor (0..1) for negative PE learning rate.
        beta: base inverse temperature; internally scaled by 10.
        rho: per-trial decay rate of Q-values toward 0 in [0,1].
        stickiness: perseveration weight added to the last chosen action’s preference.
        older_beta_scale: multiplicative factor (<1 for more exploration) applied to beta if age >= 45.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np
    alpha_pos, alpha_neg_factor, beta, rho, stickiness, older_beta_scale = model_parameters
    beta *= 10.0
    alpha_neg = np.clip(alpha_pos * alpha_neg_factor, 0.0, 1.0)

    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    # Age-dependent beta scaling: older participants get scaled beta
    beta_age = beta * (older_beta_scale if is_older > 0.5 else 1.0)

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Set-size dependent exploration: larger set size reduces effective beta
            ss_beta_factor = (3.0 / ss)  # equals 1 for ss=3, 0.5 for ss=6
            beta_eff = max(1e-6, beta_age * ss_beta_factor)

            # Softmax with perseveration
            prefs = beta_eff * Q[s, :].copy()
            if prev_action is not None:
                prefs[prev_action] += stickiness
            max_pref = np.max(prefs)
            pi = np.exp(prefs - max_pref)
            pi /= np.sum(pi)

            p_a = np.clip(pi[a], eps, 1.0)
            nll -= np.log(p_a)

            # Value decay toward zero (forgetting)
            Q *= (1.0 - rho)

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            eta = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += eta * pe

            prev_action = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited WM with Bayesian action beliefs and uncertainty-based arbitration with RL.

    The model maintains:
    - A WM system: For each state-action, a Beta posterior over reward probability (succ, fail counts),
      decaying toward an uninformative prior. WM capacity K determines how many states can be actively
      maintained; beyond capacity, WM contribution is down-weighted (availability).
    - An RL system: Standard delta-rule Q-learning.
    - Arbitration: The weight on WM vs RL is determined by comparative uncertainty. We proxy uncertainty
      by choice entropy of each system’s policy for the current state. Lower entropy => higher certainty.
      The arbitration weight is a sigmoid of the entropy difference, scaled by arb_slope.
    - Age effect: Effective WM capacity K is modulated by age group (younger get bonus, older get penalty).

    Parameters
    ----------
    states : array-like of int
        State identity at each trial (0-indexed within a block).
    actions : array-like of int
        Chosen action at each trial (0, 1, or 2).
    rewards : array-like of float
        Binary feedback (0 or 1) at each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of distinct states in that block) for each trial.
    age : array-like of float
        Age of the participant (single-element array). Used to set age group.
    model_parameters : sequence of 6 floats
        alpha: RL learning rate in [0,1].
        beta: inverse temperature for softmax; internally scaled by 10.
        K_base: base WM capacity (0..6).
        age_bonus: capacity bonus (added if younger, subtracted if older).
        wm_decay: decay of WM counts toward prior per trial in [0,1].
        arb_slope: slope controlling the sensitivity of arbitration to entropy difference.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np
    alpha, beta, K_base, age_bonus, wm_decay, arb_slope = model_parameters
    beta *= 10.0

    age_val = float(age[0])
    is_younger = 1.0 if age_val < 45 else 0.0
    # Effective capacity
    K_eff = np.clip(K_base + age_bonus * (1.0 if is_younger > 0.5 else -1.0), 0.0, 6.0)

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM Beta posteriors: initialize to uninformative prior Beta(1,1) for each (s,a)
        succ = np.ones((nS, nA))
        fail = np.ones((nS, nA))

        # Track recency to manage capacity: most recently seen states are prioritized
        recency_list = []  # list of states in order of most recent encounter (front = most recent)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Update recency list
            if s in recency_list:
                recency_list.remove(s)
            recency_list.insert(0, s)
            # Determine availability of WM for this state given capacity
            # If s is within the top-K_eff most recent states, WM is fully available; else unavailable
            # Use a soft availability proportional to how far beyond capacity the state is.
            idx = recency_list.index(s)
            avail = 1.0 if idx < int(np.floor(K_eff + 1e-9)) else 0.0

            # Compute WM expected values and policy for this state
            wm_means = succ[s, :] / np.maximum(succ[s, :] + fail[s, :], 1e-9)
            prefs_wm = beta * wm_means
            max_wm = np.max(prefs_wm)
            pi_wm = np.exp(prefs_wm - max_wm)
            pi_wm /= np.sum(pi_wm)

            # RL policy
            prefs_rl = beta * Q[s, :]
            max_rl = np.max(prefs_rl)
            pi_rl = np.exp(prefs_rl - max_rl)
            pi_rl /= np.sum(pi_rl)

            # Entropy-based arbitration: lower entropy => higher certainty
            def entropy(p):
                p_clip = np.clip(p, eps, 1.0)
                return -np.sum(p_clip * np.log(p_clip))

            H_wm = entropy(pi_wm)
            H_rl = entropy(pi_rl)
            # If WM is more certain (lower entropy), weight increases; otherwise decreases
            wm_weight = 1.0 / (1.0 + np.exp(-arb_slope * (H_rl - H_wm)))
            # Capacity availability gates the WM weight
            wm_weight *= avail

            # Mixture policy
            pi = wm_weight * pi_wm + (1.0 - wm_weight) * pi_rl
            p_a = np.clip(pi[a], eps, 1.0)
            nll -= np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay toward prior and update counts for the chosen action
            # Decay all counts toward prior Beta(1,1)
            succ = (1.0 - wm_decay) * succ + wm_decay * 1.0
            fail = (1.0 - wm_decay) * fail + wm_decay * 1.0
            # Increment chosen action's counts with observed outcome
            succ[s, a] += r
            fail[s, a] += (1.0 - r)

    return nll