def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity- and decay-modulated working memory.
    
    On each trial, choices are generated by a mixture of:
    - RL policy (softmax over Q-values) updated with a single learning rate.
    - WM policy (softmax with very high inverse temperature over W-values) that stores
      the last rewarded action per state, but suffers from decay toward uniform and
      reduced availability when set size exceeds capacity.
    
    The effective WM contribution is scaled by an availability factor that depends
    on set size relative to a WM capacity parameter K.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1, where nS is the set size in the current block).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial. Value changes denote block boundaries (no overlap of states).
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    model_parameters : iterable of floats
        [lr, wm_weight, softmax_beta, K, wm_decay]
        - lr: RL learning rate (0..1)
        - wm_weight: baseline mixture weight of WM (0..1)
        - softmax_beta: RL inverse temperature (>0); internally scaled up
        - K: WM capacity in number of items (>0, typically ~3-4)
        - wm_decay: per-trial decay rate of WM toward uniform (0..1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working-memory policy (softmax over W with high beta)
            # Apply decay toward uniform before computing policy (time-based forgetting)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            p_wm_a = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-based WM availability factor: decreases when set size > K
            nS_curr = float(block_set_sizes[t])
            avail = min(1.0, max(0.0, K / nS_curr))
            wm_eff = wm_weight * avail

            # Mixture
            p_total = wm_eff * p_wm_a + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on rewarded trials, store a near one-hot association
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]  # keep general decay already applied
                w[s, :] = (1.0 / nA) * np.ones(nA)  # reset row before strong write
                w[s, a] = 1.0 - (nA - 1) * 1e-6     # nearly one-hot
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = 1e-6
            # On errors, rely on decay only (no explicit anti-learning)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + capacity-limited WM with availability and lapses.
    
    Decision policy is a mixture of:
    - RL softmax with separate learning rates for positive and negative prediction errors.
    - WM policy that retrieves a stored action for a state (if available), otherwise uniform.
    
    WM availability depends on set size relative to capacity K, and a small lapse mixes noise
    into WM choices. WM stores state-action only on rewarded trials.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of int
        Binary rewards per trial (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    model_parameters : iterable of floats
        [lr_pos, lr_neg, wm_weight, softmax_beta, K, wm_lapse]
        - lr_pos: RL learning rate for positive PE (0..1)
        - lr_neg: RL learning rate for negative PE (0..1)
        - wm_weight: baseline WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - K: WM capacity in items (>0)
        - wm_lapse: lapse/error probability within WM policy (0..0.2)
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, K, wm_lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track whether a state has a stored WM action (binary flag via peaked distribution)
        stored_action = -np.ones(nS, dtype=int)  # -1 means no memory

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            # If a stored action exists, make a near-delta distribution with lapse
            if stored_action[s] >= 0:
                wm_probs = np.full(nA, wm_lapse / nA)
                wm_probs[stored_action[s]] += (1.0 - wm_lapse)
                # compute prob of chosen action under WM
                p_wm_a = wm_probs[a]
            else:
                # no memory: uniform (equivalent to very uncertain WM)
                p_wm_a = 1.0 / nA

            # Capacity availability
            nS_curr = float(block_set_sizes[t])
            avail = min(1.0, max(0.0, K / nS_curr))
            wm_eff = wm_weight * avail

            # Mixture
            p_total = wm_eff * p_wm_a + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: write on rewarded trial; clear on repeated errors
            if r > 0.5:
                stored_action[s] = a
                # also sharpen the W distribution for completeness (not used directly for policy)
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0 - (nA - 1) * 1e-6
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = 1e-6
            else:
                # On error, do not update memory; optional soft reset if repeated errors
                # Implement a mild drift toward no-memory via entropy increase
                if stored_action[s] >= 0:
                    # small chance to forget after an error, proportional to set size pressure
                    forget_prob = max(0.0, 1.0 - min(1.0, K / nS_curr))
                    if np.random.rand() < forget_prob:
                        stored_action[s] = -1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM arbitration by WM reliability with capacity limit.
    
    The model combines:
    - RL softmax (single learning rate).
    - WM softmax that is updated on both outcomes: reward strengthens chosen action,
      non-reward suppresses it. WM decays toward uniform.
    
    Arbitration: the WM weight is dynamically modulated by a per-state reliability trace
    that tracks recent success for that state, and by a capacity factor (K / set_size, clipped).
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen action indices per trial.
    rewards : array-like of int
        Binary rewards per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial.
    model_parameters : iterable of floats
        [lr, wm_weight, softmax_beta, alpha_rel, K, wm_decay]
        - lr: RL learning rate (0..1)
        - wm_weight: baseline WM mixture weight (0..1)
        - softmax_beta: RL inverse temperature; internally scaled
        - alpha_rel: learning rate for WM reliability (0..1)
        - K: WM capacity in items
        - wm_decay: per-trial decay of W toward uniform (0..1)
    
    Returns
    -------
    float
        Negative log-likelihood.
    """
    lr, wm_weight, softmax_beta, alpha_rel, K, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # WM reliability per state (initially low-moderate)
        rel = 0.5 * np.ones(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM decay toward uniform before policy computation
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            p_wm_a = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic arbitration weight = baseline * reliability * capacity factor
            nS_curr = float(block_set_sizes[t])
            capacity = min(1.0, max(0.0, K / nS_curr))
            wm_eff = wm_weight * rel[s] * capacity

            # Mixture
            p_total = wm_eff * p_wm_a + (1.0 - wm_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: strengthen chosen action if rewarded, suppress if not
            if r > 0.5:
                # move toward one-hot on a
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target  # aggressive attraction
            else:
                # penalize chosen action, redistribute mass to others
                penalize = 0.5
                w[s, a] = (1.0 - penalize) * w[s, a]
                redistribute = penalize * w[s, a] / (nA - 1 + eps)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = (1.0 - wm_decay) * w[s, aa] + redistribute

            # Update WM reliability trace for this state (reward-driven)
            rel[s] = (1.0 - alpha_rel) * rel[s] + alpha_rel * r

        blocks_log_p += log_p

    return -blocks_log_p