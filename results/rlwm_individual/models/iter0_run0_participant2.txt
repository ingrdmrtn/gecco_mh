def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited Working Memory (WM) with age-dependent WM contribution.

    The model assumes two parallel controllers:
    - RL: incremental Q-learning with softmax choice.
    - WM: one-shot storage of rewarded mappings with decay; expressed as a softmax over a
          per-state action memory vector. WM contribution is scaled by capacity relative to set size
          and modulated by age group.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block.
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age. Used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta, wm_base, wm_decay, capacity, age_wm_shift]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax; internally scaled by 10.
        - wm_base: baseline WM weight (0..1) controlling arbitration with RL.
        - wm_decay: WM decay rate (0..1) toward stored mapping (when rewarded) or uniform (when not).
        - capacity: WM capacity in items (0..6); scales WM contribution as capacity/set_size.
        - age_wm_shift: multiplicative reduction of WM weight for older adults (0..1). Younger unchanged.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence under the model.
    """
    import numpy as np  # assumed available per instructions

    alpha, beta, wm_base, wm_decay, capacity, age_wm_shift = model_parameters
    beta = beta * 10.0
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    all_blocks = np.unique(blocks)
    total_logp = 0.0
    nA = 3

    for b in all_blocks:
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize RL values and WM store
        Q = np.ones((nS, nA)) / nA
        W = np.ones((nS, nA)) / nA  # WM policy representation

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # RL policy
            rl_logits = beta * Q[s, :]
            rl_logits = rl_logits - np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)
            p_rl = rl_probs[a]

            # WM policy: softmax over W with a sharper temperature tied to RL beta
            beta_wm = beta * 2.0
            wm_logits = beta_wm * W[s, :]
            wm_logits = wm_logits - np.max(wm_logits)
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Arbitration weight: baseline scaled by capacity and age
            cap_factor = np.clip(capacity / max(1, nS), 0.0, 1.0)
            # Older adults have reduced WM contribution by factor (1 - age_wm_shift)
            age_factor = (1.0 - age_group * np.clip(age_wm_shift, 0.0, 1.0))
            wm_weight = np.clip(wm_base, 0.0, 1.0) * cap_factor * age_factor

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            total_logp += np.log(p_total)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.5:
                # Move W toward a one-hot mapping when rewarded
                W[s, :] = (1.0 - wm_decay) * W[s, :] + wm_decay * onehot
            else:
                # Decay toward uniform when not rewarded
                W[s, :] = (1.0 - wm_decay) * W[s, :] + wm_decay * (1.0 / nA)

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and age-/load-dependent lapse.

    Choice comes from a mixture of:
    - Softmax over Q-values (exploitation/exploration).
    - Uniform random lapse, with the lapse probability increasing with set size and for older adults.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block.
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age. Used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha_pos, alpha_neg, beta, lapse0, lapse_size_slope, lapse_age_effect]
        - alpha_pos: RL learning rate for rewards (r=1).
        - alpha_neg: RL learning rate for no-reward (r=0).
        - beta: inverse temperature; internally scaled by 10.
        - lapse0: baseline lapse bias (logit scale).
        - lapse_size_slope: effect of set size (nS-3) on lapse (logit scale).
        - lapse_age_effect: additive effect of being older (>=45) on lapse (logit scale).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    import numpy as np  # assumed available per instructions

    alpha_pos, alpha_neg, beta, lapse0, lapse_size_slope, lapse_age_effect = model_parameters
    beta = beta * 10.0
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    all_blocks = np.unique(blocks)
    total_logp = 0.0
    nA = 3

    for b in all_blocks:
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = np.ones((nS, nA)) / nA

        # Compute lapse per trial from set size and age on the logit scale
        # logit(lapse) = lapse0 + lapse_size_slope*(nS-3) + lapse_age_effect*age_group
        logit_lapse = lapse0 + lapse_size_slope * (nS - 3) + lapse_age_effect * age_group
        lapse = 1.0 / (1.0 + np.exp(-logit_lapse))
        lapse = np.clip(lapse, 0.0, 1.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Softmax policy over Q
            logits = beta * Q[s, :]
            logits = logits - np.max(logits)
            probs = np.exp(logits)
            probs = probs / np.sum(probs)

            # Mixture with lapse
            p_choice = (1.0 - lapse) * probs[a] + lapse * (1.0 / nA)
            p_choice = np.clip(p_choice, 1e-12, 1.0)
            total_logp += np.log(p_choice)

            # RL update with asymmetric learning rates
            if r > 0.5:
                Q[s, a] += alpha_pos * (1.0 - Q[s, a])
            else:
                Q[s, a] += alpha_neg * (0.0 - Q[s, a])

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Arbitration model: RL + fast memory with age-modulated arbitration, stickiness, and load-dependent forgetting.

    The model blends:
    - RL: Q-learning with softmax and a perseveration (stickiness) bias to repeat the last action.
    - Fast memory (FM): a rapidly updated memory-based policy (similar to WM), also passed through a softmax,
      and subject to the same stickiness bias.
    Arbitration weight toward FM is reduced with set size and for older adults.

    Additionally, RL values forget toward uniform more strongly under higher load.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within its block.
    actions : 1D array-like of int
        Observed chosen action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Feedback received after each choice.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        The number of distinct states in the current block (3 or 6).
    age : 1D array-like (length 1)
        Participant age. Used to define age group (younger <45, older >=45).
    model_parameters : iterable of length 6
        [alpha, beta, w0, w_age, stickiness, forget]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax; internally scaled by 10.
        - w0: baseline arbitration bias toward FM (logit scale).
        - w_age: age effect on arbitration (positive values favor younger).
                 Implemented as w0 + w_age*(0.5 - age_group).
        - stickiness: bias added to the previously chosen action in the softmax logits.
        - forget: RL forgetting rate toward uniform; scaled up with set size (nS/6).

    Returns
    -------
    float
        Negative log-likelihood of the observed actions.
    """
    import numpy as np  # assumed available per instructions

    alpha, beta, w0, w_age, stickiness, forget = model_parameters
    beta = beta * 10.0
    age_val = age[0]
    age_group = 1 if age_val >= 45 else 0  # 1=older, 0=younger

    all_blocks = np.unique(blocks)
    total_logp = 0.0
    nA = 3

    for b in all_blocks:
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = np.ones((nS, nA)) / nA
        FM = np.ones((nS, nA)) / nA  # fast memory policy representation

        prev_action = None

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Stickiness vector
            stick_vec = np.zeros(nA)
            if prev_action is not None:
                stick_vec[int(prev_action)] = stickiness

            # RL policy with stickiness
            rl_logits = beta * Q[s, :] + stick_vec
            rl_logits = rl_logits - np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)

            # FM policy with stickiness
            beta_fm = beta * 2.0
            fm_logits = beta_fm * FM[s, :] + stick_vec
            fm_logits = fm_logits - np.max(fm_logits)
            fm_probs = np.exp(fm_logits)
            fm_probs = fm_probs / np.sum(fm_probs)

            # Arbitration weight: smaller with higher load and in older adults
            # logit(w) = w0 + w_age*(0.5 - age_group) - 0.3*(nS-3)
            logit_w = w0 + w_age * (0.5 - age_group) - 0.3 * (nS - 3)
            w = 1.0 / (1.0 + np.exp(-logit_w))
            w = np.clip(w, 0.0, 1.0)

            p_total = w * fm_probs[a] + (1.0 - w) * rl_probs[a]
            p_total = np.clip(p_total, 1e-12, 1.0)
            total_logp += np.log(p_total)

            # RL update with load-dependent forgetting toward uniform
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe
            eff_forget = np.clip(forget, 0.0, 1.0) * (nS / 6.0)
            Q[s, :] = (1.0 - eff_forget) * Q[s, :] + eff_forget * (1.0 / nA)

            # FM update: fast move toward chosen action if rewarded, else toward uniform
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.5:
                FM[s, :] = 0.5 * FM[s, :] + 0.5 * onehot
            else:
                FM[s, :] = 0.8 * FM[s, :] + 0.2 * (1.0 / nA)

            prev_action = a

    return -float(total_logp)