def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM capacity mixture model with age- and load-modulated WM engagement.

    Idea:
    - Decisions are a mixture of RL and a working-memory (WM) policy.
    - WM stores the last rewarded action for each state (item-specific).
    - The mixture weight of WM is modulated by age group and by set size relative to a capacity K.
    - Younger participants rely more on WM; when set size exceeds K, WM engagement drops.
    - A small lapse process mixes in uniform random responding.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 in a block).
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2; invalid values handled as lapses).
    rewards : array-like of float
        Obtained reward on each trial (e.g., 0/1; can be negative for punishments).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6 here).
    age : array-like of float
        Participant age (first element used). Age group: younger <45.
    model_parameters : tuple/list
        (alpha, beta, wm_base, K, age_slope, lapse)
        - alpha: RL learning rate (0..1).
        - beta: softmax inverse temperature for RL (>0).
        - wm_base: baseline WM mixture logit intercept.
        - K: WM capacity in number of items (e.g., ~3-5). Effective WM weight drops when set_size > K.
        - age_slope: additional WM weight for being younger (applied on the logit scale).
        - lapse: lapse rate mixing with uniform responding (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, wm_base, K, age_slope, lapse = model_parameters
    # clamp/transform where needed
    beta = max(1e-6, beta) * 5.0  # give beta a bit more dynamic range
    alpha = min(max(alpha, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)  # keep lapses reasonable
    # age group coding
    age_val = age[0] if hasattr(age, "__len__") else float(age)
    younger = 1.0 if age_val < 45 else 0.0

    eps = 1e-12
    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA))  # RL values
        wm_store = -np.ones(nS, dtype=int)  # stored best action per state (-1 means empty)

        prev_set_size = nS  # track current set size for capacity effect

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_now = int(block_set_sizes[t])  # in case

            # RL policy
            Qs = Q[s]
            # numerically stable softmax
            m = np.max(beta * Qs)
            expv = np.exp(beta * Qs - m)
            pi_rl = expv / np.sum(expv)

            # WM policy: deterministic to stored action if available, else uniform
            if wm_store[s] >= 0:
                wm_a = wm_store[s]
                pi_wm = np.zeros(nA)
                pi_wm[wm_a] = 1.0
            else:
                pi_wm = np.ones(nA) / nA

            # Mixture weight: logit(wm_weight) = wm_base + age_slope*younger + capacity_term
            # capacity_term reduces weight if set size exceeds capacity K
            overload = max(0.0, nS_now - K)
            capacity_term = -overload  # 1 unit drop in logit per item above K
            logit_w = wm_base + age_slope * younger + capacity_term
            wm_weight = 1.0 / (1.0 + np.exp(-logit_w))

            # Final policy with lapse
            pi = (1.0 - lapse) * (wm_weight * pi_wm + (1.0 - wm_weight) * pi_rl) + lapse * (np.ones(nA) / nA)

            # Likelihood of observed action (handle invalid actions robustly)
            if 0 <= a < nA:
                p_a = max(pi[a], eps)
            else:
                # invalid response: only explainable via lapse (uniform)
                p_a = max(lapse / nA, eps)

            total_logp += np.log(p_a)

            # Learning updates
            if 0 <= a < nA:
                # RL update
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # WM update: on rewarded trials, store chosen action for this state
                if r > 0:
                    wm_store[s] = a
            else:
                # invalid response: no learning
                pass

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetrical learning rates and age/load-modulated perseveration bias.

    Idea:
    - Choices follow a softmax over Q-values with an added perseveration bonus
      for repeating the previous action. This bias is stronger under higher load
      (larger set size) and for older adults.
    - Learning uses separate learning rates for positive vs. negative prediction errors.
    - Lapse process mixes in uniform responding.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 in a block).
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2; invalid values handled as lapses).
    rewards : array-like of float
        Obtained reward on each trial (e.g., 0/1; can be negative for punishments).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6 here).
    age : array-like of float
        Participant age (first element used). Age group: older >=45.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, kappa_base, kappa_age_slope, lapse)
        - alpha_pos: learning rate for positive prediction errors (0..1).
        - alpha_neg: learning rate for negative prediction errors (0..1).
        - beta: softmax inverse temperature (>0).
        - kappa_base: base perseveration strength (added to chosen action value next trial).
        - kappa_age_slope: additional perseveration for being older (applied additively);
                           load modulation multiplies by set_size/6.
        - lapse: lapse rate mixing with uniform responding (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, beta, kappa_base, kappa_age_slope, lapse = model_parameters
    beta = max(1e-6, beta) * 5.0
    alpha_pos = min(max(alpha_pos, 0.0), 1.0)
    alpha_neg = min(max(alpha_neg, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    age_val = age[0] if hasattr(age, "__len__") else float(age)
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            load_factor = float(block_set_sizes[t]) / 6.0  # 0.5 for 3, 1.0 for 6

            # Perseveration vector
            kappa = kappa_base + kappa_age_slope * older
            kappa_eff = kappa * load_factor
            pers = np.zeros(nA)
            if prev_action is not None and 0 <= prev_action < nA:
                pers[prev_action] = kappa_eff

            # Policy
            vals = Q[s] + pers
            m = np.max(beta * vals)
            expv = np.exp(beta * vals - m)
            pi = expv / np.sum(expv)
            pi = (1.0 - lapse) * pi + lapse * (np.ones(nA) / nA)

            if 0 <= a < nA:
                p_a = max(pi[a], eps)
            else:
                p_a = max(lapse / nA, eps)

            total_logp += np.log(p_a)

            # Learning update
            if 0 <= a < nA:
                pe = r - Q[s, a]
                lr = alpha_pos if pe >= 0 else alpha_neg
                Q[s, a] += lr * pe
                prev_action = a
            else:
                # invalid action: keep prev_action to allow perseveration on next valid trial or reset?
                # We'll reset to avoid reinforcing invalids.
                prev_action = None

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Gated WM with decay + RL, with age- and load-dependent gating.

    Idea:
    - A gating process decides whether to consult WM or RL on each trial.
    - WM stores the most recently rewarded action per state and decays over time.
    - The gating probability decreases with set size (higher load) and for older adults.
    - RL uses standard delta-rule learning. Lapse mixes in uniform responding.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 in a block).
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2; invalid values handled as lapses).
    rewards : array-like of float
        Obtained reward on each trial (e.g., 0/1; can be negative for punishments).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size on each trial (3 or 6 here).
    age : array-like of float
        Participant age (first element used). Age group: younger <45.
    model_parameters : tuple/list
        (alpha, beta, gate_base, gate_age, decay, lapse)
        - alpha: RL learning rate (0..1).
        - beta: RL softmax inverse temperature (>0).
        - gate_base: baseline WM gate logit intercept.
        - gate_age: additive change in gate logit for being younger (positive -> more WM when younger).
        - decay: WM decay probability per trial; stored mappings deteriorate with this rate (0..1).
        - lapse: lapse rate mixing with uniform responding (0..1).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, gate_base, gate_age, decay, lapse = model_parameters
    beta = max(1e-6, beta) * 5.0
    alpha = min(max(alpha, 0.0), 1.0)
    decay = min(max(decay, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    age_val = age[0] if hasattr(age, "__len__") else float(age)
    younger = 1.0 if age_val < 45 else 0.0

    eps = 1e-12
    nA = 3
    total_logp = 0.0

    rng = np.random.RandomState(0)  # deterministic "noise" source for decay (pseudo; no imports inside)

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        wm_store = -np.ones(nS, dtype=int)  # stored action per state
        wm_alive = np.zeros(nS, dtype=float)  # "confidence" (1 alive, 0 dead)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            sz = float(block_set_sizes[t])

            # Apply decay to WM trace (deterministic expectation: shrink confidence)
            wm_alive = (1.0 - decay) * wm_alive
            # when confidence ~0, clear store
            if wm_alive[s] < 1e-6:
                wm_store[s] = -1

            # RL policy
            Qs = Q[s]
            m = np.max(beta * Qs)
            expv = np.exp(beta * Qs - m)
            pi_rl = expv / np.sum(expv)

            # WM policy
            if wm_store[s] >= 0 and wm_alive[s] > 1e-6:
                pi_wm = np.zeros(nA)
                pi_wm[wm_store[s]] = 1.0
            else:
                pi_wm = np.ones(nA) / nA

            # Gating probability: logit(g) = gate_base + gate_age*younger - load_term
            # load_term proportional to set size (3 -> 0.5, 6 -> 1.0 when divided by 6 then scaled).
            load_term = sz / 3.0  # 1.0 for 3, 2.0 for 6: stronger penalty at higher load
            logit_g = gate_base + gate_age * younger - load_term
            g = 1.0 / (1.0 + np.exp(-logit_g))

            pi = (1.0 - lapse) * (g * pi_wm + (1.0 - g) * pi_rl) + lapse * (np.ones(nA) / nA)

            if 0 <= a < nA:
                p_a = max(pi[a], eps)
            else:
                p_a = max(lapse / nA, eps)

            total_logp += np.log(p_a)

            # Learning and WM update
            if 0 <= a < nA:
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                if r > 0:
                    wm_store[s] = a
                    wm_alive[s] = 1.0  # refresh confidence on reward
            else:
                # invalid action: no learning
                pass

    return -float(total_logp)