def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity scaling and WM decay.

    The model mixes a model-free RL policy with a one-shot Working Memory (WM) policy.
    The WM contribution is down-weighted as the set size increases beyond a WM capacity K.
    WM stores rewarded stimulus-action associations with rapid one-shot learning and
    decays toward uniform when not reinforced.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, K, phi, rho)
        - lr: RL learning rate in [0,1].
        - wm_weight: base mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); rescaled internally.
        - K: WM capacity (positive); WM weight scales as min(1, K / set_size).
        - phi: WM one-shot learning rate toward the rewarded action in [0,1].
        - rho: WM decay toward uniform for the visited state in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K, phi, rho = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # highly deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM associative strengths
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM prior (uniform)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action (softmax trick for a)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            w_centered = W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (w_centered - w_centered[a])))

            # Effective WM weight scaled by capacity vs. set size
            cap_scale = min(1.0, max(0.0, K / float(nS)))  # in [0,1]
            w_eff = np.clip(wm_weight * cap_scale, 0.0, 1.0)

            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update for the visited state: decay toward uniform, then one-shot learn on reward
            w[s, :] = (1.0 - rho) * w[s, :] + rho * w_0[s, :]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - phi) * w[s, :] + phi * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with perseveration bias and logistic WM gating by set size.

    The choice policy mixes RL and WM, where WM weight decreases with set size via a logistic
    capacity gate. RL policy includes a perseveration bias toward the most recent action taken
    in the same state. WM stores rewarded state-action pairs with decay.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, K, tau, gamma)
        - lr: RL learning rate in [0,1].
        - wm_weight: base WM weight controlling the height of the logistic gate (>0 roughly).
        - softmax_beta: RL inverse temperature (>0); rescaled internally.
        - K: logistic midpoint for set size (capacity point where WM weight is ~wm_weight/2).
        - tau: perseveration bias added to the last chosen action in the same state (can be +/-).
        - gamma: WM decay toward uniform in [0,1] for visited state.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K, tau, gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # last chosen action per state, -1 if none

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with perseveration: add tau to last chosen action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += tau

            # WM state values
            W_s = w[s, :].copy()

            # RL policy prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Logistic WM gating by set size: higher nS -> lower weight
            # w_eff = wm_weight / (1 + exp(nS - K))
            gate = 1.0 / (1.0 + np.exp((nS - K)))
            w_eff = np.clip(wm_weight * gate, 0.0, 1.0)

            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (using unbiased Q values; perseveration is policy-only)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform for visited state; reinforce on reward
            w[s, :] = (1.0 - gamma) * w[s, :] + gamma * w_0[s, :]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # One-shot strengthen the rewarded mapping
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot  # balanced push toward the rewarded action

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Error-driven arbitration between RL and WM, with set-size dependent WM access.

    The WM weight on each trial depends on:
      - an intercept (wm_bias),
      - the inverse of current set size (smaller sets -> more WM),
      - the recent absolute RL prediction error specific to that state (higher |PE| -> more WM).

    WM stores rewarded mappings via one-shot learning and decays toward uniform; RL learns via TD.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block).
    model_parameters : tuple
        (lr, softmax_beta, wm_bias, wm_setsize_slope, wm_pe_slope, phi)
        - lr: RL learning rate in [0,1].
        - softmax_beta: RL inverse temperature (>0); rescaled internally.
        - wm_bias: baseline WM tendency (can be +/-; passed through sigmoid).
        - wm_setsize_slope: weight for 1/nS term (positive -> more WM in small sets).
        - wm_pe_slope: weight for absolute PE term (positive -> more WM when RL is unreliable).
        - phi: WM learning/retention rate; both decay to uniform and strengthening on reward in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_bias, wm_setsize_slope, wm_pe_slope, phi = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track recent absolute PE per state (initialize moderately high to encourage early WM)
        pe_abs = 0.5 * np.ones(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Error-driven arbitration: sigmoid of linear combination
            inv_set = 1.0 / float(nS)
            w_lin = wm_bias + wm_setsize_slope * inv_set + wm_pe_slope * pe_abs[s]
            w_eff = 1.0 / (1.0 + np.exp(-w_lin))
            w_eff = np.clip(w_eff, 0.0, 1.0)

            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update absolute PE trace for arbitration (EWMA within state)
            pe_abs[s] = 0.7 * pe_abs[s] + 0.3 * abs(delta)

            # WM update: decay toward uniform, then one-shot on reward
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - phi) * w[s, :] + phi * one_hot

        blocks_log_p += log_p

    return -blocks_log_p