def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age-modulated WM.
    
    The model assumes choices arise from a mixture of:
      - Model-free RL (state-action Q-values updated by reward prediction error)
      - A limited-capacity WM store that rapidly encodes rewarded mappings and decays/interferes over time
    
    Age use:
      - Older adults (age >= 45) have reduced effective WM weight and increased forgetting.
      - WM mixture weight is also scaled by the fraction of set size that can be retained (k/nS).
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State index on each trial within a block (0..nS-1).
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Binary reward on each trial (0/1).
    blocks : array-like, shape (T,)
        Block index for each trial. States reset between blocks.
    set_sizes : array-like, shape (T,)
        Set size for the active block on each trial (3 or 6 here).
    age : array-like, shape (1,) or scalar
        Participant age; used to determine age group effects.
    model_parameters : array-like, length 6
        alpha        : RL learning rate in [0,1]
        beta         : Softmax inverse temperature (>0), scaled internally
        wm_k         : WM capacity (effective slots), in [1,6]
        wm_decay     : Base WM decay/forgetting rate in [0,1]
        wm_beta      : WM choice precision (>0), scaled internally
        wm_weight    : Base WM mixture weight in [0,1]
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np

    alpha, beta, wm_k, wm_decay, wm_beta, wm_weight = model_parameters
    beta = 5.0 * max(1e-6, beta)
    wm_beta = 10.0 * max(1e-6, wm_beta)

    # Age group adjustments
    age_val = age[0] if np.ndim(age) > 0 else age
    is_older = 1.0 if age_val >= 45 else 0.0
    # Older adults: lower WM reliance and higher forgetting
    wm_weight_age = wm_weight * (0.8 if is_older else 1.0)
    wm_decay_age = np.clip(wm_decay + (0.1 if is_older else 0.0), 0.0, 1.0)

    nA = 3
    eps = 1e-12
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize RL and WM stores
        Q = np.zeros((nS, nA))  # RL values
        # WM store: confidence weights over actions per state (sums not constrained)
        W = np.zeros((nS, nA))
        # Track last time each state was visited to implement time-based decay
        last_seen = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply WM decay since last visit of this state
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
                if dt > 0:
                    W[s, :] *= (1.0 - wm_decay_age) ** dt
            last_seen[s] = t

            # Policies
            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)  # stabilize
            prl = np.exp(beta * q_s)
            prl = prl / np.sum(prl)

            # WM policy: precision-softmax on WM weights
            w_s = W[s, :]
            w_s = w_s - np.max(w_s)
            pwm = np.exp(wm_beta * w_s)
            pwm = pwm / np.sum(pwm + eps)

            # Mixture weight modulated by capacity and set size
            wm_capacity_scale = np.clip(wm_k / max(1, nS), 0.0, 1.0)
            wmix = np.clip(wm_weight_age * wm_capacity_scale, 0.0, 1.0)

            p = wmix * pwm + (1.0 - wmix) * prl
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: store rewarded action more strongly; punish others weakly
            if r > 0.5:
                # Move WM toward a one-hot for the rewarded action
                target = np.zeros(nA)
                target[a] = 1.0
                W[s, :] = (1.0 - wm_decay_age) * W[s, :] + (1.0 - (1.0 - wm_decay_age)) * target
                # Or more simply, boost chosen action and slight decay others
                W[s, a] += 1.0
            else:
                # Non-reward: slight decay of confidence
                W[s, :] *= (1.0 - 0.25 * wm_decay_age)

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with set-size- and age-modulated lapse, plus perseveration.
    
    Choices arise from:
      - State-action softmax over Q-values (dual learning rates for positive/negative PE)
      - Action perseveration (stickiness) across trials within a block
      - A lapse/epsilon component that increases with set size and more in older adults
    
    Age use:
      - Older adults (age >= 45) have higher effective lapse.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State index per trial within block.
    actions : array-like, shape (T,)
        Chosen action (0..2).
    rewards : array-like, shape (T,)
        Reward (0/1).
    blocks : array-like, shape (T,)
        Block index.
    set_sizes : array-like, shape (T,)
        Set size per trial.
    age : array-like or scalar
        Participant age for age grouping.
    model_parameters : array-like, length 6
        alpha_pos  : Learning rate for positive PE in [0,1]
        alpha_neg  : Learning rate for negative PE in [0,1]
        beta       : Inverse temperature (>0), scaled internally
        epsilon0   : Base lapse rate in [0,1]
        epsilon_ss : Lapse increase per item in set size (>=0)
        kappa      : Perseveration strength (stickiness), can be positive or negative
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    import numpy as np

    alpha_pos, alpha_neg, beta, epsilon0, epsilon_ss, kappa = model_parameters
    beta = 5.0 * max(1e-6, beta)

    age_val = age[0] if np.ndim(age) > 0 else age
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Compute lapse with set size and age effects
            eps_lapse = epsilon0 + epsilon_ss * max(0, nS_t - 1)
            # Older adults have higher lapse
            eps_lapse = np.clip(eps_lapse * (1.25 if is_older > 0.5 else 1.0), 0.0, 0.5)

            # Softmax with perseveration
            logits = beta * Q[s, :].copy()
            if prev_action is not None:
                logits[prev_action] += kappa
            logits -= np.max(logits)
            p_soft = np.exp(logits)
            p_soft = p_soft / np.sum(p_soft)

            # Lapse mixture
            punif = np.ones(nA) / nA
            p = (1.0 - eps_lapse) * p_soft + eps_lapse * punif
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            Q[s, a] += lr * pe

            prev_action = a

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + confidence-based WM mapping with interference; age shifts WM-RL mixing.
    
    The model uses:
      - Model-free RL (single learning rate) per state-action
      - A WM-like mapping per state consisting of a 'confidence' over the best action,
        updated strongly upon reward and decaying via interference proportional to set size
      - The mixture between WM and RL is modulated by set size, trial number, and age group
    
    Age use:
      - Older adults (age >= 45) rely less on WM (mixture reduced) and more on RL.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State index per trial.
    actions : array-like, shape (T,)
        Chosen action (0..2).
    rewards : array-like, shape (T,)
        Reward (0/1).
    blocks : array-like, shape (T,)
        Block index.
    set_sizes : array-like, shape (T,)
        Set size per trial.
    age : array-like or scalar
        Participant age.
    model_parameters : array-like, length 6
        alpha        : RL learning rate in [0,1]
        beta         : Softmax inverse temperature (>0), scaled internally
        wm_conf_init : Initial WM confidence boost on first reward (>0)
        interference : WM interference/decay rate per item in set size (>=0)
        mix_base     : Baseline WM mixture bias (real; will be squashed to [0,1])
        age_slope    : Age effect on mixture (negative reduces WM for older)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    import numpy as np

    alpha, beta, wm_conf_init, interference, mix_base, age_slope = model_parameters
    beta = 5.0 * max(1e-6, beta)

    age_val = age[0] if np.ndim(age) > 0 else age
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_logp = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx].astype(int)
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # WM confidence vector per state over actions
        C = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            qs = Q[s, :]
            qs = qs - np.max(qs)
            p_rl = np.exp(beta * qs)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy from confidence
            cs = C[s, :]
            cs = cs - np.max(cs)
            p_wm = np.exp(10.0 * cs)  # fixed high precision for WM readout
            p_wm = p_wm / np.sum(p_wm + eps)

            # Mixture: baseline + set size interference + age shift
            # Larger set sizes reduce WM reliance; older age further reduces WM
            mix_logit = mix_base - interference * max(0, nS_t - 1) + age_slope * (1.0 - is_older) - 0.1 * t
            wmix = sigmoid(mix_logit)
            p = wmix * p_wm + (1.0 - wmix) * p_rl
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM confidence update with interference/decay
            # Apply interference proportional to set size
            decay = np.clip(1.0 - 0.05 * interference * max(1, nS_t), 0.0, 1.0)
            C *= decay
            if r > 0.5:
                # Increase confidence for rewarded action in this state
                C[s, a] += wm_conf_init
                # Normalize to keep scale bounded
                C[s, :] = C[s, :] - np.min(C[s, :])
                maxc = np.max(C[s, :])
                if maxc > 0:
                    C[s, :] /= (1.0 + maxc)

    return -float(total_logp)