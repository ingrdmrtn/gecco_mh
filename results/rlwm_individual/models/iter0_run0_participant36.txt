def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with load-sensitive WM reliability and decay.

    The model mixes a standard RL softmax policy with a WM-based softmax policy.
    WM reliability (mixture weight) declines with set size via an exponential term.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base weight on WM policy (0..1) at low load.
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - wm_lr: WM write/learning rate when feedback is positive (0..1).
    - wm_decay: WM passive decay toward uniform per trial and load sensitivity (>=0). 
                Both decays memory every trial and reduces WM utilization as set size grows.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax over WM map)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-sensitive WM utilization: decays with set size (higher nS -> lower weight)
            # wm_weight_eff in [0,1], exponential drop with (nS-3)
            wm_weight_eff = wm_weight * np.exp(-wm_decay * max(0, nS - 3))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) passive decay toward uniform each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) reward-gated write of the chosen action
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with dynamic WM gating by memory strength and action perseveration bias.

    - RL uses softmax with a bias favoring repeating the last action in a state (perseveration).
    - WM policy softmax is combined with RL using a gate that increases with the state's WM strength.
    - WM updates toward the chosen action when rewarded, otherwise decays toward uniform.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base maximal WM contribution (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_lr: WM learning/decay rate (0..1) for both write and decay.
    - gate_temp: Gain of the logistic gate mapping WM strength to WM mixture weight (>0).
    - pers_beta: Strength of perseveration bias added to Q of the previous action in a state.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, gate_temp, pers_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        prev_a = -1 * np.ones(nS, dtype=int)  # previous action per state

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add perseveration bias to the previous action for this state
            if prev_a[s] >= 0:
                Q_s[prev_a[s]] += pers_beta

            # RL policy with bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic WM gate from WM strength
            # Strength in [0, 1-1/nA]; normalize by subtracting uniform baseline
            m_strength = np.max(W_s) - (1.0 / nA)
            # Logistic gate in [0,1], scaled by wm_weight
            gate = 1.0 / (1.0 + np.exp(-gate_temp * (10.0 * m_strength - 1.0)))
            wm_weight_eff = wm_weight * gate

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: rewarded write vs. unrewarded decay
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot
            else:
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * w_0[s, :]

            # Update perseveration memory
            prev_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + capacity-limited WM with noise and load-based arbitration.

    - RL uses separate learning rates for positive vs negative prediction errors.
    - WM stores rewarded action as a one-shot mapping but suffers interference and noise.
    - Arbitration weight on WM decreases when set size exceeds capacity K via a logistic function.
    - WM policy includes lapse/noise epsilon.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (delta > 0).
    - lr_neg: RL learning rate for negative prediction errors (delta <= 0).
    - wm_weight: Max WM contribution at or below capacity (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - K: WM capacity in number of items (approx 1..6); higher K sustains WM at larger set sizes.
    - wm_epsilon: WM noise/lapse level (0..1), also drives interference/forgetting.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, K, wm_epsilon = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration weight: logistic drop when nS > K
        # Value in (0, wm_weight); steeper drop when capacity exceeded.
        # Using slope=1 for simplicity.
        wm_weight_eff_block = wm_weight / (1.0 + np.exp(float(nS) - float(K)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with lapse/noise
            W_hat = (1.0 - wm_epsilon) * W_s + wm_epsilon * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_hat - W_hat[a])))

            # Mixture with capacity arbitration (block-constant)
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta > 0.0 else lr_neg
            q[s, a] += lr_use * delta

            # WM update:
            # Interference/forgetting toward uniform scales with epsilon and load
            intf = wm_epsilon * (float(nS) / 6.0)  # more items -> more interference
            w[s, :] = (1.0 - intf) * w[s, :] + intf * w_0[s, :]
            # Rewarded write to strengthen mapping
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                write = max(1e-6, 1.0 - wm_epsilon)  # stronger write when low noise
                w[s, :] = (1.0 - write) * w[s, :] + write * onehot

        blocks_log_p += log_p

    return -blocks_log_p