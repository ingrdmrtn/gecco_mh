Below are three standalone cognitive models that implement RL–WM hybrids tailored to the rlwm task. Each function:

- Matches the requested signature and returns the negative log-likelihood of the observed choices.
- Resets internal state at the start of each block (no cross-block carryover).
- Uses all parameters meaningfully (≤ 6 per model).
- Fills in the Working Memory policy and update steps in the provided template structure.
- Includes a brief docstring describing the model and the role of each parameter, including how set size can modulate parameters.

Note: Assumes numpy has been imported as np elsewhere (no imports are included in the functions).



def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-scaled WM weight and deterministic reward-based WM storage.
    
    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled up internally).
    - WM channel: softmax over WM weights W with very high inverse temperature (deterministic).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.
    
    WM dynamics:
    - Decay of WM toward uniform on every trial in the current state (decay rate wm_decay).
    - If reward is 1, store the chosen action deterministically in WM for that state (one-hot).
    - If reward is 0, no storage beyond decay (i.e., WM becomes more uniform).
    
    Set-size effect:
    - Effective WM weight decreases with set size: wm_weight_eff = wm_weight / (1 + size_gamma * (nS - 3)).
      This yields stronger WM contribution at set size 3 and weaker at set size 6.
    
    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Baseline mixture weight of WM vs RL (0..1).
    - softmax_beta: Inverse temperature for RL channel (scaled by 10 internally).
    - wm_decay: Decay rate toward uniform for WM (0..1).
    - size_gamma: Strength of set-size penalty on WM weight (>=0).
    """
    lr, wm_weight, softmax_beta, wm_decay, size_gamma = model_parameters
    softmax_beta *= 10.0  # RL beta scaled up (kept as in template spirit)
    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline uniform

        # Set-size-scaled WM weight
        wm_weight_eff = wm_weight / (1.0 + size_gamma * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-based WM storage: if rewarded, store chosen action deterministically
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p



def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates and action stickiness; WM decays with set size.
    
    Policy:
    - RL channel: softmax over Q with beta; action stickiness adds bias to the last action in the same state.
    - WM channel: softmax over W with high beta (deterministic-ish).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl_sticky.
    
    RL dynamics:
    - Separate learning rates for positive and negative outcomes (lr_pos, lr_neg).
    
    WM dynamics:
    - Decay toward uniform with an effective decay that increases with set size:
      phi_eff = 1 - (1 - size_decay)^(nS / 3). Larger nS -> larger decay -> weaker WM retention.
    - If reward is 1, store chosen action deterministically; if reward is 0, no new storage.
    
    Set-size effect:
    - Implemented via phi_eff above (no separate size scaling on weights; the WM gets leakier at nS=6).
    
    Parameters (tuple):
    - lr_pos: RL learning rate for rewarded trials (0..1).
    - lr_neg: RL learning rate for unrewarded trials (0..1).
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - stickiness: Action perseverance bias added to the last chosen action in a state (>=0).
    - size_decay: Base WM decay strength (0..1), transformed into phi_eff by set size.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, stickiness, size_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # per-state last chosen action; -1 means none

        # Set-size-dependent WM decay
        phi_eff = 1.0 - (1.0 - np.clip(size_decay, 0.0, 1.0)) ** (nS / 3.0)
        wm_weight_eff = wm_weight  # here, weight kept constant; set-size acts via decay

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with stickiness
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] = Q_s[last_action[s]] + stickiness
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            lr = lr_pos if r > 0.5 else lr_neg
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - phi_eff) * w[s, :] + phi_eff * w_0[s, :]

            # Reward-based storage
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p



def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM+WSLS hybrid mixture with set-size-scaled RL temperature and WM noise.
    
    Policy:
    - RL channel: softmax over Q with beta scaled down at larger set sizes.
    - WM channel: softmax over noisy WM weights W (blend with uniform via wm_noise).
    - WSLS channel: win-stay/lose-shift heuristic distribution based on last outcome in the state.
    - Mixture:
      p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * [lambda_rl * p_rl + (1 - lambda_rl) * p_wsls]
    
    WM dynamics:
    - Decay toward uniform every trial in the current state (wm_decay).
    - If rewarded, deterministically store the chosen action.
    - WM noise blends W with uniform before computing p_wm.
    
    Set-size effects:
    - RL temperature scaled down at larger set sizes: beta_eff = beta / (1 + size_beta_scale * (nS - 3)).
    - WM weight kept constant here; set size affects RL discriminability.
    
    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Weight of WM channel in the mixture (0..1).
    - softmax_beta: Baseline RL inverse temperature (scaled by 10 internally before size scaling).
    - wsls_weight: Weight on RL inside the non-WM mixture (lambda_rl in [0,1]); 1 -> pure RL, 0 -> pure WSLS.
    - wm_noise: Blend of WM with uniform before softmax (0..1); larger means noisier WM retrieval.
    - size_beta_scale: Strength of set-size penalty on beta (>=0).
    """
    lr, wm_weight, softmax_beta, wsls_weight, wm_noise, size_beta_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        # Set-size scaled RL temperature
        beta_eff = softmax_beta / (1.0 + size_beta_scale * max(0, nS - 3))
        wm_weight_eff = wm_weight

        wm_decay = 0.25  # internal fixed decay; note: not a free parameter to keep total <=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy with noise blended toward uniform
            W_s = w[s, :]
            W_s_noisy = (1.0 - wm_noise) * W_s + wm_noise * w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))
            p_wm = 1.0 / denom_wm

            # WSLS heuristic distribution
            if last_action[s] >= 0:
                if last_reward[s] > 0.5:
                    # Win-stay: high prob on last action, equal split otherwise
                    p_wsls_vec = np.ones(nA) * (0.0)
                    p_wsls_vec[:] = 0.0
                    p_wsls_vec[:] = 0.0  # explicit
                    p_wsls_vec = np.ones(nA) * (0.0)
                    # stay prob 1.0 distributed deterministically; to avoid zeros, add tiny epsilon
                    p_wsls_vec = np.ones(nA) * (1e-12)
                    p_wsls_vec[last_action[s]] = 1.0 - (nA - 1) * 1e-12
                else:
                    # Lose-shift: avoid last action, choose others uniformly
                    p_wsls_vec = np.ones(nA) * (1.0 / (nA - 1))
                    p_wsls_vec[last_action[s]] = 1e-12  # tiny but nonzero
                    # Renormalize tiny tweak
                    p_wsls_vec = p_wsls_vec / np.sum(p_wsls_vec)
            else:
                # If no history, uniform
                p_wsls_vec = np.ones(nA) / nA

            p_wsls = p_wsls_vec[a]

            # Combine non-WM channels
            p_nonwm = wsls_weight * p_rl + (1.0 - wsls_weight) * p_wsls

            # Final mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_nonwm
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: decay then store on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update history for WSLS
            last_reward[s] = r
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p