def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity gating and WM decay (one-shot storage on reward).
    
    Model idea:
    - RL system learns action values via delta rule.
    - WM system stores the most recent rewarded action for each state (deterministic retrieval),
      with global decay toward uniform and an effective mixture weight reduced by set size.
    - Mixture weight is downscaled by a capacity-like factor: min(1, K / nS).
    
    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values.
    - wm_weight: scalar in [0,1]. Baseline weight of WM in the mixture policy.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    - wm_decay: scalar in [0,1]. Global decay per trial of WM table toward uniform.
    - capacity_K: scalar > 0. Effective WM capacity (number of state-action pairs that can be reliably maintained).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity_K = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size gated WM weight
        gate = min(1.0, capacity_K / max(1.0, nS))
        wm_weight_eff_base = wm_weight * gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL policy probability for chosen action a
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM decay toward uniform (global per trial)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM policy: near-deterministic retrieval from w
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            wm_w = wm_weight_eff_base
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store one-shot if reward is 1, else revert to uniform for that state
            if r >= 1.0 - 1e-12:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + WM mixture with entropy-based gating and set-size scaling.
    
    Model idea:
    - RL system updates Q-values with learning rate and global forgetting toward uniform.
    - WM system stores a noisy correct action after rewards (prob mass 1 - wm_epsilon on chosen action),
      otherwise it passively decays toward uniform via entropy gating.
    - The mixture weight is modulated by:
        wm_weight_eff = wm_base * min(1, K / nS) * (1 - entropy(W_s)/log(nA))
      so WM contributes more when memory is sharp and set size is small.
    
    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values.
    - wm_base: scalar in [0,1]. Baseline WM mixture weight.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    - rl_forget: scalar in [0,1]. Global RL forgetting rate toward uniform per trial.
    - wm_epsilon: scalar in [0, 1). Noise in WM storage; 1 - wm_epsilon mass goes to stored action.
    - capacity_K: scalar > 0. Effective WM capacity.
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, rl_forget, wm_epsilon, capacity_K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        base_gate = min(1.0, capacity_K / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL forgetting toward uniform
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            # RL policy for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM update on reward: store noisy correct action
            if r >= 1.0 - 1e-12:
                w[s, :] = wm_epsilon / (nA - 1)
                w[s, a] = 1.0 - wm_epsilon

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy of WM for state s (natural log)
            W_safe = np.clip(W_s, 1e-12, 1.0)
            entropy = -np.sum(W_safe * np.log(W_safe))
            max_entropy = np.log(nA)
            sharpness = 1.0 - (entropy / max_entropy)  # in [0,1]

            wm_w = wm_base * base_gate * sharpness
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Passive WM decay toward uniform when no reward
            if r < 1.0 - 1e-12:
                # Use entropy gating implicitly via mixture; also softly nudge toward uniform
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM delta learning with capacity gating.
    
    Model idea:
    - RL has separate learning rates for positive and negative outcomes.
    - WM is a fast learner updated by a WM-specific learning rate toward either a one-hot (on reward)
      or uniform (on no reward), with mixture weight downscaled by set size via capacity K.
    - WM policy is highly deterministic; RL policy is softmax over Q-values.
    
    Parameters (model_parameters):
    - lr_pos: scalar in [0,1]. RL learning rate after reward=1.
    - lr_neg: scalar in [0,1]. RL learning rate after reward=0.
    - wm_weight: scalar in [0,1]. Baseline weight of WM in the mixture policy.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    - wm_eta: scalar in [0,1]. WM learning rate toward target pattern.
    - capacity_K: scalar > 0. Effective WM capacity.
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_eta, capacity_K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        gate = min(1.0, capacity_K / max(1.0, nS))
        wm_weight_eff_base = wm_weight * gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_w = wm_weight_eff_base
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            if r >= 1.0 - 1e-12:
                q[s, a] += lr_pos * (1.0 - Q_s[a])
            else:
                q[s, a] += lr_neg * (0.0 - Q_s[a])

            # WM update: delta move toward one-hot on reward, uniform on no-reward
            if r >= 1.0 - 1e-12:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]

            w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target

        blocks_log_p += log_p

    return -blocks_log_p