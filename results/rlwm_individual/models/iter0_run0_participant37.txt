def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Working-Memory mixture with age- and set-size-modulated WM, plus perseveration and lapses.
    
    Parameters
    - states: array-like of ints, state index on each trial (0..set_size-1 within a block)
    - actions: array-like of ints, chosen action on each trial (expected 0,1,2; out-of-range treated as lapse)
    - rewards: array-like of floats/ints, observed feedback on each trial (expected 0/1; other values clipped)
    - blocks: array-like of ints, block index for each trial
    - set_sizes: array-like of ints, set size for each trial (3 or 6)
    - age: array-like or scalar, participant's age; used to define older group (>=45)
    - model_parameters: iterable of 6 parameters:
        alpha_raw: RL learning rate (transformed to 0..1 via sigmoid)
        beta_raw: inverse temperature for softmax (transformed to positive via exp)
        wm_base: baseline WM reliance (mixed via sigmoid)
        wm_decay: how much WM reliance declines with set size (higher => lower WM at larger set)
        persev: perseveration strength (bias toward repeating last action), scaled for older group
        lapse_raw: lapse rate (transformed to small 0..~0.2 via sigmoid)
    
    Returns
    - neg_log_likelihood: negative log-likelihood of the observed choices under the model
    """
    # Unpack and transform parameters
    alpha_raw, beta_raw, wm_base, wm_decay, persev, lapse_raw = model_parameters
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    alpha = sigmoid(alpha_raw)            # 0..1
    beta = np.exp(beta_raw)               # >0
    lapse = 1e-6 + 0.2 * sigmoid(lapse_raw)  # small lapse component
    older = 1 if (np.asarray(age)[0] if np.ndim(age) else age) >= 45 else 0
    
    eps = 1e-12
    nA = 3
    neg_ll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM store
        Q = np.zeros((nS, nA))  # start neutral; softmax handles exploration
        # WM: store for each state the last action that yielded reward 1
        wm_has = np.zeros(nS, dtype=bool)
        wm_act = np.zeros(nS, dtype=int)

        # Perseveration: last action bias per state
        last_action = np.full(nS, -1, dtype=int)

        # Compute block-specific WM weight: declines with set size and more for older adults
        # Age penalty is additive in the logit space
        wm_logit = wm_base - wm_decay * max(0, nS - 3) - 0.5 * older
        wm_weight_block = sigmoid(wm_logit)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r_raw = block_rewards[t]
            # Clean reward to [0,1]
            r = 1.0 if r_raw >= 0.5 else 0.0

            # Build RL policy
            # Add perseveration bias in preference space
            pref = beta * Q[s].copy()
            if last_action[s] >= 0:
                # Older adults persevere more
                kappa = persev * (1.0 + 0.5 * older)
                pref[last_action[s]] += kappa
            # Softmax
            max_pref = np.max(pref)
            expv = np.exp(pref - max_pref)
            p_rl = expv / (np.sum(expv) + eps)

            # Build WM policy
            if wm_has[s]:
                # Deterministic toward memorized correct action; soften slightly to avoid zeros
                p_wm = np.full(nA, eps)
                p_wm[wm_act[s]] = 1.0 - (nA - 1) * eps
            else:
                p_wm = np.full(nA, 1.0 / nA)

            # Mixture with lapse
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Likelihood contribution
            if a < 0 or a >= nA:
                # Out-of-range action: assign tiny probability mass (as lapse outcome)
                pa = max(lapse * (1.0 / nA), eps)
            else:
                pa = max(p_final[a], eps)
            neg_ll -= np.log(pa)

            # Learning updates
            if 0 <= a < nA:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # WM update: only encode on reward, forget otherwise
                if r > 0.5:
                    wm_has[s] = True
                    wm_act[s] = a

                # Update perseveration trace
                last_action[s] = a

    return neg_ll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-RL (separate learning rates) + outcome-contingent WM with age- and set-size-dependent recall,
    plus lapse. Older adults have reduced inverse temperature and WM recall.

    Parameters
    - states: array-like of ints, state index on each trial (0..set_size-1)
    - actions: array-like of ints, chosen action (0,1,2; out-of-range treated as lapse)
    - rewards: array-like of floats/ints, observed feedback (0/1; other values clipped)
    - blocks: array-like of ints, block index for each trial
    - set_sizes: array-like of ints, set size for each trial (3 or 6)
    - age: array-like or scalar, participant's age; age>=45 => older group
    - model_parameters: iterable of 5 parameters:
        alpha_pos_raw: learning rate for positive outcomes (sigmoid to 0..1)
        alpha_neg_raw: learning rate for negative outcomes (sigmoid to 0..1)
        beta_raw: inverse temperature base (exp to >0); reduced for older adults
        wm_cap_raw: base WM capacity/recall strength (sigmoid to 0..1, scaled by set size and age)
        lapse_raw: lapse rate (sigmoid to 0..~0.2)

    Returns
    - neg_log_likelihood: negative log-likelihood of observed choices
    """
    alpha_pos_raw, alpha_neg_raw, beta_raw, wm_cap_raw, lapse_raw = model_parameters
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    alpha_pos = sigmoid(alpha_pos_raw)
    alpha_neg = sigmoid(alpha_neg_raw)
    base_beta = np.exp(beta_raw)
    older = 1 if (np.asarray(age)[0] if np.ndim(age) else age) >= 45 else 0
    # Older group explores more (lower beta)
    beta_factor_age = 0.8 if older else 1.0
    lapse = 1e-6 + 0.2 * sigmoid(lapse_raw)
    base_wm_cap = sigmoid(wm_cap_raw)  # 0..1

    eps = 1e-12
    nA = 3
    neg_ll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: last action and its outcome per state
        last_act = np.full(nS, -1, dtype=int)
        last_out = np.zeros(nS)  # 0 or 1

        # Set size and age modulate WM recall probability
        # Simple capacity-limited recall: scales inversely with set size
        size_factor = 3.0 / float(nS)  # 1.0 for 3, 0.5 for 6
        age_factor = 0.7 if older else 1.0
        wm_recall = np.clip(base_wm_cap * size_factor * age_factor, 0.0, 1.0)

        beta = base_beta * beta_factor_age

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r_raw = block_rewards[t]
            r = 1.0 if r_raw >= 0.5 else 0.0

            # RL policy
            pref = beta * Q[s]
            max_pref = np.max(pref)
            expv = np.exp(pref - max_pref)
            p_rl = expv / (np.sum(expv) + eps)

            # WM policy depends on last outcome for the same state
            if last_act[s] >= 0:
                if last_out[s] > 0.5:
                    # Repeat last rewarded action
                    p_wm = np.full(nA, eps)
                    p_wm[last_act[s]] = 1.0 - (nA - 1) * eps
                else:
                    # Avoid last punished action: split mass on the two others
                    p_wm = np.full(nA, 0.0)
                    avoid = last_act[s]
                    for aa in range(nA):
                        if aa != avoid:
                            p_wm[aa] = 0.5
                    p_wm = np.clip(p_wm, eps, 1.0)
                    p_wm = p_wm / np.sum(p_wm)
            else:
                p_wm = np.full(nA, 1.0 / nA)

            # Mixture and lapse
            p_mix = wm_recall * p_wm + (1.0 - wm_recall) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            if a < 0 or a >= nA:
                pa = max(lapse * (1.0 / nA), eps)
            else:
                pa = max(p_final[a], eps)
            neg_ll -= np.log(pa)

            # Learning
            if 0 <= a < nA:
                # Separate learning rates
                if r > 0.5:
                    Q[s, a] += alpha_pos * (r - Q[s, a])
                else:
                    Q[s, a] += alpha_neg * (r - Q[s, a])
                # Update WM trace
                last_act[s] = a
                last_out[s] = r

    return neg_ll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dirichlet-like count learning with forgetting and short-term punishment avoidance,
    modulated by age and set size, combined with softmax and lapse.

    Parameters
    - states: array-like of ints, state index per trial
    - actions: array-like of ints, chosen action (0..2; out-of-range treated as lapse)
    - rewards: array-like of floats/ints, observed feedback (0/1; other values clipped)
    - blocks: array-like of ints, block index per trial
    - set_sizes: array-like of ints, set size per trial (3 or 6)
    - age: array-like or scalar, participant's age; age>=45 => older group
    - model_parameters: iterable of 5 parameters:
        beta_raw: inverse temperature base (exp to >0)
        forget_raw: baseline forgetting rate for counts (sigmoid to 0..1), increased by age and set size
        punish_raw: strength of punishment avoidance (sigmoid to 0..1) affecting short-term memory and counts
        wm_gain_raw: gain for rewarded updates to counts (sigmoid to 0..1), reduced for older group
        lapse_raw: lapse rate (sigmoid to 0..~0.2)
    
    Returns
    - neg_log_likelihood: negative log-likelihood of the observed choices
    """
    beta_raw, forget_raw, punish_raw, wm_gain_raw, lapse_raw = model_parameters
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    beta = np.exp(beta_raw)
    base_forget = sigmoid(forget_raw)  # 0..1
    punish = sigmoid(punish_raw)       # 0..1
    base_gain = sigmoid(wm_gain_raw)   # 0..1
    lapse = 1e-6 + 0.2 * sigmoid(lapse_raw)

    older = 1 if (np.asarray(age)[0] if np.ndim(age) else age) >= 45 else 0
    eps = 1e-12
    nA = 3
    neg_ll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = np.asarray(actions)[mask]
        block_rewards = np.asarray(rewards)[mask]
        block_states = np.asarray(states)[mask]
        block_set_sizes = np.asarray(set_sizes)[mask]
        nS = int(block_set_sizes[0])

        # Dirichlet-like counts initialized as weak symmetric prior
        counts = np.ones((nS, nA))  # prior of 1 per action
        # Short-term punishment memory M (action-specific aversion) per state
        M = np.zeros((nS, nA))

        # Effective forgetting increases with set size and for older adults
        forget = base_forget + 0.10 * older + 0.05 * max(0, nS - 3)
        forget = np.clip(forget, 0.0, 0.5)  # keep within reasonable range
        # Aversion decay tied to forgetting
        aversion_decay = 1.0 - forget
        # Reward gain reduced for older adults
        gain = base_gain * (1.0 - 0.3 * older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r_raw = block_rewards[t]
            r = 1.0 if r_raw >= 0.5 else 0.0

            # Compute probability from counts and aversion
            # Convert counts to mean probabilities
            prior = counts[s] / (np.sum(counts[s]) + eps)
            # Include avoidance by subtracting aversion in preference space
            pref = beta * np.log(np.clip(prior, eps, 1.0))  # log prior as preference
            pref -= punish * M[s]  # aversive bias
            # Softmax over adjusted preferences
            max_pref = np.max(pref)
            expv = np.exp(pref - max_pref)
            p_pol = expv / (np.sum(expv) + eps)
            # Lapse mixture
            p_final = (1.0 - lapse) * p_pol + lapse * (1.0 / nA)

            if a < 0 or a >= nA:
                pa = max(lapse * (1.0 / nA), eps)
            else:
                pa = max(p_final[a], eps)
            neg_ll -= np.log(pa)

            # Updates
            # Apply forgetting to counts and aversion
            counts[s] = (1.0 - forget) * counts[s] + forget * (1.0 / nA)  # drift to symmetric prior
            M[s] *= aversion_decay

            if 0 <= a < nA:
                if r > 0.5:
                    # Reward: add gain to the chosen action's count
                    counts[s, a] += gain
                    # Decrease aversion for the chosen action on reward (forgiveness)
                    M[s, a] = max(0.0, M[s, a] - 0.5 * punish * gain)
                else:
                    # Punishment: slightly decrease the chosen action's count and increase aversion
                    # Move mass to other actions indirectly by renormalizing later
                    decrement = 0.5 * punish * (1.0 + gain)
                    counts[s, a] = max(eps, counts[s, a] - decrement)
                    # Increase aversion for chosen action
                    M[s, a] += (1.0 + gain) * punish

                # Keep counts positive and not exploding
                counts[s] = np.clip(counts[s], eps, None)

    return neg_ll