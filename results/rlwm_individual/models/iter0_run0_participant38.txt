def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited working memory, WM decay, and WM learning rate.

    The model mixes a standard model-free RL policy with a working-memory (WM) policy.
    The WM policy is strong (high inverse temperature), but its contribution is limited
    by a capacity parameter that scales with set size. WM traces also decay toward uniform.

    Parameters
    ----------
    model_parameters: tuple of 6 floats
        lr:            Learning rate for RL value updates (0..1).
        wm_weight:     Base mixture weight of WM vs RL (0..1).
        softmax_beta:  Inverse temperature for RL softmax (scaled internally).
        wm_decay:      Decay of WM toward uniform per trial (0..1).
        wm_lr:         WM learning rate when a rewarded association is observed (0..1).
        wm_capacity:   Effective WM capacity in number of state-action pairs (e.g., 1..6).
                       The WM weight is scaled by min(1, wm_capacity / set_size).

    Returns
    -------
    Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np  # assumed available per guardrails; included only for clarity of use

    lr, wm_weight, softmax_beta, wm_decay, wm_lr, wm_capacity = model_parameters
    softmax_beta *= 10.0  # higher upper bound per template
    softmax_beta_wm = 50  # very deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action a under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (nearly deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-limited WM contribution: down-weight WM as set size increases
            wm_eff = wm_weight * min(1.0, float(wm_capacity) / float(nS))

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            # Numerical guard
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning: if rewarded, move WM toward one-hot for the chosen action
            if r > 0.0:
                oh = np.zeros(nA)
                oh[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * oh

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with reward-gated WM encoding, set-size-dependent WM efficacy, and lapses.

    The model mixes RL and WM policies. WM encodes the most recent successful action
    with strength controlled by a gating parameter. WM efficacy decreases with set size
    following a power-law. A small lapse rate mixes in uniform random choice.

    Parameters
    ----------
    model_parameters: tuple of 6 floats
        lr:            RL learning rate (0..1).
        wm_weight:     Base WM mixture weight (0..1).
        softmax_beta:  RL inverse temperature (scaled internally).
        wm_gate:       Strength of WM encoding on rewarded trials (0..1). No update if r=0.
        lapse_eps:     Lapse rate mixing uniform random choice (0..0.2 typical).
        size_phi:      Exponent controlling how WM weakens with set size:
                       wm_eff = wm_weight / (1 + (set_size - 1)**size_phi)

    Returns
    -------
    Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available

    lr, wm_weight, softmax_beta, wm_gate, lapse_eps, size_phi = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM efficacy (power-law decline)
        wm_eff_block = wm_weight / (1.0 + max(0, nS - 1) ** max(0.0, size_phi))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL/WM mixture then lapse
            p_mix = wm_eff_block * p_wm + (1.0 - wm_eff_block) * p_rl
            p_total = (1.0 - lapse_eps) * p_mix + lapse_eps * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Reward-gated WM encoding: shift toward chosen action only when rewarded
            if r > 0.0:
                oh = np.zeros(nA)
                oh[a] = 1.0
                w[s, :] = (1.0 - wm_gate) * w[s, :] + wm_gate * oh
            # If unrewarded, leave WM as is (no decay here; decay effect emerges via size_phi/lapses)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-weighted WM (Dirichlet-like memory with decay and entropy-based control).

    WM stores action counts per state with a symmetric prior, decaying over time.
    The WM policy is strong when memory is certain (low entropy) and weak when uncertain (high entropy).
    The WM weight is modulated online by the normalized entropy of the WM distribution.

    Parameters
    ----------
    model_parameters: tuple of 5 floats
        lr:            RL learning rate (0..1).
        wm_weight:     Maximum WM mixture weight (0..1).
        softmax_beta:  RL inverse temperature (scaled internally).
        wm_decay:      Decay of WM counts toward the prior per trial (0..1).
        wm_alpha0:     Symmetric Dirichlet prior concentration (>0). Larger = more diffuse prior.

    Returns
    -------
    Negative log-likelihood of the observed choices.
    """
    import numpy as np  # assumed available

    lr, wm_weight, softmax_beta, wm_decay, wm_alpha0 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM represented via counts (Dirichlet-like); initialize with symmetric prior
        counts = (wm_alpha0 / nA) * np.ones((nS, nA))
        w0 = counts.copy()  # prior reference

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Convert counts to probabilities for WM policy
            W_s_probs = counts[s, :] / np.sum(counts[s, :])

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (nearly deterministic softmax over WM probs)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_probs - W_s_probs[a])))

            # Entropy-based WM weight: lower entropy => higher WM contribution
            eps = 1e-12
            H = -np.sum(W_s_probs * np.log(W_s_probs + eps))
            H_max = np.log(nA)
            wm_entropy_scale = 1.0 - (H / H_max)  # 0 at max uncertainty, 1 at full certainty
            wm_eff = wm_weight * wm_entropy_scale

            p_total = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay of counts toward prior
            counts[s, :] = (1.0 - wm_decay) * counts[s, :] + wm_decay * w0[s, :]

            # WM update of counts: increment chosen action on rewarded trials
            if r > 0.0:
                counts[s, a] += 1.0  # strengthens certainty for the chosen action

        blocks_log_p += log_p

    return -blocks_log_p