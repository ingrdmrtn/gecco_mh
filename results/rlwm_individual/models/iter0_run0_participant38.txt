def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity-limited, rewarded-only working memory.

    Idea:
    - RL learns action values per state via a delta rule.
    - WM stores the last rewarded action per state as a one-hot policy that decays toward uniform.
    - WM's contribution is capacity-limited: its effective weight declines when set size exceeds a capacity parameter.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values.
    - model_parameters[1] = wm_weight (float in [0,1]): Base mixture weight of WM policy vs RL policy.
    - model_parameters[2] = softmax_beta (float > 0): Inverse temperature for RL softmax (internally scaled by 10).
    - model_parameters[3] = wm_decay (float in [0,1]): Trial-wise decay of WM rows toward uniform.
    - model_parameters[4] = capacity_k (float > 0): WM capacity (in number of states). Effective WM weight scales as min(1, capacity_k / set_size), thus smaller for set size 6 than 3 if capacity_k < 6.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity_k = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1 / nA) * np.ones((nS, nA))   # WM policy rows (probabilities)
        w_0 = (1 / nA) * np.ones((nS, nA)) # Uniform target for decay

        # Capacity-adjusted WM weight for this block
        cap_scale = min(1.0, capacity_k / max(1.0, nS))
        wm_w_block = wm_weight * cap_scale

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM row (deterministic when a rewarded action is stored)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_w_block * p_wm + (1 - wm_w_block) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # If rewarded, store deterministic one-hot policy for this state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM win-stay/lose-shift with set-size-dependent WM efficacy.

    Idea:
    - RL uses separate learning rates for positive vs negative outcomes.
    - WM implements win-stay/lose-shift memory per state:
      - If rewarded, WM stores the chosen action as a one-hot policy.
      - If not rewarded, WM suppresses the chosen action and redistributes mass to the others.
      - WM rows decay toward uniform each trial.
    - WM's effective weight decays with set size via an exponential factor.

    Parameters
    - model_parameters[0] = lr_pos (float): RL learning rate for rewards (r=1).
    - model_parameters[1] = lr_neg (float): RL learning rate for nonrewards (r=0).
    - model_parameters[2] = wm_weight (float in [0,1]): Base WM mixture weight.
    - model_parameters[3] = softmax_beta (float > 0): Inverse temperature for RL softmax (internally scaled by 10).
    - model_parameters[4] = wm_decay (float in [0,1]): Decay rate of WM rows toward uniform.
    - model_parameters[5] = lapse_slope (float >= 0): Set-size penalty for WM; effective WM weight scales by exp(-lapse_slope*(set_size-3)) so WM is weaker at set size 6.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, lapse_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))   # RL Q-values
        w = (1 / nA) * np.ones((nS, nA))   # WM policy rows (probabilities)
        w_0 = (1 / nA) * np.ones((nS, nA)) # Uniform target for decay

        # Set-size-dependent WM efficacy
        size_penalty = np.exp(-lapse_slope * max(0, nS - 3))
        wm_w_block = wm_weight * size_penalty

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from current WM row
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_w_block * p_wm + (1 - wm_w_block) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr_use * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if r > 0.5:
                # Win-stay: store chosen action as one-hot
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Lose-shift: suppress chosen action, redistribute to other actions
                # Keep some decayed memory, then push mass off the chosen action
                remaining = 1.0 - min(1.0, w[s, a])
                w[s, a] = 0.0
                # Renormalize to sum to 1 across other actions
                sum_others = np.sum(w[s, :])
                if sum_others <= 1e-12:
                    # If all zero (can happen numerically), assign uniform over other actions
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] = 1.0 / (nA - 1)
                else:
                    w[s, :] = w[s, :] / sum_others
                    w[s, a] = 0.0  # ensure chosen action remains 0 after renorm

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size-dependent WM forgetting and choice perseveration embedded in WM.

    Idea:
    - RL uses a standard delta rule.
    - WM stores evidence for actions per state and decays toward uniform. Forgetting accelerates with larger set sizes.
    - WM also carries a perseveration bias: it blends toward the most recently chosen action for that state,
      capturing choice stickiness without altering the RL softmax directly.
    - Mixture of WM and RL policies.

    Parameters
    - model_parameters[0] = lr (float): RL learning rate for Q-values.
    - model_parameters[1] = wm_weight (float in [0,1]): Base WM mixture weight with RL.
    - model_parameters[2] = softmax_beta (float > 0): Inverse temperature for RL softmax (internally scaled by 10).
    - model_parameters[3] = wm_decay_base (float in [0,1]): Baseline WM decay toward uniform.
    - model_parameters[4] = wm_decay_slope (float >= 0): Additional decay per unit increase in set size (relative to 3). Effective decay = wm_decay_base + wm_decay_slope * (set_size-3)/3, clipped to [0,1].
    - model_parameters[5] = stickiness (float >= 0): Strength of WM blending toward the last chosen action in that state (perseveration). Larger values increase the probability of repeating the last choice, particularly when set size is large (since RL may be weaker).

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_decay_slope, stickiness = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))    # RL Q-values
        w = (1 / nA) * np.ones((nS, nA))    # WM policy rows (probabilities)
        w_0 = (1 / nA) * np.ones((nS, nA))  # Uniform target for decay

        # Track last chosen action per state for perseveration inside WM
        last_choice = -np.ones(nS, dtype=int)

        # Set-size-dependent WM decay
        extra = max(0.0, (nS - 3) / 3.0)
        wm_decay_block = np.clip(wm_decay_base + wm_decay_slope * extra, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # Add perseveration bias within WM by blending toward last chosen action
            if last_choice[s] >= 0:
                onehot_last = np.zeros(3)
                onehot_last[last_choice[s]] = 1.0
                # Blend: convex combination then renormalize
                W_s = (1 - stickiness) * W_s + stickiness * onehot_last
                W_s = W_s / max(np.sum(W_s), 1e-12)

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from (possibly perseveration-blended) W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # First decay stored WM row toward uniform
            w[s, :] = (1 - wm_decay_block) * w[s, :] + wm_decay_block * w_0[s, :]
            # If rewarded, reinforce chosen action strongly in WM (one-hot)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # Update last choice for perseveration on the next encounter
            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p