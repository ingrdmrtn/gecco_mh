def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+Capacity-limited working memory mixture with age-modulated WM capacity and lapses.

    Idea:
    - Choices are a mixture of a slow RL system and a capacity-limited WM system.
    - WM stores the last rewarded action for each state when feedback=1; otherwise it's uninformative.
    - WM influence is attenuated as set size exceeds a capacity parameter, and further reduced for older adults.
    - A lapse parameter accounts for action omissions/invalid responses and occasional random choices.

    Parameters (model_parameters):
    - alpha_rl: learning rate for RL (0..1)
    - beta: inverse temperature for RL softmax; internally scaled up by 10
    - wm_weight_base: baseline mixture weight on WM (0..1)
    - K_wm: WM capacity (in number of state-action pairs that can be maintained); interacts with set size
    - lapse: lapse probability for random choice and invalid choices (0..1)

    Inputs:
    - states: array-like, state index on each trial (0..set_size-1 within block)
    - actions: array-like, chosen action on each trial; valid actions are {0,1,2}; invalid like -2 are treated as lapses
    - rewards: array-like, feedback per trial; valid feedback in {0,1}; negative (e.g., -1) indicates no feedback/omission
    - blocks: array-like, block index per trial; values group trials into blocks
    - set_sizes: array-like, set size per trial (3 or 6 here)
    - age: array-like with a single value; age[0] in years
    - model_parameters: list or array of parameters as defined above

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_rl, beta, wm_weight_base, K_wm, lapse = model_parameters
    beta *= 10.0
    age_val = float(age[0])
    is_older = age_val >= 45.0
    # Older adult WM capacity penalty factor
    age_capacity_factor = 0.6 if is_older else 1.0
    # Scale RL learning mildly by age group to reflect reduced plasticity in older adults
    alpha_rl_eff = alpha_rl * (0.8 if is_older else 1.0)

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        Q = np.zeros((nS, nA))  # RL values
        # WM store: by default uniform. If a state has a known rewarded action from last correct feedback,
        # we set a one-hot distribution on that action.
        W = (1.0 / nA) * np.ones((nS, nA))
        # Track whether WM has a stored action for the state
        wm_has = np.zeros(nS, dtype=bool)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_curr = int(block_set_sizes[t])

            # Effective WM weight scales with capacity and set size, with age modulation
            K_eff = max(0.0, K_wm * age_capacity_factor)
            wm_weight = wm_weight_base * min(1.0, K_eff / max(1.0, float(nS_curr)))

            # RL policy
            q_s = Q[s, :]
            # Stable softmax by subtracting max
            pref = beta * (q_s - np.max(q_s))
            exp_pref = np.exp(pref)
            p_rl_vec = exp_pref / np.sum(exp_pref)

            # WM policy: if we have a stored action for this state, use it; else uniform
            w_s = W[s, :]
            p_wm_vec = w_s

            # Mixture and lapse
            p_mix_vec = wm_weight * p_wm_vec + (1.0 - wm_weight) * p_rl_vec
            p_final_vec = (1.0 - lapse) * p_mix_vec + lapse * (1.0 / nA)

            # Likelihood contribution
            if a in [0, 1, 2]:
                p_a = float(p_final_vec[a])
                nll -= np.log(max(p_a, eps))
            else:
                # Invalid/omission: assume generated by lapse alone; no learning update
                p_lapse_only = lapse + (1.0 - lapse) * (1.0 / nA)  # probability mass on random responding
                nll -= np.log(max(p_lapse_only, eps))
                # Skip learning and continue
                continue

            # Learning updates only when feedback is valid (>=0)
            if r >= 0.0:
                # RL update
                delta = r - q_s[a]
                Q[s, a] = q_s[a] + alpha_rl_eff * delta

                # WM update: store the last rewarded action; clear memory on negative feedback
                if r > 0.5:
                    # Set one-hot on action a
                    W[s, :] = 0.0
                    W[s, a] = 1.0
                    wm_has[s] = True
                else:
                    # With no reward, WM becomes uninformative for this state
                    W[s, :] = (1.0 / nA)
                    wm_has[s] = False

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning, state-specific perseveration, decay-to-baseline, and set size–dependent exploration.
    Age affects learning and decay.

    Idea:
    - Choices follow a softmax over action values with an added perseveration bias for the last chosen action in the state.
    - Asymmetric learning rates for positive vs. non-reward outcomes.
    - Values decay toward zero each trial (forgetting), stronger in older adults.
    - In larger set sizes, effective exploration increases by lowering inverse temperature (beta / set_size).
    - A lapse parameter accounts for omissions/invalid actions and random choices.

    Parameters (model_parameters):
    - alpha_pos: learning rate when r=1 (0..1)
    - alpha_neg: learning rate when r=0 (0..1)
    - beta: base inverse temperature; internally scaled by 10 and divided by set size
    - kappa_persev: perseveration strength added to last chosen action in a state
    - lambda_decay: per-trial decay rate toward zero (0..1)
    - lapse: lapse probability for random choice and invalid responses (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial as described in cognitive_model1

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, kappa_persev, lambda_decay, lapse = model_parameters
    beta *= 10.0
    age_val = float(age[0])
    is_older = age_val >= 45.0

    # Age modulation: older adults have reduced learning and stronger decay
    alpha_pos_eff = alpha_pos * (0.8 if is_older else 1.0)
    alpha_neg_eff = alpha_neg * (0.8 if is_older else 1.0)
    lambda_decay_eff = min(1.0, lambda_decay * (1.2 if is_older else 1.0))

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Track last action in each state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_curr = int(block_set_sizes[t])

            # Apply decay toward zero each trial for the current state (local decay suffices; global would also be plausible)
            Q[s, :] *= (1.0 - lambda_decay_eff)

            # Softmax with perseveration bias and set-size dependent exploration (beta / set_size)
            bias = np.zeros(nA)
            if last_action[s] in [0, 1, 2]:
                bias[last_action[s]] = kappa_persev
            # Compute preferences
            beta_eff = beta / max(1, nS_curr)
            pref = beta_eff * Q[s, :] + bias
            pref = pref - np.max(pref)
            p = np.exp(pref)
            p = p / np.sum(p)

            # Mixture with lapse
            p_final = (1.0 - lapse) * p + lapse * (1.0 / nA)

            # Likelihood
            if a in [0, 1, 2]:
                nll -= np.log(max(float(p_final[a]), eps))
            else:
                # Invalid/omission treated as lapse
                p_lapse_only = lapse + (1.0 - lapse) * (1.0 / nA)
                nll -= np.log(max(p_lapse_only, eps))
                # Do not update learning or perseveration on invalid actions
                continue

            # Update last action for perseveration
            last_action[s] = a

            # Learning with asymmetric rates when feedback valid
            if r >= 0.0:
                alpha = alpha_pos_eff if r > 0.5 else alpha_neg_eff
                delta = r - Q[s, a]
                Q[s, a] = Q[s, a] + alpha * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted RL–WM hybrid with age- and set size–modulated WM reliance.

    Idea:
    - RL provides a softmax policy over Q-values.
    - WM is a recency/count-based memory for rewarded actions (Dirichlet-like counts) forming its own softmax policy.
    - The model adaptively weights WM vs RL by a sigmoid of (WM capacity per item - RL uncertainty).
      RL uncertainty is approximated by the entropy of the RL policy.
    - WM capacity per item decreases with set size and is further reduced in older adults.
    - A lapse parameter accounts for omissions/invalid choices.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: inverse temperature for RL softmax; internally scaled by 10
    - wm_capacity: working memory capacity parameter (in items); interacts with set size and age
    - eta_unc: sensitivity of the uncertainty-based arbitration (higher => stronger reliance on WM when capacity per item > uncertainty)
    - lapse: lapse probability for random choice and invalid responses (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial as described in cognitive_model1

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_capacity, eta_unc, lapse = model_parameters
    beta *= 10.0
    age_val = float(age[0])
    is_older = age_val >= 45.0
    # Age modulation of effective WM capacity
    wm_cap_eff_age = wm_capacity * (0.6 if is_older else 1.0)

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM (count memory)
        Q = np.zeros((nS, nA))
        M = np.ones((nS, nA)) * (1.0)  # pseudo-counts; start uniform

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_curr = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            pref = beta * (q_s - np.max(q_s))
            exp_pref = np.exp(pref)
            p_rl = exp_pref / np.sum(exp_pref)

            # WM policy from counts (sharpened by large implicit temperature)
            m_s = M[s, :]
            wm_pref = m_s - np.max(m_s)
            wm_pref = 10.0 * wm_pref  # strong WM determinism when memory is clear
            exp_wm = np.exp(wm_pref)
            p_wm = exp_wm / np.sum(exp_wm)

            # RL uncertainty approximated by entropy of RL policy (in nats, max ln(3))
            entropy_rl = -np.sum(p_rl * np.log(np.maximum(p_rl, eps)))
            # Effective capacity per item (more states => less capacity per item)
            cap_per_item = wm_cap_eff_age / max(1.0, float(nS_curr))
            # Arbitration weight via sigmoid of (cap_per_item - uncertainty), scaled by eta_unc
            w_wm = 1.0 / (1.0 + np.exp(-eta_unc * (cap_per_item - entropy_rl)))

            # Mixture with lapse
            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Likelihood
            if a in [0, 1, 2]:
                nll -= np.log(max(float(p_final[a]), eps))
            else:
                # Invalid/omission: treat as lapse
                p_lapse_only = lapse + (1.0 - lapse) * (1.0 / nA)
                nll -= np.log(max(p_lapse_only, eps))
                continue

            # Learning when feedback valid
            if r >= 0.0:
                # RL update
                delta = r - q_s[a]
                Q[s, a] = q_s[a] + alpha * delta
                # WM counts update: reward strengthens the chosen action; no reward mildly weakens it
                if r > 0.5:
                    M[s, a] += 1.0
                else:
                    # Small decay toward uniform on non-reward to avoid runaway certainty
                    M[s, :] = 0.9 * M[s, :] + 0.1

    return nll