Below are three standalone cognitive models tailored to the RL–WM task and this participant’s profile (age 50 → older group). Each function:

- Takes inputs: states, actions, rewards, blocks, set_sizes, age, model_parameters
- Uses all parameters meaningfully
- Returns the negative log-likelihood of the observed choices
- Incorporates age group and set size effects
- Uses up to 6 parameters per model
- Contains clear docstrings

Note: Assumes numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory (WM) with age- and set-size–dependent WM gating and decay.

    Idea:
      - Decisions come from a mixture of a model-free RL policy and a WM policy.
      - WM stores the last rewarded action per state (one-shot memory) and decays back to uniform.
      - WM’s contribution is gated by set-size (capacity limits) and by age group (older adults have reduced WM availability).
      - RL updates with a single learning rate.
      - The same softmax inverse temperature is used for both RL and WM channels.

    Parameters (model_parameters):
      - alpha: float in (0,1), RL learning rate
      - beta: float > 0, softmax inverse temperature (will be rescaled internally)
      - wm_weight: float in [0,1], baseline mixing weight for WM
      - wm_capacity: float > 0, capacity parameter; effective WM weight scales as min(1, wm_capacity / set_size)
      - wm_decay: float in [0,1], per-trial decay of WM values toward uniform for the currently visited state
      - age_effect: float in [0,1], fractional reduction of WM gating if older (effective_wm *= (1 - age_effect))

    Inputs:
      - states: 1D array of state indices (ints)
      - actions: 1D array of chosen action indices (ints in {0,1,2})
      - rewards: 1D array of rewards (0/1)
      - blocks: 1D array of block indices (ints)
      - set_sizes: 1D array of set sizes per trial (3 or 6)
      - age: 1D array-like with a single entry (e.g., [50])
      - model_parameters: list/tuple of six floats as described

    Returns:
      - neg_log_likelihood: float, the negative log-likelihood of observed choices
    """
    alpha, beta, wm_weight, wm_capacity, wm_decay, age_effect = model_parameters
    # scale beta for a reasonable dynamic range
    beta_eff_scale = 10.0
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])  # given fixed set size per block

        # Initialize RL and WM value tables
        Q = np.zeros((nS, nA))  # RL starts neutral
        W = (1.0 / nA) * np.ones((nS, nA))  # WM starts as uniform

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_block = float(block_set_sizes[t])

            # Effective WM gate: baseline * capacity factor * age factor
            capacity_factor = min(1.0, wm_capacity / max(1.0, nS_block))
            age_factor = 1.0 - is_older * age_effect
            mix_w = wm_weight * capacity_factor * age_factor
            # clip to [0,1]
            mix_w = max(0.0, min(1.0, mix_w))

            # Compute policy from RL
            Q_s = Q[s, :]
            # stable softmax
            beta_rl = beta * beta_eff_scale
            Q_center = Q_s - np.max(Q_s)
            expQ = np.exp(beta_rl * Q_center)
            p_rl_vec = expQ / np.sum(expQ)

            # Compute policy from WM
            W_s = W[s, :]
            W_center = W_s - np.max(W_s)
            expW = np.exp(beta_rl * W_center)
            p_wm_vec = expW / np.sum(expW)

            # Mixture
            p_total_vec = mix_w * p_wm_vec + (1.0 - mix_w) * p_rl_vec
            pa = max(1e-12, p_total_vec[a])
            log_p += np.log(pa)

            # RL update (delta rule)
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: decay toward uniform for the visited state, then store on reward
            # decay
            W[s, :] = (1.0 - wm_decay) * W[s, :] + wm_decay * (1.0 / nA)
            # if rewarded, one-shot store
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with age- and set-size–dependent exploration and perseveration.

    Idea:
      - Model-free RL with separate learning rates for positive and negative outcomes.
      - Softmax inverse temperature decreases with larger set size (higher uncertainty) and for older adults.
      - Action perseveration (stickiness) that is stronger in older adults and in smaller set sizes (where habits may form more easily).
      - Perseveration adds a bias to repeat the previous action in the current block.

    Parameters (model_parameters):
      - alpha_pos: float in (0,1), learning rate after reward=1
      - alpha_neg: float in (0,1), learning rate after reward=0
      - beta: float > 0, base softmax inverse temperature (rescaled internally)
      - stickiness_base: float >= 0, baseline perseveration strength added to the last chosen action
      - age_effect: float in [0,1], proportional increase in perseveration and proportional decrease in beta if older
      - setsize_explore: float >= 0, modulates both beta and perseveration with set size

    Inputs:
      - states, actions, rewards, blocks, set_sizes: 1D arrays (see task spec)
      - age: 1D array-like with a single entry (e.g., [50])
      - model_parameters: list/tuple of six floats as described

    Returns:
      - neg_log_likelihood: float
    """
    alpha_pos, alpha_neg, beta, stickiness_base, age_effect, setsize_explore = model_parameters
    beta_eff_scale = 10.0
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize Q-values per block
        Q = np.zeros((nS, nA))

        prev_action = None  # for perseveration within block
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_block = float(block_set_sizes[t])

            # Effective beta decreases with set size and age
            beta_adj = beta * beta_eff_scale
            beta_adj = beta_adj / (1.0 + setsize_explore * max(0.0, (nS_block - 3.0)) / 3.0)
            beta_adj = beta_adj / (1.0 + is_older * age_effect)
            beta_adj = max(1e-6, beta_adj)

            # Perseveration term increases with age and decreases with set size
            kappa = stickiness_base * (1.0 + is_older * age_effect)
            kappa = kappa / (1.0 + setsize_explore * max(0.0, (nS_block - 3.0)) / 3.0)

            # Compute choice preferences = Q + perseveration bias
            pref = Q[s, :].copy()
            if prev_action is not None:
                pref[prev_action] += kappa

            # Softmax
            pref_center = pref - np.max(pref)
            expP = np.exp(beta_adj * pref_center)
            p_vec = expP / np.sum(expP)

            pa = max(1e-12, p_vec[a])
            neg_loglik -= np.log(pa)

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            alpha_use = alpha_pos if r > 0.5 else alpha_neg
            Q[s, a] += alpha_use * pe

            prev_action = a

    return float(neg_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controller between RL and elimination-based working memory (EM), with lapse.

    Idea:
      - Two policies: (1) model-free RL, (2) elimination memory (EM).
      - EM for each state keeps track of eliminated actions when they lead to reward=0,
        and locks onto a rewarded action when reward=1 (then always prefers it).
        The EM policy is uniform over non-eliminated actions unless a correct action is known.
      - A meta-controller mixes EM and RL based on a logistic gate that depends on:
          * base gate parameter,
          * age group (older -> lower EM reliance),
          * set size cost (larger sets -> lower EM reliance),
          * a simple recent reward balance within the block (more recent errors -> shift toward EM).
      - Includes a lapse rate that blends the final policy with a uniform distribution to capture occasional random choices.

    Parameters (model_parameters):
      - alpha: float in (0,1), RL learning rate
      - beta: float > 0, RL softmax inverse temperature (rescaled internally)
      - gate_base: float, baseline meta-controller bias toward EM (higher -> more EM)
      - age_effect: float >= 0, reduces the gate if older (effective gate -= age_effect)
      - setsize_cost: float >= 0, reduces the gate proportionally to (set_size - 3)
      - lapse: float in [0,0.2], probability of random choice (uniform) on any trial

    Inputs:
      - states, actions, rewards, blocks, set_sizes: 1D arrays (see task spec)
      - age: 1D array-like with a single entry (e.g., [50])
      - model_parameters: list/tuple of six floats as described

    Returns:
      - neg_log_likelihood: float
    """
    alpha, beta, gate_base, age_effect, setsize_cost, lapse = model_parameters
    beta_eff_scale = 10.0
    lapse = max(0.0, min(0.5, lapse))  # keep reasonable bound
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    nA = 3
    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # EM memory: for each state, track eliminated set and known-correct action
        eliminated = [set() for _ in range(nS)]
        known_correct = -np.ones(nS, dtype=int)  # -1 = unknown

        # simple recent reward balance within block (positive for recent success, negative for errors)
        # implemented as an exponentially smoothed running average with decay fixed at 0.7
        reward_balance = 0.0
        decay_rb = 0.7

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_block = float(block_set_sizes[t])

            # RL policy
            beta_adj = beta * beta_eff_scale
            Qs = Q[s, :]
            Q_center = Qs - np.max(Qs)
            p_rl_num = np.exp(beta_adj * Q_center)
            p_rl = p_rl_num / np.sum(p_rl_num)

            # EM policy
            if known_correct[s] >= 0:
                # deterministic preference for the known correct action
                em_pref = np.full(nA, 0.0)
                em_pref[known_correct[s]] = 1.0
                p_em = em_pref
            else:
                # choose uniformly among non-eliminated actions
                available = [aa for aa in range(nA) if aa not in eliminated[s]]
                if len(available) == 0:
                    available = list(range(nA))
                p_em = np.zeros(nA)
                for aa in available:
                    p_em[aa] = 1.0 / len(available)

            # Meta-controller gate toward EM
            # Gate combines: base, age penalty, set size penalty, and recent reward balance
            # Intuition: after recent errors (negative balance), increase EM reliance (subtracting a negative adds).
            gate_lin = gate_base
            gate_lin -= is_older * age_effect
            gate_lin -= setsize_cost * max(0.0, (nS_block - 3.0))
            gate_lin += reward_balance  # dynamic adaptation

            w_em = sigmoid(gate_lin)
            w_em = max(0.0, min(1.0, w_em))

            # Mix EM and RL
            p_mix = w_em * p_em + (1.0 - w_em) * p_rl

            # Lapse mixture with uniform
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            pa = max(1e-12, p_final[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # EM update
            if r > 0.5:
                known_correct[s] = a
                eliminated[s].clear()
            else:
                # eliminate the chosen action if not already known
                if known_correct[s] < 0:
                    eliminated[s].add(a)

            # Update recent reward balance
            reward_balance = decay_rb * reward_balance + (1.0 - decay_rb) * (1.0 if r > 0.5 else -1.0)

    return float(neg_loglik)