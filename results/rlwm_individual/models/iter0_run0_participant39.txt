def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-specific WM decay and one-shot WM learning on reward.

    On each trial, choice probability is a convex combination of:
      - RL policy: softmax over Q-values
      - WM policy: near-deterministic softmax over a fast-updating WM map

    Working memory (WM) is updated in a one-shot manner on rewarded trials and
    decays back toward uniform on every visit to the state. The decay rate depends
    on the set size (3 vs 6), capturing load-dependent WM fragility.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate for Q-values (0..1)
        - wm_weight: mixture weight for WM vs RL (0..1), higher means more WM reliance
        - softmax_beta: inverse temperature for the RL policy; internally scaled by 10
        - wm_lr: WM learning rate (0..1), how strongly a rewarded action is written into WM
        - wm_decay3: WM decay rate for set size 3 (0..1)
        - wm_decay6: WM decay rate for set size 6 (0..1)

    Set-size effects
    ----------------
    - WM decay explicitly depends on set size (wm_decay3 vs wm_decay6), modeling reduced
      WM stability under higher load (set size 6).
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay3, wm_decay6 = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # choose decay based on load
        wm_decay = wm_decay3 if nS == 3 else wm_decay6

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # 1) decay toward uniform for the visited state
            w[s] = (1 - wm_decay) * w[s] + wm_decay * w_0[s]
            # 2) write rewarded action with one-shot strength wm_lr
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_lr) * w[s] + wm_lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-scaled WM reliance (mixture weight) and gated WM learning.

    The mixture weight given to WM diminishes with set size according to a power-law,
    capturing reduced WM usage under higher load. WM learning is gated by reward and
    uses a simple write/forget rule.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1)
        - wm_weight: baseline WM mixture weight at set size 3 (0..1)
        - softmax_beta: RL inverse temperature; internally scaled by 10
        - wm_lr: WM learning rate (0..1) controlling both writing on reward and forgetting on no-reward
        - gamma: set-size sensitivity (>0). Effective WM weight scales as (3 / set_size)^gamma

    Set-size effects
    ----------------
    - Effective WM reliance per block: wm_weight_eff = wm_weight * (3 / set_size)^gamma.
      This is lower for set size 6 than for 3, reflecting reduced WM contribution under higher load.
    """
    lr, wm_weight, softmax_beta, wm_lr, gamma = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size-scaled WM mixture weight
        scale = (3.0 / float(nS)) ** gamma
        wm_weight_eff = np.clip(wm_weight * scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with set-size-scaled WM reliance
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM gated update:
            # - If reward: write toward chosen action
            # - If no reward: forget toward uniform (no explicit negative write)
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_lr) * w[s] + wm_lr * onehot
            else:
                w[s] = (1 - wm_lr) * w[s] + wm_lr * w_0[s]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with win-stay/lose-forget WM dynamics and set-size-specific WM lapses.

    WM policy approximates a win-stay process: following reward, WM shifts preference
    toward the chosen action; following no reward, WM drifts back toward uniform.
    A WM lapse parameter mixes the WM softmax with uniform choice, capturing attentional lapses
    that increase with set size.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight for WM vs RL (0..1)
        - softmax_beta: RL inverse temperature; internally scaled by 10
        - epsilon_wm3: WM lapse (0..1) for set size 3, mixing WM with uniform
        - epsilon_wm6: WM lapse (0..1) for set size 6, typically larger than epsilon_wm3
        - pers_decay: WM update step size (0..1) controlling win-stay strength and lose-forget rate

    Set-size effects
    ----------------
    - Lapse rate in WM (epsilon_wm) is higher for larger set size (6), reducing WM efficacy
      under higher load.
    """
    lr, wm_weight, softmax_beta, epsilon_wm3, epsilon_wm6, pers_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        epsilon_wm = epsilon_wm3 if nS == 3 else epsilon_wm6

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with lapse: mix softmax(W) and uniform
            p_wm_soft = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_uniform = 1.0 / nA
            p_wm = (1 - epsilon_wm) * p_wm_soft + epsilon_wm * p_uniform

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM win-stay / lose-forget update
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - pers_decay) * w[s] + pers_decay * onehot
            else:
                # drift toward uniform when unrewarded
                w[s] = (1 - pers_decay) * w[s] + pers_decay * w_0[s]

        blocks_log_p += log_p

    return -blocks_log_p