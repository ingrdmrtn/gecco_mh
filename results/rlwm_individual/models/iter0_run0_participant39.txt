Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways tailored to the rlwm task. Each function:

- Matches the required signature: (states, actions, rewards, blocks, set_sizes, model_parameters)
- Returns the negative log-likelihood of the observed choices
- Uses all parameters meaningfully (≤ 6 per model)
- Fills in only the “FILL IN” parts of the provided template logic (WM policy and WM updates), while keeping the RL structure intact
- Modulates WM contributions by set size to capture load effects


def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM weight and fast-overwrite WM.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1 within each block).
    actions : array-like, int
        Chosen action on each trial (0..nA-1).
    rewards : array-like, {0,1}
        Feedback on each trial.
    blocks : array-like, int
        Block index for each trial.
    set_sizes : array-like, int
        Set size (number of states) for each trial; constant within a block.
    model_parameters : tuple/list of 5 floats
        lr : RL learning rate in [0,1].
        wm_weight_base : Baseline weight of WM in [0,1].
        softmax_beta : Inverse temperature for RL softmax (rescaled internally).
        wm_decay : WM overwrite/decay rate in [0,1] for WM updates.
        capacity_K : Effective WM capacity (in number of items); down-weights WM when nS > K.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, capacity_K = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # very deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-adjusted WM weight: WM contributes less when set size exceeds capacity
        wm_weight_eff = wm_weight_base * np.clip(capacity_K / max(1.0, float(nS)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (probability of chosen action a)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM values with high beta (deterministic lookup)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Fast-overwrite WM:
            # - If rewarded, imprint the chosen action strongly (towards one-hot).
            # - If not rewarded, push the memory towards uniform (uncertain).
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * onehot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with confidence- and capacity-gated WM usage.

    WM contributes more when:
      - WM is confident for the current state (high max-minus-second-max in W_s),
      - Set size is within effective capacity (sigmoid gate of capacity_K - nS).

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1 within each block).
    actions : array-like, int
        Chosen action on each trial (0..nA-1).
    rewards : array-like, {0,1}
        Feedback on each trial.
    blocks : array-like, int
        Block index for each trial.
    set_sizes : array-like, int
        Set size (number of states) for each trial; constant within a block.
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight : Maximum WM weight in [0,1].
        softmax_beta : Inverse temperature for RL softmax (rescaled internally).
        wm_decay : WM decay/overwrite rate in [0,1].
        wm_threshold : Confidence threshold for WM gate (in [0,1], applied to W_s dispersion).
        capacity_K : Effective WM capacity in items (controls load-based gate).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_threshold, capacity_K = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load gate based on capacity (sigmoid of capacity_K - nS)
        gate_load = 1.0 / (1.0 + np.exp(-(capacity_K - float(nS))))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence gate: based on margin between top-1 and top-2 WM values
            # Compute top-1 and top-2 without sorting full array
            top1 = np.max(W_s)
            # second max via mask
            idx_top1 = np.argmax(W_s)
            second_max = np.max(W_s[np.arange(len(W_s)) != idx_top1]) if nA > 1 else 0.0
            conf_margin = top1 - second_max
            gate_conf = 1.0 / (1.0 + np.exp(-(conf_margin - wm_threshold)))

            wm_weight_eff = wm_weight * gate_conf * gate_load

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Similar to Model 1: imprint rewarded actions; otherwise drift toward uniform.
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * onehot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with separate learning rates and WM with load-dependent interference.
    RL stochasticity (beta) and WM weight are modulated by set size (load).

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1 within each block).
    actions : array-like, int
        Chosen action on each trial (0..nA-1).
    rewards : array-like, {0,1}
        Feedback on each trial.
    blocks : array-like, int
        Block index for each trial.
    set_sizes : array-like, int
        Set size (number of states) for each trial; constant within a block.
    model_parameters : tuple/list of 6 floats
        lr_pos : RL learning rate after reward=1 in [0,1].
        lr_neg : RL learning rate after reward=0 in [0,1].
        wm_weight : Baseline WM weight in [0,1] (will be down-weighted at high load).
        softmax_beta_base : Base inverse temperature for RL (rescaled internally).
        wm_interference : Strength of WM interference/leak toward uniform as load increases [0,1].
        beta_load_slope : Exponent for how beta scales with load (higher values -> stronger reduction at larger set sizes).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta_base, wm_interference, beta_load_slope = model_parameters
    softmax_beta_wm = 50  # keep WM highly deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # RL beta decreases with load: beta_eff = base*10 * (3/nS)^{beta_load_slope}
        softmax_beta = (softmax_beta_base * 10.0) * (3.0 / float(nS)) ** beta_load_slope
        # WM weight also decreases with load
        wm_weight_eff = wm_weight * (3.0 / float(nS))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            delta = r - Q_s[a]
            alpha = lr_pos if r > 0.5 else lr_neg
            q[s][a] += alpha * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Load-dependent WM interference (global leak toward uniform), stronger at higher nS.
            # Scale interference so that it is low at nS=3 and higher at nS=6:
            # alpha_int ranges from ~0.0..wm_interference as nS goes from 3..6.
            alpha_int = wm_interference * (max(0.0, float(nS) - 3.0) / 3.0)
            w = (1.0 - alpha_int) * w + alpha_int * w_0

            # Imprint rewarded actions (fast store) to counteract interference when informative
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong but not complete overwrite to allow gradual consolidation
                w[s, :] = 0.5 * w[s, :] + 0.5 * onehot

        blocks_log_p += log_p

    return -blocks_log_p