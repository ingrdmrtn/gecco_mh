def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) with decay and lapses.

    This model mixes a standard RL policy with a WM policy. WM encodes rewarded state-action
    associations as near-deterministic one-hot preferences, but suffers decay/interference that
    grows with set size. The effective WM contribution and retention scale with a capacity K:
    WM weight is scaled by min(1, K / nS). A lapse parameter mixes a uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block id for each trial (states do not carry over across blocks).
    set_sizes : array-like of int
        Set size for each trial (3 or 6 here).
    model_parameters : tuple/list of 6 floats
        lr            : RL learning rate in [0,1].
        wm_weight     : Base WM mixture weight in [0,1].
        softmax_beta  : Inverse temperature for RL (rescaled by *10 inside).
        wm_decay      : WM retention parameter in [0,1] (higher = more retention per trial).
        K             : WM capacity (in items). Scales WM influence as min(1, K/nS).
        lapse         : Lapse rate mixing a uniform random policy in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, K, lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity scaling: more WM impact and retention when nS <= K
        cap_scale = min(1.0, max(0.0, K / max(1.0, nS)))
        wm_weight_eff = wm_weight * cap_scale
        # Retention per trial: higher with cap_scale, bounded in [0,1]
        retention = wm_decay * cap_scale  # effective retention; decay = 1 - retention

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (near-deterministic when one-hot)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapses
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)

            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # 1) Encode rewarded associations as one-hot; otherwise leave as is
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong encoding toward one-hot
                w[s, :] = onehot
            # 2) Global decay/interference toward uniform template (larger nS -> weaker retention via cap_scale)
            w = (1.0 - (1.0 - retention)) * w + (1.0 - retention) * w_0  # i.e., w = retention*w + (1-retention)*w0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM that toggles between 'recall' and 'novelty exploration', with set-size-dependent forgetting and lapses.

    WM component operates in two modes:
    - Recall: if a state-action mapping is remembered (high WM weight at a single action),
      WM selects it nearly deterministically.
    - Novelty: if memory is weak, WM prefers the least-tried actions for that state (directed exploration).
    Forgetting grows with set size, gradually flattening WM weights toward uniform.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block id for each trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6).
    model_parameters : tuple/list of 6 floats
        lr              : RL learning rate in [0,1].
        wm_weight       : Base WM mixture weight in [0,1].
        softmax_beta    : Inverse temperature for RL (rescaled by *10 inside).
        forgetting      : Base WM forgetting rate toward uniform per trial in [0,1].
        explore_bonus   : Strength of novelty preference (scales WM beta for novelty).
        lapse           : Lapse rate mixing uniform in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, forgetting, explore_bonus, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # for recall mode
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))    # WM preference distribution per (s,a)
        w_0 = (1 / nA) * np.ones((nS, nA))  # uniform template

        # Count of how many times each action was tried in each state (for novelty)
        counts = np.zeros((nS, nA))

        # Set-size-dependent forgetting: larger sets -> faster flattening toward uniform
        f_eff = np.clip(forgetting * (nS / max(1.0, nS)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: blend recall and novelty, weighted by current memory strength
            # Memory strength inferred from how peaked w[s] is relative to uniform
            W_s = w[s, :]
            peak = np.max(W_s)
            mem_strength = np.clip((peak - 1.0 / nA) / (1.0 - 1.0 / nA), 0.0, 1.0)

            # Recall component
            p_recall = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Novelty component: prefer least tried actions in this state
            # Use a softmax over negative counts; scale by explore_bonus
            nov_pref = -counts[s, :]
            beta_novel = softmax_beta_wm * np.clip(explore_bonus, 0.0, None)
            p_novel = 1.0 / np.sum(np.exp(beta_novel * (nov_pref - nov_pref[a])))

            # Combine recall and novelty within WM
            p_wm_core = mem_strength * p_recall + (1.0 - mem_strength) * p_novel

            # Mix WM with RL and add lapses
            p_mix = np.clip(wm_weight, 0.0, 1.0) * p_wm_core + (1.0 - np.clip(wm_weight, 0.0, 1.0)) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)

            log_p += np.log(p_total)

            # Update RL
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update novelty counts
            counts[s, a] += 1.0

            # WM updates:
            # 1) If rewarded, move strongly toward one-hot for chosen action (encode)
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            # 2) Forgetting/interference toward uniform (stronger for larger sets)
            w = (1.0 - f_eff) * w + f_eff * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Set-size-dependent RL learning rate + WM win-stay traces + action stickiness and lapses.

    RL component uses different learning rates for small vs large set sizes, capturing slower
    integration in high load. WM component tracks recent rewarded actions per state through
    a decaying trace (win-stay): when rewarded, the chosen action's WM weight rises toward 1.
    WM decay is stronger in larger sets. A stickiness parameter biases RL toward repeating
    the last action taken in that state.

    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block id for each trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6).
    model_parameters : tuple/list of 6 floats
        lr_small       : RL learning rate used when nS is small (e.g., 3).
        lr_large       : RL learning rate used when nS is large (e.g., 6).
        wm_weight      : WM mixture weight in [0,1], downscaled by 3/nS.
        softmax_beta   : Inverse temperature for RL (rescaled by *10 inside).
        stickiness     : Bias added to the last action taken in a state (implemented in RL policy).
        lapse          : Lapse rate mixing uniform in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_small, lr_large, wm_weight, softmax_beta, stickiness, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM traces
        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))     # WM win-stay trace per (s,a)
        w_0 = (1 / nA) * np.ones((nS, nA))   # uniform baseline (for completeness)

        # Track last action per state for stickiness in RL policy; initialize to -1 (none)
        last_act = -1 * np.ones(nS, dtype=int)

        # Choose RL learning rate by set size
        lr = lr_small if nS <= 3 else lr_large

        # WM decay stronger in larger sets: gamma in (0,1)
        # For nS=3 -> higher retention; nS=6 -> lower retention
        gamma = np.clip(1.0 - (nS - 3) / 6.0, 0.0, 1.0)  # retention multiplier per trial

        # Scale WM weight by load (smaller sets allow more WM control)
        wm_weight_eff = wm_weight * (3.0 / max(3.0, nS))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with stickiness bias for repeating last action in this state
            Q_s = q[s, :].copy()
            if last_act[s] >= 0:
                Q_s[last_act[s]] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy derived from traces w[s]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with lapses
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)

            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update last action for stickiness
            last_act[s] = a

            # WM updates: decay traces and reinforce rewarded choice (win-stay)
            # Decay toward zero preferences (implicitly toward uniform after softmax)
            w[s, :] *= gamma
            if r > 0:
                # Raise chosen action's trace toward 1 (others remain decayed)
                w[s, a] += (1.0 - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p