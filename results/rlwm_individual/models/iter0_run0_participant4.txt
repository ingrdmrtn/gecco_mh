def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying Working Memory (WM) mixture model with age modulation.

    The model combines a slow reinforcement-learning (RL) system with a fast, capacity-limited
    working memory (WM) system. WM reliability decreases as set size increases beyond a capacity
    limit, and WM traces decay over trials. Age modulates the effective WM weight (younger group
    gets a stronger WM contribution, older group weaker).

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : 1D array-like of int
        Chosen action on each trial (0..2).
    rewards : 1D array-like of int
        Binary feedback (0/1) on each trial.
    blocks : 1D array-like of int
        Block index on each trial.
    set_sizes : 1D array-like of int
        Set size on each trial (3 or 6 in this task).
    age : 1D array-like of int/float
        Participant age; age[0] is used. Under 45 is "younger", 45+ "older".
    model_parameters : list or array-like of 5-6 floats
        [alpha, beta, wm_weight_base, wm_capacity, wm_decay, age_slope]
        - alpha: RL learning rate in [0,1].
        - beta: inverse temperature (>0) for softmax; controls exploration.
        - wm_weight_base: base WM weight parameter (transformed by sigmoid internally).
        - wm_capacity: capacity parameter (>0); WM reliability scales ~ capacity/set_size.
        - wm_decay: trace decay per trial in [0,1]; higher = faster WM decay.
        - age_slope: magnitude of age modulation on WM weight (>=0). Effective WM weight is
          scaled by (1 + age_slope) if younger, (1 - age_slope) if older. Clipped to [0,1].

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action choices under the model.
    """
    import numpy as np

    alpha, beta, wm_weight_base, wm_capacity, wm_decay, age_slope = model_parameters
    beta = max(1e-6, beta) * 10.0  # give beta a higher dynamic range
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    alpha = min(max(alpha, 0.0), 1.0)
    wm_capacity = max(1e-6, wm_capacity)

    age_val = age[0]
    is_younger = 1.0 if age_val < 45 else 0.0
    is_older = 1.0 - is_younger

    # Base WM weight via sigmoid transform to keep it in (0,1), then age modulation
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    wm_w_base = sigmoid(wm_weight_base)
    wm_w_age = wm_w_base * (1.0 + age_slope * is_younger - age_slope * is_older)
    wm_w_age = float(np.clip(wm_w_age, 0.0, 1.0))

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL and WM value tables (initialized neutral)
        Q = np.zeros((nS, nA))  # RL values
        W = np.zeros((nS, nA))  # WM values (fast, decaying)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Capacity effect: WM reliability downscales with set size
            cap_effect = min(1.0, wm_capacity / float(max(1, nS_t)))
            wm_weight_eff = np.clip(wm_w_age * cap_effect, 0.0, 1.0)

            # Policies
            Q_s = Q[s, :]
            W_s = W[s, :]

            # Softmax for RL and WM (shared beta)
            # subtract max for numerical stability
            rl_pref = beta * (Q_s - np.max(Q_s))
            wm_pref = beta * (W_s - np.max(W_s))

            p_rl = np.exp(rl_pref)
            p_rl /= (np.sum(p_rl) + eps)

            p_wm = np.exp(wm_pref)
            p_wm /= (np.sum(p_wm) + eps)

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            pa = max(p_mix[a], eps)
            nll -= np.log(pa)

            # Learning updates
            # RL delta rule
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM: decay all actions for this state toward 0, then encode current outcome strongly
            W[s, :] *= (1.0 - wm_decay)
            # One-shot storage toward observed outcome for chosen action
            # Map reward 1 -> strong positive, reward 0 -> strong negative relative to others
            # so WM discriminates quickly
            W[s, a] = r  # fast overwrite

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with perseveration bias, set-size dependent exploration, and age-dependent inverse temperature,
    plus a lapse (noise) component.

    The model uses a standard RL value update. Choice policy is softmax over action values with:
    - Perseveration bias: increases the probability of repeating the last action in a state.
    - Age-dependent beta: younger and older groups have different baseline inverse temperatures.
    - Set-size scaling of beta: larger set sizes induce more exploration (lower effective beta).
    - Lapse: with small probability, choices are random.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : 1D array-like of int
        Chosen action on each trial (0..2).
    rewards : 1D array-like of int
        Binary feedback (0/1) on each trial.
    blocks : 1D array-like of int
        Block index on each trial.
    set_sizes : 1D array-like of int
        Set size on each trial (3 or 6).
    age : 1D array-like of int/float
        Participant age; age[0] is used to select the age group.
    model_parameters : list or array-like of 6 floats
        [alpha, beta_young, beta_old, persev_kappa, lapse, size_beta_scale]
        - alpha: RL learning rate in [0,1].
        - beta_young: inverse temperature baseline for younger participants (>0).
        - beta_old: inverse temperature baseline for older participants (>0).
        - persev_kappa: strength of perseveration bias (>=0).
        - lapse: lapse rate in [0,1]; probability of random choice.
        - size_beta_scale: scaling of beta with set size (>=0). Effective beta is:
              beta_eff = beta_group / (1 + size_beta_scale * ((set_size - 3)/3)).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action choices under the model.
    """
    import numpy as np

    alpha, beta_y, beta_o, kappa, lapse, size_beta_scale = model_parameters
    alpha = np.clip(alpha, 0.0, 1.0)
    kappa = max(0.0, kappa)
    lapse = np.clip(lapse, 0.0, 1.0)
    size_beta_scale = max(0.0, size_beta_scale)

    age_val = age[0]
    is_younger = True if age_val < 45 else False
    beta_group = beta_y if is_younger else beta_o
    beta_group = max(1e-6, beta_group) * 10.0  # extend dynamic range

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Perseveration bias memory: for each state, the last chosen action index
        last_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Set-size dependent beta (more exploration at larger set sizes)
            size_factor = 1.0 + size_beta_scale * ((max(3, nS_t) - 3) / 3.0)
            beta_eff = beta_group / size_factor

            # Compute softmax with perseveration bias
            pref = Q[s, :].copy()
            # Add perseveration bias toward last chosen action in this state
            if last_action[s] >= 0:
                pref[last_action[s]] += kappa

            # Stabilize softmax
            logits = beta_eff * (pref - np.max(pref))
            p_soft = np.exp(logits)
            p_soft /= (np.sum(p_soft) + eps)

            # Lapse mixture
            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)
            pa = max(p[a], eps)
            nll -= np.log(pa)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update perseveration memory
            last_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric learning RL with age-contingent learning rates and set-size-dependent forgetting.

    The model uses separate learning rates for positive vs. negative prediction errors, and these
    rates differ by age group. Additionally, there is within-block value decay that increases with
    set size to capture higher cognitive load (forgetting under larger set sizes).

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : 1D array-like of int
        Chosen action on each trial (0..2).
    rewards : 1D array-like of int
        Binary feedback (0/1) on each trial.
    blocks : 1D array-like of int
        Block index on each trial.
    set_sizes : 1D array-like of int
        Set size on each trial (3 or 6).
    age : 1D array-like of int/float
        Participant age; age[0] is used to select the age group.
    model_parameters : list or array-like of 6 floats
        [alpha_gain_y, alpha_loss_y, alpha_gain_o, alpha_loss_o, beta, size_forget]
        - alpha_gain_y: learning rate for positive PEs (younger).
        - alpha_loss_y: learning rate for negative PEs (younger).
        - alpha_gain_o: learning rate for positive PEs (older).
        - alpha_loss_o: learning rate for negative PEs (older).
        - beta: inverse temperature (>0) for softmax choice.
        - size_forget: forgetting/decay factor (>=0). Effective per-trial decay is:
              decay = size_forget * ((set_size - 3)/3), and values are scaled by (1 - decay)
              before the update within each trial.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed action choices under the model.
    """
    import numpy as np

    agy, aly, ago, alo, beta, size_forget = model_parameters
    agy = np.clip(agy, 0.0, 1.0)
    aly = np.clip(aly, 0.0, 1.0)
    ago = np.clip(ago, 0.0, 1.0)
    alo = np.clip(alo, 0.0, 1.0)
    beta = max(1e-6, beta) * 10.0
    size_forget = max(0.0, size_forget)

    age_val = age[0]
    is_younger = True if age_val < 45 else False

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Apply set-size dependent forgetting to the entire state's action values
            decay = size_forget * ((max(3, nS_t) - 3) / 3.0)
            decay = np.clip(decay, 0.0, 1.0)
            Q[s, :] *= (1.0 - decay)

            # Softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p = np.exp(logits)
            p /= (np.sum(p) + eps)
            pa = max(p[a], eps)
            nll -= np.log(pa)

            # Asymmetric learning rate update based on age group and sign of PE
            pe = r - Q[s, a]
            if is_younger:
                alpha = agy if pe >= 0 else aly
            else:
                alpha = ago if pe >= 0 else alo
            Q[s, a] += alpha * pe

    return nll