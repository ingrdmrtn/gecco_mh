def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and decay.
    The model mixes a delta-rule RL system with a one-shot working-memory (WM) system.
    WM contributes more in small set-size (low load) blocks via a capacity gate, and less in large set-size blocks.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline mixture weight of WM relative to RL (0-1).
        softmax_beta : float
            Inverse temperature for RL policy; internally scaled by 10 for range.
        wm_capacity_K : float
            Effective WM item capacity (in "states"); increases WM influence when nS <= K.
        wm_capacity_kappa : float
            Slope of the logistic capacity gating mapping set size -> WM weight scaling.
        wm_decay : float
            WM decay toward uniform on each trial (0-1). Also controls WM learning strength as (1 - wm_decay).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_capacity_K, wm_capacity_kappa, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            # Decay WM for current state toward uniform (interference/forgetting)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-based gating of WM weight as a function of set size
            # gate in [0,1]: ~1 when nS << K, ~0 when nS >> K
            gate = 1.0 / (1.0 + np.exp(wm_capacity_kappa * (nS - wm_capacity_K)))
            eff_wm_weight = np.clip(wm_weight * gate, 0.0, 1.0)

            # Mixture policy
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot learning on rewarded trials; otherwise no specific learning beyond decay
            # Learning strength tied to (1 - wm_decay) to keep parameters bounded
            if r > 0:
                # Move current state's WM distribution toward a one-hot at the chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - (1.0 - wm_decay)) * w[s, :] + (1.0 - wm_decay) * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with set-size-scaled WM influence, asymmetric RL learning, and choice perseveration.
    WM contributes more in small set-size blocks (scaled by 3/nS), and decays over time.
    RL uses asymmetric learning rates for reward vs no-reward via a multiplicative factor eta.
    A perseveration bias adds value to repeating the last action in a state.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            Base RL learning rate for rewarded trials (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), scaled online by 3/nS.
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled by 10.
        eta : float
            Scaling of RL learning rate on non-rewarded trials; lr_neg = lr * eta.
        stickiness : float
            Choice perseveration strength added to the value of the previous action in a state.
        wm_decay : float
            WM decay toward uniform per visit to a state (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, eta, stickiness, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        prev_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Effective RL values with perseveration bias on previous action in this state
            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += stickiness

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with per-visit decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size scaling for WM weight: stronger in small set sizes
            scale = 3.0 / float(nS)  # equals 1 at nS=3, 0.5 at nS=6
            eff_wm_weight = np.clip(wm_weight * scale, 0.0, 1.0)

            # Mixture policy
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning
            lr_t = lr if r > 0 else lr * eta
            delta = r - q[s, a]
            q[s, a] += lr_t * delta

            # WM update: if rewarded, encode chosen action
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Learning strength tied to (1 - wm_decay)
                w[s, :] = (1.0 - (1.0 - wm_decay)) * w[s, :] + (1.0 - wm_decay) * target

            # Update perseveration memory
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM arbitration by uncertainty with load-modulated WM forgetting.
    Arbitration weight favors WM when WM memory is strong and RL policy is uncertain (high entropy).
    WM forgets more under high load (larger set sizes).

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Base scaling of arbitration weight (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax; internally scaled by 10.
        wm_lr : float
            WM learning rate toward one-hot on rewarded trials (0-1).
        wm_forget : float
            Base WM forgetting rate toward uniform per trial (0-1).
        arbitra_beta : float
            Sensitivity of arbitration to WM strength minus RL entropy.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_lr, wm_forget, arbitra_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy and uncertainty
            Q_s = q[s, :]
            # Softmax probabilities for entropy estimate (using same beta as policy)
            denom = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / denom
            # Entropy in [0, log(nA)]
            entropy = -np.sum(pi * (np.log(pi + 1e-12)))
            entropy_norm = entropy / np.log(nA + 1e-12)  # normalize to [0,1]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-modulated forgetting
            # Forgetting increases with set size: effective forget = 1 - (1 - base)^(nS/3)
            eff_forget = 1.0 - (1.0 - wm_forget) ** (nS / 3.0)
            w[s, :] = (1.0 - eff_forget) * w[s, :] + eff_forget * w_0[s, :]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight based on WM strength vs RL entropy
            wm_strength = np.max(W_s) - (1.0 / nA)  # 0 when uniform; increases as memory is peaked
            arb_signal = wm_strength - entropy_norm
            weight = 1.0 / (1.0 + np.exp(-arbitra_beta * arb_signal))
            eff_wm_weight = np.clip(wm_weight_base * weight, 0.0, 1.0)

            # Mixture policy
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: learn rewarded associations
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target

        blocks_log_p += log_p

    return -blocks_log_p