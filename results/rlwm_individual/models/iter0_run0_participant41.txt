def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM capacity-limited mixture with age- and set-size-modulated WM contribution.

    Parameters
    - states: array-like, state index on each trial (0..nS-1 within a block)
    - actions: array-like, chosen action on each trial (0..2; negative values denote omissions/invalid)
    - rewards: array-like, feedback on each trial (0/1; negative values denote missing/invalid)
    - blocks: array-like, block index for each trial
    - set_sizes: array-like, set size (3 or 6) for each trial
    - age: array-like or scalar, participant age (used to assign to older/younger group)
    - model_parameters: list/tuple of parameters:
        alpha: RL learning rate in [0,1]
        beta: inverse temperature for RL policy (scaled internally)
        wm_base: base WM mixture weight (0..1)
        capacity_K: WM capacity (e.g., 1..6), controls set-size effect
        decay: WM decay rate per trial (0..1) toward uniform
        epsilon: lapse rate (0..0.1), adds uniform noise to the final policy

    Returns
    - negative log-likelihood of the observed choices under the model
    """
    alpha, beta, wm_base, capacity_K, decay, epsilon = model_parameters
    # Scale beta to a wider range for sensitivity
    beta = max(1e-6, beta) * 10.0
    epsilon = max(0.0, min(0.2, epsilon))
    age_val = age if np.isscalar(age) else age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0
    unique_blocks = np.unique(blocks)
    nA = 3

    for b in unique_blocks:
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))  # WM policy table (probability-like, row-wise)
        WM_conf = np.zeros(nS)  # confidence/availability of WM for each state (0..1)

        for t in range(len(block_states)):
            a = block_actions[t]
            r = block_rewards[t]
            s = block_states[t]
            ss = block_set_sizes[t]

            # Effective WM weight depends on set size, capacity, and age (older -> reduced WM)
            # Retrieval success: sigmoid-like capacity function 1/(1+ss/K). Older reduces effective K by 30%
            K_eff = max(1e-6, capacity_K * (0.7 if is_older > 0.5 else 1.0))
            wm_retrieval = 1.0 / (1.0 + (ss / K_eff))
            # Older adults also have a direct reduction in WM base contribution
            wm_weight = wm_base * (1.0 - 0.25 * is_older) * wm_retrieval
            wm_weight = max(0.0, min(1.0, wm_weight))

            # RL policy
            Qs = Q[s, :]
            Qs_center = Qs - np.max(Qs)
            p_rl = np.exp(beta * Qs_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: use W row; amplify focused item via a high beta derived from RL beta
            beta_wm = 5.0 * beta
            Ws = W[s, :]
            Ws = np.maximum(Ws, 1e-8)
            Ws = Ws / np.sum(Ws)
            logit_wm = beta_wm * (Ws - np.max(Ws))
            p_wm = np.exp(logit_wm)
            p_wm = p_wm / np.sum(p_wm)

            # Combine with WM availability/confidence for this state
            # Confidence reflects whether we recently stored a strong memory for this state
            wm_mix = wm_weight * (0.3 + 0.7 * WM_conf[s])  # ensure some baseline WM contribution
            wm_mix = max(0.0, min(1.0, wm_mix))

            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl

            # Lapse
            p_final = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            # Only compute likelihood for valid chosen actions
            if a >= 0 and r >= 0:
                pa = max(1e-12, min(1.0, p_final[a]))
                total_logp += np.log(pa)

            # Learning and memory updates (skip if invalid action/reward)
            if a >= 0 and r >= 0:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # WM update:
                # If rewarded, store chosen action strongly; else decay toward uniform
                if r > 0.5:
                    W[s, :] = (1e-6) * np.ones(nA)
                    W[s, a] = 1.0  # focused WM trace for this state
                    WM_conf[s] = 1.0
                else:
                    # Decay both the WM distribution and its confidence
                    W[s, :] = (1.0 - decay) * W[s, :] + decay * (1.0 / nA)
                    WM_conf[s] = (1.0 - decay) * WM_conf[s]
            else:
                # Even on invalid/missed trials, allow passive WM decay to reflect time/age effects
                if 0 <= s < nS:
                    W[s, :] = (1.0 - decay) * W[s, :] + decay * (1.0 / nA)
                    WM_conf[s] = (1.0 - decay) * WM_conf[s]

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with perseveration and age-/set-size-modulated exploration.

    Parameters
    - states: array-like, state index on each trial (0..nS-1 within a block)
    - actions: array-like, chosen action on each trial (0..2; negative values denote omissions/invalid)
    - rewards: array-like, feedback on each trial (0/1; negative values denote missing/invalid)
    - blocks: array-like, block index for each trial
    - set_sizes: array-like, set size (3 or 6) for each trial
    - age: array-like or scalar, participant age (used to assign to older/younger group)
    - model_parameters: list/tuple of parameters:
        alpha_pos: RL learning rate for rewards
        alpha_neg: RL learning rate for non-rewards
        beta: base inverse temperature (scaled internally)
        tau_persev: perseveration strength (>0 increases stickiness)
        age_explore: fraction (0..0.5) that reduces beta for older adults (more exploration)
        epsilon: lapse rate

    Returns
    - negative log-likelihood of the observed choices under the model
    """
    alpha_pos, alpha_neg, beta, tau_persev, age_explore, epsilon = model_parameters
    beta = max(1e-6, beta) * 10.0
    tau_persev = max(0.0, tau_persev)
    age_explore = max(0.0, min(0.7, age_explore))
    epsilon = max(0.0, min(0.2, epsilon))
    age_val = age if np.isscalar(age) else age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Action stickiness term per state (last chosen action indicator)
        last_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = block_actions[t]
            r = block_rewards[t]
            s = block_states[t]
            ss = block_set_sizes[t]

            # Effective beta: reduced by set size (harder blocks more stochastic),
            # and further reduced for older adults by age_explore factor
            beta_eff = beta / (1.0 + 0.5 * (ss - 3))  # ss=3 -> beta, ss=6 -> beta/2
            beta_eff *= (1.0 - age_explore * is_older)
            beta_eff = max(1e-6, beta_eff)

            # Preferences: Q plus perseveration bonus for repeating last action in this state
            prefs = Q[s, :].copy()
            if last_action[s] >= 0:
                prefs[last_action[s]] += tau_persev

            # Softmax
            prefs_center = prefs - np.max(prefs)
            p = np.exp(beta_eff * prefs_center)
            p = p / np.sum(p)

            # Lapse
            p_final = (1.0 - epsilon) * p + epsilon * (1.0 / nA)

            if a >= 0 and r >= 0:
                pa = max(1e-12, min(1.0, p_final[a]))
                total_logp += np.log(pa)

                # Update last action
                last_action[s] = a

                # RL update with asymmetric learning rates
                alpha_use = alpha_pos if r > 0.5 else alpha_neg
                Q[s, a] += alpha_use * (r - Q[s, a])
            else:
                # On invalid trials, do not update Q; keep last_action unchanged
                pass

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controlled RL-WM arbitration: gating depends on set size and age.
    WM is a decaying look-up; RL is model-free. Arbitration is logistic on set size.

    Parameters
    - states: array-like, state index on each trial (0..nS-1 within a block)
    - actions: array-like, chosen action on each trial (0..2; negative values denote omissions/invalid)
    - rewards: array-like, feedback on each trial (0/1; negative values denote missing/invalid)
    - blocks: array-like, block index for each trial
    - set_sizes: array-like, set size (3 or 6) for each trial
    - age: array-like or scalar, participant age (used to assign to older/younger group)
    - model_parameters: list/tuple of parameters:
        alpha: RL learning rate
        beta: inverse temperature for both controllers (scaled internally)
        wm_decay: WM decay toward uniform (0..1)
        gate_bias: intercept of the WM gate (higher -> more WM)
        gate_slope: sensitivity of WM gate to set size (positive -> less WM in larger sets)
        epsilon: lapse rate

    Returns
    - negative log-likelihood of the observed choices under the model
    """
    alpha, beta, wm_decay, gate_bias, gate_slope, epsilon = model_parameters
    beta = max(1e-6, beta) * 10.0
    wm_decay = max(0.0, min(1.0, wm_decay))
    epsilon = max(0.0, min(0.2, epsilon))
    age_val = age if np.isscalar(age) else age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        WM = (1.0 / nA) * np.ones((nS, nA))  # WM policy-like distribution
        WM_strength = np.zeros(nS)  # how confident WM is for the state

        for t in range(len(block_states)):
            a = block_actions[t]
            r = block_rewards[t]
            s = block_states[t]
            ss = block_set_sizes[t]

            # Meta-controller gate: logistic over set size with age-adjusted bias
            # Older adults rely less on WM: reduce bias and increase slope (more sensitive to load)
            bias_eff = gate_bias - 0.5 * is_older
            slope_eff = gate_slope + 0.5 * is_older
            # Normalize set size roughly to [0,1] where 3->0, 6->1
            ss_norm = (ss - 3.0) / 3.0
            gate_input = bias_eff - slope_eff * ss_norm
            wm_gate = 1.0 / (1.0 + np.exp(-gate_input))
            # Gate is further modulated by current WM strength for this state
            wm_gate = wm_gate * (0.3 + 0.7 * WM_strength[s])
            wm_gate = max(0.0, min(1.0, wm_gate))

            # RL policy
            Qs = Q[s, :]
            Qs_center = Qs - np.max(Qs)
            p_rl = np.exp(beta * Qs_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy
            Ws = WM[s, :]
            Ws = np.maximum(Ws, 1e-8)
            Ws = Ws / np.sum(Ws)
            p_wm = np.exp(beta * (Ws - np.max(Ws)))
            p_wm = p_wm / np.sum(p_wm)

            # Arbitration
            p_mix = wm_gate * p_wm + (1.0 - wm_gate) * p_rl

            # Lapse
            p_final = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            if a >= 0 and r >= 0:
                pa = max(1e-12, min(1.0, p_final[a]))
                total_logp += np.log(pa)

                # RL update
                Q[s, a] += alpha * (r - Q[s, a])

                # WM update: reinforce chosen action distribution when rewarded, else decay
                if r > 0.5:
                    WM[s, :] = (1e-6) * np.ones(nA)
                    WM[s, a] = 1.0
                    WM_strength[s] = 1.0
                else:
                    WM[s, :] = (1.0 - wm_decay) * WM[s, :] + wm_decay * (1.0 / nA)
                    WM_strength[s] = (1.0 - wm_decay) * WM_strength[s]
            else:
                # Passive decay on invalid trials
                if 0 <= s < nS:
                    WM[s, :] = (1.0 - wm_decay) * WM[s, :] + wm_decay * (1.0 / nA)
                    WM_strength[s] = (1.0 - wm_decay) * WM_strength[s]

    return -total_logp