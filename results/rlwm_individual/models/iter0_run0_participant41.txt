def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + one-shot Working Memory (WM) with set-size-dependent decay and lapse.

    This model mixes:
    - A standard model-free RL learner (softmax over Q-values, updated with lr).
    - A one-shot WM cache that encodes the last rewarded action per state and decays toward uniform.
      WM efficacy deteriorates under higher set size via stronger decay and higher lapse.

    Parameters
    ----------
    model_parameters : tuple of 5 floats
        lr : RL learning rate in [0,1].
        wm_weight : Mixture weight in [0,1] for WM policy versus RL policy.
        softmax_beta : Inverse temperature for RL policy (scaled internally by 10).
        wm_decay_base : Base decay toward uniform for WM row update in [0,1]. Effective decay is larger when set size is larger.
        wm_lapse_base : Base WM lapse (mixture with uniform) in [0,1]. Effective lapse increases with set size.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_lapse_base = model_parameters

    softmax_beta *= 10.0  # higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM policy
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # baseline uniform (used for decay target)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Set-size-dependent WM modulations:
            # Increase decay and lapse when set size is larger (e.g., 6 vs 3)
            size_factor = (nS - 3) / max(3, nS - 0)  # 0 for nS=3, ~0.5 for nS=6
            size_factor = max(0.0, min(1.0, size_factor))
            wm_decay = wm_decay_base + (1.0 - wm_decay_base) * size_factor
            wm_lapse = wm_lapse_base + (1.0 - wm_lapse_base) * size_factor

            # WM policy (deterministic softmax over WM cache) with lapse to uniform
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(eps, denom_wm)
            p_wm = (1.0 - wm_lapse) * p_wm_det + wm_lapse * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Decay WM row toward uniform (set-size dependent)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) One-shot overwrite on rewarded trials (store chosen action deterministically)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + graded WM delta-learning with capacity-limited precision.

    This model mixes:
    - A model-free RL learner (softmax over Q, learning rate lr).
    - A graded WM value table W updated with a fast WM learning rate (wm_lr),
      and mild forgetting toward uniform. WM readout precision is capacity-limited:
      when set size exceeds capacity K, WM effective precision is diluted.

    Parameters
    ----------
    model_parameters : tuple of 5 floats
        lr : RL learning rate in [0,1].
        wm_weight : Mixture weight in [0,1] for WM policy versus RL policy.
        softmax_beta : Inverse temperature for RL policy (scaled internally by 10).
        wm_lr : WM learning rate in [0,1] for delta rule on W.
        K : WM capacity (effective items), K >= 0. Precision scales with min(1, K / set_size).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, K = model_parameters

    softmax_beta *= 10.0  # higher upper bound
    softmax_beta_wm = 50.0  # deterministic WM readout
    blocks_log_p = 0.0
    eps = 1e-12
    wm_forget = 0.05  # small forgetting toward uniform each time a state is visited

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Capacity-limited WM readout: scale WM precision by phi = min(1, K / set_size)
            nS_block = float(nS)
            phi = min(1.0, max(0.0, K / max(1.0, nS_block)))
            # Blend WM row toward uniform before softmax readout
            W_eff = (1.0 - phi) * w_0[s, :] + phi * W_s

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (graded delta rule) with mild forgetting toward uniform
            # Forgetting on the visited state's row
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            # Delta update on chosen action
            delta_wm = r - W_s[a]
            w[s, a] += wm_lr * delta_wm
            # Keep row normalized (optional but stabilizing)
            # Project to simplex by clipping then renormalizing
            w[s, :] = np.clip(w[s, :], 0.0, None)
            sum_row = np.sum(w[s, :])
            if sum_row > 0:
                w[s, :] /= sum_row
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with error-gated storage, stickiness bias, and load-sensitive noise.

    This model mixes:
    - RL system (softmax over Q with learning rate lr).
    - WM system updated only on rewarded trials (gated encoding), includes:
        * Stickiness: bias to repeat the most recent action for a state.
        * Load-sensitive noise: WM becomes noisier (higher lapse) with larger set size.
      WM policy uses a deterministic softmax over a biased WM table row.

    Parameters
    ----------
    model_parameters : tuple of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight : Mixture weight in [0,1] for WM policy versus RL policy.
        softmax_beta : Inverse temperature for RL policy (scaled internally by 10).
        wm_lr : WM learning rate in [0,1] for strengthening stored action on reward.
        stickiness : Strength >= 0 added to the last chosen action for a state in WM readout.
        wm_lapse_base : Base WM lapse in [0,1]; effective lapse increases with set size.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, stickiness, wm_lapse_base = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12
    wm_forget = 0.1  # modest forgetting toward uniform on each visit

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # track last action per state

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM readout with stickiness bias added to last action in this state
            if last_action[s] >= 0:
                W_s[last_action[s]] += stickiness

            # Load-sensitive lapse: increase with set size
            size_factor = (nS - 3) / max(3, nS - 0)
            size_factor = max(0.0, min(1.0, size_factor))
            wm_lapse = wm_lapse_base + (1.0 - wm_lapse_base) * size_factor

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(eps, denom_wm)
            p_wm = (1.0 - wm_lapse) * p_wm_det + wm_lapse * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # Forget toward uniform on the visited state's row
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Error-gated storage: reinforce chosen action only when rewarded
            if r > 0.5:
                delta_wm = 1.0 - w[s, a]
                w[s, a] += wm_lr * delta_wm
                # Normalize row to keep it a proper distribution
                w[s, :] = np.clip(w[s, :], 0.0, None)
                sum_row = np.sum(w[s, :])
                if sum_row > 0:
                    w[s, :] /= sum_row
                else:
                    w[s, :] = w_0[s, :]

            # Update last action memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p