def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size dependent WM availability.

    Idea:
    - RL: standard delta rule with softmax policy.
    - WM: fast, one-shot learning of rewarded associations with decay toward uniform.
    - Mixture: the effective WM weight scales down with set size via a capacity-limited recall function.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM contribution weight (0..1).
    - softmax_beta: inverse temperature for RL (scaled internally by 10, higher -> more deterministic).
    - wm_decay: decay rate of WM traces toward uniform upon each visit to a state (0..1).
    - wm_boost: amount by which a rewarded WM association is strengthened toward a one-hot code (0..1).
    - K: WM capacity in number of items; modulates how WM availability decreases as set size grows (>0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_boost, K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size dependent WM availability via capacity
            # Effective WM mix = wm_weight * min(1, K / nS)
            wm_avail = wm_weight * min(1.0, max(0.0, K / max(1.0, nS)))

            # Mixture
            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = max(eps, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating: decay toward uniform on visit, then boost chosen if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # move distribution toward a one-hot on the rewarded action
                w[s, :] = (1.0 - wm_boost) * w[s, :]
                w[s, a] += wm_boost

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with time-based WM forgetting and set-size scaled recall.

    Idea:
    - RL: standard delta rule with softmax.
    - WM: rapidly encodes rewarded associations; decays toward uniform per visit.
    - WM availability: decreases with set size and with time since last encounter of that state (lag-based forgetting).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_base: baseline WM contribution weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay: WM decay toward uniform when a state is visited (0..1).
    - K: WM capacity in items (>0), scales with set size.
    - time_decay: strength of lag-based forgetting (>=0). Larger -> faster WM unavailability with lag.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_base, softmax_beta, wm_decay, K, time_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last-visit time for each state to implement lag-based WM availability
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size and lag dependent WM availability
            size_term = min(1.0, max(0.0, K / max(1.0, nS)))  # <= 1
            if last_seen[s] < 0:
                lag = 0  # first time seeing this state; assume maximum availability subject to size
            else:
                lag = max(0, t - last_seen[s])
            lag_term = np.exp(-time_decay * float(lag))
            wm_avail = wm_base * size_term * lag_term
            wm_avail = max(0.0, min(1.0, wm_avail))

            # Mixture
            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = max(eps, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating
            # Decay toward uniform on visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # If rewarded, set a stronger one-shot memory for the correct action
            if r > 0.5:
                # sharpen toward the chosen action
                sharpen = 1.0 - 1e-6  # keep in simplex
                w[s, :] *= (1.0 - sharpen)
                w[s, a] += sharpen

            # Update last seen time
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with adaptive gating by relative confidence and set size.

    Idea:
    - RL: standard delta rule with softmax.
    - WM: fast, decaying associative memory of rewarded pairs.
    - Gating: the control weight allocated to WM is a sigmoid function of:
      (i) WM vs RL confidence (max activation difference), and
      (ii) set-size scaled capacity. This allows dynamic arbitration.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_bias: bias term in the gating sigmoid (can be negative/positive).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay: decay rate of WM traces toward uniform when state is visited (0..1).
    - K: WM capacity in items (>0).
    - gate_gain: slope of the gating sigmoid (>0), higher -> more categorical arbitration.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_bias, softmax_beta, wm_decay, K, gate_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence proxy: spread between best action and average
            conf_rl = np.max(Q_s) - np.mean(Q_s)
            conf_wm = np.max(W_s) - np.mean(W_s)

            # Capacity-adjusted size term centered near 0 when nS ~ 2K
            size_term = (K / max(1.0, nS)) - 0.5  # favors WM when set is small relative to K

            # Gating weight via sigmoid on combined evidence
            gate_input = wm_bias + gate_gain * (conf_wm - conf_rl + size_term)
            wm_avail = sigmoid(gate_input)
            wm_avail = max(0.0, min(1.0, wm_avail))

            # Mixture
            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = max(eps, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating: decay toward uniform and boost on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # push toward one-hot on rewarded action
                strengthen = 1.0 - 1e-6
                w[s, :] *= (1.0 - strengthen)
                w[s, a] += strengthen

        blocks_log_p += log_p

    return -blocks_log_p