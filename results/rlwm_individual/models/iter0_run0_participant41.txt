Below are three standalone cognitive models that blend reinforcement learning (RL) with a working-memory (WM) component. Each function follows the requested interface, uses all parameters meaningfully (â‰¤ 6 per model), computes a mixed policy between RL and WM, updates both value systems, and returns the negative log-likelihood of observed choices. In each model, set size (3 vs 6) modulates WM effectiveness and/or decision noise as described in the docstrings.

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with graded WM capacity and decay.
    - RL: single learning rate with softmax choice.
    - WM: fast delta-rule with decay and capacity-limited weight.
    - Mixture: choice probability is a convex combination of WM and RL policies,
      where the effective WM weight scales with set size via a capacity parameter.

    Parameters (6):
    - lr: RL learning rate in [0,1].
    - wm_weight: baseline WM mixture weight in [0,1] (weight at low load before capacity scaling).
    - softmax_beta: inverse temperature for RL (will be internally scaled; higher -> more deterministic).
    - wm_decay: per-visit decay of WM entries toward the uniform prior (0=no decay, 1=full reset).
    - k_wm: WM capacity (in items); WM weight scales approximately with min(1, k_wm / set_size).
    - wm_lr: WM learning rate in [0,1] for delta-rule updating of WM associations.

    Set size effects:
    - Effective WM weight per trial is wm_weight * min(1, k_wm / nS), so WM contributes more in set size=3 than in set size=6.
    - RL softmax temperature is not directly set-size dependent here (only WM weight is).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, k_wm, wm_lr = model_parameters

    # Scale beta to allow a broad dynamic range, as in the provided template
    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0  # WM retrieval is very deterministic when information is present

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL choice probability for the chosen action a
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # Compute WM choice probability for the chosen action a
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Capacity-limited WM mixture weight
            nS_current = float(block_set_sizes[t])
            cap_scale = min(1.0, max(0.0, k_wm / max(nS_current, 1.0)))
            wm_weight_eff = np.clip(wm_weight * cap_scale, 0.0, 1.0)

            # Mixture policy and log-likelihood
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update (signed prediction error)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay towards uniform prior for this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM delta update on chosen action; normalize to keep a distribution
            delta_w = r - W_s[a]
            w[s, a] += wm_lr * delta_w
            # Project onto simplex-like distribution
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + gated one-shot WM storage.
    - RL: separate learning rates for positive vs negative prediction errors.
    - WM: stores a one-shot association on rewarded trials when a gating mechanism opens.
      The probability of opening the gate decreases with set size.
    - Mixture: convex combination of WM and RL policies with a fixed baseline WM weight.

    Parameters (6):
    - lr_pos: RL learning rate for positive prediction errors in [0,1].
    - lr_neg: RL learning rate for negative prediction errors in [0,1].
    - wm_weight: baseline WM mixture weight in [0,1].
    - softmax_beta: inverse temperature for RL (internally scaled by 10).
    - gate_base: baseline WM gating bias (logit scale); higher -> more likely to store.
    - gate_size: sensitivity of gating to set size (positive -> less storage as set size increases).

    Set size effects:
    - WM storage gating probability p_gate = sigmoid(gate_base - gate_size*(nS-3)),
      hence larger sets yield fewer stored items.
    - RL temperature is not directly modulated by set size here; set size acts via WM gating.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gate_base, gate_size = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_current = float(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with fixed baseline weight
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM gating: probability of storing (only on rewarded trials)
            p_gate = sigmoid(gate_base - gate_size * (nS_current - 3.0))

            # Apply a small state-visit decay toward uniform to avoid stale WM when not storing
            # The decay magnitude is tied to (1 - p_gate): when gate is unlikely, WM decays more.
            decay = np.clip(1.0 - p_gate, 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # One-shot storage if rewarded and gate opens
            if r > 0.5:
                if np.random.rand() < p_gate:
                    w[s, :] = w_0[s, :].copy()
                    w[s, a] = 1.0  # store chosen action deterministically
                else:
                    # no store; keep decayed WM as above
                    pass
            # Ensure numerical stability
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with temperature scaling by set size + WM as last-rewarded association with noise.
    - RL: single learning rate; softmax inverse temperature decreases with set size (more noise at high load).
    - WM: maintains the last rewarded action per state (via delta update), and produces a near-deterministic policy mixed with a WM lapse/noise.
    - Mixture: WM and RL policies combined with a baseline WM weight.

    Parameters (5):
    - lr: RL learning rate in [0,1].
    - wm_weight: baseline WM mixture weight in [0,1].
    - softmax_beta: base inverse temperature (internally scaled by 10).
    - wm_noise: WM lapse probability in [0,1]; with wm_noise>0, WM policy is blended with uniform.
    - beta_size: sensitivity of RL temperature to set size (beta_eff = beta * (3/nS)^beta_size).

    Set size effects:
    - RL inverse temperature scales as beta_eff = softmax_beta*10 * (3 / nS)^beta_size,
      so choices are noisier for larger nS when beta_size > 0.
    - WM weight is not directly capacity scaled here, but WM's effectiveness is tempered by wm_noise.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_noise, beta_size = model_parameters

    softmax_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_current = float(block_set_sizes[t])

            # RL temperature scales with set size
            beta_eff = softmax_beta * (3.0 / max(nS_current, 1.0)) ** beta_size

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over W with near-determinism, blended with lapse
            W_s = w[s, :]
            denom_wm_det = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_det = 1.0 / max(denom_wm_det, 1e-12)
            p_wm = (1.0 - wm_noise) * p_wm_det + wm_noise * (1.0 / nA)

            # Mixture
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: store last rewarded action with delta rule; if no reward, weakly reinforce uniform
            if r > 0.5:
                # move toward one-hot on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                # strong update toward target
                w[s, :] = 0.8 * w[s, :] + 0.2 * target
            else:
                # mild decay toward uniform on non-reward
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p