def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying working-memory mixture model with age- and load-dependent WM weighting.
    
    The model mixes a standard model-free RL controller with a simple working-memory (WM)
    store that remembers the last rewarded action per state, with decay toward uniform.
    The weight on WM decreases as set size exceeds capacity and with older age.
    A small lapse probability accounts for random choices and invalid trials.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block (0..set_size-1).
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2). Invalid actions are handled via lapse.
    rewards : array-like of int
        Binary reward feedback (0 or 1). Invalid rewards (<0) trigger lapse handling.
    blocks : array-like of int
        Block index for each trial. Values group trials into independent blocks.
    set_sizes : array-like of int
        The set size for each trial (3 or 6 here). First value within a block defines nS.
    age : array-like of float
        Participant age (single-element array). Used to adjust WM effectiveness (older -> lower).
    model_parameters : sequence of floats
        Five parameters:
          - alpha: RL learning rate (0..1)
          - beta: inverse temperature for RL softmax (scaled internally by 10)
          - wm_capacity: effective WM capacity in number of state-action mappings
          - wm_decay: per-trial decay of WM towards uniform (0..1)
          - lapse: probability of random responding (0..1)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_capacity, wm_decay, lapse = model_parameters
    beta = 10.0 * beta  # increase dynamic range
    alpha = np.clip(alpha, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 1.0)
    age_val = float(age[0])

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        # Initialize RL Q-values and WM store (probabilities per state)
        Q = np.zeros((nS, nA))
        W = np.ones((nS, nA)) / nA  # WM distribution over actions per state

        # Age-adjusted WM capacity reduces with age > 45
        # Map age effect to approx [0, ~1] reduction around older adulthood
        age_penalty = max(0.0, (age_val - 45.0) / 30.0)  # 0 at 45 or below, ~1 at 75
        k_eff = max(0.0, wm_capacity - age_penalty)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]
            ss = int(b_set_sizes[t])

            # If invalid action or reward, count as pure lapse and skip learning
            invalid = (a < 0 or a >= nA or r < 0)
            if invalid:
                nll -= np.log(1.0 / nA + eps)
                # Decay WM even if trial invalid (passage of time)
                W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)
                continue

            # RL softmax policy
            q_s = Q[s, :]
            z = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(z)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM policy: use current WM distribution for that state
            p_wm_vec = W[s, :]
            p_wm = p_wm_vec[a]

            # Mixture weight depends on effective capacity, set size, and age
            # Heuristic: weight ~ min(1, k_eff / set_size)
            wm_weight = np.clip(k_eff / max(1, ss), 0.0, 1.0)

            # Combine with lapse
            p_choice = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_final = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            nll -= np.log(np.clip(p_final, eps, 1.0))

            # Learning updates
            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM decay towards uniform
            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)
            # If reward received, commit perfect memory for that state
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL model with asymmetric learning rates and age- and load-modulated perseveration (stickiness).
    
    The model uses separate learning rates for positive and negative outcomes and a
    perseveration bias favoring repeating the previous action in the same state.
    Stickiness increases with set size and with age in the older group.
    A lapse probability accounts for random choices and invalid trials.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial within a block.
    actions : array-like of int
        Chosen action on each trial (expected 0,1,2). Invalid actions handled by lapse.
    rewards : array-like of int
        Reward feedback (0 or 1). Invalid rewards (<0) trigger lapse handling.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6).
    age : array-like of float
        Participant age (single-element array). Older age increases stickiness.
    model_parameters : sequence of floats
        Six parameters:
          - alpha_pos: learning rate for rewards (0..1)
          - alpha_neg: learning rate for non-rewards (0..1)
          - beta: inverse temperature for RL softmax (scaled by 10)
          - stick_base: baseline stickiness weight
          - age_stick_slope: incremental stickiness per older-age unit
          - lapse: probability of random responding
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, stick_base, age_stick_slope, lapse = model_parameters
    beta = 10.0 * beta
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 1.0)
    age_val = float(age[0])
    older = 1.0 if age_val >= 45.0 else 0.0
    # Age factor: normalized years over 45 scaled to ~[0,1] across older adulthood
    age_over = max(0.0, (age_val - 45.0) / 30.0)

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        prev_action = -np.ones(nS, dtype=int)  # last chosen action per state

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]
            ss = int(b_set_sizes[t])

            invalid = (a < 0 or a >= nA or r < 0)
            if invalid:
                nll -= np.log(1.0 / nA + eps)
                continue

            # Stickiness intensity increases with set size and age in older group
            # kappa = base + slope * age_over (only if older), then scaled by load (ss/6)
            kappa = (stick_base + age_stick_slope * age_over * older) * (ss / 6.0)

            # Preference vector = beta * Q + kappa * I(a == prev_action[s])
            prefs = beta * (Q[s, :] - np.max(Q[s, :]))
            if prev_action[s] >= 0:
                stick_vec = np.zeros(nA)
                stick_vec[prev_action[s]] = 1.0
                prefs += kappa * stick_vec

            p_vec = np.exp(prefs)
            p_vec /= np.sum(p_vec)
            p = p_vec[a]
            p_final = (1.0 - lapse) * p + lapse * (1.0 / nA)
            nll -= np.log(np.clip(p_final, eps, 1.0))

            # RL update with asymmetric learning rates
            alpha = alpha_pos if r > 0.5 else alpha_neg
            Q[s, a] += alpha * (r - Q[s, a])

            # Update previous action for this state
            prev_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited gating WM: WM stores up to K rewarded state-action pairs,
    with age-reduced capacity and mixture with RL policy. WM retrieval is noisy-softmax.
    
    The WM system stores mappings only after rewarded trials and retains at most K states
    per block (first-in-first-out if exceeded). Older age reduces effective K. When a
    state is in WM, a high-precision WM softmax favors the stored action; otherwise WM
    is uniform. The overall policy mixes WM and RL with weight proportional to K/set_size.
    A lapse probability handles random choices and invalid trials.
    
    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions handled via lapse.
    rewards : array-like of int
        Binary reward feedback; invalid (<0) triggers lapse handling.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6). Determines mixture weight with capacity.
    age : array-like of float
        Participant age; reduces WM capacity if older.
    model_parameters : sequence of floats
        Six parameters:
          - alpha: RL learning rate (0..1)
          - beta: RL inverse temperature (scaled by 10)
          - K_base: baseline WM capacity in number of states
          - age_capacity_slope: capacity reduction per normalized older-age unit
          - beta_wm: inverse temperature for WM retrieval (high -> near-deterministic)
          - lapse: probability of random responding
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, K_base, age_capacity_slope, beta_wm, lapse = model_parameters
    beta = 10.0 * beta
    beta_wm = 10.0 * beta_wm
    alpha = np.clip(alpha, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 1.0)
    age_val = float(age[0])
    # Older-age normalized factor
    age_over = max(0.0, (age_val - 45.0) / 30.0)

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx].astype(int)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx].astype(float)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))

        # WM store: mapping state -> action, with FIFO order
        wm_map = dict()
        wm_order = []  # list of states in insertion order

        # Effective capacity, age-reduced
        K_eff = max(0.0, K_base - age_capacity_slope * age_over * K_base)
        # Use integer capacity for storage
        K_store = int(np.floor(K_eff + 1e-9))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]
            ss = int(b_set_sizes[t])

            invalid = (a < 0 or a >= nA or r < 0)
            if invalid:
                nll -= np.log(1.0 / nA + eps)
                continue

            # RL policy
            q_s = Q[s, :]
            z_rl = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(z_rl)
            p_rl_vec /= np.sum(p_rl_vec)

            # WM policy
            if s in wm_map:
                # Create logits that favor stored action
                logits = np.zeros(nA)
                logits[wm_map[s]] = 1.0
                z_wm = beta_wm * (logits - np.max(logits))
                p_wm_vec = np.exp(z_wm)
                p_wm_vec /= np.sum(p_wm_vec)
            else:
                p_wm_vec = np.ones(nA) / nA

            # Mixture weight proportional to coverage K/set_size
            wm_weight = 0.0
            if ss > 0:
                wm_weight = np.clip((K_eff / ss), 0.0, 1.0)

            p_choice = wm_weight * p_wm_vec[a] + (1.0 - wm_weight) * p_rl_vec[a]
            p_final = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            nll -= np.log(np.clip(p_final, eps, 1.0))

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM gating: store after reward, up to capacity
            if r > 0.5 and K_store > 0:
                if s in wm_map:
                    wm_map[s] = a  # update stored action
                    # move s to end of order (most recent)
                    if s in wm_order:
                        wm_order.remove(s)
                    wm_order.append(s)
                else:
                    # Need capacity; evict if full
                    if len(wm_order) >= K_store:
                        evict = wm_order.pop(0)
                        wm_map.pop(evict, None)
                    wm_map[s] = a
                    wm_order.append(s)

    return nll