def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and one-shot WM encoding.
    
    The model mixes a reinforcement-learning (RL) policy and a working-memory (WM) policy.
    WM contribution is reduced as set size increases via a capacity parameter K.
    WM values undergo decay toward a uniform prior each time the state is visited and
    learn in a one-shot manner when feedback is positive.

    Parameters (in order):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Baseline mixture weight of WM policy (0..1), applied then scaled by capacity.
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally, >0).
    - wm_learn: One-shot WM learning rate when rewarded (0..1).
    - wm_forget: WM forgetting rate toward uniform on each visit (0..1).
    - K: WM capacity (number of items effectively supported). Scales WM weight as min(1, K/nS).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_learn, wm_forget, K = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables per block
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM weight (fixed within block)
        cap_scale = min(1.0, max(0.0, K / max(1, nS)))
        wm_weight_eff = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM one-shot learning when rewarded
            if r > 0.0:
                # Move probability mass toward the chosen action
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

            # Ensure valid probabilities
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-dependent WM noise (interference) and lapse.
    
    The model mixes an RL policy and a WM policy. WM representations are corrupted
    by interference that increases with set size, modeled as a blending toward the
    uniform distribution with strength alpha = 1 - exp(-phi / nS).
    Additionally, there is an action-independent lapse epsilon.

    Parameters (in order):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Mixture weight for WM policy (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally, >0).
    - wm_forget: WM forgetting rate toward uniform on each visit (0..1).
    - phi: Interference gain controlling set-size WM noise (>0). Higher -> more noise at larger nS.
    - epsilon: Lapse probability of random choice (0..1).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_forget, phi, epsilon = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    epsilon = np.clip(epsilon, 0.0, 0.5)  # keep reasonable
    wm_weight = np.clip(wm_weight, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM interference strength
        alpha = 1.0 - np.exp(-max(1e-12, phi) / max(1, nS))  # in [0,1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Interference-corrupted WM weights
            W_s_noisy = (1.0 - alpha) * W_s + alpha * (1.0 / nA)

            # WM policy probability for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))

            # Mixture then lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # One-shot WM learning when rewarded
            if r > 0.0:
                w[s, :] *= 0.0
                w[s, a] = 1.0
            else:
                # no explicit negative learning; rely on decay + interference
                pass

            # Normalize and clip for stability
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM arbitration based on trial-wise WM reliability: time-based decay and set-size load.
    
    The model mixes RL and WM policies with a dynamic WM weight per trial and state.
    WM reliability decreases with:
      - time since the last rewarded association for that state (exponential with tau),
      - set size via a load parameter kappa.
    WM encoding is incremental (wm_learn) with a small decay derived from tau.
    
    Parameters (in order):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Baseline WM weight scaling the dynamic reliability (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally, >0).
    - tau: Time constant controlling WM reliability decay across visits (>0).
    - kappa: Load sensitivity scaling down WM reliability with set size (>=0).
    - wm_learn: WM learning rate on rewarded trials (0..1).
    
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, tau, kappa, wm_learn = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    tau = max(1e-6, tau)
    kappa = max(0.0, kappa)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)

    # Per-visit decay for WM content (derived from tau)
    # Decay factor toward uniform on each visit
    wm_forget = 1.0 - np.exp(-1.0 / tau)  # in (0,1)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last trial index of a rewarded mapping for each state
        last_rew_time = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute WM reliability for this state and trial
            if last_rew_time[s] < 0:
                dt = np.inf  # no prior reward for this state
                time_rel = 0.0
            else:
                dt = max(0, t - last_rew_time[s])
                time_rel = np.exp(-dt / tau)

            load_rel = 1.0 / (1.0 + kappa * max(0, nS - 1))
            wm_rel = time_rel * load_rel
            wm_weight_eff = np.clip(wm_weight * wm_rel, 0.0, 1.0)

            # WM policy probability for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for visited state
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM learning on reward
            if r > 0.0:
                # Incrementally move probability mass toward chosen action
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
                last_rew_time[s] = t  # update time of last confirmed mapping

            # Normalize and clip
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p