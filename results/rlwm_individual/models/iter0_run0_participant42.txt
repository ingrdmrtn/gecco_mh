Below are three standalone cognitive models that follow the requested template and return the negative log-likelihood of the observed choices. Each model mixes reinforcement learning (RL) with a working-memory (WM) policy and uses all parameters meaningfully. They differ in how WM is represented and how set size modulates WM influence.

Note: Assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and decay.

    Idea:
    - RL learns Q-values per state-action via a standard delta rule.
    - WM stores high-confidence action values quickly but decays over time.
    - WM weight is reduced in larger set sizes according to a capacity K (i.e., limited slots).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight (0..1) before capacity scaling.
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10).
    - K: WM capacity (in number of items; scales WM contribution as min(1, K/set_size)).
    - wm_decay: WM decay (0..1); higher means faster decay towards baseline within a block.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12

    blocks_log_p = 0.0
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight modulated by set size and capacity
        cap_factor = min(1.0, max(0.0, K / max(1, nS)))
        wm_weight_eff = np.clip(wm_weight * cap_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (near-deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Decay towards baseline for all actions in state s
            # - If rewarded, boost chosen action towards 1 (one-shot learning tendency)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                # Nudge chosen action up, then renormalize softly via softmax policy shape
                w[s, a] = w[s, a] + wm_decay * 1.0
                # Keep values bounded; no need to renormalize for softmax policy, but keep stability
                w[s, :] = np.maximum(w[s, :], 0.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM as one-shot store with set-size-dependent decay/interference.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM stores the last rewarded action per state strongly (one-shot), but WM decays faster
      when set size is larger (interference).
    - WM mixture weight is fixed but the WM trace itself weakens with set size.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - wm_weight: Mixture weight of WM (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10).
    - wm_decay_base: Base WM decay per trial (0..1).
    - interference: Additional decay factor per unit set size (>=0), increasing decay with larger sets.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay_base, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay: larger sets â†’ stronger decay
        wm_decay = np.clip(wm_decay_base + interference * max(0, nS - 1), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update:
            # - Strong decay towards baseline for the state (interference)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # - If reward, store the chosen action strongly (one-shot memory)
            if r > 0.5:
                # Pull WM for state s towards one-hot on action a
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] = w[s, a] + wm_decay * 1.0
                w[s, :] = np.maximum(w[s, :], 0.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with perseveration bias + WM as a recency-weighted Dirichlet memory with forgetting.

    Idea:
    - RL updates standard Q-values.
    - Add a perseveration bias to the RL policy for repeating the last action in a state.
    - WM is a recency-weighted count (Dirichlet-like) that forgets at rate phi each trial.
      It captures recent successful actions and is less reliable in larger sets due to lower
      relative counts.
    - WM and RL are mixed via a set-size-modulated WM weight (so that WM contributes less in larger sets).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM weight (0..1), scaled by 1/sqrt(set_size).
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10).
    - phi: WM forgetting rate (0..1): higher means faster forgetting per trial.
    - kappa: Perseveration bias added to the chosen action from last visit to the state (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, phi, kappa = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = np.ones((nS, nA))  # Dirichlet-like counts start uniform
        w_0 = np.ones((nS, nA))  # baseline counts
        last_action = -np.ones(nS, dtype=int)  # track last action per state, -1 if none

        # Set-size modulation of WM weight (e.g., 1/sqrt(set_size))
        wm_scale = 1.0 / max(1.0, np.sqrt(float(nS)))
        wm_weight_eff = np.clip(wm_weight * wm_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bias to RL action values for last action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] = Q_s[last_action[s]] + kappa

            # RL policy with perseveration-adjusted Q
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: use softmax over normalized WM counts
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update (Dirichlet with forgetting):
            # - Forget counts towards baseline
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]
            # - If rewarded, increment count for chosen action (recent success boosts WM)
            if r > 0.5:
                w[s, a] = w[s, a] + 1.0

            # Track last action in this state for perseveration on next visit
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p