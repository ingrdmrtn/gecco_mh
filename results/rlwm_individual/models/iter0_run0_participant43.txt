Below are three standalone cognitive models that implement RL+WM mechanisms for the rlwm task. Each function:

- Matches the required signature: (states, actions, rewards, blocks, set_sizes, model_parameters).
- Returns the negative log-likelihood of the observed choices.
- Uses all parameters meaningfully (≤ 6 per model).
- Fills in the missing policy and update equations for the working memory (WM) system.
- Allows WM influence to depend on set size (cognitive load).

Note: Assumes numpy as np is already imported.


def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with capacity-limited WM weight, fast WM updating, and WM decay.
    
    Idea:
    - Choices are a mixture of a model-free RL policy and a near-deterministic WM policy.
    - WM influence is scaled down under higher set size via a capacity parameter K.
      Effective WM weight per block: wm_weight_eff = wm_weight * min(1, K / nS).
    - WM updates quickly (wm_lr) and decays within-state toward a neutral prior (wm_decay) when visited.
    
    Parameters (tuple):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base mixture weight for WM (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled up internally by x10).
    - wm_lr: WM learning rate for updating the chosen action’s value based on immediate outcome (0..1).
    - wm_decay: Decay toward prior for the visited state’s WM row (0..1).
    - K: WM capacity parameter that scales WM contribution by min(1, K/nS) (>=0).
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-adjusted WM weight for this block
        cap_scale = min(1.0, max(0.0, K) / max(1.0, float(nS)))
        wm_w_block = wm_weight * cap_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (given by template trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values with high inverse temperature
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            p_total = max(p_total, 1e-12)  # numerical stability
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update for the visited state:
            # 1) decay the whole row toward prior
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) update chosen action toward immediate outcome r
            w[s, a] = (1.0 - wm_lr) * w[s, a] + wm_lr * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with global WM leak and load-dependent WM precision.
    
    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM precision (inverse temperature) drops with higher set size: 
      beta_wm_eff = softmax_beta_wm / (1 + eta * (nS - 3)), eta >= 0
    - WM table undergoes a global leak each trial toward a neutral prior (wm_leak), 
      modeling time-based decay/interference in high-load blocks.
    - WM updates quickly with wm_lr on the visited state.
    
    Parameters (tuple):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base mixture weight for WM (0..1).
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - wm_lr: WM learning rate (0..1).
    - wm_leak: Global WM leak per trial toward prior (0..1).
    - eta: Load-sensitivity of WM precision; larger eta reduces WM precision more as set size increases (>=0).
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_leak, eta = model_parameters
    softmax_beta *= 10.0
    base_softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision
        denom = 1.0 + max(0.0, eta) * max(0.0, float(nS) - 3.0)
        softmax_beta_wm_eff = base_softmax_beta_wm / denom

        log_p = 0.0
        for t in range(len(block_states)):
            # Apply global WM leak (time-based decay) before choice
            w = (1.0 - wm_leak) * w + wm_leak * w_0

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with reduced precision under higher load
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (fast overwrite toward observed outcome)
            w[s, a] = (1.0 - wm_lr) * w[s, a] + wm_lr * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + gated WM based on reward and capacity, with lapse.
    
    Idea:
    - WM stores strong associations only after rewarded outcomes (gated by reward).
      If rewarded, WM for that state becomes a near one-hot preference for the chosen action.
      If not rewarded, WM decays toward neutral.
    - WM contribution is state-specific: higher when the state has a strong WM trace.
      We use the strength of the WM row (its max value) to gate WM utilization dynamically.
    - WM influence also scales with capacity under load: min(1, K / nS).
    - A small lapse epsilon mixes in uniform choice probability.
    
    Parameters (tuple):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - K: WM capacity parameter; scales WM influence by min(1, K / nS) (>=0).
    - decay_f: WM decay per visit toward neutral when unrewarded (0..1).
    - epsilon: Lapse probability mixing in uniform random choice (0..1).
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K, decay_f, epsilon = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    epsilon = min(max(epsilon, 0.0), 1.0)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        cap_scale = min(1.0, max(0.0, K) / max(1.0, float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # State-specific WM gate based on WM trace strength (max value in WM row)
            wm_strength_s = float(np.max(W_s))  # in [1/3, 1]
            wm_gate_s = wm_weight * cap_scale * wm_strength_s

            # Mixture with state-specific WM gate
            p_mix = wm_gate_s * p_wm + (1.0 - wm_gate_s) * p_rl

            # Add lapse (uniform random) for robustness/noise
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM gated storage/update:
            if r >= 0.5:
                # Rewarded: store a strong one-hot-like trace for chosen action
                # Move row toward one-hot on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]  # mild reset before strong imprint
                w[s, :] = 0.0 * w[s, :] + target  # overwrite to strong trace
            else:
                # Unrewarded: decay WM toward neutral prior for this state
                w[s, :] = (1.0 - decay_f) * w[s, :] + decay_f * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p