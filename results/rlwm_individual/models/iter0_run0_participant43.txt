def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working-memory (WM) mixture model with age-dependent WM effectiveness.

    Idea:
    - Choices are a mixture of a slow RL system and a fast WM system that stores rewarded S->A pairs.
    - WM has limited capacity, degrades with set size, and decays over time; older adults have lower
      effective WM contribution and faster decay (consistent with age-related WM declines).
    - RL updates with a single learning rate.
    - Action selection via softmax; final policy mixes RL and WM with a lapse.

    Parameters (model_parameters):
    - alpha: scalar in (0,1], RL learning rate
    - beta: inverse temperature (>0) for both RL and WM softmax
    - wm_weight_base: base WM mixture weight before adjustments
    - wm_decay: WM decay rate per trial (0..1), higher => faster WM forgetting
    - capacity_slope: how strongly larger set sizes reduce WM reliance
    - lapse: action-independent lapse rate (0..1)

    Inputs:
    - states: np.array of state indices per trial (int)
    - actions: np.array of chosen actions per trial (0..2)
    - rewards: np.array of rewards per trial (0/1)
    - blocks: np.array of block indices per trial
    - set_sizes: np.array of set size per trial (3 or 6)
    - age: np.array with single entry (participant age in years)
    - model_parameters: list/tuple of 6 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_weight_base, wm_decay, capacity_slope, lapse = model_parameters
    # Avoid degenerate values
    beta = max(1e-6, beta) * 5.0  # moderate scaling for identifiability
    lapse = min(max(lapse, 1e-8), 0.25)  # cap lapse
    alpha = min(max(alpha, 1e-6), 1.0)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    capacity_slope = max(0.0, capacity_slope)

    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_states = states[idx]
        b_setsizes = set_sizes[idx]

        nA = 3
        nS = int(b_setsizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA))  # start neutral
        # WM value table: starts flat. W[s,a] behaves like a cached policy for state s
        W = np.ones((nS, nA)) / nA

        # Age-dependent adjustments
        # Older adults: lower effective WM weight and faster decay.
        # Implement by scaling capacity penalty and decay for older adults.
        capacity_penalty_factor = 1.0 + 0.5 * is_older   # 50% stronger penalty to WM in older
        decay_factor = 1.0 + 0.5 * is_older             # 50% faster WM decay in older

        # Compute WM mixture weight once per block as a function of set size
        # Map wm_weight_base -> sigmoid to keep in (0,1)
        base = 1.0 / (1.0 + np.exp(-wm_weight_base))
        # Capacity penalty increases from set size 3 to 6
        cap_penalty = capacity_slope * capacity_penalty_factor * max(0, nS - 3) / 3.0
        wm_w_block = base * (1.0 - cap_penalty)
        wm_w_block = min(max(wm_w_block, 0.0), 1.0)

        # Effective decay per trial
        wm_decay_eff = 1.0 - (1.0 - wm_decay) / decay_factor
        wm_decay_eff = min(max(wm_decay_eff, 0.0), 1.0)

        prev_state = None
        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]

            # Compute RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            exp_rl = np.exp(logits_rl)
            p_rl = exp_rl / np.sum(exp_rl)

            # Compute WM policy: softmax over W row
            logits_wm = beta * (W[s, :] - np.max(W[s, :]))
            exp_wm = np.exp(logits_wm)
            p_wm = exp_wm / np.sum(exp_wm)

            # Mixture with lapse
            p_mix = wm_w_block * p_wm + (1.0 - wm_w_block) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Accumulate NLL
            p_choice = max(1e-12, p_final[a])
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay on the current state row (local WM maintenance)
            W[s, :] = (1.0 - wm_decay_eff) * W[s, :] + wm_decay_eff * (np.ones(nA) / nA)

            # WM storage on rewarded trials: move mass toward the rewarded action
            # This implements a fast "binding" of the rewarded action for this state.
            if r > 0.5:
                # Make a peaked distribution toward the chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                # Blend current WM row with target; stronger push when reward obtained
                # Use the same alpha to modulate insertion strength for parsimony
                W[s, :] = (1.0 - alpha) * W[s, :] + alpha * target

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with perseveration and age-modulated exploration.

    Idea:
    - Separate learning rates for positive vs. negative feedback (older adults often show altered
      sensitivity to negative outcomes).
    - Choice bias toward repeating the last action taken in a state ("state-wise perseveration"),
      stronger in older adults.
    - Exploration temperature increases with set size, and older adults are more exploratory
      (lower beta) via an age-dependent factor.
    - Lapse for robustness.

    Parameters (model_parameters):
    - alpha_pos: learning rate for r=1
    - alpha_neg: learning rate for r=0
    - beta_base: base inverse temperature (>0)
    - perseveration: bias weight added to the last action in a state
    - age_explore_bias: scales how much older age reduces beta (>=0)
    - lapse: action-independent lapse rate

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: tuple/list of 6 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_base, perseveration, age_explore_bias, lapse = model_parameters

    # Clamp parameters
    alpha_pos = min(max(alpha_pos, 1e-6), 1.0)
    alpha_neg = min(max(alpha_neg, 1e-6), 1.0)
    beta_base = max(beta_base, 1e-6) * 5.0
    perseveration = max(0.0, perseveration)
    age_explore_bias = max(0.0, age_explore_bias)
    lapse = min(max(lapse, 1e-8), 0.25)

    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_states = states[idx]
        b_setsizes = set_sizes[idx]

        nA = 3
        nS = int(b_setsizes[0])
        Q = np.zeros((nS, nA))
        # Track last action taken in each state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Age-modulated beta: older => lower beta_exp
        beta_age = beta_base / (1.0 + age_explore_bias * is_older)
        # Set size effect on exploration: larger set => lower beta
        beta_block = beta_age / (1.0 + 0.5 * max(0, nS - 3))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]

            # Build perseveration bias vector for this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                # Older adults show stronger perseveration
                persev_eff = perseveration * (1.0 + 0.5 * is_older)
                bias[last_action[s]] += persev_eff

            logits = beta_block * (Q[s, :] - np.max(Q[s, :])) + bias
            exp_logits = np.exp(logits - np.max(logits))
            p = exp_logits / np.sum(exp_logits)
            p_final = (1.0 - lapse) * p + lapse * (1.0 / nA)

            p_choice = max(1e-12, p_final[a])
            nll -= np.log(p_choice)

            # Update Q with asymmetric learning rates
            pe = r - Q[s, a]
            lr = alpha_pos if r > 0.5 else alpha_neg
            Q[s, a] += lr * pe

            # Update perseveration memory
            last_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Leaky RL with set-size-sensitive forgetting and global stickiness, age-dependent forgetting.

    Idea:
    - Q-values are subject to leaky forgetting (toward zero) that grows with set size (higher load).
    - Older adults exhibit stronger forgetting (larger leak), which reduces the stability of learned
      values, especially under high load.
    - Global stickiness (choice inertia) biases toward repeating the immediately previous action
      irrespective of state (reflects motor/response habits).
    - Standard RL learning with a single learning rate; softmax choice with lapse.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: inverse temperature (>0)
    - forgetting: baseline leak per trial (0..1)
    - setsize_sensitivity: scales how much forgetting increases from 3->6 set size (>=0)
    - stickiness: weight for global choice inertia (>=0)
    - lapse: action-independent lapse (0..1)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays per trial
    - model_parameters: tuple/list of 6 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, forgetting, setsize_sensitivity, stickiness, lapse = model_parameters

    alpha = min(max(alpha, 1e-6), 1.0)
    beta = max(beta, 1e-6) * 5.0
    forgetting = min(max(forgetting, 0.0), 1.0)
    setsize_sensitivity = max(0.0, setsize_sensitivity)
    stickiness = max(0.0, stickiness)
    lapse = min(max(lapse, 1e-8), 0.25)

    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_states = states[idx]
        b_setsizes = set_sizes[idx]

        nA = 3
        nS = int(b_setsizes[0])

        Q = np.zeros((nS, nA))
        # Global stickiness implemented as a last-action indicator across trials
        last_global_action = -1

        # Compute effective forgetting for this block:
        # increases with set size and more so for older adults.
        setsize_factor = 1.0 + setsize_sensitivity * max(0, nS - 3) / 3.0
        age_forgetting_factor = 1.0 + 0.5 * is_older
        forget_eff = 1.0 - (1.0 - forgetting) / (setsize_factor * age_forgetting_factor)
        forget_eff = min(max(forget_eff, 0.0), 1.0)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = b_rewards[t]

            # Apply leak to the active state's action values (working memory limits affect maintained values)
            Q[s, :] *= (1.0 - forget_eff)

            # Build global stickiness bias
            bias = np.zeros(nA)
            if last_global_action >= 0:
                bias[last_global_action] += stickiness * (1.0 + 0.25 * is_older)

            logits = beta * (Q[s, :] - np.max(Q[s, :])) + bias
            exp_logits = np.exp(logits - np.max(logits))
            p = exp_logits / np.sum(exp_logits)
            p_final = (1.0 - lapse) * p + lapse * (1.0 / nA)

            p_choice = max(1e-12, p_final[a])
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update global stickiness memory
            last_global_action = a

    return nll