Below are three standalone cognitive models that implement different ways in which working memory (WM) and reinforcement learning (RL) might jointly drive choices under different set sizes (low vs high cognitive load). Each model follows the provided template and returns the negative log-likelihood of the observed actions.

Note: Assume numpy as np is already imported.

----------------
def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with capacity scaling and decay.

    Idea:
    - RL learns Q-values with a standard delta rule.
    - WM stores the last rewarded response for each state (one-shot), but memory decays toward uniform.
    - WM influence is reduced when set size exceeds a capacity parameter.
    
    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline mixture weight for WM (0..1)
    - softmax_beta: inverse temperature for RL policy (positive); internally scaled by 10
    - wm_decay: per-trial decay of WM map toward uniform (0..1)
    - capacity_k: WM capacity in number of items; effective WM weight is scaled by min(1, capacity_k / set_size)
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity_k = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM when a memory is present

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM representations
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight for this block (capacity scaled)
        cap_scale = min(1.0, capacity_k / max(1.0, float(nS)))
        wm_w_eff_block = wm_weight * cap_scale

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply WM decay toward uniform each trial (global)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Policies
            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob for observed action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM map
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_w_eff_block * p_wm + (1.0 - wm_w_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: one-shot store only on rewarded trials
            if r > 0.5:
                # Overwrite to a strong, peaked memory at the rewarded action
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size-dependent WM weighting and negative-feedback suppression.

    Idea:
    - RL learns Q-values (delta rule).
    - WM map is decayed toward uniform but is also updated by:
        - Positive feedback: store strong association for chosen action (one-shot).
        - Negative feedback: suppress the chosen action in WM (tagging it as unlikely).
    - WM mixture weight is modulated by set size using a logistic (sigmoid) function:
        wm_weight_eff = wm_weight * sigmoid(wm_setsize_slope * (3.5 - set_size))
      which increases WM reliance for small set sizes and decreases for large ones.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL policy; internally scaled by 10
    - wm_decay: per-trial decay of WM map toward uniform (0..1)
    - wm_neg_suppress: amount to penalize chosen action in WM on negative feedback (>=0)
    - wm_setsize_slope: slope of logistic modulation of WM weight by set size (can be positive)
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_neg_suppress, wm_setsize_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Logistic modulation by set size (anchors around ~3.5)
        sig = 1.0 / (1.0 + np.exp(-wm_setsize_slope * (3.5 - float(nS))))
        wm_w_eff_block = wm_weight * sig

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Global decay toward uniform (interference)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_w_eff_block * p_wm + (1.0 - wm_w_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update rules:
            if r > 0.5:
                # Positive feedback: store chosen action strongly
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
            else:
                # Negative feedback: suppress chosen action in WM (avoidance tag)
                # Ensure non-negativity by flooring at small epsilon
                w[s, a] = max(w[s, a] - wm_neg_suppress, 1e-6)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + resource-limited WM with load-dependent decay and fast WM learning.

    Idea:
    - RL is standard delta-learning.
    - WM stores action tendencies per state and updates quickly (alpha_wm).
    - WM decays toward uniform at a rate that increases with set size (interference):
        wm_decay_eff = 1 - exp(-wm_decay_load * set_size)
    - WM influence is also capacity-limited via a global factor min(1, capacity_C / set_size).

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_alpha: WM fast learning rate toward a one-hot mapping on rewarded trials (0..1)
    - wm_decay_load: controls how decay grows with set size (>=0)
    - capacity_C: WM capacity in number of items; scales WM weight by min(1, C / set_size)
    """
    lr, wm_weight, softmax_beta, wm_alpha, wm_decay_load, capacity_C = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay
        wm_decay_eff = 1.0 - np.exp(-wm_decay_load * float(nS))
        # Capacity-based WM mixture scaling
        cap_scale = min(1.0, capacity_C / max(1.0, float(nS)))
        wm_w_eff_block = wm_weight * cap_scale

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Interference/decay increases with set size
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_w_eff_block * p_wm + (1.0 - wm_w_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM fast learning: move toward one-hot on rewarded trials; small anti-update on non-reward
            if r > 0.5:
                # Move W_s toward a one-hot peak at the chosen action
                target = w_0[s, :].copy()
                target[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target
            else:
                # Slight push away from the chosen action to reflect trial-and-error
                w[s, a] = (1.0 - wm_alpha) * w[s, a] + wm_alpha * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p

----------------

Notes on the models:
- All three models use an RL component mixed with a WM component, and they differ in how WM is updated and how load (set size) modulates WM influence.
- Model 1: simple capacity scaling and decay; WM only writes on reward.
- Model 2: set-size-dependent WM weighting via logistic modulation, plus negative-feedback suppression in WM.
- Model 3: resource-limited WM with load-dependent decay and fast WM learning; includes a capacity-based scaling of the WM mixture.

All parameters are used meaningfully, and each function returns the negative log-likelihood for the observed choices.