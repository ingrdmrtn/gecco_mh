def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + one-shot Working Memory with set-size-dependent mixture and decay

    Mixture policy combines:
    - Model-free RL Q-learning
    - A working-memory (WM) store that encodes rewarded stimulus-action pairs via one-shot learning,
      with passive decay toward uniform.

    Parameters
    ----------
    model_parameters : list or array-like of length 6
        lr : float
            RL learning rate for Q-learning (0..1).
        wm_weight_small : float
            Mixture weight for WM in small set size blocks (nS=3).
        wm_weight_large : float
            Mixture weight for WM in large set size blocks (nS=6).
        softmax_beta : float
            Inverse temperature for RL softmax (will be multiplied by 10 internally).
        wm_decay : float
            Passive decay rate of WM toward uniform on each trial (0..1).
        wm_store : float
            One-shot storage strength in WM when rewarded (0..1). 1.0 stores a near one-hot.

    Set-size effects
    ----------------
    - WM mixture weight depends on set size: wm_weight_small if nS=3, wm_weight_large if nS=6.
    """
    lr, wm_weight_small, wm_weight_large, softmax_beta, wm_decay, wm_store = model_parameters
    softmax_beta *= 10.0  # higher upper bound for RL
    softmax_beta_wm = 50.0  # highly deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL values and WM distributions
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent mixture
        wm_weight_block = wm_weight_small if nS <= 3 else wm_weight_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for chosen action (softmax trick using selected action as baseline)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # mixture
            p_total = p_wm * wm_weight_block + (1.0 - wm_weight_block) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Passive decay of the entire WM store toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # One-shot storage on reward: move row toward one-hot at chosen action
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_store) * w[s, :] + wm_store * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-dependent learning rate + Capacity-limited WM (recall probability) + forgetting

    Policy arbitration is probabilistic recall:
    - With probability p_recall = min(1, k_capacity / nS), behavior follows WM softmax.
    - Otherwise, behavior follows RL softmax.

    WM encodes rewarded pairs via one-shot assignment and forgets passively toward uniform.

    Parameters
    ----------
    model_parameters : list or array-like of length 5
        lr_small : float
            RL learning rate used when set size is small (nS=3).
        lr_large : float
            RL learning rate used when set size is large (nS=6).
        softmax_beta : float
            Inverse temperature for RL softmax (will be multiplied by 10 internally).
        k_capacity : float
            WM capacity in number of items; controls recall probability p_recall = min(1, k_capacity / nS).
        forget_rate : float
            Global WM forgetting rate toward uniform each trial (0..1).

    Set-size effects
    ----------------
    - RL learning rate: lr_small for nS=3, lr_large for nS=6.
    - WM mixture weight effectively equals p_recall = min(1, k_capacity / nS), hence smaller for larger set sizes.
    """
    lr_small, lr_large, softmax_beta, k_capacity, forget_rate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent RL learning rate and WM recall probability
        lr = lr_small if nS <= 3 else lr_large
        wm_weight = min(1.0, float(k_capacity) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture by recall probability
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global forgetting toward uniform
            w = (1.0 - forget_rate) * w + forget_rate * w_0
            # One-shot store upon reward
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot  # full overwrite reflects successful encoding

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM delta-rule with decay and choice stickiness; WM weight scales with set size

    Policy is a mixture of:
    - RL softmax over Q-values
    - WM softmax over W-values
    Both components include a perseveration (stickiness) bias toward the most recent action
    chosen for that state.

    WM is updated via a delta-rule on the selected action and passively decays toward uniform.

    Parameters
    ----------
    model_parameters : list or array-like of length 6
        lr_rl : float
            Learning rate for RL Q-learning (0..1).
        lr_wm : float
            Learning rate for WM delta-rule on the chosen action (0..1).
        wm_weight_base : float
            Base mixture weight for WM (scaled by set size as 3/nS).
        softmax_beta : float
            Inverse temperature shared for RL and WM policies (RL component is scaled by 10 internally).
        wm_decay : float
            Passive decay rate of WM toward uniform each trial (0..1).
        stickiness : float
            Choice perseveration weight added to the last action taken in the same state.

    Set-size effects
    ----------------
    - WM mixture weight is scaled by 3/nS: wm_weight_eff = wm_weight_base * (3 / nS), halving WM influence at nS=6.
    """
    lr_rl, lr_wm, wm_weight_base, softmax_beta, wm_decay, stickiness = model_parameters
    beta_rl = softmax_beta * 10.0
    beta_wm = 50.0  # deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track previous action per state for stickiness
        prev_act = -1 * np.ones(nS, dtype=int)

        wm_weight_eff = wm_weight_base * (3.0 / float(nS))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Construct stickiness preference vector
            stick_vec = np.zeros(nA)
            if prev_act[s] >= 0:
                stick_vec[prev_act[s]] = stickiness

            # RL policy with stickiness
            pref_rl = q[s, :] + stick_vec
            p_rl = 1.0 / np.sum(np.exp(beta_rl * (pref_rl - pref_rl[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            pref_wm = w[s, :] + stick_vec
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (pref_wm - pref_wm[a])))

            # Mixture
            p_total = p_wm * wm_weight_eff + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr_rl * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Passive decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # Delta-rule update on chosen action then renormalize row to keep a distribution
            w[s, a] += lr_wm * (r - w[s, a])
            # Ensure row is a proper distribution
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update stickiness memory
            prev_act[s] = a

        blocks_log_p += log_p

    return -blocks_log_p