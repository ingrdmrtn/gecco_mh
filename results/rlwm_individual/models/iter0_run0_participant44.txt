def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with WM decay and age- and load-dependent WM weighting.

    Model idea:
    - Choices arise from a mixture of reinforcement learning (RL) and working memory (WM).
    - WM stores the currently believed correct action for each state with strong determinism, but decays toward uniform.
    - WM contribution is stronger in low load (set size 3) and weaker in high load (set size 6).
    - Older adults (age >= 45) rely less on WM than younger adults.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline mixture weight for WM (0..1), before age and load modulation
    - softmax_beta: inverse temperature for RL policy (scaled internally)
    - wm_decay: decay rate of WM toward uniform on each trial (0..1)
    - age_wm_bias: proportional reduction of WM weight for older adults (0..1); effective WM weight is reduced by this fraction if age>=45
    
    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, age_wm_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    # Age effect on WM: older adults rely less on WM
    wm_weight_age = wm_weight * (1.0 - age_wm_bias * older)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: strong softmax on WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load modulation of WM: scale by 3/nS (stronger in set size 3)
            load_scale = 3.0 / float(nS)
            wm_mix = np.clip(wm_weight_age * load_scale, 0.0, 1.0)

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform, then incorporate feedback
            # First decay entire WM map slightly
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # If rewarded, set the state's WM to prefer the chosen action strongly
            if r > 0.5:
                # Move this state's WM distribution toward one-hot on chosen action
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
                # Overwrite with a strong preference for the rewarded action
                w[s, :] = (1.0 / nA) * np.ones(nA)
                w[s, a] = 1.0
                # Normalize to a proper distribution
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with separate learning rates and perseveration, mixed with a contingency-based WM.

    Model idea:
    - RL uses separate learning rates for positive and negative outcomes.
    - A perseveration bias favors repeating the last action taken in the same state.
    - WM stores the last rewarded action for a state; if available, it provides a near-deterministic policy.
    - WM contribution is stronger in low load (set size 3) and reduced for older adults.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewards (0..1)
    - lr_neg: RL learning rate for no-reward (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled internally)
    - perseveration: strength of repeating last action within a state (>=0)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, perseveration = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM distribution per state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)
        # Track last rewarded action per state for WM gating
        last_rewarded_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Incorporate perseveration as a bias added to the previously chosen action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration

            # RL softmax prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if a last rewarded action exists, be near-deterministic on it; else uniform
            if last_rewarded_action[s] >= 0:
                W_s = (1.0 / nA) * np.ones(nA)
                W_s[last_rewarded_action[s]] = 1.0
                W_s = W_s / np.sum(W_s)
            else:
                W_s = w_0[s, :]

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM mixture weight is stronger at low load and reduced for older adults
            load_scale = 3.0 / float(nS)
            wm_mix = np.clip(wm_weight * load_scale * (1.0 - 0.5 * older), 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rate; modest age effect on negative learning
            eta = lr_pos if r > 0.5 else lr_neg * (1.0 - 0.3 * older)
            q[s, a] += eta * (r - q[s, a])

            # Update WM memory: store last rewarded action; clear on errors
            if r > 0.5:
                last_rewarded_action[s] = a
                w[s, :] = (1.0 / nA) * np.ones(nA)
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # On non-reward, we slightly relax WM certainty for this state
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]
                # Do not overwrite last_rewarded_action, but allow it to stay until next reward
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity-limited WM and lapse.

    Model idea:
    - RL provides graded value-based choice via softmax.
    - WM is capacity-limited: an item contributes with probability proportional to K / set_size.
    - Older adults have reduced effective WM availability.
    - A global lapse mixes in uniform choice noise.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled internally)
    - wm_weight: baseline WM mixture weight (0..1)
    - capacity_K: WM capacity in number of items (>=0)
    - lapse: probability of random responding (0..1)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, capacity_K, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track whether a state's mapping is currently in WM (approximate via non-uniformity)
        # We will still compute policy via W_s, but availability scales the mixture weight.
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective probability that the item is in WM: min(1, K/nS), reduced for older adults
            p_in_wm = min(1.0, float(capacity_K) / float(nS))
            p_in_wm *= (0.75 if older > 0.5 else 1.0)

            wm_mix = np.clip(wm_weight * p_in_wm, 0.0, 1.0)

            # Combine RL and WM, then apply lapse
            p_mixture = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_mixture + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: if rewarded, store strong one-hot; if not, decay toward uniform
            if r > 0.5:
                w[s, :] = (1.0 / nA) * np.ones(nA)
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Greater decay toward uniform when set size is larger (more interference)
                decay = 0.5 * (float(nS) / 6.0)  # between ~0.25 (nS=3) and 0.5 (nS=6)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p