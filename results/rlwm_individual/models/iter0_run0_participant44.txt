def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and set-size-modulated WM weight and an action lapse.
    
    The model blends a model-free RL policy and a simple working-memory (WM) policy that stores
    the most recently rewarded action for each state. WM weight decreases with larger set sizes
    (higher cognitive load) and for older adults. Learning rate also differs by age group.
    
    Negative log-likelihood of the observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index on each trial. Within a block, states are assumed to be 0..(set_size-1).
    actions : array-like of int
        Observed actions on each trial. If an action is not in {0,1,2}, it is mapped modulo 3.
    rewards : array-like of float
        Feedback on each trial (can be 0/1; model will also handle other numeric values).
    blocks : array-like of int
        Block index for each trial. Learning and WM stores reset at each new block.
    set_sizes : array-like of int
        Set size corresponding to each trial's block (e.g., 3 or 6). Used to modulate WM weight.
    age : array-like of float
        Participant age (single-element array). Used to determine age group.
    model_parameters : sequence of 6 floats
        Parameters: 
        - alpha_young: baseline learning-rate parameter (mapped to (0,1) via sigmoid)
        - alpha_old_delta: additive age effect on learning rate (sigmoid, added if older)
        - wm_base: baseline WM mixture weight (sigmoid)
        - wm_setsize_slope: effect of set size (3 vs 6) on WM weight (linear term inside sigmoid)
        - beta: softmax inverse temperature (>0 after scaling by 10)
        - lapse: action-independent lapse probability mixed with uniform (sigmoid, typically small)
    
    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    import numpy as np  # assumed already imported per instructions; kept here for clarity if environment needs it
    
    # Unpack parameters
    alpha_young, alpha_old_delta, wm_base, wm_setsize_slope, beta, lapse = model_parameters
    
    # Helper transforms
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    
    beta_eff = max(1e-6, beta) * 10.0
    lapse = np.clip(sigmoid(lapse), 0.0, 0.5)  # keep lapse sensible
    
    # Age group
    ag = 1 if age[0] >= 45 else 0
    
    # Learning rate with age modulation
    alpha = np.clip(sigmoid(alpha_young) + ag * sigmoid(alpha_old_delta), 1e-6, 1.0)
    
    # Negative log-likelihood accumulator
    total_loglik = 0.0
    
    # Iterate over blocks
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        
        if len(block_states) == 0:
            continue
        
        nA = 3
        nS = int(block_set_sizes[0])  # set size for the whole block
        
        # Initialize RL values and WM distributions
        Q = np.zeros((nS, nA), dtype=float)
        W = np.ones((nS, nA), dtype=float) / nA  # WM probability distribution per state
        
        # Compute WM mixture weight per block as a function of set size and age
        # Internal linear drive for WM weight
        # term_size: 0 for set size 3, negative (approx -1) for set size 6
        term_size = (3.0 - float(nS)) / 3.0
        wm_drive = wm_base + wm_setsize_slope * term_size - 0.5 * ag  # older -> lower WM weight
        wm_weight = np.clip(sigmoid(wm_drive), 0.0, 1.0)
        
        # Age can also slightly reduce effective beta (more exploration in older)
        beta_b = beta_eff / (1.0 + 0.3 * ag)
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a_raw = int(block_actions[t])
            r = float(block_rewards[t])
            
            # Map any out-of-range action to the 3-action space to compute likelihood
            a = a_raw % nA
            
            # RL policy: softmax over Q[s,:]
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            prl = np.exp(beta_b * q_center)
            prl = prl / np.sum(prl)
            
            # WM policy: use W[s,:] as a probability distribution
            pwm = W[s, :].copy()
            pwm = pwm / np.sum(pwm)  # ensure normalized
            
            # Mixture of WM and RL, then add lapse to uniform
            p_mix = wm_weight * pwm + (1.0 - wm_weight) * prl
            p = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            
            total_loglik += np.log(p[a])
            
            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta
            
            # WM update:
            # If rewarded, store the chosen action with high confidence (one-hot).
            # If not rewarded, partially relax toward uniform to reflect uncertainty.
            if r > 0.0:
                W[s, :] = 1e-6
                W[s, a] = 1.0 - (nA - 1) * 1e-6
            else:
                # Gentle decay toward uniform (no extra parameter used)
                W[s, :] = 0.5 * W[s, :] + 0.5 * (1.0 / nA)
    
    return float(-total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning, perseveration bias, and set-size- and age-modulated decay.
    
    The model is purely RL-based but includes:
    - Separate learning rates for positive vs negative prediction errors.
    - A perseveration bias that favors repeating the last action chosen in the same state.
    - Value decay toward zero that is stronger under larger set sizes and for older adults.
    - Age also reduces inverse temperature (more exploration in older participants).
    
    Returns the negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index on each trial. Assumed 0..(set_size-1) within block.
    actions : array-like of int
        Observed actions, mapped modulo 3 for likelihood computation.
    rewards : array-like of float
        Trial feedback.
    blocks : array-like of int
        Block indices. Learning (Q) resets at each block.
    set_sizes : array-like of int
        Set sizes per trial. Used to modulate decay strength within a block.
    age : array-like of float
        Participant age (single-element array). Used to determine age group.
    model_parameters : sequence of 5 floats
        Parameters:
        - alpha_pos: learning-rate parameter for positive PEs (sigmoid to (0,1))
        - alpha_neg: learning-rate parameter for negative PEs (sigmoid to (0,1))
        - beta: inverse temperature (>0 after scaling by 10)
        - persev_weight: perseveration strength (sigmoid scaled)
        - decay: baseline decay rate toward zero (sigmoid to (0,1))
    
    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    import numpy as np  # assumed imported
    
    alpha_pos, alpha_neg, beta, persev_weight, decay = model_parameters
    
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    
    # Map parameters
    a_pos = np.clip(sigmoid(alpha_pos), 1e-6, 1.0)
    a_neg = np.clip(sigmoid(alpha_neg), 1e-6, 1.0)
    beta_eff = max(1e-6, beta) * 10.0
    rho = sigmoid(persev_weight) * 2.0  # scale perseveration to a meaningful range
    d_base = sigmoid(decay)
    
    ag = 1 if age[0] >= 45 else 0
    
    total_loglik = 0.0
    
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        
        if len(block_states) == 0:
            continue
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        Q = np.zeros((nS, nA), dtype=float)
        last_action = -np.ones(nS, dtype=int)  # last action per state; -1 means none yet
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a_raw = int(block_actions[t])
            r = float(block_rewards[t])
            a = a_raw % nA
            
            # Set-size- and age-modulated decay toward zero
            # Stronger decay with larger set sizes and in older adults.
            set_size = float(nS)
            d_eff = d_base * (set_size / 6.0) * (1.0 + 0.5 * ag)
            d_eff = np.clip(d_eff, 0.0, 1.0)
            Q[s, :] *= (1.0 - d_eff)
            
            # Perseveration bias: bonus to repeating last action in this state
            pref = Q[s, :].copy()
            if last_action[s] >= 0:
                kappa = rho * (1.0 + 0.5 * ag)  # older -> stronger perseveration
                pref[last_action[s]] += kappa
            
            # Temperature reduced in older adults
            beta_b = beta_eff / (1.0 + 0.3 * ag)
            
            # Softmax
            pref_center = pref - np.max(pref)
            p = np.exp(beta_b * pref_center)
            p = p / np.sum(p)
            p = np.clip(p, 1e-12, 1.0)
            total_loglik += np.log(p[a])
            
            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            if pe >= 0.0:
                Q[s, a] += a_pos * pe
            else:
                Q[s, a] += a_neg * pe
            
            # Update last action memory for perseveration
            last_action[s] = a
    
    return float(-total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + finite-capacity WM (slot model) with age-modulated capacity, retrieval noise, and lapse.
    
    WM is modeled as a limited-capacity store that can hold up to K state-action mappings
    within a block. If set size <= K, the state is likely in WM; otherwise, WM coverage is partial.
    The probability that a given state is in WM is approximated by min(1, K / set_size).
    When a state is in WM and has a stored correct action (from prior reward), the WM policy
    chooses that action with probability 1 - eps (and uniform otherwise). Otherwise, RL governs choice.
    
    Age modulates the effective capacity K (older adults have different K via K_age_delta)
    and slightly reduces inverse temperature (more exploration).
    
    Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index per trial. Assumed 0..(set_size-1) in a block.
    actions : array-like of int
        Observed actions, mapped modulo 3 for likelihood computation.
    rewards : array-like of float
        Trial feedback.
    blocks : array-like of int
        Block index per trial. Q values and WM stores reset each block.
    set_sizes : array-like of int
        Set sizes per trial (e.g., 3, 6). Used to compute WM coverage weight.
    age : array-like of float
        Participant age (single-element array) to determine age group.
    model_parameters : sequence of 6 floats
        Parameters:
        - alpha: RL learning rate (sigmoid to (0,1))
        - beta: inverse temperature (>0 after scaling by 10)
        - K_base: baseline WM capacity parameter (mapped via sigmoid to [0,6])
        - K_age_delta: additive capacity change for older adults (can be +/-; clipped to [0,6])
        - wm_eps: WM retrieval noise (sigmoid to (0,0.5))
        - lapse: action-independent lapse probability (sigmoid, mixed with uniform)
    
    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    import numpy as np  # assumed imported
    
    alpha, beta, K_base, K_age_delta, wm_eps, lapse = model_parameters
    
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    
    a = np.clip(sigmoid(alpha), 1e-6, 1.0)
    beta_eff = max(1e-6, beta) * 10.0
    eps = np.clip(sigmoid(wm_eps), 0.0, 0.5)
    lapse = np.clip(sigmoid(lapse), 0.0, 0.5)
    
    ag = 1 if age[0] >= 45 else 0
    
    # Baseline capacity in [0,6]
    K0 = 6.0 * sigmoid(K_base)
    K = K0 + (K_age_delta * ag)
    K = float(np.clip(K, 0.0, 6.0))
    
    total_loglik = 0.0
    
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        
        if len(block_states) == 0:
            continue
        
        nA = 3
        nS = int(block_set_sizes[0])
        
        Q = np.zeros((nS, nA), dtype=float)
        # WM store: -1 means unknown; otherwise store the action index believed to be correct
        wm_store = -np.ones(nS, dtype=int)
        
        # WM coverage weight for this block
        if nS > 0:
            wm_cov = min(1.0, K / float(nS))
        else:
            wm_cov = 0.0
        
        # Age reduces beta slightly
        beta_b = beta_eff / (1.0 + 0.3 * ag)
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a_obs = int(block_actions[t]) % nA
            r = float(block_rewards[t])
            
            # RL policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            prl = np.exp(beta_b * q_center)
            prl = prl / np.sum(prl)
            
            # WM policy
            if wm_store[s] >= 0:
                wm_probs = np.ones(nA, dtype=float) * (eps / nA)
                wm_probs[wm_store[s]] += (1.0 - eps)
            else:
                # No stored mapping yet; WM provides no guidance
                wm_probs = np.ones(nA, dtype=float) / nA
            
            # Effective WM weight applies only if we have a stored mapping
            has_mem = 1.0 if wm_store[s] >= 0 else 0.0
            w = wm_cov * has_mem
            
            p_mix = w * wm_probs + (1.0 - w) * prl
            p = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            
            total_loglik += np.log(p[a_obs])
            
            # RL update
            pe = r - Q[s, a_obs]
            Q[s, a_obs] += a * pe
            
            # WM update: store mapping only on rewarded trials
            if r > 0.0:
                wm_store[s] = a_obs
    
    return float(-total_loglik)