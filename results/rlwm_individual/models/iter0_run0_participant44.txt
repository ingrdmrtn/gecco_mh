Below are three standalone cognitive models that follow the requested template logic, each returning the negative log-likelihood of the participant’s choices. Each model mixes a reinforcement-learning (RL) policy with a working-memory (WM) policy and accounts for set-size effects differently.

Model 1: RL + WM with capacity-limited WM usage and decay
- WM is more heavily relied upon in small set sizes, controlled by capacity K and slope tau.
- WM stores the last correct action for a state after rewarded outcomes and decays toward uniform otherwise.
- RL uses a single learning rate.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM usage and decay.

    Parameters
    ----------
    states : array-like of int
        State on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback outcome on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block each trial belongs to.
    model_parameters : sequence of floats
        lr : RL learning rate in [0,1].
        wm_weight : Base WM mixture weight in [0,1].
        softmax_beta : Inverse temperature for RL (scaled by 10 internally).
        wm_decay : WM decay toward uniform in [0,1]; 1 means immediate decay to uniform.
        K : WM capacity parameter; higher values imply better WM usage at larger set sizes.
        tau : Slope controlling how sharply WM usage falls with set size.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, K, tau = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    eps = 1e-12

    unique_blocks = np.unique(blocks)
    blocks_log_p = 0.0

    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and WM weights
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy (softmax over Q)
            Q_s = q[s, :]
            Q_centered = Q_s - np.max(Q_s)  # stabilize softmax
            denom_rl = np.sum(np.exp(softmax_beta * Q_centered))
            p_rl = np.exp(softmax_beta * Q_centered[a]) / max(denom_rl, eps)

            # WM policy (softmax over W with high beta)
            W_s = w[s, :]
            W_centered = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * W_centered))
            p_wm = np.exp(softmax_beta_wm * W_centered[a]) / max(denom_wm, eps)

            # Capacity-limited mixture weight as a function of set size
            # p_use = sigmoid( (K - nS)/tau )
            p_use = 1.0 / (1.0 + np.exp((nS - K) / max(tau, eps)))
            w_eff = np.clip(wm_weight * p_use, 0.0, 1.0)

            # Mixture policy
            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial for this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning: on rewarded trials, store the correct action deterministically
            # Use wm_learn tied to decay (fast update if low decay)
            wm_learn = 1.0 - wm_decay
            if r > 0.0:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: RL + WM with perseveration and capacity-limited WM usage
- Adds a perseveration bias to the RL policy: a tendency to repeat the last action taken in the same state.
- WM as in Model 1 with decay and capacity K, but without the extra slope parameter.
- Keeps total parameters ≤ 6 by using a single slope (=1) in the capacity function.

def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with state-wise perseveration bias and capacity-limited WM.

    Parameters
    ----------
    states : array-like of int
        State on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback outcome on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block each trial belongs to.
    model_parameters : sequence of floats
        lr : RL learning rate in [0,1].
        wm_weight : Base WM mixture weight in [0,1].
        softmax_beta : Inverse temperature for RL (scaled by 10 internally).
        wm_decay : WM decay toward uniform in [0,1].
        K : WM capacity parameter.
        persev_beta : Choice perseveration strength added to the last action in a state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, K, persev_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    unique_blocks = np.unique(blocks)
    blocks_log_p = 0.0

    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration (initialize to -1)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += persev_beta

            Q_centered = Q_s - np.max(Q_s)
            denom_rl = np.sum(np.exp(softmax_beta * Q_centered))
            p_rl = np.exp(softmax_beta * Q_centered[a]) / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            W_centered = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * W_centered))
            p_wm = np.exp(softmax_beta_wm * W_centered[a]) / max(denom_wm, eps)

            # Capacity-limited WM usage; use a unit slope for simplicity
            p_use = 1.0 / (1.0 + np.exp((nS - K)))  # slope fixed at 1
            w_eff = np.clip(wm_weight * p_use, 0.0, 1.0)

            # Mixture
            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM learning on reward
            wm_learn = 1.0 - wm_decay
            if r > 0.0:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: RL with asymmetric learning rates + WM with lapse + novelty-driven exploration
- RL has separate learning rates for positive and negative outcomes.
- WM policy includes a lapse/guessing component epsilon_wm.
- RL includes a novelty bonus for underexplored state-action pairs, scaled by set size, to capture exploration under load.
- WM usage is stronger in smaller set sizes via a simple scaling rule.

def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with lapse + novelty exploration.

    Parameters
    ----------
    states : array-like of int
        State on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback outcome on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block each trial belongs to.
    model_parameters : sequence of floats
        lr_pos : RL learning rate for positive prediction errors in [0,1].
        lr_neg : RL learning rate for negative prediction errors in [0,1].
        softmax_beta : Inverse temperature for RL (scaled by 10 internally).
        wm_weight : Base WM mixture weight in [0,1].
        epsilon_wm : WM lapse probability in [0,1] (probability of random choice within WM policy).
        novelty_scale : Scale of novelty bonus; larger values encourage exploration of under-sampled actions.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, epsilon_wm, novelty_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    unique_blocks = np.unique(blocks)
    blocks_log_p = 0.0

    for b in unique_blocks:
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Visit counts for novelty bonus
        visit_counts = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Novelty bonus scaled by set size: higher set size -> relatively stronger bonus
            # bonus decays with sqrt of visits to emphasize early exploration
            bonus = novelty_scale * (nS / 6.0) / np.sqrt(1.0 + visit_counts[s, :])

            # RL policy with novelty bonus
            Q_eff = q[s, :] + bonus
            Q_centered = Q_eff - np.max(Q_eff)
            denom_rl = np.sum(np.exp(softmax_beta * Q_centered))
            p_rl = np.exp(softmax_beta * Q_centered[a]) / max(denom_rl, eps)

            # WM policy with lapse
            W_s = w[s, :]
            W_centered = W_s - np.max(W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * W_centered))
            p_wm_soft = np.exp(softmax_beta_wm * W_centered[a]) / max(denom_wm, eps)
            p_wm = (1.0 - epsilon_wm) * p_wm_soft + epsilon_wm * (1.0 / nA)

            # WM usage stronger in smaller set sizes: scale by 3/nS (clipped)
            w_eff = np.clip(wm_weight * (3.0 / max(nS, 1.0)), 0.0, 1.0)

            # Mixture policy
            p_total = w_eff * p_wm + (1.0 - w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # Update visit counts AFTER policy computation
            visit_counts[s, a] += 1.0

            # WM updating: simple storage on reward, mild decay otherwise
            # Here we set a modest fixed decay (dependent on epsilon_wm so parameter is used meaningfully):
            wm_decay = np.clip(0.5 * epsilon_wm, 0.0, 1.0)
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                wm_learn = 1.0 - wm_decay
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size effects
- Model 1: WM reliance is a smooth sigmoid function of set size relative to capacity K (with slope tau).
- Model 2: Similar capacity effect but with a fixed slope and an added perseveration component that can mimic response habits under load.
- Model 3: WM reliance scales as 3/nS, and RL is enhanced with a set-size-scaled novelty bonus to capture more exploration in larger sets.