Below are three standalone cognitive models that implement RLâ€“WM mixtures while respecting the provided template structure. Each model returns the negative log-likelihood of the observed choices and uses all provided parameters. All models adapt the working-memory (WM) contribution as a function of set size (load).

Note: Assumes numpy is already imported as np, as requested. No imports are included in the functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with WM capacity scaling, WM decay, and WM error-correction.

    Idea:
    - RL learns slowly across all set sizes via a standard delta rule.
    - WM stores rewarded associations in a near-deterministic table that decays toward uniform.
    - The influence of WM on choice is scaled by a capacity parameter relative to set size.
    - WM also has a small corrective learning rate on negative feedback.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base mixture weight for WM (0..1); actual effective weight scales with capacity.
        softmax_beta: float
            Inverse temperature for RL policy; internally scaled by 10.
        K: float
            WM capacity parameter (in items). WM influence scales as min(1, K / nS).
        phi: float
            WM decay-to-prior rate per trial (0..1); higher means faster decay to uniform.
        eta_wm: float
            WM corrective learning rate on negative feedback (0..1).
    """
    lr, wm_weight, softmax_beta, K, phi, eta_wm = model_parameters
    softmax_beta *= 10.0  # higher dynamic range
    softmax_beta_wm = 50  # very deterministic
    
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value/probability tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM prior (uniform)

        # Effective WM mixture weight scales with capacity and load
        wm_scale = min(1.0, K / max(1.0, nS))
        wm_mix = np.clip(wm_weight * wm_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM table (treated as "Q" for policy)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward prior each trial
            w = (1.0 - phi) * w + phi * w_0

            # WM update:
            # If rewarded: store a near-deterministic mapping for this state
            if r > 0.5:
                # Set one-hot preference for the rewarded action
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                # Normalize to a proper distribution-like vector
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # On negative feedback, suppress the chosen action a bit (error-corrective)
                w[s, a] = max(0.0, w[s, a] * (1.0 - eta_wm))
                # Renormalize row
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM mixture, with WM weight decreasing exponentially with load,
    and a within-state perseveration bias on RL policy.

    Idea:
    - RL uses separate learning rates for rewards and non-rewards.
    - Perseveration bias increases the probability of repeating the previous action in that state in RL policy.
    - WM is one-shot when rewarded (stores deterministic mapping), with minimal decay via implicit noise (no explicit parameter).
    - Effective WM mixing weight decays with set size via exp(-nS / K).

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        alpha_pos: float
            RL learning rate for positive feedback (0..1).
        alpha_neg: float
            RL learning rate for negative feedback (0..1).
        wm_weight: float
            Base WM mixture weight (0..1); scaled by exp(-nS / K).
        softmax_beta: float
            Inverse temperature for RL policy; internally scaled by 10.
        K: float
            Load-sensitivity parameter for WM mixture. Larger K reduces the drop of WM with set size.
        persev: float
            Perseveration bonus added to the RL value of the previously selected action in the same state.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, K, persev = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50  # near-deterministic WM

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track previous action per state for perseveration
        prev_action = -1 * np.ones(nS, dtype=int)

        # Exponential drop of WM weight with load
        wm_mix = wm_weight * np.exp(-float(nS) / max(1e-6, K))
        wm_mix = np.clip(wm_mix, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add perseveration bonus to RL preferences for the last chosen action in this state
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += persev

            # RL probability of the chosen action with the adjusted preferences
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax on WM table
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s, a]
            if r > 0.5:
                q[s, a] += alpha_pos * delta
            else:
                q[s, a] += alpha_neg * delta

            # WM update:
            # One-shot storage on reward: deterministic mapping
            if r > 0.5:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # On negative feedback, slightly nudge away from the chosen action
                # (implicit noise via blending with prior)
                blend = 0.1  # small fixed blend to introduce minimal decay/noise
                w[s, :] = (1.0 - blend) * w[s, :] + blend * w_0[s, :]
                # light suppression of chosen action
                w[s, a] *= 0.9
                w[s, :] /= np.sum(w[s, :])

            # Update perseveration memory
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay + WM gating by capacity and WM precision scaling with set size.

    Idea:
    - RL values decay toward uniform each trial, capturing forgetting/interference across items.
    - WM influence on policy is gated by capacity L relative to set size nS: wm_weight * min(1, L/nS).
    - WM precision/temperature is modulated by a parameter tau_wm and set size (reducing effective precision with higher load).
    - WM stores deterministic associations on reward; on non-reward it weakly suppresses the chosen action.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM mixture weight (0..1); scaled by min(1, L/nS).
        softmax_beta: float
            Inverse temperature for RL policy; internally scaled by 10.
        L: float
            WM capacity (in items). WM influence scales as min(1, L / nS).
        d: float
            RL decay rate toward uniform (0..1) per trial within a block.
        tau_wm: float
            WM precision modulator (>0). Effective WM inverse temperature is 50 * (tau_wm / nS).
    """
    lr, wm_weight, softmax_beta, L, d, tau_wm = model_parameters
    softmax_beta *= 10.0
    base_wm_beta = 50.0  # base WM inverse temperature

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM gating by capacity
        wm_mix = wm_weight * min(1.0, L / max(1.0, nS))
        wm_mix = np.clip(wm_mix, 0.0, 1.0)

        # WM precision decreases with load; modulated by tau_wm
        softmax_beta_wm = base_wm_beta * (tau_wm / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL decay toward uniform before updating
            q = (1.0 - d) * q + d * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                # Deterministic storage on reward
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Weak suppression and slight diffusion to prior to avoid overconfidence on errors
                w[s, a] *= 0.9
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p