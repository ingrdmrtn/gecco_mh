def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age-adjusted capacity and decay.
    
    The model assumes two systems:
      - Model-free RL learns Q-values via a Rescorla-Wagner update and selects with softmax.
      - A capacity-limited WM stores (state->action) associations when feedback is correct,
        but decays over time. Retrieval probability scales with effective capacity and set size.
      - Arbitration: policy is a mixture between WM and RL, with retrieval probability as the WM weight.
      - A small lapse rate mixes in uniform choice on every trial.
    
    Inputs:
      - states: np.array of ints, state index on each trial (0..nS-1 within block)
      - actions: np.array of ints, chosen action on each trial (0,1,2). Out-of-range treated as invalid and skipped.
      - rewards: np.array of floats/ints, feedback per trial (1 or 0). -1 indicates invalid/missing and is skipped.
      - blocks: np.array of ints, block id per trial
      - set_sizes: np.array of ints, set size per trial (3 or 6 for this task); constant within block
      - age: np.array with a single scalar age
      - model_parameters: tuple/list with up to 6 parameters:
          alpha       : RL learning rate in [0,1]
          beta        : inverse temperature (>0), scaled internally
          K_base      : baseline WM capacity (in items)
          k_age       : capacity reduction applied if older adult (>=45); K_eff = max(0, K_base - k_age*older)
          wm_decay    : per-trial WM decay in [0,1]
          lapse       : lapse mixing with uniform choice in [0,1]
    
    Returns:
      - negative log-likelihood of the observed choices under the model
    """
    alpha, beta, K_base, k_age, wm_decay, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0  # scale beta to allow sharper policies
    lapse = np.clip(lapse, 0.0, 1.0)
    alpha = np.clip(alpha, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    older = 1.0 if age_val >= 45.0 else 0.0
    
    nll = 0.0
    nA = 3
    eps = 1e-12
    
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        
        nS = int(block_set_sizes[0])
        # RL values
        Q = np.zeros((nS, nA))
        # WM store: strength per action for each state (normalized to form a distribution)
        W = np.zeros((nS, nA))
        
        # Age-adjusted effective capacity and retrieval probability
        K_eff = max(0.0, K_base - k_age * older)
        p_retrieve_block = min(1.0, K_eff / max(1.0, float(nS)))
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            
            # Skip invalid trials (invalid action or missing reward)
            if a < 0 or a >= nA or r < 0:
                # Apply decay of WM even on skipped trials to reflect passage of time
                W *= (1.0 - wm_decay)
                continue
            
            # RL policy
            prefs_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            exp_rl = np.exp(prefs_rl)
            pi_rl = exp_rl / np.sum(exp_rl)
            
            # WM policy: convert W[s,:] into a valid distribution; if empty -> uniform
            w_s = W[s, :].copy()
            if np.sum(w_s) <= 0:
                pi_wm = np.ones(nA) / nA
            else:
                pi_wm = w_s / np.sum(w_s)
            
            # Arbitration: WM weight is retrieval probability given capacity and set size
            w_wm = p_retrieve_block
            
            # Total choice probability with lapse
            pi_mix = w_wm * pi_wm + (1.0 - w_wm) * pi_rl
            pi = (1.0 - lapse) * pi_mix + lapse * (1.0 / nA)
            pa = max(eps, pi[a])
            nll -= np.log(pa)
            
            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta
            
            # WM update: reinforce correct mapping; otherwise passive decay
            if r >= 0.5:
                # Store a sharp association for the rewarded action (one-hot), but allow existing traces to remain weak
                W[s, :] *= (1.0 - wm_decay)  # decay others
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite toward one-hot by setting to max of current and one-hot
                W[s, :] = np.maximum(W[s, :], one_hot)
            else:
                # No reliable information; decay WM traces
                W *= (1.0 - wm_decay)
    
    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with action stickiness and set-size-dependent exploration; age modulates stickiness.
    
    Key ideas:
      - Separate learning rates for positive and negative prediction errors.
      - Softmax inverse temperature decreases with larger set size (more exploration when nS=6).
      - Action stickiness adds a bias toward repeating the last action in the current state.
      - Older adults exhibit stronger stickiness via an age-dependent boost.
    
    Inputs:
      - states: np.array of ints, state index on each trial (0..nS-1 within block)
      - actions: np.array of ints, chosen action on each trial (0,1,2). Out-of-range treated as invalid and skipped.
      - rewards: np.array of floats/ints, feedback per trial (1 or 0). -1 indicates invalid/missing and is skipped.
      - blocks: np.array of ints, block id per trial
      - set_sizes: np.array of ints, set size per trial (3 or 6), constant within block
      - age: np.array with a single scalar age
      - model_parameters: tuple/list of 6 parameters:
          alpha_pos : learning rate for positive prediction errors (r - Q > 0), in [0,1]
          alpha_neg : learning rate for negative prediction errors (r - Q < 0), in [0,1]
          beta      : base inverse temperature (>0), scaled internally
          gamma_ss  : set-size scaling; beta_eff = beta / (1 + gamma_ss*(nS-3))
          phi_base  : baseline stickiness added to preference of the last action in that state
          phi_age   : additional stickiness if older adult (>=45)
    
    Returns:
      - negative log-likelihood of the observed choices under the model
    """
    alpha_pos, alpha_neg, beta, gamma_ss, phi_base, phi_age = model_parameters
    beta = max(1e-6, beta) * 5.0  # scale beta
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    gamma_ss = max(0.0, gamma_ss)
    
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    older = 1.0 if age_val >= 45.0 else 0.0
    phi = phi_base + phi_age * older
    
    nll = 0.0
    nA = 3
    eps = 1e-12
    
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        
        nS = int(block_set_sizes[0])
        beta_eff = beta / (1.0 + gamma_ss * max(0, nS - 3))
        
        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # last chosen action per state; -1 means none yet
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            
            # Skip invalid trials
            if a < 0 or a >= nA or r < 0:
                continue
            
            # Preferences with stickiness toward last action in this state
            prefs = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0:
                prefs[last_action[s]] += phi
            
            # Softmax policy
            exp_p = np.exp(prefs)
            pi = exp_p / np.sum(exp_p)
            pa = max(eps, pi[a])
            nll -= np.log(pa)
            
            # Update last action
            last_action[s] = a
            
            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            Q[s, a] += lr * pe
    
    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and uncertainty-based WM arbitration; age biases arbitration toward WM.
    
    Components:
      - RL with replacing eligibility traces: E <- lambda*E; E[s,a]=1; Q += alpha*delta*E
      - A simple WM store that tracks recent correct associations and decays over time.
      - Arbitration weight uses a logistic function of:
          * set-size pressure (favor WM when smaller sets),
          * current WM certainty for the state,
          * RL uncertainty (entropy) with opposite sign,
          * age group (older adults favor WM).
      - Final policy is a mixture between WM and RL with the computed arbitration weight.
    
    Inputs:
      - states: np.array of ints, state index on each trial (0..nS-1 within block)
      - actions: np.array of ints, chosen action on each trial (0,1,2). Out-of-range treated as invalid and skipped.
      - rewards: np.array of floats/ints, feedback per trial (1 or 0). -1 indicates invalid/missing and is skipped.
      - blocks: np.array of ints, block id per trial
      - set_sizes: np.array of ints, set size per trial (3 or 6), constant within block
      - age: np.array with a single scalar age
      - model_parameters: tuple/list with up to 6 parameters:
          alpha     : RL learning rate in [0,1]
          beta      : inverse temperature (>0), scaled internally
          lam       : eligibility trace decay (lambda) in [0,1]
          theta0    : arbitration intercept (logit scale)
          theta1    : arbitration weight on set-size pressure (more positive -> stronger WM for small sets)
          theta_age : additional arbitration bias if older adult (>=45), positive favors WM
    
    Returns:
      - negative log-likelihood of the observed choices under the model
    """
    alpha, beta, lam, theta0, theta1, theta_age = model_parameters
    alpha = np.clip(alpha, 0.0, 1.0)
    lam = np.clip(lam, 0.0, 1.0)
    beta = max(1e-6, beta) * 8.0  # scale beta to allow sharper policies
    
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    older = 1.0 if age_val >= 45.0 else 0.0
    
    nll = 0.0
    nA = 3
    eps = 1e-12
    
    def softmax(x):
        z = x - np.max(x)
        e = np.exp(z)
        return e / np.sum(e)
    
    def entropy(p):
        p_safe = np.clip(p, eps, 1.0)
        return -np.sum(p_safe * np.log(p_safe))
    
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        
        nS = int(block_set_sizes[0])
        
        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))  # eligibility traces
        WM = np.zeros((nS, nA))  # WM strengths per state-action
        
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            
            # Skip invalid trials
            if a < 0 or a >= nA or r < 0:
                # Passive eligibility decay across time
                E *= lam
                # Passive WM decay to reflect forgetting between encounters
                WM *= 0.9
                continue
            
            # RL policy
            pi_rl = softmax(beta * Q[s, :])
            u_rl = entropy(pi_rl) / np.log(nA)  # normalized entropy in [0,1]
            
            # WM policy: normalize WM strengths; if empty -> uniform
            wm_s = WM[s, :]
            if np.sum(wm_s) > 0:
                pi_wm = wm_s / np.sum(wm_s)
                wm_cert = np.max(pi_wm)  # certainty in [1/3,1]
            else:
                pi_wm = np.ones(nA) / nA
                wm_cert = 1.0 / nA
            
            # Arbitration weight (logistic)
            # Set-size pressure: smaller sets (nS=3) increase WM weight; larger sets decrease it.
            ss_pressure = (3.0 - (nS - 3.0))  # equals 3 for nS=3, and 0 for nS=6
            # Combine features on logit scale
            logit_w = theta0 + theta1 * ss_pressure + theta_age * older + (wm_cert - u_rl)
            w = 1.0 / (1.0 + np.exp(-logit_w))
            w = np.clip(w, 0.0, 1.0)
            
            # Mixture policy
            pi = w * pi_wm + (1.0 - w) * pi_rl
            pa = max(eps, pi[a])
            nll -= np.log(pa)
            
            # RL update with eligibility traces (replacing traces)
            E *= lam
            E[s, :] = 0.0
            E[s, a] = 1.0
            delta = r - Q[s, a]
            Q += alpha * delta * E
            
            # WM update: strengthen correct mapping, otherwise mild decay
            if r >= 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Move WM toward one-hot by additive boost and renormalize implicitly via next use
                WM[s, :] = 0.5 * WM[s, :] + 1.0 * one_hot
            else:
                WM[s, :] *= 0.9  # decay on incorrect/zero reward
    
    return nll