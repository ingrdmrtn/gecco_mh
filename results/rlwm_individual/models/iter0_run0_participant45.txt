def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with set-size- and age-sensitive WM gating and simple WM decay.

    Idea:
    - Choices are driven by a mixture of incremental RL values (Q) and a fast working-memory (WM) store (W).
    - WM contribution is downweighted when set size increases (capacity-limited), and further reduced for older adults.
    - WM stores the last rewarded action per state and decays toward uniform otherwise.

    Parameters (model_parameters):
    - lr: RL learning rate for Q values (0..1).
    - wm_weight: Base mixture weight for WM vs RL (0..1), further modulated by set size and age.
    - softmax_beta: Inverse temperature for RL policy. Internally scaled by 10 for a broader search.
    - C_base: WM capacity scale (in "items"); larger means WM holds more items. Used to gate WM by set size.
    - wm_decay: Decay rate of WM toward uniform on non-reinforced trials (0..1). 0=no decay, 1=full reset.

    Age use:
    - Older adults (age >= 45) get a lower effective capacity: C_eff = 0.6 * C_base; young: C_eff = C_base.
    - Effective WM weight per block is wm_weight * gate(set size; C_eff), where gate = 1 / (1 + (nS/C_eff)^2).
    - This makes WM contribute less in larger sets and even less so for older adults.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, C_base, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value representations
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-sensitive capacity gating of WM by set size
        C_eff = C_base * (0.6 if is_older else 1.0)
        gate = 1.0 / (1.0 + (float(nS) / max(C_eff, 1e-6))**2)
        wm_block_weight = np.clip(wm_weight * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_block_weight * p_wm + (1.0 - wm_block_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - If rewarded, store chosen action strongly (one-shot).
            # - If not rewarded, decay toward uniform.
            if r > 0.5:
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with asymmetric RL learning rates, WM capacity probability, and choice stickiness.
    
    Idea:
    - RL uses separate learning rates for gains and losses (older adults often show altered learning asymmetries).
    - WM has a limited capacity: probability that WM contributes scales as p_cap = min(1, K_eff / set_size).
    - WM is one-shot for rewarded actions. WM influence is wm_weight * p_cap.
    - Add state-wise choice stickiness (perseveration) to RL action values; older adults show stronger stickiness.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight: Base WM mixture weight (0..1) before capacity scaling.
    - softmax_beta: Inverse temperature for RL; scaled internally by 10.
    - rho: Age effect on WM capacity (0..1). Older K_eff = rho * K_young. We set K_young = 4 by default inside model.
    - stickiness: Perseveration strength added to RL for the last chosen action in a state (>=0).

    Age use:
    - If age >= 45 (older), effective WM capacity K_eff = rho * K_young; else K_eff = K_young.
    - Stickiness scaled up for older adults: stickiness_eff = stickiness * (1.3 if older else 1.0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, rho, stickiness = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    # Fixed nominal young capacity; older capacity scaled by rho
    K_young = 4.0
    K_eff_factor = rho if is_older else 1.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action = -np.ones(nS, dtype=int)

        # WM capacity probability per block
        K_eff = K_young * K_eff_factor
        p_cap = np.clip(K_eff / max(float(nS), 1e-6), 0.0, 1.0)
        wm_block_weight = np.clip(wm_weight * p_cap, 0.0, 1.0)

        stickiness_eff = stickiness * (1.3 if is_older else 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with stickiness bonus on last chosen action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness_eff

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (near-deterministic)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_block_weight * p_wm + (1.0 - wm_block_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: store if rewarded; otherwise keep as-is (no decay here)
            if r > 0.5:
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM uncertainty arbitration with age- and load-dependent lapses and WM decay.

    Idea:
    - Both RL (Q) and WM (W) maintain action preferences.
    - Arbitration is dynamic: higher weight given to the system with lower entropy (more certainty).
    - Add a lapse probability that grows with set size and is larger for older adults.
    - WM decays to uniform on every trial by wm_decay, and is refreshed when rewarded.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Base WM weight before entropy-based arbitration (0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 internally.
    - entropy_temp: Temperature for converting entropies into arbitration weights (>0). Larger = more sensitivity.
    - wm_decay: Decay toward uniform for WM each trial (0..1).
    - lapse_base: Baseline lapse rate component (0..0.2 typical), further scaled by set size and age.

    Age use:
    - Lapse increases with set size and more steeply if older:
      lapse = lapse_base * ((nS - 3)/3 if nS>3 else 0) * (1.5 if age>=45 else 1.0)
    - This captures greater load sensitivity in older adults.
    - Arbitration also indirectly reflects age because WM is less reliable under decay when sets are larger.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, entropy_temp, wm_decay, lapse_base = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Lapse parameter for this block, increases with load and age
        load_factor = max(float(nS) - 3.0, 0.0) / 3.0  # 0 for 3, 1 for 6
        lapse = np.clip(lapse_base * load_factor * (1.5 if is_older else 1.0), 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL and WM policies
            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute normalized choice probabilities for arbitration entropy calc
            probs_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs_rl = probs_rl / max(np.sum(probs_rl), eps)
            probs_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            probs_wm = probs_wm / max(np.sum(probs_wm), eps)

            # Entropy (lower entropy => more certainty => higher weight)
            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))
            H_wm = -np.sum(probs_wm * np.log(np.clip(probs_wm, eps, 1.0)))

            # Convert entropies into weights (softmax over negative entropies)
            cert_rl = np.exp(( -H_rl) / max(entropy_temp, eps))
            cert_wm = np.exp(( -H_wm) / max(entropy_temp, eps))
            total_cert = cert_rl + cert_wm
            arb_wm = (cert_wm / max(total_cert, eps))

            # Combine with base WM weight
            wm_mix = np.clip(wm_weight_base * arb_wm, 0.0, 1.0)

            # Final policy with lapses (uniform random with probability lapse)
            p_total = (1.0 - lapse) * (wm_mix * p_wm + (1.0 - wm_mix) * p_rl) + lapse * (1.0 / 3.0)
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each trial, then reinforce if rewarded
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W

        blocks_log_p += log_p

    return -blocks_log_p