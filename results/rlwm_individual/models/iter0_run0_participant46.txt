def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-dependent forgetting and capacity scaling.

    Model summary:
    - Choices come from a mixture of a reinforcement-learning (RL) policy and a working-memory (WM) policy.
    - WM entries decay toward uniform with a rate that increases with set size (more forgetting at set size 6).
    - Effective WM weight is scaled by an estimated WM capacity relative to the current set size.

    Parameters (model_parameters):
    - lr: RL and WM learning rate (0..1). Controls speed of updating values.
    - wm_weight: Base WM mixture weight (real, mapped via logistic to 0..1). Higher favors WM.
    - softmax_beta: Inverse temperature for RL softmax (scaled x10 inside; higher = more deterministic RL).
    - rho_base: Base WM decay rate (0..1). Larger means faster WM forgetting toward uniform.
    - rho_slope: Increment to WM decay per extra item beyond 3 (can be negative/positive, clipped to 0..1).
    - wm_capacity: Effective WM capacity (>=0). WM contribution scales approximately as min(1, wm_capacity / set_size).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rho_base, rho_slope, wm_capacity = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # highly deterministic WM
    blocks_log_p = 0.0

    # logistic transform for a stable [0,1] base WM weight
    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # set-size dependent WM decay (clip to [0,1])
        rho = rho_base + rho_slope * max(0, (nS - 3))
        rho = float(np.clip(rho, 0.0, 1.0))

        # capacity-relative WM weight scaling
        cap_scale = float(np.clip((wm_capacity / max(1.0, nS)), 0.0, 1.0))
        base_wm = logistic(wm_weight)
        wm_mix_const = float(np.clip(base_wm * cap_scale, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Global WM decay toward uniform each trial (set-size dependent)
            w = (1.0 - rho) * w + rho * w_0

            # RL policy probability
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability (softmax over WM weights)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_mix_const * p_wm + (1.0 - wm_mix_const) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Always drift a bit back toward uniform (already applied globally above)
            # 2) If error, move toward uniform; if correct, imprint a stronger one-hot for chosen action
            if r < 0.5:
                # move this state's WM row toward uniform
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]
            else:
                # push chosen action toward 1, others down
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr

            # renormalize to a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-dependent WM weighting and a choice perseverance kernel.

    Model summary:
    - Choices come from a mixture of RL and WM policies.
    - WM weight diminishes exponentially with set size (i.e., more load -> less WM reliance).
    - A choice kernel (perseveration) biases RL policy toward repeating the most recent action in a state.
    - WM updates with its own learning rate.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_bias: Bias term mapped via logistic to base WM weight; transformed and down-weighted by set size.
    - softmax_beta: Inverse temperature for RL softmax (scaled x10 inside).
    - eta_wm: WM learning rate (0..1) and choice-kernel update rate.
    - size_alpha: Positive factor controlling exponential drop of WM weight with larger set sizes.
    - pers_beta: Strength of perseveration kernel added to RL values (>=0 increases tendency to repeat last action).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_bias, softmax_beta, eta_wm, size_alpha, pers_beta = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Choice kernel (per-state recency of chosen action)
        ck = np.zeros((nS, nA))

        # Base WM weight transformed to [0,1]
        base_wm = logistic(wm_weight_bias)
        # Exponential attenuation with set size (>=3)
        size_factor = np.exp(-max(0.0, (nS - 3)) * max(0.0, size_alpha))
        wm_mix_const = float(np.clip(base_wm * size_factor, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective RL values with perseveration kernel
            Q_s = q[s, :] + pers_beta * ck[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from WM table
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix_const * p_wm + (1.0 - wm_mix_const) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: learn toward one-hot on correct trials; drift toward uniform on errors
            if r < 0.5:
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * w_0[s, :]
            else:
                w[s, :] = (1.0 - eta_wm) * w[s, :]
                w[s, a] += eta_wm
            # normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            # Choice kernel update (within-state recency)
            ck[s, :] = (1.0 - eta_wm) * ck[s, :]
            ck[s, a] += eta_wm

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian-like WM (Dirichlet counts) with set-size-dependent interference.

    Model summary:
    - RL policy mixes with a WM policy formed from Dirichlet-like counts per state-action (posterior means).
    - WM experiences interference that grows with set size and with the ratio set_size / capacity.
    - WM updates add kappa pseudo-counts to the chosen action if rewarded, otherwise to non-chosen actions.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight (real, mapped via logistic).
    - softmax_beta: RL inverse temperature (scaled x10 inside).
    - kappa: WM pseudo-count increment per trial (>0); larger -> sharper WM distributions.
    - phi_base: Base WM interference/decay rate toward a flat prior (0..1).
    - wm_capacity: Effective capacity; larger capacity reduces interference when set size is large.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa, phi_base, wm_capacity = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM represented by Dirichlet-like counts M; start with symmetric prior of 1
        M = np.ones((nS, nA))
        # We'll compute w (posterior mean) from M each trial
        w = (M / np.sum(M, axis=1, keepdims=True))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent interference: larger set size and lower capacity => stronger decay toward prior
        cap = max(1.0, wm_capacity)
        interference_scale = float(nS / cap)
        phi = float(np.clip(phi_base * interference_scale, 0.0, 1.0))

        wm_mix_const = float(np.clip(logistic(wm_weight), 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Interference/decay: counts drift toward the symmetric prior of 1's
            M = (1.0 - phi) * M + phi * 1.0

            # Form WM policy from posterior mean of Dirichlet
            w = M / np.sum(M, axis=1, keepdims=True)

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix_const * p_wm + (1.0 - wm_mix_const) * p_rl
            p_total = float(np.clip(p_total, 1e-12, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM counts update (Dirichlet-like)
            if r >= 0.5:
                # credit chosen action
                M[s, a] += kappa
            else:
                # penalize chosen action by crediting alternatives equally
                alt = [i for i in range(nA) if i != a]
                for j in alt:
                    M[s, j] += kappa / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p

"""
Notes on set-size effects across models:
- Model 1: WM decay rho increases with set size via rho_slope; WM mixture weight is scaled by capacity/set-size ratio.
- Model 2: WM mixture weight is exponentially attenuated with larger set sizes via size_alpha; perseveration affects RL choice.
- Model 3: WM interference phi scales with set size divided by capacity; larger sets induce stronger decay toward a flat prior.
"""