def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + WM mixture with set-size dependent WM availability and decay
    - RL: tabular Q-learning with softmax policy.
    - WM: one-shot storage of the last rewarded action per state, with deterministic readout.
    - Arbitration: convex mixture of RL and WM action probabilities with WM weight reduced under larger set sizes.
    - Load effect: WM representation decays toward uniform more strongly when set size is larger.

    Parameters (model_parameters):
    - lr: float in [0,1], reinforcement learning rate for Q updates.
    - wm_weight: float in [0,1], baseline mixture weight of WM relative to RL at set size 3. Effective weight scales as wm_weight*(3/nS).
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10 for a higher dynamic range).
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for the chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY:
            # Readout via a high-beta softmax on WM values; probability of the chosen action
            # Effective WM weight reduced in larger sets
            wm_weight_eff = wm_weight * (3.0 / nS)
            # Compute WM softmax probability for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WORKING MEMORY UPDATING:
            # 1) Decay WM toward uniform; stronger decay when set size is larger
            #    leak = (nS-3)/nS -> 0 at 3, 0.5 at 6
            leak = max(0.0, (nS - 3.0) / nS)
            w = (1 - leak) * w + leak * w_0
            # 2) If rewarded, store the chosen action deterministically for this state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded, weakly suppress the chosen action in WM for that state
                w[s, a] = max(0.0, w[s, a] - 0.1)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL + capacity-gated WM with action-specific inhibition on errors
    - RL: tabular Q-learning with softmax policy.
    - WM: stores rewarded S->A mappings. On errors, WM reduces the salience of the chosen action (lose-shift).
    - Arbitration: convex mixture with constant WM mixture weight, but WM readout becomes less deterministic at higher set sizes.

    Parameters (model_parameters):
    - lr: float in [0,1], reinforcement learning rate.
    - wm_weight: float in [0,1], mixture weight for WM throughout (applied equally across set sizes).
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base WM determinism
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY:
            # Effective WM beta is reduced under higher load (less deterministic retrieval)
            beta_wm_eff = softmax_beta_wm * (3.0 / nS)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture policy (constant WM mixture weight across loads)
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WORKING MEMORY UPDATING:
            # Reward: write one-shot mapping
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Error: inhibit the chosen action in WM; soft encourage alternatives
                w[s, a] = max(0.0, w[s, a] - 0.3)
                # small attractiveness boost to other actions
                others = [aa for aa in range(nA) if aa != a]
                for aa in others:
                    w[s, aa] += 0.15
                # mild normalization to keep values in a comparable range
                row_max = np.max(w[s, :])
                if row_max > 0:
                    w[s, :] = w[s, :] / row_max
            # Global mild leak toward uniform (task demands, interference)
            w = 0.95 * w + 0.05 * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Uncertainty-weighted arbitration between RL and WM with load-driven interference
    - RL: tabular Q-learning with softmax policy.
    - WM: one-shot mapping store on reward; subject to interference proportional to set size.
    - Arbitration: WM weight scales with (a) baseline wm_weight, (b) set-size factor (3/nS),
                   and (c) RL uncertainty (normalized entropy). The more uncertain RL is, the more WM is used.

    Parameters (model_parameters):
    - lr: float in [0,1], reinforcement learning rate.
    - wm_weight: float in [0,1], baseline WM weight that scales arbitration strength.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base WM determinism
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probabilities (full vector) for uncertainty calculation
            # Numerically stable softmax
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Qc)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = p_rl_vec[a]

            # WORKING MEMORY POLICY:
            # WM softmax for chosen action
            Wc = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * Wc)
            p_wm_vec = expW / np.sum(expW)
            p_wm = p_wm_vec[a]

            # Uncertainty-weighted arbitration:
            # RL uncertainty = normalized entropy in [0,1]
            eps = 1e-12
            H = -np.sum(p_rl_vec * np.log(p_rl_vec + eps))
            H_norm = H / np.log(nA)
            # Set-size reduction of WM availability
            load_factor = 3.0 / nS
            wm_weight_eff = wm_weight * load_factor * H_norm

            # Mixture policy
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WORKING MEMORY UPDATING:
            # Interference increases with set size: leak toward uniform across all states
            interference = max(0.0, (nS - 3.0) / nS)  # 0 at 3, 0.5 at 6
            w = (1 - 0.2 * interference) * w + (0.2 * interference) * w_0
            # On reward, set one-shot mapping for this state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On error, slight demotion of chosen action in WM for this state
                w[s, a] = max(0.0, w[s, a] - 0.1)

        blocks_log_p += log_p

    return -blocks_log_p