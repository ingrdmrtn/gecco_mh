def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Capacity-limited Working Memory (WM) + RL mixture with decay and rewarded-only storage.
    - RL: tabular Q-learning with softmax choice.
    - WM: a one-shot mapping store (per state) that decays toward uniform and only stores when rewarded.
    - Arbitration: fixed wm_weight, but WM policy attenuated by a capacity parameter that scales with set size (nS).

    Parameters (tuple/list):
      lr:               RL learning rate (0..1)
      wm_weight:        Mixture weight of WM policy in final choice (0..1)
      softmax_beta:     Inverse temperature for RL softmax (scaled internally)
      wm_decay:         WM decay toward uniform each trial (0..1), higher = faster decay
      wm_capacity:      Effective WM capacity (in number of items); WM influence scales with min(1, capacity/nS)
      wm_store_strength:Strength of WM storage on rewarded trials (0..1); how deterministically the correct action is stored
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity, wm_store_strength = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Capacity-limited WM contribution: scale WM policy toward uniform when set size exceeds capacity
            cap_scale = np.minimum(1.0, wm_capacity / max(1, nS))
            p_wm_choice = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Attenuate WM toward uniform with (1 - cap_scale)
            p_wm = cap_scale * p_wm_choice + (1.0 - cap_scale) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            # small numerical guard
            p_total = np.maximum(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM representation for this state toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Rewarded trial: store selected action deterministically by wm_store_strength
                w[s, :] = (1.0 - wm_store_strength) * w[s, :]
                w[s, a] += wm_store_strength
            # Normalize to keep a valid distribution
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: WM with binding noise under load + RL mixture; WM stores wins and anti-stores losses.
    - RL: tabular Q-learning with softmax.
    - WM: noisy binding that increases with set size, decays toward uniform, and updates on both reward and no-reward.
      On reward: store the chosen action (one-shot).
      On no-reward: suppress chosen action and redistribute mass to alternatives ("anti-store").
    - Arbitration: fixed wm_weight, but WM policy computed from a binding-noisy WM state.

    Parameters:
      lr:                RL learning rate (0..1)
      wm_weight:         Mixture weight of WM policy (0..1)
      softmax_beta:      Inverse temperature for RL (scaled internally)
      wm_decay:          WM decay toward uniform per trial (0..1)
      binding_noise:     Load-sensitivity of WM binding (>=0). Effective noise = binding_noise * max(0, (nS-3)/3)
      wm_store_strength: Strength of WM storage/anti-storage (0..1)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, binding_noise, wm_store_strength = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Binding noise increases with load: from set size 3 to 6 scales linearly
            load_factor = max(0.0, (nS - 3) / 3.0)  # 0 at 3-items, 1 at 6-items
            noise = np.minimum(1.0, binding_noise * load_factor)
            # Noisy WM representation
            W_eff = (1.0 - noise) * W_s + noise * (1.0 / nA) * np.ones_like(W_s)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.maximum(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Rewarded: store chosen action
                w[s, :] = (1.0 - wm_store_strength) * w[s, :]
                w[s, a] += wm_store_strength
            else:
                # Unrewarded: anti-store chosen action (suppress it and redistribute mass)
                reduce = wm_store_strength * np.minimum(1.0, w[s, a])
                # Remove mass from chosen action
                w[s, a] = max(1e-12, w[s, a] - reduce)
                # Redistribute equally to other actions
                idx = [i for i in range(nA) if i != a]
                w[s, idx] += reduce / (nA - 1)
            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Uncertainty-gated WM with adaptive temperature and decay; WM updates on reward and weakly on no-reward.
    - RL: Q-learning with softmax.
    - WM: decays toward uniform; stores on reward, and performs weak repulsion from the chosen action on no-reward.
    - Arbitration within WM policy: WM softmax temperature is adjustable; WM policy itself is sharpened or flattened
      depending on RL uncertainty (entropy) and set size. Under high RL uncertainty and small set size, WM policy is sharper.

    Parameters:
      lr:                RL learning rate (0..1)
      wm_weight:         Mixture weight of WM policy in final choice (0..1)
      softmax_beta:      Inverse temperature for RL (scaled internally)
      wm_decay:          WM decay toward uniform per trial (0..1)
      wm_temp_scale:     Scales the WM inverse temperature (>=0); higher = more deterministic WM choice
      wm_store_strength: Strength of WM storage on reward and repulsion on no-reward (0..1)
    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_temp_scale, wm_store_strength = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base WM determinism
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute RL uncertainty (entropy of RL softmax policy over actions)
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_probs / np.sum(rl_probs)
            entropy = -np.sum(rl_probs * np.log(np.maximum(rl_probs, 1e-12))) / np.log(nA)  # normalized 0..1

            # Set-size factor: more load reduces WM sharpness
            load_factor = max(0.0, (nS - 3) / 3.0)  # 0 at 3, 1 at 6
            # Effective WM beta scales with wm_temp_scale, reduced by load, increased by RL uncertainty
            beta_wm_eff = softmax_beta_wm * (1.0 + wm_temp_scale * (1.0 - load_factor) * entropy)
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.maximum(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Reward: positive storage
                w[s, :] = (1.0 - wm_store_strength) * w[s, :]
                w[s, a] += wm_store_strength
            else:
                # No-reward: weak repulsion from chosen action
                repulse = 0.5 * wm_store_strength
                reduce = repulse * np.minimum(1.0, w[s, a])
                w[s, a] = max(1e-12, w[s, a] - reduce)
                idx = [i for i in range(nA) if i != a]
                w[s, idx] += reduce / (nA - 1)
            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p