def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and decay-based WM, modulated by age and set size.

    The model mixes a standard model-free RL policy with a working-memory (WM) policy.
    WM is treated as a decaying item-specific store that is stronger for smaller set sizes
    and weaker/decays faster in older adults. The WM mixture weight depends on an
    effective WM capacity relative to current set size. A small lapse rate captures
    random responding.

    Parameters
    ----------
    states : array-like, int
        State indices on each trial within block (0..nS-1 for that block).
    actions : array-like, int
        Chosen action indices (0..2).
    rewards : array-like, float or int
        Binary rewards (0 or 1).
    blocks : array-like, int
        Block indices corresponding to each trial.
    set_sizes : array-like, int
        Set size of the presented stimuli on each trial (constant within a block).
    age : array-like, float or int
        Participant's age; first element is used to determine age group.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, wm_decay, wm_capacity, lapse]
        - alpha: RL learning rate in [0,1].
        - beta: softmax inverse temperature scale; internally scaled up (x10).
        - wm_decay: WM decay rate per trial in [0,1].
        - wm_capacity: baseline WM capacity term controlling WM reliance.
        - lapse: lapse probability in [0,1] of uniform random choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_decay, wm_capacity, lapse = model_parameters
    beta_eff_scale = 10.0
    beta = beta * beta_eff_scale

    # Age group modulation
    a = age[0]
    is_older = 1.0 if a >= 45 else 0.0

    # Age-modulated capacity and decay
    # Older adults: slightly reduced effective capacity and higher decay
    cap_age_shift = -0.5 * is_older + 0.5 * (1 - is_older)
    capacity_eff_base = wm_capacity + cap_age_shift
    decay_eff_base = wm_decay * (1.2 * is_older + 0.8 * (1 - is_older))
    decay_eff_base = min(max(decay_eff_base, 0.0), 1.0)

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL Q-values initialized to uniform baseline
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: probabilities over actions and strengths per state
        # W: action-prob distribution in WM; start uniform and low strength
        W = (1.0 / nA) * np.ones((nS, nA))
        # Strength toward having a single correct item in WM (0..1)
        strength = np.zeros(nS)

        # Effective mixture weight based on capacity vs set size
        # WM more dominant when capacity exceeds set size; less so in older adults and larger sets
        size_penalty = (nS - 3)  # 0 for 3-set, positive for larger sets
        cap_eff = capacity_eff_base - 0.5 * size_penalty  # penalize larger sets
        # Gate function to get mixture weight in (0,1)
        wm_weight_block = 1.0 / (1.0 + np.exp(-(cap_eff)))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a_ch = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            exp_rl = np.exp(logits_rl)
            p_rl = exp_rl / np.sum(exp_rl)

            # WM decay toward uniform each trial (state-specific and global weakening)
            W = (1.0 - decay_eff_base) * W + decay_eff_base * (1.0 / nA)
            # Strength decays as well
            strength = (1.0 - decay_eff_base) * strength

            # WM policy for this state: sharpened version of W using its strength
            # Blend uniform and W_s by strength
            W_s = W[s, :]
            p_wm_s = strength[s] * W_s + (1.0 - strength[s]) * (1.0 / nA)

            # Final mixture with lapse
            mix = wm_weight_block
            p_mix = mix * p_wm_s + (1.0 - mix) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            nll -= np.log(max(p_final[a_ch], eps))

            # RL update
            pe = r - Q[s, a_ch]
            Q[s, a_ch] += alpha * pe

            # WM update: if reward, store this action with high confidence; else mildly reduce confidence
            if r > 0.5:
                W[s, :] = (eps)  # near-zero others
                W[s, a_ch] = 1.0 - (nA - 1) * eps
                strength[s] = min(1.0, strength[s] + 0.6)  # strengthen memory on correct feedback
            else:
                # On error, reduce strength; keep distribution as-is (already decayed)
                strength[s] = max(0.0, strength[s] - 0.2)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with age- and set-size-modulated sensitivity, global decay, and choice stickiness.

    This model uses separate learning rates for positive and negative prediction errors,
    softmax sensitivity that is reduced under higher set size and in older adults,
    value decay (forgetting) that is stronger in older adults, and a global action
    stickiness bias toward repeating the last chosen action.

    Parameters
    ----------
    states : array-like, int
        State indices on each trial within block (0..nS-1 for that block).
    actions : array-like, int
        Chosen action indices (0..2).
    rewards : array-like, float or int
        Binary rewards (0 or 1).
    blocks : array-like, int
        Block indices corresponding to each trial.
    set_sizes : array-like, int
        Set size of the presented stimuli on each trial (constant within a block).
    age : array-like, float or int
        Participant's age; first element is used to determine age group.
    model_parameters : list or tuple of 5 floats
        [alpha_pos, alpha_neg, beta, phi, kappa]
        - alpha_pos: learning rate for positive PE (rewarded trials).
        - alpha_neg: learning rate for negative PE (unrewarded trials).
        - beta: base inverse temperature; internally scaled and modulated by set size and age.
        - phi: base value decay rate toward zero; transformed to [0,1] and increased with age.
        - kappa: choice stickiness weight added to the last chosen action's preference.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, phi, kappa = model_parameters
    beta *= 10.0  # scale to typical range

    a = age[0]
    is_older = 1.0 if a >= 45 else 0.0

    # Map phi to [0,1] with logistic, then modulate by age
    phi_mapped = 1.0 / (1.0 + np.exp(-phi))
    phi_eff_age = phi_mapped * (1.2 * is_older + 0.8 * (1.0 - is_older))
    phi_eff_age = min(max(phi_eff_age, 0.0), 0.99)

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Q initialized at zero; decay will keep it near zero absent learning
        Q = np.zeros((nS, nA))

        # Track last chosen action (global) for stickiness
        last_action = None

        # Set- and age-modulated beta
        # Larger sets reduce sensitivity; older adults also slightly reduced sensitivity
        beta_block = beta * (3.0 / float(nS)) * (0.9 if is_older else 1.1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a_ch = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply global decay toward zero before computing policy
            Q *= (1.0 - phi_eff_age)

            # Softmax with stickiness bias
            pref = Q[s, :].copy()
            if last_action is not None:
                pref[last_action] += kappa
            logits = beta_block * (pref - np.max(pref))
            p = np.exp(logits)
            p = p / np.sum(p)

            nll -= np.log(max(p[a_ch], eps))

            # Learning update with asymmetric alphas
            pe = r - Q[s, a_ch]
            if pe >= 0:
                Q[s, a_ch] += alpha_pos * pe
            else:
                Q[s, a_ch] += alpha_neg * pe

            # Update last action
            last_action = a_ch

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Gated WM-RL hybrid with entropy-based WM reliance, modulated by age and set size.

    The policy is a convex combination of a model-free RL softmax and a WM-driven policy.
    WM stores a single most-likely action per state when rewarded (one-shot-like), with
    a reliability that changes with feedback and decays with noise. The gate favoring WM
    increases when WM for that state is confident (low entropy), decreases for larger
    set sizes and in older adults.

    Parameters
    ----------
    states : array-like, int
        State indices on each trial within block (0..nS-1 for that block).
    actions : array-like, int
        Chosen action indices (0..2).
    rewards : array-like, float or int
        Binary rewards (0 or 1).
    blocks : array-like, int
        Block indices corresponding to each trial.
    set_sizes : array-like, int
        Set size of the presented stimuli on each trial (constant within a block).
    age : array-like, float or int
        Participant's age; first element is used to determine age group.
    model_parameters : list or tuple of 5 floats
        [alpha, beta, wm_gate_base, wm_reliance_gain, noise]
        - alpha: RL learning rate in [0,1].
        - beta: base inverse temperature for RL softmax (scaled internally).
        - wm_gate_base: baseline gating bias toward WM (logit space).
        - wm_reliance_gain: gain on (1 - entropy) to weight WM when it's confident.
        - noise: WM noise parameter; lower is sharper WM policy; age increases effective noise.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_gate_base, wm_reliance_gain, noise = model_parameters
    beta *= 10.0

    a = age[0]
    is_older = 1.0 if a >= 45 else 0.0

    # Age-modulated WM noise (older -> noisier WM)
    noise_eff = max(1e-3, noise * (1.2 * is_older + 0.8 * (1.0 - is_older)))
    beta_wm = 5.0 / noise_eff  # higher => sharper WM policy

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM representation: a categorical distribution per state
        WM = (1.0 / nA) * np.ones((nS, nA))
        # Reliability per state (0..1)
        R = np.zeros(nS)

        # Set-size term reduces WM gating in larger sets
        size_term = -0.6 * (nS - 3)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a_ch = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: softmax over WM distribution scaled by beta_wm
            # Centered to encourage choice of the stored peak while allowing spread
            wm_centered = WM[s, :] - (1.0 / nA)
            logits_wm = beta_wm * (wm_centered - np.max(wm_centered))
            p_wm = np.exp(logits_wm)
            p_wm = p_wm / np.sum(p_wm)

            # WM entropy (normalized to [0,1]) and gate
            wm_dist = WM[s, :]
            ent = -np.sum(wm_dist * np.log(wm_dist + eps)) / np.log(nA)
            # Higher (1 - ent) => more confident WM
            age_term = -0.6 if is_older else 0.6
            gate_logit = wm_gate_base + wm_reliance_gain * (1.0 - ent) + age_term + size_term
            gate = 1.0 / (1.0 + np.exp(-gate_logit))
            gate = min(max(gate, 0.0), 1.0)

            # Mixture policy
            p = gate * p_wm + (1.0 - gate) * p_rl
            nll -= np.log(max(p[a_ch], eps))

            # RL update
            pe = r - Q[s, a_ch]
            Q[s, a_ch] += alpha * pe

            # WM update: reward-locked storage and reliability changes
            if r > 0.5:
                # Store chosen action strongly
                WM[s, :] = eps
                WM[s, a_ch] = 1.0 - (nA - 1) * eps
                R[s] = min(1.0, R[s] + 0.5)
            else:
                # Reduce reliability and diffuse WM slightly toward uniform
                R[s] = max(0.0, R[s] - 0.2)
                WM[s, :] = 0.9 * WM[s, :] + 0.1 * (1.0 / nA)

            # Add small diffusion each trial proportional to noise
            WM[s, :] = (1.0 - 0.1 * noise_eff) * WM[s, :] + (0.1 * noise_eff) * (1.0 / nA)
            WM[s, :] = np.maximum(WM[s, :], eps)
            WM[s, :] = WM[s, :] / np.sum(WM[s, :])

    return nll