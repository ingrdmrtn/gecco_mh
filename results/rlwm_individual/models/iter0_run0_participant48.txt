def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity- and decay-limited working memory, plus lapse.

    Idea:
    - RL learns Q-values per state-action via a standard delta rule.
    - WM stores rewarded associations but is limited by capacity (K) and subject to decay (phi).
    - The WM contribution is down-weighted as set size increases relative to capacity.
    - A small lapse mixes in uniform random choice.

    Parameters (model_parameters):
    - lr: learning rate for RL (0..1)
    - wm_weight: base mixture weight for WM (0..1)
    - softmax_beta: inverse temperature for RL softmax (scaled internally)
    - phi: WM update/decay rate (0..1); larger = faster overwriting/decay
    - K: WM capacity in number of state-action associations (>0)
    - lapse: lapse probability mixing in random choice (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, phi, K, lapse = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM weight given load (capacity-limited)
        cap_scale = min(1.0, K / max(1.0, nS))
        eff_wm_weight = wm_weight * cap_scale

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action a under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse-enhanced mixture
            p_mix = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward baseline, then strengthen chosen action if rewarded
            # Decay/forgetting
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]
            # Reward-gated writing to WM (store correct mapping)
            if r > 0:
                # Move mass toward a; simple additive push of magnitude phi
                w[s, a] = w[s, a] + phi * (1.0 - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with load-dependent WM weight and asymmetric RL learning rates.

    Idea:
    - RL uses separate learning rates for positive/negative prediction errors.
    - WM stores only positively confirmed (rewarded) state-action associations; no decay within block.
    - WM influence depends on load: separate mixture weights for small vs. large set sizes.
    - A lapse parameter mixes in uniform choice.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for positive prediction errors (0..1)
    - alpha_neg: RL learning rate for negative prediction errors (0..1)
    - wm_weight_small: WM mixture weight when nS <= 3 (0..1)
    - wm_weight_large: WM mixture weight when nS > 3 (0..1)
    - softmax_beta: inverse temperature for RL softmax (scaled internally)
    - lapse: lapse probability mixing in random choice (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    alpha_pos, alpha_neg, wm_weight_small, wm_weight_large, softmax_beta, lapse = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # WM store
        w_0 = (1 / nA) * np.ones((nS, nA))

        eff_wm_weight = wm_weight_small if nS <= 3 else wm_weight_large

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic readout of stored mapping)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse-enhanced mixture
            p_mix = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0 else alpha_neg
            q[s][a] += alpha * delta

            # WM update: write on positive feedback only (no decay within block)
            if r > 0:
                # Store one-hot mapping for rewarded action in this state
                w[s, :] = w_0[s, :] * 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + choice-kernel WM (perseveration memory) with load-dependent WM scaling and lapse.

    Idea:
    - RL learns Q-values via a delta rule.
    - WM is modeled as a choice kernel per state that favors repeating recent actions,
      regardless of reward (recency-based working memory/perseveration).
    - WM influence scales down as a power law of set size to capture load sensitivity.
    - A lapse parameter mixes in uniform choice.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: base mixture weight for WM (0..1)
    - softmax_beta: inverse temperature for RL softmax (scaled internally)
    - kappa: choice-kernel update/decay rate (0..1); higher = stronger/shorter-memory
    - gamma: load sensitivity exponent for WM weight (>0); effective WM ~ 1/(nS^gamma)
    - lapse: lapse probability mixing in random choice (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, kappa, gamma, lapse = parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # choice kernel per state
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent scaling of WM influence
        eff_wm_weight = wm_weight / (max(1.0, nS) ** max(0.0, gamma))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from choice kernel (perseverative bias)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse-enhanced mixture
            p_mix = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Choice-kernel WM update: decay toward uniform + reinforce chosen action
            w[s, :] = (1.0 - kappa) * w[s, :] + kappa * w_0[s, :]
            w[s, a] = w[s, a] + kappa * (1.0 - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p