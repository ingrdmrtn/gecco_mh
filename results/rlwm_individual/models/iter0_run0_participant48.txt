def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture model.

    On each trial, action selection is a mixture of:
    - RL policy: softmax over Q-values (state-action values).
    - WM policy: softmax over a fast-learning, fast-decaying WM store W.
      WM weight is reduced as set size increases, implementing capacity limits.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight: scalar in [0,1], base mixture weight on WM when set size is low.
    - softmax_beta: inverse temperature for RL policy; internally scaled by 10.
    - wm_lr: scalar in [0,1], WM learning rate (fast one-shot update).
    - wm_decay: scalar in [0,1], per-trial decay of WM toward uniform baseline.
    - capacity_k: nonnegative scalar controlling how strongly WM weight drops with larger set size.
                  WM effective weight = wm_weight * sigmoid(-capacity_k*(nS-3)).

    Returns:
    - Negative log-likelihood of observed actions given the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, capacity_k = model_parameters
    softmax_beta *= 10.0  # higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size gated WM weight (capacity effect)
        # At nS=3, gate ~ wm_weight; at nS=6, it shrinks depending on capacity_k
        wm_gate = 1.0 / (1.0 + np.exp(capacity_k * (nS - 3)))
        wm_weight_eff = wm_weight * wm_gate

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy for chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and update (fast, capacity-limited memory)
            # First decay toward uniform baseline
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Then fast error-correcting update on chosen action
            w[s, a] += wm_lr * (r - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + WM with one-shot binding, lapse, and set-size gating.

    - RL: separate learning rates for positive vs negative prediction errors.
    - WM: when rewarded, bind chosen action to the state (near one-shot), with decay over time.
          WM policy includes a lapse/guessing component.
    - Mixture weight is set-size gated via a logistic transform of a linear set-size term.

    Parameters (model_parameters):
    - lr_pos: scalar in [0,1], RL learning rate for positive prediction errors.
    - lr_neg: scalar in [0,1], RL learning rate for negative prediction errors.
    - wm_weight_base: real-valued base that is transformed to a weight via logistic and set-size term.
    - softmax_beta: inverse temperature for RL policy; internally scaled by 10.
    - wm_lapse: scalar in [0,1], lapse rate in WM policy (probability of uniform random choice).
    - wm_decay: scalar in [0,1], per-trial decay of WM bindings toward uniform.

    Notes:
    - Effective WM weight = sigmoid(wm_weight_base + k*(3 - nS)), where k is fixed at 1 for simplicity.
      This increases WM weight at small set size and reduces it at large set size.
    Returns:
    - Negative log-likelihood of observed actions given the model.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, wm_lapse, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    k_set = 1.0  # fixed slope for set-size gating
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size gated WM weight via logistic transform
        wm_lin = wm_weight_base + k_set * (3 - nS)
        wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_lin))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W with lapse
            W_s = w[s, :]
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - wm_lapse) * p_wm_soft + wm_lapse * (1.0 / nA)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM decay then binding upon reward
            # Decay toward uniform each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # If rewarded, bind chosen action strongly (near one-shot)
            if r > 0.0:
                # Move chosen action toward 1, others toward 0, but keep values bounded via convex mixing
                # Use a high-gain update by overwriting the row toward a one-hot vector
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend current WM row with one-hot to emulate rapid binding
                bind_strength = 0.9  # fixed strong binding inside WM update
                w[s, :] = (1.0 - bind_strength) * w[s, :] + bind_strength * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with perseveration bias + decaying WM traces, with set-size power-law gating.

    - RL: standard TD update, but policy includes a 'stickiness' bias (kappa) to repeat
          the last action taken in the same state.
    - WM: a decaying trace store that is boosted by reward on the chosen action and decays to uniform.
          Policy is softmax over WM traces.
    - Mixture: WM contribution scales with set size via a power-law (3/nS)^wm_alpha,
               capturing strong WM at low load and weak at high load.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values.
    - wm_weight: scalar in [0,1], base WM mixture weight when set size is minimal.
    - softmax_beta: inverse temperature for RL; internally scaled by 10.
    - kappa: nonnegative scalar, perseveration bias weight (stickiness) in RL policy.
    - wm_decay: scalar in [0,1], per-trial decay of WM toward uniform.
    - wm_alpha: nonnegative scalar, exponent for set-size power-law gating of WM weight.

    Returns:
    - Negative log-likelihood of observed actions given the model.
    """
    lr, wm_weight, softmax_beta, kappa, wm_decay, wm_alpha = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # track last chosen action per state; -1 means none yet

        # Set-size power-law gating for WM weight
        wm_weight_eff = wm_weight * (3.0 / float(nS))**wm_alpha

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with perseveration bias: implement by adding bias to Q_s before softmax
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                # Convert stickiness kappa (in policy units) to Q units by dividing by beta
                Q_s[last_action[s]] += (kappa / max(softmax_beta, 1e-8))
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy over traces
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay and reward-boosted trace update
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward boosts trace for chosen action; small anti-boost for others to sharpen
            boost = 0.5  # fixed within-model gain for reward sharpening
            if r > 0.0:
                w[s, a] += boost * (1.0 - w[s, a])
                others = [i for i in range(nA) if i != a]
                for o in others:
                    w[s, o] += boost * (0.0 - w[s, o])

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p