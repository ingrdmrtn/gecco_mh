def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with WM lapse under high set size and WM decay.

    Model summary:
    - RL system: standard Q-learning with softmax.
    - WM system: stores the most recently rewarded action per state with fast, high-beta softmax.
      It decays toward uniform and is less available under high set size (nS=6) via a lapse.
    - Policy: mixture of WM and RL with a fixed mixture weight.
    
    Parameters (model_parameters):
    - lr: RL learning rate for Q-values and WM encoding strength (0..1).
    - wm_weight: Mixture weight between WM and RL (0..1).
    - softmax_beta: Inverse temperature for RL (rescaled internally by 10).
    - wm_alpha: WM encoding strength when reward is 1 (0..1).
    - wm_decay: Per-trial decay of WM toward uniform for the visited state (0..1).
    - wm_lapse6: Lapse probability inside the WM policy at set size 6 (0..1). For set size 3, lapse=0.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, wm_decay, wm_lapse6 = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM lapse depends on set size (0 for 3, wm_lapse6 for 6)
        wm_lapse = 0.0 if nS <= 3 else np.clip(wm_lapse6, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action with internal lapse to uniform
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - wm_lapse) * p_wm_soft + wm_lapse * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the visited state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding on correct feedback: move toward one-hot on chosen action
            if r > 0.5:
                # push chosen action up and renormalize implicitly by softmax usage (keep values bounded)
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-dependent WM noise and state-wise perseveration bias.

    Model summary:
    - RL: Q-learning with softmax.
    - WM: stores last rewarded action per state as a strong memory; on non-reward, memory decays.
    - WM internal noise (eta) increases with set size: eta = sigmoid(eta_base + eta_slope*(nS-3)).
      WM policy is a mixture of near-deterministic memory softmax and uniform with weight eta.
    - WM perseveration bias: tendency to repeat the last action taken in that state (adds to WM logits).
    - Policy: mixture of WM and RL with fixed wm_weight.

    Parameters (model_parameters):
    - lr: RL learning rate and WM forgetting on errors (0..1).
    - wm_weight: Mixture weight between WM and RL (0..1).
    - softmax_beta: Inverse temperature for RL (rescaled internally by 10).
    - eta_base: Baseline WM lapse/noise at set size 3 (real number, passed through sigmoid).
    - eta_slope: Increase in WM lapse per +3 items in set size (real number).
    - pers_bias: Additive bias on WM values for the last action in that state (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, eta_base, eta_slope, pers_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM store
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track last action per state for perseveration
        last_action = -np.ones(nS, dtype=int)

        # Capacity-dependent WM noise
        eta = 1.0 / (1.0 + np.exp(-(eta_base + eta_slope * (nS - 3))))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # Add perseveration bias for last action in this state (if any)
            if last_action[s] >= 0:
                W_s[last_action[s]] += max(0.0, pers_bias)

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with internal noise to uniform
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - eta) * p_wm_soft + eta * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            if r > 0.5:
                # Overwrite toward chosen action on correct feedback
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Forget toward uniform on errors with rate lr
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

            # Update last action tracker after observing this trial
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with binding errors (state misbinding) increasing under higher set size.

    Model summary:
    - RL: Q-learning with softmax.
    - WM: stores rewarded actions; also subject to two interference processes:
        (a) Retrieval binding error: with probability epsilon (larger at set size 6),
            WM retrieves a random other state's memory instead of the correct state.
        (b) Encoding interference: on rewarded trials, a fraction chi of the WM update
            leaks into other states uniformly (spreading activation).
    - Policy: mixture of WM and RL with fixed wm_weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight between WM and RL (0..1).
    - softmax_beta: Inverse temperature for RL (rescaled internally by 10).
    - wm_alpha: WM encoding strength toward the chosen action on reward (0..1).
    - epsilon6: Retrieval binding error probability at set size 6 (0..1). At set size 3, epsilon=0.
    - chi: Encoding interference fraction spread uniformly to other states on reward (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, epsilon6, chi = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        epsilon = 0.0 if nS <= 3 else np.clip(epsilon6, 0.0, 1.0)
        chi = np.clip(chi, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # Build a retrieval-weighted WM value by mixing correct state's WM with other states' WM
            W_correct = w[s, :]
            if nS > 1:
                # Average over other states' WM values as the misbound source
                others = [i for i in range(nS) if i != s]
                W_others_mean = np.mean(w[others, :], axis=0)
            else:
                W_others_mean = w[s, :]

            W_retrieval = (1.0 - epsilon) * W_correct + epsilon * W_others_mean

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy using potentially misbound memory
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_retrieval - W_retrieval[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with encoding interference
            if r > 0.5:
                # Update the correct state toward one-hot on chosen action
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
                # Leak a fraction chi of this update uniformly into other states
                if nS > 1 and chi > 0.0:
                    leak_vec = np.zeros(nA)
                    leak_vec[a] = wm_alpha
                    # Distribute across other states
                    for j in range(nS):
                        if j == s:
                            continue
                        w[j, :] = (1.0 - (chi / (nS - 1))) * w[j, :] + (chi / (nS - 1)) * (w_0[j, :] * (1.0 - np.sum(leak_vec)) + leak_vec)
            else:
                # On errors, slight relaxation toward uniform for the visited state
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p