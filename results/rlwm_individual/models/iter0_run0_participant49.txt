Below are three standalone cognitive models that combine reinforcement learning (RL) and working memory (WM) in different ways. Each function follows the requested template structure, returns the negative log-likelihood of the observed choices, and uses all parameters meaningfully. No imports are included (assume numpy as np is already imported).



def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity-limited, decaying working memory and set-size scaling.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM has a limited capacity K and decays toward a uniform baseline with rate wm_decay.
    - The effective WM mixture weight scales with set size nS as min(1, K/nS).
    - When a rewarded outcome is observed, WM stores the chosen action for that state
      with strength eta (creating a peaked distribution over actions for that state).

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], baseline contribution of WM to the choice policy.
    - softmax_beta: float >= 0, inverse temperature for RL; internally scaled by 10.
    - K: float > 0, WM capacity (how many items can be reliably maintained).
    - wm_decay: float in [0,1], leak of WM per encounter of a state (toward uniform).
    - eta: float in [0,1], WM storage strength when r=1 for the current state.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, K, wm_decay, eta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM choice
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action a under softmax(Q)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: softmax over W_s (deterministic if strong memory)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Set-sizeâ€“scaled WM weight: capacity-limited
            scale_capacity = min(1.0, max(0.0, K / max(1.0, nS)))
            wm_weight_eff = wm_weight * scale_capacity

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the encountered state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage on reward: concentrate mass on chosen action with strength eta
            if r > 0.0:
                w[s, :] = (1.0 - eta) * w_0[s, :]
                w[s, a] += eta

        blocks_log_p += log_p

    return -blocks_log_p



def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with load-dependent exploration (beta) and leaky WM.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM contributes with a fixed weight but the RL softmax temperature becomes
      more exploratory under higher set sizes (lower beta).
    - WM decays toward a uniform baseline at rate wm_decay and stores rewarded
      actions for the current state.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], mixture weight for WM (constant across load).
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - beta_load: float >= 0, load sensitivity; higher means more exploration at larger set sizes.
      Effective beta: softmax_beta / (1 + beta_load * (nS - 3)/3).
    - wm_decay: float in [0,1], leak of WM per encounter of a state (toward uniform).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, beta_load, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent beta (lower at larger set size)
        load_factor = 1.0 + beta_load * max(0.0, (nS - 3) / 3.0)
        beta_eff = softmax_beta / load_factor

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with load-dependent beta
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Constant WM weight mixture
            wm_weight_eff = wm_weight
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for the encountered state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage on reward
            if r > 0.0:
                # Equivalent to placing a peaked distribution on the chosen action
                eta = 1.0
                w[s, :] = (1.0 - eta) * w_0[s, :]
                w[s, a] += eta

        blocks_log_p += log_p

    return -blocks_log_p



def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM gated by load.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM contributes less under higher load via a power-law scaling (3/nS)^alpha_load.
      This approximates that WM becomes less available as set size increases.
    - WM decays toward uniform and stores rewarded actions for the current state.

    Parameters (model_parameters):
    - lr_pos: float in [0,1], RL learning rate for positive PE (r - Q > 0).
    - lr_neg: float in [0,1], RL learning rate for negative PE (r - Q < 0).
    - wm_weight: float in [0,1], baseline WM weight (before load scaling).
    - softmax_beta: float >= 0, inverse temperature for RL; internally scaled by 10.
    - alpha_load: float >= 0, load exponent; effective WM weight scales as
      wm_weight * (3 / nS) ** alpha_load.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, alpha_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent WM weight via power-law scaling
        base = 3.0 / max(1.0, float(nS))
        wm_weight_eff_block = wm_weight * (base ** alpha_load)
        wm_weight_eff_block = max(0.0, min(1.0, wm_weight_eff_block))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture policy with block-level load-scaled WM weight
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # WM simple leaky storage rule: mild decay each encounter
            wm_decay = 0.2  # small default leak to ensure use of the WM pathway
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage on reward: concentrate mass on chosen action
            if r > 0.0:
                eta = 1.0
                w[s, :] = (1.0 - eta) * w_0[s, :]
                w[s, a] += eta

        blocks_log_p += log_p

    return -blocks_log_p

Notes on differences and load effects:
- Model 1: WM effectiveness scales with capacity K relative to set size nS, plus decay and storage strength eta.
- Model 2: Load reduces RL inverse temperature (more exploration in large sets); WM weight is constant but WM itself decays.
- Model 3: RL has asymmetric learning rates; WM weight declines as a power of set size (3/nS)^alpha_load.

All three models assume WM is particularly beneficial when it has recently stored rewarded mappings for a state and is harmed by higher loads (either via weight scaling or exploration changes).