def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory (WM) with age-modulated capacity and decay.
    
    The model mixes a slow model-free RL system with a fast WM system that can cache
    state-action mappings when set size is within capacity. Older adults are modeled
    as having reduced effective WM capacity.
    
    Parameters
    ----------
    states : array-like (int)
        State index on each trial (0..nS-1 within block).
    actions : array-like (int)
        Chosen action on each trial (0..2).
    rewards : array-like (float/int in {0,1})
        Feedback on each trial.
    blocks : array-like (int)
        Block identifier per trial. Learning resets across blocks.
    set_sizes : array-like (int)
        Set size for the current trial/block (3 or 6).
    age : array-like or scalar
        Participant age. Used to set age group (older if >=45).
    model_parameters : sequence of 6 floats
        alpha_rl : RL learning rate in [0,1]
        beta     : Softmax inverse temperature (scaled by 10 internally)
        wm_base  : Base weight of WM in the mixture, in [0,1]
        capacity : WM capacity parameter (in items), e.g., around 3-6
        decay    : WM decay/leak per trial in [0,1]
        age_eff  : Proportional reduction of capacity if older (>=45), in [0,1]
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed sequence of choices.
    """
    alpha_rl, beta, wm_base, capacity, decay, age_eff = model_parameters
    beta = beta * 10.0
    # Determine age group
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM value table
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))

        # Age-modulated effective capacity
        eff_capacity = max(0.0, capacity * (1.0 - age_eff * is_older))

        # Compute WM mixture weight as a function of set size vs capacity
        # If set size <= capacity, WM_weight ~ wm_base; otherwise it decreases toward 0
        # Use a smooth min-capacity rule: weight = wm_base * min(1, eff_capacity / nS)
        wm_weight_block = wm_base * min(1.0, eff_capacity / max(1.0, float(nS)))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl_vec = np.exp(logits_rl)
            p_rl_vec = p_rl_vec / (np.sum(p_rl_vec) + eps)
            p_rl_a = p_rl_vec[a]

            # WM policy (softmax on W)
            logits_wm = beta * (W[s, :] - np.max(W[s, :]))
            p_wm_vec = np.exp(logits_wm)
            p_wm_vec = p_wm_vec / (np.sum(p_wm_vec) + eps)
            p_wm_a = p_wm_vec[a]

            # Mixture
            p_choice = wm_weight_block * p_wm_a + (1.0 - wm_weight_block) * p_rl_a
            total_log_p += np.log(max(p_choice, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM update: decay toward uniform; reinforced overwrite toward one-hot
            # Decay step (leak)
            W[s, :] = (1.0 - decay) * W[s, :] + decay * (1.0 / nA)

            # If rewarded, push WM toward one-hot at chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use full overwrite strength proportional to reward (fast WM learning)
                W[s, :] = (1.0 - decay) * W[s, :] + decay * target

            # Normalize W row to avoid drift
            row_sum = np.sum(W[s, :])
            if row_sum > 0:
                W[s, :] = W[s, :] / row_sum

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + lightweight WM cache + lapse, all modulated by age and load.
    
    The model combines:
    - RL with separate learning rates for positive and negative outcomes.
    - A one-shot WM cache that stores the most recently rewarded action per state.
    - A lapse process that is higher in older adults.
    
    Set size reduces both WM contribution and effective beta (more exploration under load).
    
    Parameters
    ----------
    states : array-like (int)
        State per trial.
    actions : array-like (int)
        Chosen action per trial (0..2).
    rewards : array-like (0/1)
        Reward feedback.
    blocks : array-like (int)
        Block labels; learning resets per block.
    set_sizes : array-like (int)
        Set size (3 or 6).
    age : array-like or scalar
        Participant age; older group is age >= 45.
    model_parameters : sequence of 6 floats
        alpha_pos    : RL learning rate for rewards (r=1)
        alpha_neg    : RL learning rate for non-rewards (r=0)
        beta_base    : Base inverse temperature (scaled by 10 internally)
        wm_w_base    : Base WM mixture weight in [0,1]
        lapse_base   : Base lapse rate before age modulation (logit-space)
        age_slope    : Age modulation factor applied to both beta and lapse for older group
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_base, wm_w_base, lapse_base, age_slope = model_parameters
    # Age group
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    # Age-modulated inverse temperature and lapse
    beta = (beta_base * 10.0) / (1.0 + age_slope * is_older)
    # Lapse in probability space via sigmoid; increase with age
    lapse_logit = lapse_base + age_slope * is_older
    lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
    lapse = min(max(lapse, 0.0), 0.49)  # cap lapse to <0.5 for numerical stability

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM structures
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM cache: probability vectors; start uniform
        W = (1.0 / nA) * np.ones((nS, nA))

        # Load-modulated parameters
        # Larger set sizes reduce effective beta and WM weight
        beta_eff = beta / (1.0 + 0.5 * max(0, nS - 3))
        wm_weight = wm_w_base / (1.0 + max(0, nS - 3))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy: emphasize last rewarded action, otherwise gradually leak to uniform
            logits_wm = beta_eff * (W[s, :] - np.max(W[s, :]))
            p_wm = np.exp(logits_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = (1.0 - lapse) * p_mix[a] + lapse * (1.0 / nA)
            total_log_p += np.log(max(p_choice, eps))

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            lr = alpha_pos if r > 0.5 else alpha_neg
            Q[s, a] += lr * pe

            # WM update:
            # If rewarded, set a sharp one-hot cache on chosen action.
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0
            else:
                # On non-reward, leak toward uniform to forget unreliable mapping
                leak = 0.2  # fixed small leak
                W[s, :] = (1.0 - leak) * W[s, :] + leak * (1.0 / nA)

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian working memory (Dirichlet counts) with age-modulated forgetting and capacity-based mixing.
    
    The WM system maintains Dirichlet counts over actions for each state that are updated
    in a reward-contingent manner, with exponential forgetting. Older adults are modeled
    as having higher effective forgetting. The policy mixes WM's predictive distribution
    with RL's softmax over Q-values. WM's influence decreases when set size exceeds capacity.
    
    Parameters
    ----------
    states : array-like (int)
        State per trial.
    actions : array-like (int)
        Chosen action per trial (0..2).
    rewards : array-like (0/1)
        Feedback per trial.
    blocks : array-like (int)
        Block labels; learning resets per block.
    set_sizes : array-like (int)
        Set size (3 or 6).
    age : array-like or scalar
        Participant age; older group is age >= 45.
    model_parameters : sequence of 6 floats
        alpha_rl   : RL learning rate in [0,1]
        beta       : Inverse temperature for RL softmax (scaled by 10 internally)
        wm_prior   : Prior strength for Dirichlet counts (>0), e.g., 1.0
        forgetting : Base forgetting rate per trial in [0,1]
        capacity   : Capacity parameter controlling WM mixture via load
        age_eff    : Multiplier on forgetting if older (>=45); forgetting*(1+age_eff)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta, wm_prior, forgetting, capacity, age_eff = model_parameters
    beta = beta * 10.0
    # Age group and forgetting modulation
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0
    f_eff_mult = 1.0 + age_eff * is_older

    eps = 1e-12
    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM (Dirichlet counts)
        Q = (1.0 / nA) * np.ones((nS, nA))
        counts = wm_prior * np.ones((nS, nA))  # Dirichlet alpha parameters

        # WM mixture weight as a smooth function of capacity vs load
        # w = sigmoid(capacity - nS) scaled to [0,1]
        w_wm = 1.0 / (1.0 + np.exp(-(capacity - float(nS))))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # WM predictive distribution (Dirichlet mean)
            wm_probs = counts[s, :] / (np.sum(counts[s, :]) + eps)

            # RL softmax
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Mixture
            p_mix = w_wm * wm_probs + (1.0 - w_wm) * p_rl
            p_choice = p_mix[a]
            total_log_p += np.log(max(p_choice, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM update with age-modulated forgetting and reward-contingent increments:
            # Apply forgetting (shrink toward prior)
            f_eff = min(1.0, forgetting * f_eff_mult)
            # Decay counts toward prior wm_prior equally for numerical stability:
            counts[s, :] = (1.0 - f_eff) * counts[s, :] + f_eff * wm_prior

            # Add outcome-dependent evidence:
            if r > 0.5:
                # Reward: increase evidence for chosen action
                counts[s, a] += 1.0
            else:
                # No reward: increase evidence for alternatives (implicit negative evidence)
                alt = [aa for aa in range(nA) if aa != a]
                incr = 1.0 / len(alt)
                for aa in alt:
                    counts[s, aa] += incr

    return -float(total_log_p)