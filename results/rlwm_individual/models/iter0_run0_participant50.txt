def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with age-modulated capacity and lapse-to-nonresponse.
    
    This model assumes choices arise from a mixture of:
      - Reinforcement learning (RL) over Q-values
      - A fast, capacity-limited working-memory (WM) store that can encode stimulus-action mappings,
        with effectiveness that falls as set size exceeds capacity and with age-related reductions.
      - Lapses that yield a no-response action (-2)
    
    Negative log-likelihood of the observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within block (0..set_size-1).
    actions : array-like of int
        Chosen action on each trial. Valid actions are {0,1,2}. -2 denotes a no-response/lapse.
    rewards : array-like of float or int
        Feedback on each trial. Can be -1, 0, or 1. Positive (>0) indicates correct.
    blocks : array-like of int
        Block index per trial. States reset across blocks.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like of float or int
        Participant age; age[0] is used. Older group is age >= 45.
    model_parameters : tuple/list
        (alpha, beta, wm_strength, K_base, gamma_age, lapse)
        - alpha: RL learning rate in [0,1]
        - beta: softmax inverse temperature (>0), internally scaled up for sharper choice
        - wm_strength: base WM contribution weight in [0,1]
        - K_base: baseline WM capacity (in items), effective capacity scales this by age
        - gamma_age: fractional reduction in capacity for older adults (0..1); K_eff = K_base*(1 - gamma_age*is_old)
        - lapse: probability of a lapse yielding a -2 response
    
    Returns
    -------
    float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, wm_strength, K_base, gamma_age, lapse = model_parameters
    beta_eff = beta * 10.0
    is_old = 1.0 if age[0] >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM is a probabilistic policy per state; start as uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        # Track whether WM has a confident item for a state (for decays)
        wm_conf = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Effective capacity and WM weight as a function of set size and age
            K_eff = max(0.0, K_base * (1.0 - gamma_age * is_old))
            load_factor = min(1.0, K_eff / max(1, nS_t))
            wm_weight_eff = wm_strength * load_factor

            # RL softmax policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(beta_eff * Qc)
            p_rl_vec = expQ / np.sum(expQ)

            # WM policy (deterministic if confident, else uses its current distribution)
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            expW = np.exp(10.0 * Wc)  # WM is sharp if confident
            p_wm_vec = expW / np.sum(expW)

            # Mixture policy over task responses (0..2)
            p_task_vec = wm_weight_eff * p_wm_vec + (1.0 - wm_weight_eff) * p_rl_vec

            # Combine with lapse-to-no-response (-2)
            # If a == -2: probability is lapse
            # If a in 0..2: probability is (1 - lapse) * p_task_vec[a]
            if a == -2:
                p_total = max(1e-12, min(1.0, lapse))
            else:
                a_clip = int(np.clip(a, 0, nA - 1))
                p_total = max(1e-12, (1.0 - lapse) * p_task_vec[a_clip])

            log_p += np.log(p_total)

            # Learning updates only if an action from task set was chosen
            if a in [0, 1, 2]:
                # RL update
                r_bin = 1.0 if r > 0 else 0.0
                delta = r_bin - Q_s[a]
                q[s, a] = Q_s[a] + alpha * delta

                # WM update:
                # If rewarded, encode with high confidence; if not, weaken WM for this state.
                if r_bin > 0.5:
                    # Store one-hot in WM with strong confidence
                    w[s, :] = 1e-6
                    w[s, a] = 1.0 - (nA - 1) * 1e-6  # stays normalized
                    wm_conf[s] = 1.0
                else:
                    # On error, reduce confidence and flatten slightly
                    w[s, :] = 0.9 * w[s, :] + 0.1 * (1.0 / nA)
                    wm_conf[s] = max(0.0, wm_conf[s] - 0.25)

            # Passive WM decay across time, faster under overload and in older adults
            decay_base = 0.02 + 0.08 * is_old
            overload = max(0.0, (nS_t - K_eff) / max(1.0, nS_t))
            decay = np.clip(decay_base + 0.1 * overload, 0.0, 0.25)
            if wm_conf[s] > 0.0:
                # Relax WM distribution slightly toward uniform
                w[s, :] = (1.0 - decay) * w[s, :] + decay * (1.0 / nA)
                wm_conf[s] = max(0.0, wm_conf[s] - decay)

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates, perseveration, age- and load-modulated exploration, and lapse-to-nonresponse.
    
    This model is a pure RL account augmented with:
      - Asymmetric learning rates for positive vs. non-positive outcomes
      - Action perseveration (stickiness) within each state
      - Age and load dependent reduction of inverse temperature (more noise for older adults under high load)
      - Lapses that lead to -2 actions

    Negative log-likelihood of the observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block.
    actions : array-like of int
        Action chosen per trial; -2 indicates a lapse/nonresponse.
    rewards : array-like of float or int
        Feedback; positive (>0) is treated as correct.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float or int
        Participant age; older group is age >= 45.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, kappa, age_noise_scale, lapse)
        - alpha_pos: learning rate for positive outcomes (0..1)
        - alpha_neg: learning rate for non-positive outcomes (0..1)
        - beta: base inverse temperature (>0), scaled by load and age
        - kappa: perseveration weight added to the last taken action in a state
        - age_noise_scale: scales how much beta is reduced for older adults under higher set size
        - lapse: probability of lapse resulting in -2 response
    
    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, beta, kappa, age_noise_scale, lapse = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])
        q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration; initialize with -1 (none)
        last_a = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Age- and load-modulated temperature: larger set size lowers beta for older adults
            load_ratio = max(1.0, nS_t) / 3.0  # 1 for 3, 2 for 6
            beta_eff = beta * 10.0 / (1.0 + is_old * age_noise_scale * (load_ratio - 1.0))

            # RL values with perseveration bias
            Q_s = q[s, :].copy()
            if last_a[s] in [0, 1, 2]:
                Q_s[last_a[s]] += kappa  # bias toward repeating last action in this state

            # Softmax over task actions
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(beta_eff * Qc)
            p_task_vec = expQ / np.sum(expQ)

            # Combine with lapse-to-no-response
            if a == -2:
                p_total = max(1e-12, min(1.0, lapse))
            else:
                a_clip = int(np.clip(a, 0, nA - 1))
                p_total = max(1e-12, (1.0 - lapse) * p_task_vec[a_clip])

            log_p += np.log(p_total)

            # Learning and perseveration updates if a task action was chosen
            if a in [0, 1, 2]:
                r_pos = 1.0 if r > 0 else 0.0
                alpha_use = alpha_pos if r_pos > 0.5 else alpha_neg
                delta = r_pos - q[s, a]
                q[s, a] = q[s, a] + alpha_use * delta
                last_a[s] = a

        blocks_log_p += log_p

    return -float(blocks_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with gated state cache (WM), cache decay, and age/load-modulated lapse.
    
    A hybrid model where a 'cache' stores a recent action for a state via a gating mechanism,
    approximating a simple WM. Gating success depends on capacity and age. Cached entries decay
    over time. Choices mix between cache-driven policy and RL. Lapses increase with load for older adults.

    Negative log-likelihood of observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block.
    actions : array-like of int
        Chosen action per trial; -2 denotes no-response.
    rewards : array-like of float or int
        Feedback; positive (>0) treated as correct.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float or int
        Participant age; older group is age >= 45.
    model_parameters : tuple/list
        (alpha, beta, capacity_K, gate_noise, wm_decay, lapse_base)
        - alpha: RL learning rate (0..1)
        - beta: inverse temperature for RL (>0), scaled internally
        - capacity_K: WM-like cache capacity in items
        - gate_noise: reduces cache gating for older adults; effective gate p *= (1 - gate_noise*is_old)
        - wm_decay: per-trial decay rate of cached entries (0..1)
        - lapse_base: baseline lapse probability; increased by load for older adults
    
    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, capacity_K, gate_noise, wm_decay, lapse_base = model_parameters
    beta_eff_base = beta * 10.0
    is_old = 1.0 if age[0] >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # Cache: for each state, a vector of strengths over actions (starts uniform low)
        cache = (1.0 / nA) * np.ones((nS, nA))
        cache_conf = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_t = int(block_set_sizes[t])

            # Effective gating probability: capacity-limited and age-reduced
            gate_p = min(1.0, max(0.0, capacity_K / max(1, nS_t)))
            gate_p *= max(0.0, 1.0 - gate_noise * is_old)

            # Lapse increases with load for older adults
            load_ratio = max(1.0, nS_t) / 3.0
            lapse_eff = np.clip(lapse_base * (1.0 + is_old * (load_ratio - 1.0)), 0.0, 0.5)

            # RL policy
            Q_s = q[s, :]
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(beta_eff_base * Qc)
            p_rl_vec = expQ / np.sum(expQ)

            # Cache-driven policy: sharp if confident, else tends to uniform
            C_s = cache[s, :]
            Cc = C_s - np.max(C_s)
            expC = np.exp(10.0 * Cc)
            p_cache_vec = expC / np.sum(expC)

            # Mix cache and RL proportionally to cache confidence and gate
            cache_weight = np.clip(cache_conf[s] * gate_p, 0.0, 1.0)
            p_task_vec = cache_weight * p_cache_vec + (1.0 - cache_weight) * p_rl_vec

            # Combine with lapse-to-no-response
            if a == -2:
                p_total = max(1e-12, lapse_eff)
            else:
                a_clip = int(np.clip(a, 0, nA - 1))
                p_total = max(1e-12, (1.0 - lapse_eff) * p_task_vec[a_clip])

            log_p += np.log(p_total)

            # Learning and cache updates if a task action was chosen
            if a in [0, 1, 2]:
                r_bin = 1.0 if r > 0 else 0.0

                # RL update
                delta = r_bin - q[s, a]
                q[s, a] = q[s, a] + alpha * delta

                # Cache gating and update: store last chosen action, stronger if rewarded
                if np.random.rand() < gate_p:  # stochastic gating into WM-like cache
                    # Encode chosen action with strength proportional to reward
                    target_strength = 0.9 if r_bin > 0.5 else 0.6
                    cache[s, :] = (1.0 - target_strength) / (nA - 1)
                    cache[s, a] = target_strength
                    cache_conf[s] = min(1.0, 0.5 * cache_conf[s] + (0.5 if r_bin > 0.5 else 0.25))
                else:
                    # No update to cache content, but confidence may slowly fade
                    cache_conf[s] = max(0.0, cache_conf[s] - 0.1)

            # Cache decay each trial for visited state
            cache[s, :] = (1.0 - wm_decay) * cache[s, :] + wm_decay * (1.0 / nA)
            cache_conf[s] = max(0.0, cache_conf[s] - wm_decay)

        blocks_log_p += log_p

    return -float(blocks_log_p)