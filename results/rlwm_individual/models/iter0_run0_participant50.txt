def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory (WM) mixture with WM decay.

    Policy:
    - RL: softmax over Q-values with inverse-temperature softmax_beta*10.
    - WM: sharp softmax (beta=50) over WM weights, but diluted by a capacity factor:
        p_slot = min(1, capacity / set_size).
        Effective WM policy = p_slot * softmax(W) + (1 - p_slot) * uniform.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Learning/Updating:
    - RL: delta-rule with learning rate lr.
    - WM: global decay toward a uniform baseline w_0 with wm_decay each trial,
           then state-specific update toward a target vector t_s:
           t_s = w_0[s,:]; t_s[a] = r
           w[s,:] <- (1 - wm_lr) * w[s,:] + wm_lr * t_s

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: mixture weight of WM vs RL (0..1).
    - softmax_beta: base inverse-temperature; internally scaled by 10 for RL.
    - wm_lr: WM learning rate toward target trace (0..1).
    - wm_decay: per-trial WM decay toward uniform (0..1).
    - capacity: WM capacity in number of items (e.g., 1..6); shapes dilution with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy for chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with capacity dilution
            W_s = w[s, :]
            p_soft_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_slot = min(1.0, float(capacity) / float(nS))
            p_wm = p_slot * p_soft_wm + (1.0 - p_slot) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM state-specific update toward target memory t
            t_vec = np.copy(w_0[s, :])
            t_vec[a] = r
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * t_vec

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with state activation and set-size interference.

    Policy:
    - RL: softmax over Q with inverse-temperature softmax_beta*10.
    - WM: sharp softmax over W, with an effective inverse-temperature:
        beta_wm_eff = 50 * activation[s] * gamma(set_size)
      where:
        activation[s] in [0,1] reflects recency/strength of WM for state s,
        updated to 1 on visit and decays by (1 - decay_lambda) each trial.
        gamma(set_size) = 1 / (1 + phi * (set_size - 3)) attenuates WM under load.
      If beta_wm_eff is near 0, WM policy approaches uniform.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Learning/Updating:
    - RL: delta-rule with learning rate lr.
    - WM: per-trial state activation decay; WM values updated toward a target vector
           with persistence wm_persist:
           t[a] = r, others = 0; w[s,:] <- (1 - wm_persist)*w[s,:] + wm_persist*t

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: mixture weight of WM vs RL (0..1).
    - softmax_beta: base inverse-temperature; internally scaled by 10 for RL.
    - phi: set-size interference strength; higher => stronger WM attenuation with larger sets.
    - decay_lambda: activation decay rate per trial (0..1).
    - wm_persist: WM update persistence toward the target trace (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, phi, decay_lambda, wm_persist = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # start with no strong WM traces
        w_0 = np.zeros((nS, nA))
        activation = np.zeros(nS)  # per-state WM activation in [0,1]

        # Set-size attenuation factor for this block
        gamma = 1.0 / (1.0 + phi * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Activation decay
            activation *= (1.0 - decay_lambda)

            # WM softmax with activation- and set-size-adjusted temperature
            beta_wm_eff = softmax_beta_wm * max(activation[s], 1e-6) * gamma
            W_s = w[s, :]
            if beta_wm_eff > 1e-6:
                p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            else:
                p_wm = 1.0 / nA  # effectively uniform if no activation

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: set activation and move toward target trace
            activation[s] = 1.0
            t_vec = np.zeros(nA)
            t_vec[a] = r
            w[s, :] = (1.0 - wm_persist) * w[s, :] + wm_persist * t_vec

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with reward-asymmetric learning + slot-like WM buffer with forgetting.

    Policy:
    - RL: softmax over Q with inverse-temperature softmax_beta*10.
    - WM: capacity-limited buffer. A given state's WM availability is modeled
           as p_in = min(1, K / set_size). If in WM, policy is sharp softmax over W;
           otherwise uniform. Thus:
           p_wm = p_in * softmax(W) + (1 - p_in) * uniform.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Learning/Updating:
    - RL: asymmetrical delta-rule:
        if r=1: lr_pos = lr
        if r=0: lr_neg = lr * eta
    - WM: per-trial forgetting toward uniform baseline (forget), and on rewarded trials
           store a one-hot trace of the chosen action (overwrite-like):
           if r==1: w[s,:] <- one_hot(a)
           if r==0: no strengthening, only forgetting operates this step

    Parameters (model_parameters):
    - lr: base RL learning rate for positive outcomes (0..1).
    - wm_weight: mixture weight of WM vs RL (0..1).
    - softmax_beta: base inverse-temperature; internally scaled by 10 for RL.
    - eta: negative learning-rate multiplier for RL (0..1), so lr_neg = lr * eta.
    - K: WM capacity (in items), shapes p_in vs set size.
    - forget: per-trial WM forgetting rate toward uniform (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, eta, K, forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-based inclusion probability for this block
        p_in = min(1.0, float(K) / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with slot-like inclusion
            W_s = w[s, :]
            p_soft_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_in * p_soft_wm + (1.0 - p_in) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            if r > 0.5:
                q[s, a] += lr * (r - Q_s[a])
            else:
                q[s, a] += (lr * eta) * (r - Q_s[a])

            # WM forgetting toward uniform
            w = (1.0 - forget) * w + forget * w_0

            # WM storage on reward (overwrite toward one-hot)
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p