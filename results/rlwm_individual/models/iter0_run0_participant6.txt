def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM hybrid with capacity- and age-modulated WM engagement and WM decay.

    The model assumes choices arise from a mixture of:
      - Reinforcement Learning (RL) with a softmax policy over state-action Q values.
      - A capacity-limited Working Memory (WM) system that stores the last rewarded action per state,
        but decays over time.

    WM engagement depends on effective capacity relative to the current set size, and on age group:
      - Effective WM capacity K_eff = wm_capacity * (1 + age_sign * age_effect), where age_sign=+1 for younger (<45) and -1 for older (>=45).
      - WM mixing weight p_wm = clip(K_eff / set_size, 0, 1).
    WM memory strength for each state decays by wm_decay per trial and is reset to 1 when a rewarded action is observed.

    A small lapse probability epsilon mixes in uniform random choice.

    Parameters (model_parameters):
      - alpha_rl: RL learning rate in (0,1).
      - beta: softmax inverse temperature (scaled up internally).
      - wm_capacity: baseline WM capacity (in items).
      - wm_decay: per-trial decay of WM memory strength in (0,1).
      - age_effect: scales how much younger vs older modulates WM capacity; positive => younger have higher K_eff, older lower.
      - epsilon: lapse probability mixed with a uniform policy.

    Inputs:
      - states: array of state indices (0..nS-1 within block).
      - actions: array of chosen actions (0..2).
      - rewards: array of rewards (0 or 1).
      - blocks: array of block indices (integers).
      - set_sizes: array of set sizes per trial (3 or 6).
      - age: array-like of length 1 or scalar with participant age in years.
      - model_parameters: list/tuple of parameters as above.

    Returns:
      - Negative log-likelihood of the observed choices under the model.
    """
    alpha_rl, beta, wm_capacity, wm_decay, age_effect, epsilon = model_parameters
    beta = beta * 10.0
    # Normalize/clip some parameters to safe ranges
    epsilon = np.clip(epsilon, 1e-6, 0.2)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    alpha_rl = np.clip(alpha_rl, 0.0, 1.0)

    # Age group
    if isinstance(age, (list, tuple, np.ndarray)):
        age_val = float(age[0])
    else:
        age_val = float(age)
    age_sign = 1.0 if age_val < 45 else -1.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM structures
        Q = np.zeros((nS, nA))  # optimistic/neutral start
        last_rewarded_action = -1 * np.ones(nS, dtype=int)  # -1 means no memory yet
        mem_strength = np.zeros(nS)  # in [0,1], decays over time

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)  # stabilize
            exp_q = np.exp(beta * q_s)
            prl = exp_q[a] / (np.sum(exp_q) + 1e-12)

            # WM engagement weight depends on set size and age
            K_eff = wm_capacity * (1.0 + age_sign * age_effect)
            p_wm = np.clip(K_eff / max(set_size, 1.0), 0.0, 1.0)

            # WM policy for this state
            if last_rewarded_action[s] >= 0 and mem_strength[s] > 0:
                # Confidence-weighted deterministic policy
                # With strength c, choose stored action with prob c + (1-c)/nA
                c = np.clip(mem_strength[s], 0.0, 1.0)
                p_mem = (c + (1.0 - c) / nA) if a == last_rewarded_action[s] else ((1.0 - c) / nA)
            else:
                p_mem = 1.0 / nA  # no memory

            # Mixture with lapse
            p_choice = (1.0 - epsilon) * (p_wm * p_mem + (1.0 - p_wm) * prl) + epsilon * (1.0 / nA)
            total_log_p += np.log(max(p_choice, 1e-12))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha_rl * delta

            # WM update: decay all states in block per trial, then store if rewarded
            mem_strength *= (1.0 - wm_decay)
            if r > 0.5:
                last_rewarded_action[s] = a
                mem_strength[s] = 1.0

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and age-modulated perseveration that weakens with set size.

    The model uses:
      - A softmax policy over Q-values.
      - Asymmetric learning rates for positive vs negative prediction errors.
      - A perseveration bias that adds a bonus to repeating the last action in a given state.
        The perseveration strength decreases with set size and depends on age group.

    Perseveration strength:
      kappa_eff = kappa * (1 + age_sign * age_effect) * (3.0 / set_size),
      where age_sign=+1 for younger (<45) and -1 for older (>=45). Thus, with positive age_effect,
      younger show stronger perseveration, and larger set sizes reduce it.

    A small lapse probability epsilon mixes in uniform random choice.

    Parameters (model_parameters):
      - alpha_pos: learning rate for positive PE (r - Q > 0).
      - alpha_neg: learning rate for negative PE (r - Q < 0).
      - beta: softmax inverse temperature (scaled up internally).
      - kappa: baseline perseveration strength.
      - age_effect: scales age modulation of perseveration.
      - epsilon: lapse probability mixed with a uniform policy.

    Inputs:
      - states: array of state indices (0..nS-1 within block).
      - actions: array of chosen actions (0..2).
      - rewards: array of rewards (0 or 1).
      - blocks: array of block indices.
      - set_sizes: array of set sizes per trial (3 or 6).
      - age: array-like of length 1 or scalar with participant age.
      - model_parameters: list/tuple of parameters as above.

    Returns:
      - Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, beta, kappa, age_effect, epsilon = model_parameters
    beta = beta * 10.0
    epsilon = np.clip(epsilon, 1e-6, 0.2)
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)

    # Age group
    if isinstance(age, (list, tuple, np.ndarray)):
        age_val = float(age[0])
    else:
        age_val = float(age)
    age_sign = 1.0 if age_val < 45 else -1.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            # Effective perseveration
            kappa_eff = kappa * (1.0 + age_sign * age_effect) * (3.0 / max(set_size, 1.0))

            # Compute softmax with action-bias term
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = kappa_eff

            logits = beta * Q[s, :] + bias
            logits = logits - np.max(logits)
            exp_logits = np.exp(logits)
            denom = np.sum(exp_logits) + 1e-12
            p_soft = exp_logits[a] / denom

            # Lapse mixture
            p_choice = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)
            total_log_p += np.log(max(p_choice, 1e-12))

            # Update last action
            last_action[s] = a

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with global forgetting and a 'rule' system, with age- and set-size-modulated rule engagement.

    The model mixes:
      - RL system: softmax over Q-values with per-trial global forgetting toward 0.
      - Rule system: once a rewarded action is observed for a state, it is stored as the 'rule'
        and chosen deterministically when the rule system is engaged.

    Rule engagement weight depends on set size and age group:
      - w_rule = clip((rule_base * (1 + age_sign * age_effect)) / set_size, 0, 1),
        where age_sign=+1 for younger (<45) and -1 for older (>=45). Thus, with positive age_effect,
        younger rely more on rules, and larger set sizes reduce rule engagement.

    RL forgetting:
      - On each trial, all Q-values decay by factor (1 - phi_eff), with
        phi_eff = phi * (set_size / 6.0) * (1 + 0.5 * (age_sign < 0) * age_effect),
        i.e., more forgetting for larger set sizes and for older participants when age_effect>0.

    A lapse probability epsilon mixes in uniform choice.

    Parameters (model_parameters):
      - alpha: RL learning rate in (0,1).
      - beta: softmax inverse temperature (scaled up internally).
      - phi: baseline forgetting rate per trial in (0,1).
      - rule_base: baseline rule engagement numerator (capacity-like).
      - age_effect: scales age modulation of rule engagement and forgetting.
      - epsilon: lapse probability.

    Inputs:
      - states: array of state indices (0..nS-1 within block).
      - actions: array of chosen actions (0..2).
      - rewards: array of rewards (0 or 1).
      - blocks: array of block indices.
      - set_sizes: array of set sizes per trial (3 or 6).
      - age: array-like of length 1 or scalar with participant age.
      - model_parameters: list/tuple of parameters as above.

    Returns:
      - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, phi, rule_base, age_effect, epsilon = model_parameters
    beta = beta * 10.0
    epsilon = np.clip(epsilon, 1e-6, 0.2)
    alpha = np.clip(alpha, 0.0, 1.0)
    phi = np.clip(phi, 0.0, 1.0)

    # Age group
    if isinstance(age, (list, tuple, np.ndarray)):
        age_val = float(age[0])
    else:
        age_val = float(age)
    age_sign = 1.0 if age_val < 45 else -1.0

    total_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        rule_action = -1 * np.ones(nS, dtype=int)  # -1 if no rule learned yet

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            # Compute effective forgetting this trial
            older = 1.0 if age_sign < 0 else 0.0
            phi_eff = phi * (set_size / 6.0) * (1.0 + 0.5 * older * max(age_effect, 0.0))
            phi_eff = np.clip(phi_eff, 0.0, 1.0)

            # Apply global forgetting to all Qs
            Q *= (1.0 - phi_eff)

            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s)
            prl = exp_q[a] / (np.sum(exp_q) + 1e-12)

            # Rule engagement weight
            w_rule = np.clip((rule_base * (1.0 + age_sign * age_effect)) / max(set_size, 1.0), 0.0, 1.0)

            # Rule policy
            if rule_action[s] >= 0:
                p_rule = 1.0 if a == rule_action[s] else 0.0
            else:
                p_rule = 1.0 / nA

            # Mixture with lapse
            p_choice = (1.0 - epsilon) * (w_rule * p_rule + (1.0 - w_rule) * prl) + epsilon * (1.0 / nA)
            total_log_p += np.log(max(p_choice, 1e-12))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Rule acquisition: if rewarded, store as rule
            if r > 0.5:
                rule_action[s] = a

    return -total_log_p