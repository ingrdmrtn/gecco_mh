def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working-memory (WM) mixture model with age-modulated WM capacity.

    Idea:
    - Choices are a mixture of a reinforcement learner (RL) and a capacity-limited WM store.
    - WM contributes more when the block set size is within the agentâ€™s effective capacity.
    - WM traces decay toward uniform when items are not refreshed.
    - Age modulates effective WM capacity: younger participants gain a modest capacity boost, older participants incur a capacity penalty.

    Parameters (model_parameters):
    - alpha: scalar in (0,1), RL learning rate.
    - beta: positive scalar, softmax inverse temperature for RL policy (higher = more deterministic).
    - wm_capacity_base: positive scalar, baseline WM capacity (in number of state-action pairs storable effectively).
    - wm_decay: scalar in (0,1), per-trial decay of WM traces toward uniform (higher = faster decay).
    - lapse: scalar in [0,1), lapse rate adding uniform noise to choices.

    Inputs:
    - states: np.array of shape (T,), integer state IDs per trial (0..nS-1 within block).
    - actions: np.array of shape (T,), integer chosen actions per trial (0..2).
    - rewards: np.array of shape (T,), binary reward per trial (0/1).
    - blocks: np.array of shape (T,), integer block index per trial.
    - set_sizes: np.array of shape (T,), integer set size per trial (3 or 6).
    - age: array-like with a single number (years).
    - model_parameters: list/tuple [alpha, beta, wm_capacity_base, wm_decay, lapse].

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, wm_capacity_base, wm_decay, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0  # scale for sharper RL; ensure positive
    lapse = min(max(lapse, 0.0), 0.2)  # modest cap for numerical stability
    age = float(age[0])

    # Age modulation of capacity:
    # Younger than 45: get +20% scaled by how far from 45; Older: -30% penalty.
    if age >= 45:
        age_factor = 1.0 - 0.30
    else:
        # linearly scale boost up to +20% at age 0, and 0% at 45
        age_factor = 1.0 + 0.20 * (45.0 - age) / 45.0
    eff_capacity_base = max(0.1, wm_capacity_base * age_factor)

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA))  # start unbiased
        W = (1.0 / nA) * np.ones((nS, nA))  # WM distribution per state starts uniform

        # Block-specific WM mixture weight from capacity vs set size
        # More WM reliance when capacity exceeds set size; cap at [0,1]
        wm_weight_block = max(0.0, min(1.0, eff_capacity_base / float(nS)))
        beta_wm = 5.0 * beta  # WM is sharp when informative

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s_ctr = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_ctr)
            pi_rl = exp_q / np.sum(exp_q)

            # WM policy (state-specific distribution W[s])
            # Sharpen WM via softmax over W logits to avoid zero-prob issues.
            w_s = W[s, :]
            # Avoid log(0): use a softmax over log(w_s + small)
            wm_logits = np.log(np.clip(w_s, 1e-8, 1.0))
            wm_logits -= np.max(wm_logits)
            exp_w = np.exp(beta_wm * wm_logits)
            pi_wm = exp_w / np.sum(exp_w)

            # Mixture with lapse
            pi_mix = wm_weight_block * pi_wm + (1.0 - wm_weight_block) * pi_rl
            pi_final = (1.0 - lapse) * pi_mix + lapse * (1.0 / nA)

            p_a = max(1e-12, float(pi_final[a]))
            total_log_p += np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay toward uniform
            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)

            # WM update on rewarded trials: store a sharp one-hot-like memory
            if r > 0.5:
                W[s, :] = (1e-4) / (nA - 1)
                W[s, a] = 1.0 - (nA - 1) * (1e-4) / (nA - 1)

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size-specific learning rate and age-modulated perseveration (choice stickiness).

    Idea:
    - Set size modulates learning rate (smaller sets allow faster learning).
    - Choices are generated via a softmax over Q-values plus a bias to repeat the last action
      taken in the same state (perseveration/stickiness).
    - Age modulates perseveration: older participants show more stickiness, younger less.

    Parameters (model_parameters):
    - alpha_small: RL learning rate for set size 3.
    - alpha_large: RL learning rate for set size 6.
    - beta: softmax inverse temperature (scaled up internally).
    - perseveration_base: baseline stickiness weight added to the last-chosen action per state.
    - lapse: lapse rate adding uniform noise to choices.

    Inputs:
    - states: np.array of shape (T,), integer state IDs per trial.
    - actions: np.array of shape (T,), integer chosen actions per trial.
    - rewards: np.array of shape (T,), binary reward per trial.
    - blocks: np.array of shape (T,), integer block index per trial.
    - set_sizes: np.array of shape (T,), integer set size per trial (3 or 6).
    - age: array-like with a single number (years).
    - model_parameters: list/tuple [alpha_small, alpha_large, beta, perseveration_base, lapse].

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha_small, alpha_large, beta, perseveration_base, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0
    lapse = min(max(lapse, 0.0), 0.2)
    age = float(age[0])

    # Age modulation for perseveration:
    # Older: +50% stickiness. Younger: scaled reduction up to -20% at age 0, 0% at 45.
    if age >= 45:
        persev_factor = 1.0 + 0.50
    else:
        persev_factor = 1.0 - 0.20 * (45.0 - age) / 45.0
    perseveration = max(0.0, perseveration_base * persev_factor)

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Select learning rate by set size
        alpha = alpha_small if nS == 3 else alpha_large

        # Initialize Q and last-chosen action per state
        Q = np.zeros((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Add stickiness bias to the last action chosen in this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = perseveration

            # Softmax over Q + bias
            logits = beta * (Q[s, :] + bias)
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            # Lapse mixture
            pi_final = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p_a = max(1e-12, float(pi_final[a]))
            total_log_p += np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update last action for this state
            last_action[s] = a

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + one-shot inference (OS) mixture with age- and set-size-dependent WM reliance and noise.

    Idea:
    - Maintain RL values via standard delta rule.
    - Maintain a sparse "one-shot" WM store: when a state-action is rewarded once, store it as a
      candidate correct mapping. WM policy is near-deterministic but with age-modulated noise.
    - The mixture weight between WM and RL depends on whether effective capacity exceeds set size
      via a sigmoid. Age modulates both effective capacity and WM noise.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: RL softmax inverse temperature (scaled internally).
    - capacity_base: baseline WM capacity (in items).
    - wm_noise: baseline WM choice noise (0..0.5), higher = noisier WM.
    - mix_slope: positive slope of the capacity-to-weight sigmoid (controls sharpness).
    - lapse: lapse rate adding uniform noise to choices.

    Inputs:
    - states: np.array of shape (T,), integer state IDs per trial.
    - actions: np.array of shape (T,), integer chosen actions per trial.
    - rewards: np.array of shape (T,), binary reward per trial.
    - blocks: np.array of shape (T,), integer block index per trial.
    - set_sizes: np.array of shape (T,), integer set size per trial (3 or 6).
    - age: array-like with a single number (years).
    - model_parameters: list/tuple [alpha, beta, capacity_base, wm_noise, mix_slope, lapse].

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    alpha, beta, capacity_base, wm_noise, mix_slope, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0
    lapse = min(max(lapse, 0.0), 0.2)
    mix_slope = max(1e-6, mix_slope)
    age = float(age[0])

    # Age effects:
    # Capacity: younger +20% scaled by distance to 45; older -30%.
    if age >= 45:
        capacity_factor = 1.0 - 0.30
        noise_factor = 1.0 + 0.50  # older: noisier WM
    else:
        capacity_factor = 1.0 + 0.20 * (45.0 - age) / 45.0
        noise_factor = 1.0 - 0.20 * (45.0 - age) / 45.0  # younger: cleaner WM
    capacity_eff_base = max(0.1, capacity_base * capacity_factor)
    wm_noise_eff_base = min(0.49, max(0.0, wm_noise * noise_factor))

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Mixture weight from capacity vs set size via sigmoid: w = sigmoid(k*(cap - nS))
        cap_vs_set = capacity_eff_base - float(nS)
        wm_weight = 1.0 / (1.0 + np.exp(-mix_slope * cap_vs_set))

        # Initialize RL and WM store
        Q = np.zeros((nS, nA))
        # For each state, -1 means unknown; otherwise store the action believed correct
        wm_map = -1 * np.ones(nS, dtype=int)

        # WM noise per block (could depend on size; here hold constant per block)
        wm_noise_eff = wm_noise_eff_base

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s_ctr = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_ctr)
            pi_rl = exp_q / np.sum(exp_q)

            # WM policy: if mapped, assign high prob to mapped action, else uniform
            if wm_map[s] >= 0:
                wm_a = wm_map[s]
                pi_wm = np.full(nA, wm_noise_eff / (nA - 1))
                pi_wm[wm_a] = 1.0 - wm_noise_eff
            else:
                pi_wm = np.ones(nA) / nA

            # Mixture + lapse
            pi_mix = wm_weight * pi_wm + (1.0 - wm_weight) * pi_rl
            pi_final = (1.0 - lapse) * pi_mix + lapse * (1.0 / nA)

            p_a = max(1e-12, float(pi_final[a]))
            total_log_p += np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update WM map on first reward or to reinforce mapping
            if r > 0.5:
                wm_map[s] = a

    return -total_log_p