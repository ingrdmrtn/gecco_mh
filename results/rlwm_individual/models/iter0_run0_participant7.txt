Below are three standalone cognitive models that follow the requested RL+WM template and return the negative log-likelihood of the observed choices. Each model uses all of its parameters (no more than 6 total) and includes load-dependent WM contributions.

Note: Assumes numpy as np is already imported in the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + WM with capacity-limited WM (logistic drop with set size) and global WM decay/encoding.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial.
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, capacity, kappa, wm_alpha)
        - lr: RL learning rate in [0,1]
        - wm_weight: baseline mixture weight of WM in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - capacity: WM capacity (around 3â€“5); logistic midpoint for load effect
        - kappa: slope (>0) of logistic load penalty on WM efficacy
        - wm_alpha: WM decay/encoding strength in [0,1]; controls both global decay to baseline
                    and encoding of rewarded action

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, capacity, kappa, wm_alpha = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight drops with load via a logistic centered at 'capacity'
        # higher nS -> lower effective WM weight
        eff_wm_weight_block = wm_weight / (1.0 + np.exp(kappa * (nS - capacity)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy likelihood for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values (deterministic-ish)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL, with load-adjusted WM weight
            p_total = eff_wm_weight_block * p_wm + (1.0 - eff_wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - wm_alpha) * w + wm_alpha * w_0
            # WM encoding of rewarded action (fast, one-shot-like)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + episodic WM with asymmetric RL learning rates and action perseveration.
    WM has item-capacity scaling: effective WM weight scales as min(1, capacity/nS).
    WM encodes last rewarded action per state (no decay); RL includes stickiness bias.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial.
    model_parameters : tuple
        (alpha_pos, alpha_neg, softmax_beta, wm_weight, capacity, tau)
        - alpha_pos: RL learning rate for positive prediction errors
        - alpha_neg: RL learning rate for negative prediction errors
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - wm_weight: baseline WM mixture weight in [0,1]
        - capacity: WM item capacity; scales WM as min(1, capacity/nS)
        - tau: action perseveration strength (bias toward last chosen action in a state)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight, capacity, tau = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Perseveration traces: last action per state (one-hot)
        b_pref = np.zeros((nS, nA))

        # Load-scaled WM weight: proportional to capacity / nS (capped at 1)
        eff_wm_weight_block = wm_weight * min(1.0, capacity / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL with perseveration bias toward last action in this state
            Q_s_eff = q[s, :] + tau * b_pref[s, :]
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s_eff - Q_s_eff[a])))

            # WM: encodes last rewarded action per state (episodic mapping)
            # If never rewarded, remains uniform (initialized).
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = eff_wm_weight_block * p_wm + (1.0 - eff_wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetry
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0.0 else alpha_neg
            q[s, a] += alpha * delta

            # WM update: on reward, store one-hot for this state (no decay)
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot

            # Update perseveration: last chosen action in this state
            b_pref[s, :] = 0.0
            b_pref[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + gated WM with exponential load penalty, WM Hebbian encoding with forgetting,
    and a global lapse component.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial.
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_decay_lambda, epsilon_lapse, beta_wm_scale)
        - lr: RL learning rate
        - wm_weight: baseline WM mixture weight in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled
        - wm_decay_lambda: WM decay/encoding strength in [0,1]; also sets load penalty slope
        - epsilon_lapse: lapse probability in [0,1] for uniform random choice
        - beta_wm_scale: scales WM inverse temperature (multiplies fixed high beta)

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay_lambda, epsilon_lapse, beta_wm_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0 * max(beta_wm_scale, 0.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM weight decreases exponentially with load beyond 3
        eff_wm_weight_block = wm_weight * np.exp(-wm_decay_lambda * max(0.0, nS - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL likelihood
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM likelihood
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture (RL + WM), then add lapse to uniform
            p_mix = eff_wm_weight_block * p_wm + (1.0 - eff_wm_weight_block) * p_rl
            p_total = (1.0 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting and encoding (Hebbian-like)
            # Decay toward uniform for the whole WM
            w = (1.0 - wm_decay_lambda) * w + wm_decay_lambda * w_0
            # If rewarded, strengthen the chosen action association for this state
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_decay_lambda) * w[s, :] + wm_decay_lambda * onehot

        blocks_log_p += log_p

    return -blocks_log_p