def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM capacity mixture with age- and set-size–dependent WM contribution.

    This model mixes a model-free reinforcement learner (RL) with a simple
    working-memory (WM) policy that stores the most recently rewarded action for each state.
    The WM contribution is capacity-limited and reduced in larger set sizes.
    Older adults are assumed to have reduced effective WM contribution.

    Negative log-likelihood of observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..nS-1 within each block).
    actions : array-like of int
        Action chosen at each trial (valid actions are 0,1,2; invalid/missed trials may be -2).
    rewards : array-like of float
        Reward observed at each trial (1 or 0; invalid trials may be -1).
    blocks : array-like of int
        Block index for each trial. State identities reset each block.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial; constant within a block.
    age : array-like of int or float
        Age of the participant, provided as a one-element array.
    model_parameters : tuple/list
        (alpha, beta, wm_base, capacity_k, lapse, older_reduction)
        - alpha: RL learning rate (0..1).
        - beta: Softmax inverse temperature for RL (positive).
        - wm_base: Baseline WM mixture weight (0..1).
        - capacity_k: WM capacity in items (0..6), determines how much WM can support at a given set size.
        - lapse: Lapse probability to guard against zero probabilities (0..0.2).
        - older_reduction: Proportional reduction of WM mixture if age >= 45 (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_base, capacity_k, lapse, older_reduction = model_parameters
    beta = max(1e-6, beta) * 10.0
    lapse = min(max(lapse, 1e-8), 0.49)  # keep bounded
    alpha = min(max(alpha, 0.0), 1.0)
    capacity_k = max(0.0, capacity_k)
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    total_loglik = 0.0
    nA = 3
    uniform = np.ones(nA) / nA

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.array(actions)[idx]
        block_rewards = np.array(rewards)[idx]
        block_states = np.array(states)[idx]
        block_set_sizes = np.array(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.ones((nS, nA)) / nA

        # WM store: -1 means unknown; else 0..2 is the memorized best action
        WM_store = -1 * np.ones(nS, dtype=int)

        # Effective WM weight per block, scaled by age and capacity
        wm_age_scale = 1.0 - older_reduction * is_older
        wm_capacity_scale = min(1.0, capacity_k / max(1.0, float(nS)))
        wm_weight_block = np.clip(wm_base * wm_age_scale * wm_capacity_scale, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Skip invalid trials (e.g., timeouts with action -2 or reward -1)
            if (a < 0) or (r < 0):
                # no updates from invalid trials
                continue

            # RL policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: peaked on stored action if available, otherwise uniform
            if WM_store[s] >= 0:
                wm_vec = np.full(nA, 0.0)
                # epsilon-soft for WM to avoid zero probabilities
                eps_wm = 0.05
                wm_vec[:] = eps_wm / (nA - 1)
                wm_vec[WM_store[s]] = 1.0 - eps_wm
                p_wm = wm_vec
            else:
                p_wm = uniform

            # Mixture with small lapse
            p_mix = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * uniform

            # Log-likelihood contribution
            p_a = max(1e-12, p_final[a])
            total_loglik += np.log(p_a)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update rule: store last rewarded action for this state
            # If reward is 1, commit action to WM; if reward is 0, clear memory for that state
            if r > 0.5:
                WM_store[s] = a
            else:
                # In distractor/large set sizes, failures may degrade WM faster:
                # Clear only if set size is large; otherwise keep previous if any.
                if nS >= 6:
                    WM_store[s] = -1

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Valence-asymmetric RL with set-size–dependent forgetting and choice stickiness,
    modulated by age group.

    This model is purely RL-based but includes:
    - Separate learning rates for positive and negative prediction errors.
    - Per-trial decay (forgetting) towards a neutral prior that increases with set size.
    - Choice stickiness that biases repeating the previous action within a block.
    - Older adults exhibit stronger forgetting (decay) than younger adults.

    Returns the negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..nS-1 within each block).
    actions : array-like of int
        Action chosen at each trial (valid 0,1,2; invalid/missed trials may be -2).
    rewards : array-like of float
        Reward observed at each trial (1 or 0; invalid trials may be -1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial; constant within a block.
    age : array-like of int or float
        Age of the participant, provided as a one-element array.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, decay_base, stickiness, age_forget)
        - alpha_pos: Learning rate for positive prediction errors (0..1).
        - alpha_neg: Learning rate for negative prediction errors (0..1).
        - beta: Softmax inverse temperature (positive).
        - decay_base: Baseline forgetting rate toward uniform (0..1 per trial).
        - stickiness: Bias added to the last chosen action (can be positive or negative).
        - age_forget: Proportional increase of decay if age >= 45 (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, decay_base, stickiness, age_forget = model_parameters
    beta = max(1e-6, beta) * 10.0
    alpha_pos = np.clip(alpha_pos, 0.0, 1.0)
    alpha_neg = np.clip(alpha_neg, 0.0, 1.0)
    decay_base = np.clip(decay_base, 0.0, 1.0)
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    total_loglik = 0.0
    nA = 3
    prior = np.ones(nA) / nA

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.array(actions)[idx]
        block_rewards = np.array(rewards)[idx]
        block_states = np.array(states)[idx]
        block_set_sizes = np.array(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Values and previous action per state
        Q = np.ones((nS, nA)) / nA
        prev_action = -np.ones(nS, dtype=int)

        # Effective decay scaled by set size and age
        set_scale = (float(nS) / 6.0)  # more forgetting when more items to keep track of
        decay_eff = np.clip(decay_base * (1.0 + age_forget * is_older) * set_scale, 0.0, 1.0)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            if (a < 0) or (r < 0):
                # invalid trial: no learning, no likelihood
                continue

            # Apply decay toward prior before choosing
            Q[s, :] = (1.0 - decay_eff) * Q[s, :] + decay_eff * prior

            # Sticky choice bias: add bias to logits for repeating previous action in this state
            stick_vec = np.zeros(nA)
            if prev_action[s] >= 0:
                stick_vec[prev_action[s]] = stickiness

            logits = beta * (Q[s, :] - np.max(Q[s, :])) + stick_vec
            p = np.exp(logits)
            p = p / np.sum(p)

            p_a = max(1e-12, p[a])
            total_loglik += np.log(p_a)

            # RL update with valence-asymmetric learning
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Update stickiness memory
            prev_action[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus (UCB-like), set-size and age–dependent exploration,
    lapse, and perseveration.

    The action values combine a learned Q-value with an uncertainty/exploration bonus
    inversely related to the number of visits to a state-action pair. The exploration bonus
    is scaled up in larger set sizes and modulated by age group (younger > older or vice versa
    depending on parameter sign). A small lapse rate mixes in a uniform policy. A perseveration
    bias favors repeating the previous action within the state.

    Returns the negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..nS-1 within each block).
    actions : array-like of int
        Action chosen at each trial (valid 0,1,2; invalid/missed trials may be -2).
    rewards : array-like of float
        Reward observed at each trial (1 or 0; invalid trials may be -1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (3 or 6) for each trial; constant within a block.
    age : array-like of int or float
        Age of the participant, provided as a one-element array.
    model_parameters : tuple/list
        (alpha, beta, bonus_weight, age_explore, lapse, perseveration)
        - alpha: RL learning rate (0..1).
        - beta: Softmax inverse temperature (positive).
        - bonus_weight: Base weight for the uncertainty bonus (>=0).
        - age_explore: Age modulation of exploration; exploration scaling factor is
                       (1+age_explore) if age<45 else (1-age_explore), clipped to >=0.
        - lapse: Lapse probability mixing uniform policy (0..0.2).
        - perseveration: Additive bias to repeat the previous action within a state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, bonus_weight, age_explore, lapse, perseveration = model_parameters
    beta = max(1e-6, beta) * 10.0
    alpha = np.clip(alpha, 0.0, 1.0)
    lapse = min(max(lapse, 1e-8), 0.49)
    bonus_weight = max(0.0, bonus_weight)
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    is_younger = 1.0 if age_val < 45 else 0.0
    # Age-dependent exploration scale; ensure non-negative
    age_scale = (1.0 + age_explore) * is_younger + (1.0 - age_explore) * (1.0 - is_younger)
    age_scale = max(0.0, age_scale)

    total_loglik = 0.0
    nA = 3
    uniform = np.ones(nA) / nA

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.array(actions)[idx]
        block_rewards = np.array(rewards)[idx]
        block_states = np.array(states)[idx]
        block_set_sizes = np.array(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = np.ones((nS, nA)) / nA
        counts = np.zeros((nS, nA))  # visit counts per state-action
        prev_action = -np.ones(nS, dtype=int)

        # Exploration is harder with larger set sizes: scale bonus up with set size
        set_scale = float(nS) / 6.0  # 0.5 for 3, 1.0 for 6
        bonus_scale = bonus_weight * age_scale * set_scale

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            if (a < 0) or (r < 0):
                continue

            # UCB-like bonus: higher when count is low
            bonus = 1.0 / np.sqrt(np.maximum(1.0, counts[s, :] + 1.0))
            bonus_vals = bonus_scale * bonus

            # Perseveration bias toward previous action for this state
            persev_vec = np.zeros(nA)
            if prev_action[s] >= 0:
                persev_vec[prev_action[s]] = perseveration

            logits = beta * (Q[s, :] + bonus_vals - np.max(Q[s, :] + bonus_vals)) + persev_vec
            p = np.exp(logits)
            p = p / np.sum(p)

            p_final = (1.0 - lapse) * p + lapse * uniform
            p_a = max(1e-12, p_final[a])
            total_loglik += np.log(p_a)

            # Learning and bookkeeping
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe
            counts[s, a] += 1.0
            prev_action[s] = a

    return -float(total_loglik)