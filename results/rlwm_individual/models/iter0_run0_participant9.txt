def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited Working Memory with age- and set-size–dependent arbitration.

    The model mixes a model-free RL controller with a capacity-limited WM store.
    WM contribution scales down with set size and with an age-group factor; younger adults
    have stronger effective WM reliance than older (age >= 45). WM stores perfect, quickly
    decaying item-action associations when rewarded, and otherwise drifts toward uniform.

    Parameters
    ----------
    states : array-like of int
        State (stimulus) index on each trial within the block. Assumed 0..(set_size-1).
    actions : array-like of int
        Chosen action on each trial, in {0,1,2}.
    rewards : array-like of int
        Binary reward on each trial in {0,1}.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (constant within block; either 3 or 6).
    age : array-like of float
        Age in years, array of length 1 or per-trial identical. Used to define age group.
        Younger: age < 45; Older: age >= 45.
    model_parameters : iterable of 6 floats
        alpha_rl: RL learning rate in [0,1].
        beta: inverse temperature (scaled internally) for RL softmax.
        wm_capacity: nominal WM capacity (slots), nonnegative real, used as a divisor vs set size.
        wm_decay: decay rate of WM traces toward uniform on each visit to a state in [0,1].
        wm_reliance_base: baseline WM reliance before scaling (mapped by logistic to 0..1).
        age_effect: multiplier in [0,1] controlling how much older adults reduce WM reliance.

    Returns
    -------
    float
        Negative log-likelihood of the observed action sequence.
    """
    alpha_rl, beta, wm_capacity, wm_decay, wm_reliance_base, age_effect = model_parameters
    # Scale beta to a reasonable range like the reference template
    beta = beta * 10.0
    # Age group
    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA))
        # WM stores a distribution over actions per state (starts uniform)
        W = (1.0 / nA) * np.ones((nS, nA))

        # Compute block-specific capacity factor: min(1, capacity / set_size)
        cap = max(0.0, wm_capacity)
        cap_factor = min(1.0, cap / float(nS))
        # Baseline WM reliance (0..1) via logistic, then scale by capacity and age group
        wm_base = 1.0 / (1.0 + np.exp(-wm_reliance_base))
        wm_weight_block = wm_base * cap_factor * (1.0 - age_effect * is_older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s_centered = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_centered)
            p_rl = exp_q / np.sum(exp_q)

            # WM policy from stored distribution
            w_s = W[s, :]

            # Mixture
            p_a = wm_weight_block * w_s[a] + (1.0 - wm_weight_block) * p_rl[a]
            nll -= np.log(max(p_a, 1e-12))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha_rl * delta

            # WM update: decay toward uniform whenever the state is visited
            W[s, :] = (1.0 - wm_decay) * W[s, :] + wm_decay * (1.0 / nA)

            # If rewarded, store a perfect association for this state (one-hot)
            if r > 0.5:
                oh = np.zeros(nA)
                oh[a] = 1.0
                W[s, :] = oh

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning, perseveration bias, and set-size/age–dependent lapses.

    The model is purely RL for values, but the action policy includes:
    - Perseveration: a transient bias toward repeating the most recent action (sticky choice).
    - Lapses: a mixture with uniform random choice, increasing with set size and for older adults.
    - Asymmetric learning rates for positive vs negative prediction errors.
    - Softmax inverse temperature scaled down under greater cognitive load (larger set size)
      and for older adults.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial in {0,1,2}.
    rewards : array-like of int
        Binary reward per trial {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Age in years. Younger: <45; Older: >=45.
    model_parameters : iterable of 6 floats
        alpha_pos: learning rate for rewarded trials.
        alpha_neg: learning rate for unrewarded trials.
        beta_base: base softmax inverse temperature (scaled internally).
        perseveration_weight: bias added to the most recent action's preference.
        lapse_base: baseline lapse propensity (mapped by logistic).
        age_effect: scales both lapse increase and beta decrease for older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta_base, perseveration_weight, lapse_base, age_effect = model_parameters
    # Base temperature scaling
    beta_base = beta_base * 10.0

    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_action = None  # for perseveration

        # Compute block-wise modifiers
        # Larger set size reduces effective beta; older further reduces
        size_factor = 3.0 / float(nS)  # 1.0 for 3, 0.5 for 6
        beta_block = beta_base * size_factor / (1.0 + age_effect * is_older)

        # Lapse rate: logistic baseline, scaled up by set size and age
        lapse_base_sig = 1.0 / (1.0 + np.exp(-lapse_base))
        # Normalize set size from 3->1, 6->2; then scale
        lapse_block = lapse_base_sig * (float(nS) / 3.0) * (1.0 + age_effect * is_older)
        # Keep within [0, 0.5) for numerical stability
        lapse_block = min(max(lapse_block, 0.0), 0.49)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Base preferences from Q
            prefs = Q[s, :].copy()
            # Add perseveration bias to last action
            if last_action is not None:
                prefs[last_action] += perseveration_weight

            # Softmax
            prefs_centered = prefs - np.max(prefs)
            exp_p = np.exp(beta_block * prefs_centered)
            p_soft = exp_p / np.sum(exp_p)

            # Lapse mixture with uniform
            p_a = (1.0 - lapse_block) * p_soft[a] + lapse_block * (1.0 / nA)
            nll -= np.log(max(p_a, 1e-12))

            # Update
            delta = r - Q[s, a]
            alpha = alpha_pos if r > 0.5 else alpha_neg
            Q[s, a] += alpha * delta

            # Update perseveration memory
            last_action = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Mixture-of-experts arbitration: model-free RL and a limited-slot cache (WM-like) with
    uncertainty-, set-size-, and age–dependent arbitration.

    - RL expert: standard delta-rule with softmax policy.
    - Cache expert: stores deterministic action mappings for a limited number of states
      (capacity constrained). Retrieved as a one-hot policy if stored; otherwise uniform.
    - Arbitration: weight assigned to the cache increases with RL uncertainty (entropy of RL policy)
      and decreases with set size; older adults are biased away from cache usage.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1).
    actions : array-like of int
        Chosen action per trial in {0,1,2}.
    rewards : array-like of int
        Binary reward per trial {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Age in years. Younger: <45; Older: >=45.
    model_parameters : iterable of 6 floats
        alpha: RL learning rate.
        beta: RL inverse temperature (scaled internally).
        arbitration_bias: bias term passed through logistic to set baseline cache weight.
        uncertainty_slope: scales the impact of RL uncertainty on arbitration.
        cache_capacity: nominal maximum number of states that can be cached (slots).
        age_effect: scales reduction of cache usage and capacity for older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, arbitration_bias, uncertainty_slope, cache_capacity, age_effect = model_parameters
    beta = beta * 10.0

    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        # Effective cache capacity per block with age penalty
        cap_nominal = max(0.0, cache_capacity)
        cap_eff = cap_nominal * (1.0 - 0.5 * age_effect * is_older)
        cap_slots = int(np.clip(np.round(cap_eff), 0, nS))

        # Cache data structures
        cached_action = -1 * np.ones(nS, dtype=int)  # -1 means not cached
        last_reinf_time = -1 * np.ones(nS, dtype=int)  # timestamps for eviction
        time_counter = 0
        n_cached = 0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            q_s_centered = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_centered)
            p_rl = exp_q / np.sum(exp_q)

            # RL uncertainty by entropy of p_rl (0..ln nA), normalize to [0,1]
            H = -np.sum(p_rl * np.log(np.clip(p_rl, 1e-12, 1.0)))
            H_norm = H / np.log(nA)

            # Set-size normalization: 3 -> 0, 6 -> 1
            size_norm = (float(nS) - 3.0) / 3.0

            # Cache policy
            if cached_action[s] >= 0:
                w_cache = np.zeros(nA)
                w_cache[cached_action[s]] = 1.0
            else:
                w_cache = np.ones(nA) / nA

            # Arbitration weight for cache via logistic of bias + features
            # Increase with uncertainty, decrease with set size and with older age
            logit_w = arbitration_bias + uncertainty_slope * (H_norm - size_norm) - (age_effect * is_older)
            w = 1.0 / (1.0 + np.exp(-logit_w))
            # Ensure within numeric limits
            w = min(max(w, 1e-6), 1.0 - 1e-6)

            # Mixture policy
            p_a = w * w_cache[a] + (1.0 - w) * p_rl[a]
            nll -= np.log(max(p_a, 1e-12))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Cache maintenance
            # If rewarded, install/refresh cache entry for this state
            if r > 0.5:
                if cached_action[s] < 0:
                    # Need to add a new entry if capacity allows, else evict oldest
                    if n_cached < cap_slots:
                        n_cached += 1
                    else:
                        # Evict the cached state with the smallest timestamp
                        # Only consider currently cached states
                        cached_indices = np.where(cached_action >= 0)[0]
                        if cached_indices.size > 0:
                            oldest_idx = cached_indices[np.argmin(last_reinf_time[cached_indices])]
                            cached_action[oldest_idx] = -1
                            last_reinf_time[oldest_idx] = -1
                    # Add/overwrite current state
                    cached_action[s] = a
                else:
                    # If already cached, update action to the recently rewarded one
                    cached_action[s] = a
                # Refresh timestamp
                last_reinf_time[s] = time_counter
            else:
                # If unrewarded and this state is cached to the chosen action, clear it
                if cached_action[s] == a:
                    cached_action[s] = -1
                    last_reinf_time[s] = -1
                    if n_cached > 0:
                        n_cached -= 1

            time_counter += 1

    return nll