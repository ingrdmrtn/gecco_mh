def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with set-size-dependent WM weighting and WM decay.

    Idea:
    - Decisions come from a mixture of a model-free RL system and a working-memory (WM) system.
    - WM contributes less as set size increases (capacity-limited), implemented as a power-law decrease.
    - WM stores stimulus-action associations with decay toward an uninformative prior.

    Parameters (6 total):
    - lr: scalar in [0,1], learning rate for RL Q-values.
    - wm_weight: base mixture weight for WM in low load (nS=3).
    - softmax_beta: inverse temperature for RL (scaled internally, higher -> more deterministic).
    - wm_decay: per-trial decay of WM values toward uniform (0=no decay, 1=full reset each trial).
    - wm_eta: per-update WM learning rate for the observed state-action pair.
    - capacity_k: exponent controlling how quickly WM weight falls with set size; effective WM weight scales as (3/nS)^capacity_k.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_eta, capacity_k = model_parameters
    softmax_beta *= 10.0  # higher dynamic range
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM mixture weight
        wm_weight_eff = wm_weight * (3.0 / nS) ** capacity_k
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax, stabilized relative to chosen action)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy (softmax on WM table)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)  # numerical guard
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uninformative prior
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM local update for current state-action
            # Move entire state's WM toward 0, then push chosen toward observed reward
            # This stores high value for rewarded action and low for others
            w[s, :] = (1.0 - wm_eta) * w[s, :]
            w[s, a] += wm_eta * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with confidence-gated WM contribution and set-size modulation.

    Idea:
    - RL provides continuous learning.
    - WM stores one-hot associations when reward is observed, with global decay.
    - WM contribution is dynamically gated by its "confidence" (how peaked the WM distribution is for this state),
      and by set size (larger set size reduces reliance on WM).

    Parameters (6 total):
    - lr: RL learning rate for Q-values.
    - softmax_beta: RL inverse temperature (scaled internally).
    - wm_weight: base maximum weight of WM when highly confident at nS=3.
    - wm_decay: WM decay toward uniform each trial (0=no decay, 1=reset).
    - wm_confidence: gain on confidence gating; higher makes the gate more sensitive to WM peakiness.
    - kappa_ns: set-size penalty exponent; effective weight scales as (3/nS)^kappa_ns.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, wm_decay, wm_confidence, kappa_ns = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Base set-size scaling
        setsize_scale = (3.0 / nS) ** kappa_ns
        setsize_scale = np.clip(setsize_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Confidence gating: peakiness of WM distribution for this state
            # Compute difference between top two WM values
            sorted_W = np.sort(W_s)[::-1]
            peak_gap = sorted_W[0] - sorted_W[1] if nA >= 2 else sorted_W[0]
            # Map to gate via logistic
            gate_conf = 1.0 / (1.0 + np.exp(-wm_confidence * (peak_gap - 0.0)))
            wm_gate = wm_weight * setsize_scale * gate_conf
            wm_gate = np.clip(wm_gate, 0.0, 1.0)

            # Mixture
            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update rule: when rewarded, store a one-hot association for that state
            if r > 0.5:
                # Move W_s toward a one-hot on chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend strongly with the one-hot; here we let decay control interference across trials,
                # and use a strong overwrite on reward
                w[s, :] = 0.0
                w[s, a] = 1.0
            # If not rewarded, leave WM as is after decay (no explicit encoding of wrong action)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + slot-limited WM mixture.

    Idea:
    - RL uses separate learning rates for positive and negative outcomes.
    - WM acts like a limited-capacity slot memory: only a subset of items can be perfectly stored.
      Effective WM influence scales with min(1, C_slots / nS), where C_slots is a capacity parameter.
    - WM entries decay toward uniform, and only rewarded actions are stored as one-hot patterns.

    Parameters (6 total):
    - lr_pos: RL learning rate for rewards (r=1).
    - lr_neg: RL learning rate for non-rewards (r=0).
    - softmax_beta: RL inverse temperature (scaled internally).
    - wm_weight: base WM mixture weight when nS <= C_slots.
    - wm_decay: WM decay toward uniform each trial.
    - C_slots: WM capacity in number of items (continuous allowed; effective fraction is min(1, C_slots/nS)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_decay, C_slots = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Slot-limited WM scaling by set size
        wm_scale = np.clip(C_slots / float(nS), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight * wm_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            if r >= 0.5:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: on reward, store perfect association for this state
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # On non-reward, do not store new information (after decay)

        blocks_log_p += log_p

    return -blocks_log_p