def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with uncertainty-based arbitration and sharpened WM policy.

    Arbitration: The effective WM weight is determined on each trial by comparing the
    entropies of the RL and WM policies: when WM is more certain (lower entropy),
    WM receives more weight; otherwise RL dominates. The arbitration uses a sigmoid
    with slope arb_slope and bias wm_weight.

    WM policy: Softmax over WM weights with a tunable sharpness (wm_sharpness) that
    scales how deterministically WM selects an action.

    WM updates: Decay toward uniform at rate wm_decay each trial; then a one-shot
    write that moves probability mass toward the chosen action if rewarded and away
    from it if not.

    Parameters (model_parameters):
    - lr: scalar in [0,1], learning rate for RL Q-values
    - wm_weight: base arbitration bias for WM (mapped via sigmoid with entropy diff)
    - softmax_beta: inverse temperature for RL policy (scaled by 10 internally)
    - wm_decay: per-trial decay of WM toward uniform (0=no decay, 1=instant reset)
    - arb_slope: slope of the arbitration sigmoid with entropy difference input
    - wm_sharpness: scales WM inverse temperature (>=0; 1 = default sharpness)
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, arb_slope, wm_sharpness = model_parameters
    softmax_beta *= 10.0  # higher upper bound as per template
    softmax_beta_wm = 50.0  # very deterministic base
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy likelihood of chosen action (as given)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Construct WM policy with adjustable sharpness
            beta_wm_eff = softmax_beta_wm * max(wm_sharpness, 0.0)
            logits_wm = beta_wm_eff * (W_s - W_s[a])
            denom_wm = np.sum(np.exp(logits_wm))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Uncertainty-based arbitration using entropy difference
            # Compute full distributions for entropy estimates
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            pi_rl = pi_rl / max(np.sum(pi_rl), 1e-12)
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))

            pi_wm = np.exp(beta_wm_eff * (W_s - np.max(W_s)))
            pi_wm = pi_wm / max(np.sum(pi_wm), 1e-12)
            H_wm = -np.sum(pi_wm * np.log(np.clip(pi_wm, 1e-12, 1.0)))

            # Arbitration weight for WM via sigmoid of entropy difference
            x = wm_weight + arb_slope * (H_rl - H_wm)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-x))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Reward-dependent one-shot write
            if r >= 0.5:
                # Move probability mass toward chosen action
                step = (1.0 - wm_decay)
                w[s, :] = (1.0 - step) * w[s, :]
                w[s, a] += step
            else:
                # Move probability away from chosen action
                step = 0.5 * (1.0 - wm_decay)
                redistribute = step / (nA - 1.0)
                w[s, a] = max(w[s, a] - step, 0.0)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

                # Renormalize for numerical safety
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + capacity-limited WM mixture.

    RL: Two learning rates for positive and negative prediction errors (lr_pos, lr_neg).
    WM: A mixture component that is strong when set size is below capacity and drops
    with a power-law when set size exceeds capacity.

    WM updates: Immediate binding on reward (strong write), and mild suppression of the
    chosen action on negative feedback. No passive decay beyond capacity gating.

    Parameters (model_parameters):
    - lr_pos: learning rate for positive prediction errors (r - Q > 0)
    - lr_neg: learning rate for negative prediction errors (r - Q < 0)
    - wm_weight: base WM weight when nS well below capacity (0..1 before gating)
    - softmax_beta: inverse temperature for RL (scaled by 10 internally)
    - wm_capacity: capacity parameter (effective number of states WM can handle)
    - capacity_gamma: exponent controlling how fast WM reliance declines with nS/capacity
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_capacity, capacity_gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-limited WM weight (power-law decline with set size)
        ratio = max(nS, 1) / max(wm_capacity, 1e-6)
        drop = min(ratio, 1e6) ** max(capacity_gamma, 0.0)
        gate = 1.0 / (1.0 + drop)  # ~1 when nS << capacity, ~0 when nS >> capacity
        wm_weight_eff = np.clip(wm_weight * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy likelihood of chosen action (as given)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with valence asymmetry
            delta = r - Q_s[a]
            if delta >= 0:
                q[s, a] += lr_pos * delta
            else:
                q[s, a] += lr_neg * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rewarded: strong binding of the chosen action
            if r >= 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Negative feedback: suppress chosen action mildly
                suppress = 0.5
                w[s, a] = (1.0 - suppress) * w[s, a]
                # Renormalize toward a proper distribution
                residual = 1.0 - w[s, a]
                redistribute = residual / (nA - 1.0)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = redistribute

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size–dependent WM decay and lapses.

    WM decay accelerates with set size, capturing load-induced memory degradation.
    A lapse component introduces occasional random choices whose probability rises
    with set size. WM write speed is the complement of its decay (faster writes when
    decay is small).

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values
    - wm_weight: base mixture weight for WM (constant across trials/blocks)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - wm_base_decay: baseline WM decay toward uniform per trial
    - decay_slope: additional decay per unit increase in set size
    - lapse_base: base log-odds controlling lapse probability modulated by set size
    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_base_decay, decay_slope, lapse_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM decay
        wm_decay_eff = np.clip(wm_base_decay + decay_slope * max(nS - 1, 0), 0.0, 1.0)
        # Lapse probability increases with set size
        lapse = 1.0 / (1.0 + np.exp(-(lapse_base + np.log(max(nS, 1)))))

        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy likelihood of chosen action (as given)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform controlled by set-size–dependent rate
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Write toward the chosen action; write speed is complement of decay
            write = (1.0 - wm_decay_eff)
            if r >= 0.5:
                # Strong consolidation when rewarded
                w[s, :] = (1.0 - write) * w[s, :]
                w[s, a] += write
            else:
                # Mild repulsion from the chosen action when not rewarded
                step = 0.5 * write
                w[s, a] = max(w[s, a] - step, 0.0)
                add = step / (nA - 1.0)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += add
                # Renormalize
                ssum = np.sum(w[s, :])
                if ssum > 0:
                    w[s, :] /= ssum

        blocks_log_p += log_p

    return -blocks_log_p