def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+Capacity-limited Working Memory mixture with age-dependent capacity and action stickiness.

    This model assumes choices are driven by a mixture of:
    - Reinforcement learning (RL) values updated via a learning rate (alpha),
    - A capacity-limited working memory (WM) that stores the last rewarded action per state.
    The WM contribution is scaled by the block set size via an effective capacity parameter K,
    which is age-sensitive. We also include an action stickiness (perseveration) bias.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1].
    - beta: Inverse temperature for RL softmax choice.
    - K_base: Baseline WM capacity (in number of items).
    - age_shift: Capacity decrement if participant is older (added to K_base; typically negative).
    - kappa: Perseveration bias added to the last chosen action in a given state.

    Inputs:
    - states: array of state indices per trial (0..set_size-1).
    - actions: array of chosen actions per trial (0,1,2).
    - rewards: array of received rewards per trial (0 or 1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (constant within a block, e.g., 3 or 6).
    - age: array-like with a single value: participant's age in years.
    - model_parameters: list/tuple of parameters described above.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, K_base, age_shift, kappa = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values initialized uniformly
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: -1 means no stored rewarded action yet
        wm_store = -1 * np.ones(nS, dtype=int)

        # Track last chosen action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Effective WM capacity adjusted by age group
        K_eff = K_base + age_shift * is_older
        if K_eff < 0:
            K_eff = 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            ss = int(block_set_sizes[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += kappa
            # Softmax probability of chosen action (stable formulation)
            denom = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom, 1e-12)

            # WM policy: if there is a stored rewarded action for this state, choose it
            lapse = 0.05  # small WM lapse
            if wm_store[s] >= 0:
                wm_a = wm_store[s]
                if a == wm_a:
                    p_wm = 1.0 - lapse
                else:
                    p_wm = lapse / (nA - 1)
            else:
                # No info in WM: uniform
                p_wm = 1.0 / nA

            # Capacity-limited WM weight: proportional to K_eff / set size (capped at 1)
            wm_weight = min(1.0, K_eff / max(ss, 1))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += alpha * delta

            # WM update: store only if rewarded
            if r > 0.5:
                wm_store[s] = a

            # Update perseveration tracker
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with Win-Stay/Lose-Shift heuristic, set-size and age modulations.

    Choices arise from a mixture of:
    - Asymmetric RL: separate learning rates for positive and negative prediction errors.
    - A Win-Stay/Lose-Shift (WSLS) heuristic acting like a fast, load-sensitive process.

    Set-size effect: WSLS mixture weight decays with larger set sizes (3 vs 6).
    Age effect: older adults show reduced negative learning rate (less sensitivity to losses).

    Parameters (model_parameters):
    - alpha_pos: Learning rate for positive prediction errors.
    - alpha_neg_base: Baseline learning rate for negative prediction errors.
    - beta: Inverse temperature for RL softmax.
    - w0: Baseline mixture weight for WSLS at set size 3 (in [0,1]).
    - gamma_age: Age modulation of negative learning rate (alpha_neg = alpha_neg_base * (1 - gamma_age * is_older)).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: list/tuple of parameters described above.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg_base, beta, w0, gamma_age = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    # Age-modulated negative learning rate
    alpha_neg = alpha_neg_base * (1.0 - gamma_age * is_older)
    alpha_neg = max(alpha_neg, 0.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # For WSLS heuristic: track last action and reward per state
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            ss = int(block_set_sizes[t])

            # RL policy
            Q_s = q[s, :]
            denom = np.sum(np.exp(beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom, 1e-12)

            # WSLS policy: prescribes repeating last rewarded action, otherwise shift
            eps_wsls = 0.1  # internal noise of heuristic
            if last_action[s] >= 0:
                if last_reward[s] > 0.5:
                    # Win-Stay: repeat last action
                    if a == last_action[s]:
                        p_wsls = 1.0 - eps_wsls
                    else:
                        p_wsls = eps_wsls / (nA - 1)
                else:
                    # Lose-Shift: avoid last action
                    if a == last_action[s]:
                        p_wsls = eps_wsls / (nA - 1)
                    else:
                        # Two alternative actions share mass 1-eps
                        p_wsls = (1.0 - eps_wsls) / (nA - 1)
            else:
                # No history: uniform
                p_wsls = 1.0 / nA

            # Mixture weight decays with set size; slight additional reduction if older
            w = w0 * (3.0 / max(ss, 1.0))
            w *= (1.0 - 0.2 * is_older)
            w = np.clip(w, 0.0, 1.0)

            p_total = w * p_wsls + (1.0 - w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += alpha_pos * pe
            else:
                q[s, a] += alpha_neg * pe

            # Update WSLS traces
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian RL with uncertainty-driven exploration, modulated by set size and age, plus stickiness.

    The agent maintains Beta(a,b) posteriors over reward probabilities for each state-action pair.
    Choice is guided by the expected value (mean of Beta) plus an uncertainty bonus (variance),
    with exploration tuned by set size and age. An action stickiness term biases repeating the last
    action in a state.

    Parameters (model_parameters):
    - prior: Symmetric Beta prior parameter (>0) for both alpha and beta (e.g., 1.0 is uniform).
    - beta: Inverse temperature for softmax over augmented values.
    - eta: Base weight on uncertainty bonus (higher -> more directed exploration).
    - age_shift: Additive increase in eta for older adults (eta_eff = (eta + age_shift*is_older)/set_size).
    - kappa: Stickiness bonus added to the last chosen action within a state.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified.
    - model_parameters: list/tuple of parameters described above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    prior, beta, eta, age_shift, kappa = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Beta-Bernoulli posterior parameters per (state, action)
        alpha_succ = prior * np.ones((nS, nA))
        beta_fail = prior * np.ones((nS, nA))

        # Stickiness tracker
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            ss = int(block_set_sizes[t])

            # Posterior mean and variance for each action in this state
            a_params = alpha_succ[s, :]
            b_params = beta_fail[s, :]
            means = a_params / (a_params + b_params)
            vars_ = (a_params * b_params) / (((a_params + b_params) ** 2) * (a_params + b_params + 1.0))

            # Uncertainty-driven exploration term
            eta_eff = (eta + age_shift * is_older) / max(ss, 1.0)
            augmented = means + eta_eff * vars_

            # Add stickiness to last chosen action
            if last_action[s] >= 0:
                augmented[last_action[s]] += kappa

            # Softmax probability of chosen action
            denom = np.sum(np.exp(beta * (augmented - augmented[a])))
            p = 1.0 / max(denom, 1e-12)
            log_p += np.log(p)

            # Bayesian update
            if r > 0.5:
                alpha_succ[s, a] += 1.0
            else:
                beta_fail[s, a] += 1.0

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p