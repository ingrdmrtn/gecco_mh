def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces plus capacity-limited, decay-prone working memory and age-sensitive interference.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within block.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Binary reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index for each trial; states reset per block.
    set_sizes : array-like of int
        Set size on that block (3 or 6).
    age : array-like (length 1)
        Age in years; used to form age group: younger (<45) vs older (>=45).
    model_parameters : list/tuple of length 6
        - lr_rl: RL learning rate for Q in [0,1]
        - beta: inverse temperature (>0) for RL softmax
        - lambda_et: eligibility-trace decay (0..1), higher = longer trace
        - wm_slots: effective WM capacity (continuous, 0..6), scales WM weight as slots / set_size
        - gate_strength: reduces WM decay/interference when larger (>0). Effective gating weaker in older group.
        - lapse: lapse probability (0..0.2) of choosing uniformly at random

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr_rl, beta, lambda_et, wm_slots, gate_strength, lapse = model_parameters
    beta = max(1e-6, beta) * 5.0
    lambda_et = np.clip(lambda_et, 0.0, 1.0)
    wm_slots = np.clip(wm_slots, 0.0, 6.0)
    gate_strength = max(1e-6, gate_strength)
    lapse = np.clip(lapse, 0.0, 0.2)

    nA = 3
    age_group = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize RL values and eligibility traces
        Q = (1.0 / nA) * np.ones((nS, nA))
        E = np.zeros((nS, nA))

        # WM store: last rewarded action per state with a confidence trace
        stored_action = -1 * np.ones(nS, dtype=int)
        conf = np.zeros(nS)  # confidence [0..1] that stored action is correct

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Load index: 0 for 3, 1 for 6
            load = (max(nS, 3) - 3) / 3.0

            # Age-sensitive WM decay/interference: larger load and older age increase decay; gate_strength protects
            # d in [0,1]
            base_interference = 0.15 + 0.45 * load
            age_multiplier = 1.0 + 0.5 * age_group
            protection = 1.0 / (1.0 + gate_strength * (1.0 - 0.3 * age_group))
            d = np.clip(base_interference * age_multiplier * protection, 0.0, 1.0)

            # Decay WM confidence
            conf *= (1.0 - d)

            # RL policy
            logits_rl = beta * Q[s, :].copy()
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl /= np.sum(p_rl)

            # WM policy: if something stored for s, focus probability on stored action proportional to confidence
            logits_wm = np.zeros(nA)
            sa = int(stored_action[s])
            if sa >= 0 and conf[s] > 1e-12:
                # Make WM sharper with higher confidence
                logits_wm[sa] = 5.0 * beta * conf[s]
            logits_wm -= np.max(logits_wm)
            p_wm = np.exp(logits_wm)
            p_wm /= np.sum(p_wm)

            # Capacity-limited WM weight: slots / set size, scaled by current state's confidence
            w_cap = np.clip(wm_slots / max(1, nS), 0.0, 1.0)
            w = np.clip(w_cap * conf[s], 0.0, 1.0)

            # Mixture policy with lapse
            p_mix = w * p_wm + (1.0 - w) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p = np.clip(p_final[a], 1e-12, 1.0)
            total_loglik += np.log(p)

            # Learning: eligibility traces with lambda
            delta = r - Q[s, a]
            # Decay all traces
            E *= lambda_et
            # Replace trace for chosen state-action
            E[s, a] = 1.0
            # Update Q with traces
            Q += lr_rl * delta * E

            # WM write on reward
            if r > 0.5:
                stored_action[s] = a
                conf[s] = 1.0

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Mixture of model-free RL and state-specific WSLS (win-stay/lose-switch),
    with load-dependent arbitration and age-dependent lapses.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within block.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Binary reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size on that block (3 or 6).
    age : array-like (length 1)
        Age in years; used to set age group (younger <45, older >=45).
    model_parameters : list/tuple of length 6
        - alpha: RL learning rate in [0,1]
        - beta: inverse temperature for RL (>0)
        - wsls_bias: strength of WSLS tendency (>0), higher means more deterministic WSLS
        - mix_bias0: base arbitration bias toward WSLS (real-valued)
        - mix_load_slope: how load (6 vs 3) shifts arbitration (real-valued)
        - lapse_age: controls overall lapse and its scaling by age (real-valued)

    Returns
    -------
    float
        Negative log-likelihood under the model.
    """
    alpha, beta, wsls_bias, mix_bias0, mix_load_slope, lapse_age = model_parameters
    beta = max(1e-6, beta) * 5.0
    wsls_bias = max(1e-6, wsls_bias) * 5.0
    age_group = 1.0 if age[0] >= 45 else 0.0

    # Age-dependent lapse: older group has higher baseline lapses for the same parameter
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    base_lapse = 0.02 + 0.08 * age_group
    lapse = np.clip(base_lapse * sigmoid(lapse_age), 0.0, 0.25)

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Keep track of last action and reward per state for WSLS
        last_a = -1 * np.ones(nS, dtype=int)
        last_r = -np.ones(nS)  # -1 means no history

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            load = (max(nS, 3) - 3) / 3.0  # 0 for 3, 1 for 6

            # RL softmax
            logits_rl = beta * Q[s, :].copy()
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl /= np.sum(p_rl)

            # WSLS policy for this state
            p_wsls = np.ones(nA) / nA
            if last_a[s] >= 0 and last_r[s] >= -0.5:
                if last_r[s] > 0.5:
                    # Win-stay: bias toward repeating last action
                    logits_wsls = np.zeros(nA)
                    logits_wsls[last_a[s]] = wsls_bias
                    logits_wsls -= np.max(logits_wsls)
                    p_wsls = np.exp(logits_wsls)
                    p_wsls /= np.sum(p_wsls)
                else:
                    # Lose-switch: bias toward other two actions equally
                    logits_wsls = np.zeros(nA)
                    other = [i for i in range(nA) if i != last_a[s]]
                    logits_wsls[other] = wsls_bias
                    logits_wsls -= np.max(logits_wsls)
                    p_wsls = np.exp(logits_wsls)
                    p_wsls /= np.sum(p_wsls)

            # Arbitration: logistic function of base bias + load; older subtly shifted toward RL
            # mix is weight on WSLS
            mix_input = mix_bias0 + mix_load_slope * load - 0.25 * age_group
            mix = sigmoid(mix_input)
            p_mix = mix * p_wsls + (1.0 - mix) * p_rl

            # Lapse
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p = np.clip(p_final[a], 1e-12, 1.0)
            total_loglik += np.log(p)

            # Update RL
            Q[s, a] += alpha * (r - Q[s, a])

            # Update WSLS memory
            last_a[s] = a
            last_r[s] = r

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with visit-interval-dependent forgetting and state-specific choice kernel,
    with age- and load-modulated decay.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within block.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of float/int
        Binary reward feedback (0/1) on each trial.
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size on that block (3 or 6).
    age : array-like (length 1)
        Age in years; used to set age group (younger <45, older >=45).
    model_parameters : list/tuple of length 6
        - alpha: RL learning rate in [0,1]
        - beta: inverse temperature (>0)
        - decay_base: per-trial baseline forgetting toward prior (0..1)
        - age_decay_boost: extra forgetting multiplier for older group (>=0)
        - kernel_gain: strength of state-specific choice kernel (perseveration/frequency)
        - kernel_age_scale: factor scaling kernel_gain in older group (can be positive or negative)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, decay_base, age_decay_boost, kernel_gain, kernel_age_scale = model_parameters
    beta = max(1e-6, beta) * 5.0
    decay_base = np.clip(decay_base, 0.0, 1.0)
    age_decay_boost = max(0.0, age_decay_boost)
    age_group = 1.0 if age[0] >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # State-specific choice kernel
        K = np.zeros((nS, nA))
        # Track last visit time for each state
        last_visit = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply visit-interval-dependent forgetting to Q for this state
            if last_visit[s] >= 0:
                lag = max(1, t - int(last_visit[s]))
                load = (max(nS, 3) - 3) / 3.0  # 0 for 3, 1 for 6
                # Per-trial decay rate d, amplified by load and age for older group
                d = np.clip(decay_base * (1.0 + 0.7 * load) * (1.0 + age_decay_boost * age_group), 0.0, 1.0)
                # Aggregate decay over lag trials: rho = 1 - (1-d)^lag
                rho = 1.0 - (1.0 - d) ** lag
                prior = (1.0 / nA)
                Q[s, :] = (1.0 - rho) * Q[s, :] + rho * prior
            # else: no prior visit, keep initialization

            # Compute policy: RL plus state-specific choice kernel with age scaling
            kernel_scale = kernel_gain * (1.0 + kernel_age_scale * age_group)
            logits = beta * Q[s, :] + kernel_scale * K[s, :]
            logits -= np.max(logits)
            p_vec = np.exp(logits)
            p_vec /= np.sum(p_vec)
            p = np.clip(p_vec[a], 1e-12, 1.0)
            total_loglik += np.log(p)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # Update choice kernel: mild decay then increment chosen action
            K[s, :] *= 0.5
            K[s, a] += 1.0

            # Update last visit time
            last_visit[s] = t

    return -float(total_loglik)