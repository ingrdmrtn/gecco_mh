def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity-limited mixture and state-wise choice stickiness.

    Policy:
    - Mixture of RL softmax and WM softmax policies.
    - WM mixture weight scales with set size via a simple capacity C: wm_weight = wm_weight_base * min(1, C / nS).
      This yields stronger WM influence in small set sizes and weaker influence in large set sizes.
    - RL includes a state-wise choice stickiness bias that adds a preference bonus psi to the previously chosen action in that state.

    Learning:
    - RL updates with a single learning rate lr.
    - WM updates with its own learning rate alpha_wm toward a one-hot code on rewarded trials and decays toward uniform on non-rewarded trials.

    Parameters (all used):
    - model_parameters[0]: lr (float) RL learning rate.
    - model_parameters[1]: wm_weight_base (float in [0,1]) Base mixture weight for WM before capacity scaling.
    - model_parameters[2]: softmax_beta (float) RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: alpha_wm (float in [0,1]) WM learning rate for updating WM table.
    - model_parameters[4]: C (float > 0) WM effective capacity; governs how wm_weight scales with set size.
    - model_parameters[5]: psi (float) State-wise stickiness bias added to the RL preference for repeating last action in the same state.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, alpha_wm, C, psi = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Capacity-scaled WM weight
            wm_weight = wm_weight_base * min(1.0, C / max(1.0, float(nS)))

            # RL policy with stickiness bias in preferences
            Q_s = q[s, :]
            pref_rl = softmax_beta * Q_s.copy()
            if last_action[s] >= 0:
                pref_rl[last_action[s]] += psi
            p_rl = 1.0 / np.sum(np.exp(pref_rl - pref_rl[a]))

            # WM policy (deterministic softmax over WM table)
            W_s_vec = w[s, :]
            pref_wm = softmax_beta_wm * W_s_vec
            p_wm = 1.0 / np.sum(np.exp(pref_wm - pref_wm[a]))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: approach one-hot on reward; decay toward uniform on no-reward
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * target
            else:
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * w_0[s, :]

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and load penalty.

    Policy:
    - Action probability is a mixture of WM and RL policies.
    - The WM mixture weight is dynamically determined by a meta-controller that compares WM confidence
      to RL confidence, with an explicit penalty for larger set sizes (cognitive load).
    - WM confidence = max(W_s) - mean(W_s); RL confidence = max(Q_s) - mean(Q_s).
      Arbitration weight sigma = sigmoid(arb_temp * (WMconf - RLconf - gamma_load * log(nS))).
      Final wm_weight = wm_weight_base * sigma.

    Learning:
    - RL updates with learning rate lr.
    - WM updates with learning rate wm_eta toward a one-hot on rewarded trials; decays toward uniform on non-rewarded trials.

    Parameters (all used):
    - model_parameters[0]: lr (float) RL learning rate.
    - model_parameters[1]: wm_weight_base (float in [0,1]) Base scaling for WM contribution.
    - model_parameters[2]: softmax_beta (float) RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: wm_eta (float in [0,1]) WM learning rate.
    - model_parameters[4]: gamma_load (float >= 0) Load penalty strength in arbitration (scales with log set size).
    - model_parameters[5]: arb_temp (float) Sensitivity of arbitration to confidence differences.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, wm_eta, gamma_load, arb_temp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute arbitration weight from relative confidence and load
            Q_s = q[s, :]
            W_s_vec = w[s, :]

            wm_conf = np.max(W_s_vec) - np.mean(W_s_vec)
            rl_conf = np.max(Q_s) - np.mean(Q_s)
            load_term = gamma_load * np.log(max(1.0, float(nS)))
            x = arb_temp * (wm_conf - rl_conf - load_term)
            sigma = 1.0 / (1.0 + np.exp(-x))
            wm_weight = wm_weight_base * sigma

            # RL policy
            pref_rl = softmax_beta * Q_s
            p_rl = 1.0 / np.sum(np.exp(pref_rl - pref_rl[a]))

            # WM policy
            pref_wm = softmax_beta_wm * W_s_vec
            p_wm = 1.0 / np.sum(np.exp(pref_wm - pref_wm[a]))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with reward sensitivity + WM with load-dependent precision.

    Policy:
    - Mixture of RL and WM softmaxes.
    - RL uses standard softmax with inverse temperature softmax_beta, but the reward entering the PE is scaled by rho.
    - WM readout precision degrades with set size via a lower effective WM inverse temperature:
      beta_wm_eff = beta_wm_base / (1 + k_load * nS). Thus WM becomes less precise under higher load.

    Learning:
    - RL updates with learning rate lr and reward sensitivity rho: PE = rho * r - Q.
    - WM updates toward one-hot on reward and decays toward uniform on no-reward using the same lr (shared learning rate).

    Parameters (all used):
    - model_parameters[0]: lr (float) Shared learning rate for RL and WM updates.
    - model_parameters[1]: wm_weight_base (float in [0,1]) Mixture weight for WM policy.
    - model_parameters[2]: softmax_beta (float) RL inverse temperature (internally scaled by 10).
    - model_parameters[3]: beta_wm_base (float) Base WM inverse temperature before load degradation.
    - model_parameters[4]: k_load (float >= 0) Strength of load-induced drop in WM precision.
    - model_parameters[5]: rho (float >= 0) Reward sensitivity in RL prediction error.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, beta_wm_base, k_load, rho = model_parameters
    softmax_beta *= 10.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision
        beta_wm_eff = beta_wm_base / (1.0 + k_load * max(1.0, float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s_vec = w[s, :]

            # RL policy
            pref_rl = softmax_beta * Q_s
            p_rl = 1.0 / np.sum(np.exp(pref_rl - pref_rl[a]))

            # WM policy with load-dependent precision
            pref_wm = beta_wm_eff * W_s_vec
            p_wm = 1.0 / np.sum(np.exp(pref_wm - pref_wm[a]))

            # Mixture (fixed base weight)
            p_total = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update with reward sensitivity
            pe = rho * r - Q_s[a]
            q[s, a] += lr * pe

            # WM update using shared learning rate
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * target
            else:
                w[s, :] = (1.0 - lr) * w[s, :] + lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p