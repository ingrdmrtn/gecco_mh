def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size and age-modulated forgetting plus action perseveration.

    Idea:
    - Standard delta-rule RL updated within each block.
    - Q-values undergo proactive interference/forgetting toward the uniform prior, stronger for larger set sizes
      and for older adults.
    - Action perseveration: a "stickiness" bonus is added to the last chosen action in the same state.
    - Both mechanisms capture working-memory load effects without an explicit WM store.

    Parameters (model_parameters):
    - alpha: learning rate for RL (0..1).
    - beta: inverse temperature for softmax (>0).
    - f_base: base forgetting rate per trial toward uniform (0..1).
    - age_forget_mult: multiplicative increase of forgetting for older adults (>=0).
    - kappa: action perseveration weight added to the last action in the current state (can be positive or negative).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (resets learning within block).
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single entry: participant age in years.
    - model_parameters: list/tuple with [alpha, beta, f_base, age_forget_mult, kappa].

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, f_base, age_forget_mult, kappa = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize Q to uniform prior and last-action memory per state
        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Compute effective forgetting rate: stronger with larger set size and in older adults
            # Scale base forgetting approximately linearly with set size and age.
            f_eff = f_base * (curr_set / 3.0) * (1.0 + age_forget_mult * is_older)
            f_eff = max(0.0, min(1.0, f_eff))

            # Apply forgetting toward uniform prior (proactive interference)
            Q[s, :] = (1.0 - f_eff) * Q[s, :] + f_eff * (1.0 / nA)

            # Softmax with perseveration/stickiness bonus for repeating the last action in this state
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0:
                logits[last_action[s]] += kappa

            exp_logits = np.exp(logits)
            probs = exp_logits / np.sum(exp_logits)
            p_choice = max(probs[a], 1e-12)
            nll -= np.log(p_choice)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update last action memory for perseveration
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic WM with interference-based recall probability.

    Idea:
    - RL learns Q-values via delta rule.
    - In parallel, an episodic WM stores the most recently rewarded action per state and its last encountered time.
    - On each trial, the decision policy is a mixture of RL softmax and a WM policy that selects the stored action
      for the current state if "recalled". The recall probability decays exponentially with the number of intervening
      trials since last update of that state's memory.
    - Larger set sizes yield more interference (faster decay); older adults exhibit faster decay than younger adults.
    - A small lapse probability accounts for undirected responding.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: inverse temperature for RL softmax (>0).
    - tau_wm: WM time constant controlling recall/forgetting in units of intervening trials (>0).
    - age_decay_scale: multiplicative increase in forgetting (decay) for older adults (>=0).
    - epsilon: lapse probability for random choice (0..1).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (resets learning within block).
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single entry: participant age in years.
    - model_parameters: list/tuple with [alpha, beta, tau_wm, age_decay_scale, epsilon].

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, tau_wm, age_decay_scale, epsilon = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Episodic WM store: action and last time index updated. -1 = none
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_last_time = -1 * np.ones(nS, dtype=int)

        # Global time within block for interference counting
        time_t = 0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM recall probability decays with intervening trials and set size, amplified in older adults
            if wm_last_time[s] >= 0 and wm_action[s] >= 0:
                delta_t = max(0, time_t - wm_last_time[s])
                # Effective time constant decreases with set size and age (more interference -> faster forgetting)
                tau_eff = tau_wm / ((curr_set / 3.0) * (1.0 + age_decay_scale * is_older))
                tau_eff = max(tau_eff, 1e-6)
                p_recall = np.exp(-delta_t / tau_eff)
                # WM policy picks stored action deterministically if recalled; otherwise uniform over actions
                p_wm = (p_recall * (1.0 if a == wm_action[s] else 0.0)) + (1.0 - p_recall) * (1.0 / nA)
            else:
                # No memory yet: uniform WM
                p_recall = 0.0
                p_wm = 1.0 / nA

            # Mixture of WM and RL (weighted by recall probability)
            # Intuition: when recall succeeds, WM dominates; otherwise RL drives choice.
            # Add lapse on top.
            p_mix = p_recall * p_wm + (1.0 - p_recall) * p_rl
            p_choice = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            nll -= np.log(p_choice)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM store update: if rewarded, store this action and update timestamp;
            # if not rewarded, we keep existing memory but refresh timestamp slightly toward recency.
            if r > 0:
                wm_action[s] = a
                wm_last_time[s] = time_t
            else:
                # Interference without successful encoding: small recency effect
                if wm_last_time[s] >= 0:
                    wm_last_time[s] = max(wm_last_time[s], time_t - 1)

            time_t += 1

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-control of exploration: set size and age modulate inverse temperature.

    Idea:
    - Pure RL with softmax choice.
    - The meta-controller adjusts inverse temperature trial-by-trial:
      beta_eff = beta0 - beta_set_gain * ((set_size - 3) / 3) + beta_age_gain * I[older],
      so larger set sizes promote exploration (lower beta), while older adults exploit more (higher beta).
    - A small lapse probability captures stimulus-independent responding.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta0: baseline inverse temperature (>0).
    - beta_set_gain: magnitude by which larger set sizes reduce inverse temperature (>=0).
    - beta_age_gain: added inverse temperature for older adults (can be >=0).
    - epsilon: lapse probability for random choice (0..1).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (resets learning within block).
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single entry: participant age in years.
    - model_parameters: list/tuple with [alpha, beta0, beta_set_gain, beta_age_gain, epsilon].

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta0, beta_set_gain, beta_age_gain, epsilon = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Meta-controlled inverse temperature
            beta_eff = beta0 - beta_set_gain * ((curr_set - 3.0) / 3.0) + beta_age_gain * is_older
            beta_eff = max(beta_eff, 1e-6)

            logits = beta_eff * (Q[s, :] - np.max(Q[s, :]))
            exp_logits = np.exp(logits)
            probs = exp_logits / np.sum(exp_logits)
            p_soft = probs[a]
            p_choice = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            nll -= np.log(p_choice)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll