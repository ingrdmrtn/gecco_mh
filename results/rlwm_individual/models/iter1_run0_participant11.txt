def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and state-specific choice stickiness, with age- and load-modulated inverse temperature.

    Mechanism:
    - Q-learning with per-visit forgetting toward 0.
    - Softmax policy over Q-values plus a stickiness bias toward repeating the last action in the same state.
    - Effective inverse temperature is reduced under higher set size and for older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2). Trials with invalid actions are ignored.
    rewards : array-like of float
        Reward on each trial (typically 0 or 1). Trials with rewards not in {0,1} are ignored.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states) for each trial/block.
    age : array-like (single value)
        Participant age in years; used to form age group (older >= 45).
    model_parameters : list or tuple of floats
        [alpha, beta, rho_forget, tau_stick, age_temp]
        - alpha: RL learning rate (0..1).
        - beta: base inverse temperature for softmax (>0).
        - rho_forget: forgetting rate applied to Q(s,·) on visits (0..1).
        - tau_stick: stickiness bias added to the last-chosen action in the same state (can be +/-).
        - age_temp: scales temperature by age group; reduces β for older, boosts for younger (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, rho_forget, tau_stick, age_temp = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0
    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        # Load scaling: higher set size -> lower effective β
        load_scale = 3.0 / max(1.0, float(nS))
        # Age scaling: older -> reduce β; younger -> increase β
        age_scale = (1.0 - age_temp * is_old) * (1.0 + (1.0 - is_old) * 0.5 * age_temp)
        beta_eff_base = beta * load_scale * age_scale

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or (r not in [0.0, 1.0]):
                continue

            # Construct logits: β*Q plus stickiness for repeating last action in same state
            q_s = Q[s, :].copy()
            # Per-visit forgetting toward 0 on the current state's Q-values
            Q[s, :] = (1.0 - rho_forget) * Q[s, :]

            logits = beta_eff_base * q_s
            if last_action[s] >= 0 and last_action[s] < nA:
                logits[last_action[s]] += tau_stick

            # Softmax policy
            logits = logits - np.max(logits)
            exp_logits = np.exp(logits)
            p = exp_logits / (np.sum(exp_logits) + eps)

            nll -= np.log(max(p[a], eps))

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update last action memory for stickiness
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited episodic WM with time-based decay, age-sensitive arbitration.

    Mechanism:
    - RL module: Q-learning with softmax policy.
    - WM module: stores the most recent rewarded action per state, for up to K_slots unique states.
      WM retrieval probability decays with the time since last visit: retention^lag.
    - Arbitration: weight toward WM increases when the state is in WM and decays with lag;
      scaled by capacity usage, set size (K_slots/nS), and age group.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2). Trials with invalid actions are ignored.
    rewards : array-like of float
        Reward on each trial (0 or 1). Trials with rewards not in {0,1} are ignored.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states) for each trial/block.
    age : array-like (single value)
        Participant age in years; used to form age group (older >= 45).
    model_parameters : list or tuple of floats
        [alpha, beta, K_slots, retention, age_wm]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - K_slots: WM capacity in number of distinct states (>=1).
        - retention: per-visit retention parameter; retrieval scales as retention^lag (0..1).
        - age_wm: ages adjust WM weight: reduce for older, increase for younger (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, K_slots, retention, age_wm = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0
    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        # WM structures
        wm_action = -1 * np.ones(nS, dtype=int)   # stored rewarded action per state (if any)
        wm_time = -1 * np.ones(nS, dtype=int)     # last time step when WM for state was updated
        wm_in_list = []                           # list of states currently occupying WM slots (for LRU eviction)

        # Age and load scaling for WM arbitration
        load_factor = min(1.0, float(K_slots) / max(1.0, float(nS)))  # less weight when set size exceeds capacity
        age_factor = (1.0 - age_wm * is_old) * (1.0 + (1.0 - is_old) * 0.5 * age_wm)

        t_global = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])
            t_global += 1

            if not (0 <= s < nS) or not (0 <= a < nA) or (r not in [0.0, 1.0]):
                continue

            # RL policy
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy and arbitration weight
            if wm_action[s] >= 0:
                lag = max(0, t_global - wm_time[s])
                w_rec = (retention ** lag)
                w = w_rec * load_factor * age_factor
                w = np.clip(w, 0.0, 1.0)
                p_wm = np.ones(nA) * (eps)
                p_wm /= np.sum(p_wm)  # uniform tiny base
                p_wm[wm_action[s]] = 1.0 - (nA - 1) * eps  # concentrate mass on stored action
            else:
                w = 0.0
                p_wm = np.ones(nA) / nA

            p = w * p_wm + (1.0 - w) * p_rl
            nll -= np.log(max(p[a], eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: only store if rewarded
            if r == 1.0:
                # If state not already in WM, ensure capacity
                if wm_action[s] < 0 and len(wm_in_list) >= int(max(1, int(round(K_slots)))):
                    # Evict least recently updated state
                    evict_state = wm_in_list.pop(0)
                    wm_action[evict_state] = -1
                    wm_time[evict_state] = -1
                # Place/update this state in WM as most recent
                if s in wm_in_list:
                    wm_in_list.remove(s)
                wm_in_list.append(s)
                wm_action[s] = a
                wm_time[s] = t_global

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Kalman Q-learning (uncertainty-adaptive learning) with UCB-softmax exploration,
    modulated by set size and age.

    Mechanism:
    - For each (state, action), maintain a posterior mean and variance over expected reward.
    - Update via a scalar Kalman filter with process noise q_process and assumed observation noise r_noise.
    - Action values for choice are mean + c_eff * sqrt(variance) (UCB bonus).
    - Effective exploration bonus c_eff is reduced by larger set sizes and older age.
    - Softmax over UCB values produces choice probabilities.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2). Trials with invalid actions are ignored.
    rewards : array-like of float
        Reward on each trial (0 or 1). Trials with rewards not in {0,1} are ignored.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states) for each trial/block.
    age : array-like (single value)
        Participant age in years; used to form age group (older >= 45).
    model_parameters : list or tuple of floats
        [beta, c_explore, q_process, v0, age_ucb]
        - beta: inverse temperature for softmax over UCB values (>0).
        - c_explore: base coefficient for UCB exploration bonus (>=0).
        - q_process: process noise added to variance on each update (>=0).
        - v0: initial variance for each (state, action) (>0).
        - age_ucb: scales exploration bonus by age group; reduces for older, boosts for younger (>=0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    beta, c_explore, q_process, v0, age_ucb = model_parameters
    is_old = 1.0 if age[0] >= 45 else 0.0
    nll = 0.0
    eps = 1e-12
    r_noise = 0.25  # assumed observation noise for binary rewards

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        m = np.zeros((nS, nA)) + 0.0     # posterior means
        v = np.ones((nS, nA)) * max(v0, eps)  # posterior variances

        # Load and age scaling for UCB bonus
        load_scale = 3.0 / max(1.0, float(nS))  # larger sets -> smaller bonus
        age_scale = (1.0 - age_ucb * is_old) * (1.0 + (1.0 - is_old) * 0.5 * age_ucb)
        c_eff_base = c_explore * load_scale * age_scale

        for t in range(len(block_states)):
            a = int(block_actions[t])
            r = float(block_rewards[t])
            s = int(block_states[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or (r not in [0.0, 1.0]):
                continue

            # UCB values
            bonus = c_eff_base * np.sqrt(np.maximum(v[s, :], 0.0))
            ucb = m[s, :] + bonus

            # Softmax over UCB
            logits = beta * (ucb - np.max(ucb))
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            nll -= np.log(max(p[a], eps))

            # Kalman update for visited (s,a)
            # Predict step: add process noise to variance
            v[s, a] = v[s, a] + q_process
            # Kalman gain
            denom = v[s, a] + r_noise
            k = v[s, a] / (denom + eps)
            # Update mean and variance
            m[s, a] = m[s, a] + k * (r - m[s, a])
            v[s, a] = (1.0 - k) * v[s, a]

    return nll