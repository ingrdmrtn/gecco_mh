def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with separate RL learning rates and set-size–modulated WM weight.

    Overview:
      - RL: tabular Q-learning with separate positive/negative learning rates.
      - WM: one-shot cache that strengthens the rewarded action and otherwise decays toward uniform.
      - Mixture: p = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.
      - Set-size modulation: the effective WM weight is a logistic transform of a base weight
        adjusted by a slope term that penalizes larger set sizes.

    Parameters (model_parameters):
      - lr_pos: positive learning rate for RL (update when r=1).
      - lr_neg: negative learning rate for RL (update when r=0).
      - base_wm_weight: base mixture weight for WM before set-size scaling (0-1).
      - softmax_beta: inverse temperature for RL softmax (scaled internally by 10).
      - wm_decay: decay rate of WM toward uniform and also its learning strength (0-1).
      - wm_logit_slope: slope controlling how set size affects the WM weight (can be negative).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, base_wm_weight, softmax_beta, wm_decay, wm_logit_slope = model_parameters
    softmax_beta *= 10  # RL has higher dynamic range
    softmax_beta_wm = 50  # WM is near-deterministic
    blocks_log_p = 0

    # Helper: numerical safe logit/sigmoid
    def _logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    def _sigmoid(x):
        return 1 / (1 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Compute a set-size–modulated WM weight for this block
        # Smaller set sizes increase the WM weight; larger ones decrease it.
        # We start from the base weight in logit space and add a slope term times (reference - nS).
        ref = 3.0  # reference small set size
        wm_weight_block = _sigmoid(_logit(base_wm_weight) + wm_logit_slope * (ref - float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM table (near-deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            wm_weight = wm_weight_block  # use block-level set-size adjusted weight
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with separate learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr * delta

            # WM update:
            # 1) Global decay toward uniform for this state
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) If rewarded, strengthen the chosen action (one-shot cache toward one-hot)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * target
            # Keep bounded for numerical stability
            w[s, :] = np.clip(w[s, :], -5.0, 5.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Q-decay + WM recency kernel; WM weight decreases with encountered load.

    Overview:
      - RL: tabular Q-learning with softmax and per-state Q-decay (forgetting) to uniform.
      - WM: a recency/choice kernel within each state; rewarded choices increase tendency to repeat,
             unrewarded choices decrease it. WM policy uses its own inverse temperature.
      - Mixture: p = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.
      - Load-adaptive WM weight: starts at wm_weight_base and decreases as more unique states
        from the set are encountered within the block.

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - softmax_beta: RL inverse temperature (scaled internally by 10).
      - wm_weight_base: base WM mixture weight before load scaling (0-1).
      - wm_beta: WM inverse temperature (controls WM policy determinism).
      - wm_learn: WM learning/decay strength (0-1) used for both decay to uniform and kernel updates.
      - q_decay: RL decay rate toward uniform for the current state's Q (0-1).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_beta, wm_learn, q_decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = wm_beta  # here WM temperature is a parameter
    blocks_log_p = 0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # recency kernel centered at zero
        w_0 = np.zeros((nS, nA))  # baseline for WM kernel

        seen = np.zeros(nS, dtype=bool)
        n_seen = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            if not seen[s]:
                seen[s] = True
                n_seen += 1

            # Effective WM weight decreases with fraction of unique states encountered
            load_frac = n_seen / float(nS)
            wm_weight = np.clip(wm_weight_base * (1.0 - load_frac), 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL: choice probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM: softmax over recency kernel
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with per-state Q-decay toward uniform
            q[s, :] = (1 - q_decay) * q[s, :] + q_decay * (1.0 / nA)
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward baseline and reward-signed choice kernel update
            # 1) Decay the entire state's WM kernel toward zero
            w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * w_0[s, :]
            # 2) Reward-signed adjustment on chosen action
            #    Increase if rewarded, decrease if not
            signed = 1.0 if r > 0 else -1.0
            w[s, a] += wm_learn * signed
            # Bound for numerical stability
            w[s, :] = np.clip(w[s, :], -10.0, 10.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Arbitration by uncertainty: WM vs RL weight determined by entropy difference.

    Overview:
      - RL: tabular Q-learning with softmax.
      - WM: associative table, softmax policy with its own temperature and leaky memory.
      - Arbitration: trial-wise WM weight is a sigmoid of bias + kappa * (H_RL - H_WM),
        where H are the entropies of the current policies. When RL is more uncertain than WM,
        the model relies more on WM, and vice versa.
      - Mixture: p = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - bias: bias term in the arbitration (in logit space of WM weight).
      - softmax_beta: RL inverse temperature (scaled internally by 10).
      - kappa: sensitivity of arbitration to entropy difference (>=0).
      - wm_forget: WM leak toward uniform (0-1).
      - wm_beta: WM inverse temperature.

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, bias, softmax_beta, kappa, wm_forget, wm_beta = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = wm_beta
    blocks_log_p = 0

    def _softmax(x, beta):
        x_centered = x - np.max(x)  # numerical stability
        ex = np.exp(beta * x_centered)
        return ex / np.sum(ex)

    def _entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    def _sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Full policy distributions
            pi_rl = _softmax(Q_s, softmax_beta)
            pi_wm = _softmax(W_s, softmax_beta_wm)

            # Choice probabilities for the chosen action
            p_rl = pi_rl[a]
            p_wm = pi_wm[a]

            # Entropy-based arbitration
            H_rl = _entropy(pi_rl)
            H_wm = _entropy(pi_wm)
            wm_weight = _sigmoid(bias + kappa * (H_rl - H_wm))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leaky memory toward uniform; add reward-tagged boost to chosen action
            w[s, :] = (1 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]
            if r > 0:
                # reward strengthens association to chosen action
                w[s, a] += (1 - wm_forget)
            # Bound WM values for stability
            w[s, :] = np.clip(w[s, :], -10.0, 10.0)

        blocks_log_p += log_p

    return -blocks_log_p