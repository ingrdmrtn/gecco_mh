def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size dependent WM noise and global choice stickiness.

    Core ideas:
    - RL learns Q-values with a single learning rate.
    - WM stores recent correct actions, but becomes noisier as set size increases.
    - Arbitration mixes WM and RL policies with a WM weight that shrinks with set size.
    - Global choice stickiness biases repeating the immediately previous action.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: scalar in [0,1], baseline WM mixture weight before set-size scaling and noise.
    - softmax_beta: scalar >=0, RL inverse temperature (internally scaled by 10).
    - gamma: scalar >=0, controls how quickly WM degrades with set size (larger gamma -> more WM noise and lower WM weight as nS grows).
    - stickiness: scalar >=0, strength of global perseveration bias toward the previous action.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, gamma, stickiness = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # very deterministic WM retrieval
    blocks_log_p = 0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size effect on WM: capacity_factor in (0,1], noise = 1 - capacity_factor
        # If nS=3 => factor ~ 1, if nS larger => factor decreases as 1/(1+gamma*(nS-3))
        capacity_factor = 1.0 / (1.0 + gamma * max(0, nS - 3))
        wm_noise = 1.0 - capacity_factor  # increases with set size and gamma

        log_p = 0
        prev_action = None  # global perseveration state
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (full vector)
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Qc)
            pvec_rl = expQ / np.sum(expQ)
            p_rl = pvec_rl[a]

            # WM policy (full vector), then add set-size dependent retrieval noise toward uniform
            Wc = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * Wc)
            pvec_wm = expW / np.sum(expW)
            pvec_wm_noisy = (1.0 - wm_noise) * pvec_wm + wm_noise * (1.0 / nA)
            p_wm = pvec_wm_noisy[a]

            # Arbitration: WM weight shrinks with set size via capacity_factor
            wm_weight_eff = wm_weight * capacity_factor

            # Mixture policy (vector)
            pvec_mix = wm_weight_eff * pvec_wm_noisy + (1.0 - wm_weight_eff) * pvec_rl

            # Add global stickiness bias toward previous action
            if prev_action is not None:
                bias = np.ones(nA)
                bias[prev_action] = np.exp(stickiness)
                pvec_final = pvec_mix * bias
                pvec_final = pvec_final / np.sum(pvec_final)
            else:
                pvec_final = pvec_mix

            p_total = pvec_final[a]
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform on each encounter (decay stronger when set size is larger)
            decay = 0.1 + 0.4 * wm_noise  # in [0.1,0.5]
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM encoding: on reward, move toward a one-hot memory with strength ~ wm_weight_eff
            if r > 0:
                # soften others toward uniform and boost chosen toward 1
                w[s, :] = (1.0 - wm_weight_eff) * w[s, :] + wm_weight_eff * w_0[s, :]
                w[s, a] = (1.0 - wm_weight_eff) * w[s, a] + wm_weight_eff * 1.0
                # clip for numerical stability
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

            # update perseveration state
            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with PE-gated WM retrieval and time-decaying memory traces.

    Core ideas:
    - RL uses a single learning rate and softmax policy.
    - WM is retrieved more when surprise (unsigned prediction error) is high for the state.
    - WM encodes after rewards, with strength proportional to wm_weight; traces decay over time.
    - Arbitration weight also scales inversely with set size (implicit capacity effect: ~3/nS).

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: scalar in [0,1], baseline WM contribution/encoding strength.
    - softmax_beta: scalar >=0, RL inverse temperature (internally scaled by 10).
    - eta: scalar >=0, sensitivity of WM retrieval gate to unsigned prediction error.
    - tau: scalar, retrieval threshold (higher tau requires larger PE for WM to engage).
    - decay: scalar in [0,1], per-encounter WM decay toward uniform for the visited state.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, eta, tau, decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    decay = np.clip(decay, 0.0, 1.0)
    blocks_log_p = 0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last unsigned prediction error per state for retrieval gating
        pe_abs = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax (vector)
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Qc)
            pvec_rl = expQ / np.sum(expQ)
            p_rl = pvec_rl[a]

            # WM softmax (vector)
            Wc = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * Wc)
            pvec_wm = expW / np.sum(expW)
            p_wm = pvec_wm[a]

            # Retrieval gate: larger when unsigned PE for this state is high
            # gate in [0,1]: sigmoid(eta*(pe_abs[s] - tau))
            gate = 1.0 / (1.0 + np.exp(-eta * (pe_abs[s] - tau)))

            # Set-size capacity scaling: implicit factor ~ 3/nS
            capacity_factor = min(1.0, 3.0 / max(1.0, nS))

            wm_weight_eff = wm_weight * gate * capacity_factor

            # Mixture policy
            pvec_mix = wm_weight_eff * pvec_wm + (1.0 - wm_weight_eff) * pvec_rl
            p_total = pvec_mix[a]
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update the PE trace for retrieval gating on next encounter of this state
            pe_abs[s] = abs(delta)

            # WM decay on each encounter with this state
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM encoding: after reward, move toward a one-hot memory with strength wm_weight
            if r > 0:
                w[s, :] = (1.0 - wm_weight) * w[s, :] + wm_weight * w_0[s, :]
                w[s, a] = (1.0 - wm_weight) * w[s, a] + wm_weight * 1.0
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty bonus + WM with finite lifetime and power-law capacity scaling.

    Core ideas:
    - RL softmax includes a directed exploration bonus based on action-visit uncertainty.
    - WM stores the last rewarded action per state with a finite lifetime counted in state visits.
    - WM influence scales with set size via a power law (3/nS)^alpha_n.
    - Arbitration mixes WM and RL policies.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - wm_weight: scalar in [0,1], baseline WM mixture weight before capacity scaling.
    - softmax_beta: scalar >=0, RL inverse temperature (internally scaled by 10).
    - alpha_n: scalar >=0, exponent controlling how WM influence decays with set size (factor = (3/nS)^alpha_n).
    - beta_unc: scalar >=0, strength of directed exploration bonus 1/sqrt(N_sa).
    - L_life: positive scalar, WM lifetime in number of visits to the state before forgetting.

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, alpha_n, beta_unc, L_life = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    L_life = max(1.0, L_life)  # ensure positive
    blocks_log_p = 0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Counts for uncertainty bonus and WM lifetimes per state
        counts = np.zeros((nS, nA))
        wm_age = np.zeros(nS)  # number of visits to state since last rewarded encoding

        # Capacity factor via power law
        capacity_factor = min(1.0, (3.0 / max(1.0, nS)) ** max(0.0, alpha_n))

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Increment visit counts (before policy to use prior counts for bonus)
            # Note: use counts+1 in bonus to avoid div by zero
            bonus = beta_unc / np.sqrt(counts[s, :] + 1.0)

            Q_s = q[s, :]
            Q_aug = Q_s + bonus  # directed exploration
            # RL softmax with augmented values (vector)
            Qc = Q_aug - np.max(Q_aug)
            expQ = np.exp(softmax_beta * Qc)
            pvec_rl = expQ / np.sum(expQ)
            p_rl = pvec_rl[a]

            # WM softmax (vector)
            W_s = w[s, :]
            Wc = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * Wc)
            pvec_wm = expW / np.sum(expW)
            p_wm = pvec_wm[a]

            wm_weight_eff = wm_weight * capacity_factor

            # Mixture policy
            pvec_mix = wm_weight_eff * pvec_wm + (1.0 - wm_weight_eff) * pvec_rl
            p_total = pvec_mix[a]
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update counts after observing the choice
            counts[s, a] += 1.0

            # WM lifetime management per state (counted in state visits)
            # Age increases by 1 on each visit to the state
            wm_age[s] += 1.0
            if wm_age[s] >= L_life:
                # Forget when lifetime exceeded
                w[s, :] = w_0[s, :]
                wm_age[s] = 0.0  # reset after forgetting

            # WM encoding on reward: store a one-hot policy and reset age
            if r > 0:
                # Move toward one-hot on chosen action
                w[s, :] = (1.0 - wm_weight_eff) * w[s, :] + wm_weight_eff * w_0[s, :]
                w[s, a] = (1.0 - wm_weight_eff) * w[s, a] + wm_weight_eff * 1.0
                w[s, :] = np.clip(w[s, :], 0.0, 1.0)
                wm_age[s] = 0.0  # reset lifetime upon successful encoding

        blocks_log_p += log_p

    return -blocks_log_p