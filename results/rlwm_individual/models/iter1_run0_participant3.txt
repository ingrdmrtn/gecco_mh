def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age- and load-dependent forgetting and lapse.

    Mechanism:
    - Two systems combine to produce choice probabilities:
      1) Model-free RL (Q-learning) with global forgetting that scales with set size and age.
      2) WM store of last rewarded action per state. WM influence is a logistic function of
         an age-dependent capacity contrasted with current set size.
    - Final policy includes a load- and age-modulated lapse (uniform random responding).

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: inverse temperature for RL softmax (>0)
    - phi_forget: base forgetting rate for Q-values per trial (0..1), scaled by load and age
    - k_capacity: slope controlling how WM weight declines with load relative to age capacity
    - lapse_base: base lapse rate scaled by load and age (0..1)

    Inputs:
    - states: array of int, state index on each trial (0..set_size-1 within block)
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as lapses.
    - rewards: array of numeric, feedback per trial; <=0 mapped to 0, >0 to 1
    - blocks: array of int, block index for each trial (resets learning across blocks)
    - set_sizes: array of int, set size for each trial (3 or 6)
    - age: array-like with a single element, participant age in years
    - model_parameters: list/tuple of 5 floats [alpha, beta, phi_forget, k_capacity, lapse_base]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, phi_forget, k_capacity, lapse_base = model_parameters
    is_older = 1 if age[0] >= 45 else 0

    nll = 0.0
    eps = 1e-12
    unique_blocks = np.unique(blocks)

    # Age-dependent capacity anchor: younger ~4, older ~3 (arbitrary but plausible WM slots here)
    capacity_younger = 4.0
    capacity_older = 3.0

    for b in unique_blocks:
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(b_set_sizes[0])

        # Initialize RL and WM stores
        Q = np.zeros((nS, nA))
        wm_store = -np.ones(nS, dtype=int)  # -1 means no memory yet

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = max(1, int(b_set_sizes[t]))

            # Apply forgetting before computing policy: stronger with load and age
            # Forget rate increases with set size and for older adults
            forget_rate = phi_forget * (float(load) / 6.0) * (1.0 + 0.5 * is_older)
            forget_rate = max(0.0, min(1.0, forget_rate))
            Q *= (1.0 - forget_rate)

            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy and weight
            if is_older:
                capacity_anchor = capacity_older
            else:
                capacity_anchor = capacity_younger
            # WM weight: higher when load is below capacity anchor
            wm_weight = 1.0 / (1.0 + np.exp(-k_capacity * (capacity_anchor - float(load))))
            if wm_store[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_store[s])] = 1.0
            else:
                p_wm = np.ones(nA) / nA
                # If no memory, effectively reduce WM weight
                wm_weight = 0.0

            # Mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Lapse increases with load and age
            lapse = lapse_base * max(0.0, (float(load) - 3.0) / 3.0) * (1.0 + 0.5 * is_older)
            lapse = max(0.0, min(0.49, lapse))
            p = (1.0 - lapse) * p_mix + lapse * (np.ones(nA) / nA)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # Skip learning on invalid action
                continue
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store last rewarded action
            if r > 0.0:
                wm_store[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian associative learning with uncertainty-driven exploration and age-modulated priors.

    Mechanism:
    - For each state-action pair, maintain Beta-Bernoulli posterior over reward probability:
      Beta(alpha_sa, beta_sa). Reward updates alpha, non-reward updates beta.
    - Action selection via softmax over utility:
        U(a|s) = beta_decision * E[p(r=1|s,a)] + tau * sqrt(Var[p(r=1|s,a)])
      where Var is the Beta posterior variance (uncertainty bonus).
    - Older adults carry stronger priors (slower updating), scaling with set size.

    Parameters (model_parameters):
    - beta_decision: inverse temperature on expected value
    - tau: weight on uncertainty bonus (directs exploration)
    - rho_prior: multiplicative increase in prior strength if older (>=45)
    - kappa0: base symmetric prior strength per action (alpha0=beta0=kappa0 before scaling)
    - lapse: mixture with uniform random responding (0..1)

    Inputs:
    - states: array of int, state index on each trial
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as lapses.
    - rewards: array of numeric, feedback per trial; <=0 mapped to 0, >0 to 1
    - blocks: array of int, block index for each trial (resets learning across blocks)
    - set_sizes: array of int, set size per trial (modulates prior strength)
    - age: array-like with a single element, participant age in years
    - model_parameters: list/tuple of 5 floats [beta_decision, tau, rho_prior, kappa0, lapse]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    beta_decision, tau, rho_prior, kappa0, lapse = model_parameters
    is_older = 1 if age[0] >= 45 else 0

    nll = 0.0
    eps = 1e-12
    unique_blocks = np.unique(blocks)

    for b in unique_blocks:
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(b_set_sizes[0])

        # Initialize Beta prior parameters per state-action
        # Prior strength increases with load and more so for older adults (slower learning).
        prior_scale_age = (1.0 + is_older * rho_prior)

        alpha_sa = np.zeros((nS, nA))
        beta_sa = np.zeros((nS, nA))

        # Set initial priors proportional to load for the first trial; updated per trial with current load
        # We will lazily initialize on first encounter of a state using current load.
        initialized = np.zeros((nS, nA), dtype=bool)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = max(1, int(b_set_sizes[t]))

            # Initialize priors for this state's actions if not yet set (using current load)
            if not np.any(initialized[s, :]):
                kappa = kappa0 * prior_scale_age * (float(load) / 3.0)
                alpha_sa[s, :] = kappa
                beta_sa[s, :] = kappa
                initialized[s, :] = True

            # Compute expected value and uncertainty for all actions in state s
            a_s = alpha_sa[s, :]
            b_s = beta_sa[s, :]
            mean = a_s / (a_s + b_s + eps)
            var = (a_s * b_s) / (((a_s + b_s) ** 2) * (a_s + b_s + 1.0) + eps)
            util = beta_decision * mean + tau * np.sqrt(np.maximum(var, 0.0))

            # Softmax with lapse
            util = util - np.max(util)
            p_soft = np.exp(util)
            p_soft = p_soft / (np.sum(p_soft) + eps)
            lapse_c = max(0.0, min(0.49, lapse))
            p = (1.0 - lapse_c) * p_soft + lapse_c * (np.ones(nA) / nA)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # Skip updating on invalid action
                continue
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # Update posterior for chosen action in current state
            alpha_sa[s, a] += r
            beta_sa[s, a] += (1.0 - r)

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-learning-rate RL with age- and load-driven arbitration with WM.

    Mechanism:
    - RL component: Q-learning with separate learning rates for positive vs negative prediction errors.
    - WM component: stores last rewarded action per state (one-shot, deterministic recall).
    - Arbitration: weight on WM determined by a logistic function of load and age:
        w = sigmoid(omega_load * (C - set_size) - omega_age * is_older)
      where C is a nominal capacity (fixed at 4). Older age reduces WM weight.
    - Choice: p = w * p_WM + (1 - w) * softmax_beta(Q).

    Parameters (model_parameters):
    - alpha_pos: learning rate for positive prediction errors (0..1)
    - alpha_neg: learning rate for negative prediction errors (0..1)
    - beta: inverse temperature for RL softmax (>0)
    - omega_load: slope governing how WM weight declines with set size
    - omega_age: penalty on WM weight for older adults (>=45), larger -> less WM reliance

    Inputs:
    - states: array of int, state index on each trial
    - actions: array of int, chosen action on each trial (0,1,2). Out-of-range treated as uniform for likelihood and no learning.
    - rewards: array of numeric, feedback per trial; <=0 mapped to 0, >0 to 1
    - blocks: array of int, block index for each trial (resets learning across blocks)
    - set_sizes: array of int, set size per trial (3 or 6)
    - age: array-like with a single element, participant age in years
    - model_parameters: list/tuple of 5 floats [alpha_pos, alpha_neg, beta, omega_load, omega_age]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg, beta, omega_load, omega_age = model_parameters
    is_older = 1 if age[0] >= 45 else 0

    nll = 0.0
    eps = 1e-12
    unique_blocks = np.unique(blocks)

    capacity_nominal = 4.0  # nominal WM capacity anchor

    for b in unique_blocks:
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        wm_store = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = max(1, int(b_set_sizes[t]))

            # RL softmax policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy and arbitration weight
            if wm_store[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_store[s])] = 1.0
                has_mem = 1.0
            else:
                p_wm = np.ones(nA) / nA
                has_mem = 0.0

            # Arbitration weight: depends on load (more load -> less WM) and age (older -> less WM)
            w_raw = omega_load * (capacity_nominal - float(load)) - omega_age * is_older
            w = 1.0 / (1.0 + np.exp(-w_raw))
            # If no memory stored for this state, reduce WM influence
            w = w * has_mem

            p = w * p_wm + (1.0 - w) * p_rl

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                continue
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # RL update with dual learning rates
            delta = r - Q[s, a]
            if delta >= 0.0:
                Q[s, a] += alpha_pos * delta
            else:
                Q[s, a] += alpha_neg * delta

            # WM update on rewarded trials
            if r > 0.0:
                wm_store[s] = a

    return nll