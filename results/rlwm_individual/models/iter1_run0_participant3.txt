def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size–dependent precision, decay, and lapse.

    Mechanism:
    - RL learns state-action values via delta rule and softmax choice.
    - WM stores mappings via one-shot strengthening on reward and decays toward uniform.
    - WM choice precision decreases with set size (load), and includes a lapse probability.

    The final choice probability is a fixed mixture between RL and WM controlled by wm_weight.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within each block).
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, capacity_slope, phi_wm, eps_lapse)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM vs RL in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - capacity_slope: scales the reduction of WM precision with set size; beta_wm_eff = 50 / (1 + capacity_slope*(nS-1)).
        - phi_wm: WM retention/learning rate (0..1); higher -> faster one-shot and faster decay to baseline when not rewarded.
        - eps_lapse: WM lapse probability (0..1), mixing WM policy with uniform noise.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_slope, phi_wm, eps_lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM precision decreases with set size (load)
        kappa = 1.0 / (1.0 + capacity_slope * max(nS - 1, 0))
        beta_wm_eff = softmax_beta_wm * kappa

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax with set-size–dependent precision and lapse to uniform
            p_wm_soft = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = (1.0 - eps_lapse) * p_wm_soft + eps_lapse * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform each trial; one-shot strengthening on reward
            # Decay (leak) toward uniform
            w[s, :] = (1.0 - phi_wm) * w[s, :] + phi_wm * w_0[s, :]
            # If rewarded, strengthen the chosen action toward 1-hot
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - phi_wm) * w[s, :] + phi_wm * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based confidence and asymmetric WM learning.

    Mechanism:
    - RL: standard delta-rule learning and softmax choice.
    - WM: one-shot storage with separate rates for reward (eta_plus) and no-reward (eta_minus),
           plus continuous decay toward uniform.
    - WM policy confidence depends on the sharpness of WM distribution via 1 - normalized entropy,
      scaled by entropy_slope, which modulates the WM softmax precision.

    The final choice probability is a fixed mixture between RL and WM controlled by wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, eta_plus, eta_minus, entropy_slope)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM vs RL in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - eta_plus: WM learning rate on reward (0..1) toward one-hot of chosen action.
        - eta_minus: WM anti-learning rate on no-reward (0..1) pushing away from chosen action.
        - entropy_slope: scales impact of WM certainty (1 - normalized entropy) on WM precision.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, eta_plus, eta_minus, entropy_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute WM certainty from entropy; normalized by log(nA)
            eps = 1e-12
            W_safe = np.clip(W_s, eps, 1.0)
            W_safe = W_safe / np.sum(W_safe)  # ensure normalization
            entropy = -np.sum(W_safe * np.log(W_safe))
            certainty = 1.0 - entropy / np.log(nA)  # 0..1
            beta_wm_eff = softmax_beta_wm * (1.0 + entropy_slope * certainty)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Base decay to uniform each trial (small default leakage tied to eta parameters)
            leak = 0.5 * (eta_plus + eta_minus)
            w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Asymmetric update depending on reward
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.0:
                # Move toward chosen action one-hot
                w[s, :] = (1.0 - eta_plus) * w[s, :] + eta_plus * one_hot
            else:
                # Push away from chosen action: blend toward the complement distribution
                complement = (np.ones(nA) - one_hot) / (nA - 1)
                w[s, :] = (1.0 - eta_minus) * w[s, :] + eta_minus * complement

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + gated WM availability with set-size dependent gating and maintenance.

    Mechanism:
    - RL: standard delta-rule learning and softmax choice.
    - WM: a fast, near-deterministic store, but its availability is controlled by a
      state-specific gate m[s] in [0,1] that reflects whether the item is currently in focus.
      The WM policy precision is scaled by m[s].
    - Gate dynamics: on each trial, m[s] increases when rewarded with a probability/logit that
      depends on set size (smaller sets -> easier gating), and decays otherwise.

    The final choice probability is a fixed mixture between RL and WM controlled by wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, gate_bias, gate_setsize_slope, rho)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM vs RL in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - gate_bias: baseline tendency to open the WM gate on reward (logit space; mapped via sigmoid).
        - gate_setsize_slope: effect of 1/nS on gate opening (positive -> more gating in small sets).
        - rho: maintenance/decay parameter for gate strength m[s]; m <- (1-rho)*m + rho*target.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_bias, gate_setsize_slope, rho = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # State-specific WM gate strength
        m = np.zeros(nS)  # start with no gated items

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM precision scaled by gate strength m[s]; if m[s] ~ 0 -> near-uniform
            beta_wm_eff = softmax_beta_wm * np.clip(m[s], 0.0, 1.0)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM content updating: decay toward uniform; reward-driven sharpening
            w[s, :] = (1.0 - rho) * w[s, :] + rho * w_0[s, :]
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - rho) * w[s, :] + rho * one_hot

            # Gate updating: increase on reward with set-size dependent probability (logistic), decay otherwise
            inv_set = 1.0 / float(nS)
            gate_open_logit = gate_bias + gate_setsize_slope * inv_set
            gate_open = 1.0 / (1.0 + np.exp(-gate_open_logit))  # 0..1
            target_m = gate_open if r > 0.0 else 0.0
            m[s] = (1.0 - rho) * m[s] + rho * target_m

        blocks_log_p += log_p

    return -blocks_log_p