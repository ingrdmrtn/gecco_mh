def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with entropy-gated WM utilization and load-driven interference.

    Policy:
    - RL: standard softmax over Q-values (beta scaled by 10 as in template).
    - WM: deterministic softmax over WM values.
    - Mixture: WM weight increases when WM distribution is confident (low entropy),
               and WM contents are degraded more under larger set sizes.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight_base: Base weight on WM in the mixture (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - wm_write: WM write rate toward the rewarded action (0..1).
    - wm_interference: Load-sensitive WM interference strength (>=0); larger values
                       increase passive decay per trial as set size grows.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_write, wm_interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic softmax on W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-gated WM reliability: more weight when WM distribution is sharp
            H = -np.sum(W_s * np.log(np.maximum(W_s, 1e-12)))
            H_max = np.log(nA)
            conf = 1.0 - (H / H_max)  # 0 (uniform) .. 1 (one-hot)
            # Load-driven passive WM interference per trial (stronger as set size grows)
            load_factor = max(0, nS - 3)
            wm_weight_eff = wm_weight_base * conf

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive interference toward uniform due to load
            decay = 1.0 - 1.0 / (1.0 + wm_interference * load_factor)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM write toward one-hot of chosen action when rewarded (error-correcting)
            onehot = np.zeros(nA)
            onehot[a] = 1.0 if r > 0.0 else 0.0
            w[s, :] = (1.0 - wm_write) * w[s, :] + wm_write * (onehot + (1.0 - np.sum(onehot)) * w_0[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with slot-like capacity K and replacement pressure.

    Policy:
    - RL: standard softmax over Q-values (beta scaled by 10 as in template).
    - WM: deterministic softmax over WM values.
    - Mixture: WM contribution scales with capacity utilization min(1, K/nS).

    WM dynamics:
    - When rewarded, WM writes toward the chosen action with strength proportional
      to effective capacity utilization u = min(1, K/nS) and a replacement pressure.
    - Passive drift toward uniform increases when set size exceeds K (overload).

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base mixture weight on WM (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - K: WM capacity in number of items (>=0).
    - replace_rate: Replacement/interference pressure when overloaded (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, K, replace_rate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective capacity utilization
        u = min(1.0, float(K) / max(1.0, float(nS)))
        # Overload factor for nS > K
        overload = max(0.0, (float(nS) - float(K))) / max(1.0, float(nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture weight scales with capacity utilization
            wm_weight_eff = wm_weight * u

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive replacement/interference when overloaded
            # More drift toward uniform as overload and replace_rate increase
            drift = overload * (1.0 - np.exp(-replace_rate))
            w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

            # WM write on reward: stronger when within capacity and with higher replace_rate
            if r > 0.0:
                write_strength = u * (1.0 - np.exp(-replace_rate))
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with delay- and load-dependent binding failures and WM-driven choice stickiness.

    Policy:
    - RL: standard softmax over Q-values (beta scaled by 10 as in template).
    - WM: deterministic softmax over WM values, augmented by a stickiness bias toward
          the last action chosen in the same state (capturing choice autocorrelation).
    - Mixture: WM reliability decreases with both set size and delay (trials since last
               seeing the state), modeling binding failures.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base mixture weight on WM (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - stickiness: Additive bias applied within the WM channel to the last chosen action (>=0).
    - delay_decay: Exponential decay rate of WM reliability with inter-visit delay (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, delay_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track last seen trial index and last action per state
        last_time = -1 * np.ones(nS, dtype=int)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with stickiness bias for last chosen action in this state
            if last_action[s] >= 0:
                W_s[last_action[s]] += stickiness
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM reliability decays with delay since last visit and with load
            delay = 0 if last_time[s] < 0 else (t - last_time[s])
            load_penalty = 1.0 + max(0, nS - 3)
            wm_rel = np.exp(-delay_decay * delay) / load_penalty
            wm_weight_eff = wm_weight * wm_rel

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive decay toward uniform each time state is visited (binding fragility)
            local_decay = 1.0 - np.exp(-delay_decay * max(1, delay))
            w[s, :] = (1.0 - local_decay) * w[s, :] + local_decay * w_0[s, :]

            # WM update: bind chosen action more strongly when rewarded; slight unbinding otherwise
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.0:
                w[s, :] = 0.5 * w[s, :] + 0.5 * onehot
            else:
                # on errors, nudge away from the chosen action slightly
                w[s, a] = 0.5 * w[s, a] + 0.5 * (1.0 / nA)

            # Update last seen tracking
            last_time[s] = t
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p