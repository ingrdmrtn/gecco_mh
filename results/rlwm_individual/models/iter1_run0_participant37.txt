def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Slot-capacity WM with storage noise and decay scaled by set size.

    Model idea:
    - RL: standard Rescorla-Wagner update (as given in the template).
    - WM: for each state, stores a sharp distribution on the rewarded action when reward occurs,
      with storage noise (wm_eta). Between reinforced updates, WM decays toward uniform at a rate
      that increases with set size (slot-capacity gating): effective_wm_weight = wm_weight * min(1, K / nS).
    - Policy: mixture of RL softmax and WM softmax with a fixed high beta for WM.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values (used in provided RL update).
    - wm_weight: scalar in [0,1]. Baseline mixture weight for WM contribution.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    - wm_decay: scalar in [0,1]. Per-trial decay of WM toward uniform in absence of reward.
    - wm_eta: scalar in [0,1]. WM storage reliability; 1 - wm_eta mass goes to rewarded action, remaining spreads uniformly.
    - capacity_K: scalar > 0. WM slot capacity; WM mixture weight is scaled by min(1, K/nS).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_eta, capacity_K = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # capacity-based gating
        gate = min(1.0, float(capacity_K) / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: sharp softmax on WM distribution
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size scaled WM mixture weight (trial-wise overwrite of variable allowed)
            wm_weight = np.clip(wm_weight * gate, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (given)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - If rewarded: store a peaked distribution with storage noise wm_eta.
            # - If not rewarded: decay toward uniform at rate increasing with set size.
            if r >= 1.0 - 1e-12:
                w[s, :] = wm_eta / nA
                w[s, a] = 1.0 - wm_eta + (wm_eta / nA)
            else:
                # decay factor scales with set size: larger nS -> faster decay
                decay_eff = 1.0 - (1.0 - wm_decay) * min(1.0, 1.0 * (1.0 / max(1.0, nS)))
                w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Interference-based WM with cross-state contamination and entropy gating.

    Model idea:
    - RL: standard Rescorla-Wagner update (from template).
    - WM: after reward, store a peaked distribution on chosen action with noise wm_noise.
      On each trial, WM for the current state is contaminated by a weighted average of WM traces
      from other states. This interference grows with set size (interference parameter).
    - Mixture weight is adjusted by both capacity scaling and current WM sharpness (entropy gating):
      wm_weight_eff = wm_weight * min(1, K / nS) * (1 - H(W_s)/log(nA)).

    Parameters:
    - lr: RL learning rate (used in template RL update).
    - wm_weight: baseline WM mixture weight.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - interference: scalar in [0,1]. Degree of cross-state contamination per trial.
    - wm_noise: scalar in [0,1]. Storage noise when encoding a rewarded action.
    - capacity_K: scalar > 0. Capacity scaling factor for WM weight.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, interference, wm_noise, capacity_K = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        cap_gate = min(1.0, float(capacity_K) / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply cross-state interference before computing policy:
            if nS > 1 and interference > 0:
                # average of other states' WM
                others = [i for i in range(nS) if i != s]
                if len(others) > 0:
                    mean_other = np.mean(w[others, :], axis=0)
                    # interference strength scales with set size (more items -> more interference)
                    inter_eff = interference * min(1.0, (nS - 1) / max(1.0, capacity_K))
                    w[s, :] = (1.0 - inter_eff) * w[s, :] + inter_eff * mean_other

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based gating
            W_safe = np.clip(W_s, 1e-12, 1.0)
            H = -np.sum(W_safe * np.log(W_safe))
            sharp = 1.0 - H / np.log(nA)
            wm_weight = np.clip(wm_weight * cap_gate * sharp, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            if r >= 1.0 - 1e-12:
                # encode rewarded action with noise
                w[s, :] = wm_noise / nA
                w[s, a] = 1.0 - wm_noise + (wm_noise / nA)
            else:
                # mild relaxation to prior when not rewarded
                w[s, :] = 0.2 * w[s, :] + 0.8 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + RPE-gated WM with leak and set-size scaling.

    Model idea:
    - RL: standard.
    - WM: updated toward a peaked distribution on the chosen action with separate gains
      for positive vs negative outcomes (alpha_pos_wm, alpha_neg_wm). Also leaks toward uniform
      every trial with leak rate lambda_wm. WM mixture weight is dynamically scaled by both
      set size and current WM sharpness.
    - Intuition: positive RPEs should strengthen WM representations (fast binding),
      while negative outcomes should purge or weaken WM traces.

    Parameters:
    - lr: RL learning rate (template).
    - wm_weight: baseline WM mixture weight.
    - softmax_beta: RL inverse temperature (scaled by 10).
    - alpha_pos_wm: in [0,1]. WM binding rate after reward=1.
    - alpha_neg_wm: in [0,1]. WM unbinding rate after reward=0 (toward uniform).
    - lambda_wm: in [0,1]. Ongoing WM leak toward uniform every trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_pos_wm, alpha_neg_wm, lambda_wm = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # capacity scaling for WM mixture weight
        size_scale = min(1.0, 3.0 / max(1.0, nS))  # modest built-in set-size dependence

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # continuous leak toward uniform for the current state
            w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy softmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM sharpness via entropy for gating
            W_safe = np.clip(W_s, 1e-12, 1.0)
            H = -np.sum(W_safe * np.log(W_safe))
            sharp = 1.0 - H / np.log(nA)

            # dynamic wm_weight scaled by set size and sharpness
            wm_weight = np.clip(wm_weight * size_scale * sharp, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update rules:
            # If rewarded, move toward a peaked vector on chosen action
            if r >= 1.0 - 1e-12:
                target = np.full(nA, 0.0)
                target[:] = 0.0
                target += 0.0  # explicit for clarity
                target = (1.0 / nA) * np.ones(nA)
                target[a] = 1.0  # peak
                w[s, :] = (1.0 - alpha_pos_wm) * w[s, :] + alpha_pos_wm * target
            else:
                # if not rewarded, unbind toward uniform with its own rate
                w[s, :] = (1.0 - alpha_neg_wm) * w[s, :] + alpha_neg_wm * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p