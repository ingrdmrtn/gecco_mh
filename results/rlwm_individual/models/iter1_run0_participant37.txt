def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM with probabilistic storage and forgetting.

    Mechanism:
    - RL channel: standard delta-rule Q-learning, softmax policy with beta (scaled by 10).
    - WM channel: state-specific associative weights w[s,:] approximate a one-hot cache.
      • Probabilistic storage upon reward with probability scaled by WM capacity relative to set size.
      • Forgetting toward uniform at each visit with rate p_forget.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Set-size effect:
    - Effective WM storage probability is reduced when set size exceeds capacity K:
      p_store_eff = p_store * min(1, K / nS). Thus, at set size 6 with K<6,
      fewer successful WM writes occur than at set size 3.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - p_store: Base probability to store a rewarded action in WM (0..1).
    - p_forget: Forgetting/decay rate of WM toward uniform at each visit (0..1).
    - K: WM capacity in number of state-action pairs (>=1). Influences storage as K/nS.
    """
    lr, wm_weight, softmax_beta, p_store, p_forget, K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled storage probability
        cap_scale = min(1.0, max(0.0, K) / float(nS))
        p_store_eff = np.clip(p_store * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting toward uniform on each visit
            w[s, :] = (1.0 - p_forget) * w[s, :] + p_forget * w_0[s, :]

            # WM storage: on rewarded trial, with probability p_store_eff, cache the chosen action
            if r > 0.5:
                if np.random.rand() < p_store_eff:
                    w[s, :] = 0.0
                    w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM recency-and-outcome coding, and set-size-scaled RL beta.

    Mechanism:
    - RL channel: Q-learning with separate learning rates for positive vs negative outcomes.
      Softmax policy with inverse temperature beta_eff = beta * (1 + k_beta * (nS - 3)).
      Thus beta increases with set size if k_beta>0 (more exploitation) or decreases if k_beta<0.
    - WM channel: outcome-gated recency update:
      • If reward=1: w[s,:] moves toward a one-hot on chosen action (win-stay).
      • If reward=0: w[s,:] moves toward uniform over non-chosen actions (lose-shift tendency).
      Recency governs the step size of this WM update.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Set-size effect:
    - beta_eff scales with set size via k_beta, modulating RL determinism under higher load.

    Parameters (tuple):
    - lr_pos: RL learning rate for positive PE (rewarded trials) (0..1).
    - lr_neg: RL learning rate for negative PE (unrewarded trials) (0..1).
    - wm_weight: Mixture weight of WM vs RL (0..1).
    - softmax_beta: Base RL inverse temperature; internally scaled by 10 and then by size factor.
    - recency: WM step size for recency/outcome update (0..1).
    - k_beta: Linear scaling factor of RL beta with set size (can be negative or positive).
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, recency, k_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_eff = softmax_beta * (1.0 + k_beta * (nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0.0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM outcome-gated recency update
            if r > 0.5:
                # Move toward a one-hot vector on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # Move toward uniform over non-chosen actions (lose-shift)
                target = np.ones(nA) / (nA - 1)
                target[a] = 0.0

            w[s, :] = (1.0 - recency) * w[s, :] + recency * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated WM with set-size-weighted WM mixture.

    Mechanism:
    - RL channel: standard Q-learning, softmax with beta (scaled by 10).
    - WM channel: WM stores a one-hot action for a state only when surprise is high:
      gating condition depends on |prediction error| exceeding a threshold theta.
      Storage occurs with probability pe_gate when gated; otherwise no new storage.
      WM decays toward uniform each visit with decay = (1 - pe_gate) (higher pe_gate => slower decay).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.

    Set-size effect:
    - WM mixture weight decreases with set size via a logistic transform:
      wm_weight_eff = wm_weight / (1 + exp(gamma * (nS - 4.5))).
      For gamma > 0, WM weight is larger at nS=3 and smaller at nS=6.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight before set-size modulation (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - pe_gate: Probability to commit to WM when |PE| > theta; also sets 1 - decay.
    - theta: PE magnitude threshold for WM gating (>=0).
    - gamma: Set-size sensitivity of WM mixture weight (can be positive or negative).
    """
    lr, wm_weight, softmax_beta, pe_gate, theta, gamma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Logistic set-size modulation of WM mixture weight
        wm_weight_eff = wm_weight / (1.0 + np.exp(gamma * (nS - 4.5)))
        decay = max(0.0, min(1.0, 1.0 - pe_gate))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay toward uniform each visit
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # PE-gated WM storage: if surprise is high, possibly commit chosen action
            if abs(pe) > theta:
                if np.random.rand() < pe_gate:
                    w[s, :] = 0.0
                    w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p