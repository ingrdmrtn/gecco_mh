def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Capacity-limited Working Memory (WM) mixture model with age and set-size effects.

    Core idea:
    - Choices are a mixture of reinforcement learning (RL) and a fast WM policy.
    - WM contribution depends on set size via a capacity function and on age group.
    - Older adults have reduced WM weight relative to younger adults.
    - A small lapse probability allows off-task/invalid responses.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: Inverse temperature for RL softmax (scaled internally; higher -> more deterministic)
    - wm_weight_base: Baseline mixture weight of WM relative to RL (0..1)
    - wm_capacity: Effective WM capacity C; WM availability scales as exp(-set_size/C)
    - gamma_age: Multiplicative factor (<1 typically) on WM weight for older adults (>45), 1 means no age effect
    - lapse: Lapse probability (0..1) that produces random or off-task responding

    Inputs:
    - states: 1D array of state indices within block (0..set_size-1)
    - actions: 1D array of chosen actions (0..2 nominally; may contain invalid values)
    - rewards: 1D array of feedback (0/1 nominally; may contain other values which are clipped to [0,1] for learning)
    - blocks: 1D array of block indices aligning with trials
    - set_sizes: 1D array of set size on each trial (constant within a block; values like 3 or 6)
    - age: 1D array or list with a single number (participant's age)
    - model_parameters: list/tuple of parameters described above

    Returns:
    - Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, wm_weight_base, wm_capacity, gamma_age, lapse = model_parameters
    # scale beta to allow a wide dynamic range without requiring huge raw values
    beta = max(1e-6, beta) * 10.0
    alpha = min(max(alpha, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)  # cap lapse reasonably
    C = max(1e-6, wm_capacity)

    older = 1 if age[0] > 45 else 0
    # age factor reduces WM weight for older group if gamma_age < 1
    age_factor = gamma_age if older == 1 else 1.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM memory
        Q = np.zeros((nS, nA))
        # WM memory represented as a categorical distribution per state; start uniform
        W = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        # Keep track of "known" action for WM: last rewarded action per state (optional, aids sharpness)
        wm_known_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            r_clip = 1.0 if r > 0.5 else 0.0  # clip to {0,1} for learning

            # RL policy: probability of chosen action under softmax
            logits = beta * Q[s, :]
            # numerically stable log-softmax
            m = np.max(logits)
            exps = np.exp(logits - m)
            Z = np.sum(exps)
            # if action is valid, get RL prob; else set to 0 (only lapse can generate invalid actions)
            if 0 <= a < nA:
                p_rl = exps[a] / max(1e-12, Z)
            else:
                p_rl = 0.0

            # Working Memory availability depends on set size (smaller -> higher availability)
            ss = float(block_set_sizes[t])
            wm_availability = np.exp(-ss / C)  # decreases with set size; controlled by capacity C
            wm_weight = wm_weight_base * wm_availability * age_factor
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            # WM policy: if we have a known action for this state, assign higher mass to it
            # Build W_s distribution: if known action exists, make it one-hot; otherwise keep current W
            if wm_known_action[s] >= 0:
                W_s = np.zeros(nA, dtype=float)
                W_s[wm_known_action[s]] = 1.0
            else:
                W_s = W[s, :]

            if 0 <= a < nA:
                p_wm = W_s[a]
            else:
                p_wm = 0.0

            # Combine RL and WM with lapse. For invalid actions, only lapse contributes.
            if 0 <= a < nA:
                p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
                p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            else:
                # off-task/invalid choice; assign small probability proportional to lapse
                p_total = max(1e-12, lapse * 0.1)

            total_loglik += np.log(max(p_total, 1e-12))

            # Learning updates
            # RL update
            if 0 <= a < nA:
                delta = r_clip - Q[s, a]
                Q[s, a] += alpha * delta

            # WM update: if rewarded, store this action deterministically; else keep prior W
            if r_clip > 0.5 and 0 <= a < nA:
                wm_known_action[s] = a
                W[s, :] = 0.0
                W[s, a] = 1.0
            # If not rewarded, do not overwrite WM (assumes selective encoding of successful mappings)

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pure RL with age- and set-size-dependent learning, inverse temperature, and forgetting.

    Core idea:
    - Single RL system with softmax choice.
    - Learning rate and inverse temperature scale with cognitive load (set size) and age group.
    - A small forgetting/decay term pulls Q-values toward 0 between trials; older adults forget more.
    - Lapse handles random/off-task responses.

    Parameters (model_parameters):
    - alpha_base: Baseline learning rate
    - beta_base: Baseline inverse temperature (scaled internally)
    - decay_base: Baseline per-trial forgetting rate toward 0 (0=no decay)
    - age_mod_alpha: Multiplicative factor on alpha for older adults (>45); 1=no age effect
    - age_mod_beta: Multiplicative factor on beta for older adults (>45); 1=no age effect
    - lapse: Lapse probability (0..1)

    Inputs:
    - states: 1D array of state indices within block (0..set_size-1)
    - actions: 1D array of chosen actions (0..2 nominally; may contain invalid values)
    - rewards: 1D array of feedback (0/1 nominally; clipped to [0,1] for learning)
    - blocks: 1D array of block indices
    - set_sizes: 1D array of set size on each trial
    - age: 1D array/list with a single number (participant's age)
    - model_parameters: list/tuple described above

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha_base, beta_base, decay_base, age_mod_alpha, age_mod_beta, lapse = model_parameters
    beta_base = max(1e-6, beta_base) * 10.0
    alpha_base = min(max(alpha_base, 0.0), 1.0)
    decay_base = min(max(decay_base, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    older = 1 if age[0] > 45 else 0
    alpha_age = alpha_base * (age_mod_alpha if older == 1 else 1.0)
    beta_age = beta_base * (age_mod_beta if older == 1 else 1.0)
    # Ensure non-negativity
    alpha_age = min(max(alpha_age, 0.0), 1.0)
    beta_age = max(1e-6, beta_age)

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            r_clip = 1.0 if r > 0.5 else 0.0

            ss = float(block_set_sizes[t])
            # Load-dependent modulation: learn faster and act more deterministically in smaller sets
            load_scale = 3.0 / max(1.0, ss)  # equals 1 for set size 3; 0.5 for 6
            alpha_t = min(max(alpha_age * load_scale, 0.0), 1.0)
            beta_t = max(1e-6, beta_age * load_scale)

            # Apply forgetting toward 0 before observing outcome (older adults effectively decay more due to lower load_scale)
            decay_t = 1.0 - decay_base  # retain fraction
            Q[s, :] *= decay_t

            # Softmax policy
            logits = beta_t * Q[s, :]
            m = np.max(logits)
            exps = np.exp(logits - m)
            Z = np.sum(exps)

            if 0 <= a < nA:
                p_choice = exps[a] / max(1e-12, Z)
                p_total = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            else:
                p_total = max(1e-12, lapse * 0.1)

            total_loglik += np.log(max(p_total, 1e-12))

            # RL update
            if 0 <= a < nA:
                delta = r_clip - Q[s, a]
                Q[s, a] += alpha_t * delta

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Slot-based WM with probabilistic item-in-WM mixing, age effects on slot count, and WM inverse temperature.

    Core idea:
    - A fixed number of WM "slots" K can hold perfect stimulus-action mappings for some states.
    - Probability the current state is in WM is approximately min(1, K_eff / set_size), where
      K_eff = K_slots for younger, gamma_age * K_slots for older adults.
    - If the state is in WM and has been rewarded before, WM chooses via a sharp softmax; otherwise RL policy is used.
    - Lapse handles random/off-task responses.

    Parameters (model_parameters):
    - alpha: RL learning rate
    - beta: Inverse temperature for RL softmax (scaled internally)
    - K_slots: Effective number of WM slots (capacity in items)
    - wm_beta: Inverse temperature for WM softmax (sharpness of WM policy; scaled internally)
    - gamma_age: Multiplicative factor (<1 typical) reducing effective slots in older adults (>45)
    - lapse: Lapse probability

    Inputs:
    - states: 1D array of state indices within block (0..set_size-1)
    - actions: 1D array of chosen actions (0..2 nominally; may contain invalid values)
    - rewards: 1D array of feedback (0/1 nominally; clipped to [0,1] for learning)
    - blocks: 1D array of block indices
    - set_sizes: 1D array of set size on each trial
    - age: 1D array/list with a single number (participant's age)
    - model_parameters: list/tuple described above

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, beta, K_slots, wm_beta, gamma_age, lapse = model_parameters
    beta = max(1e-6, beta) * 10.0
    wm_beta = max(1e-6, wm_beta) * 10.0
    alpha = min(max(alpha, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)
    K_slots = max(0.0, K_slots)

    older = 1 if age[0] > 45 else 0
    K_eff_mult = gamma_age if older == 1 else 1.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        # WM memory: store last rewarded action per state; -1 indicates unknown
        wm_store = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            r_clip = 1.0 if r > 0.5 else 0.0

            # RL policy
            logits_rl = beta * Q[s, :]
            m_rl = np.max(logits_rl)
            exps_rl = np.exp(logits_rl - m_rl)
            Z_rl = np.sum(exps_rl)
            p_rl = exps_rl[a] / max(1e-12, Z_rl) if 0 <= a < nA else 0.0

            # WM policy: if we have a stored action for this state, use sharp softmax favoring that action
            if wm_store[s] >= 0:
                logits_wm = np.zeros(nA)
                logits_wm[wm_store[s]] = wm_beta  # sharp preference for stored action
                m_wm = np.max(logits_wm)
                exps_wm = np.exp(logits_wm - m_wm)
                Z_wm = np.sum(exps_wm)
                p_wm = exps_wm[a] / max(1e-12, Z_wm) if 0 <= a < nA else 0.0
            else:
                # No stored mapping -> WM behaves like uniform noise over valid actions
                p_wm = (1.0 / nA) if 0 <= a < nA else 0.0

            # Mixing weight equals probability the state is represented in WM
            ss = float(block_set_sizes[t])
            K_eff = K_slots * K_eff_mult
            wm_weight = min(1.0, K_eff / max(1.0, ss))

            if 0 <= a < nA:
                p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
                p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            else:
                p_total = max(1e-12, lapse * 0.1)

            total_loglik += np.log(max(p_total, 1e-12))

            # Learning and WM storage updates
            if 0 <= a < nA:
                delta = r_clip - Q[s, a]
                Q[s, a] += alpha * delta
            if r_clip > 0.5 and 0 <= a < nA:
                wm_store[s] = a  # store successful mapping

    return -total_loglik