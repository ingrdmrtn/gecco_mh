def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with load-dependent arbitration and supervised WM learning.

    Mechanism:
    - RL learns action values per state via a Rescorla-Wagner rule.
    - WM stores a supervised mapping for rewarded actions, decaying otherwise.
    - Arbitration between WM and RL depends on set size (lower WM weight at higher load).
      The WM weight is linearly interpolated between wm_strength3 (nS=3) and wm_strength6 (nS=6).
    - WM capacity is implicit via lower wm_strength6 and decay dynamics.

    Parameters:
    - lr: RL learning rate in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10 for dynamic range.
    - wm_strength3: WM weight when set size is 3 (low load), in [0,1].
    - wm_strength6: WM weight when set size is 6 (high load), in [0,1].
    - wm_learn: WM supervised learning rate towards rewarded 1-hot mapping in [0,1].
    - wm_forget: WM decay rate toward uniform after non-reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_strength3, wm_strength6, wm_learn, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interpolate WM weight by load: 0 at nS=3 -> wm_strength3, 1 at nS=6 -> wm_strength6
        load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
        wm_weight = (1.0 - load_term) * wm_strength3 + load_term * wm_strength6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of chosen action (reparameterized for stability)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM row (very deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM supervised update:
            # If rewarded, move w[s] toward one-hot on chosen action.
            # If not rewarded, decay w[s] toward uniform.
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # Normalize to avoid drift (optional but stabilizing)
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-driven WM arbitration with load penalty and noisy WM traces.

    Mechanism:
    - RL learns state-action values.
    - WM stores noisy traces biased toward recently rewarded actions; on errors, WM relaxes
      toward uniform with a controllable "noise" floor (wm_noise).
    - Arbitration weight for WM increases when RL is uncertain (high entropy of Q),
      but is penalized by load (more states -> lower WM weight).
      wm_weight_base is the baseline WM weight at zero-entropy and low load.
      wm_weight_load_slope increases the penalty as set size grows.
    - This arbitration is recomputed on each trial using the current state's RL entropy.

    Parameters:
    - lr: RL learning rate in [0,1].
    - softmax_beta: RL inverse temperature, scaled by 10 internally.
    - wm_weight_base: Base WM weight in [0,1].
    - wm_weight_load_slope: Load penalty for WM weight in [0,1], applied when nS increases.
    - wm_learn: WM learning rate toward rewarded one-hot mapping in [0,1].
    - wm_noise: Noise floor toward uniform after non-reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_weight_load_slope, wm_learn, wm_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load penalty computation: 0 at nS=3, 1 at nS=6
        load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
        load_penalty = wm_weight_load_slope * load_term

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax policy probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL uncertainty via entropy of softmax over Q_s
            logits = softmax_beta * (Q_s - np.max(Q_s))
            probs = np.exp(logits)
            probs = probs / np.sum(probs)
            entropy = -np.sum(probs * np.log(np.clip(probs, 1e-12, 1.0)))  # in nats
            max_entropy = np.log(nA)
            unc = entropy / max_entropy  # normalized uncertainty in [0,1]

            # WM weight is boosted by uncertainty but penalized by load, then clipped
            wm_weight = np.clip(wm_weight_base * (1.0 + unc) * (1.0 - load_penalty), 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # If rewarded, move toward one-hot for chosen action (supervised).
            # If not rewarded, move toward a noisy uniform (wm_noise blends uniform and current).
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                noisy_uniform = (1.0 - wm_noise) * w[s, :] + wm_noise * w_0[s, :]
                w[s, :] = noisy_uniform

            # Renormalize and bound
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + WM with interference and perseveration bias.

    Mechanism:
    - RL uses asymmetric learning rates for positive vs. negative prediction errors.
    - WM provides a fast supervised map that is susceptible to between-state interference
      that increases with load (larger set sizes cause WM rows to decay toward uniform
      proportionally to wm_interference_rate).
    - Action selection includes a perseveration (choice stickiness) term added to RL values.
    - Final policy is a mixture of WM and RL.

    Parameters:
    - lr_pos: RL learning rate for positive prediction errors in [0,1].
    - lr_neg: RL learning rate for negative prediction errors in [0,1].
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight: Fixed WM weight in [0,1] across loads (interference handles load).
    - wm_interference_rate: Per-trial WM decay-to-uniform factor scaled by load in [0,1].
    - perseveration_beta: Strength of choice stickiness added to last chosen action in RL softmax.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_interference_rate, perseveration_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Keep track of last action for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Load scaling for interference (0 at 3, 1 at 6)
        load_scale = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += perseveration_beta  # add stickiness to previously chosen

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update: supervised with interference
            if r > 0.5:
                # Move toward one-hot of the chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target  # fast update; implicit "WM snap"
            else:
                # Slight decay toward uniform on error
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Between-state interference scaled by load: every trial, all WM rows decay a bit
            decay = wm_interference_rate * load_scale
            if decay > 0.0:
                w = (1.0 - decay) * w + decay * w_0

            # Renormalize rows to avoid drift
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums[row_sums == 0.0] = 1.0
            w = np.clip(w, 1e-8, 1.0)
            w /= row_sums

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p