def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with age- and set-size–modulated WM contribution and WM decay.

    The model mixes a model-free RL policy with a simple working-memory (WM) policy.
    WM stores state-action associations strongly when rewarded, decays over trials,
    and contributes more when set size is small relative to WM capacity. Older adults
    are assumed to have lower WM capacity and higher decay (fixed, no extra params).

    Parameters
    ----------
    states : array-like of int
        State indices observed on each trial (0-indexed within block).
    actions : array-like of int
        Chosen actions on each trial. Valid actions are {0,1,2}. Invalid actions (<0 or >=3)
        are treated as misses and do not contribute to likelihood or learning.
    rewards : array-like of float or int
        Feedback on each trial. Valid rewards are {0,1}. Negative rewards are treated
        as missing/timeout and skip learning for that trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size (number of states in the block) for each trial.
    age : array-like or scalar
        Participant age in years. Age >= 45 is considered older group.
    model_parameters : list or array-like of float
        [alpha, beta, wm_weight_base, K_base, decay]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax decision noise (>0).
        - wm_weight_base: baseline WM mixture weight (0..1), scaled by capacity/set size.
        - K_base: WM capacity in items (>=1). Effective capacity reduced by 1 if older.
        - decay: WM decay toward uniform per trial (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, wm_weight_base, K_base, decay = model_parameters
    try:
        a_age = float(age[0])
    except Exception:
        a_age = float(age)
    is_older = 1.0 if a_age >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))  # WM policy proxy (probabilities-like)
        uniform = (1.0 / nA) * np.ones(nA)

        # Age-adjusted WM capacity and decay (no extra parameters)
        K_eff = max(1.0, K_base - 1.0 * is_older)
        decay_eff = np.clip(decay + 0.15 * is_older, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ns = int(block_set_sizes[t])

            # Skip invalid trials in likelihood and learning
            if (a < 0) or (a >= nA) or (r < 0):
                # Still decay WM to reflect time passing
                W = (1.0 - decay_eff) * W + decay_eff * (1.0 / nA)
                continue

            # RL policy
            q_s = Q[s, :]
            q_s_centered = q_s - np.max(q_s)
            rl_probs = np.exp(beta * q_s_centered)
            rl_probs = rl_probs / np.sum(rl_probs + 1e-12)

            # WM policy: convert W row to a softmaxed policy with the same beta
            w_s = W[s, :]
            w_s_centered = w_s - np.max(w_s)
            wm_probs = np.exp(beta * w_s_centered)
            wm_probs = wm_probs / np.sum(wm_probs + 1e-12)

            # Set-size and capacity scaled WM weight
            cap_ratio = np.clip(K_eff / float(ns), 0.0, 1.0)
            wm_weight = np.clip(wm_weight_base * cap_ratio, 0.0, 1.0)

            # Mixture policy
            mix_probs = wm_weight * wm_probs + (1.0 - wm_weight) * rl_probs
            pa = np.clip(mix_probs[a], 1e-12, 1.0)
            total_loglik += np.log(pa)

            # RL update
            delta = r - q_s[a]
            Q[s, a] = q_s[a] + alpha * delta

            # WM update with decay toward uniform, then incorporate current outcome
            # Decay entire W table slightly each trial (tonic decay)
            W = (1.0 - decay_eff) * W + decay_eff * (1.0 / nA)

            # Reward-driven WM sharpening for current state
            if r > 0.5:
                # Push WM for this state toward a one-hot at chosen action
                W[s, :] = (1.0 - decay_eff) * W[s, :] + decay_eff * uniform
                W[s, :] = 0.05 * np.ones(nA) + 0.85 * (np.arange(nA) == a).astype(float)
            else:
                # If not rewarded, slightly downweight chosen action and renormalize
                W[s, a] = 0.5 * W[s, a]
                row_sum = np.sum(W[s, :])
                if row_sum > 0:
                    W[s, :] = W[s, :] / row_sum

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model-free RL with perseveration bias, set-size–dependent exploration, and age-modulated
    inverse temperature and lapse.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0-indexed within block).
    actions : array-like of int
        Chosen actions per trial. Invalid actions (<0 or >=3) are skipped.
    rewards : array-like of float or int
        Reward feedback per trial (0 or 1). Negative values indicate missing and are skipped.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for each trial (3 or 6).
    age : array-like or scalar
        Participant age in years. Age >= 45 is older group.
    model_parameters : list or array-like of float
        [alpha, beta_base, persev, beta_size_slope, epsilon]
        - alpha: RL learning rate (0..1).
        - beta_base: base inverse temperature (>0).
        - persev: perseveration bias weight added to last chosen action in a state (>=0).
        - beta_size_slope: reduces beta as set size increases (>=0).
        - epsilon: lapse probability mixing with uniform (0..0.5).

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha, beta_base, persev, beta_size_slope, epsilon = model_parameters
    try:
        a_age = float(age[0])
    except Exception:
        a_age = float(age)
    is_older = 1.0 if a_age >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # track last action per state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ns = int(block_set_sizes[t])

            if (a < 0) or (a >= nA) or (r < 0):
                # no learning on invalid/missed trials
                continue

            # Age- and set-size–modulated beta
            beta_eff = beta_base * (1.0 - 0.30 * is_older)
            beta_eff = beta_eff / (1.0 + beta_size_slope * max(0, ns - 3) / 3.0)
            beta_eff = max(1e-6, beta_eff)

            # Perseveration bias vector for this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += persev

            # Softmax over Q + bias
            logits = Q[s, :] + bias
            logits = logits - np.max(logits)
            probs = np.exp(beta_eff * logits)
            probs = probs / np.sum(probs + 1e-12)

            # Age-modulated lapse (older -> more lapses, no extra params)
            eps_eff = np.clip(epsilon * (1.0 + 0.5 * is_older), 0.0, 0.5)
            mix_probs = (1.0 - eps_eff) * probs + eps_eff * (1.0 / nA)

            pa = np.clip(mix_probs[a], 1e-12, 1.0)
            total_loglik += np.log(pa)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha * delta

            # Update perseveration memory
            last_action[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid model with asymmetric RL learning rates, forgetting, and a set-size–gated WM shortcut.
    Age increases forgetting and lapse.

    Mechanism:
    - Model-free RL with separate learning rates for positive and negative RPEs.
    - Value forgetting (leaky decay toward zero) each time a state is visited.
    - A WM gate that, when set size is small, boosts probability of repeating the most recent
      rewarded action in that state (one-step memory). Gate strength decreases with set size.
    - Older adults have stronger forgetting and higher lapse probability (no extra params).

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0-indexed within block).
    actions : array-like of int
        Chosen actions per trial. Invalid actions (<0 or >=3) are skipped.
    rewards : array-like of float or int
        Reward feedback per trial (0 or 1). Negative indicates missing and is skipped.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial.
    age : array-like or scalar
        Participant age in years. Age >= 45 is older group.
    model_parameters : list or array-like of float
        [alpha_pos, alpha_neg, beta, rho_forget, wm_gate]
        - alpha_pos: learning rate when r > Q (0..1).
        - alpha_neg: learning rate when r <= Q (0..1).
        - beta: inverse temperature for softmax (>0).
        - rho_forget: forgetting rate per visit for the current state's Q (0..1).
        - wm_gate: controls WM gate steepness; higher -> more WM for small sets.

    Returns
    -------
    float
        Negative log-likelihood.
    """
    alpha_pos, alpha_neg, beta, rho_forget, wm_gate = model_parameters
    try:
        a_age = float(age[0])
    except Exception:
        a_age = float(age)
    is_older = 1.0 if a_age >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        last_rewarded_action = -np.ones(nS, dtype=int)  # one-step WM

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ns = int(block_set_sizes[t])

            if (a < 0) or (a >= nA) or (r < 0):
                continue

            # Forgetting on state visit (age increases forgetting)
            rho_eff = np.clip(rho_forget + 0.15 * is_older, 0.0, 1.0)
            Q[s, :] = (1.0 - rho_eff) * Q[s, :]

            # Base softmax from RL values
            logits = Q[s, :] - np.max(Q[s, :])
            rl_probs = np.exp(beta * logits)
            rl_probs = rl_probs / np.sum(rl_probs + 1e-12)

            # WM gate: logistic drop with set size (no extra params for age)
            # g(ns) = 1 / (1 + exp((ns - 3) * wm_gate)) scaled so g(3) high, g(6) lower
            g = 1.0 / (1.0 + np.exp((ns - 3.0) * wm_gate))
            g = np.clip(g, 0.0, 1.0)

            # WM policy: if we have a last rewarded action, put a bump on it
            wm_probs = np.ones(nA) / nA
            if last_rewarded_action[s] >= 0:
                wm_probs = 0.1 * np.ones(nA)
                wm_probs[last_rewarded_action[s]] = 0.8
                wm_probs = wm_probs / np.sum(wm_probs)

            mix_probs = g * wm_probs + (1.0 - g) * rl_probs

            # Age-modulated lapse
            lapse = np.clip(0.02 + 0.06 * is_older, 0.0, 0.2)
            probs = (1.0 - lapse) * mix_probs + lapse * (1.0 / nA)

            pa = np.clip(probs[a], 1e-12, 1.0)
            total_loglik += np.log(pa)

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            alpha_eff = alpha_pos if pe > 0 else alpha_neg
            Q[s, a] = Q[s, a] + alpha_eff * pe

            # Update WM memory if rewarded
            if r > 0.5:
                last_rewarded_action[s] = a
            else:
                # If not rewarded, clear memory for this state with small probability
                # increasing with set size (harder to retain)
                clear_prob = np.clip(0.1 + 0.1 * (ns - 3) / 3.0, 0.0, 0.5)
                if np.random.rand() < clear_prob:
                    last_rewarded_action[s] = -1

    return -float(total_loglik)