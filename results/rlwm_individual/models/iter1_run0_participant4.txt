def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + slot-limited WM mixture with decay.

    Mechanism:
    - RL: standard Q-learning but with separate learning rates for positive and negative RPEs.
    - WM: a fast, one-shot working-memory trace per state with decay; mixture weight is scaled
      by an effective capacity K relative to set size (min(1, K/nS)).
    - Policy: mixture of RL softmax and WM softmax (very deterministic).

    Parameters
    ----------
    model_parameters : tuple/list of 6 floats
        lr_pos        : Learning rate for positive prediction errors (rewarded trials).
        lr_neg        : Learning rate for negative prediction errors (unrewarded trials).
        wm_weight     : Base WM mixture weight in [0,1] (scaled by min(1, K/nS)).
        softmax_beta  : RL inverse temperature (internally scaled by *10).
        wm_decay      : Per-trial WM decay toward uniform in [0,1].
        K             : WM capacity (number of effective items), scales WM weight by min(1, K/nS).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, K = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_weight * min(1.0, float(K) / max(1.0, nS))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM trace; if near-uniform, ~uniform)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            eta = lr_pos if pe >= 0 else lr_neg
            q[s, a] += eta * pe

            # WM decay toward uniform, then one-shot strengthening on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # Store the rewarded action strongly
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-specific temperature and Q forgetting + WM as noisy one-shot memory.

    Mechanism:
    - RL: standard Q-learning with trial-wise forgetting toward uniform. The softmax inverse
      temperature differs for small vs large set sizes.
    - WM: stores the last rewarded action per state and retrieves it with noise that increases
      with set size. If no stored action, WM defaults to uniform.
    - Policy: mixture of RL with WM (weight does not directly depend on set size; instead
      WM reliability decreases with set size via retrieval noise).

    Parameters
    ----------
    model_parameters : tuple/list of 6 floats
        lr            : RL learning rate.
        beta_small    : Softmax inverse temperature for RL when nS is small (e.g., 3).
        beta_large    : Softmax inverse temperature for RL when nS is large (e.g., 6).
        wm_weight     : Mixture weight for WM policy in [0,1].
        wm_noise      : WM retrieval noise scaling; effective error = wm_noise * (nS-3)/3, clipped to [0,1).
        decay_base    : Q-value forgetting rate per trial toward uniform baseline in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta_small, beta_large, wm_weight, wm_noise, decay_base = model_parameters
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # not directly used for policy; keep for template consistency
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM store: for each state, -1 means empty; otherwise stores an action 0..2
        wm_store = -1 * np.ones(nS, dtype=int)

        beta = 10.0 * (beta_small if nS <= 3 else beta_large)
        # WM retrieval error increases with set size (0 at nS=3, up to wm_noise at nS=6)
        eta = np.clip(wm_noise * max(0.0, (nS - 3) / 3.0), 0.0, 0.99)
        decay = np.clip(decay_base, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(beta * (Q_s - Q_s[a])))

            # WM policy: if stored action exists, assign (1-eta) to stored, eta spread to others; else uniform
            if wm_store[s] >= 0:
                if a == wm_store[s]:
                    p_wm = 1.0 - eta
                else:
                    p_wm = eta / (nA - 1)
            else:
                p_wm = 1.0 / nA

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            pe = r - q[s, a]
            q[s, a] += lr * pe
            # apply decay to all actions in the visited state
            q[s, :] = (1.0 - decay) * q[s, :] + decay * w_0[s, :]

            # WM update: store last rewarded action; clear on repeated failures
            if r > 0:
                wm_store[s] = a
            else:
                # On error, weakly clear memory by drifting w toward uniform (kept for template)
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall associability + WM trace with load-sensitive retention.

    Mechanism:
    - RL: state-wise associability alpha_s tracks surprise (|RPE|). The effective learning rate
      is lr0 * alpha_s, where alpha_s is updated by an exponential filter with rate kappa and
      baseline stickiness phi (prevents collapse to zero).
    - WM: fast trace that strengthens for the chosen action after rewards and decays each trial.
      Decay is stronger with larger set size via an exponential mapping controlled by wm_sensitivity.
    - Policy: mixture of RL softmax and WM softmax; WM mixture weight is capped by wm_weight_max.

    Parameters
    ----------
    model_parameters : tuple/list of 6 floats
        lr0             : Base RL learning rate, scaled by associability.
        softmax_beta     : RL inverse temperature (scaled by *10 internally).
        wm_weight_max    : Maximum WM mixture weight in [0,1].
        wm_sensitivity   : Controls how WM retention worsens with set size (higher => faster decay for larger sets).
        kappa            : Associability update rate in (0,1]; larger => faster adaptation to surprise.
        phi              : Associability floor in [0,1], ensuring minimum learning even with low surprise.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr0, softmax_beta, wm_weight_max, wm_sensitivity, kappa, phi = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12
    kappa = np.clip(kappa, 0.0, 1.0)
    phi = np.clip(phi, 0.0, 1.0)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Associability per state
        alpha = np.ones(nS)  # start at 1 (max associability)

        # WM decay factor dependent on set size: larger nS => stronger decay
        # gamma in (0,1): retention per trial. For nS=3, gamma close to 1; for nS=6, smaller.
        gamma = np.exp(-wm_sensitivity * max(0.0, (nS - 3) / 3.0))
        gamma = np.clip(gamma, 0.0, 1.0)

        wm_weight_eff = np.clip(wm_weight_max, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with associability-modulated learning rate
            pe = r - q[s, a]
            eta = lr0 * alpha[s]
            q[s, a] += eta * pe

            # Update associability (Pearce-Hall style) toward |PE|
            alpha[s] = (1.0 - kappa) * alpha[s] + kappa * np.abs(pe)
            # enforce floor phi
            alpha[s] = max(alpha[s], phi)

            # WM update: decay toward uniform, then strengthen chosen action on reward
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]
            if r > 0:
                # Potentiate chosen action
                w[s, a] += (1.0 - w[s, a])

        blocks_log_p += log_p

    return -blocks_log_p