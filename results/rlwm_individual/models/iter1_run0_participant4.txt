def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM mixture with age- and set-size-dependent WM engagement, plus value and WM decay.

    Idea:
    - Choices arise from a mixture of a model-free RL system and a working-memory (WM) system.
    - WM stores recently rewarded actions per state with a confidence that decays over time.
    - The mixture weight (omega) depends on age group and set size:
        omega = sigmoid(wm_bias + wm_age_effect * I[older] + wm_setsize_slope * ((3 - set_size)/3))
      Thus, larger set sizes reduce WM engagement; older age can reduce or increase WM depending on sign.
    - RL values decay over time with a set-size-independent decay parameter.
    - WM confidence decays by the same decay parameter, ensuring all parameters are used meaningfully.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial within a block (0..nS-1).
    actions : 1D array-like of int
        Observed action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Reward feedback.
    blocks : 1D array-like of int
        Block index of each trial.
    set_sizes : 1D array-like of int
        Set size (3 or 6) on each trial.
    age : 1D array-like (single value)
        Participant age; age[0] is used.
    model_parameters : list/array of 6 floats
        [alpha, beta, wm_bias, wm_age_effect, wm_setsize_slope, decay]
        - alpha in [0,1]: RL learning rate.
        - beta > 0: inverse temperature of RL softmax (scaled by 10 internally).
        - wm_bias: baseline log-odds of WM engagement.
        - wm_age_effect: additive effect on WM log-odds for older group (I[age>=45]).
        - wm_setsize_slope: change in WM log-odds per unit of ((3 - set_size)/3).
                            Positive values increase WM use for small sets and decrease for large sets.
        - decay in [0,1]: per-trial decay factor applied to RL values (toward 0) and WM confidence.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    alpha, beta, wm_bias, wm_age_effect, wm_setsize_slope, decay = model_parameters
    # parameter hygiene
    alpha = max(0.0, min(1.0, alpha))
    beta = max(eps, beta) * 10.0
    decay = max(0.0, min(1.0, decay))
    is_older = 1.0 if age[0] >= 45 else 0.0
    nA = 3

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        # RL values
        Q = np.zeros((nS, nA))
        # WM store: action per state and confidence in [0,1]
        wm_act = -np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS, dtype=float)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            sz = float(block_set_sizes[t])

            # Decay RL values for current state (keeps model dynamic and uses decay)
            Q[s, :] *= (1.0 - decay)

            # RL policy
            prefs = Q[s, :] - np.max(Q[s, :])
            p_rl = np.exp(beta * prefs)
            p_rl /= (np.sum(p_rl) + eps)

            # WM policy: if we have a stored action, put mass according to confidence
            if wm_act[s] >= 0 and wm_conf[s] > 0.0:
                p_wm = np.ones(nA) * ((1.0 - wm_conf[s]) / nA)
                p_wm[wm_act[s]] += wm_conf[s] * (1.0 - 1.0 / nA)
            else:
                p_wm = np.ones(nA) / nA

            # WM engagement weight depends on age and set size via logistic
            logit_omega = wm_bias + wm_age_effect * is_older + wm_setsize_slope * ((3.0 - sz) / 3.0)
            omega = 1.0 / (1.0 + np.exp(-logit_omega))
            omega = min(max(omega, 0.0), 1.0)

            p = omega * p_wm + (1.0 - omega) * p_rl
            pa = max(p[a], eps)
            nll -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: reward-locked storage and confidence dynamics
            # On reward, store chosen action with high confidence; otherwise let confidence decay
            if r > 0.5:
                wm_act[s] = a
                wm_conf[s] = 1.0
            # Confidence decays each trial (uses decay parameter meaningfully)
            wm_conf[s] *= (1.0 - decay)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-dependent directed exploration bonus, set-size-dependent forgetting, and lapse.

    Idea:
    - Standard model-free RL for values.
    - Directed exploration via a count-based bonus added to preferences:
        bonus_group / sqrt(N[s,a]) where N[s,a] is the action visit count in state s.
      The bonus magnitude depends on age group (younger vs older).
    - Larger set sizes induce stronger value decay (forgetting), scaling the decay parameter.
    - Lapse probability mixes in uniform random choice.

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial.
    actions : 1D array-like of int
        Observed action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Reward feedback.
    blocks : 1D array-like of int
        Block index.
    set_sizes : 1D array-like of int
        Set size on each trial (3 or 6).
    age : 1D array-like (single value)
        Participant age; used to choose bonus magnitude.
    model_parameters : list/array of 6 floats
        [alpha, beta, bonus_young, bonus_old, decay, lapse]
        - alpha in [0,1]: RL learning rate.
        - beta > 0: inverse temperature (scaled by 10 internally).
        - bonus_young >= 0: exploration bonus magnitude for younger participants.
        - bonus_old >= 0: exploration bonus magnitude for older participants.
        - decay in [0,1]: base forgetting rate for RL values; effective decay scales with set size.
                          Effective per-trial forgetting factor for the visited state:
                          Q[s,:] *= (1 - decay * ((set_size - 3)/3)).
        - lapse in [0,1]: lapse rate mixing in uniform random choice.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    alpha, beta, bonus_y, bonus_o, decay, lapse = model_parameters
    alpha = max(0.0, min(1.0, alpha))
    beta = max(eps, beta) * 10.0
    bonus_y = max(0.0, bonus_y)
    bonus_o = max(0.0, bonus_o)
    decay = max(0.0, min(1.0, decay))
    lapse = max(0.0, min(1.0, lapse))
    is_older = age[0] >= 45
    bonus_group = bonus_o if is_older else bonus_y
    nA = 3

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        N = np.ones((nS, nA))  # start at 1 to avoid div by zero and bias extreme bonuses

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            sz = float(block_set_sizes[t])

            # Set-size-dependent forgetting on visited state's values
            decay_eff = decay * ((max(3.0, sz) - 3.0) / 3.0)
            Q[s, :] *= (1.0 - decay_eff)

            # Directed exploration bonus added to preferences
            bonus_vec = bonus_group / np.sqrt(N[s, :])

            prefs = Q[s, :] + bonus_vec
            prefs -= np.max(prefs)
            p_soft = np.exp(beta * prefs)
            p_soft /= (np.sum(p_soft) + eps)

            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)
            pa = max(p[a], eps)
            nll -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update counts after observing choice
            N[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric learning rates (gain vs loss) with age dependence, combined with
    outcome-contingent WSLS (win-stay/lose-shift) bias scaled by set size.

    Idea:
    - Model-free RL with separate learning rates for positive and negative outcomes.
      Learning rates are age-dependent: different rates for younger vs older participants.
    - Choice policy is softmax over Q with a WSLS bias applied to the last action in the current state:
        +wsls_eff if previous outcome in that state was a win (1), and -wsls_eff if it was a loss (0).
      The WSLS bias is stronger for small sets and weaker for large sets:
        wsls_eff = wsls_base * (3 / set_size).

    Parameters
    ----------
    states : 1D array-like of int
        State index on each trial.
    actions : 1D array-like of int
        Observed action on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Reward feedback.
    blocks : 1D array-like of int
        Block index.
    set_sizes : 1D array-like of int
        Set size on each trial (3 or 6).
    age : 1D array-like (single value)
        Participant age; determines younger (<45) vs older (>=45) group for learning rates.
    model_parameters : list/array of 6 floats
        [beta, alpha_pos_y, alpha_pos_o, alpha_neg_y, alpha_neg_o, wsls_base]
        - beta > 0: inverse temperature of softmax (scaled by 10 internally).
        - alpha_pos_y in [0,1]: learning rate for rewards (younger).
        - alpha_pos_o in [0,1]: learning rate for rewards (older).
        - alpha_neg_y in [0,1]: learning rate for non-rewards (younger).
        - alpha_neg_o in [0,1]: learning rate for non-rewards (older).
        - wsls_base >= 0: baseline magnitude of WSLS bias at set size 3 (scaled by 3/set_size).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    beta, a_py, a_po, a_ny, a_no, wsls_base = model_parameters
    beta = max(eps, beta) * 10.0
    a_py = max(0.0, min(1.0, a_py))
    a_po = max(0.0, min(1.0, a_po))
    a_ny = max(0.0, min(1.0, a_ny))
    a_no = max(0.0, min(1.0, a_no))
    wsls_base = max(0.0, wsls_base)
    is_older = age[0] >= 45
    nA = 3

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        last_action = -np.ones(nS, dtype=int)
        last_reward = -np.ones(nS, dtype=int)  # -1 indicates no prior outcome

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = int(block_rewards[t])
            sz = float(block_set_sizes[t])

            # WSLS bias scaled by set size
            wsls_eff = wsls_base * (3.0 / max(3.0, sz))
            bias = np.zeros(nA)
            if last_action[s] >= 0 and last_reward[s] >= 0:
                if last_reward[s] == 1:
                    bias[last_action[s]] += wsls_eff  # win-stay
                else:
                    bias[last_action[s]] -= wsls_eff  # lose-shift

            prefs = Q[s, :] + bias
            prefs -= np.max(prefs)
            p = np.exp(beta * prefs)
            p /= (np.sum(p) + eps)

            pa = max(p[a], eps)
            nll -= np.log(pa)

            # RL update with age-dependent asymmetric learning rates
            if r == 1:
                alpha = a_po if is_older else a_py
            else:
                alpha = a_no if is_older else a_ny
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            last_action[s] = a
            last_reward[s] = r

    return nll