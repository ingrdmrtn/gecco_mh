def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and lapse.
    Arbitration favors the system (RL or WM) with lower action entropy on the current state,
    scaled by set size (3/nS) to reduce WM influence under higher load. WM learns from rewarded
    outcomes and decays toward uniform on non-rewarded visits. A small lapse probability mixes
    in uniform choices.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Baseline WM mixture weight (0-1), scaled online by both set size and entropy difference.
        softmax_beta : float
            Base RL inverse temperature; internally scaled by 10.
        epsilon_lapse : float
            Lapse probability mixing a uniform policy (0-1).
        wm_lr : float
            WM learning rate toward one-hot target when rewarded and toward uniform when not (0-1).
        arbitra_slope : float
            Slope of the sigmoid arbitration on the entropy difference H_rl - H_wm.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, epsilon_lapse, wm_lr, arbitra_slope = model_parameters
    softmax_beta *= 10.0  # higher upper bound for RL inverse temperature
    softmax_beta_wm = 50.0  # WM is near-deterministic when confident

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Entropy-based arbitration, scaled by set size
            eps = 1e-12
            H_rl = -np.sum(pvec_rl * np.log(np.clip(pvec_rl, eps, 1.0)))
            H_wm = -np.sum(pvec_wm * np.log(np.clip(pvec_wm, eps, 1.0)))
            # Positive when RL is more uncertain than WM
            diff = H_rl - H_wm
            gate = 1.0 / (1.0 + np.exp(-arbitra_slope * diff))
            eff_wm_weight = np.clip(wm_weight_base * gate * (3.0 / float(nS)), 0.0, 1.0)

            # Mixture with lapse
            p_mix = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = (1.0 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: learn toward one-hot on reward, decay toward uniform on non-reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target
            else:
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM with overload-dependent noise.
    WM becomes noisier (lower effective inverse temperature) when set size exceeds capacity K,
    and its mixture weight is reduced proportionally to K/nS. RL uses eligibility traces to
    propagate credit to recently selected state-action pairs.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base WM mixture weight (0-1), scaled by min(1, K/nS).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        lambda_elig : float
            Eligibility trace decay parameter lambda (0-1).
        wm_capacity_K : float
            WM capacity in number of items; reduces influence when nS > K.
        wm_noise_kappa : float
            Scales WM noise increase when capacity is exceeded.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_elig, wm_capacity_K, wm_noise_kappa = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm_base = 50.0

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces per state-action
        e = np.zeros((nS, nA))

        # Overload and WM temperature scaling
        overload = max(0.0, float(nS) - float(wm_capacity_K))
        beta_wm_eff = softmax_beta_wm_base / (1.0 + wm_noise_kappa * overload)
        # Mixture scaling by capacity fraction
        cap_scale = min(1.0, float(wm_capacity_K) / float(nS)) if nS > 0 else 1.0
        eff_wm_weight_block = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Mild interference/decay toward uniform under overload on each visit
            if overload > 0:
                alpha_int = np.clip(wm_noise_kappa * overload / (1.0 + wm_noise_kappa * overload), 0.0, 1.0)
                w[s, :] = (1.0 - alpha_int) * w[s, :] + alpha_int * w_0[s, :]

            # RL policy
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy with overload-dependent temperature
            W_s = w[s, :]
            logits_wm = beta_wm_eff * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            p_total = eff_wm_weight_block * p_wm + (1.0 - eff_wm_weight_block) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # Update eligibility traces
            e *= lambda_elig
            e[s, a] += 1.0

            # RL update with traces: compensate template line, then apply full trace update
            delta = r - q[s, a]
            q[s, a] += lr * delta         # template-prescribed update
            q[s, a] -= lr * delta         # cancel to implement full trace update below
            q += lr * delta * e

            # WM update: on reward, store one-hot; otherwise leave as is (interference already applied)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and volatility-adaptive temperature + WM with decay.
    RL uses separate learning rates for positive vs negative outcomes, and a per-state
    volatility signal (running absolute TD error) that downregulates the RL inverse temperature.
    WM decays toward uniform each visit but strengthens on rewarded trials. Arbitration weight
    scales with set size and is reduced under high volatility (favor RL exploration).

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr_pos : float
            RL learning rate for rewarded outcomes (0-1).
        lr_neg : float
            RL learning rate for non-rewarded outcomes (0-1).
        softmax_beta : float
            Base RL inverse temperature; internally scaled by 10; volatility reduces it.
        wm_weight_base : float
            Baseline WM mixture weight (0-1), modulated by set size and volatility.
        volatility_lr : float
            Step size for updating per-state volatility from |delta| (0-1).
        wm_decay : float
            WM decay per visit toward uniform (0-1); also used when strengthening on reward.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, volatility_lr, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Per-state volatility estimates
        v = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Volatility-adjusted RL temperature (lower beta when volatility high)
            beta_eff = softmax_beta / (1.0 + v[s])

            # RL policy
            Q_s = q[s, :]
            logits_rl = beta_eff * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM decay each visit toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Arbitration: scale by set size and reduce WM weight under volatility
            size_scale = 3.0 / float(nS)
            wm_vol_mod = 1.0 / (1.0 + v[s])  # less WM when volatile
            eff_wm_weight = np.clip(wm_weight_base * size_scale * wm_vol_mod, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s, a]
            lr_t = lr_pos if r > 0 else lr_neg
            q[s, a] += lr_t * delta

            # Update volatility from absolute TD error
            v[s] = (1.0 - volatility_lr) * v[s] + volatility_lr * np.abs(delta)

            # WM strengthen on reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use half of wm_decay as consolidation step to avoid overriding decay completely
                cons = 0.5 * wm_decay
                w[s, :] = (1.0 - cons) * w[s, :] + cons * target

        blocks_log_p += log_p

    return -blocks_log_p