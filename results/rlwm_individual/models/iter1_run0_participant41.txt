def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM mixture with uncertainty-gated WM and capacity- and noise-limited retrieval.

    Idea:
    - RL: delta-rule with softmax choice.
    - WM: fast associative store that is precise but noisy; decays toward uniform; retrieval is noisy.
    - Mixture: WM contribution is (a) capacity-limited by set size, and (b) gated by RL uncertainty
      (higher RL entropy -> rely more on WM), modulated by a gating temperature.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_strength: baseline WM mixture weight (0..1), before gating and capacity limits.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay: WM decay rate toward uniform on each visit to a state (0..1).
    - wm_noise: lapse in WM retrieval, mixing with a uniform distribution (0..0.5).
    - gate_temp: temperature of uncertainty-gating (positive). Lower -> sharper gating.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_strength, softmax_beta, wm_decay, wm_noise, gate_temp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def softmax_probs(beta, vals):
        x = beta * (vals - np.max(vals))
        ex = np.exp(x)
        return ex / np.sum(ex)

    def entropy(p):
        p = np.clip(p, eps, 1.0)
        return -np.sum(p * np.log(p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        uniform = np.ones(nA) / nA

        # Capacity factor reduces WM availability as set size grows
        cap = 1.0 / max(1.0, nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (via softmax parity in the template)
            p_rl_vec = softmax_probs(softmax_beta, Q_s)
            p_rl = p_rl_vec[a]

            # WM policy: softmax over W with high beta, but with retrieval lapse
            p_wm_vec = softmax_probs(softmax_beta_wm, W_s)
            p_wm_vec = (1.0 - wm_noise) * p_wm_vec + wm_noise * uniform
            p_wm = p_wm_vec[a]

            # Uncertainty-gating based on RL entropy in this state
            H_rl = entropy(p_rl_vec)
            # Maximum entropy for 3 actions is log(3)
            H_max = np.log(nA)
            # Normalized entropy in [0,1]
            H_norm = H_rl / H_max
            # Gate via logistic; higher uncertainty -> higher WM reliance
            gate = 1.0 / (1.0 + np.exp(-(H_norm - 0.5) / max(eps, gate_temp)))

            wm_avail = wm_strength * cap * gate

            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform on each visit to this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # If rewarded, strengthen association in WM (normalize implicitly via next softmax)
            if r > 0.5:
                # Move W toward a one-hot on action a by adding a small boost then renormalizing
                w[s, :] *= (1.0 - wm_decay)
                w[s, a] += wm_decay  # reuse decay as a strengthening magnitude
                # Renormalize to keep W_s roughly within simplex
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and perseveration + WM with interference across items.

    Idea:
    - RL: separate learning rates for positive/negative RPE; softmax choice with action perseveration.
    - WM: fast one-shot storage of rewarded associations; state-specific but subject to global
      interference that grows with set size (decay of all WM rows each trial).
    - Mixture: fixed WM weight combined with RL choice probabilities.

    Parameters (tuple):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight: mixture weight for WM contribution (0..1).
    - interference: WM global interference strength per trial (>=0), scaled by set size.
    - persev_beta: inverse temperature for action perseveration bias (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, interference, persev_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action in each state for perseveration
        last_act = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add perseveration bias to RL values for repeating the last action in the state
            if last_act[s] >= 0:
                Q_s[last_act[s]] += persev_beta

            # RL policy probability of chosen action using pairwise normalization (template-equivalent)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic over W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s][a]
            if pe >= 0:
                q[s][a] += lr_pos * pe
            else:
                q[s][a] += lr_neg * pe

            # WM global interference: every trial, all states decay toward uniform,
            # scaled by set size (more items -> more interference)
            if interference > 0:
                lam = 1.0 - 1.0 / (1.0 + interference * nS)
                w = (1.0 - lam) * w + lam * w_0

            # State-specific WM reinforcement if rewarded
            if r > 0.5:
                # Winner-take-all boost for chosen action and suppress others slightly
                boost = 0.5
                inhibit = 0.5 / (nA - 1)
                w[s, :] = (1.0 - boost - (nA - 1) * inhibit) * w[s, :] + inhibit
                w[s, a] += boost
                # Renormalize row
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum

            # Update last action for perseveration
            last_act[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM with retrieval failure driven by set size and recency (state age), plus stickiness.

    Idea:
    - RL: standard delta-rule with softmax and action stickiness bias.
    - WM: stores one-hot action when rewarded; retrieval can fail with probability that
      increases with set size and with the age (trials since last successful WM update for that state).
    - Mixture: WM contributes with weight scaled by retrieval success probability.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight: baseline WM weight (0..1).
    - fail_base: base retrieval failure rate (>0); larger -> more failures.
    - recency_decay: exponent controlling how age impacts failure (>0).
    - stickiness: bias added to the last chosen action in the current state (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, fail_base, recency_decay, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track age since last rewarded WM update per state
        age = np.zeros(nS, dtype=float)
        # Track last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Increase age for all states each trial
            age += 1.0

            # RL with stickiness
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy is nearly deterministic over W_s
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Retrieval failure probability depends on age and set size
            nS_block = float(nS)
            failure_prob = 1.0 - np.exp(-fail_base * (age[s] ** recency_decay) * nS_block)
            failure_prob = np.clip(failure_prob, 0.0, 1.0)
            success_prob = 1.0 - failure_prob

            wm_avail = wm_weight * success_prob

            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: if rewarded, write one-hot and reset age
            if r > 0.5:
                w[s, :] = w_0[s, :]  # start from uniform baseline
                w[s, a] = 1.0  # one-hot
                # renormalize to sum to 1
                w[s, :] /= np.sum(w[s, :])
                age[s] = 0.0  # reset age on successful WM update
            else:
                # mild decay toward uniform over time when not updated
                lam = 0.05
                w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p