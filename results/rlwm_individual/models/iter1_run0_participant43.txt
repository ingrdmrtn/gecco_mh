def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + gated retrieval model with age- and load-dependent gating and noisy retrieval.

    Idea:
    - Choices come from a mixture of a slow RL system (Q-values) and a lightweight "retrieval" system
      that stores the last rewarded action for a state (a sparse WM-like cache).
    - A gate determines whether retrieval is attempted on a trial; the gate closes with higher load
      (set size 6 vs 3) and for older adults.
    - Retrieval itself is noisy: even when the cached action is available, selection is softened and
      mixed with a uniform policy (to capture retrieval noise), more so for older adults.

    Parameters (model_parameters): 5 total
    - alpha: RL learning rate in (0,1]
    - beta: base inverse temperature (>0) used for both RL and retrieval softmax
    - gate_base: baseline gating log-odds (unconstrained real). Higher => more retrieval usage
    - retrieve_noise: [0,1] mixing weight with uniform during retrieval. Higher => noisier retrieval
    - age_sensitivity: >=0 strength with which older age reduces gate and increases retrieval noise

    Inputs:
    - states: np.array of state indices per trial (int)
    - actions: np.array of chosen actions per trial (0..2)
    - rewards: np.array of rewards per trial (0/1)
    - blocks: np.array of block indices per trial
    - set_sizes: np.array of set size per trial (3 or 6)
    - age: np.array with single entry (participant age in years)
    - model_parameters: list/tuple of 5 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, gate_base, retrieve_noise, age_sensitivity = model_parameters

    # Constrain/scale parameters to sensible ranges
    nll = 0.0
    alpha = min(max(alpha, 1e-6), 1.0)
    beta = max(beta, 1e-6) * 5.0
    retrieve_noise = min(max(retrieve_noise, 0.0), 1.0)
    age_sensitivity = max(0.0, age_sensitivity)

    is_older = 1.0 if age[0] >= 45 else 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx]
        b_states = states[idx].astype(int)
        b_setsize = int(set_sizes[idx][0])

        nA = 3
        nS = b_setsize

        # RL values
        Q = np.zeros((nS, nA))

        # Retrieval cache: store strengths; when rewarded we set one-hot for the rewarded action
        M = np.ones((nS, nA)) / nA  # start as uniform (no cache yet)

        # Precompute load penalty (0 for 3, 1 for 6)
        load_penalty = 0.0 if b_setsize <= 3 else 1.0

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            exp_rl = np.exp(logits_rl)
            p_rl = exp_rl / np.sum(exp_rl)

            # Retrieval policy (noisy)
            m_s = M[s, :]
            logits_ret = beta * (m_s - np.max(m_s))
            exp_ret = np.exp(logits_ret)
            p_ret_clean = exp_ret / np.sum(exp_ret)
            # Mix with uniform to capture retrieval noise; older => more noise
            noise = min(1.0, retrieve_noise * (1.0 + 0.5 * age_sensitivity * is_older))
            p_ret = (1.0 - noise) * p_ret_clean + noise * (1.0 / nA)

            # Gate probability depends on gate_base, load, and age
            gate_logit = gate_base - (0.75 * load_penalty) - (age_sensitivity * is_older)
            gate = 1.0 / (1.0 + np.exp(-gate_logit))
            gate = min(max(gate, 0.0), 1.0)

            # Mixture policy
            p = gate * p_ret + (1.0 - gate) * p_rl

            pa = max(1e-12, p[a])
            nll -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Retrieval cache update:
            # - mild relaxation toward uniform each trial
            # - if rewarded, cache that chosen action strongly (overwrite toward one-hot)
            relax = 0.10 + 0.10 * load_penalty + 0.10 * is_older  # more relaxation with load/age
            relax = min(max(relax, 0.0), 1.0)
            M[s, :] = (1.0 - relax) * M[s, :] + relax * (np.ones(nA) / nA)

            if r > 0.5:
                # Cache the rewarded mapping
                target = np.zeros(nA)
                target[a] = 1.0
                M[s, :] = 0.2 * M[s, :] + 0.8 * target  # rapid imprinting on reward

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian precision-weighted RL with age- and load-modulated forgetting and adaptive temperature.

    Idea:
    - For each state-action, maintain a Beta posterior over reward probability: Beta(succ, fail).
    - Expected value is the posterior mean; choice is softmax over these means.
    - Inverse temperature scales with posterior precision (more evidence => more exploitation).
    - Counts decay over time within a block; decay increases with larger set size and in older adults.
    - Includes a small lapse.

    Parameters (model_parameters): 6 total
    - prior_conc: >=0 prior pseudo-count for both success and failure (Beta prior is (prior, prior))
    - decay: base per-trial forgetting (0..1); 0=no forgetting, 1=full reset per step
    - beta_scale: scales the effective inverse temperature (>0)
    - lapse: action-independent lapse (0..1), mixed with the policy
    - age_forgetting: >=0 increases effective decay for older adults
    - load_gain: >=0 increases decay as set size grows (applied when nS=6 vs 3)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: list/tuple of 6 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    prior_conc, decay, beta_scale, lapse, age_forgetting, load_gain = model_parameters

    nll = 0.0
    prior_conc = max(0.0, prior_conc)
    decay = min(max(decay, 0.0), 1.0)
    beta_scale = max(beta_scale, 1e-6)
    lapse = min(max(lapse, 1e-8), 0.25)
    age_forgetting = max(0.0, age_forgetting)
    load_gain = max(0.0, load_gain)

    is_older = 1.0 if age[0] >= 45 else 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx]
        b_states = states[idx].astype(int)
        b_setsize = int(set_sizes[idx][0])

        nA = 3
        nS = b_setsize

        # Posterior counts for Beta(s,f) per state-action
        S = np.ones((nS, nA)) * prior_conc  # successes
        F = np.ones((nS, nA)) * prior_conc  # failures

        # Effective decay per trial, increased with load and age
        load_penalty = 0.0 if b_setsize <= 3 else 1.0
        eff_decay = decay + load_gain * load_penalty + age_forgetting * is_older
        eff_decay = min(max(eff_decay, 0.0), 1.0)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # Posterior means and precision for this state
            means = S[s, :] / np.maximum(S[s, :] + F[s, :], 1e-12)
            precisions = S[s, :] + F[s, :]  # larger => more certain

            # Use average precision to scale inverse temperature for this state
            avg_prec = np.mean(precisions)
            beta_eff = beta_scale * (avg_prec / (2.0 * max(prior_conc, 1e-6)))
            beta_eff = max(beta_eff, 1e-6)

            # Softmax over means with precision-weighted temperature
            logits = beta_eff * (means - np.max(means))
            expv = np.exp(logits)
            p = expv / np.sum(expv)

            # Lapse mixture
            p_final = (1.0 - lapse) * p + lapse * (1.0 / nA)

            pa = max(1e-12, p_final[a])
            nll -= np.log(pa)

            # Apply decay to all counts for this state (fading memory)
            S[s, :] = (1.0 - eff_decay) * S[s, :] + eff_decay * prior_conc
            F[s, :] = (1.0 - eff_decay) * F[s, :] + eff_decay * prior_conc

            # Update chosen action with observed outcome
            if r > 0.5:
                S[s, a] += 1.0
            else:
                F[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Habit-RL arbitration with action kernel, age bias, and load sensitivity.

    Idea:
    - Two value systems:
        1) RL: Q-values updated via TD learning.
        2) Habit cache H: rapidly strengthens rewarded S->A associations and otherwise decays to uniform.
    - Arbitration weight favors Habit in low load (set size 3) and RL in high load (set size 6).
      Older adults are biased away from Habit (less reliable WM-like processes).
    - A decaying choice kernel K adds perseveration independent of value, capturing motor/response inertia.

    Parameters (model_parameters): 6 total
    - alpha_rl: RL learning rate (0..1]
    - alpha_habit: habit learning rate (0..1] for imprinting after reward
    - beta: inverse temperature (>0) for value combination
    - arb_age_bias: real; older bias term subtracted from habit arbitration (>=0 -> less habit if older)
    - kernel_strength: >=0 amount added to the last chosen action's logit
    - kernel_decay: (0..1) decay of the choice kernel per trial

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as specified
    - model_parameters: list/tuple of 6 parameters in the order above

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_rl, alpha_habit, beta, arb_age_bias, kernel_strength, kernel_decay = model_parameters

    nll = 0.0
    alpha_rl = min(max(alpha_rl, 1e-6), 1.0)
    alpha_habit = min(max(alpha_habit, 1e-6), 1.0)
    beta = max(beta, 1e-6) * 5.0
    arb_age_bias = float(arb_age_bias)
    kernel_strength = max(0.0, kernel_strength)
    kernel_decay = min(max(kernel_decay, 0.0), 1.0)

    is_older = 1.0 if age[0] >= 45 else 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_actions = actions[idx].astype(int)
        b_rewards = rewards[idx]
        b_states = states[idx].astype(int)
        b_setsize = int(set_sizes[idx][0])

        nA = 3
        nS = b_setsize

        Q = np.zeros((nS, nA))
        H = np.ones((nS, nA)) / nA  # habit starts uniform (no preference)
        K = np.zeros(nA)            # choice kernel (global across states within block)

        # Arbitration weight for this block:
        # favor habit when set size is small; penalize when older.
        # nS in {3,6} -> load_term in {0,-3}
        load_term = 3 - nS  # 0 for 3, -3 for 6
        habit_logit = 0.75 * load_term - (arb_age_bias * is_older)
        w_habit = 1.0 / (1.0 + np.exp(-habit_logit))
        w_habit = min(max(w_habit, 0.0), 1.0)

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = b_rewards[t]

            # Combined value
            V = (1.0 - w_habit) * Q[s, :] + w_habit * H[s, :]

            # Add choice kernel to logits
            logits = beta * (V - np.max(V)) + K - np.max(K)  # center to improve stability
            expv = np.exp(logits)
            p = expv / np.sum(expv)

            pa = max(1e-12, p[a])
            nll -= np.log(pa)

            # Updates
            # RL TD update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # Habit update: decay to uniform; imprint on reward
            H[s, :] = (1.0 - 0.2) * H[s, :] + 0.2 * (np.ones(nA) / nA)  # mild decay each trial
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                H[s, :] = (1.0 - alpha_habit) * H[s, :] + alpha_habit * target

            # Choice kernel update
            K = (1.0 - kernel_decay) * K
            K[a] += kernel_strength

    return nll