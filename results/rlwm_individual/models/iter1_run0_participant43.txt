def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with capacity-gated weight and state-wise stickiness bias.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors (alpha_pos, alpha_neg).
    - WM stores a strong trace for rewarded state-action pairs; without reward it decays toward uniform.
    - WM influence is capacity-gated via a decreasing sigmoid of set size: larger nS -> weaker WM influence.
    - A state- and action-specific stickiness bias (kappa_stick) favors repeating the last action taken in that state.
      The bias affects both RL and WM policies by adding a transient bonus to the last action.
    
    Parameters (tuple):
    - alpha_pos: RL learning rate for positive prediction errors (0..1).
    - alpha_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight: Base mixture weight for WM vs RL (0..1).
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - theta_cap: Capacity pivot for WM gating; larger means stronger WM in larger set sizes.
                 Effective WM gate = wm_weight * sigmoid(theta_cap - nS).
    - kappa_stick: Stickiness strength added to the last chosen action within the same state (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, theta_cap, kappa_stick = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Keep track of last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # WM capacity gate decreases with set size: gate = wm_weight * sigmoid(theta_cap - nS)
        wm_gate_block = wm_weight * (1.0 / (1.0 + np.exp(-(theta_cap - float(nS)))))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Stickiness bias vector for current state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += kappa_stick

            # RL choice prob with stickiness
            Q_eff = Q_s + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM choice prob with stickiness
            W_eff = W_s + bias
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture with capacity gate
            wm_gate_s = wm_gate_block
            p_total = wm_gate_s * p_wm + (1.0 - wm_gate_s) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0.0 else alpha_neg
            q[s, a] += alpha * delta

            # WM update: reward-gated imprint; otherwise decay toward uniform
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Strong imprint, mild retention of previous contents
                w[s, :] = 0.3 * w[s, :] + 0.7 * target
            else:
                # Decay toward uniform when unrewarded
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with visit-based arbitration and load-dependent WM precision.

    Idea:
    - WM acts as a fast learner but is noisy under higher load (set size).
    - Arbitration shifts from WM to RL as a state is revisited: WM weight decays with visit count.
      wm_gate(s, t) = wm_weight * exp(-visits_s / tau_visit).
    - WM precision (temperature) decreases with set size via factor 1 / (1 + rho_load * (nS-3)).
    - WM updates quickly toward the chosen action after reward; decays toward uniform otherwise.
      A single parameter lambda_wm controls both imprint (on reward) and decay magnitude.

    Parameters (tuple):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Base WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - lambda_wm: WM learning/decay strength (0..1).
    - tau_visit: Visit time-constant controlling WM->RL shift (>=1e-3).
    - rho_load: Load-sensitivity of WM precision (>=0); higher means more WM noise in larger sets.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_wm, tau_visit, rho_load = model_parameters
    softmax_beta *= 10.0
    base_softmax_beta_wm = 50.0
    tau_visit = max(tau_visit, 1e-3)
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts per state for arbitration
        visits = np.zeros(nS, dtype=float)

        # Load-dependent WM precision
        wm_prec_scale = 1.0 / (1.0 + max(0.0, rho_load) * max(0.0, (float(nS) - 3.0)))
        softmax_beta_wm = base_softmax_beta_wm * wm_prec_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-reduced precision
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Visit-based arbitration: more visits -> lower WM reliance
            wm_gate_s = wm_weight * np.exp(-visits[s] / tau_visit)

            p_total = wm_gate_s * p_wm + (1.0 - wm_gate_s) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens, no-reward decays to uniform
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * target
            else:
                w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * w_0[s, :]

            # Increment visit counter AFTER using it
            visits[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with binding errors under load and adjustable WM precision.

    Idea:
    - WM stores near one-hot associations for rewarded pairs and leaks toward uniform otherwise.
    - Under higher load, retrieval from WM suffers binding errors: the wrong state's WM trace may be retrieved.
      We implement this by mixing the state's WM policy with the average WM policy of the other states.
      binding_error(nS) = min(0.5, bind_slope * max(0, nS-3) / 3).
    - WM precision (inverse temperature) is scaled by wm_temp_scale (0..1).
    - The final policy is a mixture of WM and RL controlled by wm_weight.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM (0..1).
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - leak_wm: WM leak toward uniform when unrewarded (0..1).
    - wm_temp_scale: Scales WM inverse temperature (0..1), effective beta_wm = 50 * wm_temp_scale.
    - bind_slope: Controls how quickly binding errors increase with set size (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, leak_wm, wm_temp_scale, bind_slope = model_parameters
    softmax_beta *= 10.0
    wm_temp_scale = min(max(wm_temp_scale, 0.0), 1.0)
    base_softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Binding error probability under load
        bind_err = min(0.5, max(0.0, bind_slope) * max(0.0, (float(nS) - 3.0)) / 3.0)
        softmax_beta_wm = base_softmax_beta_wm * wm_temp_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability for the correct state
            p_wm_s = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM probability if a different state's trace is (mis)retrieved: use the average of other states
            if nS > 1:
                other_idx = [i for i in range(nS) if i != s]
                W_other_mean = np.mean(w[other_idx, :], axis=0)
            else:
                W_other_mean = W_s.copy()
            p_wm_other = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_other_mean - W_other_mean[a])))

            # Binding error mixture within WM
            p_wm = (1.0 - bind_err) * p_wm_s + bind_err * p_wm_other

            # Final mixture with RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens strongly; no reward -> leak toward uniform
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.3 * w[s, :] + 0.7 * target
            else:
                w[s, :] = (1.0 - leak_wm) * w[s, :] + leak_wm * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p