def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM mixture with age- and set-size dependent WM engagement and lapse.
    
    Idea:
    - Choices are a mixture of reinforcement learning (RL) and a simple working memory (WM) policy.
    - WM stores the last rewarded action for each state (one-shot), otherwise uniform.
    - The mixture weight for WM decreases with set size (load) and is reduced for older adults.
    - A small lapse rate captures random choices and missed/invalid responses.
    
    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: inverse-temperature for RL softmax; internally scaled by 10.
    - wm_base: baseline WM mixture weight (passed through sigmoid).
    - wm_capacity_k: sensitivity of WM weight to set size (higher k = more resistant to load).
    - age_shift: additive shift to WM weight if older (positive reduces/increases after sigmoid).
               Here, negative values reduce WM weight for older adults.
    - lapse: probability of random choice (uniform over 3 actions).
    
    Inputs:
    - states: array of state indices per trial (0..nS-1 within each block).
    - actions: array of chosen actions per trial (0,1,2). If outside these, treated as lapse.
    - rewards: array of rewards per trial (0/1). Other values treated as non-learning events.
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6 here), constant within a block.
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha, beta, wm_base, wm_capacity_k, age_shift, lapse].
    
    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    alpha, beta, wm_base, wm_capacity_k, age_shift, lapse = model_parameters
    beta = beta * 10.0
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.ones((nS, nA)) / nA

        # WM store: last rewarded action per state encoded as a probability vector
        W = np.ones((nS, nA)) / nA  # uniform until a rewarded action occurs

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            # If invalid action or state out of range, count as lapse/uniform and skip learning
            valid_action = (a >= 0 and a < nA)
            if s < 0 or s >= nS:
                # Should not happen, but guard
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            # Compute RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            exp_logits = np.exp(logits)
            p_rl_vec = exp_logits / np.sum(exp_logits)

            # WM policy: distribution stored for state s
            p_wm_vec = W[s, :]

            # Set-size and age dependent WM mixture weight
            # wm_weight = sigmoid(wm_base + age_shift*is_older - (load)/wm_capacity_k)
            load = float(max(0, nS - 3))  # 0 for set size 3, 3 for set size 6
            z = wm_base + age_shift * is_older - (load / max(eps, wm_capacity_k))
            wm_weight = 1.0 / (1.0 + np.exp(-z))

            # Mixture with lapse
            if valid_action:
                p_mix = wm_weight * p_wm_vec[a] + (1.0 - wm_weight) * p_rl_vec[a]
                p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
                nll -= np.log(max(eps, p_total))
            else:
                # invalid responses: uniform
                nll -= np.log(max(eps, 1.0 / nA))

            # RL update
            if valid_action and (r == 0 or r == 1):
                delta = r - q_s[a]
                Q[s, a] += alpha * delta

            # WM update: on rewarded trials, store a one-hot for the state; else leave as is
            if valid_action and (r == 1):
                W[s, :] = 0.0
                W[s, a] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with set-size dependent exploration and perseveration, modulated by age.
    
    Idea:
    - Pure RL model with separate learning rates for reward and non-reward.
    - Inverse temperature decreases with set size (more exploration under load).
    - Older adults show additional exploration (lower effective beta) or different
      exploration tendency via an age parameter.
    - Perseveration bias to repeat the last action in a state, capturing habitual responding.
    
    Parameters (model_parameters):
    - alpha_pos: learning rate for positive prediction errors (rewarded trials).
    - alpha_neg: learning rate for negative prediction errors (unrewarded trials).
    - beta_base: baseline inverse temperature; internally scaled by 10.
    - setsize_noise: decrement in beta per extra item beyond 3 (load sensitivity).
    - rho: perseveration weight added to the last chosen action's logit for that state.
    - age_explore: additional decrement in beta for older adults (if positive reduces beta).
    
    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0,1,2); others treated as uniform.
    - rewards: array of rewards per trial (0/1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6 here).
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha_pos, alpha_neg, beta_base, setsize_noise, rho, age_explore].
    
    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    alpha_pos, alpha_neg, beta_base, setsize_noise, rho, age_explore = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0
    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = np.ones((nS, nA)) / nA
        last_action = -np.ones(nS, dtype=int)  # -1 means no previous action for this state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            valid_action = (a >= 0 and a < nA)
            if s < 0 or s >= nS:
                nll -= np.log(max(eps, 1.0 / nA))
                continue

            # Effective inverse temperature: base - load effect - age exploration effect
            load = float(max(0, int(block_set_sizes[t]) - 3))
            beta_eff = (beta_base - setsize_noise * load - age_explore * is_older) * 10.0
            # Compute softmax with perseveration bias
            logits = beta_eff * Q[s, :].copy()
            if last_action[s] >= 0 and last_action[s] < nA:
                logits[last_action[s]] += rho  # add bias to repeat
            logits = logits - np.max(logits)
            p_vec = np.exp(logits)
            p_vec = p_vec / np.sum(p_vec)

            if valid_action:
                nll -= np.log(max(eps, p_vec[a]))
            else:
                nll -= np.log(max(eps, 1.0 / nA))

            # RL update with asymmetric learning rates
            if valid_action and (r == 0 or r == 1):
                pe = r - Q[s, a]
                lr = alpha_pos if pe > 0 else alpha_neg
                Q[s, a] += lr * pe
                last_action[s] = a  # update perseveration memory
            elif valid_action:
                # if reward is invalid but action valid, still update perseveration memory
                last_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited episodic WM with time decay and age-dependent capacity.
    
    Idea:
    - RL provides graded values and is always active.
    - WM stores the most recent rewarded action per state, but only for a limited
      number of states (capacity). If capacity is exceeded, oldest stored states are dropped.
    - WM retrieval weight depends on: capacity utilization (C_eff / set size) and
      an exponential decay with time since last rewarded encoding for that state.
    - Older adults have reduced effective capacity (age_slope reduces capacity).
    
    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: inverse temperature for RL softmax; internally scaled by 10.
    - capacity_C: baseline WM capacity in number of states (e.g., around 3â€“4).
    - decay_lambda: time decay rate for WM retrieval with trials since last reward in that state.
    - eta: maximum WM mixing coefficient (0..1), scaled by capacity usage and decay.
    - age_slope: reduction in effective capacity for older adults (C_eff = capacity_C - age_slope*is_older).
    
    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0,1,2); others treated as uniform.
    - rewards: array of rewards per trial (0/1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6 here).
    - age: array-like with a single scalar age in years.
    - model_parameters: sequence of 6 floats [alpha, beta, capacity_C, decay_lambda, eta, age_slope].
    
    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    alpha, beta, capacity_C, decay_lambda, eta, age_slope = model_parameters
    beta = beta * 10.0
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.ones((nS, nA)) / nA

        # WM episodic store
        # For each state, store last rewarded action and the trial index it was stored
        wm_action = -np.ones(nS, dtype=int)  # -1 means no memory for that state
        wm_time = -np.ones(nS, dtype=int)    # last time of reward encoding
        # Keep a queue of states currently occupying capacity (by their last encoding time)
        # We'll manage eviction by time-stamp: when capacity exceeded, drop the oldest.
        stored_states = []  # list of state indices currently in WM

        # Effective capacity adjusted by age
        C_eff = max(0.0, capacity_C - age_slope * is_older)

        t_global = 0  # trial counter within block
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            valid_action = (a >= 0 and a < nA)
            if s < 0 or s >= nS:
                nll -= np.log(max(eps, 1.0 / nA))
                t_global += 1
                continue

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(logits)
            p_rl_vec = p_rl_vec / np.sum(p_rl_vec)

            # WM policy for this state
            if wm_action[s] >= 0:
                # One-hot for stored action
                p_wm_vec = np.zeros(nA)
                p_wm_vec[wm_action[s]] = 1.0
                # Decay with time since last reward for this state
                dt = max(0, t_global - wm_time[s])
                decay = np.exp(-decay_lambda * dt)
            else:
                p_wm_vec = np.ones(nA) / nA
                decay = 0.0  # no memory, so no contribution

            # Capacity usage factor: fraction of set covered by effective capacity
            cap_factor = min(1.0, C_eff / max(1.0, float(nS)))

            # Trial-wise WM mixture weight
            w_t = eta * cap_factor * decay

            # Mixture probability
            if valid_action:
                p_total = (1.0 - w_t) * p_rl_vec[a] + w_t * p_wm_vec[a]
                nll -= np.log(max(eps, p_total))
            else:
                nll -= np.log(max(eps, 1.0 / nA))

            # RL update
            if valid_action and (r == 0 or r == 1):
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

            # WM update with capacity enforcement on rewarded trials
            if valid_action and (r == 1):
                # If state not in stored_states, add it; if capacity exceeded, evict oldest
                if s not in stored_states:
                    stored_states.append(s)
                # Enforce capacity as integer ceiling
                cap_int = int(np.floor(C_eff + 1e-9))
                if cap_int < len(stored_states):
                    # Evict the oldest by wm_time
                    # Identify state with smallest wm_time
                    times = [(st, wm_time[st]) for st in stored_states]
                    # states with wm_time < 0 go first
                    times.sort(key=lambda x: x[1])
                    to_remove = times[0][0]
                    stored_states.remove(to_remove)
                    wm_action[to_remove] = -1
                    wm_time[to_remove] = -1

                wm_action[s] = a
                wm_time[s] = t_global

            t_global += 1

    return nll