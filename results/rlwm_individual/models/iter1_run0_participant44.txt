def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with uncertainty-driven exploration and age-dependent forgetting.

    Idea:
    - RL learns Q-values with a single learning rate, but adds an uncertainty (UCB-like) bonus that drives directed exploration early on.
    - WM stores a peaked distribution over the last rewarded action for each state and decays with interference and age.
    - WM contributes more under low load (set size 3) and for younger adults; older adults show stronger WM forgetting.
    - Mixture: p = wm_mix * p_wm + (1 - wm_mix) * p_rl.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - exploration_c: scale of uncertainty bonus for RL (>=0)
    - wm_forgetting: baseline WM decay rate per trial (0..1), higher = faster forgetting

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, exploration_c, wm_forgetting = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL value and uncertainty trackers
        q = (1.0 / nA) * np.ones((nS, nA))
        sa_counts = 1e-6 + np.zeros((nS, nA))  # to compute uncertainty bonus

        # WM distribution per state and a neutral prior
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Directed exploration bonus: higher when counts are low
            bonus = exploration_c / np.sqrt(sa_counts[s, :] + 1e-6)

            # RL policy with uncertainty bonus
            Q_s = q[s, :] + bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: use current WM distribution for state s
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load and age scale WM contribution
            load_scale = 3.0 / float(nS)  # 1.0 for set size 3, 0.5 for set size 6
            # Older adults get reduced WM weight and higher forgetting
            wm_mix = np.clip(wm_weight_base * load_scale * (1.0 - 0.4 * older), 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            sa_counts[s, a] += 1.0

            # WM updating
            # Reward strengthens a one-hot memory for the chosen action.
            if r > 0.5:
                w[s, :] = (1.0 - 0.9) * w_0[s, :]  # small baseline
                w[s, a] += 0.9
                w[s, :] /= np.sum(w[s, :])
            else:
                # No reward: partial suppression of chosen action and decay toward uniform
                w[s, a] = 0.5 * w[s, a] + 0.5 * w_0[s, a]
                # Normalize
                w[s, :] = w[s, :] / np.sum(w[s, :])

            # Global WM decay each trial reflecting interference, modulated by load and age
            decay = np.clip(wm_forgetting * (nS / 3.0) * (1.0 + 0.5 * older), 0.0, 1.0)
            # Blend each state's distribution toward uniform
            w = (1.0 - decay) * w + decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and probabilistic WM retrieval, with age- and load-dependent retrieval.

    Idea:
    - RL uses eligibility traces to propagate credit within a block; traces decay with lambda.
    - WM stores action distributions per state; retrieval succeeds with probability that decreases with load and age.
    - When retrieval succeeds, WM provides a near-deterministic policy; otherwise, it contributes little.
    - Mixture weight is the product of base WM weight and retrieval success probability.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM contribution (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - lambda_trace: eligibility trace decay (0..1)
    - retrieval_base: base probability of successful WM retrieval (0..1)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, lambda_trace, retrieval_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and eligibility traces
        q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))

        # WM distributions
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Retrieval probability decreases with load and age
            load_penalty = (nS - 3) / 3.0  # 0 for 3, 1 for 6
            pr_retrieve = np.clip(retrieval_base * (1.0 - 0.5 * load_penalty) * (1.0 - 0.4 * older), 0.0, 1.0)

            wm_mix = np.clip(wm_weight * pr_retrieve, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            # Decay traces
            e *= lambda_trace
            # Increment trace for chosen state-action
            e[s, a] += 1.0

            delta = r - q[s, a]
            # Apply update to all state-actions proportional to their trace
            q += lr * delta * e

            # WM update
            if r > 0.5:
                # Store a sharp distribution for the rewarded action
                w[s, :] = (1.0 - 0.9) * w_0[s, :]
                w[s, a] += 0.9
                w[s, :] /= np.sum(w[s, :])
            else:
                # Partial unlearning toward uniform
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dynamic arbitration between RL and WM based on online WM reliability, with age-dependent noise.

    Idea:
    - RL updates Q-values with standard delta rule.
    - WM stores action distributions per state; when reward observed, WM sharpens for that action; otherwise, decays.
    - An arbitration variable tracks WM reliability (how often WM would have selected the rewarded action).
    - Mixture weight is a sigmoid of a baseline plus the current reliability, reduced by load and age.
    - Older adults also have reduced effective RL inverse temperature (more decision noise).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_base: baseline WM bias in the arbiter (can be negative or positive)
    - softmax_beta: base RL inverse temperature (scaled internally by 10)
    - arb_lr: learning rate for updating WM reliability (0..1)
    - age_noise: multiplicative reduction of RL beta for older adults (0..1; larger => more reduction)
    - gain_k: gain on reliability in the arbiter (>0)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, arb_lr, age_noise, gain_k = model_parameters
    # Age-dependent RL temperature: older -> lower effective beta
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0
    softmax_beta = (softmax_beta * 10.0) * (1.0 - age_noise * older)
    softmax_beta = max(1e-6, softmax_beta)
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration reliability per state: probability WM is correct
        wm_rel = 0.5 * np.ones(nS)  # start neutral

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from current distribution
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load penalty to WM arbitration (higher set size -> lower WM mix)
            load_penalty = (nS - 3) / 3.0  # 0 or 1
            # Mixture via sigmoid of baseline + gain * reliability, then reduce by load and age
            logits = wm_base + gain_k * (wm_rel[s] - 0.5)
            wm_mix_raw = 1.0 / (1.0 + np.exp(-logits))
            wm_mix = wm_mix_raw * (1.0 - 0.5 * load_penalty) * (1.0 - 0.4 * older)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                # Sharpen toward chosen action
                w[s, :] = 0.1 * w_0[s, :]
                w[s, a] += 0.9
                w[s, :] /= np.sum(w[s, :])
                # Reliability update: WM was "diagnostic" if it favored the chosen action
                wm_correct_prob = W_s[a]  # probability WM assigned to chosen action before update
                # Move reliability toward 1 if WM supported the rewarded choice, else toward 0
                target = 1.0 if wm_correct_prob == np.max(W_s) else 0.7
                wm_rel[s] += arb_lr * (target - wm_rel[s])
            else:
                # Decay toward uniform when not rewarded
                w[s, :] = 0.6 * w[s, :] + 0.4 * w_0[s, :]
                w[s, :] /= np.sum(w[s, :])
                # Reliability update: move toward lower reliability when WM favored a wrong action
                wm_wrong_prob = W_s[a]
                target = 0.2 + 0.3 * (1.0 - wm_wrong_prob)  # lower if WM strongly backed the wrong action
                wm_rel[s] += arb_lr * (target - wm_rel[s])

        blocks_log_p += log_p

    return -blocks_log_p