def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with load-sensitive WM reliability and WM decay.

    Model idea:
    - RL system: standard delta rule with softmax choice.
    - WM system: one-shot, reward-gated storage of the correct action per state, with decay toward uniform.
    - Load sensitivity: WM retrieval reliability decreases with larger set size (nS), implemented as a logistic function
      of set size that modulates WM policy toward uniform noise.
    - Mixture: weighted average of WM and RL policies using wm_weight from parameters (as required by the template).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy in the final choice probability (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled up internally by 10).
    - wm_decay: Decay toward uniform in WM store per trial (0..1).
    - wm_confidence: Baseline WM reliability at low load (0..1).
    - wm_load_sensitivity: Positive value; higher means stronger WM degradation when nS increases (>0).

    Returns:
    - Negative log-likelihood of the observed choices across all blocks.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_confidence, wm_load_sensitivity = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action probability using softmax
            # Implement using Q_s directly (no extra biases here)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Load-dependent WM success probability (logistic drop with set size)
            # p_success in [0,1], high when nS small, low when nS large
            p_success = wm_confidence / (1 + np.exp(wm_load_sensitivity * (nS - 3)))
            # WM softmax over WM strengths
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1 / denom_wm
            # Mix with uniform according to success probability
            p_wm = p_success * p_wm_soft + (1 - p_success) * (1 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform each trial
            w[s] = (1 - wm_decay) * w[s] + wm_decay * w_0[s]
            # If rewarded, refresh memory strongly toward the rewarded action (one-shot)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Overwrite by moving W_s toward one-hot; use same wm_decay as step size to keep parameter count
                w[s] = (1 - wm_decay) * w[s] + wm_decay * one_hot
            # Normalize to keep as probabilities
            w[s] = np.maximum(w[s], 1e-12)
            w[s] = w[s] / np.sum(w[s])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM recency memory with interference under load + choice stickiness.

    Model idea:
    - RL system: two learning rates for gains vs. losses, softmax choice with added stickiness bias.
    - WM system: recency-based memory that stores the last chosen action for each state regardless of reward.
      Under high load (nS=6), WM suffers interference: memory retrieval is partially contaminated by other states'
      stored actions (confusion parameter).
    - Stickiness (perseveration) bias increases the logit for the most recent action in the block, applied to both RL and WM.
    - Mixture: wm_weight mixes WM and RL policies as per template.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight: Mixture weight for WM policy (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - confusion: Degree of WM interference across states at high load (0..1). Effective only when nS>3.
    - stickiness: Choice perseveration strength added to softmax logits for the last chosen action (>0 increases bias).

    Returns:
    - Negative log-likelihood of the observed choices across all blocks.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, confusion, stickiness = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_action_global = None  # for stickiness within block

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL with stickiness
            Q_s = q[s, :].copy()
            if last_action_global is not None:
                Q_s[last_action_global] += stickiness
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM with interference under load: mix current state's memory with the average of others when nS>3
            W_s = w[s, :].copy()
            if nS > 3 and confusion > 0:
                others_mask = np.ones(nS, dtype=bool)
                others_mask[s] = False
                # Average memory across other states
                mean_other = np.mean(w[others_mask, :], axis=0) if np.any(others_mask) else w_0[s]
                W_s = (1 - confusion) * W_s + confusion * mean_other

            # Add stickiness to WM logits
            W_eff = W_s.copy()
            if last_action_global is not None:
                W_eff[last_action_global] += stickiness

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_wm = 1 / denom_wm

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            if pe >= 0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Recency-based storage: set memory toward last chosen action for this state
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            # Overwrite aggressively to reflect recency
            w[s] = one_hot
            # Track global last action for stickiness
            last_action_global = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + WM capacity limit + lapse mixture within each subsystem.

    Model idea:
    - RL system: delta rule with per-trial forgetting toward uniform at the current state, softmax choice;
      choice probabilities include a lapse rate that pulls toward uniform (undirected exploration).
    - WM system: stores rewarded action with fast overwriting, decays to uniform, and is constrained by an
      effective capacity parameter kappa: WM success â‰ˆ min(1, kappa/nS). Larger sets reduce WM reliability.
      WM choice also includes lapse.
    - Final mixture uses wm_weight as per template.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - rl_forget: RL forgetting rate toward uniform at the visited state (0..1).
    - lapse: Lapse probability applied within each subsystem policy (0..1).
    - kappa: WM capacity (continuous, 0..6). Higher kappa sustains WM under load better.

    Returns:
    - Negative log-likelihood of the observed choices across all blocks.
    """
    lr, wm_weight, softmax_beta, rl_forget, lapse, kappa = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM success probability from capacity
        wm_success = np.clip(kappa / max(1.0, float(nS)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy softmax for chosen action
            p_rl_soft = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            # Add lapse within RL subsystem
            p_rl = (1 - lapse) * p_rl_soft + lapse * (1 / nA)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax, then mix with uniform based on capacity-derived success and lapse
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1 / denom_wm
            p_wm_cap = wm_success * p_wm_soft + (1 - wm_success) * (1 / nA)
            p_wm = (1 - lapse) * p_wm_cap + lapse * (1 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform at the visited state
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # Forgetting pulls entire state's Qs toward uniform baseline
            q[s] = (1 - rl_forget) * q[s] + rl_forget * w_0[s]

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            decay_rate = 0.5  # fixed internal rate to avoid extra parameters; decay every trial at the visited state
            w[s] = (1 - decay_rate) * w[s] + decay_rate * w_0[s]
            # Reward-gated strengthening of chosen action
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                strengthen = 0.5
                w[s] = (1 - strengthen) * w[s] + strengthen * one_hot
            # Normalize
            w[s] = np.maximum(w[s], 1e-12)
            w[s] = w[s] / np.sum(w[s])

        blocks_log_p += log_p

    return -blocks_log_p