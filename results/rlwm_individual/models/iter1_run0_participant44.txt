def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with capacity- and decay-gated working memory.
    
    Idea:
    - Choices arise from a mixture of a model-free RL controller (Q-learning) and a
      working-memory (WM) controller that stores state→action associations with decay.
    - WM influence is scaled by set-size via an effective capacity K: as set size grows beyond K,
      WM contributes less to choice.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr           : RL learning rate (0..1) for updating Q-values.
      wm_weight    : Base mixture weight (0..1) for WM vs RL when capacity is not limiting.
      softmax_beta : Inverse temperature for RL policy; internally scaled up 10x to broaden range.
      wm_decay     : Per-trial decay (0..1) of WM toward uniform (higher = faster forgetting).
      wm_capacity  : WM capacity (K) in number of state–action pairs effectively maintained.
      wm_eta       : WM encoding strength (0..1) when feedback is observed.
    
    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity, wm_eta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states  = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute RL action probability of chosen action a
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # Compute WM action probability of chosen action a
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Capacity-gated mixture weight for this block's set size
            K_eff = min(wm_capacity, nS)
            wm_mix = wm_weight * (K_eff / max(1.0, nS))

            # Mixture policy and log-likelihood
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay (global toward uniform)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding
            # - On reward: move WM row toward one-hot for chosen action a
            # - On no reward: slightly suppress chosen action's WM weight
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * one_hot
            else:
                # Suppress chosen action and renormalize slightly toward uniform
                w[s, a] = (1.0 - wm_eta) * w[s, a]
                # Optional light renormalization to prevent collapse
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with uncertainty-gated WM and lapses.
    
    Idea:
    - Same RL and WM controllers as in model1.
    - WM influence is modulated by:
        (a) set-size via capacity K
        (b) WM "confidence" in the current state (max vs. runner-up difference)
    - Additionally includes an undirected lapse probability that chooses uniformly at random.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr           : RL learning rate (0..1) for Q updates (also used as WM encoding rate).
      wm_weight    : Base WM weight (0..1).
      softmax_beta : Inverse temperature for RL policy; internally scaled 10x.
      wm_decay     : WM decay per trial (0..1) toward uniform.
      wm_capacity  : WM capacity (K) in number of items.
      lapse        : Lapse probability (0..1) for uniform random choice.
    
    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_capacity, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states  = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Capacity-based base mixing
            K_eff = min(wm_capacity, nS)
            wm_mix_base = wm_weight * (K_eff / max(1.0, nS))

            # WM confidence gating: difference between top-1 and top-2 WM entries
            sorted_W = np.sort(W_s)
            if len(sorted_W) >= 2:
                conf = max(0.0, sorted_W[-1] - sorted_W[-2])
            else:
                conf = 0.0
            wm_mix = wm_mix_base * conf  # stronger WM reliance when a clear WM favorite exists

            # Combine with lapse
            p_choice = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding using lr as encoding strength
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot
            else:
                # Reduce chosen action probability in WM, then light renormalization
                w[s, a] = (1.0 - lr) * w[s, a]
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with load-specific WM weights and asymmetric WM encoding.
    
    Idea:
    - RL as before.
    - WM has different mixture weights for low-load (set size <= 3) vs high-load (set size > 3) blocks.
    - WM encodes rewarded associations strongly and pushes probability mass away from the chosen action
      after non-reward (asymmetric update controlled by wm_neg_eta).
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr              : RL learning rate (0..1).
      wm_weight_small : WM mixture weight (0..1) for small set size blocks (<=3).
      wm_weight_large : WM mixture weight (0..1) for large set size blocks (>3).
      softmax_beta    : Inverse temperature for RL policy; internally scaled 10x.
      wm_decay        : WM decay per trial (0..1) toward uniform.
      wm_neg_eta      : WM negative encoding strength (0..1) applied when reward=0; also used for positive updates.
    
    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight_small, wm_weight_large, softmax_beta, wm_decay, wm_neg_eta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states  = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set load-specific WM mixture
        wm_mix_block = wm_weight_small if nS <= 3 else wm_weight_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_mix_block * p_wm + (1.0 - wm_mix_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Asymmetric WM encoding
            if r > 0.0:
                # Positive encoding toward one-hot of chosen action (use wm_neg_eta as general encoding rate)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_neg_eta) * w[s, :] + wm_neg_eta * one_hot
            else:
                # Negative encoding: push mass away from chosen action
                # Reduce chosen action and redistribute to others proportionally to their current weights
                reduce = wm_neg_eta * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    other_sum = np.sum(w[s, others])
                    if other_sum > 0:
                        w[s, others] += reduce * (w[s, others] / other_sum)
                    else:
                        # If others collapsed, distribute uniformly to others
                        w[s, others] += reduce / (nA - 1)

                # Renormalize softly
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p