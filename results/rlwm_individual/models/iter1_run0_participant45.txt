def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and age- and set-size–modulated lapses.

    The model uses a standard Q-learning policy with:
      - Softmax inverse temperature reduced under higher set sizes (cognitive load).
      - A lapse probability that increases with set size and is further modulated by age group.
      - A forgetting process that decays Q-values toward a uniform prior.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial within each block. Assumed 0..(set_size-1).
    actions : array-like of int
        Chosen actions on each trial. Valid actions are {0,1,2}. If action not in {0,1,2}, the model
        assigns uniform choice probability and skips updates for that trial.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Values <0 are treated as invalid trials (uniform prob, no update).
    blocks : array-like of int
        Block index per trial. Values partition the trials into independent learning episodes.
    set_sizes : array-like of int
        Set size presented on each trial (constant within block).
    age : array-like of float
        Participant age; only the first element is used. Age >= 45 treated as older group.
    model_parameters : tuple/list of floats
        (alpha, beta_base, lapse_base, age_lapse, decay)
        - alpha: RL learning rate (0..1).
        - beta_base: baseline inverse temperature for softmax (>0).
        - lapse_base: baseline lapse logit (can be any real); converted via sigmoid.
        - age_lapse: additive shift on lapse logit for older adults (>=45). Positive increases lapses.
        - decay: forgetting rate toward uniform (0..1) applied to the currently visited state each trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta_base, lapse_base, age_lapse, decay = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    total_loglik = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # Handle invalid trials uniformly
            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # Set-size reduces effective inverse temperature
            beta_t = beta_base / (1.0 + max(0, set_size_t - 3) / 3.0)

            # RL policy
            logits = beta_t * Q[s, :]
            logits -= np.max(logits)
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # Lapse probability (uniform responding), modulated by set size and age
            # Lapse logit increases with log(set size) and age
            lapse_logit = lapse_base + age_lapse * is_older + 0.5 * np.log(max(1, set_size_t))
            lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
            lapse = np.clip(lapse, 0.0, 1.0)

            p = (1.0 - lapse) * p_rl + lapse * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            p /= np.sum(p)

            total_loglik += np.log(p[a])

            # Forgetting toward uniform prior for the visited state
            Q[s, :] = (1.0 - decay) * Q[s, :] + decay * (1.0 / nA)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty bonus exploration, modulated by age and set size.

    The model adds an uncertainty bonus to action values to encourage exploration, where
    uncertainty decreases with visits to a state-action pair. Older adults can differ in the
    strength of this exploration bonus. Set size reduces the impact of the bonus.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial within each block. Assumed 0..(set_size-1).
    actions : array-like of int
        Chosen actions on each trial. Valid actions are {0,1,2}. If action not in {0,1,2}, the model
        assigns uniform choice probability and skips updates for that trial.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Values <0 are treated as invalid trials (uniform prob, no update).
    blocks : array-like of int
        Block index per trial. Values partition the trials into independent learning episodes.
    set_sizes : array-like of int
        Set size presented on each trial (constant within block).
    age : array-like of float
        Participant age; only the first element is used. Age >= 45 treated as older group.
    model_parameters : tuple/list of floats
        (alpha, beta, gamma0, age_gamma)
        - alpha: RL learning rate (0..1).
        - beta: softmax inverse temperature (>0).
        - gamma0: baseline strength of the uncertainty bonus.
        - age_gamma: additive shift to the uncertainty bonus for older adults (>=45).
          Positive values increase exploration in older adults.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, gamma0, age_gamma = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    total_loglik = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for uncertainty

        # Exploration bonus strength for this block, scaled by set size on each trial
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # Uncertainty bonus decreases with visits; scaled by age and set size
            gamma = (gamma0 + age_gamma * is_older) / max(1.0, np.sqrt(set_size_t))
            U = 1.0 / np.sqrt(N[s, :] + 1.0)

            logits = beta * (Q[s, :] + gamma * U)
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)
            p = np.clip(p, 1e-12, 1.0)

            total_loglik += np.log(p[a])

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update visit count after choosing action
            N[s, a] += 1.0

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL–WM gating via logistic mixture with age and set-size effects.

    The model mixes a reinforcement learning (RL) policy with a simple working-memory (WM) policy.
    WM stores the most recently rewarded action for each state (one-slot memory per state).
    The mixture gate is a logistic function of age group and set size: WM is favored for smaller sets
    and can be reduced or enhanced in older adults via an age-dependent bias.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial within each block. Assumed 0..(set_size-1).
    actions : array-like of int
        Chosen actions on each trial. Valid actions are {0,1,2}. If action not in {0,1,2}, the model
        assigns uniform choice probability and skips updates for that trial.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Values <0 are treated as invalid trials (uniform prob, no update).
    blocks : array-like of int
        Block index per trial. Values partition the trials into independent learning episodes.
    set_sizes : array-like of int
        Set size presented on each trial (constant within block).
    age : array-like of float
        Participant age; only the first element is used. Age >= 45 treated as older group.
    model_parameters : tuple/list of floats
        (alpha, beta, theta0, theta_age, theta_size)
        - alpha: RL learning rate (0..1).
        - beta: softmax inverse temperature (>0).
        - theta0: baseline logit for WM gating (any real).
        - theta_age: additive shift on WM gating logit for older adults (>=45).
        - theta_size: coefficient on (3 - set size) in WM gating logit; positive favors WM for smaller sets.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    alpha, beta, theta0, theta_age, theta_size = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    total_loglik = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        wm_cache = -1 * np.ones(nS, dtype=int)  # -1 means no memory for the state yet

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # RL policy
            logits_rl = beta * Q[s, :]
            logits_rl -= np.max(logits_rl)
            p_rl = np.exp(logits_rl)
            p_rl /= np.sum(p_rl)

            # WM policy: one-hot on last rewarded action for state; uniform if none
            if wm_cache[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[wm_cache[s]] = 1.0
            else:
                p_wm = (1.0 / nA) * np.ones(nA)

            # Gating: logistic mixture weight depending on age and set size
            gate_logit = theta0 + theta_age * is_older + theta_size * (3.0 - float(set_size_t))
            g = 1.0 / (1.0 + np.exp(-gate_logit))
            g = np.clip(g, 0.0, 1.0)

            p = g * p_wm + (1.0 - g) * p_rl
            p = np.clip(p, 1e-12, 1.0)
            p /= np.sum(p)

            total_loglik += np.log(p[a])

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update on reward
            if r >= 1.0:
                wm_cache[s] = a

    return -total_loglik