def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent WM failures and RL choice stickiness.

    Idea:
    - RL: single learning rate and softmax choice; adds per-state choice stickiness bias toward the last chosen action in that state.
    - WM: fast, near-deterministic cache for rewarded state-action pairs with decay toward uniform.
    - Load effect: WM's effective mixture weight is reduced by a load-dependent failure probability that increases with set size.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base mixture weight for WM (0..1).
        softmax_beta: float
            Inverse temperature for RL policy; internally scaled by 10.
        stickiness: float
            Bias added to the utility of the last chosen action in a state (>=0).
        kappa_wm: float
            WM update rate (0..1). Larger values make WM more rapidly reflect the most recent feedback.
        chi_load: float
            Load sensitivity (>=0). Higher values increase the WM failure probability with set size.

    Returns
    -------
    nll: float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, kappa_wm, chi_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state last action for stickiness (-1 indicates none yet)
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-dependent WM failure probability (linear ramp, clipped)
        fail = np.clip(chi_load * (max(nS - 1, 0)) / 5.0, 0.0, 1.0)
        wm_mix = wm_weight * (1.0 - fail)
        wm_mix = np.clip(wm_mix, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with choice stickiness for the last taken action in this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic choice based on cached association
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update with decay and reinforcement/suppression
            if r > 0.5:
                # Move distribution toward chosen action (cache rewarded association)
                w[s, :] = (1.0 - kappa_wm) * w[s, :] + kappa_wm * np.eye(nA)[a]
            else:
                # Decay toward uniform on non-reward (weaken incorrect memory)
                w[s, :] = (1.0 - kappa_wm) * w[s, :] + kappa_wm * w_0[s, :]

            # Normalize WM distribution for numerical stability
            w[s, :] = w[s, :] / np.sum(w[s, :])

            # Update stickiness memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-based arbitration and load-scaled WM influence.

    Idea:
    - RL: standard delta rule and softmax choice.
    - WM: fast updating toward last rewarded action in a state; decays toward uniform otherwise.
    - Arbitration: compute trial-wise confidence for WM (peakedness of W_s) vs RL (spread of Q_s).
      Mix policies via a logistic function of (WM_conf - RL_conf), scaled by a base WM weight and
      a load penalty that reduces WM influence as set size increases.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_base: float
            Base WM mixture weight (0..1) before arbitration and load scaling.
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        lam_wm: float
            WM update rate (0..1). Moves W_s toward one-hot on reward and toward uniform otherwise.
        gamma: float
            Arbitration sensitivity (>0). Higher values make mixing more sensitive to confidence differences.
        phi: float
            Load penalty (>0). Effective WM weight scales as wm_base / (1 + phi*(nS-1)).

    Returns
    -------
    nll: float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_base, softmax_beta, lam_wm, gamma, phi = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled base WM influence
        wm_base_load = wm_base / (1.0 + phi * max(nS - 1, 0.0))
        wm_base_load = np.clip(wm_base_load, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence of WM: peak minus second peak; RL: max minus mean (spread)
            sorted_W = np.sort(W_s)
            wm_conf = sorted_W[-1] - sorted_W[-2] if nA >= 2 else sorted_W[-1]
            rl_conf = np.max(Q_s) - np.mean(Q_s)

            # Arbitration weight (logistic of confidence difference), scaled by load-adjusted base
            mix_core = 1.0 / (1.0 + np.exp(-gamma * (wm_conf - rl_conf)))
            wm_mix = wm_base_load * mix_core
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                w[s, :] = (1.0 - lam_wm) * w[s, :] + lam_wm * np.eye(nA)[a]
            else:
                w[s, :] = (1.0 - lam_wm) * w[s, :] + lam_wm * w_0[s, :]

            # Normalize WM distribution
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with win-stay/lose-shift updating and load-scaled WM precision.

    Idea:
    - RL: standard delta rule with softmax.
    - WM: episodic-like buffer that strongly encodes rewarded action (win-stay) and suppresses
      the chosen action after non-reward (lose-shift), implemented as a distribution over actions.
    - Capacity/load: WM mixture weight scales with min(1, C/nS). WM precision (inverse temperature)
      decreases as set size exceeds capacity via omega_load.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM mixture weight (0..1), further scaled by capacity relative to set size.
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        alpha_wm: float
            WM learning rate (0..1): how strongly WM updates on each trial.
        C: float
            WM capacity in number of states (>=1). WM influence scales as min(1, C/nS).
        omega_load: float
            Load sensitivity for WM precision (>=0). WM beta = 50 / (1 + omega_load * max(0, nS - C)).

    Returns
    -------
    nll: float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, alpha_wm, C, omega_load = model_parameters
    softmax_beta *= 10.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM mixture and precision
        cap_scale = min(1.0, float(C) / max(1.0, float(nS)))
        wm_mix = np.clip(wm_weight * cap_scale, 0.0, 1.0)
        softmax_beta_wm = 50.0 / (1.0 + omega_load * max(0.0, float(nS) - float(C)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: win-stay / lose-shift distribution update
            if r > 0.5:
                # Push mass toward the chosen action
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * np.eye(nA)[a]
            else:
                # Suppress the chosen action and redistribute to others
                w[s, a] = (1.0 - alpha_wm) * w[s, a]
                # Increase others slightly, then normalize
                others = [i for i in range(nA) if i != a]
                increment = alpha_wm / (len(others) + 1e-12)
                for i in others:
                    w[s, i] = (1.0 - alpha_wm) * w[s, i] + increment

            # Normalize WM distribution
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p