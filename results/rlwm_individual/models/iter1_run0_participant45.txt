def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with age- and load-gated WM arbitration and asymmetric RL learning.

    Idea:
    - RL learns with separate positive/negative learning rates.
    - WM stores rewarded associations and chooses nearly deterministically.
    - Arbitration weight for WM is set by a logistic capacity gate that depends on set size and age.
      Older adults are assumed to have an additional capacity penalty.
    - WM updates only when rewarded and only with a certain probability (encoding noise).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 internally (>=0).
    - wm_capacity: Center of the logistic gating; higher => more WM reliance across loads.
    - age_penalty: Additive penalty applied to the capacity gate if older (>=45), reducing WM weight.
    - wm_refresh_prob: Probability to encode a rewarded state-action into WM on a given trial (0..1).

    Inputs:
    - states: array of state indices per trial.
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0/1).
    - blocks: array of block indices per trial.
    - set_sizes: array of set sizes per trial (3 or 6 within a block).
    - age: array-like with a single value (participant age).
    - model_parameters: tuple/list with 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_capacity, age_penalty, wm_refresh_prob = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # nearly deterministic WM policy
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM representations
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute WM arbitration weight via a logistic "capacity gate"
        # load_term: 0 for set size 3, 3 for set size 6
        load_term = float(nS) - 3.0
        gate_input = wm_capacity - load_term - age_penalty * is_older
        wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for the chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy for the chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_use * delta

            # WM updating: probabilistic encoding on reward
            if r > 0.5:
                if np.random.rand() < np.clip(wm_refresh_prob, 0.0, 1.0):
                    new_W = np.zeros(nA)
                    new_W[a] = 1.0
                    w[s, :] = new_W
            # If not rewarded, WM remains unchanged (no decay in this model)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM store with trial-wise age-modulated lapses.

    Idea:
    - RL learns action values; WM is a limited-capacity store of recently learned (rewarded) state-action pairs.
    - WM contributes only if the current state is in WM; otherwise decisions rely on RL.
    - Lapse probability increases over trials within a block and more steeply for older adults, capturing fatigue.
    - WM capacity K determines how many distinct states can be actively represented; eviction is FIFO.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 internally (>=0).
    - wm_weight_base: Weight of WM when a state is in WM (0..1).
    - K_capacity: Integer-like (clipped to [1,6]) WM capacity in number of states.
    - lapse0: Baseline lapse probability at block start (0..0.3 typical).
    - lapse_slope_age: Additional lapse slope per normalized trial for older adults (0..0.5 typical).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as defined in the task.
    - model_parameters: tuple/list with 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, K_capacity, lapse0, lapse_slope_age = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    # Clip/round capacity to feasible range [1,6]
    K_capacity = int(np.clip(np.round(K_capacity), 1, 6))

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track which states are currently in WM and their insertion order for FIFO eviction
        wm_states = []  # list of state indices in order of insertion

        T = len(block_states)
        log_p = 0.0
        for t in range(T):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Determine if current state is represented in WM
            in_wm = (s in wm_states) and (np.max(W_s) > (1.0 / nA))

            # Trial-wise lapse (fatigue): increases over block and more for older adults
            norm_t = 0.0 if T <= 1 else (t / (T - 1.0))
            lapse = np.clip(lapse0 + (lapse_slope_age * is_older) * norm_t, 0.0, 0.5)

            # Use WM only if the state is in WM; otherwise WM contributes 0 weight
            wm_weight_eff = wm_weight_base if in_wm else 0.0
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Final with lapse to uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with capacity limit:
            # If rewarded, encode s->a as one-hot and manage capacity with FIFO eviction
            if r > 0.5:
                if s not in wm_states:
                    wm_states.append(s)
                    if len(wm_states) > K_capacity:
                        # Evict the oldest state from WM by resetting its row to uniform
                        s_evict = wm_states.pop(0)
                        w[s_evict, :] = w_0[s_evict, :]

                # Encode deterministic mapping for this state
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W
            # If unrewarded, leave WM unchanged (no decay in this model)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with decay + WM one-shot learning, with load- and age-scaled arbitration and RL forgetting.

    Idea:
    - RL values decay over time (forgetting), capturing age-related declines in retention.
    - WM stores rewarded associations (one-shot). No explicit WM decay here.
    - Arbitration weight for WM shrinks with set size (load) and is further reduced for older adults.
    - WM used alongside RL; older adults rely less on WM and more on decayed RL.
    - RL decay is applied to the visited state on each trial to reflect use-dependent forgetting.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature, scaled by 10 internally (>=0).
    - wm_weight_base: Baseline WM mixture weight (0..1).
    - rl_decay: Per-trial decay toward uniform for the visited state's Q row (0..1).
    - load_sensitivity: Scales how strongly larger set sizes reduce WM weight (>=0).
    - age_wm_penalty: Additional multiplicative reduction of WM weight for older adults (0..1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays as defined in the task.
    - model_parameters: tuple/list with 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, rl_decay, load_sensitivity, age_wm_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load- and age-dependent WM weighting
        load_term = max(float(nS) - 3.0, 0.0)  # 0 for 3, 3 for 6
        load_scale = np.exp(-np.clip(load_sensitivity, 0.0, 10.0) * load_term)
        age_scale = (1.0 - np.clip(age_wm_penalty, 0.0, 1.0)) if is_older else 1.0
        wm_weight = np.clip(wm_weight_base * load_scale * age_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy for chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Apply RL decay toward uniform on the visited state (use-dependent forgetting)
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * w_0[s, :]

            # WM updating: one-shot learning on reward
            if r > 0.5:
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W
            # No WM decay in this model

        blocks_log_p += log_p

    return -blocks_log_p