def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL (dual learning rates) + capacity-limited WM with set-size scaled contribution and lapse
    - RL: tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: one-shot storage of rewarded action per state, with decay toward uniform governed by how much set size exceeds capacity K.
    - Arbitration: convex mixture of WM and RL action probabilities; WM weight is scaled by min(1, K/nS).
    - Lapse: with probability epsilon, respond uniformly at random (independent of policies).

    Parameters (model_parameters):
    - alpha_pos: float in [0,1], learning rate when prediction error is positive.
    - alpha_neg: float in [0,1], learning rate when prediction error is negative.
    - wm_weight_base: float in [0,1], baseline WM mixture weight before set-size scaling.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - K_slots: float > 0, effective WM capacity in number of state-action pairs.
    - epsilon: float in [0,1], lapse rate mixing in a uniform random policy.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, K_slots, epsilon = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic WM readout
    blocks_log_p = 0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with high precision over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size dependent WM availability via capacity K
            wm_avail = min(1.0, max(0.0, float(K_slots) / float(nS)))
            wm_weight_eff = wm_weight_base * wm_avail

            # Mixture and lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with dual learning rates
            delta = r - Q_s[a]
            alpha = alpha_pos if delta >= 0 else alpha_neg
            q[s, a] += alpha * delta

            # WM decay stronger when set size exceeds capacity
            overload = max(0.0, float(nS) - float(K_slots))
            leak = overload / float(nS)  # in [0,1)
            w = (1.0 - leak) * w + leak * w_0

            # WM update: store rewarded action as one-hot trace
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL with forgetting and choice stickiness + surprise-gated WM
    - RL: tabular Q-learning with single learning rate, plus passive forgetting toward uniform (stronger at larger set sizes).
    - Choice stickiness: add a bias kappa to repeat the immediately previous action (state-independent).
    - WM: deterministic readout; updates are gated by surprise (|prediction error|) via a sigmoid function.
    - Arbitration: mixture of WM and RL, with WM weight reduced at larger set sizes.

    Parameters (model_parameters):
    - alpha: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - wm_weight: float in [0,1], baseline WM mixture weight before set-size scaling.
    - phi_forget: float in [0,1], passive forgetting rate toward uniform; effective forgetting scales with set size.
    - kappa: float, choice stickiness bias added to the last chosen action's preference.
    - gamma_gate: float >= 0, sensitivity of WM gating to surprise |delta|.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, softmax_beta, wm_weight, phi_forget, kappa, gamma_gate = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        prev_action = None
        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply passive RL forgetting (stronger under higher load)
            phi_eff = min(1.0, max(0.0, phi_forget * (float(nS) / 6.0)))
            q = (1.0 - phi_eff) * q + phi_eff * (1.0 / nA)

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add choice stickiness bias to RL preferences
            if prev_action is not None:
                bias = np.zeros(nA)
                bias[int(prev_action)] = kappa
                Q_eff = Q_s + bias
            else:
                Q_eff = Q_s

            # RL policy on biased values
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Set-size dependent WM weight
            wm_weight_eff = wm_weight * (3.0 / float(nS))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM surprise-gated update
            # Gate strength in [0,1] via sigmoid on |delta|
            gate = 1.0 / (1.0 + np.exp(-gamma_gate * abs(delta)))
            # Favor storing only when rewarded, with strength determined by gate
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - gate) * w[s, :] + gate * one_hot
            else:
                # If not rewarded, partially relax toward uniform depending on gate
                w[s, :] = (1.0 - 0.5 * gate) * w[s, :] + (0.5 * gate) * w_0

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: RL + WM with load-dependent WM precision (decay) via power-law scaling
    - RL: tabular Q-learning with single learning rate and softmax.
    - WM: stores rewarded actions deterministically, but decays each trial with a leak set by a precision parameter
      that worsens with set size according to a power law.
    - Arbitration: mixture of RL and WM with set-size scaled WM weight.
    - Lapse: epsilon random responding.

    Parameters (model_parameters):
    - alpha: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - wm_weight_base: float in [0,1], baseline WM weight before set-size scaling.
    - sigma0: float >= 0, base WM noise/decay rate controlling leak at set size 3.
    - p_exp: float >= 0, exponent controlling how WM decay grows with nS (leak ~ 1 - exp(-sigma0*(nS/3)^p)).
    - epsilon: float in [0,1], lapse rate mixing in a uniform random policy.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, softmax_beta, wm_weight_base, sigma0, p_exp, epsilon = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Compute WM leak based on set size via power law
        load_ratio = float(nS) / 3.0
        leak = 1.0 - np.exp(-sigma0 * (load_ratio ** p_exp))
        leak = min(max(leak, 0.0), 1.0)

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_weight_eff = wm_weight_base * (3.0 / float(nS))
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM decay and update
            w = (1.0 - leak) * w + leak * w_0
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p