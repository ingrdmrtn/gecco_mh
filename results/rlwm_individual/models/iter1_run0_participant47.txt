def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and load-dependent forgetting.

    Idea:
    - Choices are a mixture of model-free RL and a working-memory (WM) policy.
    - WM stores the last rewarded action per state with near-deterministic retrieval.
    - WM effectiveness decays faster with larger set sizes (load).
    - Arbitration weight for WM is attenuated when set size exceeds a capacity K.

    Parameters
    ----------
    model_parameters : list or array-like of length 5
        lr : float
            Learning rate for RL value updates (0..1).
        wm_weight : float
            Baseline mixture weight on WM (0..1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled up internally).
        K : float
            WM capacity (in number of items). When nS <= K, WM is stronger; otherwise weaker.
        wm_decay : float
            Base forgetting rate for WM per trial (0..1). Forgetting accelerates with larger nS.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM weight based on capacity and load
        # More weight when nS <= K, less when nS > K
        cap_factor = 1.0 / (1.0 + np.exp(1.5 * (nS - K)))  # sigmoid transition
        wm_weight_eff = np.clip(wm_weight * cap_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Load-dependent decay toward uniform before retrieval
            # Faster decay as set size increases
            decay_eff = 1.0 - np.exp(-wm_decay * max(nS, 1))
            w[s, :] = (1 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: encode rewarded associations strongly, otherwise keep decayed trace
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with interference-based WM and reward-modulated WM learning.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM maintains graded action weights per state and is updated by a WM learning rate.
    - Interference between items increases with set size: WM for a state is contaminated
      by other statesâ€™ memories, reducing precision in larger sets.

    Parameters
    ----------
    model_parameters : list or array-like of length 5
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Baseline WM mixture weight (0..1).
        softmax_beta : float
            RL inverse temperature (scaled up internally).
        phi_base : float
            Base WM interference strength (0..1). Effective interference scales with set size.
        wm_lr : float
            WM learning rate (0..1), controlling how quickly WM updates toward new info.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi_base, wm_lr = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference grows with number of other items
        phi_eff = np.clip(phi_base * max(nS - 1, 0) / max(1, (nA + 2)), 0.0, 1.0)
        wm_weight_eff = wm_weight  # fixed baseline mixture here

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Interference: WM trace for this state is contaminated by average of other states
            if nS > 1:
                avg_other = np.mean(np.delete(w, s, axis=0), axis=0)
            else:
                avg_other = w_0[s, :]
            W_s = (1.0 - phi_eff) * w[s, :] + phi_eff * avg_other

            # Policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-modulated learning
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0.5:
                # Move toward the chosen action (store it)
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * onehot
            else:
                # Move partially back toward uniform when not rewarded
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM arbitration by uncertainty and load.

    Idea:
    - Both RL and WM generate policies; arbitration weight is computed dynamically
      from the relative uncertainty (entropy) of RL vs WM and penalized by set size.
    - WM decays slowly toward uniform and encodes stronger traces for rewarded trials.

    Parameters
    ----------
    model_parameters : list or array-like of length 6
        lr : float
            RL learning rate (0..1).
        wm_weight : float
            Baseline WM weight multiplier (0..1).
        softmax_beta : float
            RL inverse temperature (scaled up internally).
        wm_decay : float
            WM decay per trial toward uniform (0..1).
        arbit_bias : float
            Bias term for arbitration (can be negative/positive).
        arbit_slope : float
            Slope scaling the effect of entropy difference on arbitration.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, arbit_bias, arbit_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        p = p / np.sum(p)
        return -np.sum(p * np.log(p + 1e-12))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # WM decay toward uniform each trial for the current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM choice probabilities (full vectors) for entropy
            rl_logits = softmax_beta * Q_s
            rl_probs = np.exp(rl_logits - np.max(rl_logits))
            rl_probs = rl_probs / np.sum(rl_probs)
            wm_logits = softmax_beta_wm * W_s
            wm_probs = np.exp(wm_logits - np.max(wm_logits))
            wm_probs = wm_probs / np.sum(wm_probs)

            # Scalar probabilities for the chosen action (for likelihood)
            p_rl = rl_probs[a]
            p_wm = wm_probs[a]

            # Arbitration by entropy difference and load penalty
            H_rl = entropy(rl_probs)
            H_wm = entropy(wm_probs)
            # Higher H_rl relative to H_wm -> favor WM; penalize WM as nS increases
            load_penalty = 0.25 * max(nS - 3, 0)  # zero for set size 3, grows with 6, etc.
            arb_input = arbit_bias + arbit_slope * (H_rl - H_wm) - load_penalty
            wm_weight_t = 1.0 / (1.0 + np.exp(-arb_input))  # sigmoid
            wm_weight_t = np.clip(wm_weight * wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens the chosen action trace
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong push toward the correct association on rewarded trials
                w[s, :] = 0.5 * w[s, :] + 0.5 * onehot

        blocks_log_p += log_p

    return -blocks_log_p