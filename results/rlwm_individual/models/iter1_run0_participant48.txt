Below are three standalone cognitive models tailored to the RL-WM task. Each model is a mixture of reinforcement learning (RL) and working memory (WM), with distinct mechanisms for how WM contributes and how set size modulates WMâ€™s influence.

Notes:
- All models follow the provided template structure and return the negative log-likelihood.
- Each uses up to 6 parameters, all used meaningfully.
- WM policy is a near-deterministic softmax over WM weights (softmax_beta_wm = 50).
- WM values decay toward uniform over time; rewarded pairings are stored as one-hot.
- Set size (nS) modulates WM in different ways per model (capacity/slope, gating, overload interference).

Model 1: Capacity-limited WM mixture (RLWM with forgetting and capacity-sigmoid)
def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with capacity-limited WM contribution.
    
    Mechanism:
    - RL: tabular Q-learning with learning rate lr and softmax temperature softmax_beta.
    - WM: item-specific store with decay toward uniform (decay), updated to one-hot on rewarded trials.
    - WM weight is block-wise scaled by a sigmoidal function of set size relative to capacity (capacity, slope),
      capturing that WM contributes more in small set sizes and less in large ones.
    
    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1), further downscaled by capacity function
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - decay: WM decay rate toward uniform each trial (0..1)
    - capacity: WM capacity (in number of items); higher capacity preserves WM influence for larger sets
    - slope: Slope of the capacity-related sigmoid; smaller => sharper transition with set size
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, decay, capacity, slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM block weight scaled by capacity function of set size
        wm_block = wm_weight / (1.0 + np.exp((nS - capacity) / max(1e-8, slope)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights for this state
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_block * p_wm + (1.0 - wm_block) * p_rl
            log_p += np.log(p_total + 1e-15)  # numerical stability

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - decay) * w + decay * w_0
            # On rewarded trial, store the one-hot correct action for this state
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


Model 2: PE-gated WM mixture with set-size penalty
def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with trial-wise gating of WM based on absolute prediction error and set-size penalty.
    
    Mechanism:
    - RL: tabular Q-learning with learning rate lr and softmax temperature softmax_beta.
    - WM: item-specific store with decay toward uniform (decay), updated to one-hot on rewarded trials.
    - WM mixture weight is computed each trial using a logistic gate:
        gate_t = sigmoid(wm_weight - nS_bias*(nS-3) - pe_sensitivity*|delta_prev|)
      Larger set sizes and larger recent unsigned PEs reduce WM reliance, shifting to RL.
    
    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM drive before gating (unbounded, used in logistic)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - decay: WM decay rate toward uniform each trial (0..1)
    - pe_sensitivity: how strongly unsigned PE reduces WM gating (>0)
    - nS_bias: how strongly larger set sizes reduce WM gating (>0)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, decay, pe_sensitivity, nS_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        prev_delta = 0.0  # previous trial's PE (within block)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise WM weight gate: penalize by set size and prior unsigned PE
            gate_input = wm_weight - nS_bias * (nS - 3.0) - pe_sensitivity * abs(prev_delta)
            wm_gate_t = sigmoid(gate_input)

            p_total = wm_gate_t * p_wm + (1.0 - wm_gate_t) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - decay) * w + decay * w_0
            if r > 0.0:
                # Store rewarded association
                w[s, :] = 0.0
                w[s, a] = 1.0

            prev_delta = delta  # for next trial's gating

        blocks_log_p += log_p

    return -blocks_log_p


Model 3: Overload-interference WM with asymmetric RL learning rates
def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with WM overload interference and asymmetric RL learning rates.
    
    Mechanism:
    - RL: tabular Q-learning with separate learning rates for rewards vs. no-rewards (lr_pos, lr_neg).
    - WM: item-specific store with decay toward uniform (decay), updated to one-hot on rewarded trials.
    - When set size exceeds WM capacity, WM policy is diluted toward uniform (interference),
      and the mixture weight is reduced proportionally to overload.
    
    Parameters:
    - lr_pos: RL learning rate for positive outcomes (0..1)
    - lr_neg: RL learning rate for negative outcomes (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - decay: WM decay rate toward uniform each trial (0..1)
    - capacity: WM capacity (in number of items); governs overload effects on WM policy and weight
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, decay, capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Overload factor when set size exceeds capacity
        overload = max(0.0, nS - capacity)
        interference = overload / max(1.0, float(nS))  # how much WM policy is diluted to uniform
        wm_block = wm_weight * (1.0 - interference)   # downweight WM under overload

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Interference-diluted WM policy when overloaded
            W_eff = (1.0 - interference) * W_s + interference * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            p_total = wm_block * p_wm + (1.0 - wm_block) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update with asymmetric learning rates
            lr_use = lr_pos if r > 0.0 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr_use * delta

            # WM decay toward uniform
            w = (1.0 - decay) * w + decay * w_0
            if r > 0.0:
                # Store rewarded association
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p