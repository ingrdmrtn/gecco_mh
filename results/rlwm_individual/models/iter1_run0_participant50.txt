def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting and WM lapse increasing with set size.

    Idea:
    - RL learns with delta rule and includes value forgetting toward uniform (captures load/interference).
    - WM produces near-deterministic choices, but with a lapse rate that increases with set size via a sigmoid.
      The effective WM choice probability is a mixture: (1 - lapse)*softmax(W) + lapse*(1/nA).
    - WM writes one-shot associations on reward and decays toward uniform otherwise, with decay tied to lapse.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base arbitration weight for WM contribution (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - rl_forget: RL forgetting rate toward uniform each time a state is visited (0..1).
    - wm_lapse0: Baseline WM lapse logistic intercept (can be negative/positive).
    - wm_lapse_slope: Positive slope for how much lapse grows with set size (higher = more lapses in larger sets).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_lapse0, wm_lapse_slope = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute a block-specific lapse from set size via sigmoid
        # Larger nS -> larger lapse
        lapse = 1.0 / (1.0 + np.exp(-(wm_lapse0 + wm_lapse_slope * (nS - 3.0))))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with high inverse temperature
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_soft = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Add lapse toward uniform (load-dependent)
            p_wm = (1.0 - lapse) * p_wm_soft + lapse * (1.0 / nA)

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL forgetting toward uniform for the visited state
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # RL delta update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            if r > 0.0:
                # One-shot write to the chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Decay toward uniform when negative/neutral feedback; tie to lapse
                w[s, :] = (1.0 - lapse) * w[s, :] + lapse * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recency-gated availability and WM precision scaling.

    Idea:
    - RL uses standard delta learning.
    - WM contribution is dynamically gated by recency of the current state's last visit and by set size:
        wm_weight_eff = wm_weight * exp(-wm_recency * lag) * (k_avail / (k_avail + nS - 1))
      where lag is trials since the state was last seen in this block.
    - WM precision is adjustable via a scaling of the WM inverse temperature.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - wm_recency: Positive rate controlling how fast WM availability decays with lag (trials since last seen).
    - wm_beta_scale: Multiplier for WM inverse temperature (precision) relative to baseline 50 (>0).
    - k_avail: Positive constant controlling how set size compresses WM availability (larger k reduces set-size penalty).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_recency, wm_beta_scale, k_avail = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    base_softmax_beta_wm = 50.0  # very deterministic baseline
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last seen trial index per state within the block
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute lag since last seen
            lag = t - last_seen[s] if last_seen[s] >= 0 else 1e6  # very large if first time
            # Compute dynamic WM weight: recency gating and set-size penalty
            avail_recency = np.exp(-wm_recency * float(lag))
            avail_setsize = k_avail / (k_avail + float(nS) - 1.0)
            wm_weight_eff = wm_weight * avail_recency * avail_setsize

            # Use effective WM weight in the mixture by shadowing the variable name
            wm_weight = wm_weight_eff

            # WM policy with adjustable precision
            softmax_beta_wm = base_softmax_beta_wm * wm_beta_scale
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Mixture policy
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            if r > 0.0:
                # Rewarded: one-shot write
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # Unrewarded: mild decay toward uniform, strength increases with time since last seen
                # Map lag to a bounded decay rate
                decay = 1.0 - np.exp(-wm_recency * float(lag))
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Update last seen time for this state
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration modulated by set size.

    Idea:
    - RL uses delta learning.
    - Arbitration between RL and WM varies per trial via a logistic function of:
        (WM confidence - RL uncertainty) + set-size modulation.
      WM confidence is approximated by max(W_s); RL uncertainty by entropy of the RL softmax policy.
      The resulting wm_weight is:
        wm_weight_eff = sigmoid(arbitration_bias + arbitration_slope*(conf_wm - uncert_rl_norm) + nS_sensitivity*(3 - nS))
      where uncert_rl_norm is RL policy entropy normalized to [0,1], and smaller nS boosts WM.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Not directly used as a constant weight; serves as a baseline via arbitration_bias interaction
                      by being added into the bias term to ensure parameter is used meaningfully.
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - arbitration_slope: Positive slope controlling sensitivity to (WM confidence - RL uncertainty).
    - arbitration_bias: Baseline bias toward WM vs RL arbitration.
    - nS_sensitivity: Weight for set-size modulation (positive => favors WM in small sets).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, arbitration_slope, arbitration_bias, nS_sensitivity = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy and full distribution for entropy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits_rl)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]
            # Entropy normalized to [0,1] by dividing by log(nA)
            entropy_rl = -np.sum(pi_rl * (np.log(pi_rl + 1e-12)))
            uncert_rl_norm = entropy_rl / np.log(nA)

            # WM policy (high precision)
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            pi_wm = np.exp(logits_wm)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = pi_wm[a]

            # WM confidence as max probability under WM
            conf_wm = np.max(pi_wm)

            # Dynamic arbitration weight (shadowing name to reuse template mixture line)
            bias_term = arbitration_bias + wm_weight_base  # incorporate wm_weight_base meaningfully
            setsize_term = nS_sensitivity * (3.0 - float(nS))
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(bias_term + arbitration_slope * (conf_wm - uncert_rl_norm) + setsize_term)))

            wm_weight = wm_weight_eff  # shadow the template variable

            # Mixture
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL delta update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-driven strengthening; small load-dependent decay otherwise
            if r > 0.0:
                eta = 0.8  # strong write on reward
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # Mild decay toward uniform; larger nS -> faster decay
                decay = 1.0 / (float(nS) + 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p