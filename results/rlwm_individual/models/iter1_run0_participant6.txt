def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-specific gating and decay.

    Idea:
    - Choices are a mixture of model-free RL and a one-shot WM store for rewarded S-A pairs.
    - WM mixture weight depends on set size: one weight for low load (nS <= 3), another for high load (nS > 3).
    - WM traces decay toward a uniform prior each trial; rewarded trials overwrite the WM trace for that state.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight_low: WM mixture weight used when set size <= 3 (0..1).
    - wm_weight_high: WM mixture weight used when set size > 3 (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_decay: WM decay rate toward uniform (0..1) applied each trial.
    
    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_low, wm_weight_high, softmax_beta, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # Working memory policy: softmax over WM traces
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-specific WM gating
            wm_weight_eff = wm_weight_low if nS <= 3 else wm_weight_high
            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform and overwrite on rewarded trials
            w = (1 - wm_decay) * w + wm_decay * w_0
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent WM decay and action stickiness.

    Idea:
    - Choices are a mixture of RL and WM.
    - WM decays faster as set size increases (interference): decay = base + k * f(load), clipped to [0,1].
    - RL choices include a stickiness bias to repeat the last action taken in the same state.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: global WM mixture weight (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_decay_base: base WM decay rate toward uniform (0..1).
    - wm_interference: additional decay per unit load beyond 3 (>=0), increases decay with set size.
    - stickiness: action repetition bias added to the chosen action from last visit to the same state (can be +/-).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay_base, wm_interference, stickiness = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track previous action per state for stickiness
        prev_action = -1 * np.ones(nS, dtype=int)

        # Compute load-dependent WM decay (more states => more interference)
        load_factor = max(0, nS - 3)  # 0 for 3, grows for 4..6
        wm_decay_eff = np.clip(wm_decay_base + wm_interference * load_factor, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Add stickiness bias to RL action values for current state
            Q_s = q[s,:].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += stickiness

            W_s = w[s,:]

            # RL and WM policies (probability of chosen action a)
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update (without stickiness in learning)
            delta = r-q[s][a]
            q[s][a] += lr*delta

            # WM decay with load-dependent interference and overwrite on reward
            w = (1 - wm_decay_eff) * w + wm_decay_eff * w_0
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

            # Update previous action for stickiness
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with decay-to-prior + WM recency-gated mixture and no-go encoding.

    Idea:
    - RL values decay toward a uniform prior each trial (useful under volatility/limited memory).
    - WM contributes only when the state has been seen recently (within a recency window in trials).
    - WM stores both go (rewarded action becomes 1.0) and no-go (punished action suppressed) information.
    - WM traces also decay toward uniform at a rate determined by the recency window (larger window => slower decay).

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight (0..1) when WM is engaged.
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - rl_decay: rate (0..1) that RL Q-values decay toward uniform each trial.
    - wm_window: positive scalar; WM engages if the state was seen within the last wm_window trials in the block.
                 Also controls WM decay rate: decay = 1 / max(1, wm_window).
    - wm_no_go: suppression strength (0..1) applied to the chosen action in WM after negative feedback.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, rl_decay, wm_window, wm_no_go = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        q_0 = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last time each state was seen
        last_seen = -1 * np.ones(nS, dtype=int)
        # WM decay determined by recency window (bigger window => slower decay)
        wm_decay = 1.0 / max(1.0, wm_window)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL decay-to-prior
            q = (1 - rl_decay) * q + rl_decay * q_0

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Recency-gated WM engagement
            seen_recently = (last_seen[s] >= 0) and ((t - last_seen[s]) <= wm_window)
            wm_weight_eff = wm_weight if seen_recently else 0.0

            p_total = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0

            # WM update: go on positive, no-go on negative
            if r > 0:
                # Overwrite with a strong "go" memory for the rewarded action
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
            else:
                # Suppress the chosen action (no-go) and renormalize softly
                w[s, a] = (1 - wm_no_go) * w[s, a]
                # renormalize row to sum to 1 while keeping relative proportions
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] = w[s, :] / row_sum
                else:
                    w[s, :] = w_0[s, :]

            # Update recency
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p