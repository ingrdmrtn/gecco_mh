def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-local forgetting and a state-specific choice-kernel (recency) bias,
    with age- and set-size-modulated forgetting.

    Mechanism:
    - Q-learning with exponential forgetting applied to the current state's action values.
    - A choice-kernel K tracks recent action usage per state to capture short-term recency bias.
    - The choice probability is a softmax over beta*Q + ck_weight*K.
    - Forgetting increases with set size and is stronger for older adults.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0-based within each block).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial; values identify block boundaries (values reset).
    set_sizes : array-like of int
        Set size for each trial; constant within a block.
    age : array-like or scalar
        Participant age; used to assign age group (<45 younger, >=45 older).
    model_parameters : tuple/list
        Five parameters:
        - alpha: Learning rate for Q-learning (0..1).
        - beta: Inverse temperature (>0).
        - decay_base: Base forgetting rate applied to Q in the visited state (0..1).
        - ck_alpha: Learning rate for the choice-kernel update (0..1).
        - ck_weight: Weight of the choice-kernel in decision (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, decay_base, ck_alpha, ck_weight = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        K = (1.0 / nA) * np.ones((nS, nA))  # neutral initial recency

        # Age and set size modulate forgetting: larger sets and older age -> more forgetting
        ss = float(block_set_sizes[0])
        age_forget_factor = 1.2 if is_older else 0.9
        decay = decay_base * age_forget_factor * (ss / 3.0)
        decay = min(max(decay, 0.0), 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            logits = beta * Q[s, :] + ck_weight * K[s, :]
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)
            pa = max(p[a], eps)
            nll -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # State-local forgetting (toward 0 baseline)
            Q[s, :] *= (1.0 - decay)

            # Choice-kernel update: exponential recency toward one-hot of chosen action
            K[s, :] = (1.0 - ck_alpha) * K[s, :]
            K[s, a] += ck_alpha * (1.0 - K[s, a])

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-bonus (UCB-like) exploration, modulated by age and set size.

    Mechanism:
    - Standard Q-learning for value.
    - An exploration bonus inversely proportional to the square root of visit counts
      encourages sampling uncertain actions in each state.
    - The bonus strength is larger in younger adults and in smaller set sizes.
    - Choice is softmax over beta*(Q + bonus).

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0-based within each block).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial; values identify block boundaries (values reset).
    set_sizes : array-like of int
        Set size for each trial; constant within a block.
    age : array-like or scalar
        Participant age; used to assign age group (<45 younger, >=45 older).
    model_parameters : tuple/list
        Four parameters:
        - alpha: Learning rate for Q-learning (0..1).
        - beta: Inverse temperature (>0).
        - ucb_weight: Base weight of the uncertainty bonus (>=0).
        - age_explore_shift: Modulates bonus by age (>=0). Effective weight is
          ucb_weight*(1+age_explore_shift) for younger and ucb_weight*(1-age_explore_shift) for older.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, ucb_weight, age_explore_shift = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        N = 1e-6 * np.ones((nS, nA))  # small prior to avoid div-by-zero

        ss = float(block_set_sizes[0])
        # Younger explore more; smaller set sizes amplify bonus
        age_scale = (1.0 + age_explore_shift) if is_younger else (1.0 - age_explore_shift)
        ss_scale = 3.0 / ss  # 1.0 at 3, 0.5 at 6
        bonus_weight = max(0.0, ucb_weight * age_scale * ss_scale)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # UCB-like bonus: larger for less-visited actions
            bonus = bonus_weight * (1.0 / np.sqrt(N[s, :]))

            logits = beta * (Q[s, :] + bonus)
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)
            pa = max(p[a], eps)
            nll -= np.log(pa)

            # Update counts and Q
            N[s, a] += 1.0
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL with an interference-prone focused memory cache and lapse.

    Mechanism:
    - RL system learns Q-values via standard Q-learning.
    - A focused memory cache stores a single "known-correct" action per state with a confidence.
      When engaged, the WM policy deterministically prefers the cached action.
    - Engagement probability (p_focus) is higher in younger adults and smaller set sizes.
    - Cache confidence decays globally due to interference, stronger in larger set sizes and older age.
    - Final policy mixes WM and RL according to p_focus, plus a lapse to uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0-based within each block).
    actions : array-like of int
        Chosen action (0..2) on each trial.
    rewards : array-like of {0,1}
        Obtained reward on each trial.
    blocks : array-like of int
        Block index per trial; values identify block boundaries (values reset).
    set_sizes : array-like of int
        Set size for each trial; constant within a block.
    age : array-like or scalar
        Participant age; used to assign age group (<45 younger, >=45 older).
    model_parameters : tuple/list
        Five parameters:
        - alpha: Learning rate for Q-learning (0..1).
        - beta: Inverse temperature for RL (>0).
        - focus_base: Base probability of engaging focused WM (0..1).
        - interference_rate: Rate at which WM confidence decays per trial due to interference (0..1).
        - lapse: Probability of a uniform random choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, focus_base, interference_rate, lapse = model_parameters
    age_val = age[0] if hasattr(age, "__len__") else age
    is_younger = 1 if age_val < 45 else 0

    nll = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM cache: for each state, store cached action and confidence (0..1)
        cached_action = -1 * np.ones(nS, dtype=int)
        cache_conf = np.zeros(nS)

        ss = float(block_set_sizes[0])
        # Engagement higher in younger and smaller set sizes
        age_focus_scale = 1.2 if is_younger else 0.8
        p_focus = focus_base * age_focus_scale * (3.0 / ss)
        p_focus = min(max(p_focus, 0.0), 1.0)

        # Interference stronger in larger set sizes and older adults
        age_interf_scale = 1.2 if not is_younger else 0.9
        decay_factor = 1.0 - (interference_rate * age_interf_scale * (ss / 3.0))
        decay_factor = min(max(decay_factor, 0.0), 1.0)

        for t in range(len(block_states)):
            # Apply global interference decay to cache confidence each trial
            cache_conf *= decay_factor
            # Drop cache entries with very low confidence
            low_conf_mask = cache_conf < 1e-3
            cached_action[low_conf_mask] = -1
            cache_conf[low_conf_mask] = 0.0

            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            rl_logits = beta * Q[s, :]
            rl_logits = rl_logits - np.max(rl_logits)
            p_rl = np.exp(rl_logits)
            p_rl /= np.sum(p_rl)

            # WM policy: deterministic if a cached action exists for the state and has confidence
            if cached_action[s] >= 0 and cache_conf[s] > 0.0:
                p_wm = np.zeros(nA)
                p_wm[cached_action[s]] = 1.0
            else:
                # If no cache, WM provides no information; fall back to uniform for WM component
                p_wm = (1.0 / nA) * np.ones(nA)

            # Mixture of WM and RL with lapse
            p_mix = p_focus * p_wm + (1.0 - p_focus) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            pa = max(p_final[a], eps)
            nll -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM cache update: store rewarded associations with full confidence
            if r > 0.5:
                cached_action[s] = a
                cache_conf[s] = 1.0

    return nll