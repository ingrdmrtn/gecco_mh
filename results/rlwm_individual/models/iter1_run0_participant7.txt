def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + one-shot WM cache with age- and set-size–modulated WM usage and per-state stickiness.

    Idea:
    - An RL system learns Q-values with separate learning rates for positive and negative prediction errors.
    - A lightweight WM cache stores the most recently rewarded action for each state ("one-shot" update).
    - On each choice, behavior mixes a WM policy (if a cached action exists) and an RL softmax with stickiness.
    - WM influence is down-weighted in larger set sizes and up-weighted for younger adults.

    Parameters (model_parameters):
    - alpha_pos: (0,1) learning rate for positive PE (reward > expected).
    - alpha_neg: (0,1) learning rate for negative PE (reward < expected).
    - beta: >0, inverse temperature for RL softmax (scaled internally).
    - kappa: real, per-state perseveration bias added to the previous action's logit.
    - wm_one_shot: (0,1), baseline weight for using WM when a cache exists (before age and set-size scaling).
    - lapse: [0,1), lapse probability mixing in uniform random responding.

    Inputs:
    - states: array (T,), state indices per trial (0..nS-1 within block).
    - actions: array (T,), chosen actions (0..2).
    - rewards: array (T,), binary reward (0/1).
    - blocks: array (T,), block index per trial.
    - set_sizes: array (T,), set size per trial (3 or 6).
    - age: array-like with a single numeric age in years.
    - model_parameters: list/tuple [alpha_pos, alpha_neg, beta, kappa, wm_one_shot, lapse]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, kappa, wm_one_shot, lapse = model_parameters
    beta = max(1e-6, beta) * 8.0
    lapse = min(max(lapse, 0.0), 0.25)
    age = float(age[0])

    # Age factor for WM usage: younger -> boost, older -> penalty
    if age >= 45.0:
        age_wm_factor = 0.75
    else:
        # linear boost up to ~25% at age 0, ~20% at age 21
        age_wm_factor = 1.0 + 0.25 * (45.0 - age) / 45.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))
        # WM cache: -1 indicates no cached action yet; otherwise 0..2
        cache = -1 * np.ones(nS, dtype=int)
        # Per-state previous action for stickiness
        prev_a = -1 * np.ones(nS, dtype=int)

        # Set-size attenuation of WM usage: smaller sets -> closer to 1
        setsize_wm_factor = min(1.0, 3.0 / float(nS))
        wm_weight_block = np.clip(wm_one_shot * age_wm_factor * setsize_wm_factor, 0.0, 1.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax with per-state stickiness
            logits = beta * Q[s, :].copy()
            if prev_a[s] >= 0:
                logits[prev_a[s]] += kappa
            logits = logits - np.max(logits)
            pi_rl = np.exp(logits)
            pi_rl = pi_rl / np.sum(pi_rl)

            # WM policy: if cached, choose it deterministically; otherwise uniform
            if cache[s] >= 0:
                pi_wm = np.zeros(nA)
                pi_wm[cache[s]] = 1.0
            else:
                pi_wm = np.ones(nA) / nA

            # Mix WM and RL, add lapse
            pi_mix = wm_weight_block * pi_wm + (1.0 - wm_weight_block) * pi_rl
            pi_final = (1.0 - lapse) * pi_mix + lapse * (1.0 / nA)

            p = max(1e-12, float(pi_final[a]))
            total_log_p += np.log(p)

            # RL update with separate alphas
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # WM one-shot caching upon reward
            if r > 0.5:
                cache[s] = a  # store current rewarded action as correct guess

            # Update stickiness memory
            prev_a[s] = a

    return -total_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with set-size and age–modulated temperature, uncertainty-driven exploration bonus, and forgetting.

    Idea:
    - A single RL system learns Q-values but includes:
      (i) an uncertainty bonus encouraging exploration of less-tried actions;
      (ii) a per-trial forgetting/decay toward zero;
      (iii) inverse temperature scaled by set size and age group.

    Parameters (model_parameters):
    - alpha: (0,1), learning rate for Q updates.
    - beta_base: >0, base inverse temperature (scaled internally).
    - beta_explore: >=0, magnitude of directed exploration bonus (optimism under uncertainty).
    - forgetting: (0,1), global decay toward zero applied to all Q each trial.
    - age_temp_shift: real, multiplicative temperature shift: beta_eff = beta_base * (1 + age_temp_shift*age_group) * (3/nS);
                      age_group = +1 for younger (<45), -1 for older (>=45).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with a single numeric age in years.
    - model_parameters: [alpha, beta_base, beta_explore, forgetting, age_temp_shift]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta_base, beta_explore, forgetting, age_temp_shift = model_parameters
    beta_base = max(1e-6, beta_base) * 6.0
    forgetting = np.clip(forgetting, 0.0, 0.2)  # small-to-moderate decay per trial
    beta_explore = max(0.0, beta_explore)
    age_years = float(age[0])
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize Q and action counts for uncertainty
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action

        # Effective temperature scales down for larger sets and with age_group
        beta_eff = beta_base * (1.0 + age_temp_shift * age_group) * (3.0 / float(nS))
        beta_eff = max(1e-6, beta_eff)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Directed exploration via uncertainty bonus
            U_s = beta_explore / np.sqrt(1.0 + N[s, :])

            logits = beta_eff * (Q[s, :] + U_s)
            logits -= np.max(logits)
            pi = np.exp(logits)
            pi = pi / np.sum(pi)

            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # Update counts then values
            N[s, a] += 1.0

            # Q-learning update
            Q[s, a] += alpha * (r - Q[s, a])

            # Global forgetting toward zero after each trial
            Q *= (1.0 - forgetting)

    return -total_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-process RL–WM with capacity-gated mixing, age-modulated WM refresh, and error-sensitive learning.

    Idea:
    - RL learns Q-values; learning accelerates following errors (meta-learning of learning rate).
    - WM maintains a probabilistic action distribution per state that can be refreshed and sharpened.
    - Mixture weight of WM vs RL depends on set size (capacity gate) and age group.
    - RL includes per-state perseveration (bias toward repeating last action).

    Parameters (model_parameters):
    - alpha: (0,1), base RL learning rate.
    - beta: >0, inverse temperature for RL policy (scaled internally).
    - wm_refresh: (0,1), strength of WM refresh/sharpening upon visiting a state; age scales this.
    - cap_slope: real, controls how strongly set size gates WM weight (positive = more WM for small sets).
    - persev: real, perseveration bias added to previous action’s logit (per state).
    - error_sens: >=0, multiplicative boost of learning rate on error trials (r=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays (T,)
    - age: array-like with a single numeric age in years.
    - model_parameters: [alpha, beta, wm_refresh, cap_slope, persev, error_sens]

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, wm_refresh, cap_slope, persev, error_sens = model_parameters
    beta = max(1e-6, beta) * 8.0
    wm_refresh = np.clip(wm_refresh, 0.0, 1.0)
    error_sens = max(0.0, error_sens)
    age_years = float(age[0])
    is_young = 1.0 if age_years < 45.0 else 0.0
    is_old = 1.0 - is_young

    # Age scaling for WM refresh: younger boost, older penalty
    age_wm_scale = 1.2 * is_young + 0.8 * is_old

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL and WM stores
        Q = np.zeros((nS, nA))
        W = np.ones((nS, nA)) / nA  # WM distribution per state
        prev_a = -1 * np.ones(nS, dtype=int)

        # Capacity gate: transform set size to WM weight via sigmoid
        # gate_input increases when nS is small; cap_slope controls steepness; age biases toward WM if young
        cap_term = (3.0 / float(nS)) - 0.5  # ~+0.5 for nS=3, ~0 for nS=6
        age_bias = 0.3 if is_young > 0.5 else -0.3
        gate_input = cap_slope * cap_term + age_bias
        wm_weight_block = 1.0 / (1.0 + np.exp(-5.0 * gate_input))  # sharp sigmoid

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy with perseveration
            logits = beta * Q[s, :].copy()
            if prev_a[s] >= 0:
                logits[prev_a[s]] += persev
            logits -= np.max(logits)
            pi_rl = np.exp(logits)
            pi_rl = pi_rl / np.sum(pi_rl)

            # WM policy is the current W[s,:]
            pi_wm = W[s, :].copy()

            # Mix policies (no lapse here; weight is block-constant)
            pi = wm_weight_block * pi_wm + (1.0 - wm_weight_block) * pi_rl
            p = max(1e-12, float(pi[a]))
            total_log_p += np.log(p)

            # Error-sensitive RL learning rate
            alpha_eff = alpha * (1.0 + error_sens * (1.0 - r))
            Q[s, a] += alpha_eff * (r - Q[s, a])

            # WM refresh on visit, sharpen toward the chosen action
            refresh = wm_refresh * age_wm_scale * (3.0 / float(nS))  # smaller sets, more effective refresh
            refresh = np.clip(refresh, 0.0, 1.0)
            if refresh > 0.0:
                # Move W[s] toward one-hot(a)
                target = np.zeros(nA)
                target[a] = 1.0
                W[s, :] = (1.0 - refresh) * W[s, :] + refresh * target

            # Additional sharpening if rewarded
            if r > 0.5:
                bonus = 0.5 * refresh
                target = np.zeros(nA)
                target[a] = 1.0
                W[s, :] = (1.0 - bonus) * W[s, :] + bonus * target

            # Renormalize WM distribution for numerical stability
            W[s, :] = np.maximum(W[s, :], 1e-8)
            W[s, :] = W[s, :] / np.sum(W[s, :])

            prev_a[s] = a

    return -total_log_p