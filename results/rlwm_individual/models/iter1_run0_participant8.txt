def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with selective maintenance (set-size- and age-dependent forgetting) and per-state
    choice stickiness.

    The agent learns Q-values with a standard delta rule. Within each visited state,
    unchosen actions decay toward zero with a forgetting rate that increases with set size
    and is amplified for older adults. A per-state stickiness bias favors repeating the last
    chosen action in that state, capturing working-memory-like short-term perseveration.

    Parameters
    ----------
    states : array-like of int
        State index on each trial within the block (0..nS-1).
    actions : array-like of int
        Observed action (0..2). If outside [0,2], treated as lapse for likelihood and learning.
    rewards : array-like of float
        Trial feedback (commonly 0/1). If negative (e.g., -1), treated as invalid: uniform likelihood and no learning.
    blocks : array-like of int
        Block index per trial. Learning resets each block.
    set_sizes : array-like of int
        Set size for each trial (3 or 6). Modulates forgetting.
    age : array-like of float
        Participant age; single-element array. Older group: age > 45, younger otherwise.
    model_parameters : iterable of 5 floats
        alpha   : Learning rate for Q-updates (0..1).
        beta    : Inverse temperature for softmax (>0).
        kappa   : Stickiness strength added to the chosen action in that state (can be >=0).
        phi0    : Baseline forgetting rate (0..1) controlling decay of unchosen actions.
        age_gain: Additional forgetting applied in older group (>=0). Reduced for younger.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, kappa, phi0, age_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val > 45 else 0.0
    # Soft age scaling: even younger may have small penalty, but much smaller
    age_scale = is_older * 1.0 + (1.0 - is_older) * 0.2

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        # Initialize Q-values to zero; unknown state-action pairs start neutral
        Q = np.zeros((nS, nA))
        # Last chosen action per state for stickiness; -1 means none
        last_a = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Compute effective forgetting rate in this trial/state
            # Larger set sizes and older age -> more forgetting
            phi_eff = phi0 + age_gain * age_scale + max(0.0, ss - 3.0) * (phi0 / 3.0)
            phi_eff = np.clip(phi_eff, 0.0, 1.0)

            # If invalid action or state index out of range, assign uniform prob and skip learning
            if not (0 <= s < nS) or not (0 <= a < nA):
                total_log_p += np.log(1.0 / nA)
                continue

            # Softmax over biased Q (stickiness adds kappa to last chosen in this state)
            prefs = Q[s, :].copy()
            if last_a[s] >= 0 and last_a[s] < nA:
                prefs[last_a[s]] += kappa

            # Softmax with numerical stability
            m = np.max(beta * prefs)
            expv = np.exp(beta * prefs - m)
            p = expv / np.sum(expv)
            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Handle invalid/missing reward: skip learning if reward < 0
            if r < 0:
                last_a[s] = a  # still update stickiness memory of last choice
                continue

            # Selective maintenance: decay unchosen actions in this visited state
            for a2 in range(nA):
                if a2 != a:
                    Q[s, a2] *= (1.0 - phi_eff)

            # Standard Q-learning update for chosen action
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update stickiness memory
            last_a[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-adaptive temperature and decay, modulated by set size and age.

    The agent learns Q-values with a delta rule and within-state decay. The choice
    inverse temperature beta is dynamically adapted based on a per-state uncertainty
    signal (running mean absolute prediction error). Set size increases uncertainty,
    reducing effective beta. Older adults are modeled as having lower baseline beta
    (more noise), i.e., less sensitivity to value differences, after controlling for set size.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Observed action (0..2). If invalid, treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). If negative, treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6). Larger set size reduces effective beta via added uncertainty.
    age : array-like of float
        Participant age; single-element array. Older group: age > 45.
    model_parameters : iterable of 5 floats
        alpha        : Learning rate for Q-updates (0..1).
        beta_base    : Baseline inverse temperature (pre-adaptation).
        eta_beta     : Sensitivity of beta to uncertainty (>=0); higher -> lower beta under uncertainty.
        lambda_decay : Within-state decay toward zero applied to all actions before update (0..1).
        age_shift    : Age-related beta shift; reduces beta for older, mildly increases for younger.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_base, eta_beta, lambda_decay, age_shift = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val > 45 else 0.0
    # Apply an age-dependent offset: older -> lower beta; younger -> slight increase
    beta_age_offset = -age_shift * is_older + 0.3 * age_shift * (1.0 - is_older)

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        # Per-state running uncertainty (mean absolute PE)
        U = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA):
                total_log_p += np.log(1.0 / nA)
                continue

            # Set-size penalty on beta (larger sets -> more uncertainty/noise)
            setsize_penalty = 0.5 * max(0.0, ss - 3.0)

            # Compute effective beta using a softplus to ensure positivity
            # Higher uncertainty U[s] and larger set size reduce effective beta.
            beta_eff_raw = beta_base + beta_age_offset - eta_beta * U[s] - setsize_penalty
            # Softplus approximation for positive inverse temperature
            beta_eff = np.log1p(np.exp(beta_eff_raw)) + 1e-6

            # Softmax with state-specific beta
            m = np.max(beta_eff * Q[s, :])
            expv = np.exp(beta_eff * Q[s, :] - m)
            p = expv / np.sum(expv)
            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Invalid reward: skip learning if negative
            if r < 0:
                continue

            # Within-state decay toward zero for all actions (forgetting/competition)
            Q[s, :] *= (1.0 - lambda_decay)

            # Q-learning update on chosen action
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update uncertainty (running mean absolute PE) with lambda_decay as smoothing rate
            rho = np.clip(lambda_decay, 0.0, 1.0)
            U[s] = (1.0 - rho) * U[s] + rho * abs(pe)

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor–critic with WM-gated policy mixture. WM gating depends on set size and age.

    The policy is a mixture between:
    - An actor-derived softmax over preferences updated via policy-gradient (actor–critic).
    - A WM policy that deterministically selects the last rewarded action in the current state.

    The WM mixture weight decreases with set size and is further reduced for older adults.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). If negative, treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6). Larger sets reduce WM reliance.
    age : array-like of float
        Participant age; single-element array. Older group: age > 45.
    model_parameters : iterable of 5 floats
        alpha_c : Critic learning rate for state value V (0..1).
        alpha_a : Actor learning rate for policy preferences (0..1).
        beta    : Inverse temperature for actor softmax (>0).
        wm_cap  : WM capacity/affinity (higher -> more WM weight).
        age_pen : Additional WM penalty applied in older group (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices.
    """
    alpha_c, alpha_a, beta, wm_cap, age_pen = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val > 45 else 0.0
    # Younger get a small fraction of the age penalty to reflect mild load sensitivity
    age_penalty = age_pen * (is_older * 1.0 + (1.0 - is_older) * 0.3)

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        # Initialize actor preferences and critic values
        P = np.zeros((nS, nA))
        V = np.zeros(nS)
        # WM store: last rewarded action per state; -1 if none
        wm_store = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA):
                total_log_p += np.log(1.0 / nA)
                continue

            # Actor softmax policy
            m = np.max(beta * P[s, :])
            expv = np.exp(beta * P[s, :] - m)
            pi = expv / np.sum(expv)

            # WM policy: deterministic if memory available, otherwise uniform
            if wm_store[s] >= 0 and wm_store[s] < nA:
                p_wm = np.zeros(nA)
                p_wm[wm_store[s]] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # WM weight via logistic transform; decreases with set size and age penalty
            wm_raw = wm_cap - (ss - 3.0) - age_penalty
            wm_weight = 1.0 / (1.0 + np.exp(-wm_raw))

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * pi
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            if r < 0:
                continue

            # TD error with state-value baseline
            delta = r - V[s]
            V[s] += alpha_c * delta

            # Policy-gradient update toward the taken action, scaled by TD error
            for a2 in range(nA):
                grad = (1.0 if a2 == a else 0.0) - pi[a2]
                P[s, a2] += alpha_a * delta * grad

            # WM store updates only on rewarded trials
            if r > 0:
                wm_store[s] = a

    return -float(total_log_p)