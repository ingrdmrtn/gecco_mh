def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM capacity-limited one-shot WM model.
    A mixture of:
      - RL: Q-learning with softmax choice.
      - WM: one-shot learning in WM with decay to uniform, where WM effectiveness
             is capacity-limited by set size.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight between WM and RL policies (0-1). Higher means more WM reliance.
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        wm_lr : float
            One-shot WM learning rate when rewarded (0-1): pushes WM toward the chosen action.
        wm_decay : float
            WM decay rate toward uniform per trial; scaled by set size (0-1).
        wm_capacity : float
            WM capacity (in items). Effective WM policy is down-weighted by a sigmoid of set size relative to capacity.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, wm_capacity = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Capacity-limited WM: blend current WM map with uniform depending on set size vs capacity.
            # phi in [0,1] decreases with larger nS relative to wm_capacity (sigmoid with unit slope).
            phi = 1.0 / (1.0 + np.exp(nS - wm_capacity))
            W_eff = (1 - phi) * w_0[s, :] + phi * W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total + 1e-12)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM decays faster with larger set sizes
            decay_n = wm_decay * (nS / 6.0)
            w[s, :] = (1 - decay_n) * w[s, :] + decay_n * w_0[s, :]
            # One-shot update when rewarded: push probability mass to chosen action
            if r > 0:
                w[s, :] = (1 - wm_lr) * w[s, :]
                w[s, a] += wm_lr
            # Normalize to a proper distribution
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= w[s, :].sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Confidence-weighted WM model.
    A mixture of:
      - RL: Q-learning with softmax.
      - WM: fast associative store whose decision temperature scales with its confidence.
            WM confidence is derived from the peakedness of the WM distribution and
            decays with set size.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight between WM and RL policies (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        wm_strength : float
            WM associative learning strength toward the chosen action when rewarded (0-1).
        wm_forget : float
            Baseline WM forgetting rate toward uniform (0-1), scaled by set size.
        wm_beta_scale : float
            Exponent scaling for how WM confidence sharpens WM policy temperature (>0).
            Larger values make high-confidence WM much more deterministic.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_strength, wm_forget, wm_beta_scale = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Confidence from separation between top-1 and top-2 WM preferences.
            sortW = np.sort(W_s)
            conf = max(0.0, sortW[-1] - sortW[-2])  # in [0,1]
            # Decay effective confidence with set size via the learned W_s itself (handled in updates),
            # and modulate the WM temperature by confidence.
            beta_eff = softmax_beta_wm * (conf ** max(0.0, wm_beta_scale))
            p_wm = 1 / np.sum(np.exp(beta_eff * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total + 1e-12)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Forgetting increases with set size
            forget_n = wm_forget * (nS / 6.0)
            w[s, :] = (1 - forget_n) * w[s, :] + forget_n * w_0[s, :]

            if r > 0:
                # Rewarded: move toward chosen action
                w[s, :] = (1 - wm_strength) * w[s, :]
                w[s, a] += wm_strength
            else:
                # Unrewarded: gently move away from chosen action
                remove = 0.5 * wm_strength * w[s, a]
                w[s, a] -= remove
                redistribute = remove / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= w[s, :].sum()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM recency (perseveration) with set-size-dependent interference.
    A mixture of:
      - RL: Q-learning with softmax.
      - WM: recency trace that favors the most recently chosen action in each state.
            Reward modulates how strongly the recency trace is updated, and larger set sizes
            cause stronger decay (interference) toward uniform.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight between WM and RL policies (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        recency_lr : float
            Learning rate for updating the WM recency trace toward the chosen action (0-1).
        decay_rate : float
            WM decay rate toward uniform per trial; scaled by set size (0-1).
        reward_mod : float
            Reward modulation factor (0-1): weight of update when rewarded vs. unrewarded.
            Effective update = recency_lr * reward_mod if rewarded, recency_lr * (1 - reward_mod) otherwise.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, recency_lr, decay_rate, reward_mod = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy is a softmax over the recency trace.
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total + 1e-12)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Interference: stronger decay with larger set size
            decay_n = decay_rate * (nS / 6.0)
            w[s, :] = (1 - decay_n) * w[s, :] + decay_n * w_0[s, :]

            # Recency update, modulated by reward
            eff_lr = recency_lr * (reward_mod if r > 0 else (1.0 - reward_mod))
            w[s, :] = (1 - eff_lr) * w[s, :]
            w[s, a] += eff_lr

            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= w[s, :].sum()

        blocks_log_p += log_p

    return -blocks_log_p