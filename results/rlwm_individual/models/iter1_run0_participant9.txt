def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL with state-local forgetting and directed exploration, modulated by set size and age.

    Mechanism:
    - Model-free RL updates Q(s,a) with a single learning rate.
    - When a state is visited, non-chosen action values in that state decay toward 0 (forgetting),
      capturing load- and age-related maintenance limits.
    - Directed exploration bonus encourages sampling less-visited actions using a count-based bonus.
      The magnitude of the bonus scales with set size and age group.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial in {0,1,2}.
    rewards : array-like of int
        Binary reward per trial in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; e.g., 3 or 6).
    age : array-like of float
        Age in years. Used to define age group (younger <45, older >=45).
    model_parameters : iterable of 5 floats
        alpha          : RL learning rate in [0,1].
        beta           : inverse temperature base, scaled internally by 10.
        decay_base     : base decay strength mapped by logistic to (0,1).
        age_effect     : nonnegative factor scaling age-group differences on decay and exploration.
        explore_bonus  : base scale of directed exploration bonus.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, decay_base, age_effect, explore_bonus = model_parameters
    beta = beta * 10.0

    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Q initialized at zero (neutral)
        Q = np.zeros((nS, nA))
        # Visit counts for directed exploration
        N = np.zeros((nS, nA))

        # Map decay_base to (0,1)
        base_decay = 1.0 / (1.0 + np.exp(-decay_base))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            # Effective decay increases with set size and age (older forget more under load)
            eff_decay = base_decay * (set_size / 6.0) * (1.0 + age_effect * is_older)
            eff_decay = min(max(eff_decay, 0.0), 1.0)

            # Directed exploration: count-based bonus
            # Larger sets encourage more exploration; older adults explore less (scaled by age_effect)
            bonus_scale = (set_size / 6.0) * (1.0 - age_effect * is_older)
            bonus_scale = max(0.0, bonus_scale)
            bonus = np.zeros(nA)
            for aa in range(nA):
                bonus[aa] = explore_bonus * bonus_scale / np.sqrt(1.0 + N[s, aa])

            # Softmax over Q + bonus
            prefs = Q[s, :] + bonus
            prefs_centered = prefs - np.max(prefs)
            expv = np.exp(beta * prefs_centered)
            p = expv / np.sum(expv)

            pa = max(p[a], 1e-12)
            nll -= np.log(pa)

            # Update counts and RL values
            N[s, a] += 1.0
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # State-local forgetting: decay all non-chosen actions for the visited state
            for aa in range(nA):
                if aa != a:
                    Q[s, aa] *= (1.0 - eff_decay)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Dual-rate RL with set-size/age arbitration and cross-state interference.

    Mechanism:
    - Two RL systems learn in parallel: a "fast" learner and a "slow" learner (different alphas).
    - Arbitration mixes their action preferences; reliance on the fast system decreases with set size
      and for older adults (age >= 45).
    - Cross-state interference: with larger set sizes and in older adults, the Q-values for the
      current state are blended with the global mean Q across states, modeling confusion/interference.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial in {0,1,2}.
    rewards : array-like of int
        Binary reward per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block).
    age : array-like of float
        Age in years; older group if age >= 45.
    model_parameters : iterable of 6 floats
        alpha_fast         : fast learner learning rate in [0,1].
        alpha_slow         : slow learner learning rate in [0,1].
        beta               : inverse temperature base, scaled by 10 internally.
        mix_base           : baseline arbitration weight mapped by logistic to (0,1) before scaling.
        interference_base  : base interference level mapped by logistic to (0,1).
        age_effect         : nonnegative factor scaling age-group effects on mix and interference.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_fast, alpha_slow, beta, mix_base, interference_base, age_effect = model_parameters
    beta = beta * 10.0

    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    # Map bases through logistic to keep in 0..1
    mix0 = 1.0 / (1.0 + np.exp(-mix_base))
    interf0 = 1.0 / (1.0 + np.exp(-interference_base))

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        Qf = np.zeros((nS, nA))
        Qs = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            # Arbitration weight on fast system:
            # higher for small set, reduced for older adults
            w_fast = mix0 * (3.0 / set_size) * (1.0 - 0.5 * age_effect * is_older)
            w_fast = min(max(w_fast, 0.0), 1.0)

            # Interference level rises with set size and age
            gamma = interf0 * ((set_size - 3.0) / 3.0) * (1.0 + age_effect * is_older)
            gamma = min(max(gamma, 0.0), 1.0)

            # Compute global means (avoid NaNs; means of current matrices)
            mean_Qf = np.mean(Qf, axis=0) if nS > 0 else np.zeros(nA)
            mean_Qs = np.mean(Qs, axis=0) if nS > 0 else np.zeros(nA)

            Qf_tilde = (1.0 - gamma) * Qf[s, :] + gamma * mean_Qf
            Qs_tilde = (1.0 - gamma) * Qs[s, :] + gamma * mean_Qs

            Qmix = w_fast * Qf_tilde + (1.0 - w_fast) * Qs_tilde

            # Softmax policy
            prefs_centered = Qmix - np.max(Qmix)
            expv = np.exp(beta * prefs_centered)
            p = expv / np.sum(expv)

            pa = max(p[a], 1e-12)
            nll -= np.log(pa)

            # Updates for both systems (no interference applied to learning target)
            pe_f = r - Qf[s, a]
            pe_s = r - Qs[s, a]
            Qf[s, a] += alpha_fast * pe_f
            Qs[s, a] += alpha_slow * pe_s

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL + RPE-gated Working Memory with load- and age-dependent arbitration.

    Mechanism:
    - RL learns Q(s,a) with a standard delta rule.
    - A simple WM store keeps the last rewarded action for a state when a surprising positive
      prediction error occurs (|RPE| > gate_threshold and reward=1). WM entries decay toward
      uniform upon each revisit (forgetting).
    - Arbitration between WM and RL depends on set size (smaller set -> stronger WM) and age
      (older adults rely less on WM). WM policy uses a separate, high inverse temperature.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial in {0,1,2}.
    rewards : array-like of int
        Binary reward per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block).
    age : array-like of float
        Age in years; older group if age >= 45.
    model_parameters : iterable of 6 floats
        alpha          : RL learning rate in [0,1].
        beta           : RL inverse temperature base, scaled by 10 internally.
        beta_wm        : WM inverse temperature base, scaled by 10 internally.
        gate_threshold : nonnegative threshold on absolute RPE for WM encoding (with reward=1).
        wm_forget      : WM forgetting rate per state visit in [0,1].
        age_effect     : nonnegative factor reducing WM arbitration weight for older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, beta_wm, gate_threshold, wm_forget, age_effect = model_parameters
    beta = beta * 10.0
    beta_wm = beta_wm * 10.0

    a0 = age[0] if hasattr(age, "__len__") else age
    is_older = 1.0 if a0 >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))
        # WM store as probability distributions per state; start as uniform
        W = (1.0 / nA) * np.ones((nS, nA))
        # Track whether WM has a strong stored association for a state
        has_mem = np.zeros(nS, dtype=float)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size = float(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_centered = q_s - np.max(q_s)
            expv = np.exp(beta * q_centered)
            p_rl = expv / np.sum(expv)

            # WM policy: if memory exists -> near one-hot; else uniform
            w_s = W[s, :]
            # Apply forgetting toward uniform on each visit
            W[s, :] = (1.0 - wm_forget) * W[s, :] + wm_forget * (1.0 / nA)
            w_s = W[s, :]

            # Softmax with WM inverse temperature (even if near-one-hot, this stabilizes NLL)
            wm_centered = w_s - np.max(w_s)
            expw = np.exp(beta_wm * wm_centered)
            p_wm = expw / np.sum(expw)

            # Arbitration weight: stronger for small set, reduced for older adults
            base_wm = 1.0 / (1.0 + (set_size - 3.0))  # 1.0 for set=3, 0.5 for set=5, 0.25 for set=6
            w_eff = base_wm * (1.0 - age_effect * is_older)
            w_eff = min(max(w_eff, 0.0), 1.0)

            p = w_eff * p_wm + (1.0 - w_eff) * p_rl

            pa = max(p[a], 1e-12)
            nll -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # RPE-gated WM encoding on positive surprising reward
            if (r >= 0.5) and (abs(pe) > gate_threshold):
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                W[s, :] = one_hot
                has_mem[s] = 1.0

    return nll