def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with set-size-specific WM weighting, WM interference, and perseveration bias.

    Idea:
    - A model-free RL system learns Q-values with a standard delta rule.
    - A WM system stores recent correct associations and is near-deterministic.
    - The mixture weight for WM is different for low (set size = 3) and high load (set size = 6).
    - Under high load, WM representations are noisier (interference toward uniform).
    - Choices also exhibit a perseveration bias toward the last action taken in the same state.

    Parameters (6 total):
    - lr: scalar in [0,1], learning rate for RL Q-values.
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_weight_small: WM mixture weight when set size is 3.
    - wm_weight_large: WM mixture weight when set size is 6.
    - wm_interference: additional blending of WM values toward uniform when set size is larger (scaled by (nS-3)/3).
    - pers_beta: strength of perseveration bias toward the last action in the same state (added to RL utilities before softmax).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_weight_small, wm_weight_large, wm_interference, pers_beta = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size-specific WM mixture weight
        wm_weight_eff = wm_weight_small if nS <= 3 else wm_weight_large
        wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

        # Interference scale with set size
        interf = wm_interference * max(0, (nS - 3) / 3.0)
        interf = float(np.clip(interf, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy with perseveration utility boost
            U_rl = Q_s.copy()
            if last_action[s] >= 0:
                U_rl[last_action[s]] += pers_beta
            denom_rl = np.sum(np.exp(softmax_beta * (U_rl - U_rl[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM interference toward uniform under high load
            W_eff = (1.0 - interf) * W_s + interf * w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - 0.0) * w + 0.0 * w_0  # no global decay term here; interference is handled in policy

            # WM update: error-corrective, reward-driven strengthening of chosen action
            # Move W_s slightly toward a one-hot on the chosen action when reward is high; else slight push away
            if r > 0.5:
                # strengthen chosen action, renormalize softly by bounded update
                gain = 0.5  # fixed internal gain to keep within param cap (no extra param)
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain
            else:
                # small demotion of the chosen action toward uniform
                loss_gain = 0.2
                w[s, a] = (1.0 - loss_gain) * w[s, a] + loss_gain * (1.0 / nA)
                others = [aa for aa in range(nA) if aa != a]
                for aa in others:
                    w[s, aa] = (1.0 - 0.0) * w[s, aa]  # keep others unchanged

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Q-value forgetting + WM retrieval probability (set-size dependent) + lapse noise.

    Idea:
    - RL system updates with a delta rule and includes per-trial forgetting toward uniform.
    - WM system stores rewarded associations (one-shot) and decays toward uniform each trial.
    - On each trial, WM guides choice with probability that depends on set size (different hit rates for 3 vs 6),
      otherwise RL guides choice.
    - A small undirected lapse epsilon blends final choice probability toward uniform.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_hit3: probability WM is available (retrieved) when set size is 3.
    - wm_hit6: probability WM is available when set size is 6.
    - wm_decay: WM decay rate per trial toward uniform baseline.
    - epsilon: lapse probability mixing policy with uniform (0=no lapse, 1=pure uniform).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_hit3, wm_hit6, wm_decay, epsilon = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    wm_hit3 = float(np.clip(wm_hit3, 0.0, 1.0))
    wm_hit6 = float(np.clip(wm_hit6, 0.0, 1.0))
    epsilon = float(np.clip(epsilon, 0.0, 1.0))
    wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM hit probability
        p_hit = wm_hit3 if nS <= 3 else wm_hit6

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM softmax (deterministic)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration by availability
            p_mix = p_hit * p_wm + (1.0 - p_hit) * p_rl

            # Lapse toward uniform
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with delta rule
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # RL forgetting toward uniform (small implicit via Q shrinkage)
            phi = 0.0  # fixed to 0 to respect param cap; here forgetting handled by bounded learning already

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM one-shot update: only reinforce when reward is obtained
            if r > 0.5:
                # Move W_s toward a one-hot on chosen action by the same wm_decay step (reusing parameter)
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reliability-based arbitration (entropy comparator) and WM plasticity.

    Idea:
    - Both RL and WM produce policies; arbitration weight for WM is computed dynamically from their relative
      reliabilities, operationalized as negative policy entropy (lower entropy -> higher reliability).
    - Arbitration combines a bias term and a gain on the entropy difference; also includes a modest set-size effect
      by adding log(nS/3) to the arbitration input (no extra parameter).
    - WM undergoes decay toward uniform, and plasticity that is sensitive to reward (promotion) and non-reward (demotion).

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_eta: WM plasticity step size for updating the current state's WM values.
    - wm_decay: WM decay rate per trial toward uniform baseline.
    - arb_gain: slope controlling sensitivity of WM weight to the entropy difference (H_RL - H_WM).
    - arb_bias: bias term shifting the WM weight up or down independent of reliability.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_eta, wm_decay, arb_gain, arb_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    wm_eta = float(np.clip(wm_eta, 0.0, 1.0))
    wm_decay = float(np.clip(wm_decay, 0.0, 1.0))

    def entropy_from_logits(logits):
        # logits can be any utilities; compute softmax then entropy
        probs = np.exp(logits - np.max(logits))
        probs = probs / np.sum(probs)
        probs = np.clip(probs, 1e-12, 1.0)
        return -np.sum(probs * np.log(probs))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy probability of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Reliability-based arbitration:
            # Compute entropies of RL and WM induced policies (from their utilities)
            H_rl = entropy_from_logits(softmax_beta * Q_s)
            H_wm = entropy_from_logits(softmax_beta_wm * W_s)

            # Set-size bias term: larger sets shift weight away from WM (via +log(nS/3))
            load_term = np.log(max(nS, 1) / 3.0)

            wm_weight_eff = sigmoid(arb_bias + arb_gain * (H_rl - H_wm + load_term))
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM plasticity at current state:
            # Reward: move toward a one-hot on chosen action; Non-reward: demote chosen and mildly promote others
            if r > 0.5:
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            else:
                # demote chosen
                w[s, a] = (1.0 - wm_eta) * w[s, a] + wm_eta * (1.0 / nA)
                # small redistribution to others to keep mass roughly balanced
                others = [aa for aa in range(nA) if aa != a]
                for aa in others:
                    w[s, aa] = (1.0 - 0.0) * w[s, aa]

        blocks_log_p += log_p

    return -blocks_log_p