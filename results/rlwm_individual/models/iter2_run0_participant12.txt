def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Set-size-specific Q-learning with age-dependent within-state perseveration.

    Summary
    - Standard model-free Q-learning.
    - Separate learning rates for small vs large set sizes: alpha_3 and alpha_6.
    - Softmax choice with inverse temperature beta.
    - Age-dependent within-state perseveration: a bias (kappa) added to the logit of the last chosen action
      for the same state (captures tendency to repeat actions; typically stronger under higher WM load).
      Younger and older groups have different kappa parameters.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based).
    actions : array-like of int
        Chosen action at each trial. Valid: {0,1,2}.
    rewards : array-like of int
        Observed rewards; expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within each block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older.
    model_parameters : tuple/list of floats
        (alpha_3, alpha_6, beta, kappa_young, kappa_old)
        - alpha_3: learning rate in set size 3 blocks
        - alpha_6: learning rate in set size 6 blocks
        - beta: inverse temperature for softmax
        - kappa_young: perseveration strength for younger group
        - kappa_old: perseveration strength for older group

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_3, alpha_6, beta, kappa_y, kappa_o = model_parameters
    age_val = age[0]
    kappa = kappa_y if age_val < 45 else kappa_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # within-state last chosen action

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1) or (s < 0) or (s >= nS):
                # Invalid trial (e.g., lapses encoded as negative or out-of-range). Assume uniform.
                nll -= np.log(1.0 / nA)
                continue

            # Learning rate depends on set size
            alpha = alpha_3 if ss == 3 else alpha_6

            # Softmax with within-state perseveration bias on the previous action
            logits = beta * Q[s, :].copy()
            if last_action[s] >= 0:
                logits[last_action[s]] += kappa

            # Numerically stable softmax
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)

            p = np.clip(p_vec[a], 1e-12, 1.0)
            nll -= np.log(p)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Update within-state perseveration memory
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    State-wise actor-critic with asymmetric learning and age-dependent sensitivity to negative outcomes.

    Summary
    - Policy is represented by state-action preferences (actor), converted to probabilities via softmax (beta).
    - A state-value baseline (critic) estimates expected reward.
    - Actor updates use different learning rates for positive vs negative outcomes:
        alpha_pos for r=1, alpha_neg_age for r=0, where alpha_neg_age depends on age group.
      This captures differential sensitivity to negative feedback, often varying with age.
    - Critic updates with separate learning rate (nu).
    - Set size enters through state indexing but does not directly change parameters, allowing a clean
      test of whether load effects can be captured by learning asymmetries alone.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based).
    actions : array-like of int
        Chosen action at each trial. Valid: {0,1,2}.
    rewards : array-like of int
        Observed rewards; expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within each block (used for nS only).
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older (selects alpha_neg).
    model_parameters : tuple/list of floats
        (alpha_pos, alpha_neg_young, alpha_neg_old, beta, nu)
        - alpha_pos: actor learning rate when r=1
        - alpha_neg_young: actor learning rate when r=0 for younger participants
        - alpha_neg_old: actor learning rate when r=0 for older participants
        - beta: inverse temperature of the softmax policy
        - nu: critic learning rate for state values

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_pos, alpha_neg_y, alpha_neg_o, beta, nu = model_parameters
    age_val = age[0]
    alpha_neg = alpha_neg_y if age_val < 45 else alpha_neg_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]

        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        prefs = np.zeros((nS, nA))  # actor preferences
        V = np.zeros(nS)           # critic value

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                continue

            logits = beta * prefs[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            p = np.clip(pi[a], 1e-12, 1.0)
            nll -= np.log(p)

            # TD error with bandit-like state value baseline
            delta = r - V[s]

            # Critic update
            V[s] += nu * delta

            # Actor update (REINFORCE with baseline), with asymmetric learning rate
            alpha = alpha_pos if r > 0.5 else alpha_neg
            # Update only chosen action towards higher preference, others decrease proportionally
            for aa in range(nA):
                if aa == a:
                    prefs[s, aa] += alpha * (1.0 - pi[aa]) * delta
                else:
                    prefs[s, aa] += -alpha * pi[aa] * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with entropy-regularized exploration scaled by set size and age, plus value decay.

    Summary
    - Standard Q-learning with learning rate alpha.
    - Q-values decay toward zero at each decision in the visited state with rate 'decay',
      modeling memory drift/interference under load.
    - Exploration is increased in larger set sizes via an entropy-like regularization:
      the effective inverse temperature is reduced as set size grows,
      with an age-dependent scaling (ent_w_young vs ent_w_old).
      beta_eff = beta / (1 + ent_w_age * (set_size - 3))
      Thus, larger sets or higher ent_w lead to more stochastic choices.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based).
    actions : array-like of int
        Chosen action at each trial. Valid: {0,1,2}.
    rewards : array-like of int
        Observed rewards; expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within each block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older (selects ent_w).
    model_parameters : tuple/list of floats
        (alpha, beta, ent_w_young, ent_w_old, decay)
        - alpha: learning rate for Q-learning
        - beta: base inverse temperature
        - ent_w_young: exploration scaling weight for younger group
        - ent_w_old: exploration scaling weight for older group
        - decay: value decay in the current state (0=no decay, 1=full reset each visit)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, ent_w_y, ent_w_o, decay = model_parameters
    age_val = age[0]
    ent_w = ent_w_y if age_val < 45 else ent_w_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = int(block_set_sizes[t])

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                continue

            # Entropy-driven exploration scaling with set size and age
            beta_eff = beta / (1.0 + ent_w * (ss - 3.0))
            beta_eff = max(beta_eff, 1e-6)

            # Softmax policy
            logits = beta_eff * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)

            p = np.clip(p_vec[a], 1e-12, 1.0)
            nll -= np.log(p)

            # Decay values in current state to model drift/interference
            Q[s, :] *= (1.0 - np.clip(decay, 0.0, 1.0))

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

    return nll