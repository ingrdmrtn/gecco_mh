def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated WM reliance, asymmetric RL learning rates, and lapse.

    Policy:
      - RL: tabular Q-learning with softmax; separate learning rates for positive vs negative prediction errors.
      - WM: one-shot associative store that encodes rewarded mappings without decay.
      - Arbitration: trial-wise WM weight increases when RL is uncertain (low max softmax probability).
      - Lapse: with small probability, choice is uniform random.

    Parameters (model_parameters):
      - lr_pos: RL learning rate for positive prediction errors (0-1).
      - lr_neg: RL learning rate for negative prediction errors (0-1).
      - wm_gain: maximum contribution of WM under uncertainty (0-1).
      - softmax_beta: inverse temperature for RL (scaled internally by 10).
      - gate_temp: slope for uncertainty gating; larger -> sharper reliance on WM when RL is uncertain (>=0).
      - lapse: probability of random responding on any trial (0-0.2 recommended).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_gain, softmax_beta, gate_temp, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over WM weights (near-deterministic)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-gated mixture: wm weight increases when RL is uncertain (1 - max_p)
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_probs / np.sum(rl_probs)
            max_p = np.max(rl_probs)
            uncert = 1.0 - max_p
            wm_weight_t = np.clip(wm_gain * (1.0 - np.exp(-gate_temp * uncert)) if gate_temp > 0 else wm_gain * uncert, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1 - wm_weight_t) * p_rl
            # Lapse to uniform
            p_total = (1 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM update: store rewarded mapping as one-hot; no decay
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited slot-based WM with probabilistic storage and strength.

    Policy:
      - RL: tabular Q-learning with softmax.
      - WM: stores up to K distinct state-action associations when rewarded.
            If capacity is full and a new state is stored, evicts the oldest (FIFO).
            Stored mappings are near one-hot with configurable strength.
      - Mixture: wm_weight scaled by effective capacity use: wm_weight_eff = wm_weight * min(1, K / set_size).
      - Storage noise: on reward, store with probability store_prob; otherwise skip storage.

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - softmax_beta: inverse temperature for RL (scaled by 10 internally).
      - wm_weight: base WM mixture weight (0-1).
      - K_slots: integer-like capacity (number of states WM can hold, 0-6).
      - store_prob: probability to encode into WM upon reward (0-1).
      - wm_strength: strength of WM encoding toward one-hot (0-1); 1 = perfect one-hot.

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, wm_softmax_beta, wm_weight, K_slots, store_prob, wm_strength = model_parameters
    # Note: wm_softmax_beta is not used directly as a temperature; it governs the RL temperature via template.
    softmax_beta = wm_softmax_beta
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM bookkeeping for capacity and recency (FIFO)
        in_mem = np.zeros(nS, dtype=bool)
        fifo_queue = []

        cap_scale = min(1.0, (float(K_slots) / float(nS)) if nS > 0 else 0.0)
        wm_weight_eff = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy (if not in memory, distribution remains closer to uniform)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM storage with capacity and strength
            if r > 0 and np.random.rand() < store_prob:
                # If new state and capacity full, evict oldest
                if (not in_mem[s]) and (np.sum(in_mem) >= int(np.floor(K_slots))):
                    # Evict oldest
                    if len(fifo_queue) > 0:
                        evict_s = fifo_queue.pop(0)
                        in_mem[evict_s] = False
                        w[evict_s, :] = w_0[evict_s, :]

                # Store/refresh this state
                in_mem[s] = True
                # Move s to end of FIFO if already present
                if s in fifo_queue:
                    fifo_queue = [x for x in fifo_queue if x != s]
                fifo_queue.append(s)

                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_strength) * w_0[s, :] + wm_strength * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reliability-based arbitration and WM decay.

    Policy:
      - RL: tabular Q-learning with softmax.
      - WM: associative table that updates toward one-hot on reward and decays toward uniform each trial.
      - Arbitration: trial-wise WM weight determined by a logistic function of relative reliabilities:
          wm_weight_t = sigmoid( logit(wm_weight0) + arb_beta * (rel_wm - rel_rl) )
        where rel_rl decreases with absolute prediction error and rel_wm reflects how consistent WM is.

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - softmax_beta: inverse temperature for RL (scaled by 10 internally).
      - wm_weight0: baseline WM mixture weight in logit center (0-1).
      - wm_eta_w: WM learning rate toward one-hot when rewarded (0-1).
      - wm_forget: WM decay rate toward uniform each trial (0-1).
      - arb_beta: arbitration slope; larger -> stronger shifts toward the more reliable system (>=0).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, wm_eta_w, wm_forget, arb_beta = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Reliability traces
        rel_rl = 0.5
        rel_wm = 0.5

        # Helper functions
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))
        def logit(p):
            p = np.clip(p, 1e-6, 1-1e-6)
            return np.log(p/(1-p))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight from relative reliability
            wm_weight_t = sigmoid(logit(wm_weight0) + arb_beta * (rel_wm - rel_rl))

            p_total = wm_weight_t * p_wm + (1 - wm_weight_t) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            w[s, :] = (1 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM learn on reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_eta_w) * w[s, :] + wm_eta_w * target

            # Update reliabilities:
            # - RL reliability higher when absolute PE is small
            rel_rl = 0.8 * rel_rl + 0.2 * (1.0 - np.abs(delta))
            # - WM reliability higher when WM favors chosen action and gets rewarded, else decreases
            wm_choice_prob = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # reward-consistent WM confidence
            rel_wm = 0.8 * rel_wm + 0.2 * (wm_choice_prob if r > 0 else (1.0 - wm_choice_prob))

        blocks_log_p += log_p

    return -blocks_log_p