def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated forgetting and count-based exploration bonus plus lapse.

    The agent learns Q-values with a learning rate and applies a global forgetting/decay toward 0.
    Forgetting increases with set size and with older age. An exploration bonus inversely
    proportional to action-visit counts encourages exploring under-visited actions. A lapse mixes
    the policy with uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary rewards (0 or 1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define age group.
    model_parameters : tuple/list of floats
        (alpha, beta, kappa, forget_base, forget_age, lapse)
        - alpha: learning rate (squashed to 0..1)
        - beta: inverse temperature (scaled internally)
        - kappa: scale of count-based bonus (softplus to ensure >=0)
        - forget_base: base forgetting magnitude (sigmoid to 0..1)
        - forget_age: additional forgetting for older group (added before sigmoid)
        - lapse: lapse rate mixing with uniform policy (sigmoid to 0..0.5)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, kappa, forget_base, forget_age, lapse = model_parameters

    # Parameter transforms
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def softplus(x):
        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)

    alpha = sigmoid(alpha)
    beta = sigmoid(beta) * 12.0
    kappa = softplus(kappa)
    lapse = sigmoid(lapse) * 0.5

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize Q and counts
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for exploration bonus

        # Compute effective forgetting rate for this block
        # More forgetting when set size is larger; older age increases forgetting.
        # Base in (0..1), add age shift before sigmoid, then scale by load factor.
        base_forget = sigmoid(forget_base + forget_age * older)
        load_factor = (nS - 3) / 3.0  # 0 for set size 3, 1 for set size 6
        forget_eff = base_forget * load_factor  # in 0..1

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Count-based exploration bonus for the current state
            bonus = kappa / (np.sqrt(N[s, :] + 1.0))

            prefs = Q[s, :] + bonus
            prefs_centered = prefs - np.max(prefs)
            exp_p = np.exp(beta * prefs_centered)
            pi = exp_p / (np.sum(exp_p) + eps)

            # Lapse mixture
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Apply global forgetting/decay toward 0 before update
            if forget_eff > 0:
                Q *= (1.0 - forget_eff)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update counts
            N[s, a] += 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL with episodic recall of last rewarded action, modulated by age and load, plus lapse.

    The agent blends:
      - Model-free RL (softmax over Q-values).
      - An episodic component that, with some probability, recalls the last rewarded action
        for the current state and chooses it deterministically.

    The recall probability is higher for smaller set sizes and reduced in older age.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary rewards (0 or 1) on each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define age group.
    model_parameters : tuple/list of floats
        (alpha, beta, mem_base, mem_size_slope, mem_age_penalty, lapse)
        - alpha: RL learning rate (sigmoid to 0..1)
        - beta: inverse temperature for RL (scaled internally)
        - mem_base: baseline log-odds of episodic recall
        - mem_size_slope: effect of set size on recall (positive means more recall for smaller sets)
        - mem_age_penalty: reduction in recall for older group (added to log-odds)
        - lapse: lapse rate mixing final policy with uniform (sigmoid to 0..0.5)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, mem_base, mem_size_slope, mem_age_penalty, lapse = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    alpha = sigmoid(alpha)
    beta = sigmoid(beta) * 12.0
    lapse = sigmoid(lapse) * 0.5

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        # Episodic memory: store last rewarded action per state; -1 if none yet.
        epi_action = -np.ones(nS, dtype=int)
        epi_available = np.zeros(nS, dtype=bool)

        # Compute recall probability for this block (can be viewed as gate)
        # Higher for smaller set sizes; reduced in older age.
        size_term = mem_size_slope * (3.5 - nS)  # + for nS=3, - for nS=6 if slope>0
        logit_recall = mem_base + size_term - mem_age_penalty * older
        p_recall = sigmoid(logit_recall)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            prefs = Q[s, :]
            prefs_centered = prefs - np.max(prefs)
            exp_p = np.exp(beta * prefs_centered)
            pi_rl = exp_p / (np.sum(exp_p) + eps)

            # Episodic policy
            if epi_available[s]:
                onehot = np.zeros(nA)
                onehot[epi_action[s]] = 1.0
                pi = p_recall * onehot + (1.0 - p_recall) * pi_rl
            else:
                pi = pi_rl

            # Lapse mixture
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Episodic memory update: only store on rewarded trials
            if r > 0.0:
                epi_action[s] = a
                epi_available[s] = True

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL combined with a win-stay/lose-shift (WSLS) heuristic whose weight depends on age and load,
    and with adaptive inverse temperature based on running reward rate (meta-control).

    - RL: softmax over Q-values with beta adapted by recent reward rate.
    - WSLS: if the current state was seen previously in the block:
        * after reward=1, repeat last action with prob 1;
        * after reward=0, select uniformly among the two other actions.
      If unseen, WSLS is uniform.
    - Mixing weight w_ws depends on age group and set size.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..2).
    rewards : array-like of float/int
        Binary rewards (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block of each trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to define age group.
    model_parameters : tuple/list of floats
        (alpha, beta0, ws_base, ws_age_slope, ws_size_slope, eta_beta)
        - alpha: RL learning rate (sigmoid to 0..1)
        - beta0: baseline inverse temperature before adaptation
        - ws_base: baseline log-odds for WSLS weight
        - ws_age_slope: increase in WSLS weight for older group (added to log-odds)
        - ws_size_slope: reduction in WSLS weight for larger set size (multiplied by indicator of nS==6)
        - eta_beta: sensitivity of beta to deviations of reward rate from 0.5

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, ws_base, ws_age_slope, ws_size_slope, eta_beta = model_parameters

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))
    def softplus(x):
        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)

    alpha = sigmoid(alpha)
    # baseline beta is positive via softplus; then scaled moderately
    beta0 = softplus(beta0) * 8.0
    eta_beta = eta_beta  # can be positive or negative; used directly

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Track last action and last reward for WSLS per state
        last_act = -np.ones(nS, dtype=int)
        last_rew = -np.ones(nS, dtype=int)  # -1 means unseen; 0/1 otherwise

        # Mixing weight for WSLS depends on age and load
        logit_w = ws_base + ws_age_slope * older - ws_size_slope * (1 if nS == 6 else 0)
        w_ws = sigmoid(logit_w)

        # Running reward rate for beta adaptation
        rbar = 0.5  # initialize to neutral
        rbar_alpha = 0.2  # fixed smoothing for reward rate

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Adaptive beta based on recent reward rate
            beta_t = softplus(beta0 + eta_beta * (rbar - 0.5))

            # RL policy
            prefs = Q[s, :]
            prefs_centered = prefs - np.max(prefs)
            exp_p = np.exp(beta_t * prefs_centered)
            pi_rl = exp_p / (np.sum(exp_p) + eps)

            # WSLS policy for this state
            if last_act[s] == -1 or last_rew[s] == -1:
                pi_ws = np.ones(nA) / nA
            else:
                pi_ws = np.zeros(nA)
                if last_rew[s] >= 1:
                    # Win-stay: repeat last action
                    pi_ws[last_act[s]] = 1.0
                else:
                    # Lose-shift: choose any of the other two actions uniformly
                    others = [aa for aa in range(nA) if aa != last_act[s]]
                    pi_ws[others[0]] = 0.5
                    pi_ws[others[1]] = 0.5

            # Mixture
            pi = w_ws * pi_ws + (1.0 - w_ws) * pi_rl

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WSLS memory
            last_act[s] = a
            last_rew[s] = int(r)

            # Update running reward rate
            rbar = (1.0 - rbar_alpha) * rbar + rbar_alpha * r

    return nll