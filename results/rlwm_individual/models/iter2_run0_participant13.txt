def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with asymmetric learning rates, WM decay, and action perseveration (stickiness).
    
    This model blends RL with a WM store. RL uses separate learning rates for positive and negative
    prediction errors. WM stores recent correct actions with decay. A state-independent perseveration
    bias (stickiness) biases the RL policy toward repeating the last chosen action within a block.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr_pos : float
            RL learning rate for positive prediction errors (0-1).
        lr_neg : float
            RL learning rate for negative prediction errors (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        stickiness : float
            Choice perseveration bias added to the value of the previous action (in value units).
        wm_decay : float
            WM decay rate per trial within a block (0-1), pulling WM rows toward uniform.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, stickiness, wm_decay = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        prev_action = None  # for stickiness

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Apply stickiness bias to RL values
            if prev_action is not None:
                Q_s[prev_action] += stickiness

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            # Modulate WM weight slightly by set size (heavier WM impact in small sets)
            size_mod = min(1.0, 3.0 / max(1.0, nS))
            eff_w = np.clip(wm_weight * size_mod, 0.0, 1.0)
            p_total = p_wm * eff_w + (1.0 - eff_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s, a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global WM decay toward uniform
            if wm_decay > 0:
                w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Reward-dependent WM reinforcement (one-shot for rewards, partial for non-rewards)
            if r > 0.0:
                # Make row peaked at chosen action
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Partial push toward chosen action while preserving normalization
                alpha_neg = 0.2
                w[s, :] = (1.0 - alpha_neg) * w[s, :]
                w[s, a] += alpha_neg
                # Normalize
                w[s, :] /= np.sum(w[s, :])

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM (slot model) + lapse.
    
    WM stores up to K states with one-shot, near-deterministic associations after reward.
    When capacity is exceeded, the oldest stored state is dropped. RL learns in parallel.
    A small lapse rate mixes in a uniform choice probability, capturing attentional lapses.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        capacity_K : float
            WM capacity in number of states; effective capacity is clipped to [1, nS].
        lapse : float
            State-independent lapse rate mixing in uniform choice probability (0-1).
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, capacity_K, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])
        K_eff = int(np.clip(np.round(capacity_K), 1, nS))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Book-keeping for WM storage order (FIFO)
        stored_flags = np.zeros(nS, dtype=bool)
        fifo_order = []  # list of state indices in order of storage

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with set-size modulation (WM more helpful at small set sizes)
            size_mod = min(1.0, K_eff / max(1.0, nS))
            eff_w = np.clip(wm_weight * size_mod, 0.0, 1.0)
            p_mix = eff_w * p_wm + (1.0 - eff_w) * p_rl

            # Lapse
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Store deterministic association after reward, with capacity
            if r > 0.0:
                # If state not stored, add to FIFO, evict if over capacity
                if not stored_flags[s]:
                    fifo_order.append(s)
                    stored_flags[s] = True
                    if len(fifo_order) > K_eff:
                        evict = fifo_order.pop(0)
                        stored_flags[evict] = False
                        w[evict, :] = w_0[evict, :].copy()
                # Overwrite WM row to peak at chosen action
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Non-reward: small drift toward chosen action if stored; no new storage
                if stored_flags[s]:
                    alpha_neg = 0.2
                    w[s, :] = (1.0 - alpha_neg) * w[s, :]
                    w[s, a] += alpha_neg
                    w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-based meta-learning of learning rate + noisy WM recall scaled by load.
    
    RL learning rate is adapted on each trial based on unsigned prediction error (surprise)
    such that larger surprises transiently increase the effective learning rate. WM stores
    rewarded associations as a peaked but noisy distribution whose noise increases with set size.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            Baseline RL learning rate (0-1).
        wm_weight : float
            Base mixture weight on WM (0-1).
        softmax_beta : float
            Inverse temperature for RL softmax (scaled by 10 internally).
        eta_meta : float
            Gain on surprise-driven adaptation of learning rate (>=0). Effective lr is clipped to [0,1].
        wm_conf_noise : float
            Base WM noise level controlling how much mass leaks from the stored action; increases with set size.
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, eta_meta, wm_conf_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-dependent WM influence: more noise/less weight at larger set sizes
            load_scale = 3.0 / max(3.0, float(nS))  # 1.0 at nS<=3, drops toward 0.5 at nS=6
            eff_w = np.clip(wm_weight * load_scale, 0.0, 1.0)

            p_total = eff_w * p_wm + (1.0 - eff_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with surprise-based meta-learning of lr
            pe = r - Q_s[a]
            lr_eff = np.clip(lr + eta_meta * abs(pe), 0.0, 1.0)
            q[s, a] += lr_eff * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Passive small decay toward uniform
            decay = 0.05
            w = (1.0 - decay) * w + decay * w_0

            if r > 0.0:
                # Store a noisy but peaked distribution centered on chosen action.
                # Noise increases with set size via wm_conf_noise.
                # Compute error mass epsilon in [0, 1), proportional to load.
                eps_load = np.clip(wm_conf_noise * (nS - 1) / nS, 0.0, 0.9)
                w[s, :] = eps_load / (nA - 1)
                w[s, a] = 1.0 - eps_load
            else:
                # On non-reward, slightly reduce confidence on current choice
                gamma = 0.2
                w[s, a] = (1.0 - gamma) * w[s, a]
                # Re-normalize row
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p