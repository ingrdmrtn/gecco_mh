def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with per-state perseveration bias.

    Mechanism:
    - RL: standard delta-rule learning and softmax choice.
    - WM: a fast store that decays toward uniform but is capacity-limited:
      its precision is scaled by min(1, K/nS), so WM is sharper in small set sizes.
    - Perseveration: within a state, the WM policy is biased toward repeating the
      last chosen action (bias added to WM preferences), capturing a habit-like tendency
      that can aid performance when the WM trace is strong.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, K, decay_lambda, pers_bias)
        - lr: RL learning rate in [0,1].
        - wm_weight: base mixture weight for WM vs RL in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - K: WM capacity (effective number of items that can be sharply represented).
        - decay_lambda: WM update/decay rate toward either uniform (when no reward) or
          toward the rewarded one-hot (when reward), in [0,1].
        - pers_bias: additive WM bias for repeating the last action in the same state (can be +/-).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K, decay_lambda, pers_bias = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration bias
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with capacity-limited precision and perseveration bias
            cap_scale = min(1.0, float(K) / float(nS + 1e-12))
            beta_wm_eff = softmax_beta_wm * cap_scale

            W_eff = W_s.copy()
            if last_action[s] >= 0:
                W_eff[last_action[s]] += pers_bias

            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_eff - W_eff[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update with decay and reward-conditional target
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            target = (1.0 - r) * w_0[s, :] + r * one_hot
            w[s, :] = (1.0 - decay_lambda) * w[s, :] + decay_lambda * target

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with dynamic, set-size and novelty gated mixture.

    Mechanism:
    - RL: delta-rule with separate learning rates for positive and negative outcomes.
    - WM: near-deterministic store. The WM mixture weight is not fixed; it is dynamically
      modulated by set size (smaller sets -> higher WM reliance) and by state novelty
      (earlier visits -> more WM reliance), via a logistic transform around wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        (lr_pos, lr_neg, wm_weight, softmax_beta, ss_slope, novelty_slope)
        - lr_pos: RL learning rate after reward=1 in [0,1].
        - lr_neg: RL learning rate after reward=0 in [0,1].
        - wm_weight: base WM mixture weight in [0,1] (converted to logit internally).
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - ss_slope: slope for the effect of 1/nS on the WM mixture (positive -> more WM in small sets).
        - novelty_slope: slope for the effect of novelty on the WM mixture (positive -> more WM early on).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, ss_slope, novelty_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track visits per state to quantify novelty (higher novelty early on)
        visits = np.zeros(nS)

        # Precompute base logit of wm_weight
        wm_weight = np.clip(wm_weight, 1e-6, 1 - 1e-6)
        base_logit = np.log(wm_weight / (1.0 - wm_weight))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Dynamic WM mixture weight: depends on set size and novelty
            inv_set = 1.0 / float(nS)
            novelty = 1.0 / (1.0 + visits[s])  # 1 on first visit, decays thereafter
            mix_logit = base_logit + ss_slope * inv_set + novelty_slope * novelty
            wm_eff = 1.0 / (1.0 + np.exp(-mix_logit))

            # WM policy (precision fixed, but mixture is dynamic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with dynamic weight
            p_total = p_wm * wm_eff + (1.0 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            lr = lr_pos if r > 0.0 else lr_neg
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: load-dependent refresh; reward strengthens, else drifts to uniform
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            lambda_wm = 1.0 / float(nS)  # heavier decay under larger sets
            target = (1.0 - r) * w_0[s, :] + r * one_hot
            w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * target

            # Update novelty tracker
            visits[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice stickiness + WM precision reduced by load with controlled maintenance.

    Mechanism:
    - RL: delta-rule learning with a choice stickiness term added to Q for the last
      action chosen in the current state, promoting short-run consistency.
    - WM: fast, near-deterministic store that decays toward uniform at rate wm_decay.
      Its effective precision is reduced under higher load via a hyperbolic load factor:
      beta_wm_eff = beta_wm / (1 + wm_alpha*(nS-1)).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_alpha, wm_decay, stickiness)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM vs RL in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - wm_alpha: load-sensitivity controlling how WM precision drops with set size (>=0).
        - wm_decay: WM maintenance/decay rate toward target (uniform if no reward, one-hot if rewarded) in [0,1].
        - stickiness: additive bias applied to the last chosen action for that state in the RL policy.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, wm_decay, stickiness = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # RL policy with stickiness
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-dependent precision
            load_factor = 1.0 + wm_alpha * max(0.0, float(nS) - 1.0)
            beta_wm_eff = softmax_beta_wm / load_factor
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update with controlled maintenance
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            target = (1.0 - r) * w_0[s, :] + r * one_hot
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p