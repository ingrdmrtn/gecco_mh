def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + capacity-gated working memory with interference-based decay, age-dependent WM gating.
    
    Mechanism
    - A model-free Q-learner runs within blocks.
    - A simple working-memory (WM) store holds a single candidate action per state with a strength in [0,1].
      The WM item passively decays each trial due to interference that grows with set size and with age.
    - WM gating (probability of storing/refreshing a state-action after each trial) depends on set size and age.
    - Policy is a mixture: p = w_s * p_WM + (1 - w_s) * p_RL, where w_s is the current WM strength for the active state.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State indices (0..nS-1) for each trial (within-block).
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Reward on each trial (0 or 1).
    blocks : array-like, shape (T,)
        Block index per trial. Learning resets between blocks.
    set_sizes : array-like, shape (T,)
        Set size (3 or 6) for each trial; constant within a block.
    age : array-like or scalar
        Participant age. Older group is age >= 45.
    model_parameters : array-like, length 6
        alpha       : RL learning rate in [0,1]
        beta        : Inverse temperature for RL softmax (>0); internally scaled
        g0          : WM gating baseline (real), logistic mapped to [0,1]
        g_ss        : WM gating set-size slope (real): larger set size reduces gating if negative
        g_age       : WM gating age penalty (>=0 increases penalty for older adults)
        decay_wm    : Base WM decay rate per trial in [0,1]; will scale with set size and age
    
    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, g0, g_ss, g_age, decay_wm = model_parameters
    beta = 5.0 * max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize RL values and WM store per block
        Q = np.zeros((nS, nA))
        wm_action = -1 * np.ones(nS, dtype=int)      # stored action for each state (-1 means empty)
        wm_strength = np.zeros(nS, dtype=float)       # strength in [0,1]

        # Interference-scaled decay (more decay with larger set, and if older)
        interference = (nS / 6.0) * (1.0 + 0.5 * is_older)
        decay_eff = np.clip(decay_wm * interference, 0.0, 1.0)

        # WM gating probability depends on set size and age
        # Larger nS lowers gating if g_ss < 0; older lowers gating via g_age
        gate_p = sigmoid(g0 + g_ss * (3.0 - nS) - g_age * is_older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Passive WM decay due to interference (applies to all states each trial)
            if decay_eff > 0:
                wm_strength *= (1.0 - decay_eff)
                # Drop items that are extremely weak
                wm_action[wm_strength < 1e-6] = -1
                wm_strength[wm_strength < 1e-6] = 0.0

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy for this state
            if wm_action[s] >= 0 and wm_strength[s] > 0:
                p_wm = np.full(nA, eps)
                p_wm[wm_action[s]] = 1.0
                p_wm = p_wm / np.sum(p_wm)
            else:
                p_wm = np.ones(nA) / nA

            # Mixture weight equals current WM strength (bounded [0,1])
            w_mix = float(np.clip(wm_strength[s], 0.0, 1.0))
            p_total = w_mix * p_wm + (1.0 - w_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            p_total = p_total / np.sum(p_total)

            total_logp += np.log(p_total[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: with probability gate_p, store/refresh
            # If rewarded, store chosen action strongly (set to 1.0).
            # If not rewarded, we still encode but with weaker strength (half of previous).
            if gate_p > 0:
                # Use a Bernoulli gate; as likelihood we can't sample, so use expectation update:
                # deterministically apply the expected effect of gating.
                # Expected new strength = (1-gate_p)*old + gate_p*target_strength
                target_strength = 1.0 if r > 0.5 else 0.5 * wm_strength[s]
                wm_strength[s] = (1.0 - gate_p) * wm_strength[s] + gate_p * target_strength
                # Update stored action when we have positive feedback, else keep if stronger than zero
                if r > 0.5:
                    wm_action[s] = a
                else:
                    # If no stored action yet, we may still save the attempted action with small probability mass
                    if wm_action[s] < 0 and wm_strength[s] > 0:
                        wm_action[s] = a

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Q-learning with state-wise choice kernel (habit) and set-size/age-modulated kernel reliance.
    
    Mechanism
    - Model-free Q-learning with softmax choice.
    - A separate choice kernel K_s,a captures recency/frequency of choosing actions in each state.
      It updates with its own learning rate eta_k.
    - Decision preference is a weighted sum of Q and K: pref = beta_q * Q + beta_k_eff * K.
    - Kernel influence increases with set size (when ss_scale > 0) and with age (age_bias > 0).
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State indices per trial.
    actions : array-like, shape (T,)
        Chosen action per trial (0..2).
    rewards : array-like, shape (T,)
        Binary reward per trial (0/1).
    blocks : array-like, shape (T,)
        Block index per trial; learning resets between blocks.
    set_sizes : array-like, shape (T,)
        Set size on each trial (3 or 6), constant within block.
    age : array-like or scalar
        Participant age; older if age >= 45.
    model_parameters : array-like, length 6
        alpha     : Q-learning rate in [0,1]
        beta_q    : Inverse temperature for values (>0), internally scaled
        eta_k     : Choice-kernel learning rate in [0,1]
        beta_k    : Kernel inverse temperature weight (>0), internally scaled
        age_bias  : Multiplier (>=0) of kernel weight for older adults
        ss_scale  : Exponent controlling set-size scaling of kernel reliance (real)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    nA = 3
    eps = 1e-12

    alpha, beta_q, eta_k, beta_k, age_bias, ss_scale = model_parameters
    beta_q = 5.0 * max(beta_q, eps)
    beta_k = 5.0 * max(beta_k, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize values and choice kernel
        Q = np.zeros((nS, nA))
        K = np.zeros((nS, nA))  # starts neutral

        # Kernel scaling with set size and age
        # Larger sets or older age -> greater reliance on kernel if parameters positive
        ss_factor = (nS / 3.0) ** ss_scale if ss_scale != 0 else 1.0
        beta_k_eff = beta_k * ss_factor * (1.0 + age_bias * is_older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute preferences and choice probabilities
            prefs = beta_q * (Q[s, :] - np.max(Q[s, :])) + beta_k_eff * (K[s, :] - np.max(K[s, :]))
            # For numerical stability, subtract max again
            prefs = prefs - np.max(prefs)
            p = np.exp(prefs)
            p = p / (np.sum(p) + eps)
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Choice kernel update (state-wise, soft one-hot with learning rate)
            # Move chosen action toward 1, others toward 0
            K[s, :] = (1.0 - eta_k) * K[s, :]
            K[s, a] += eta_k * (1.0 - K[s, a])

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Hybrid Bayesian-Delta learner with age- and set-size-dependent arbitration.
    
    Mechanism
    - Maintain Beta-Bernoulli counts per state-action: successes S and counts N.
      Bayesian expected value V_hat = S / (N + eps).
    - In parallel, maintain a delta-rule Q with learning rate alpha and reward sensitivity rho.
    - Action selection uses a softmax over the combined value:
        V_mix = w * V_hat + (1 - w) * Q
      where arbitration weight w depends on set size and age via a logistic function.
    - Older adults and larger set sizes reduce reliance on Bayesian counts if k_age > 0 and k_ss > 0.
    
    Parameters
    ----------
    states : array-like, shape (T,)
        State indices per trial.
    actions : array-like, shape (T,)
        Chosen actions (0..2).
    rewards : array-like, shape (T,)
        Binary rewards (0/1).
    blocks : array-like, shape (T,)
        Block index per trial (resets).
    set_sizes : array-like, shape (T,)
        Set size (3 or 6), constant within block.
    age : array-like or scalar
        Participant age; older if age >= 45.
    model_parameters : array-like, length 6
        alpha   : Delta-rule learning rate for Q in [0,1]
        beta    : Inverse temperature for softmax (>0), internally scaled
        k0      : Arbitration intercept (real), logistic -> baseline reliance on V_hat
        k_ss    : Set-size slope for arbitration (real): larger set size decreases w if positive
        k_age   : Age penalty (>=0) on arbitration weight for older adults
        rho     : Reward sensitivity scaling for the delta update (>0)
    
    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, k0, k_ss, k_age, rho = model_parameters
    beta = 5.0 * max(beta, eps)
    rho = max(rho, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize Bayesian counts and Q-values
        Suc = np.zeros((nS, nA))  # successes
        Cnt = np.zeros((nS, nA))  # total counts
        Q = np.zeros((nS, nA))

        # Arbitration weight w depends on set size and age (constant within a block)
        w = sigmoid(k0 - k_ss * (nS - 3.0) - k_age * is_older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute Bayesian expectation and mixed value
            V_hat = Suc[s, :] / (Cnt[s, :] + eps)
            V_mix = w * V_hat + (1.0 - w) * Q[s, :]

            # Softmax policy
            Vc = V_mix - np.max(V_mix)
            p = np.exp(beta * Vc)
            p = p / (np.sum(p) + eps)
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # Update Bayesian counts
            Cnt[s, a] += 1.0
            Suc[s, a] += r

            # Delta-rule Q update with reward sensitivity rho
            pe = (rho * r) - Q[s, a]
            Q[s, a] += alpha * pe

    return -float(total_logp)