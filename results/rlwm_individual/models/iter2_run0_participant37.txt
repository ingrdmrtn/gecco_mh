def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with RL-uncertainty gating and set-size attenuation.

    Model idea:
    - RL system learns Q-values with a single learning rate.
    - WM system stores the last rewarded action for a state as a sharp distribution and otherwise
      decays toward uniform with rate wm_decay.
    - Arbitration weight for WM increases when RL is uncertain (low value spread) and decreases
      with set size (capacity limit).
    
    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values.
    - wm_base: scalar in [0,1]. Baseline WM mixture weight.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - wm_decay: scalar in [0,1]. Decay of WM towards uniform on non-rewarded trials.
    - gate_temp: scalar > 0. Sharpness of uncertainty-based gating (sigmoid temperature).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, wm_decay, gate_temp = model_parameters
    softmax_beta *= 10.0  # as per template
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL choice probability for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Policy for the Working Memory (WM)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty of RL: lower spread => higher WM weight
            spread = np.max(Q_s) - np.mean(Q_s)  # in [0,1] roughly
            # Map spread to [0,1] where 1 means very uncertain (low spread)
            uncert = 1.0 / (1.0 + np.exp(gate_temp * (spread - 0.25)))  # sigmoid around 0.25
            # Set-size attenuation (3/nS) reduces WM contribution in larger sets
            wm_w = wm_base * (3.0 / max(1.0, nS)) * uncert
            wm_w = max(0.0, min(1.0, wm_w))

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating
            if r >= 1.0 - 1e-12:
                # Store a sharp memory of the rewarded action
                w[s, :] = (1.0 / (nA - 1)) * 0.0  # start with zeros
                w[s, :] = w_0[s, :] * 0.0  # ensure zeros
                w[s, :] = 1e-12  # avoid zeros
                w[s, :] = w[s, :] / np.sum(w[s, :])  # normalize tiny
                w[s, :] = w_0[s, :]  # reset to uniform
                w[s, :] = w_0[s, :] * 0.0 + 1e-12
                # finally set sharp peak
                w[s, :] = (1.0 / nA) * np.ones(nA) * 0.0 + 1e-12
                w[s, a] = 1.0 - 1e-6
                # renormalize
                w[s, :] = w[s, :] / np.sum(w[s, :])
            else:
                # Decay toward uniform when no reward
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Dual learning-rate RL + probabilistic WM storage with capacity-shaped gating.

    Model idea:
    - RL system updates Q-values with separate learning rates for positive and negative PEs.
    - WM system represents, per state, a probability distribution over actions that is updated
      with a simple delta rule with asymmetric sensitivity controlled by wm_noise.
    - Arbitration uses a logistic capacity gating based on set size: WM weight decreases with
      larger set size according to a slope parameter, scaled by a base strength.

    Parameters (model_parameters):
    - lr_pos: scalar in [0,1]. RL learning rate for positive prediction errors.
    - lr_neg: scalar in [0,1]. RL learning rate for negative prediction errors.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - wm_strength: scalar in [0,1]. Baseline strength of WM in the mixture.
    - wm_noise: scalar in [0,1]. Controls WM update asymmetry (lower = more sensitive to rewards).
    - cap_slope: scalar > 0. Steepness of the logistic gating vs. set size.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_strength, wm_noise, cap_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-shaped gating as a function of set size
        # Larger nS -> smaller WM weight; midpoint ~ 4
        base_gate = wm_strength * (1.0 / (1.0 + np.exp(cap_slope * (nS - 4.0))))
        base_gate = max(0.0, min(1.0, base_gate))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_w = base_gate  # constant within block (depends on set size)
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # WM updating with asymmetric sensitivity controlled by wm_noise
            # Rewarded: increase probability on chosen action more strongly when wm_noise is low
            if r >= 1.0 - 1e-12:
                alpha_w = max(1e-6, 1.0 - wm_noise)
                # Move mass toward the chosen action
                w[s, a] += alpha_w * (1.0 - w[s, a])
                # Slightly reduce others to keep normalized
                others = [i for i in range(nA) if i != a]
                mass_to_remove = alpha_w * np.sum(w[s, others])
                w[s, others] = w[s, others] * (1.0 - alpha_w)
                # Renormalize defensively
                w[s, :] = np.clip(w[s, :], 1e-9, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                # Non-reward: decrease probability on chosen action with smaller step when wm_noise is high
                alpha_w = max(1e-6, wm_noise)
                w[s, a] += -alpha_w * w[s, a]
                # redistribute to others uniformly
                redistribute = alpha_w * w[s, a] if w[s, a] > 0 else 0.0
                others = [i for i in range(nA) if i != a]
                w[s, others] += redistribute / (nA - 1) if (nA - 1) > 0 else 0.0
                # Soft pull toward uniform for stability
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
                w[s, :] = np.clip(w[s, :], 1e-9, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM arbitration by reliability with interference and action stickiness.

    Model idea:
    - RL system updates Q-values with a single learning rate.
    - WM system stores per-state action distributions; rewarded trials make WM sharp,
      non-rewarded trials induce decay plus interference proportional to set size.
    - Arbitration uses relative reliability estimated from recent errors (exponentially
      weighted traces) for RL and WM, scaled by a base weight and set size.
    - Action stickiness biases both RL and WM policies toward repeating the last action
      in that state.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - wm_weight_base: scalar in [0,1]. Baseline WM weight before reliability and set size scaling.
    - wm_interference: scalar in [0,1]. Degree of WM interference/decay with larger set sizes.
    - stickiness: scalar >= 0. Additive bias for repeating last action.
    - pe_decay: scalar in (0,1]. Decay for error traces (higher = faster adaptation).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_weight_base, wm_interference, stickiness, pe_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Reliability traces per state
        e_rl = np.ones(nS) * 0.5
        e_wm = np.ones(nS) * 0.5

        # Last action per state for stickiness (-1 = none)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Stickiness bias vector
            bias = np.zeros(3)
            if last_action[s] >= 0:
                bias[last_action[s]] += stickiness

            # RL policy with stickiness
            Q_s = q[s, :] + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with stickiness and interference-softened W_s
            # Apply interference: pull W_s toward uniform proportional to set size
            gamma = wm_interference * max(0, nS - 1) / max(1, nS)
            W_s_raw = w[s, :]
            W_s = (1.0 - gamma) * W_s_raw + gamma * w_0[s, :]
            W_s = W_s + bias  # add stickiness in WM space
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Reliability-based arbitration with set size scaling
            rel_wm = 1.0 / (1.0 + e_wm[s])
            rel_rl = 1.0 / (1.0 + e_rl[s])
            rel_sum = rel_wm + rel_rl
            rel_frac_wm = rel_wm / rel_sum if rel_sum > 0 else 0.5
            wm_w = wm_weight_base * (3.0 / max(1.0, nS)) * rel_frac_wm
            wm_w = max(0.0, min(1.0, wm_w))

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - (q[s, a])
            q[s, a] += lr * delta

            # Update reliability traces
            e_rl[s] = (1.0 - pe_decay) * e_rl[s] + pe_decay * abs(delta)
            # WM error proxy: mismatch between WM belief and outcome on chosen action
            wm_err = (1.0 - w[s, a]) if r >= 1.0 - 1e-12 else w[s, a]
            e_wm[s] = (1.0 - pe_decay) * e_wm[s] + pe_decay * wm_err

            # WM updating
            if r >= 1.0 - 1e-12:
                # Encode rewarded action as sharp distribution
                w[s, :] = (1.0 / nA) * np.ones(nA) * 0.0 + 1e-12
                w[s, a] = 1.0 - 1e-6
                w[s, :] /= np.sum(w[s, :])
            else:
                # Interference-driven decay toward uniform
                w[s, :] = (1.0 - gamma) * w[s, :] + gamma * w_0[s, :]

            # Update last action
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p