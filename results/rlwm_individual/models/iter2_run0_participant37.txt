def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + working-memory gating with age- and load-dependent WM weighting, WM decay, and action stickiness.

    Core idea:
    - A standard model-free RL (Q-learning) system learns action values per state.
    - A short-lived working-memory (WM) store keeps a decaying one-hot trace of the most recently rewarded action per state.
    - Policy is a softmax over a combined logit: beta*Q plus a WM logit scaled by a gate that depends on set size and age.
    - An action stickiness term biases repeating the previous action.
    
    Parameters (model_parameters; all used):
    - alpha: RL learning rate in [0,1]
    - beta: Inverse temperature (>0), internally scaled (x10)
    - wm_strength: Baseline WM influence on policy (>0)
    - wm_decay: Per-trial WM decay factor in [0,1]
    - stickiness: Bias (logit units) for repeating the previous action (can be +/-)
    - age_gate_bias: Multiplicative factor applied to WM strength for older adults (>45)
    
    Inputs:
    - states: 1D array of state indices within block (0..set_size-1)
    - actions: 1D array of chosen actions (0..2 nominally; may contain invalid values)
    - rewards: 1D array of feedback (0/1 nominally; clipped to [0,1] for learning)
    - blocks: 1D array of block indices
    - set_sizes: 1D array of set size on each trial
    - age: 1D array/list with a single number (participant's age)
    - model_parameters: list/tuple described above
    
    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, beta, wm_strength, wm_decay, stickiness, age_gate_bias = model_parameters
    alpha = min(max(alpha, 0.0), 1.0)
    beta = max(1e-6, beta) * 10.0
    wm_strength = max(0.0, wm_strength)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    stickiness = float(stickiness)
    age_gate_bias = max(0.0, age_gate_bias)

    older = 1 if age[0] > 45 else 0
    nA = 3
    total_loglik = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        WM = np.zeros((nS, nA))  # decaying one-hot of last rewarded action
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = 1.0 if block_rewards[t] > 0.5 else 0.0
            ss = float(block_set_sizes[t])

            # Decay WM globally per trial
            WM *= wm_decay

            # Compute logits: RL + WM + stickiness
            # Load gating: smaller sets increase WM influence; older get multiplicative age_gate_bias
            load_gain = 3.0 / max(1.0, ss)  # 1.0 for 3, 0.5 for 6
            wm_gate = wm_strength * load_gain * (age_gate_bias if older == 1 else 1.0)

            rl_logits = beta * Q[s, :]
            wm_logits = wm_gate * WM[s, :] * beta  # align scale with beta for comparable influence
            stick_logits = np.zeros(nA)
            if prev_action is not None and 0 <= prev_action < nA:
                stick_logits[prev_action] += stickiness

            logits = rl_logits + wm_logits + stick_logits
            m = np.max(logits)
            exps = np.exp(logits - m)
            Z = np.sum(exps)

            if 0 <= a < nA and Z > 0:
                p = exps[a] / max(eps, Z)
            else:
                p = eps  # invalid actions get tiny probability

            total_loglik += np.log(max(p, eps))

            # Learning updates if valid action
            if 0 <= a < nA:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # WM update: on rewarded trials, set a one-hot trace; on non-reward, just leave decay
                if r > 0.5:
                    WM[s, :] = 0.0
                    WM[s, a] = 1.0

                prev_action = a
            else:
                # if invalid, reset prev_action so stickiness doesn't reinforce garbage
                prev_action = None

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with uncertainty-adaptive temperature, age shift, and lapse.

    Core idea:
    - Two learning rates: alpha_pos for positive PEs and alpha_neg for negative PEs.
    - Inverse temperature adapts with learning/experience: as a state is visited more, choice becomes more certain.
    - Cognitive load (set size) reduces beta; older adults have an additional beta reduction (age_temp_shift).
    - A small lapse mixes in uniform random choice to account for off-task/invalid responses.

    Parameters (model_parameters; all used):
    - alpha_pos: Learning rate for positive prediction errors [0,1]
    - alpha_neg: Learning rate for negative prediction errors [0,1]
    - beta_base: Baseline inverse temperature (>0), internally scaled (x10)
    - beta_gain: Additional beta gained with experience/visits (>0)
    - age_temp_shift: Beta reduction applied if older adult (>0 reduces beta when older)
    - lapse: Lapse probability in [0,0.5]

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: see task description
    - model_parameters: list/tuple described above

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, beta_base, beta_gain, age_temp_shift, lapse = model_parameters
    alpha_pos = min(max(alpha_pos, 0.0), 1.0)
    alpha_neg = min(max(alpha_neg, 0.0), 1.0)
    beta_base = max(1e-6, beta_base) * 10.0
    beta_gain = max(0.0, beta_gain) * 10.0
    age_temp_shift = max(0.0, age_temp_shift) * 10.0
    lapse = min(max(lapse, 0.0), 0.5)

    older = 1 if age[0] > 45 else 0
    nA = 3
    total_loglik = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        visits = np.zeros(nS)  # counts per state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = 1.0 if block_rewards[t] > 0.5 else 0.0
            ss = float(block_set_sizes[t])

            # Update experience count before computing beta to reflect current knowledge
            visits[s] += 1.0
            exp_factor = 1.0 - np.exp(-visits[s])  # 0..~1 as visits increase

            # Load reduces beta; older lowers beta further by age_temp_shift
            beta_t = (beta_base + beta_gain * exp_factor) * (3.0 / max(1.0, ss))
            if older == 1:
                beta_t = max(1e-6, beta_t - age_temp_shift)

            logits = beta_t * Q[s, :]
            m = np.max(logits)
            exps = np.exp(logits - m)
            Z = np.sum(exps)

            if 0 <= a < nA and Z > 0:
                p_choice = exps[a] / max(eps, Z)
                p = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            else:
                p = max(eps, lapse * (1.0 / nA))

            total_loglik += np.log(max(p, eps))

            # Learning update if valid action
            if 0 <= a < nA:
                pe = r - Q[s, a]
                lr = alpha_pos if pe >= 0.0 else alpha_neg
                Q[s, a] += lr * pe

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL mixed with a short-term eligibility-trace expert; mixture weight depends on set size and age.
    
    Core idea:
    - Model-free RL (Q-learning) provides a stable value estimate.
    - A short-term memory (STM) expert uses an eligibility trace E that spikes on recently rewarded actions,
      decaying with lambda_trace; this approximates an episodic/last-reward heuristic.
    - Policy is a convex mixture of softmax over Q and softmax over E.
    - Mixture weight eta_t increases for small set sizes (easier WM) and decreases in older adults.
    - A perseveration bias adds logit to the previously chosen action within the RL channel.

    Parameters (model_parameters; all used):
    - alpha: RL learning rate [0,1]
    - beta: Inverse temperature (>0), scaled x10; used for both channels' softmax
    - lambda_trace: Eligibility trace decay per trial in [0,1]
    - eta_base: Baseline mixture logit (transformed via sigmoid to [0,1])
    - age_eta_slope: Change in mixture logit if older adult (can be +/-)
    - perseveration: Logit bias added to last chosen action in the RL channel (can be +/-)

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: see task description

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha, beta, lambda_trace, eta_base, age_eta_slope, perseveration = model_parameters
    alpha = min(max(alpha, 0.0), 1.0)
    beta = max(1e-6, beta) * 10.0
    lambda_trace = min(max(lambda_trace, 0.0), 1.0)
    eta_base = float(eta_base)
    age_eta_slope = float(age_eta_slope)
    perseveration = float(perseveration)

    older = 1 if age[0] > 45 else 0
    nA = 3
    total_loglik = 0.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))  # eligibility-like STM trace
        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = 1.0 if block_rewards[t] > 0.5 else 0.0
            ss = float(block_set_sizes[t])

            # Decay eligibility
            E *= lambda_trace

            # Mixture weight: base + load + age effects -> sigmoid
            load_logit = (3.0 / max(1.0, ss)) - 1.0  # 0 at set size 3; -0.5 at size 6
            eta_logit = eta_base + load_logit + (age_eta_slope if older == 1 else 0.0)
            eta_t = sigmoid(eta_logit)

            # RL channel logits with perseveration
            rl_logits = beta * Q[s, :].copy()
            if prev_action is not None and 0 <= prev_action < nA:
                rl_logits[prev_action] += perseveration

            m_rl = np.max(rl_logits)
            exps_rl = np.exp(rl_logits - m_rl)
            Z_rl = np.sum(exps_rl)
            p_rl = exps_rl / max(eps, Z_rl)

            # STM expert logits from eligibility trace
            stm_logits = beta * E[s, :]
            m_stm = np.max(stm_logits)
            exps_stm = np.exp(stm_logits - m_stm)
            Z_stm = np.sum(exps_stm)
            p_stm = exps_stm / max(eps, Z_stm)

            # Mixture policy
            p_mix = eta_t * p_stm + (1.0 - eta_t) * p_rl

            if 0 <= a < nA:
                p = p_mix[a]
            else:
                p = eps

            total_loglik += np.log(max(p, eps))

            if 0 <= a < nA:
                # RL update
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # STM update: reward sets a spike on chosen action
                if r > 0.5:
                    E[s, a] = 1.0

                prev_action = a
            else:
                prev_action = None

    return -total_loglik