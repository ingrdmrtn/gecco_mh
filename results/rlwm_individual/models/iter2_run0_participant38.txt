Below are three standalone cognitive models that follow the requested template structure. Each returns the negative log-likelihood of the observed choices and uses all parameters meaningfully. In each model, set size modulates specific mechanisms as described in the docstrings.

Note: Assume numpy is available as np. No imports are included inside the functions.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM with one-shot encoding and set-size-dependent decay +
    mixture weight that depends on set size.

    Rationale:
    - RL system learns action values with separate learning rates for positive vs negative outcomes.
    - WM system acts like a one-shot episodic mapping per state: when rewarded, it stores the chosen
      action as a near-deterministic mapping; when unrewarded, it weakly avoids the chosen action.
    - WM traces decay toward uniform with a decay rate that increases with set size (nS).
    - The arbitration (mixture weight) in favor of WM decreases with set size via a logistic transform.

    Parameters (6 total)
    - model_parameters[0] = lr_pos in [0,1]: RL learning rate for positive outcomes (r=1).
    - model_parameters[1] = lr_neg in [0,1]: RL learning rate for negative outcomes (r=0).
    - model_parameters[2] = softmax_beta > 0: RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_weight_base (real): baseline for WM mixture weight (logit space).
    - model_parameters[4] = wm_weight_slope (real): slope for WM weight vs set size (logit space).
      wm_weight = sigmoid(wm_weight_base + wm_weight_slope*(3 - nS)); favors WM more at nS=3.
    - model_parameters[5] = wm_decay_base in [0,1]: base WM decay, scaled by set size as:
      wm_decay_block = 1 - (1 - wm_decay_base)^nS (more decay for larger nS).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_weight_slope, wm_decay_base = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM mixture weight (constant within block)
        # Larger nS -> smaller wm_weight if wm_weight_slope > 0
        wm_weight_block = 1.0 / (1.0 + np.exp(-(wm_weight_base + wm_weight_slope * (3 - nS))))

        # Set-size-dependent WM decay
        wm_decay_block = 1.0 - (1.0 - wm_decay_base) ** max(nS, 1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM row
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update (asymmetric learning rates)
            delta = r - Q_s[a]
            lr_eff = lr_pos if r > 0.5 else lr_neg
            q[s, a] += lr_eff * delta

            # WM decay toward uniform (set-size dependent)
            w = (1.0 - wm_decay_block) * w + wm_decay_block * w_0

            # WM one-shot encoding:
            if r > 0.5:
                # On reward: store the chosen action as a one-hot mapping for this state
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On no reward: avoid chosen action mildly while preserving normalization
                dec = min(0.5, w[s, a])  # fixed avoidance strength, capped by current prob
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with perseveration bias (choice stickiness) + capacity-limited WM with precision scaling.

    Rationale:
    - RL: standard delta rule with a softmax policy, but choices are biased toward repeating
      the last action in the same state (stickiness).
    - WM: a leaky associative store per state updated by a WM-specific learning rate (wm_prec),
      and its policy uses a high inverse temperature that scales down with set size according to
      a capacity limit.
    - Set size effects:
      - Effective WM precision scales as min(1, K_capacity / nS) times wm_prec_base.
      - The WM policy softmax uses this effective precision (reduced for larger sets).

    Parameters (6 total)
    - model_parameters[0] = lr in [0,1]: RL learning rate.
    - model_parameters[1] = wm_weight in [0,1]: mixture weight for WM.
    - model_parameters[2] = softmax_beta > 0: RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_prec_base in [0,1]: base WM update strength and policy precision scale.
    - model_parameters[4] = K_capacity > 0: WM capacity (number of items with high precision).
    - model_parameters[5] = stickiness >= 0: perseveration bias (adds to logits of last chosen action).

    Returns
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_prec_base, K_capacity, stickiness = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    base_wm_beta = 50   # maximum WM inverse temperature when fully precise

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM precision scaling with capacity
        prec_scale = min(1.0, float(K_capacity) / max(nS, 1))
        wm_prec = wm_prec_base * prec_scale
        softmax_beta_wm = base_wm_beta * prec_scale  # policy becomes less deterministic with larger nS

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Build stickiness bias vector for this state
            bias = np.zeros(3)
            if last_action[s] >= 0:
                bias[last_action[s]] = stickiness

            # RL policy with perseveration added to logits
            Q_s = q[s, :] + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with precision-scaled inverse temperature and perseveration
            W_s = w[s, :] + bias
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM leaky update (toward uniform) plus associative update
            # Leak magnitude proportional to (1 - prec_scale) to reflect load; reuse wm_prec for learning
            leak = (1.0 - prec_scale) * 0.5  # modest leak that increases with set size
            w = (1.0 - leak) * w + leak * w_0

            if r > 0.5:
                # Reward: move WM row toward one-hot on chosen action with rate wm_prec
                w[s, :] = (1.0 - wm_prec) * w[s, :]
                w[s, a] += wm_prec
            else:
                # No reward: move away from chosen action slightly with rate wm_prec/2
                dec = min(wm_prec / 2.0, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce–Hall learning rate modulation + WM persistence that degrades with set size +
    set-size-dependent arbitration.

    Rationale:
    - RL: the learning rate adapts to surprise (absolute prediction error) via a Pearce–Hall rule:
      alpha_t(s) = clip(alpha_base + alpha_gain * |delta_prev(s)|, 0, 1), per state.
    - WM: rewarded actions are stored as near one-hot; between trials, WM decays toward uniform
      with a persistence parameter raised to a power that increases with set size (more items -> more decay).
    - Arbitration: WM weight is a logistic function of set size via 1/nS (higher for smaller sets).

    Parameters (6 total)
    - model_parameters[0] = alpha_base in [0,1]: baseline RL learning rate.
    - model_parameters[1] = alpha_gain in [0,1]: gain on the absolute previous PE (per state).
    - model_parameters[2] = softmax_beta > 0: RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_w0 (real): baseline for WM weight (logit space).
    - model_parameters[4] = wm_w1 (real): slope on 1/nS for WM weight (logit space).
      wm_weight = sigmoid(wm_w0 + wm_w1 * (1.0 / nS)).
    - model_parameters[5] = wm_persist in [0,1]: per-item WM persistence; effective block-level
      persistence = wm_persist^(nS-1), so larger sets yield stronger decay.

    Returns
    - Negative log-likelihood of observed choices.
    """
    alpha_base, alpha_gain, softmax_beta, wm_w0, wm_w1, wm_persist = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # deterministic WM policy

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state previous absolute PE (for Pearce–Hall modulation)
        prev_abs_pe = np.zeros(nS)

        # Set-size-dependent WM weight and persistence
        wm_weight_block = 1.0 / (1.0 + np.exp(-(wm_w0 + wm_w1 * (1.0 / max(nS, 1)))))
        eff_persist = wm_persist ** max(nS - 1, 0)  # 1 when nS=1, decreases with nS>1
        wm_decay_block = 1.0 - eff_persist          # decay toward uniform

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with Pearce–Hall adaptive learning rate
            delta = r - q[s, a]
            alpha_t = np.clip(alpha_base + alpha_gain * prev_abs_pe[s], 0.0, 1.0)
            q[s, a] += alpha_t * delta
            prev_abs_pe[s] = abs(delta)

            # WM decay toward uniform with set-size-dependent persistence
            w = (1.0 - wm_decay_block) * w + wm_decay_block * w_0

            # WM encoding: reinforced one-shot update on reward; minimal change on no-reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Mild avoidance without adding parameters
                dec = min(0.25, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

        blocks_log_p += log_p

    return -blocks_log_p