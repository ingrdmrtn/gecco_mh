def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with RL decay and load-dependent WM interference.

    Mechanisms
    - RL: delta rule with post-update decay toward a uniform prior (forgetting).
    - WM: stores rewarded actions but suffers load-dependent interference that
      blends the stored one-hot trace with uniform as set size increases.
    - Arbitration: fixed WM weight (wm_weight) blending RL and WM policies.
    - Precision: WM softmax precision declines with set size via wm_beta_bias.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight : Mixture weight for WM in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        rl_decay : Post-update decay toward uniform for visited state in [0,1].
        wm_interference : Base WM interference in [0,1]; increases with set size.
        wm_beta_bias : Positive value controlling WM precision loss with set size.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rl_decay, wm_interference, wm_beta_bias = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # base deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM temperature and interference scale with set size
        beta_wm_eff = softmax_beta_wm / (1.0 + max(0.0, wm_beta_bias) * max(0, nS - 3))
        wm_int_eff = 1.0 - (1.0 - np.clip(wm_interference, 0.0, 1.0)) * (3.0 / max(3.0, float(nS)))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with decay to uniform
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * w_0[s, :]

            # WM update with load-dependent interference
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_int_eff) * onehot + wm_int_eff * w_0[s, :]
            else:
                # If not rewarded, drift toward uniform (forget)
                w[s, :] = (1.0 - wm_int_eff) * w[s, :] + wm_int_eff * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with surprise-based arbitration and state-wise stickiness.

    Mechanisms
    - RL: delta rule softmax plus state-wise stickiness (repeat last action bias).
    - WM: stores rewarded action deterministically (one-hot), otherwise decays.
    - Arbitration: dynamic WM weight based on RL surprise |r - Q(s,a)|.
      Low surprise -> higher WM control; high surprise -> higher RL control.
      The weight is a sigmoid of arb_bias - arb_gain*|delta| blended with wm_weight.
    - Load: WM precision is reduced with set size (via fixed base beta).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight : Base mixture weight in [0,1] (used within arbitration).
        softmax_beta : Base RL inverse temperature (rescaled internally by *10).
        arb_gain : Sensitivity of arbitration to surprise (>=0).
        arb_bias : Bias term for arbitration (any real).
        stickiness : Additive bias to repeat previous action within a state (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, arb_gain, arb_bias, stickiness = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness
        prev_action = -1 * np.ones(nS, dtype=int)

        # Reduce WM precision with load by simple scaling
        beta_wm_eff = softmax_beta_wm / max(1.0, float(nS) / 3.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # Apply state-wise stickiness to RL values
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += stickiness

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Surprise-based arbitration (lower surprise -> more WM)
            delta_pred = r - q[s][a]
            surprise = abs(delta_pred)
            arb_signal = arb_bias - max(0.0, arb_gain) * surprise
            wm_gate = 1.0 / (1.0 + np.exp(-arb_signal))  # sigmoid
            wm_weight_eff = np.clip(0.5 * wm_weight + 0.5 * wm_gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: store rewarded action; else light decay
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]

            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with load-sensitive RL temperature and experience-driven WM gating.

    Mechanisms
    - RL: delta rule with inverse temperature reduced by set size (load).
      beta_RL_eff = (softmax_beta*10) * exp(-(beta_load_bias + beta_load_gain*(nS-3))).
    - WM: noisy memory updated toward one-hot on reward and toward uniform otherwise,
      with a single forgetting rate wm_forgot.
    - Arbitration: base wm_weight transformed to logit and increased with the
      within-state correct streak (more consistent success -> rely more on WM).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight : Base mixture weight in [0,1].
        softmax_beta : Base RL inverse temperature (rescaled internally by *10).
        beta_load_bias : Load bias term (>=0) reducing RL beta as set size grows.
        beta_load_gain : Load gain per extra item (>=0).
        wm_forgot : WM update rate in [0,1] for both learning and forgetting.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, beta_load_bias, beta_load_gain, wm_forgot = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    # Helper: logit transform for base wm_weight
    wm_weight = np.clip(wm_weight, 1e-6, 1 - 1e-6)
    base_logit = np.log(wm_weight / (1.0 - wm_weight))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-sensitive RL temperature
        load_term = max(0.0, beta_load_bias) + max(0.0, beta_load_gain) * max(0, nS - 3)
        beta_rl_eff = softmax_beta * np.exp(-load_term)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track correct streak per state
        streak = np.zeros(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy with load-adjusted temperature
            p_rl = 1.0 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Experience-driven WM gating using within-state correct streak
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(base_logit + streak[s])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update with single forgetting/learning rate
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_forgot) * w[s, :] + wm_forgot * onehot
                streak[s] += 1
            else:
                w[s, :] = (1.0 - wm_forgot) * w[s, :] + wm_forgot * w_0[s, :]
                streak[s] = 0

        blocks_log_p += log_p

    return -blocks_log_p