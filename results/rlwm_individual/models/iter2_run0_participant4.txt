def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated forgetting plus state-wise choice stickiness.

    Core ideas:
      - Model-free Q-learning within each block and state.
      - When a state is visited, all actions' Q-values in that state decay toward a neutral prior,
        with a forgetting rate that depends on age group and set size (cognitive load).
      - A state-specific perseveration (stickiness) bias increases the probability of repeating
        the last action taken in that state.

    Parameters
    ----------
    states : 1D array-like of int
        State index at each trial (0..set_size-1, within block).
    actions : 1D array-like of int
        Chosen action at each trial (0..2).
    rewards : 1D array-like of {0,1}
        Outcome on each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size for the current block on each trial (constant within a block).
    age : 1D array-like of float
        Participant age; age[0] is used. Age >= 45 -> older group, else younger.
    model_parameters : tuple/list
        (alpha, beta, forget_young, forget_old, stickiness)
        - alpha: learning rate for chosen action value update.
        - beta: inverse temperature for softmax choice.
        - forget_young: baseline forgetting rate for younger group (0..1).
        - forget_old: baseline forgetting rate for older group (0..1).
        - stickiness: strength of state-wise perseveration bias (added to chosen action's logit if it matches last action in that state).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, forget_young, forget_old, stickiness = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0
    nA = 3

    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set = set_sizes[mask]
        nS = int(block_set[0])

        # Initialize Q-values to neutral prior
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Track last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Age-specific base forgetting
        base_forget = forget_old if is_older else forget_young
        # Load modulation: more forgetting as set size increases (relative to 3)
        load_factor = max(1.0, float(nS) / 3.0)
        f_eff = base_forget * load_factor
        # Ensure forgetting rate within [0,1)
        f_eff = min(max(f_eff, 0.0), 0.999)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply forgetting to the visited state's action values toward a neutral prior
            neutral = 1.0 / nA
            Q[s, :] = (1.0 - f_eff) * Q[s, :] + f_eff * neutral

            # Compute softmax with state-wise stickiness bias
            q_s = Q[s, :].copy()
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = stickiness
            logits = beta * q_s + bias
            logits = logits - np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p = max(p_vec[a], 1e-12)
            total_log_p += np.log(p)

            # Q-learning update for chosen action
            Q[s, a] += alpha * (r - Q[s, a])

            # Update stickiness memory
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited working memory (WM) with age-dependent capacity and gating noise.

    Policy is a mixture of:
      - RL softmax over Q-values.
      - WM policy that, when available for the current state, chooses the stored rewarded action.
        WM has a limited number of state-action slots (capacity), which depends on age group.
        Access to WM is imperfect due to a gating noise parameter.
      - The mixture weight is trial- and block-dependent: proportional to (effective capacity / set size)
        and to the gating success probability.

    WM dynamics:
      - If a state receives reward, store its (state -> action) in WM.
      - If capacity exceeded, evict the least-recently rewarded state (LRU) deterministically.
      - Only states currently in WM benefit from the WM policy.

    Parameters
    ----------
    states : 1D array-like of int
        State index at each trial (0..set_size-1, within block).
    actions : 1D array-like of int
        Chosen action at each trial (0..2).
    rewards : 1D array-like of {0,1}
        Outcome on each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size for the current block on each trial (constant within a block).
    age : 1D array-like of float
        Participant age; age[0] is used. Age >= 45 -> older group, else younger.
    model_parameters : tuple/list
        (alpha, beta, K_young, K_old, eta_gate)
        - alpha: learning rate for RL Q-learning.
        - beta: inverse temperature for RL softmax.
        - K_young: WM capacity (in number of states) for younger group (can be non-integer; rounded to nearest int per block).
        - K_old: WM capacity for older group (rounded per block).
        - eta_gate: WM gating noise (0..1); probability of failing to use WM even if available.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, K_young, K_old, eta_gate = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0
    nA = 3

    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set = set_sizes[mask]
        nS = int(block_set[0])

        # RL initialization
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: mapping from state to action if in WM, -1 otherwise
        wm_action = -1 * np.ones(nS, dtype=int)
        # Recency list for LRU eviction (store states, most recent at the end)
        recency = []

        # Effective integer capacity per block based on age
        K_base = K_old if is_older else K_young
        K_eff = int(np.round(K_base))
        K_eff = int(min(max(K_eff, 0), nS))  # clamp between 0 and set size

        # Mixture scaling due to load and gating
        # Probability of using WM when available
        p_use_base = max(0.0, min(1.0, 1.0 - eta_gate))
        # Load-adjusted average availability (capacity relative to set size)
        load_mix = (K_eff / float(nS)) if nS > 0 else 0.0
        mix_scale = p_use_base * load_mix  # global cap on WM influence this block

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :]
            logits_rl = beta * (q_s - np.max(q_s))
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)

            # WM policy for this state (deterministic if available, else uniform)
            if wm_action[s] >= 0:
                wm_vec = np.zeros(nA)
                wm_vec[wm_action[s]] = 1.0
                wm_available = 1.0
            else:
                wm_vec = np.ones(nA) / nA
                wm_available = 0.0

            # Trial-wise mixture: limit WM influence by both availability and global mix scale
            mix_t = mix_scale * wm_available
            p_mix_vec = mix_t * wm_vec + (1.0 - mix_t) * p_rl_vec

            p = max(p_mix_vec[a], 1e-12)
            total_log_p += np.log(p)

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # WM update with LRU eviction
            if r > 0.5:
                # If already stored, update recency
                if wm_action[s] >= 0:
                    # Move s to most recent position
                    if s in recency:
                        recency = [x for x in recency if x != s]
                    recency.append(s)
                    wm_action[s] = a
                else:
                    # Need capacity to add
                    if K_eff > 0:
                        # Evict if full
                        if np.sum(wm_action >= 0) >= K_eff:
                            # Evict least recent
                            if len(recency) > 0:
                                to_evict = recency[0]
                                recency = recency[1:]
                                wm_action[to_evict] = -1
                        # Add current state
                        wm_action[s] = a
                        # Update recency
                        if s in recency:
                            recency = [x for x in recency if x != s]
                        recency.append(s)
                    else:
                        # No capacity; cannot store
                        pass
            else:
                # Optional: if not rewarded, do not store; keep existing mapping
                pass

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-sensitive (Bayesian) RL with age- and load-dependent exploration and perseveration.

    Belief representation:
      - For each state-action, maintain Beta posterior over Bernoulli reward with symmetric prior kappa/2.
      - Expected value Q = successes / (successes + failures).
      - Uncertainty u = 1 / sqrt(total observations) for that state-action.

    Policy:
      - Softmax over Q with an effective inverse temperature that decreases with uncertainty and load.
      - Age-dependent exploration control: younger/older groups have different sensitivity (tau parameters)
        to uncertainty and load.
      - A perseveration bias increases the logit of repeating the last action in the same state.

      beta_eff(s) = beta0 / (1 + tau_age * u_bar(s) * (set_size / 3))
      where u_bar(s) is the mean uncertainty across actions in state s.

    Parameters
    ----------
    states : 1D array-like of int
        State index at each trial (0..set_size-1, within block).
    actions : 1D array-like of int
        Chosen action at each trial (0..2).
    rewards : 1D array-like of {0,1}
        Outcome on each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size for the current block on each trial (constant within a block).
    age : 1D array-like of float
        Participant age; age[0] is used. Age >= 45 -> older group, else younger.
    model_parameters : tuple/list
        (kappa, beta0, tau_young, tau_old, perseveration)
        - kappa: prior strength; initializes successes and failures to kappa/2 each.
        - beta0: base inverse temperature.
        - tau_young: uncertainty/load sensitivity for younger group (higher -> more exploration via lower beta).
        - tau_old: uncertainty/load sensitivity for older group.
        - perseveration: bias added to the previous action's logit within the same state.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    kappa, beta0, tau_young, tau_old, perseveration = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0
    tau_age = tau_old if is_older else tau_young
    nA = 3

    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set = set_sizes[mask]
        nS = int(block_set[0])

        # Initialize Beta posterior counts
        prior = max(kappa, 1e-8) / 2.0
        successes = prior * np.ones((nS, nA))
        failures = prior * np.ones((nS, nA))

        # Track last action per state for perseveration
        last_action = -1 * np.ones(nS, dtype=int)

        load_factor = float(nS) / 3.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Posterior means and uncertainties
            counts = successes[s, :] + failures[s, :]
            Q_s = successes[s, :] / np.maximum(counts, 1e-12)
            u_s = 1.0 / np.sqrt(np.maximum(counts, 1e-12))
            u_bar = float(np.mean(u_s))

            # Effective inverse temperature
            beta_eff = beta0 / (1.0 + tau_age * u_bar * load_factor)
            beta_eff = max(beta_eff, 1e-6)

            # Softmax with perseveration bias
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = perseveration
            logits = beta_eff * Q_s + bias
            logits = logits - np.max(logits)
            exp_logits = np.exp(logits)
            p_vec = exp_logits / np.sum(exp_logits)
            p = max(p_vec[a], 1e-12)
            total_log_p += np.log(p)

            # Update Beta counts
            successes[s, a] += r
            failures[s, a] += (1.0 - r)

            # Update last action
            last_action[s] = a

    return -float(total_log_p)