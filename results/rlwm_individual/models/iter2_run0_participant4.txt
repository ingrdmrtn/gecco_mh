def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with load-dependent retention.

    Mechanism:
    - RL: standard Rescorla-Wagner update with single learning rate.
    - WM: a fast, one-shot winner-take-all trace that is capacity-limited.
      Retention decays toward uniform; decay slows when set size is within capacity.
    - Policy: fixed mixture of RL softmax and WM softmax. The WM mixture weight is
      downscaled by an effective capacity ratio min(1, K_slots / nS).

    Parameters
    ----------
    model_parameters : list/tuple of 5 floats
        lr                : RL learning rate in (0,1].
        wm_weight_base    : Base WM mixture weight in [0,1] (before capacity downscaling).
        softmax_beta      : RL inverse temperature (scaled by *10 internally).
        K_slots           : WM slot capacity (positive; around 3-6).
        wm_persist        : Controls WM retention; higher => stronger retention (slower decay).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, K_slots, wm_persist = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM weight
        cap_ratio = min(1.0, max(0.0, K_slots) / max(1.0, nS))
        wm_weight_eff = np.clip(wm_weight_base * cap_ratio, 0.0, 1.0)

        # Load-dependent WM retention (gamma close to 1 => strong retention)
        gamma = np.exp(-max(0.0, 1.0 / (1.0 + wm_persist * cap_ratio)))
        gamma = np.clip(gamma, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM trace
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform, then reward-gated boost/suppression
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]
            if r > 0:
                # one-shot strengthening of chosen action
                w[s, a] += (1.0 - w[s, a])
            else:
                # mild suppression of the chosen action when not rewarded
                w[s, a] *= 0.5

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-based arbitration with capacity-limited WM and load-sensitive decay.

    Mechanism:
    - RL: standard Rescorla-Wagner with single learning rate.
    - WM: fast trace that decays; decay is stronger when set size exceeds capacity K.
           WM policy sharpness is controlled by wm_gain (scales WM values).
    - Arbitration: trial-wise WM mixture weight is a sigmoid of (WM confidence - RL confidence),
      downscaled by a capacity factor min(1, K / nS).
      Confidence is the margin between the top and second-best action values per system.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr               : RL learning rate in (0,1].
        softmax_beta     : RL inverse temperature (scaled by *10 internally).
        K_capacity       : WM capacity parameter (positive).
        wm_decay_base    : Base WM decay rate (>0). Larger => faster decay; amplified by overload.
        arbitration_slope: Slope for the sigmoid arbitration mapping (positive).
        wm_gain          : Multiplicative gain applied to WM values before softmax (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, K_capacity, wm_decay_base, arbitration_slope, wm_gain = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12
    arbitration_slope = max(0.0, arbitration_slope)
    wm_gain = max(eps, wm_gain)
    K_capacity = max(eps, K_capacity)
    wm_decay_base = max(eps, wm_decay_base)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity factor and load-dependent WM retention
        overload = max(0.0, nS - K_capacity)
        cap_factor = np.clip(K_capacity / max(1.0, nS), 0.0, 1.0)
        # gamma close to 1 when no overload; decreases with overload
        gamma = np.exp(-wm_decay_base * (1.0 + overload))
        gamma = np.clip(gamma, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with gain
            W_eff = wm_gain * W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Arbitration based on confidence margins
            # Confidence = top - second-best value
            q_sorted = np.sort(Q_s)[::-1]
            w_sorted = np.sort(W_s)[::-1]
            conf_rl = q_sorted[0] - q_sorted[1] if nA > 1 else q_sorted[0]
            conf_wm = w_sorted[0] - w_sorted[1] if nA > 1 else w_sorted[0]

            wm_weight_dyn = 1.0 / (1.0 + np.exp(-arbitration_slope * (conf_wm - conf_rl)))
            wm_weight_eff = np.clip(cap_factor * wm_weight_dyn, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay then reward-gated shaping
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]
            if r > 0:
                w[s, a] += (1.0 - w[s, a])
            else:
                # soft penalty for chosen action on negative feedback
                w[s, a] = 0.5 * w[s, a] + 0.5 * w_0[s, a]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Dual-process RL (valence-asymmetric, load-modulated) + WM with load-driven interference.

    Mechanism:
    - RL: separate learning rates for positive vs. negative prediction errors.
          Effective learning rate shrinks with set size via a linear load_effect term.
    - WM: one-shot encoding of rewarded actions, but subject to global interference that
          increases with set size, controlled by xi. Interference blends WM toward uniform
          across all states each trial, mimicking crosstalk under high load.
    - Policy: fixed mixture between RL softmax and WM softmax.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr_pos        : RL learning rate for positive PEs in (0,1].
        lr_neg        : RL learning rate for negative PEs in (0,1].
        softmax_beta  : RL inverse temperature (scaled by *10 internally).
        wm_weight     : Fixed WM mixture weight in [0,1].
        load_effect   : Scales how much larger set sizes reduce the RL learning rate (>=0).
        xi            : WM interference strength with load (>=0); larger => stronger global decay.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, load_effect, xi = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    load_effect = max(0.0, load_effect)
    xi = max(0.0, xi)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load modulation for RL learning rate (shrinks with larger set size)
        load_scale = max(0.0, 1.0 - load_effect * max(0.0, (nS - 3.0) / max(1.0, 3.0)))

        # WM global interference factor increases with set size
        inter = np.clip(xi * max(0.0, (nS - 3.0) / max(1.0, 3.0)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax on current WM state trace
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry + load modulation
            pe = r - Q_s[a]
            eta = (lr_pos if pe >= 0 else lr_neg) * load_scale
            q[s][a] += eta * pe

            # WM update:
            # 1) Global interference toward uniform across all states (load-dependent)
            w = (1.0 - inter) * w + inter * w_0

            # 2) State-specific update: reward-driven one-shot strengthening; small penalty if not rewarded
            if r > 0:
                w[s, a] += (1.0 - w[s, a])
            else:
                w[s, a] = 0.5 * w[s, a] + 0.5 * w_0[s, a]

        blocks_log_p += log_p

    return -blocks_log_p