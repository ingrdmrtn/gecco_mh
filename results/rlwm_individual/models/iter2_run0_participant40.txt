def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reward-gated encoding, decay, and set-size-sensitive arbitration.

    Idea:
    - RL learns slowly via TD; policy is as in the template.
    - WM stores high-fidelity state-action mappings when rewarded, and otherwise drifts toward uniform.
    - Arbitration weight is adapted by set size: WM contributes more in small set sizes and less in large ones.

    Parameters
    ----------
    model_parameters : list or array of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Baseline WM mixture weight (0-1), modulated by set size.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_decay : float
            WM decay-to-uniform strength per visit (0-1).
        wm_size_sensitivity : float
            Controls how WM arbitration scales with set size; positive values increase WM weight
            in small sets and decrease it in large sets.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, wm_size_sensitivity = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute WM choice probabilities from current WM values
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Set-size-sensitive arbitration: more WM weight when nS is small
            # eff_w = sigmoid( logit(wm_weight_base) + wm_size_sensitivity * (3/nS - 1) )
            # Implemented via logistic transform ensuring [0,1]
            base_logit = np.log(np.clip(wm_weight_base, 1e-6, 1-1e-6)) - np.log(1 - np.clip(wm_weight_base, 1e-6, 1-1e-6))
            size_term = wm_size_sensitivity * (3.0 / float(nS) - 1.0)
            wm_weight = 1.0 / (1.0 + np.exp(-(base_logit + size_term)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM toward uniform each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # If rewarded, store a sharp WM trace for the chosen action
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use the same wm_decay as a learning strength toward the target
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference-limited WM (set-size-dependent noise) and dynamic WM confidence.

    Idea:
    - RL policy as in template (softmax over Q).
    - WM suffers interference that scales with set size; WM policy is a mixture of WM trace and uniform.
    - WM confidence (max activation) modulates arbitration weight on each trial.
    - WM traces decay toward uniform each visit and strengthen on reward.

    Parameters
    ----------
    model_parameters : list or array of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline mixture weight for WM (0-1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_decay : float
            Per-visit decay of WM toward uniform (0-1).
        wm_interference_kappa : float
            Strength of set-size-driven WM interference; larger kappa => more mixing with uniform
            as set size increases.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_interference_kappa = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute interference coefficient for this block
        # Mix WM with uniform: gamma in [0,1], increasing in nS
        gamma = 1.0 - np.exp(-wm_interference_kappa * float(nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Interference-limited WM policy: mixture of WM trace with uniform before softmax
            W_eff = (1.0 - gamma) * W_s + gamma * w_0[s, :]
            logits_wm = softmax_beta_wm * (W_eff - np.max(W_eff))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Trial-wise WM confidence from peakedness of W_s
            wm_conf = np.max(W_s)  # in [1/nA, 1]
            # Modulate arbitration by WM confidence
            eff_w = wm_weight * wm_conf * (3.0 / float(nS))  # down-weight in larger set sizes
            wm_weight = np.clip(eff_w, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-gated strengthening of the chosen action trace
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with resource-rational WM temperature and logistic arbitration.

    Idea:
    - RL policy as in template.
    - WM policy uses a state-dependent effective inverse temperature that scales with set size:
      beta_wm_eff = beta_wm_base * (C / nS)^phi, so WM is more deterministic in small sets.
    - WM traces decay and are reinforced on reward.
    - Arbitration uses a logistic function of set size with free bias and slope (applied per block).

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Base WM mixture weight prior to set-size logistic modulation (0-1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_capacity_C : float
            Capacity scaling constant for WM temperature (>0).
        wm_phi : float
            Exponent controlling how strongly WM temperature declines with set size.
        wm_decay : float
            Per-visit WM decay toward uniform (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_capacity_C, wm_phi, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base; will be rescaled by (C/nS)^phi

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM inverse temperature for this block (resource shared across nS)
        size_ratio = np.clip(wm_capacity_C / max(1.0, float(nS)), 1e-6, 1e6)
        beta_wm_eff = softmax_beta_wm * (size_ratio ** wm_phi)

        # Logistic arbitration as a function of set size
        # eff_w = sigmoid( logit(base) + slope * (3/nS - 1) ), slope absorbed into base via wm_weight_base
        base_logit = np.log(np.clip(wm_weight_base, 1e-6, 1-1e-6)) - np.log(1 - np.clip(wm_weight_base, 1e-6, 1-1e-6))
        size_term = (3.0 / float(nS) - 1.0)  # negative for large sets, positive for small sets
        # Use wm_phi as an additional slope on arbitration sensitivity (shares role with WM temp exponent)
        wm_logit = base_logit + wm_phi * size_term
        block_wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))
        block_wm_weight = np.clip(block_wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            logits_wm = beta_wm_eff * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Use the block-level WM arbitration weight computed from set size
            wm_weight = block_wm_weight

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-based strengthening
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p