def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size scaling, recency-based availability, and lapse.

    Idea:
    - RL: delta-rule with softmax.
    - WM: stores a one-hot action when rewarded; decays toward uniform when not rewarded.
    - WM availability: scaled by (slots / set_size) to reflect capacity limits, and by a
      recency factor that decays with the age (trials since last reward for that state).
    - Final policy: mixture of WM and RL, with an added lapse (uniform) component.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight: baseline WM mixture weight (0..1).
    - K_slots: number of WM slots available (0..6); effective WM contribution scales with K_slots/nS.
    - rehearsal_base: base factor in (0..1) controlling recency decay of WM availability per state;
        availability scales as rehearsal_base**age.
    - lapse: lapse rate mixing in uniform choice (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, K_slots, rehearsal_base, lapse = model_parameters
    softmax_beta *= 10.0  # keep template scaling
    softmax_beta_wm = 50.0  # very deterministic as in template
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track recency (age since last rewarded WM update) for each state
        age = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Increase age for all states each trial (global time passes)
            age += 1.0

            Q_s = q[s, :]
            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax on w)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Availability of WM for this state depends on capacity and recency
            cap_scale = np.clip(float(K_slots) / max(1.0, float(nS)), 0.0, 1.0)
            recency_scale = np.clip(rehearsal_base, 0.0, 1.0) ** max(0.0, age[s])
            wm_avail = wm_weight * cap_scale * recency_scale

            # Mixture with lapse
            p_mixture = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = (1.0 - lapse) * p_mixture + lapse * (1.0 / nA)

            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward writes one-hot; no-reward decays towards uniform
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
                age[s] = 0.0  # reset recency on successful WM write
            else:
                # gentle decay toward uniform on errors
                lam = 0.05
                w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-specific learning rates and WM precision/weight modulated by RL uncertainty.

    Idea:
    - RL: delta-rule with separate learning rates for small (nS=3) vs large (nS=6) sets.
    - WM: fast supervised update toward chosen action on reward; slight anti-update on error.
    - WM precision: increases in smaller sets (beta_wm_eff), controlled by gamma.
    - Arbitration: WM weight scaled by RL uncertainty for the current state, measured by
      the entropy of the RL softmax over actions. Higher RL uncertainty -> more WM reliance.

    Parameters (tuple):
    - lr_small: RL learning rate used when set size = 3.
    - lr_large: RL learning rate used when set size = 6.
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight_base: baseline WM mixture weight (0..1).
    - wm_eta: WM learning rate for moving toward one-hot on reward and away on error.
    - gamma: scales WM precision advantage in small sets; beta_wm_eff = 50*(1 + gamma*(6-nS)/3).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_small, lr_large, softmax_beta, wm_weight_base, wm_eta, gamma = model_parameters
    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # learning rate for this block based on set size
        lr = lr_small if nS <= 3 else lr_large

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy and entropy (uncertainty)
            # Use a stabilized softmax to get probabilities and entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi = pi / np.sum(pi)
            p_rl = pi[a]
            entropy = -np.sum(np.clip(pi, eps, 1.0) * np.log(np.clip(pi, eps, 1.0)))
            max_entropy = np.log(nA)
            rl_uncertainty = entropy / max_entropy  # in [0,1]

            # WM policy with precision advantage in small sets
            beta_wm_eff = softmax_beta_wm * (1.0 + gamma * (max(0, 6 - nS) / 3.0))
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Arbitration weighs WM more when RL is uncertain
            wm_avail = wm_weight_base * rl_uncertainty
            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl

            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update
            if r > 0.5:
                # move toward one-hot for chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # small anti-update away from chosen action
                anti = w[s, :].copy()
                anti[a] = 0.0
                if np.sum(anti) > 0:
                    anti = anti / np.sum(anti)
                    w[s, :] = (1.0 - 0.5 * wm_eta) * w[s, :] + (0.5 * wm_eta) * anti
                # slight pull toward uniform to avoid collapse
                lam = 0.02
                w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid with PE-based arbitration, RL forgetting by set size, and WM interference across states.

    Idea:
    - RL: delta-rule plus decay toward uniform (forgetting) that is stronger in larger sets.
    - WM: stores rewarded action as one-hot; continuous decay toward uniform each trial.
    - WM interference: when set size is large, the WM representation for a state is corrupted
      toward the average WM across other states, controlled by an interference parameter.
    - Arbitration: WM weight increases when the absolute previous prediction error is small
      (i.e., stable mapping) and decreases when large; also scaled by (3/nS).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_weight_base: baseline WM weight (0..1).
    - rl_decay_base: base RL decay (0..1) per trial; effective decay scales with set size.
    - interference: WM interference strength (>=0), increases corruption in larger sets.
    - pe_sensitivity: controls how strongly PE reduces WM reliance (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, rl_decay_base, interference, pe_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last absolute prediction error per state for arbitration
        last_abs_pe = np.ones(nS) * 1.0  # start with high uncertainty

        # Effective RL decay scaled by set size (more decay in larger sets)
        rl_decay = 1.0 - np.clip(rl_decay_base * (float(nS) / 3.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference
            # Compute effective W_s corrupted toward mean of others when set size is large
            if nS > 1:
                mean_other = (np.sum(w, axis=0) - w[s, :]) / max(1, nS - 1)
            else:
                mean_other = w[s, :].copy()
            alpha_int = np.clip(interference * max(0.0, (nS - 3) / 3.0), 0.0, 1.0)
            W_eff = (1.0 - alpha_int) * w[s, :] + alpha_int * mean_other
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Arbitration: more WM when previous PE is small and set size is small
            pe_term = 1.0 / (1.0 + pe_sensitivity * last_abs_pe[s])
            size_scale = 3.0 / max(3.0, float(nS))
            wm_avail = np.clip(wm_weight_base * pe_term * size_scale, 0.0, 1.0)

            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # apply decay for all states each trial
            q = rl_decay * q + (1.0 - rl_decay) * (1.0 / nA)

            # WM update: reward writes one-hot; continuous decay toward uniform each trial
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            # global WM decay each trial (stronger when set size is large via alpha_int influence above)
            lam = 0.03
            w = (1.0 - lam) * w + lam * w_0

            # update last absolute PE for arbitration
            last_abs_pe[s] = abs(delta)

        blocks_log_p += log_p

    return -blocks_log_p