def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with WM capacity limit and decay.

    Mechanism:
    - RL: single learning rate with softmax choice.
    - WM: fast one-shot encoding (toward a one-hot on chosen action) with decay to uniform over time.
    - Arbitration: WM is weighted less when set size exceeds an internal capacity parameter.
        wm_weight_eff = wm_weight * min(1, capacity / nS)
      This implements a simple capacity-limited WM that is more influential in set size 3 than 6.

    Parameters (in order):
    - lr: Learning rate for RL (and WM encoding) in [0,1].
    - wm_weight: Baseline mixture weight on WM policy in [0,1].
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_decay: Trial-wise decay of WM toward uniform in [0,1]; higher means faster forgetting.
    - capacity: Effective WM capacity (>=1); when nS > capacity, WM influence is down-weighted.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    lr = np.clip(lr, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    capacity = max(1.0, float(capacity))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-limited WM mixture weight
        wm_weight_eff = wm_weight * min(1.0, capacity / max(1.0, nS))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over current WM weights for the state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform for all states (forgetting)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding for the current state: move toward a one-hot at chosen action
            w[s, :] = (1.0 - lr) * w[s, :]
            w[s, a] += lr

            # Keep WM a valid distribution
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load penalty.

    Mechanism:
    - RL: single learning rate with softmax.
    - WM: fast learning with its own learning rate (wm_learn) toward the chosen action; on errors, weak anti-Hebbian push.
    - Arbitration: WM weight is a logistic function of
        base bias (w0) + meta_kappa * (confidence_wm - confidence_rl) - load_slope * (nS - 3).
      Where confidence is 1 - normalized entropy of the policy distribution.
      Thus WM dominates when it's sharper than RL and at small set sizes.

    Parameters (in order):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_learn: WM encoding rate (0..1).
    - w0: Baseline logit bias favoring WM (> -inf, < inf).
    - meta_kappa: Sensitivity to confidence difference (>=0).
    - load_slope: Linear penalty on WM with larger set size (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_learn, w0, meta_kappa, load_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    lr = np.clip(lr, 0.0, 1.0)
    wm_learn = np.clip(wm_learn, 0.0, 1.0)
    meta_kappa = max(0.0, float(meta_kappa))
    load_slope = max(0.0, float(load_slope))

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        p = p / np.sum(p)
        return -np.sum(p * np.log(p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Policies
            Q_s = q[s, :]
            W_s = w[s, :]

            # Softmax probabilities for full distributions (for confidence estimation)
            prl_full = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_full = prl_full / np.sum(prl_full)

            pwm_full = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm_full = pwm_full / np.sum(pwm_full)

            # Scalar choice probabilities for observed action
            p_rl = prl_full[a]
            p_wm = pwm_full[a]

            # Confidence via normalized entropy (0..1)
            max_H = np.log(nA)
            conf_rl = 1.0 - entropy(prl_full) / max_H
            conf_wm = 1.0 - entropy(pwm_full) / max_H

            # Load-adjusted, entropy-driven arbitration
            logit = w0 + meta_kappa * (conf_wm - conf_rl) - load_slope * (nS - 3.0)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-logit))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # Mixture likelihood
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: Hebbian on reward; weak anti-Hebbian on no reward
            if r > 0.0:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
            else:
                # push away slightly from chosen action, redistribute to others
                take = 0.5 * wm_learn * w[s, a]
                w[s, a] -= take
                redistribute = take / (nA - 1)
                for ao in range(nA):
                    if ao != a:
                        w[s, ao] += redistribute

            # Keep WM a valid distribution
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with loss attenuation + WM with interference across states.

    Mechanism:
    - RL: single learning rate with asymmetric scaling of negative prediction errors by lambda_neg (0..1).
      Update: q[s,a] += lr * (r - Q) if r>=Q; else lr * lambda_neg * (r - Q).
    - WM: on reward, encode strongly toward chosen action; on no reward, decay chosen action.
      Additionally, encoding at one state interferes with other states' WM representations:
        a fraction rho_eff of the encoded mass is spread to the same action across other states.
      Interference increases with set size: rho_eff = rho * (nS - 1) / nS.
    - Arbitration: fixed wm_weight mixture independent of performance, but WM is near-deterministic.

    Parameters (in order):
    - lr: Base learning rate for RL and WM encoding (0..1).
    - lambda_neg: Attenuation of negative prediction errors in RL (0..1).
    - wm_weight: Mixture weight on WM policy (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - rho: Interference strength across states (0..1).
    - lapse: Small choice noise added to both policies (0..0.1 recommended), improves numerical stability.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, lambda_neg, wm_weight, softmax_beta, rho, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    lr = np.clip(lr, 0.0, 1.0)
    lambda_neg = np.clip(lambda_neg, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    rho = np.clip(rho, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 0.25)  # small stabilizing noise

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference magnitude increases with set size
        rho_eff = rho * (max(1, nS) - 1) / max(1, nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with small lapse
            prl_full = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_full = prl_full / np.sum(prl_full)
            prl_full = (1.0 - lapse) * prl_full + lapse / nA
            p_rl = prl_full[a]

            # WM policy with small lapse
            pwm_full = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm_full = pwm_full / np.sum(pwm_full)
            pwm_full = (1.0 - lapse) * pwm_full + lapse / nA
            p_wm = pwm_full[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with attenuated negative errors
            delta = r - Q_s[a]
            if delta >= 0:
                q[s, a] += lr * delta
            else:
                q[s, a] += lr * lambda_neg * delta

            # WM local update at state s
            if r > 0.0:
                # Encode toward chosen action
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                # Decay chosen action; redistribute mass to others
                take = lr * w[s, a]
                w[s, a] -= take
                w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
                add = take / (nA - 1)
                for ao in range(nA):
                    if ao != a:
                        w[s, ao] += add

            # Interference: spread a fraction of the change to the same action across other states
            # Move other states slightly toward choosing action 'a'
            if nS > 1:
                for so in range(nS):
                    if so == s:
                        continue
                    # shift toward one-hot on action a by rho_eff * lr
                    alpha = rho_eff * lr
                    w[so, :] = (1.0 - alpha) * w[so, :]
                    w[so, a] += alpha
                    # normalize each affected state's WM row
                    w[so, :] = np.clip(w[so, :], 1e-9, 1.0)
                    w[so, :] /= np.sum(w[so, :])

            # Normalize current state's WM row
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p