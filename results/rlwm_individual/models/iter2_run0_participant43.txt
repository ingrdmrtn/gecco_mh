def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with time-based WM decay, load-dependent WM arbitration, and perseveration bias.

    Core ideas:
    - RL updates action values with a single learning rate.
    - WM forms a sharp association when rewarded and decays toward uniform with time since last visit.
    - WM contribution to choice is reduced under higher load (larger set size) via a logistic load effect.
    - A perseveration bias adds value to the previously chosen action in the current state.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight0: Baseline mixture weight for WM (0..1), modulated by set size.
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - wm_decay_time: Controls how quickly WM decays with elapsed trials since last visit (>0).
                      Effective per-visit decay d = 1 - exp(-age / wm_decay_time).
    - load_slope: Sensitivity of WM weight to load; higher values reduce WM weight more at nS=6 (any real).
    - perseveration: Choice stickiness added to the last chosen action in a state (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay_time, load_slope, perseveration = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track ages for time-based WM decay and last action for perseveration
        ages = np.zeros(nS, dtype=float)
        last_action = -np.ones(nS, dtype=int)

        # Load-dependent arbitration: reduce WM weight as set size increases
        # wm_weight_eff = wm_weight0 / (1 + exp(load_slope * (nS - 3)))
        wm_weight_eff = wm_weight0 / (1.0 + np.exp(load_slope * (float(nS) - 3.0)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply time-based decay to WM for current state using its age
            # d in [0,1): more decay as age grows
            age = ages[s]
            if wm_decay_time > 0:
                d = 1.0 - np.exp(-age / float(wm_decay_time))
            else:
                d = 1.0  # maximal decay if wm_decay_time is zero or negative
            w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]

            # Construct perseveration bias vector for this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] += perseveration

            # RL policy with perseveration bias
            Q_s = q[s, :].copy() + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with perseveration bias
            W_s = w[s, :].copy() + bias
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: reward-driven sharpening, otherwise gentle drift already handled by decay
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Strong overwrite toward the rewarded action
                w[s, :] = 0.2 * w[s, :] + 0.8 * target

            # Update bookkeeping: reset age of visited state; increment others
            ages += 1.0
            ages[s] = 0.0
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-weighted arbitration and lapses.

    Core ideas:
    - RL updates action values with a single learning rate.
    - WM is a fast but leaky associative store that sharpens on reward and otherwise leaks.
    - Arbitration weight for WM is dynamic: depends on relative certainty (1 - normalized entropy)
      of WM vs RL in the current state.
    - A small lapse rate mixes in uniform random choice.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline WM weight (0..1) controlling the midpoint of arbitration.
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - gate_temp: Sensitivity of arbitration to certainty difference (>=0). Higher -> more dynamic reweighting.
    - epsilon: Lapse rate (0..0.2+) mixing in uniform choice.
    - wm_leak: Leak of WM toward uniform on non-rewarded trials in that state (0..1).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, gate_temp, epsilon, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    def softmax_probs(vec, beta):
        z = vec - np.max(vec)
        e = np.exp(beta * z)
        return e / np.sum(e)

    def norm_entropy(p):
        # Normalized entropy in [0,1]
        p = np.clip(p, 1e-12, 1.0)
        H = -np.sum(p * np.log(p))
        return H / np.log(len(p))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(x):
        x = np.clip(x, 1e-6, 1 - 1e-6)
        return np.log(x / (1.0 - x))

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute dynamic arbitration using certainty difference
            p_rl_full = softmax_probs(Q_s, softmax_beta)
            p_wm_full = softmax_probs(W_s, softmax_beta_wm)
            cert_rl = 1.0 - norm_entropy(p_rl_full)
            cert_wm = 1.0 - norm_entropy(p_wm_full)
            # Combine baseline (as logit) with certainty difference
            wm_weight_dyn = sigmoid(logit(wm_weight_base) + gate_temp * (cert_wm - cert_rl))

            # Final mixture with lapses
            p_mix = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: sharpen on reward; otherwise leak toward uniform
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use wm_leak as a step size toward the one-hot target
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * target
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM cache with interference under load.

    Core ideas:
    - RL learns gradually.
    - WM acts as a capacity-limited cache: a state is either in WM (near one-hot policy if rewarded) or not.
    - When WM is full, new rewarded associations evict the least recent state.
    - Interference increases with load: updating one state partially disrupts WM traces of other cached states.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM when the state is in cache (0..1).
    - softmax_beta: RL inverse temperature (scaled by x10 internally).
    - cap_prop: Proportion controlling WM capacity (0..1). Effective capacity = 1 + round(2 * cap_prop) in {1,2,3}.
    - interf: Interference strength applied to other cached states on each rewarded update (0..1).
    - recency_tau: Controls recency score decay; larger -> slower decay (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, cap_prop, interf, recency_tau = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    def capacity_from_prop(x):
        # Map [0,1] to {1,2,3}
        return int(1 + np.round(2.0 * np.clip(x, 0.0, 1.0)))

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM cache bookkeeping
        cap = capacity_from_prop(cap_prop)
        in_cache = np.zeros(nS, dtype=bool)
        # Recency scores for eviction (higher = more recent)
        recency = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Decay recency scores
            if recency_tau > 0:
                recency *= np.exp(-1.0 / float(recency_tau))
            else:
                recency *= 0.0

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # If not in cache, WM contributes little because w[s,:] is near-uniform
            wm_contrib = wm_weight if in_cache[s] else 0.0
            p_total = wm_contrib * p_wm + (1.0 - wm_contrib) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM caching and interference
            if r >= 0.5:
                # Ensure s is in cache; evict if necessary
                if not in_cache[s]:
                    # Evict least recent among cached if at capacity
                    if np.sum(in_cache) >= cap:
                        # among cached states, find min recency
                        cached_idx = np.where(in_cache)[0]
                        to_evict = cached_idx[np.argmin(recency[cached_idx])]
                        in_cache[to_evict] = False
                        w[to_evict, :] = w_0[to_evict, :].copy()
                        recency[to_evict] = 0.0
                    in_cache[s] = True
                # Set a sharp one-hot WM trace for current state
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.1 * w[s, :] + 0.9 * target
                recency[s] = 1.0

                # Interference to other cached states grows with set size (more crowding)
                if np.sum(in_cache) > 1:
                    factor = interf * max(0.0, (float(nS) - 3.0)) / 3.0  # 0 at nS=3, up to interf at nS=6
                    if factor > 0:
                        others = np.where(in_cache & (np.arange(nS) != s))[0]
                        w[others, :] = (1.0 - factor) * w[others, :] + factor * w_0[others, :]
                        # Slight recency decrease for others
                        recency[others] *= (1.0 - 0.5 * factor)

            else:
                # No new information: optional mild drift of non-cached states toward uniform
                if not in_cache[s]:
                    w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p