def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity-weighted WM control and perseveration bias.
    
    Idea:
    - Standard model-free RL.
    - WM policy strength and precision degrade with set size via a capacity-like precision parameter.
    - Perseveration/stickiness bias adds value to repeating the last action within a block.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr                : RL learning rate (0..1).
      wm_weight_base    : Baseline WM mixture weight (0..1) before capacity scaling.
      softmax_beta      : RL inverse temperature; internally scaled by 10.
      wm_precision      : Capacity-like parameter; larger => better WM precision and control; interacts with set size.
      perseveration_beta: Stickiness bias added to the last action's logit (>=0).
      wm_decay          : WM decay per trial toward uniform (0..1).
    
    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_precision, perseveration_beta, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-weighted WM mixture and precision (declines with nS)
        cap = wm_precision / (wm_precision + nS)  # in (0,1), smaller for larger set size
        wm_mix_block = np.clip(wm_weight_base * cap, 0.0, 1.0)
        wm_beta_block = softmax_beta_wm * cap

        log_p = 0.0
        prev_a = None
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with perseveration bias
            logits_rl = softmax_beta * q[s, :].copy()
            if prev_a is not None and 0 <= prev_a < nA:
                logits_rl[prev_a] += perseveration_beta
            max_lr = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_lr)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy with capacity-scaled precision
            logits_wm = wm_beta_block * w[s, :].copy()
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture policy
            p_total = wm_mix_block * p_wm + (1.0 - wm_mix_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding with capacity-scaled strength
            kappa = cap
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - kappa) * w[s, :] + kappa * one_hot
            else:
                # Gentle suppression of the chosen action and redistribution
                reduce = kappa * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    other_sum = np.sum(w[s, others])
                    if other_sum > 0:
                        w[s, others] += reduce * (w[s, others] / other_sum)
                    else:
                        w[s, others] += reduce / (nA - 1)
                # Renormalize
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

            prev_a = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + WM gated by surprise (prediction error) + lapse.
    
    Idea:
    - RL has separate learning rates for positive vs negative prediction errors.
    - WM's contribution is gated by surprise magnitude (|PE|): higher surprise => stronger WM control.
    - Lapse adds a probability of uniformly random choice.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr_pos       : RL learning rate for positive PE (0..1).
      lr_neg       : RL learning rate for negative PE (0..1).
      wm_weight    : Base WM weight (0..1), scaled by surprise magnitude on each trial.
      softmax_beta : RL inverse temperature; internally scaled by 10.
      wm_decay     : WM decay per trial toward uniform (0..1).
      lapse        : Lapse probability of uniform random choice (0..1).
    
    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = softmax_beta * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Compute PE for gating (using current RL values)
            delta = r - q[s, a]
            surprise = abs(delta)
            wm_mix = np.clip(wm_weight * surprise, 0.0, 1.0)

            # Mixture with lapse
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding: bind rewarded pairs more strongly; weak effect otherwise
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                # Slight preference away from the chosen action
                reduce = 0.25 * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += reduce / (nA - 1)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay and load-dependent temperature + WM with shared forgetting and rewarded encoding.
    
    Idea:
    - RL includes decay toward the uniform prior each trial (captures forgetting under load).
    - RL temperature decreases with set size via a size-temperature parameter (less precise at larger set sizes).
    - WM contributes with a constant mixture weight but shares the same forgetting/decay parameter.
    - WM encodes only when rewarded with a dedicated success update strength.
    
    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr                  : RL learning rate (0..1).
      wm_weight           : WM mixture weight (0..1), used in all blocks.
      softmax_beta        : Base RL inverse temperature; internally scaled by 10 and reduced by load.
      q_decay             : Global decay (0..1) applied per trial to both Q-values and WM toward uniform.
      gamma_size_temp     : Load sensitivity for RL temperature (>=0); larger => stronger temperature drop with set size.
      wm_update_success   : WM encoding strength for rewarded trials (0..1).
    
    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, q_decay, gamma_size_temp, wm_update_success = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent RL temperature: larger set sizes => lower effective beta
        beta_rl_block = softmax_beta / (1.0 + gamma_size_temp * max(0, nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = beta_rl_block * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay
            delta = r - q[s, a]
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA)
            q[s, a] += lr * delta

            # WM decay (shared forgetting parameter)
            w = (1.0 - q_decay) * w + q_decay * w_0

            # WM encoding on reward only
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_update_success) * w[s, :] + wm_update_success * one_hot

        blocks_log_p += log_p

    return -blocks_log_p