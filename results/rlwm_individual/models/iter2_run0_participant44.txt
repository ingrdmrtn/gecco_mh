def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL(eligibility-trace) + resource-rational WM with capacity- and age-dependent decay.

    Model idea:
    - RL: single-step TD with an eligibility trace to boost within-state credit assignment.
    - WM: stores a near-deterministic action for a state when recently rewarded; otherwise it decays toward uniform.
    - Arbitration: mixture between RL and WM, where WM weight is stronger at low load (set size 3) and reduced by age.
      WM also decays faster with larger set sizes (interference) and more for older adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled internally)
    - trace_lambda: eligibility trace decay (0..1)
    - wm_decay_base: baseline WM decay rate per trial (0..1)
    - age_wm_penalty: additional WM penalty per older adult (>=0), reduces WM mix and increases WM decay

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, trace_lambda, wm_decay_base, age_wm_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM distribution per state; starts uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Eligibility traces for RL
        e = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action (near-deterministic when peaked)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load- and age-dependent arbitration
            # Stronger WM at low set size; penalize WM for older adults
            load_scale = 3.0 / float(nS)  # 1.0 for set size 3, 0.5 for set size 6
            wm_mix = wm_weight * load_scale
            wm_mix *= (1.0 - age_wm_penalty * older)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - q[s, a]
            # Set eligibility for chosen state-action to 1, decay others
            e *= trace_lambda
            e[s, a] = 1.0
            q += lr * delta * e

            # WM updating with capacity- and age-dependent decay
            # Interference/decay increases with set size and age
            wm_decay = wm_decay_base * (float(nS) / 3.0) * (1.0 + age_wm_penalty * older)
            wm_decay = np.clip(wm_decay, 0.0, 1.0)

            if r > 0.5:
                # Encode rewarded action strongly (sharpen toward one-hot)
                target = (1.0 / nA) * np.ones(nA)
                target[a] = 1.0
                target /= np.sum(target)
                w[s, :] = (1.0 - wm_decay) * target + wm_decay * w_0[s, :]
            else:
                # Without reward, memory drifts toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Ensure normalization
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted arbitration: RL + recency-based WM with confidence gating.

    Model idea:
    - RL: single learning rate, softmax choice.
    - WM: stores a probabilistic policy that is sharpened by recent reward; confidence decays with recency.
    - Arbitration: WM vs RL mixture determined by a logistic function of (WM confidence - RL uncertainty),
      scaled by a reliability parameter. WM confidence decays with time since the last reward in that state.
      RL uncertainty is tracked as an exponential moving average of squared prediction errors.
    - Age reduces WM contribution and increases reliance on RL under uncertainty.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: inverse temperature for RL policy (scaled internally)
    - wm_reliability: slope scaling WM confidence in arbitration (>=0)
    - rl_conf_bias: slope scaling RL uncertainty in arbitration (>=0)
    - wm_recency_rate: decay rate controlling how fast WM confidence drops with time since last reward (>=0)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_reliability, rl_conf_bias, wm_recency_rate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track RL uncertainty (EMA of squared PEs) per state-action
        rl_unc = np.zeros((nS, nA))
        unc_alpha = 0.1  # fixed smoothing

        # Track recency of last rewarded action per state
        last_reward_time = -np.ones(nS, dtype=int)
        t_global = 0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM confidence via recency of reward
            if last_reward_time[s] >= 0:
                dt = max(0, t_global - last_reward_time[s])
                wm_conf = np.exp(-wm_recency_rate * dt)  # [0,1]
            else:
                wm_conf = 0.0

            # Compute RL uncertainty in the current state as mean over actions
            rl_unc_s = np.mean(rl_unc[s, :])

            # Arbitration signal: logistic of (wm_reliability * wm_conf - rl_conf_bias * rl_unc_s),
            # modulated by load and age
            load_scale = 3.0 / float(nS)
            arb_signal = wm_reliability * wm_conf * load_scale - rl_conf_bias * rl_unc_s
            # Age reduces WM weight
            arb_signal -= 0.5 * older
            wm_mix = 1.0 / (1.0 + np.exp(-arb_signal))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # Update RL uncertainty (EMA of squared PE) for chosen action and diffuse slightly
            rl_unc[s, a] = (1.0 - unc_alpha) * rl_unc[s, a] + unc_alpha * (pe ** 2)
            # Small diffusion to other actions in the state
            others = [i for i in range(3) if i != a]
            rl_unc[s, others] = (1.0 - 0.1 * unc_alpha) * rl_unc[s, others] + 0.1 * unc_alpha * (pe ** 2)

            # WM updating: if reward, sharpen toward chosen action; else, gentle decay toward uniform
            if r > 0.5:
                w[s, :] = 0.1 * w[s, :] + 0.9 * w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
                last_reward_time[s] = t_global
            else:
                # Decay toward uniform; faster decay with higher load and age
                decay = 0.2 * (float(nS) / 3.0) * (1.0 + 0.5 * older)
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

            t_global += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value forgetting + frequency-based WM subject to interference; age-dependent exploration.

    Model idea:
    - RL: single learning rate with between-trial forgetting toward neutral; softmax choice.
    - WM: maintains Dirichlet-like frequency counts per state; rewarded actions increase counts.
      Interference/leak pushes WM toward uniform more strongly at higher set sizes and in older adults.
    - Arbitration: fixed baseline WM weight scaled by load and age.
    - Age also modulates inverse temperature (reduced for older adults).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base inverse temperature for RL (scaled internally)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - rl_forgetting: RL forgetting rate toward neutral values per trial (0..1)
    - interference: WM interference strength per trial (0..1)
    - beta_age_gain: scales inverse temperature with age group (>=0), applied as 1/(1+beta_age_gain*older)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, rl_forgetting, interference, beta_age_gain = model_parameters
    # Age-reduced inverse temperature
    age = age[0]
    older = 1.0 if age >= 45 else 0.0
    softmax_beta = softmax_beta * 10.0 / (1.0 + beta_age_gain * older)
    softmax_beta_wm = 50.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values initialized to neutral
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM: Dirichlet-like counts and probabilities
        counts = np.ones((nS, nA))  # start with symmetric prior
        w = counts / np.sum(counts, axis=1, keepdims=True)
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM stronger at low load, reduced in older adults
            load_scale = 3.0 / float(nS)
            wm_mix = wm_weight_base * load_scale * (1.0 - 0.3 * older)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL forgetting toward neutral before update
            q[s, :] = (1.0 - rl_forgetting) * q[s, :] + rl_forgetting * w_0[s, :]

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM frequency update with interference/leak
            if r > 0.5:
                counts[s, a] += 1.0
            # Apply interference/leak toward uniform; stronger with load and age
            leak = interference * (float(nS) / 3.0) * (1.0 + 0.5 * older)
            leak = np.clip(leak, 0.0, 1.0)
            counts[s, :] = (1.0 - leak) * counts[s, :] + leak * np.mean(counts[s, :])

            # Recompute WM distribution
            w[s, :] = counts[s, :] / np.sum(counts[s, :])

        blocks_log_p += log_p

    return -blocks_log_p