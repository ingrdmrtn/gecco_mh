def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces mixed with capacity-limited WM; age modulates trace persistence.

    The model blends a reinforcement learning (RL) softmax policy with a working-memory (WM)
    policy that stores the last rewarded action for each state. The mixture weight depends on
    set size via a simple capacity rule. RL uses an eligibility trace, whose decay (lambda)
    depends on the participant's age group.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within each block).
    actions : array-like of int
        Chosen actions per trial. Valid actions are {0,1,2}. Invalid actions (<0 or >=3)
        are treated as lapses: uniform choice probability; no learning updates.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Negative rewards are treated as invalid (uniform; no update).
    blocks : array-like of int
        Block index per trial. Blocks define independent learning episodes.
    set_sizes : array-like of int
        Set size per trial (constant within a block; generally 3 or 6).
    age : array-like of float
        Participant age; age[0] is used. Age >= 45 defines "older" group.
    model_parameters : tuple/list of floats
        (alpha, beta, lambda_young, lambda_old, capacity_k)
        - alpha: learning rate for RL (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - lambda_young: eligibility trace decay for younger adults (0..1).
        - lambda_old: eligibility trace decay for older adults (0..1); typically lower if traces are weaker.
        - capacity_k: effective WM capacity in items; WM weight g = min(1, k / set_size).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, lambda_young, lambda_old, capacity_k = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    lam = lambda_old if is_older > 0.5 else lambda_young

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        # Initialize Q-values and eligibility traces
        Q = (1.0 / nA) * np.ones((nS, nA))
        E = np.zeros((nS, nA))
        # WM cache: stores last rewarded action per state; -1 means unknown
        wm_cache = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = max(1, int(block_set_sizes[t]))

            # Invalid trials: assign uniform prob and skip updates
            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                # decay traces even on invalid step to keep time running
                E *= lam
                continue

            # RL policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # WM policy: if we have a remembered correct action for this state, choose it deterministically
            if wm_cache[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[wm_cache[s]] = 1.0
            else:
                p_wm = (1.0 / nA) * np.ones(nA)

            # Capacity-based WM gating by set size
            g = min(1.0, max(0.0, float(capacity_k) / float(set_size_t)))
            p = g * p_wm + (1.0 - g) * p_rl
            p = np.clip(p, 1e-12, 1.0)
            p /= np.sum(p)

            total_loglik += np.log(p[a])

            # RL update with eligibility traces
            # Decay all traces
            E *= lam
            # Increment eligibility for current (s,a)
            E[s, a] += 1.0
            # TD error on current choice
            delta = r - Q[s, a]
            # Update all Q via eligibility
            Q += alpha * delta * E

            # WM update only if rewarded: store the rewarded action as the "correct" for this state
            if r >= 1.0:
                wm_cache[s] = a

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning and age-by-load lapse, mixed into epsilon-greedy softmax.

    The model uses a baseline learning rate modulated by an asymmetry parameter to create
    different effective learning rates for positive vs. negative outcomes. Choice policy
    is a mixture of a softmax over Q-values and a uniform random lapse. The lapse rate
    increases with cognitive load in older adults.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within each block).
    actions : array-like of int
        Chosen actions per trial. Valid actions are {0,1,2}. Invalid actions are treated
        as uniform choices and do not drive learning.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Negative values treated as invalid (uniform; no update).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 defines older group.
    model_parameters : tuple/list of floats
        (alpha_base, beta, asym, eps0, chi)
        - alpha_base: baseline learning rate (0..1).
        - beta: softmax inverse temperature (>0).
        - asym: learning-rate asymmetry factor (-1..1); pos outcomes get alpha_base*(1+asym),
                neg outcomes get alpha_base*(1-asym), both clipped to [0,1].
        - eps0: baseline lapse logit (any real), mapped through sigmoid to (0,1).
        - chi: scales an age-by-load modulation of lapse; epsilon_t = sigmoid(eps0 + chi * is_older * (set_size - 3)).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta, asym, eps0, chi = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    # Precompute asymmetric learning rates (will be clipped again if needed)
    alpha_pos = np.clip(alpha_base * (1.0 + asym), 0.0, 1.0)
    alpha_neg = np.clip(alpha_base * (1.0 - asym), 0.0, 1.0)

    nA = 3
    total_loglik = 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = int(block_set_sizes[t])

            # Compute lapse probability for this trial
            eps_t = sigmoid(eps0 + chi * is_older * (float(set_size_t) - 3.0))

            if (a < 0) or (a >= nA) or (r < 0):
                # invalid: assume uniform
                total_loglik += -np.log(nA)
                continue

            # Softmax policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p_soft = np.exp(logits)
            p_soft /= np.sum(p_soft)

            # Epsilon-greedy mixture with uniform random choice
            p = (1.0 - eps_t) * p_soft + eps_t * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            p /= np.sum(p)

            total_loglik += np.log(p[a])

            # RL update with asymmetric learning
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated WMâ€“RL mixture with age-specific decision noise and WM decay.

    The model combines an RL softmax with a WM policy derived from a decaying memory trace.
    The mixture gate favors WM when WM confidence exceeds RL confidence (1 - normalized entropy).
    Age is used to set the decision temperature (beta) directly, allowing older adults to
    exhibit different choice stochasticity. WM traces decay over time, and correct rewards
    refresh the trace for the chosen action.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within each block).
    actions : array-like of int
        Chosen actions per trial. Valid actions are {0,1,2}. Invalid actions are treated
        as uniform choices and do not drive updates.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Negative values treated as invalid (uniform; no update).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; 3 or 6).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 defines older group.
    model_parameters : tuple/list of floats
        (alpha, beta_young, beta_old, decay_wm, eta)
        - alpha: RL learning rate (0..1).
        - beta_young: inverse temperature for younger adults (>0).
        - beta_old: inverse temperature for older adults (>0).
        - decay_wm: WM trace decay per trial (0..1); larger means faster forgetting.
        - eta: sensitivity of the mixture gate to the confidence difference (any real).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_y, beta_o, decay_wm, eta = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0
    beta = beta_o if is_older > 0.5 else beta_y

    nA = 3
    log_nA = np.log(nA)
    total_loglik = 0.0

    def softmax(qrow, beta_local):
        logits = beta_local * qrow
        logits -= np.max(logits)
        p = np.exp(logits)
        return p / np.sum(p)

    def entropy(p):
        p_safe = np.clip(p, 1e-12, 1.0)
        return -np.sum(p_safe * np.log(p_safe))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM traces per state-action: strengths in [0,1]
        M = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Decay all WM traces each trial
            M *= (1.0 - decay_wm)

            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # RL policy
            p_rl = softmax(Q[s, :], beta)
            H_rl = entropy(p_rl)
            c_rl = 1.0 - (H_rl / log_nA)  # normalized confidence in [0,1]

            # WM policy from normalized trace
            m_row = M[s, :].copy()
            if np.sum(m_row) > 0:
                p_wm = m_row / np.sum(m_row)
                c_wm = np.sum(m_row) / np.sum(np.ones_like(m_row))  # average strength (0..1)
            else:
                p_wm = (1.0 / nA) * np.ones(nA)
                c_wm = 0.0

            # Gate depends on confidence difference (WM vs RL)
            gate_logit = eta * (c_wm - c_rl)
            g = 1.0 / (1.0 + np.exp(-gate_logit))
            g = np.clip(g, 0.0, 1.0)

            p = g * p_wm + (1.0 - g) * p_rl
            p = np.clip(p, 1e-12, 1.0)
            p /= np.sum(p)

            total_loglik += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: reinforce trace strongly if reward; otherwise mild imprint
            if r >= 1.0:
                M[s, :] *= 0.0
                M[s, a] = 1.0
            else:
                # small imprint of the attempted action without reward
                M[s, a] = max(M[s, a], 0.1 * (1.0 - decay_wm))

    return -total_loglik