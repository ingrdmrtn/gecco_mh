def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, availability-based WM with age-modulated capacity and WM decay.

    Idea:
    - RL learns gradually from rewards.
    - WM stores state-action associations in a capacity-limited buffer: the probability that a state is
      successfully stored/retrieved scales with capacity K relative to set size (K/nS).
    - Older adults have reduced effective capacity (age_capacity_drop).
    - WM traces decay toward uniform at rate wm_decay.
    - Arbitration is availability-based: the probability to use WM on a trial equals the current WM
      availability strength m[s] for that state.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>=0).
    - K_base: Baseline WM capacity in "slots" (>=0).
    - age_capacity_drop: Proportion reduction of capacity for older adults (0..1).
    - wm_decay: Per-trial decay of WM trace and availability toward 0/uniform (0..1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list with 5 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, K_base, age_capacity_drop, wm_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM traces
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM availability strength per state (0..1); decays over time
        m = np.zeros(nS)

        # Effective capacity
        K_eff = max(0.0, float(K_base) * (1.0 - is_older * np.clip(age_capacity_drop, 0.0, 1.0)))
        # Probability that a given state can be stably stored/retrieved in WM
        store_prob = min(1.0, K_eff / max(float(nS), 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Availability-based arbitration
            p_use_wm = np.clip(m[s], 0.0, 1.0)
            p_total = p_use_wm * p_wm + (1.0 - p_use_wm) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update and decay
            # Decay WM trace and availability each trial
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            m[s] = (1.0 - wm_decay) * m[s]

            # If rewarded, attempt to store deterministically (one-shot), gated by capacity-based probability
            if r > 0.5:
                # Stochastic storage into WM based on capacity; we approximate by increasing availability by store_prob
                # and replacing the WM trace toward the one-hot action proportional to store_prob.
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - store_prob) * w[s, :] + store_prob * target
                # Boost availability toward 1 by store_prob
                m[s] = np.clip(m[s] + store_prob * (1.0 - m[s]), 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-gated arbitration and memory strength, with age- and load-dependent scaling.

    Idea:
    - RL learns from rewards; policy uncertainty (entropy) modulates reliance on WM.
    - WM stores rewarded actions (one-shot) and maintains a per-state memory strength that decays over time.
    - WM mixture weight is a product of:
        (a) baseline wm_weight_base,
        (b) load attenuation (larger set size reduces WM reliance),
        (c) age attenuation for older adults,
        (d) memory strength for this state,
        (e) an entropy gate that downweights WM when RL is highly uncertain.
      Together, this yields dynamic arbitration that tracks both internal uncertainty and memory availability.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>=0).
    - wm_weight_base: Baseline WM mixture weight (0..1).
    - entropy_gate: Controls sensitivity to RL entropy when gating WM (>=0).
    - mem_decay: Per-trial decay of WM memory strength (0..1).

    Age and load effects:
    - WM weight is reduced by load via 1/(1 + max(0, nS-3)).
    - WM weight is reduced further for older adults by a fixed factor derived from entropy_gate.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list with 5 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, entropy_gate, mem_decay = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state memory strength (0..1)
        mem = np.zeros(nS)

        # Load attenuation: 1 for set size 3, 1/4 for set size 6
        load_factor = 1.0 / (1.0 + max(0.0, float(nS) - 3.0))

        # Age attenuation: older adults get an extra multiplicative penalty derived from entropy_gate
        age_factor = 1.0 / (1.0 + is_older * np.clip(entropy_gate, 0.0, 10.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probabilities (for entropy calculation)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits)
            pi = pi / max(np.sum(pi), eps)

            # Entropy of RL policy (0..log nA). Normalize to 0..1 by dividing by log(nA).
            H = -np.sum(pi * np.log(np.clip(pi, eps, 1.0)))
            H_norm = H / np.log(nA)

            # RL choice probability of observed action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Entropy gate: reduce WM reliance when RL is highly uncertain
            gate = np.exp(-np.clip(entropy_gate, 0.0, 10.0) * H_norm)

            # Dynamic WM weight
            wm_weight = np.clip(wm_weight_base * load_factor * age_factor * gate * np.clip(mem[s], 0.0, 1.0), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM memory strength decay
            mem[s] = (1.0 - mem_decay) * mem[s]

            # WM update: if rewarded, one-shot store and strengthen memory
            if r > 0.5:
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W
                # Increase memory strength toward 1
                mem[s] = np.clip(mem[s] + (1.0 - mem[s]) * 1.0, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-scaled learning effectiveness + WM persistence and arbitration.

    Idea:
    - RL uses a single learning rate but its effective impact is scaled by kappa that declines
      with higher load and for older adults (capturing reduced learning effectiveness).
    - WM stores rewarded actions (one-shot) and persists across trials with partial decay (wm_persist).
    - Arbitration uses a baseline WM mixture weight scaled by load and age, and boosted by the
      current WM certainty (sharply peaked WM vector).

    Parameters (model_parameters):
    - lr_base: Base RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 (>=0).
    - wm_weight_base: Baseline WM mixture weight (0..1).
    - kappa_scale_age: Multiplicative reduction in learning effectiveness for older adults (0..1).
    - kappa_scale_load: Reduction per extra item beyond 3 (>=0).
    - wm_persist: Per-trial persistence of WM trace (0..1), where 1 = no decay, 0 = full reset to uniform.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list with 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, softmax_beta, wm_weight_base, kappa_scale_age, kappa_scale_load, wm_persist = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-based scaling factor for learning effectiveness (kappa)
        extra = max(0.0, float(nS) - 3.0)
        load_scale = 1.0 / (1.0 + np.clip(kappa_scale_load, 0.0, 10.0) * extra)
        age_scale = 1.0 - is_older * np.clip(kappa_scale_age, 0.0, 1.0)
        kappa = np.clip(load_scale * age_scale, 0.0, 1.0)

        # WM mixture scaling by load and age
        wm_age_scale = age_scale  # reuse same age_scale to reduce WM for older adults
        wm_load_scale = load_scale
        wm_base = np.clip(wm_weight_base * wm_age_scale * wm_load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # WM certainty: how peaked is W_s vs uniform
            uniform = w_0[s, :]
            wm_certainty = np.clip(0.5 * (np.max(W_s) - np.mean(uniform)) * 2.0, 0.0, 1.0)

            wm_weight = np.clip(wm_base * (0.5 + 0.5 * wm_certainty), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with effectiveness kappa
            delta = r - Q_s[a]
            q[s, a] += (lr_base * kappa) * delta

            # WM persistence/decay each visit
            w[s, :] = wm_persist * w[s, :] + (1.0 - wm_persist) * w_0[s, :]

            # If rewarded, refresh WM to one-hot for chosen action (one-shot)
            if r > 0.5:
                new_W = np.zeros(nA)
                new_W[a] = 1.0
                w[s, :] = new_W

        blocks_log_p += log_p

    return -blocks_log_p