def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Dual-rate RL + WM mixture with set-size–specific WM weight and WM leak.
    - RL: Q-learning with asymmetric learning rates for positive/negative prediction errors.
    - WM: decays toward uniform within-state; rewarded actions are stored/sharpened.
    - Arbitration: WM weight explicitly depends on set size via two separate weights
      for low load (set size 3) and high load (set size 6).

    Parameters:
      lr_pos:        RL learning rate for positive PE (>=0, <=1)
      lr_neg:        RL learning rate for negative PE (>=0, <=1)
      softmax_beta:  RL inverse temperature (scaled internally by 10; >=0)
      wm_w_small:    WM mixture weight when set size is small (3) (0..1)
      wm_w_large:    WM mixture weight when set size is large (6) (0..1)
      wm_decay:      WM decay toward uniform within-state each encounter (0..1);
                     also sets storage strength via store_strength = clip(2*wm_decay,0,1)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_w_small, wm_w_large, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–specific WM mixture weight
        if nS <= 3:
            wm_weight_eff = np.clip(wm_w_small, 0.0, 1.0)
        elif nS >= 6:
            wm_weight_eff = np.clip(wm_w_large, 0.0, 1.0)
        else:
            # linear interpolation between the two (generalizes beyond 3/6)
            alpha = (nS - 3) / max(1.0, (6 - 3))
            wm_weight_eff = np.clip((1 - alpha) * wm_w_small + alpha * wm_w_large, 0.0, 1.0)

        store_strength = np.clip(2.0 * max(0.0, wm_decay), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of the chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM state-action weights (sharp/deterministic)
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM decay toward uniform, then store rewarded action strongly
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                # sharpen distribution toward chosen action
                w[s, :] = (1.0 - store_strength) * w[s, :]
                w[s, a] += store_strength
                # renormalize
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Capacity-limited WM cache with recency/robustness-based replacement.
    - RL: Standard Q-learning with softmax.
    - WM: A cache can store up to C states. If current state s is cached, WM policy is used;
           otherwise WM contributes only a uniform prior. Encoding occurs on rewarded trials.
           Replacement on overflow trades off recency and per-state success history.
    - Arbitration: Mixture weight scaled by proportion of set covered by capacity (cap/nS).

    Parameters:
      lr:               RL learning rate (0..1)
      softmax_beta:     RL inverse temperature (scaled internally by 10; >=0)
      wm_weight_base:   Base WM mixture weight before capacity scaling (0..1)
      wm_capacity:      WM capacity in number of states (continuous; rounded to int 0..6)
      replace_bias:     Replacement trade-off (0..1): 0=evict least recent, 1=evict least successful
      encode_strength:  Strength (0..1) of sharpening toward rewarded action when encoded

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, wm_capacity, replace_bias, encode_strength = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # WM probabilities per state if cached
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM cache bookkeeping
        cap = int(np.clip(np.round(wm_capacity), 0, nS))
        in_cache = np.zeros(nS, dtype=bool)
        last_seen = -1 * np.ones(nS)  # recency timestamps
        success_hist = np.zeros(nS)   # cumulative rewarded encounters for replacement criterion
        t_global = 0

        # Effective mixture weight scales with fraction of set that can be held
        coverage = 0.0 if nS == 0 else (cap / float(nS))
        wm_weight_eff = np.clip(wm_weight_base * coverage, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):
            t_global += 1

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: use cached WM if present, otherwise uniform
            if in_cache[s]:
                wm_logits = w[s, :]
            else:
                wm_logits = w_0[s, :]  # uniform proxy when not cached

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update recency always
            last_seen[s] = t_global

            # WM encoding and cache management: only on rewarded trials
            if r > 0:
                success_hist[s] += 1

                if not in_cache[s]:
                    # If cache has room, add; else evict based on combined priority
                    if np.sum(in_cache) < cap:
                        in_cache[s] = True
                        w[s, :] = w_0[s, :].copy()
                    else:
                        if cap > 0:
                            # compute eviction scores for cached states
                            cached_indices = np.where(in_cache)[0]
                            # lower score => worse, to be evicted
                            # recency score: larger is better; invert by subtracting min
                            if cached_indices.size > 0:
                                rec_vals = last_seen[cached_indices]
                                rec_norm = (rec_vals - np.min(rec_vals)) / (np.max(rec_vals) - np.min(rec_vals) + 1e-8)
                                suc_vals = success_hist[cached_indices]
                                suc_norm = (suc_vals - np.min(suc_vals)) / (np.max(suc_vals) - np.min(suc_vals) + 1e-8)
                                priority = (1 - replace_bias) * rec_norm + replace_bias * suc_norm
                                evict_idx = cached_indices[np.argmin(priority)]
                                in_cache[evict_idx] = False
                            in_cache[s] = True
                            w[s, :] = w_0[s, :].copy()

                # Sharpen WM distribution for this state toward chosen action
                if in_cache[s]:
                    w[s, :] = (1.0 - encode_strength) * w[s, :]
                    w[s, a] += encode_strength
                    w[s, :] = np.clip(w[s, :], 1e-12, None)
                    w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Uncertainty-weighted arbitration between RL and WM with load penalty.
    - RL: Q-learning with softmax; maintains per-state visit counts as a proxy for RL certainty.
    - WM: Dirichlet memory over actions per state; rewarded trials increase the concentration
           on the chosen action, yielding a categorical WM policy.
    - Arbitration: Trial-wise weight determined by relative confidence (WM vs. RL), penalized
                   under higher set size.

    Parameters:
      lr:                  RL learning rate (0..1)
      softmax_beta:        RL inverse temperature (scaled internally by 10; >=0)
      wm_conc0:            Initial Dirichlet concentration per action for WM (>=0); higher => less confident initially
      arb_slope:           Slope (>0) mapping confidence difference into WM weight (sigmoid)
      init_q_uncertainty:  Pseudo-count controlling initial RL uncertainty (>=0)
      load_penalty:        Linear penalty factor (>0) applied to WM weight with increasing set size

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_conc0, arb_slope, init_q_uncertainty, load_penalty = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM as Dirichlet concentrations per state-action
        w_alpha = np.full((nS, nA), max(1e-8, wm_conc0), dtype=float)
        w = np.zeros((nS, nA))  # will hold mean probs from Dirichlet when needed
        w_0 = (1 / nA) * np.ones((nS, nA))

        # RL uncertainty proxy: state visit counts
        visit_counts = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM expected probabilities from Dirichlet (mean of categorical)
            alpha_s = w_alpha[s, :]
            alpha_sum = np.sum(alpha_s)
            if alpha_sum <= 0:
                W_s = w_0[s, :]
            else:
                W_s = alpha_s / alpha_sum

            # WM policy (deterministic softmax over WM mean probs)
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Confidence estimates
            # RL confidence increases with visit count
            rl_conf = 1.0 - (1.0 / (visit_counts[s] + init_q_uncertainty + 1.0))
            rl_conf = np.clip(rl_conf, 0.0, 1.0)
            # WM confidence increases with total concentration beyond the prior
            wm_conf = 0.0
            if alpha_sum > 0:
                wm_conf = 1.0 - (wm_conc0 * nA) / (alpha_sum + wm_conc0 * nA)
                wm_conf = np.clip(wm_conf, 0.0, 1.0)

            # Arbitration weight with load penalty
            # Base WM weight from sigmoid of confidence difference
            base_w = 1.0 / (1.0 + np.exp(-arb_slope * (wm_conf - rl_conf)))
            # Penalize WM under larger set sizes linearly
            wm_weight_eff = np.clip(base_w - load_penalty * max(0, (nS - 3)) / 3.0, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            visit_counts[s] += 1

            # WM update: increase concentration mostly on rewarded chosen action
            if r > 0:
                w_alpha[s, a] += 1.0
            else:
                # small non-reward update to keep WM calibrated (weak evidence against chosen action)
                # distribute a tiny mass to nonchosen actions
                if nA > 1:
                    w_alpha[s, :] += (0.1 / (nA - 1))
                    w_alpha[s, a] -= 0.1  # net zero-sum tweak
                    w_alpha[s, :] = np.clip(w_alpha[s, :], 1e-8, None)

        blocks_log_p += log_p

    return -blocks_log_p