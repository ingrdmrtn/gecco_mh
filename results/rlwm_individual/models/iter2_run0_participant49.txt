def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-learning of learning rate and a decaying one-shot WM cache, age- and load-gated.

    Mechanism
    - RL values Q(s,a) updated with a dynamic learning rate alpha_t that increases with recent surprise
      (absolute PE), implementing a simple meta-learning of learning rate.
    - A WM cache W(s,a) encodes a one-shot memory of the last rewarded action in each state with
      strength that decays over time.
    - Policy is a mixture of RL softmax and WM softmax. The WM weight is reduced by set size (load)
      and further reduced for older adults.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) reduces WM influence.
    model_parameters : sequence of 6 floats
        alpha0         : Baseline learning rate for RL (0..1)
        beta           : Inverse temperature for both systems (scaled by 10 internally)
        wm_strength0   : Baseline WM mixture weight (0..1 in logistic space internally)
        wm_decay       : Decay (leak) of WM cache per encounter (0..1)
        age_wm_penalty : Reduction to WM gate for older adults (>=0)
        meta_eta       : Sensitivity of learning-rate to surprise |PE| (>=0)

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed actions.
    """
    alpha0, beta, wm_strength0, wm_decay, age_wm_penalty, meta_eta = model_parameters
    beta = beta * 10.0

    # fetch age and group
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    # logistic helper for parameters intended to be in (0,1)
    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    # keep alpha0 within (0,1) via logistic transform around 0.5 baseline if provided unconstrained
    alpha0_eff = np.clip(alpha0, 0.0, 1.0)
    wm0_gate = logistic(wm_strength0)  # interpret as logit space input

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = np.zeros((nS, nA))  # WM strengths per action; boosted to 1 on rewarded action
        last_pe_abs = 0.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy: prefer actions with higher WM strength
            w_s = W[s, :].copy()
            w_s = w_s - np.max(w_s)
            p_wm = np.exp(beta * w_s)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Gate: reduce WM by load and age
            # Load factor: 1.0 at set size 3, 0.0 at set size 6 (linear)
            load_gate = max(0.0, min(1.0, 1.0 - (nS - 3) / 3.0))
            # Age penalty reduces WM gating for older adults
            age_gate = 1.0 - age_wm_penalty * is_older
            age_gate = min(max(age_gate, 0.0), 1.0)
            gate = wm0_gate * load_gate * age_gate
            gate = min(max(gate, 0.0), 1.0)

            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            pa = p_mix[a]
            total_log_p += np.log(max(pa, eps))

            # Compute PE and dynamic learning rate
            pe = r - Q[s, a]
            # learning rate increases with surprise (previous abs PE)
            # map meta_eta linearly through logistic to keep within (0,1)
            alpha_t = alpha0_eff + (1.0 - alpha0_eff) * (1.0 - np.exp(-meta_eta * max(last_pe_abs, 0.0)))
            alpha_t = min(max(alpha_t, 0.0), 1.0)

            # RL update
            Q[s, a] += alpha_t * pe

            # WM update: decay and then set chosen action to r if rewarded, else small trace
            W[s, :] *= (1.0 - wm_decay)
            if r > 0.5:
                # store strong memory of rewarded action
                W[s, a] = 1.0
            else:
                # encode weak negative trace to discourage via WM
                W[s, a] = max(W[s, a], 0.0) * (1.0 - wm_decay)

            last_pe_abs = abs(pe)

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with WM-based Win-Stay/Lose-Shift bias and age/load-dependent lapses.

    Mechanism
    - Actor-Critic:
        Actor preferences P(s,a) updated toward chosen action by TD error from a Critic V(s).
    - WM Heuristic:
        Per state, if last outcome was a win, bias to repeat last action; if loss, bias to shift.
        This bias is injected additively into the actor preferences before softmax.
    - Lapse probability:
        With epsilon (dependent on age and load), choose uniformly at random among actions.
        Older adults and larger set size increase lapse probability.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) increases lapses.
    model_parameters : sequence of 6 floats
        alpha_c        : Critic learning rate (0..1)
        alpha_a        : Actor learning rate (0..1)
        beta           : Inverse temperature for actor softmax (scaled by 10)
        lapse_base     : Baseline logit of lapse probability
        age_lapse_gain : Additive effect on lapse logit for older adults (>=0 increases lapses)
        load_gain      : Additive effect on lapse logit per (+3) increase in set size (>=0 increases lapses)

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed actions.
    """
    alpha_c, alpha_a, beta, lapse_base, age_lapse_gain, load_gain = model_parameters
    beta = beta * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    # WM heuristic memory per state
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize Actor-Critic and WM traces
        P = np.zeros((nS, nA))  # actor preferences
        V = np.zeros(nS)        # critic values
        last_action = -np.ones(nS, dtype=int)  # last action taken in this state
        last_reward = np.zeros(nS)             # last reward in this state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute lapse probability epsilon with logistic link
            load_term = load_gain * ((nS - 3.0) / 3.0)  # 0 at 3, +load_gain at 6
            age_term = age_lapse_gain * is_older
            lapse_logit = lapse_base + age_term + load_term
            epsilon = 1.0 / (1.0 + np.exp(-lapse_logit))
            epsilon = min(max(epsilon, 0.0), 1.0)

            # Actor softmax policy with WM heuristic bias
            prefs = P[s, :].copy()

            # WM Win-Stay/Lose-Shift bias implemented as additive bump
            wm_bias = np.zeros(nA)
            if last_action[s] >= 0:
                if last_reward[s] >= 0.5:
                    # win: bias repeating last action
                    wm_bias[last_action[s]] += 1.0
                else:
                    # loss: bias shifting away; distribute bias to other actions
                    for aa in range(nA):
                        if aa != last_action[s]:
                            wm_bias[aa] += 0.5  # split shift bias
            # Add WM bias to preferences
            prefs = prefs + wm_bias

            # Softmax over biased preferences
            prefs = prefs - np.max(prefs)
            pi = np.exp(beta * prefs)
            pi = pi / (np.sum(pi) + eps)

            # Final policy with lapses
            p_final = (1.0 - epsilon) * pi + epsilon * (1.0 / nA)
            pa = p_final[a]
            total_log_p += np.log(max(pa, eps))

            # TD error and updates
            delta = r - V[s]
            V[s] += alpha_c * delta

            # Actor update: move preference toward chosen action proportional to delta
            # Softmax gradient approximation: increase chosen preference, decrease others
            for aa in range(nA):
                if aa == a:
                    P[s, aa] += alpha_a * delta * (1.0 - pi[aa])
                else:
                    P[s, aa] -= alpha_a * delta * (pi[aa])

            # Update WM heuristic traces
            last_action[s] = a
            last_reward[s] = r

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with eligibility traces and unchosen-action decay; age/load modulate temperature.

    Mechanism
    - Standard Q-learning augmented with:
        (a) Eligibility traces across actions within the same state to spread credit locally.
        (b) Decay (forgetting) of unchosen actions in the current state toward the average value.
    - Decision temperature decreases with effective control; older age and higher load increase noise.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like or scalar
        Participant age; older (>=45) increases decision noise (reduces beta).
    model_parameters : sequence of 6 floats
        alpha             : Learning rate for Q (0..1)
        beta0             : Baseline inverse temperature (scaled by 10)
        lambda_tr         : Eligibility trace strength across actions in same state (0..1)
        decay_unchosen    : Decay rate for unchosen actions in visited state (0..1)
        age_temp_slope    : Reduction of effective beta for older adults (>=0)
        load_temp_slope   : Reduction of effective beta per (+3) increase in set size (>=0)

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions.
    """
    alpha, beta0, lambda_tr, decay_unchosen, age_temp_slope, load_temp_slope = model_parameters
    beta0 = beta0 * 10.0

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Effective temperature with age and load reductions
            load_level = (nS - 3.0) / 3.0  # 0 at 3, 1 at 6
            beta_eff = beta0 * np.exp(-age_temp_slope * is_older - load_temp_slope * load_level)

            # Policy
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)
            pi = np.exp(beta_eff * q_s)
            pi = pi / (np.sum(pi) + eps)
            pa = pi[a]
            total_log_p += np.log(max(pa, eps))

            # TD error using Q(s,a)
            pe = r - Q[s, a]

            # Eligibility traces across actions within the same state:
            # chosen action gets 1, others get lambda_tr normalized
            elig = np.full(nA, lambda_tr / max(nA - 1, 1))
            elig[a] = 1.0

            # Update all actions in the state using eligibilities
            Q[s, :] += alpha * pe * elig

            # Decay unchosen actions in the current state toward mean value (stabilize learning)
            avg_q = np.mean(Q[s, :])
            for aa in range(nA):
                if aa != a:
                    Q[s, aa] += decay_unchosen * (avg_q - Q[s, aa])

    return -float(total_log_p)