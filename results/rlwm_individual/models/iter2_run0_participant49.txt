def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with load-scaled WM weight.

    Idea:
    - Choices are a mixture of an RL softmax policy and a WM softmax policy.
    - WM has limited capacity: its contribution is down-weighted as set size grows,
      via an effective WM weight wm_weight_eff = wm_weight_base * min(1, K_wm / nS).
    - WM stores rewarded actions with strength wm_strength (as a one-hot distribution).
      No explicit WM decay is modeled; interference is captured by the capacity scaling.
    - RL learns with a single learning rate lr.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate (state-action Rescorla-Wagner).
    - wm_weight_base: float in [0,1], base mixture weight for WM.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - K_wm: float >= 0, WM capacity in number of items; reduces WM influence when nS > K_wm.
    - wm_strength: float in [0,1], strength with which rewarded actions are stored in WM.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, K_wm, wm_strength = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Capacity-adjusted WM weight
        cap_factor = min(1.0, max(0.0, float(K_wm) / max(1.0, float(nS))))
        wm_weight_eff = wm_weight_base * cap_factor

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: store rewarded action with strength wm_strength
            if r > 0.0:
                w[s, :] = (1.0 - wm_strength) * w_0[s, :]
                w[s, a] += wm_strength
            # If no reward, leave WM as is (no explicit decay; load is handled via capacity scaling)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall attention + PE-gated WM with leak.

    Idea:
    - RL learning rate is state-specific and tracks surprise (absolute prediction error),
      following a Pearce-Hall style update of attention.
    - WM contributes via a mixture whose effective weight increases with surprise
      (large unsigned PE => WM reliance increases).
    - WM representation leaks toward uniform with rate wm_decay; rewarded outcomes
      write a strong one-hot memory for that state.

    Parameters (model_parameters):
    - lr0: float in [0,1], base RL learning-rate scale.
    - wm_weight_base: float in [0,1], base WM mixture weight (gated by surprise).
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - k_pe: float >= 0, sensitivity of WM weight to unsigned prediction error.
            Effective WM weight = wm_weight_base * (1 - exp(-k_pe * |PE|)).
    - phi_attn: float in [0,1], update rate for Pearce-Hall attention (per state).
                alpha_s <- (1-phi_attn)*alpha_s + phi_attn*|PE|, and lr_s = lr0 * alpha_s.
    - wm_decay: float in [0,1], leak of WM per visit toward uniform.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr0, wm_weight_base, softmax_beta, k_pe, phi_attn, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Initialize Pearce-Hall attention per state
        alpha_s = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture weight gated by unsigned PE (computed using current Q)
            pe_unsigned = abs(r - Q_s[a])
            wm_gate = 1.0 - np.exp(-k_pe * pe_unsigned)
            wm_weight_eff = wm_weight_base * wm_gate

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall attention
            delta = r - Q_s[a]
            alpha_s[s] = (1.0 - phi_attn) * alpha_s[s] + phi_attn * abs(delta)
            lr_s = lr0 * alpha_s[s]
            q[s, a] += lr_s * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM write on rewarded outcomes
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-specific learning + Q decay, and load-dependent WM precision.

    Idea:
    - RL uses separate learning rates for positive vs. negative prediction errors.
    - Q-values for the visited state decay toward a uniform prior each encounter (to capture interference).
    - WM contributes via a mixture, but its precision (inverse temperature) declines with load.
      Effective WM beta: beta_wm_eff = softmax_beta_wm / (1 + gamma_wm * (nS - 3)/3).
    - Mixture weight wm_weight is constant across loads; the policy precision change captures load effects.

    Parameters (model_parameters):
    - lr_pos: float in [0,1], RL learning rate for positive PE (delta > 0).
    - lr_neg: float in [0,1], RL learning rate for negative PE (delta < 0).
    - wm_weight: float in [0,1], mixture weight for WM.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - gamma_wm: float >= 0, load sensitivity of WM precision; higher => noisier WM at larger set sizes.
    - q_decay: float in [0,1], per-visit decay of Q(s,Â·) toward uniform.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, gamma_wm, q_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent WM precision
        load_term = max(0.0, (nS - 3) / 3.0)
        beta_wm_eff = softmax_beta_wm / (1.0 + gamma_wm * load_term)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Q decay toward uniform prior for the visited state
            q[s, :] = (1.0 - q_decay) * q[s, :] + q_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy with load-reduced precision
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with valence-specific learning rates
            delta = r - Q_s[a]
            if delta >= 0.0:
                q[s, a] += lr_pos * delta
            else:
                q[s, a] += lr_neg * delta

            # WM update: reward-dependent write
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p