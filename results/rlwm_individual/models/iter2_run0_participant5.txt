def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Probabilistic slot-based Working Memory with capacity limits and decay.

    Mechanism
    - RL: standard delta-rule with softmax policy.
    - WM: a limited-capacity store with K slots per block. Each state has a probability m_in[s]
      of being in WM. If in WM and has a stored action, WM proposes a near-deterministic policy
      for that action; otherwise near-uniform with small noise epsilon.
      Capacity pressure (nS > K) reduces inclusion probability; memory membership and strength
      also decay between visits (state-specific).

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for Q-values.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - K_slots: float >= 0
            Effective WM capacity (in number of states); larger values allow more states to be
            maintained with high probability.
        - decay: float in [0,1]
            Per-encounter decay of WM membership probability and memory strength.
        - epsilon: float in [0, 0.5)
            WM policy noise; probability mass that is spread over non-remembered actions.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, K_slots, decay, epsilon = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # will hold WM policy W_s each step
        w_0 = (1 / nA) * np.ones((nS, nA)) # prior/uniform WM policy

        # WM latent: membership probability and stored action strength per state
        m_in = np.minimum(1.0, (K_slots / np.maximum(nS, eps))) * np.ones(nS)
        strength = np.zeros((nS, nA))  # confidence over actions if in WM

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Build WM policy for current state
            # Normalize strength to distribution; if no strength, use uniform
            if np.sum(strength[s, :]) > 0:
                mem_pref = strength[s, :] / np.maximum(np.sum(strength[s, :]), eps)
            else:
                mem_pref = w_0[s, :]

            # Near-deterministic toward memory, but allow noise epsilon
            # and scale by membership probability (expected WM policy)
            wm_det = (1 - epsilon) * mem_pref + epsilon * (1.0 / nA)
            W_s = m_in[s] * wm_det + (1 - m_in[s]) * w_0[s, :]
            w[s, :] = W_s  # store for policy layer

            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay of membership and strength (state-specific, per visit)
            m_in[s] = (1 - decay) * m_in[s]
            strength[s, :] *= (1 - decay)

            # WM encoding/update based on feedback
            if r > 0:
                # Encode rewarded action strongly; suppress others slightly
                strength[s, a] += 1.0
                others = np.arange(nA) != a
                strength[s, others] *= (1 - 0.25)
                # Increase membership probability when we receive a reward
                m_in[s] = np.clip(m_in[s] + 0.5 * (K_slots / np.maximum(nS, eps)), 0.0, 1.0)
            else:
                # On error, weakly encode "avoid chosen" by distributing a small mass to others
                others = np.arange(nA) != a
                strength[s, others] += 0.05

            # Keep WM policy up to date
            if np.sum(strength[s, :]) > 0:
                mem_pref = strength[s, :] / np.maximum(np.sum(strength[s, :]), eps)
            else:
                mem_pref = w_0[s, :]
            wm_det = (1 - epsilon) * mem_pref + epsilon * (1.0 / nA)
            W_s = m_in[s] * wm_det + (1 - m_in[s]) * w_0[s, :]
            w[s, :] = W_s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Episodic-recall Working Memory with lag-dependent accessibility and misbinding.

    Mechanism
    - RL: standard delta-rule with softmax.
    - WM: each state stores the most recent rewarded action and its recency (lag since last reward).
      Recall probability increases with capacity per set size and decays exponentially with lag.
      When recalled, WM proposes a near-deterministic policy for the stored action, with a small
      misbinding probability that places mass on non-stored actions.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for Q-values.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - capacity: float >= 0
            Scales baseline recall availability as min(1, capacity / set_size).
        - tau: float > 0
            Time constant controlling exponential decay of recall with lag (trials since last reward).
        - misbind: float in [0, 0.5)
            Probability mass diverted from the stored action to other actions when recall occurs.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, capacity, tau, misbind = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))   # will hold WM policy W_s each step
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM store: last rewarded action and lag since that reward per state
        last_rewarded = -1 * np.ones(nS, dtype=int)
        lag = np.full(nS, np.inf)

        # Baseline recall availability given set size
        base_recall = min(1.0, capacity / max(nS, 1))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Compute recall probability for this state
            # If never rewarded, effective lag is infinite -> recall probability -> 0
            if np.isfinite(lag[s]):
                p_recall = base_recall * np.exp(-lag[s] / max(tau, eps))
            else:
                p_recall = 0.0
            p_recall = np.clip(p_recall, 0.0, 1.0)

            # Build WM policy W_s
            if last_rewarded[s] >= 0 and p_recall > 0:
                stored_a = last_rewarded[s]
                wm = np.full(nA, misbind / (nA - 1))
                wm[stored_a] = 1.0 - misbind
                W_s = p_recall * wm + (1 - p_recall) * w_0[s, :]
            else:
                W_s = w_0[s, :]
            w[s, :] = W_s

            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update WM episodic store and lags
            # Increase lags for all states by 1 on each trial (implicit); we track only current s
            if np.isfinite(lag[s]):
                lag[s] += 1.0  # increment lag since last reward for this state
            # On reward, set episodic memory for this state
            if r > 0:
                last_rewarded[s] = a
                lag[s] = 0.0  # reset lag

            # Keep WM policy array synchronized (optional stabilization)
            w[s, :] = W_s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Fast-decaying WM value system (dual Q) with set-size-dependent forgetting and valence-asymmetric WM learning.

    Mechanism
    - RL: slow delta-rule Q-values with softmax.
    - WM: separate fast value table W with near-deterministic policy (softmax_beta_wm).
      WM learns quickly from outcomes with asymmetric learning rates for positive/negative feedback
      and forgets toward a uniform baseline faster at larger set sizes.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for Q-values (slow system).
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - lr_wm_pos: float in [0,1]
            WM learning rate for positive outcomes.
        - lr_wm_neg: float in [0,1]
            WM learning rate for negative outcomes.
        - decay_slope: float in [0,1]
            Scales WM forgetting with set size: decay = clip(decay_slope * (nS - 1) / nS, 0, 1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, lr_wm_pos, lr_wm_neg, decay_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))    # RL values
        w = (1 / nA) * np.ones((nS, nA))    # WM fast values/policy table
        w_0 = (1 / nA) * np.ones((nS, nA))  # WM baseline toward which it forgets

        # Set-size dependent decay toward baseline
        decay = np.clip(decay_slope * (nS - 1) / max(nS, 1), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Construct WM policy from current WM values (normalized to [0,1] via softmax-like map)
            # Here we simply use the values as preferences; normalize to a distribution
            W_s_vals = w[s, :]
            # Convert to a proper distribution (avoid degeneracy)
            if np.allclose(W_s_vals, W_s_vals[0]):
                W_s = w_0[s, :].copy()
            else:
                # Soft normalization by exponentiating centered preferences
                prefs = W_s_vals - np.mean(W_s_vals)
                exp_prefs = np.exp(np.clip(prefs, -10, 10))
                W_s = exp_prefs / np.maximum(np.sum(exp_prefs), eps)
            w[s, :] = W_s  # keep w as policy probabilities

            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM forgetting toward baseline
            w[s, :] = (1 - decay) * w[s, :] + decay * w_0[s, :]

            # WM learning (fast, valence-asymmetric)
            # Convert current W_s (probabilities) to pseudo-values by moving mass toward chosen action
            if r > 0:
                # Increase chosen action probability and renormalize
                w[s, a] += lr_wm_pos * (1.0 - w[s, a])
                others = np.arange(nA) != a
                w[s, others] *= (1.0 - lr_wm_pos)
            else:
                # Decrease chosen action probability; shift mass to others
                reduction = lr_wm_neg * w[s, a]
                w[s, a] -= reduction
                others = np.arange(nA) != a
                if np.sum(w[s, others]) > 0:
                    w[s, others] += reduction * (w[s, others] / np.maximum(np.sum(w[s, others]), eps))
                else:
                    w[s, others] += reduction / (nA - 1)

            # Ensure valid probability distribution
            w[s, :] = np.clip(w[s, :], eps, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p