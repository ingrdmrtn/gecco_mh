def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + interference-driven WM + error-gated arbitration + lapse.

    Idea:
    - RL uses standard delta learning on Q-values (softmax with inverse temperature).
    - WM stores one-shot associative weights per state; when a new state is first encountered,
      it causes interference that decays WM for all other states toward uniform (retroactive interference).
    - Arbitration weight for WM is reduced on surprising trials: larger current RL PE -> less reliance on WM.
    - A small lapse epsilon mixes a uniform random choice into the final policy.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline mixture weight for WM contribution (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10).
    - interference_rate: Magnitude of WM decay applied to all other states when a new state is introduced (0..1).
    - error_sensitivity: Scales the reduction of WM arbitration as a function of current |PE| (>=0).
    - lapse_eps: Lapse rate mixed with uniform choice (0..0.2 suggested).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, interference_rate, error_sensitivity, lapse_eps = model_parameters
    softmax_beta *= 10  # higher upper bound per template
    softmax_beta_wm = 50  # very deterministic WM per template

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        seen_state = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM association strengths for the current state
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Arbitration: error-gated reduction of WM weight based on current PE magnitude
            pe_now = r - Q_s[a]
            wm_weight_t = wm_weight_base * np.exp(-error_sensitivity * abs(pe_now))
            wm_weight_t = max(0.0, min(1.0, wm_weight_t))

            # Mixture with lapse
            p_mix = p_wm * wm_weight_t + (1.0 - wm_weight_t) * p_rl
            p_total = (1.0 - lapse_eps) * p_mix + lapse_eps * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL learning
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating
            # - On reward: write one-shot association toward the chosen action.
            # - On new-state encounter: apply interference decay to all other states.
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot  # moderate-strength write
            else:
                # small passive drift toward uniform when no reward
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Interference: when encountering a never-before-seen state, decay WM for all other states
            is_new_state = not seen_state[s]
            if is_new_state:
                other_idx = np.arange(nS) != s
                w[other_idx, :] = (1.0 - interference_rate) * w[other_idx, :] + interference_rate * w_0[other_idx, :]
                seen_state[s] = True

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WSLS-style WM + surprise-based arbitration.

    Idea:
    - RL is standard delta learner with softmax policy.
    - WM implements a win-stay/lose-shift rule at the state level:
      when rewarded, it stores the chosen action deterministically; when not rewarded, it clears toward uniform.
    - Arbitration weight for WM is modulated by RL surprise (|PE|): more surprise -> rely less on WM.
      A sigmoid transform allows flexible mapping from |PE| to WM weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL (scaled internally by 10).
    - ws_strength: Strength of WM write on reward (0..1); higher -> more deterministic WSLS.
    - pe_slope: Slope of sigmoid mapping from |PE| to arbitration (>=0).
    - pe_bias: Bias of the sigmoid mapping (can be positive or negative).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, ws_strength, pe_slope, pe_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # deterministic WM

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM distribution w[s], intended to approximate WSLS
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Arbitration: WM weight decreases with |PE| via a sigmoid transform
            pe = r - Q_s[a]
            # map |pe| to [0,1] via sigmoid, then combine with base weight multiplicatively
            wm_mod = 1.0 / (1.0 + np.exp(pe_slope * (abs(pe) - pe_bias)))
            wm_weight_t = wm_weight_base * wm_mod
            wm_weight_t = max(0.0, min(1.0, wm_weight_t))

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update implementing WSLS:
            # - On reward: move strongly toward one-hot of chosen action (win-stay)
            # - On no reward: revert toward uniform (lose-shift)
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                write = ws_strength
                w[s, :] = (1.0 - write) * w[s, :] + write * one_hot
            else:
                # clear toward uniform on losses
                clear = min(1.0, 0.5 + 0.5 * ws_strength)
                w[s, :] = (1.0 - clear) * w[s, :] + clear * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian working memory (Dirichlet) with confidence-based arbitration.

    Idea:
    - RL: standard Q-learning with softmax policy.
    - WM: for each state, maintain Dirichlet counts over actions with recency decay.
      The predictive distribution over actions (Dirichlet-multinomial) serves as the WM policy.
      Successful outcomes add evidence to the chosen action; recency decay models forgetting.
    - Arbitration weight scales with WM confidence: higher peak probability in WM increases its weight.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Maximum WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_decay: Recency decay toward prior for WM counts per trial (0..1).
    - wm_concentration: Symmetric prior concentration for Dirichlet (>=0; 0 -> very sharp with little prior).
      Larger values make WM more conservative.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_concentration = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # not used directly; WM uses predictive distribution below but retained per template spirit

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM represented as Dirichlet counts c; initialize with symmetric prior
        prior = wm_concentration * np.ones((nS, nA)) + 1e-8  # tiny epsilon to avoid zeros
        c = prior.copy()
        w = (c / np.sum(c, axis=1, keepdims=True))  # store normalized predictive as 'w' for compatibility
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM predictive distribution from Dirichlet: normalize counts
            c_sum = np.sum(c[s, :])
            if c_sum <= 0:
                W_s = w_0[s, :]
            else:
                W_s = c[s, :] / c_sum

            # Use predictive probability directly as p_wm for chosen action
            p_wm = max(W_s[a], 1e-12)

            # Confidence-based arbitration: higher max(W_s) above uniform increases WM weight
            confidence = max(0.0, np.max(W_s) - (1.0 / nA)) / (1.0 - (1.0 / nA))
            wm_weight_t = wm_weight * confidence

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (Dirichlet with recency decay and reward-dependent evidence)
            # Recency decay toward prior
            c[s, :] = (1.0 - wm_decay) * c[s, :] + wm_decay * prior[s, :]
            # Add evidence on reward
            if r > 0.0:
                c[s, a] += 1.0

            # Keep a normalized copy in w for completeness (not used for policy directly)
            denom_all = np.sum(c, axis=1, keepdims=True)
            denom_all[denom_all <= 0] = 1.0
            w = c / denom_all

        blocks_log_p += log_p

    return -blocks_log_p