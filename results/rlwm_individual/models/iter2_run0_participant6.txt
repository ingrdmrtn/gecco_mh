def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and global lapse.

    Idea:
    - Choices are a mixture of model-free RL and a WM policy derived from a fast, decaying categorical store.
    - Arbitration weight depends on the difference in uncertainty (entropy) between the WM and RL policies:
      when WM is sharper (lower entropy) than RL, rely more on WM; the reverse when RL is sharper.
    - WM traces decay toward uniform each trial and are overwritten on rewarded outcomes.
    - A small, global lapse adds uniform choice noise.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_base: baseline WM mixture bias (logit scale), can be negative/positive.
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_decay: WM decay rate toward uniform (0..1) applied each trial.
    - arbitrate_k: sensitivity of arbitration to entropy difference (>=0). Higher -> more adaptive switching.
    - lapse_epsilon: probability of a uniform random choice (0..1) applied after mixing.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_base, softmax_beta, wm_decay, arbitrate_k, lapse_epsilon = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # Working-memory policy: softmax over WM values
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute full policy distributions to estimate entropy for arbitration
            # RL distribution
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            rl_pi = rl_exp / np.sum(rl_exp)
            # WM distribution
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_exp = np.exp(wm_logits)
            wm_pi = wm_exp / np.sum(wm_exp)

            # Entropy (in nats)
            eps = 1e-12
            H_rl = -np.sum(rl_pi * np.log(rl_pi + eps))
            H_wm = -np.sum(wm_pi * np.log(wm_pi + eps))

            # Arbitration via logistic of base bias + entropy contrast
            # Positive arbitrate_k increases WM weight when WM is more certain (H_wm < H_rl)
            wm_logit = wm_base + arbitrate_k * (H_rl - H_wm)
            wm_weight_eff = 1 / (1 + np.exp(-wm_logit))

            p_mix = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            # Global lapse to uniform
            p_total = (1 - lapse_epsilon) * p_mix + lapse_epsilon * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform, then overwriting on reward
            w = (1 - wm_decay) * w + wm_decay * w_0
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-specific learning rate + WM with load-dependent interference.

    Idea:
    - RL learns with a learning rate that depends on the block set size (low vs high load).
    - WM is a fast store that decays more when more unique states have been encountered (interference).
      This creates stronger WM under low set sizes and early in the block.
    - WM and RL are mixed with a fixed weight scaled by set size (automatic load attenuation).

    Parameters (model_parameters):
    - lr_small: RL learning rate used when set size <= 3 (0..1).
    - lr_large: RL learning rate used when set size > 3 (0..1).
    - wm_weight0: baseline WM mixture weight before load scaling (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - interference_rate: controls how fast WM decays with the number of unique states seen (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_small, lr_large, wm_weight0, softmax_beta, interference_rate = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        # Track how many unique states have been encountered to drive interference
        seen_states = set()

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # RL learning rate by load
        lr = lr_small if nS <= 3 else lr_large

        # Set-size scaling of WM mixture: smaller sets -> more WM influence
        # Simple inverse scaling bounded to [0,1]
        wm_weight_eff_base = wm_weight0 * (3.0 / max(nS, 1.0))
        wm_weight_eff_base = min(1.0, max(0.0, wm_weight_eff_base))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Update seen states and compute interference-driven decay factor
            if s not in seen_states:
                seen_states.add(s)
            n_seen = len(seen_states)
            # Interference increases with the fraction of the set explored
            # Convert to a per-trial decay toward uniform
            decay_t = 1 - np.exp(-interference_rate * (n_seen / max(1.0, nS)))
            w = (1 - decay_t) * w + decay_t * w_0

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm*wm_weight_eff_base + (1-wm_weight_eff_base)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Reward-dependent WM strengthening (overwrite with delta spike on reward)
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited win-stay/lose-shift WM with delay-based decay and lapse.

    Idea:
    - RL provides a smooth value-learning backbone.
    - WM implements a simple heuristic store per state: if the last encounter of a state was rewarded,
      it favors repeating that action (win-stay); if it was unrewarded, it disfavors that action (lose-shift).
    - WM evidence decays as a function of delay (trials since last visit to that state).
    - The extent to which WM can influence choice is limited by a soft capacity K_slots: effective WM weight
      scales as min(1, K_slots / set_size). A small global lapse adds uniform noise.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - K_slots: approximate WM capacity in number of states (>=0). Determines load-dependent WM influence.
    - decay_delay: exponential decay rate per trial of WM traces with inter-visit delay (>=0).
    - epsilon_lapse: probability of a uniform random choice (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, K_slots, decay_delay, epsilon_lapse = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # For win-stay/lose-shift memory: track last action and reward per state, and last trial seen
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)
        last_seen_trial = -1 * np.ones(nS, dtype=int)

        # Capacity-limited WM influence weight
        wm_weight_eff = min(1.0, max(0.0, K_slots / max(1.0, nS)))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Construct WM preferences via win-stay/lose-shift with delay-based decay
            # Start from uniform
            w[s, :] = w_0[s, :]
            if last_seen_trial[s] >= 0:
                delay = max(1, t - last_seen_trial[s])
                strength = np.exp(-decay_delay * delay)
                if last_reward[s] > 0:
                    # Win-stay: bias toward last action
                    pref = w_0[s, :].copy()
                    pref[last_action[s]] = 1.0
                    # Interpolate by strength toward the deterministic win-stay distribution
                    w[s, :] = (1 - strength) * w_0[s, :] + strength * pref
                else:
                    # Lose-shift: bias away from last action
                    pref = np.ones(nA) / (nA - 1)
                    pref[last_action[s]] = 0.0
                    w[s, :] = (1 - strength) * w_0[s, :] + strength * pref
            # Else, remain uniform for first encounter

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy from current win-stay/lose-shift preferences
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix with capacity-limited weight, then apply lapse
            p_mix = p_wm*wm_weight_eff + (1-wm_weight_eff)*p_rl
            p_total = (1 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Update memory traces for state s
            last_action[s] = a
            last_reward[s] = r
            last_seen_trial[s] = t

        blocks_log_p += log_p

    return -blocks_log_p