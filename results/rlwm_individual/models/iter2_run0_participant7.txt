def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + precision-modulated WM with RPE-gated encoding and action stickiness.

    Idea
    - RL: standard delta rule with softmax.
    - WM: a graded action-value table W updated more strongly when surprising rewards occur (RPE-gated).
    - Precision of WM policy degrades with set size (load), yielding worse performance at nS=6.
    - Perseveration bias (stickiness) affects the WM policy to capture action repetition within state.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial.
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_precision, rpe_gate, stickiness)
        - lr: RL learning rate in [0,1]
        - wm_weight: fixed mixture weight of WM vs RL in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10
        - wm_precision: scales WM inverse temperature (>0); higher = more precise WM policy
        - rpe_gate: controls how strongly RPE gates WM encoding (>0). Larger => more RPE sensitivity
        - stickiness: additive bias on the last action in WM logits (can be >0 or <0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_precision, rpe_gate, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base determinism for WM logits

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        # Load-dependent WM precision (degrades with set size)
        load_factor = 1.0 / (1.0 + max(0, nS - 3))  # =1 for 3, ~0.5 for 6
        wm_beta_eff_scale = wm_precision * load_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values with precision scaling and stickiness on last action
            logits = softmax_beta_wm * wm_beta_eff_scale * W_s
            if last_action[s] >= 0:
                logits[last_action[s]] += stickiness
            # compute chosen-action probability from logits
            logits -= np.max(logits)  # numerical stability
            exp_logits = np.exp(logits)
            p_wm = exp_logits[a] / np.sum(exp_logits)

            # Mixture of WM and RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: RPE-gated encoding toward one-hot of chosen action
            # Gate strength via a smooth squashing of |delta|
            gate = 1.0 / (1.0 + np.exp(-rpe_gate * (abs(delta))))  # in (0,1)
            # Encoding rate tied to gate (stronger with surprising outcomes); only move W_s toward chosen action
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            eta = gate  # bounded in (0,1)
            w[s, :] = (1.0 - eta) * W_s + eta * onehot

            # Update stickiness memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with decay + WM with decay and uncertainty-based arbitration that is load-sensitive.

    Idea
    - Both RL and WM suffer decay toward uniform each trial (interference/forgetting).
    - Arbitration weight for WM is dynamically modulated by:
        (i) a base wm_weight,
        (ii) a load penalty (stronger penalty when nS=6),
        (iii) the relative confidence (peakiness) of each system on the current state.
    - WM encodes on rewarded trials and weakly on non-rewarded trials.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, load_sensitivity, rl_decay, wm_decay)
        - lr: RL learning rate in [0,1]
        - wm_weight: base WM mixture weight in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10
        - load_sensitivity: >0, penalizes WM contribution as set size increases
        - rl_decay: per-trial decay of Q toward uniform in [0,1]
        - wm_decay: per-trial decay of W toward uniform in [0,1]

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, load_sensitivity, rl_decay, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load penalty on WM mixture
        load_penalty = np.exp(-load_sensitivity * max(0, nS - 3))  # <= 1; smaller at nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply decay before using policies (captures time-based interference)
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA) * np.ones_like(q)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            # System confidence (peakiness relative to mean)
            conf_rl = max(1e-8, np.max(Q_s) - np.mean(Q_s))
            conf_wm = max(1e-8, np.max(W_s) - np.mean(W_s))

            base_mix = wm_weight * load_penalty
            # Relative arbitration based on confidence
            if (conf_rl + conf_wm) > 0:
                dyn_wm = base_mix * (conf_wm / (conf_rl + conf_wm))
            else:
                dyn_wm = 0.5 * base_mix  # fallback

            dyn_wm = min(max(dyn_wm, 0.0), 1.0)

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax over W)
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            exp_wm = np.exp(logits_wm)
            p_wm = exp_wm[a] / np.sum(exp_wm)

            p_total = dyn_wm * p_wm + (1.0 - dyn_wm) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: encode more when rewarded, less when not
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            eta = 0.7 if r > 0.0 else 0.2  # encoding strength conditional on outcome
            w[s, :] = (1.0 - eta) * W_s + eta * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + error-driven WM gate with load-dependent lapse and decay.

    Idea
    - WM contribution is enabled more when recent prediction errors are small (stable mapping),
      and suppressed when errors are large. The gate is computed from current RPE.
    - Lapses increase with load, mixing in uniform choice noise.
    - WM decays toward uniform, with rate modulated by load via the same lapse-related parameters.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, gate_threshold, lapse_base, load_lapse_slope)
        - lr: RL learning rate in [0,1]
        - wm_weight: base WM weight in [0,1]
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10
        - gate_threshold: threshold on |RPE| (0..1) that controls WM gating; smaller => WM engages more
        - lapse_base: base lapse probability (0..0.49)
        - load_lapse_slope: increases lapse and WM decay with load (>0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_threshold, lapse_base, load_lapse_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent lapse and WM decay
        load_term = max(0, nS - 3)
        epsilon = min(0.49, max(0.0, lapse_base + load_lapse_slope * load_term))
        wm_decay = min(1.0, max(0.0, lapse_base + load_lapse_slope * load_term))  # tie decay to lapse

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax)
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            exp_wm = np.exp(logits_wm)
            p_wm = exp_wm[a] / np.sum(exp_wm)

            # Compute current absolute RPE for gating (based on current Q)
            delta = r - Q_s[a]
            # gate near 1 when |delta| below threshold (stable), near 0 when above (volatile)
            # Using a steep logistic around gate_threshold without extra params
            gate_scale = 10.0
            g = 1.0 / (1.0 + np.exp(gate_scale * (abs(delta) - gate_threshold)))

            mix = wm_weight * g  # effective WM mix under gate
            p_mix = mix * p_wm + (1.0 - mix) * p_rl

            # Lapse to uniform with load dependence
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM decay every trial (load-dependent)
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # WM encoding: one-shot toward chosen action, stronger when rewarded
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            eta = 0.8 if r > 0.0 else 0.3
            w[s, :] = (1.0 - eta) * w[s, :] + eta * onehot

        blocks_log_p += log_p

    return -blocks_log_p