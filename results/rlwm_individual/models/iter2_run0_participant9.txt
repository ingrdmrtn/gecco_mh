def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working-memory (WM) mixture with age-modulated capacity.

    Mechanism
    - RL: tabular Q-learning with fixed learning rate alpha and inverse temperature beta.
    - WM: per-state associative memory of the last rewarded action. When WM has an
      item for the current state, it proposes that action with high probability
      (1 - wm_noise), with the remaining mass spread over other actions.
    - Mixture: policy is a convex combination of WM and RL. The WM weight is a
      logistic function of an "effective capacity" minus the current set size.
      Older adults (>=45) have reduced effective capacity via an age shift.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, K_base, age_K_shift, wm_noise]
        - alpha: learning rate for Q-learning, passed through sigmoid to [0,1].
        - beta: inverse temperature for softmax on Q-values (>= 0).
        - K_base: baseline WM capacity-like term in the WM weight logistic.
        - age_K_shift: reduction applied to K_base for older adults (>=45). Younger get 0 shift.
        - wm_noise: probability mass that WM spreads away from its proposed action.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, K_base, age_K_shift, wm_noise = model_parameters

    # Parameter transforms and bounds
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(1e-6, beta)
    wm_noise = np.clip(wm_noise, 0.0, 1.0)

    age_group = 1 if age[0] >= 45 else 0
    # Effective capacity reduced in older adults
    K_eff_base = K_base - age_group * np.maximum(0.0, age_K_shift)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: -1 means no stored rewarded action yet
        wm_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_rl = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            # WM policy
            if wm_action[s] >= 0:
                pi_wm = np.full(nA, wm_noise / (nA - 1.0))
                pi_wm[wm_action[s]] = 1.0 - wm_noise
            else:
                pi_wm = np.ones(nA) / nA

            # Mixture weight as logistic(K_eff - set_size)
            K_eff = K_eff_base
            w_wm = 1.0 / (1.0 + np.exp(-(K_eff - float(nS_t))))

            p_total = w_wm * pi_wm + (1.0 - w_wm) * p_rl
            p_a = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store last rewarded action for this state; if r==1 store a, else leave as is
            if r >= 1.0:
                wm_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-modulated forgetting (value decay toward prior).

    Mechanism
    - Q-learning with fixed alpha and softmax with beta.
    - Before each update, Q-values for the current state decay toward the uninformative
      prior (uniform) by a decay_t factor that increases with set size and with age.
      This captures load-sensitive maintenance deficits, stronger in older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta_base, decay_base, load_decay_gain, age_decay_gain]
        - alpha: learning rate for Q-learning (sigmoid to [0,1]).
        - beta_base: inverse temperature for softmax (>=0).
        - decay_base: baseline logit for decay; higher means more forgetting overall.
        - load_decay_gain: increases decay with larger set sizes (>=0).
        - age_decay_gain: additional decay for older adults (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta_base, decay_base, load_decay_gain, age_decay_gain = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(1e-6, beta_base)
    load_decay_gain = np.maximum(0.0, load_decay_gain)
    age_decay_gain = np.maximum(0.0, age_decay_gain)
    age_group = 1 if age[0] >= 45 else 0

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        prior = np.ones(nA) / nA

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Decay factor on logits: logistic(decay_base + load*gain + age*gain)
            load_term = load_decay_gain * max(0.0, (nS_t - 3.0)) / 3.0
            decay_logit = decay_base + load_term + age_group * age_decay_gain
            decay_t = 1.0 / (1.0 + np.exp(-decay_logit))  # in (0,1)

            # Apply decay toward prior for current state's Q-values
            Q[s, :] = (1.0 - decay_t) * Q[s, :] + decay_t * prior

            # Choice
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p = exp_logits / np.maximum(1e-12, np.sum(exp_logits))
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Blended Win-Stay/Lose-Shift (WSLS) and RL, with WSLS reliance shaped by load and age.

    Mechanism
    - RL: tabular Q-learning with alpha and softmax beta.
    - WSLS policy: within each state, if the previous outcome was a win, repeat the
      previous action; if a loss, avoid the previous action (shift). This defines a
      distribution over actions (uniform among viable choices).
    - Mixture: convex combination of RL softmax and WSLS policy. The WSLS weight is a
      logistic function of a bias term plus a load term (favoring WSLS at small set sizes)
      and an age term that reduces WSLS reliance in older adults.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45) vs older (>=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, wsls_bias, size_wsls_sensitivity, age_wsls_boost]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - wsls_bias: baseline log-odds of relying on WSLS vs RL.
        - size_wsls_sensitivity: increases WSLS weight when set size is small (can be any real).
        - age_wsls_boost: reduction in WSLS weight for older adults (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, wsls_bias, size_wsls_sensitivity, age_wsls_boost = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(1e-6, beta)
    age_wsls_boost = np.maximum(0.0, age_wsls_boost)
    age_group = 1 if age[0] >= 45 else 0

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL softmax
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_rl = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            # WSLS policy
            pi_wsls = np.ones(nA) / nA
            if last_action[s] >= 0:
                if last_reward[s] >= 0.5:  # win-stay
                    pi_wsls = np.zeros(nA)
                    pi_wsls[last_action[s]] = 1.0
                else:  # lose-shift: avoid last action, uniform over others
                    pi_wsls = np.ones(nA)
                    pi_wsls[last_action[s]] = 0.0
                    denom = np.sum(pi_wsls)
                    if denom > 0:
                        pi_wsls /= denom
                    else:
                        pi_wsls = np.ones(nA) / nA

            # WSLS weight: logistic(wsls_bias + size_sens*(3 - nS)/3 - age*boost)
            size_term = size_wsls_sensitivity * (3.0 - float(nS_t)) / 3.0
            wsls_logit = wsls_bias + size_term - age_group * age_wsls_boost
            w_wsls = 1.0 / (1.0 + np.exp(-wsls_logit))

            p_total = w_wsls * pi_wsls + (1.0 - w_wsls) * p_rl
            p_a = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

    return nll