def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-gated arbitration (set-size sensitive) + choice stickiness.

    Idea:
    - RL system uses a delta rule and softmax with choice stickiness.
    - WM stores rewarded associations one-shot and decays toward uniform.
    - Arbitration weight for WM increases with RL uncertainty (state-wise entropy)
      and decreases with larger set size (load penalty).
    - Final policy is a convex mixture between WM and RL.

    Parameters (6 total):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: base inverse temperature for RL (scaled internally by 10).
    - entropy_gate: sensitivity of WM gating to RL entropy (higher -> more WM when RL uncertain).
    - ns_bias: penalty on WM gating per additional set size items beyond 3 (load cost).
    - wm_decay: WM learning/decay rate toward a one-hot for rewarded action and toward uniform otherwise.
    - stickiness: choice perseveration weight added to the last chosen action's logit (state-dependent).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, entropy_gate, ns_bias, wm_decay, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness
        last_action_per_state = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add state-dependent stickiness to RL logits
            if last_action_per_state[s] >= 0:
                Q_s[last_action_per_state[s]] += stickiness

            # RL choice probability of chosen action a
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = pi_rl[a]

            # WM probability (deterministic-ish softmax over WM)
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            # Entropy-gated WM arbitration with set-size penalty
            # RL entropy for current state
            H_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            H_ref = 0.5 * np.log(nA)  # reference uncertainty
            wm_gate = 1.0 / (1.0 + np.exp(-(entropy_gate * (H_rl - H_ref) - ns_bias * max(0, nS - 3)))))
            wm_gate = float(np.clip(wm_gate, 0.0, 1.0))

            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay and one-shot storage on reward
            # Global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # If rewarded, push state toward one-hot at chosen action
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

            # Update stickiness memory
            last_action_per_state[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-driven arbitration (based on WM certainty) + action bias.

    Idea:
    - RL updates Q with delta rule; softmax includes an action-0 bias on logits.
    - WM stores rewarded associations using a dedicated WM learning rate (wm_learn)
      and otherwise reverts to uniform on non-rewarded trials for that state.
    - Arbitration weight for WM increases with WM certainty (max probability in W_s),
      but is penalized by set size (ns load). Two parameters shape this arbitration.
    - Final policy mixes WM and RL.

    Parameters (6 total):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL (scaled internally by 10).
    - tau: load penalty strength; larger tau reduces WM influence more when set size > 3.
    - kappa: sensitivity of WM gating to WM certainty (max entry in W_s).
    - wm_learn: WM learning rate toward one-hot for rewarded action; also controls forgetting on non-reward.
    - bias0: additive bias to action 0 in both RL and WM logits (positive favors action 0).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, tau, kappa, wm_learn, bias0 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Add action-0 bias to logits
            bias_vec = np.zeros(nA)
            bias_vec[0] = bias0

            # RL policy
            logits_rl = softmax_beta * (Q_s) + bias_vec
            logits_rl -= np.max(logits_rl)
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = pi_rl[a]

            # WM policy
            logits_wm = softmax_beta_wm * (W_s) + bias_vec
            logits_wm -= np.max(logits_wm)
            exp_wm = np.exp(logits_wm)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            # Confidence-driven arbitration with load penalty
            wm_conf = float(np.max(W_s))  # certainty of WM for state s
            load_penalty = tau * max(0, nS - 3)
            wm_gate = 1.0 / (1.0 + np.exp(-(kappa * (wm_conf - 1.0 / nA) - load_penalty)))
            wm_gate = float(np.clip(wm_gate, 0.0, 1.0))

            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: learn on reward, otherwise revert toward uniform for this state
            if r > 0.5:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
            else:
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-specific learning rates + WM with interference (swap) noise and global forgetting.

    Idea:
    - RL learning rate depends on set size (separate lr for 3 vs 6), capturing load effects on incremental learning.
    - WM stores rewarded associations one-shot; per-trial global WM forgetting toward uniform occurs with wm_forget.
    - WM retrieval suffers swap/interference noise that increases with set size; we approximate this as a mixture
      of the WM policy with uniform, controlled by interference_psi.
    - WM/RL arbitration weight is reduced under higher load via the same interference parameter.

    Parameters (6 total):
    - lr3: RL learning rate when set size <= 3.
    - lr6: RL learning rate when set size > 3.
    - softmax_beta: RL inverse temperature (scaled internally by 10).
    - wm_base: baseline arbitration weight on WM (0..1) when nS=3 and no interference.
    - interference_psi: magnitude of WM interference/swap noise that scales with set size.
    - wm_forget: global WM forgetting rate per trial toward uniform.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr3, lr6, softmax_beta, wm_base, interference_psi, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])
        lr = lr3 if nS <= 3 else lr6

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference-scaled retrieval noise and arbitration
        # Swap probability increases with set size
        swap = float(np.clip(interference_psi * (max(0, nS - 3) / 3.0), 0.0, 1.0))
        wm_weight = float(np.clip(wm_base * (1.0 - swap), 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL probability
            logits_rl = softmax_beta * Q_s
            logits_rl -= np.max(logits_rl)
            exp_rl = np.exp(logits_rl)
            pi_rl = exp_rl / max(np.sum(exp_rl), eps)
            p_rl = pi_rl[a]

            # Clean WM probability
            logits_wm = softmax_beta_wm * W_s
            logits_wm -= np.max(logits_wm)
            exp_wm = np.exp(logits_wm)
            pi_wm_clean = exp_wm / max(np.sum(exp_wm), eps)
            p_wm_clean = pi_wm_clean[a]

            # Interference-mixed WM probability (swap -> uniform)
            p_wm = (1.0 - swap) * p_wm_clean + swap * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with set-size-specific lr
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM global forgetting toward uniform
            w = (1.0 - wm_forget) * w + wm_forget * w_0
            # Rewarded one-shot strengthening
            if r > 0.5:
                w[s, :] = (1.0 - wm_forget) * w[s, :]
                w[s, a] += wm_forget

        blocks_log_p += log_p

    return -blocks_log_p