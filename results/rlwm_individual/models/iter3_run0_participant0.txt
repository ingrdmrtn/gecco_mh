def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with volatility-sensitive WM reliability and set-size-driven interference.

    Rationale:
    - RL learns incremental action values and is always on.
    - WM stores state-specific action policies that are very sharp when confident, but WM suffers
      more interference under higher set size (nS) and when recent outcomes are volatile.
    - Volatility (per state) is tracked from RL prediction errors and used to "noisify" WM for that state.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate for Q-values
    - wm_weight: scalar in [0,1], mixture weight for WM vs RL in action selection
    - softmax_beta: base inverse temperature for RL (internally scaled by 10)
    - wm_alpha: in [0,1], WM learning rate toward a one-hot distribution based on feedback
    - volatility_lr: in [0,1], learning rate for tracking per-state reward volatility from PE magnitude
    - wm_noise: >=0, scales set-size-driven global WM decay and also contributes to WM reliability via entropy/volatility

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, volatility_lr, wm_noise = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Per-state volatility estimate from absolute RL PEs
        v = np.zeros(nS)

        # Global set-size-driven WM decay each trial: 0 at nS=3, increases with nS
        global_decay = np.clip(wm_noise * max(0, (nS - 3)) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # Volatility-adjusted WM policy: noisify WM by mixing with uniform according to v[s]
            W_s_raw = w[s, :]
            W_s_eff = (1.0 - np.clip(v[s], 0.0, 1.0)) * W_s_raw + np.clip(v[s], 0.0, 1.0) * w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_eff - W_s_eff[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update state-specific volatility from RL PE magnitude
            v[s] = (1.0 - volatility_lr) * v[s] + volatility_lr * abs(delta)
            v[s] = np.clip(v[s], 0.0, 1.0)

            # Global WM decay toward uniform due to set-size interference
            if global_decay > 0:
                w = (1.0 - global_decay) * w + global_decay * w_0

            # WM update at current state toward a one-hot distribution shaped by reward
            # Rewarded: push probability mass to chosen action; Unrewarded: redistribute to unchosen actions
            if r > 0.0:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # Move away from chosen action toward others
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                redistribute = wm_alpha / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            # Renormalize to avoid drift
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-asymmetric learning + WM certainty learning with set-size-dependent forgetting.

    Rationale:
    - RL uses separate learning rates for rewards and non-rewards.
    - WM learns a sharp policy when rewarded and diffuses when not, with a state-independent mixture weight.
    - Larger set sizes induce stronger WM decay toward uniform, capturing load effects.
    
    Parameters (model_parameters):
    - lr_pos: in [0,1], RL learning rate for positive outcomes (reward=1)
    - lr_neg: in [0,1], RL learning rate for negative outcomes (reward=0)
    - wm_weight: in [0,1], mixture weight for WM vs RL in choice
    - softmax_beta: base inverse temperature for RL (internally scaled by 10)
    - wm_eta: in [0,1], WM learning rate toward a one-hot distribution on rewarded trials;
              on unrewarded trials it pushes probability mass to unchosen actions
    - setsize_k: >=0, controls how strongly WM decays with larger set sizes; decay = 1 - exp(-setsize_k*(nS-3))
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_eta, setsize_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent WM decay
        decay = 1.0 - np.exp(-max(0.0, setsize_k) * max(0, (nS - 3)))
        decay = np.clip(decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with valence-asymmetric learning rates
            delta = r - Q_s[a]
            if delta >= 0:
                q[s, a] += lr_pos * delta
            else:
                q[s, a] += lr_neg * delta

            # Global WM decay per trial due to set size
            if decay > 0:
                w = (1.0 - decay) * w + decay * w_0

            # WM update at current state
            if r > 0.0:
                # Rewarded: move toward chosen action
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                w[s, a] += wm_eta
            else:
                # Unrewarded: reduce chosen action's weight, distribute to others
                w[s, :] = (1.0 - wm_eta) * w[s, :]
                add = wm_eta / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += add

            # Renormalize
            w[s, :] = np.maximum(w[s, :], eps)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + hypothesis-testing WM (candidate elimination) with slips and set-size interference.

    Rationale:
    - WM maintains, for each state, a candidate set of actions. Positive feedback commits to the chosen action;
      negative feedback eliminates the chosen action from the candidate set.
    - Under higher set size, interference reintroduces some uncertainty by mixing the candidate distribution with uniform.
    - WM can slip (random choice) occasionally; RL runs in parallel and is mixed via wm_weight.

    Parameters (model_parameters):
    - lr: in [0,1], RL learning rate for Q-values
    - wm_weight: in [0,1], mixture weight for WM vs RL
    - softmax_beta: base inverse temperature for RL (scaled by 10 internally)
    - slip: in [0,1], probability that WM generates a random choice (lapse-like) on any trial
    - guess_temp: >=0, sharpness of WM's mapping when multiple candidates remain (higher -> sharper)
    - interference_k: >=0, strength of set-size-driven interference that mixes WM with uniform when nS is large

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, slip, guess_temp, interference_k = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM represented as a candidate distribution per state (starts uniform)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-dependent interference strength mixing candidates with uniform
        p_int = np.clip(interference_k * max(0, (nS - 3)) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy prob
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM effective candidate distribution with interference mixing
            W_s_raw = w[s, :]
            W_s_mix = (1.0 - p_int) * W_s_raw + p_int * w_0[s, :]

            # Apply "guess temperature" by scaling the logits before deterministic softmax
            logits_wm = guess_temp * W_s_mix
            denom_wm = np.sum(np.exp(softmax_beta_wm * (logits_wm - logits_wm[a])))
            p_wm_det = 1.0 / max(denom_wm, eps)

            # Slips: with probability slip, WM picks uniformly at random
            p_wm = (1.0 - slip) * p_wm_det + slip * (1.0 / nA)

            # Mixture with RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM hypothesis-testing update
            if r > 0.0:
                # Commit to chosen action for this state
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Eliminate chosen action from candidate set; renormalize remaining
                w[s, a] = 0.0
                if np.sum(w[s, :]) <= eps:
                    # If all eliminated (shouldn't happen often), reset to uniform
                    w[s, :] = w_0[s, :]
                else:
                    w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p