def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration with capacity- and age-limited working memory.

    Idea:
    - An RL system learns Q-values per state-action.
    - In parallel, a simple working-memory (WM) store keeps the last rewarded action for each state within a block.
    - Policy is a mixture: pi = w_wm * pi_wm + (1 - w_wm) * pi_rl.
    - WM reliability (w_wm) decreases with set size and in older adults.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: RL softmax inverse temperature (>0).
    - p0_wm: baseline WM reliability at set size 3 for younger adults (0..1).
    - k_set: decrement of WM reliability per additional items beyond 3 (>=0).
    - k_age: additional decrement if older adult (>=0).

    Inputs:
    - states: array of state indices per trial (0..set_size-1).
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (resets learning within block).
    - set_sizes: array of set sizes per trial (3 or 6), constant within block.
    - age: array-like with one entry: participant age in years.
    - model_parameters: list/tuple [alpha, beta, p0_wm, k_set, k_age].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, p0_wm, k_set, k_age = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    # transform p0 into logit space to apply additive decrements robustly
    eps = 1e-6
    p0 = np.clip(p0_wm, eps, 1 - eps)
    logit_p0 = np.log(p0 / (1 - p0))

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: last rewarded action for each state; -1 means unknown
        WM = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # WM reliability decreases with set size and age
            decrement = k_set * max(0, curr_set - 3) + k_age * is_older
            w_wm = sigmoid(logit_p0 - decrement)

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            exp_logits = np.exp(logits)
            p_rl = exp_logits / np.sum(exp_logits)
            p_rl_a = max(p_rl[a], 1e-12)

            # WM policy: if stored, choose that action deterministically, else uniform
            if WM[s] >= 0:
                p_wm_a = 1.0 if a == WM[s] else 0.0
            else:
                p_wm_a = 1.0 / nA

            # Mixture
            p_total = w_wm * p_wm_a + (1.0 - w_wm) * p_rl_a
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store last rewarded action; do not overwrite on errors
            if r > 0.5:
                WM[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with confidence-adaptive decision temperature, degraded by load and age.

    Idea:
    - Standard RL learning of Q-values.
    - Decision noise (softmax beta) is adapted on each trial based on within-state value confidence:
      beta_eff = (beta * (1 + g_conf * (Q_max - Q_second))) / (1 + k_load_age * (load + is_older)).
    - Larger set sizes and older age reduce effective beta (more noise); high value separation increases beta.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: base inverse temperature (>0).
    - g_conf: gain on confidence signal (>=0).
    - k_load_age: scaling of beta degradation by load and age (>=0).

    Inputs:
    - states: array of state indices per trial (0..set_size-1).
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (resets learning within block).
    - set_sizes: array of set sizes per trial (3 or 6), constant within block.
    - age: array-like with one entry: participant age in years.
    - model_parameters: list/tuple [alpha, beta, g_conf, k_load_age].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, g_conf, k_load_age = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Compute confidence = Q_max - second_best for current state
            q_s = Q[s, :]
            # second best via partition for stability
            # get indices of sorted values
            order = np.argsort(q_s)
            q_max = q_s[order[-1]]
            q_second = q_s[order[-2]]
            confidence = max(0.0, q_max - q_second)

            # Load-age degradation factor
            load = max(0, curr_set - 3) / 3.0
            degrade = 1.0 + k_load_age * (load + is_older)

            beta_eff = (beta * (1.0 + g_conf * confidence)) / degrade

            logits = beta_eff * (q_s - np.max(q_s))
            exp_logits = np.exp(logits)
            probs = exp_logits / np.sum(exp_logits)
            p_choice = max(probs[a], 1e-12)
            nll -= np.log(p_choice)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Arbitration between RL and a WSLS heuristic, with load- and age-modulated arbitration.

    Idea:
    - RL learns Q-values via delta rule.
    - A WSLS (win-stay/lose-shift) heuristic proposes the last rewarded action for a state, or
      shifts uniformly among other actions after a loss.
    - The controller mixes WSLS and RL policies with probability p_ws that declines with set size and with age.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1).
    - beta: RL softmax inverse temperature (>0).
    - p_ws_base: baseline probability of using WSLS at set size 3 for younger adults (0..1).
    - k_load: decrement in WSLS usage per extra items beyond 3 (>=0).
    - k_age: additional decrement if older adult (>=0).

    Inputs:
    - states: array of state indices per trial (0..set_size-1).
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (resets learning within block).
    - set_sizes: array of set sizes per trial (3 or 6), constant within block.
    - age: array-like with one entry: participant age in years.
    - model_parameters: list/tuple [alpha, beta, p_ws_base, k_load, k_age].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, p_ws_base, k_load, k_age = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    eps = 1e-6
    p0 = np.clip(p_ws_base, eps, 1 - eps)
    logit_p0 = np.log(p0 / (1 - p0))

    nll = 0.0
    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nA = 3
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Arbitration weight for WSLS declines with load and age
            decrement = k_load * max(0, curr_set - 3) + k_age * is_older
            p_ws = sigmoid(logit_p0 - decrement)

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            exp_logits = np.exp(logits)
            p_rl = exp_logits / np.sum(exp_logits)
            p_rl_a = max(p_rl[a], 1e-12)

            # WSLS policy
            if last_action[s] < 0:
                # no history: uniform
                p_wsls_a = 1.0 / nA
            else:
                if last_reward[s] >= 0.5:
                    p_wsls_a = 1.0 if a == last_action[s] else 0.0
                else:
                    # shift uniformly to any action other than the last one
                    if a == last_action[s]:
                        p_wsls_a = 0.0
                    else:
                        p_wsls_a = 1.0 / (nA - 1)

            p_total = p_ws * p_wsls_a + (1.0 - p_ws) * p_rl_a
            p_total = max(p_total, 1e-12)
            nll -= np.log(p_total)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

    return nll