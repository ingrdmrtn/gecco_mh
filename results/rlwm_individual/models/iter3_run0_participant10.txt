def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with time-since-last-visit gated mixture.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores a sharp distribution over the last chosen action per state, updated more strongly after reward.
    - The contribution of WM to choice is attenuated by the time since the state was last seen (lags are longer in set size 6).
      This captures that WM is effective only over short lags.

    Parameters (model_parameters):
    - lr: RL learning rate for updating Q-values (0..1).
    - wm_weight0: Baseline mixture weight of WM contribution (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled by 10 internally).
    - tau_lag: Time constant controlling decay of WM influence with inter-visit lag (>0). Larger -> slower decay.
    - wm_eta_pos: WM learning rate when reward is delivered (0..1). On no-reward, WM learns with a weaker rate wm_eta_neg = 0.2*wm_eta_pos.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight0, softmax_beta, tau_lag, wm_eta_pos = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track time since last visit to each state within the block
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute inter-visit lag for this state
            if last_seen[s] < 0:
                lag = 1e6  # effectively never seen before
            else:
                lag = t - last_seen[s]

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(1e-12, denom_rl)

            # WM policy: softmax over WM weights for current state
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Mixture weight decays with lag
            wm_weight = wm_weight0 * np.exp(-float(lag) / max(1e-6, tau_lag))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: stronger after reward, weaker after no-reward
            wm_eta_neg = 0.2 * wm_eta_pos
            eta = wm_eta_pos if r > 0.5 else wm_eta_neg
            target = np.zeros(nA)
            target[a] = 1.0
            w[s, :] = (1.0 - eta) * w[s, :] + eta * target
            # Normalize for numerical stability
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Update last seen time for this state
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent forgetting + capacity-limited WM cache (LRU).

    Idea:
    - RL: Q-values drift toward uniform with a forgetting rate that increases with set size.
    - WM: A limited-capacity cache stores the last rewarded action for up to C states within a block (LRU eviction).
      Cached states produce sharp WM policies; uncached states yield uniform WM.
    - Choice is a mixture between RL and WM with a fixed mixture weight.

    Parameters (model_parameters):
    - lr: RL learning rate for outcomes (0..1).
    - wm_weight: Mixture weight for WM contribution (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled by 10 internally).
    - rl_forget: Base RL forgetting strength (>=0). Effective forgetting grows with set size as 1 - exp(-rl_forget * nS).
    - C_capacity: Integer WM capacity (# states cached). Real values are allowed and internally rounded to nearest int >=0.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, rl_forget, C_capacity = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # LRU cache bookkeeping
        capacity = int(max(0, round(C_capacity)))
        in_cache = np.zeros(nS, dtype=bool)
        # recency list: store order of states; most recent at end
        recency = []

        # Effective RL forgetting increases with set size
        f_eff = 1.0 - np.exp(-rl_forget * float(nS))
        f_eff = np.clip(f_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(1e-12, denom_rl)

            # WM policy:
            # If state is cached: sharp one-hot distribution in w; else uniform (already in w).
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL forgetting for current state's Q (toward uniform), then standard update
            q[s, :] = (1.0 - f_eff) * q[s, :] + f_eff * w_0[s, :]
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM cache update: reward-dependent insertion and LRU maintenance
            if r > 0.5 and capacity > 0:
                # Put state into cache and store chosen action deterministically
                if not in_cache[s]:
                    # Evict if full
                    if len(recency) >= capacity:
                        evict_state = recency.pop(0)
                        in_cache[evict_state] = False
                        w[evict_state, :] = w_0[evict_state, :]
                    in_cache[s] = True
                    recency.append(s)
                else:
                    # Move to most recent
                    if s in recency:
                        recency.remove(s)
                        recency.append(s)
                # Overwrite WM distribution to the rewarded action
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # If not rewarded, we leave WM as is; optional mild stabilization toward uniform if not cached
                if not in_cache[s]:
                    w[s, :] = w_0[s, :]

            # Normalize for numerical stability
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load-biased gating.

    Idea:
    - RL updates Q-values with a single learning rate.
    - WM maintains a sharp distribution per state, updated to the chosen action on reward; otherwise it leaks toward uniform.
    - Arbitration: trial-wise WM weight depends on relative confidence (lower entropy) of WM vs RL for the current state.
      A bias term shifts arbitration toward RL under higher set size.
    - Set size influences arbitration via a load bias term and also increases WM leak.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: Inverse temperature for RL (scaled by 10 internally).
    - k_arbit: Sensitivity of arbitration to entropy difference (>=0). Higher -> stronger shift to lower-entropy system.
    - b_arbit: Baseline arbitration bias toward WM (positive) or RL (negative).
    - wm_beta: Scales WM policy determinism by multiplying softmax_beta_wm (>=0).
    - c_load: Load sensitivity: increases RL bias and WM leak with larger set sizes.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, k_arbit, b_arbit, wm_beta, c_load = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    base_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM beta and load-dependent leak
        eff_beta_wm = max(1.0, base_beta_wm * max(0.0, wm_beta))
        # Leak increases with set size via a sigmoid of c_load
        leak = 1.0 / (1.0 + np.exp(-c_load * (float(nS) - 3.0)))  # in (0,1)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probabilities for entropy
            Q_s = q[s, :]
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            pvec_rl = np.exp(logits_rl)
            pvec_rl /= np.sum(pvec_rl)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(1e-12, denom_rl)

            # WM policy probabilities for entropy
            W_s = w[s, :]
            logits_wm = eff_beta_wm * (W_s - np.max(W_s))
            pvec_wm = np.exp(logits_wm)
            pvec_wm /= np.sum(pvec_wm)
            denom_wm = np.sum(np.exp(eff_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Entropies
            H_rl = -np.sum(pvec_rl * np.log(np.maximum(pvec_rl, 1e-12)))
            H_wm = -np.sum(pvec_wm * np.log(np.maximum(pvec_wm, 1e-12)))

            # Load-dependent arbitration bias: larger set size -> more RL-biased (subtract from b_arbit)
            bias_eff = b_arbit - abs(c_load) * (float(nS) - 3.0)

            # WM weight based on entropy difference
            wm_weight = 1.0 / (1.0 + np.exp(-(k_arbit * (H_rl - H_wm) + bias_eff)))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-driven sharpening, otherwise leak toward uniform (load-dependent)
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Normalize for numerical stability
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p