def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with eligibility traces and load-scaled learning, with age-dependent trace persistence.

    Summary
    - Standard tabular Q-learning over state-action values with softmax choice rule (beta).
    - Eligibility traces e(s,a) emphasize recently chosen actions and decay over time (lambda).
    - Lambda (trace persistence) depends on age group: younger vs older.
    - Learning rate is downscaled under higher set size by a power factor phi.
      Intuition: under higher load (set size=6), learning per experience is diluted.
    - Negative log-likelihood of the observed choices is returned.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based).
    actions : array-like of int
        Chosen action at each trial; valid actions {0,1,2}.
    rewards : array-like of float/int
        Observed reward signal, expected in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within a block.
    age : array-like (length 1)
        Participant age; <45 -> younger group, >=45 -> older group.
    model_parameters : tuple/list of floats
        (alpha, beta, lambda_young, lambda_old, phi)
        - alpha: base learning rate for Q updates
        - beta: inverse temperature for softmax action selection
        - lambda_young: eligibility trace decay for younger participants
        - lambda_old: eligibility trace decay for older participants
        - phi: exponent controlling how set size scales learning (alpha_eff = alpha * (3/set_size)^phi)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, lam_y, lam_o, phi = model_parameters
    age_val = age[0]
    lam_age = lam_y if age_val < 45 else lam_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # load scaling for this block
        ss = float(block_set_sizes[0])
        load_scale = (3.0 / ss) ** phi
        alpha_eff = alpha * load_scale

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1) or (s < 0) or (s >= nS):
                # Handle invalid/missing entries robustly: assign uniform likelihood and skip learning
                nll -= np.log(1.0 / nA)
                continue

            # Likelihood
            p = np.clip(pi[a], 1e-12, 1.0)
            nll -= np.log(p)

            # TD error
            delta = r - Q[s, a]

            # Update eligibility traces: decay and then set current trace to 1 (replacing traces)
            e *= lam_age
            e[s, :] *= 0.0  # for bandit-like per-state learning, focus trace on current state only
            e[s, a] = 1.0

            # Q update with traces
            Q += alpha_eff * delta * e

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Probabilistic Win-Stay/Lose-Shift (WSLS) with load dilution and age-dependent sensitivity to losses.

    Summary
    - Heuristic policy within each state: if the last outcome in this state was a win, repeat the action
      with probability p_win; if a loss, switch away with probability p_lose(age).
    - Cognitive load (set size) dilutes the heuristic by mixing it with a uniform policy; dilution
      increases with set size controlled by kappa: gamma = 1/(1 + kappa*(set_size-3)).
    - Residual stochasticity controlled by tau via a power transform of the heuristic probabilities
      (equivalent to a softmax over log-probabilities).
    - Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0-based).
    actions : array-like of int
        Chosen action at each trial; expected in {0,1,2}.
    rewards : array-like of int
        Observed outcomes in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older (selects p_lose).
    model_parameters : tuple/list of floats
        (p_win, p_lose_young, p_lose_old, tau, kappa)
        - p_win: probability to repeat after a win in the same state
        - p_lose_young: probability to switch after a loss for younger participants
        - p_lose_old: probability to switch after a loss for older participants
        - tau: inverse temperature applied as a power transform to the heuristic probabilities
        - kappa: load dilution strength; higher => stronger dilution toward uniform when set size=6

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    p_win, p_lose_y, p_lose_o, tau, kappa = model_parameters
    age_val = age[0]
    p_lose_age = p_lose_y if age_val < 45 else p_lose_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        last_action = -np.ones(nS, dtype=int)  # last action per state
        last_reward = np.zeros(nS)             # last outcome per state

        ss = float(block_set_sizes[0])
        gamma = 1.0 / (1.0 + kappa * (ss - 3.0))  # mixing weight for heuristic vs uniform

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                # do not update history on invalid trials
                continue

            # Build heuristic distribution for this state
            heuristic = np.ones(nA) * (1.0 / nA)
            if last_action[s] >= 0:
                if last_reward[s] >= 0.5:
                    # Win: repeat with p_win
                    heuristic[:] = (1.0 - p_win) / (nA - 1)
                    heuristic[last_action[s]] = p_win
                else:
                    # Loss: shift away with p_lose_age, split across the other two
                    heuristic[:] = p_lose_age / (nA - 1)
                    heuristic[last_action[s]] = 1.0 - p_lose_age
            else:
                # No history for this state: uniform
                heuristic[:] = 1.0 / nA

            # Load dilution toward uniform
            diluted = gamma * heuristic + (1.0 - gamma) * (np.ones(nA) / nA)

            # Power transform (temperature): p^tau normalized
            transformed = np.power(np.clip(diluted, 1e-12, 1.0), tau)
            probs = transformed / np.sum(transformed)

            p = np.clip(probs[a], 1e-12, 1.0)
            nll -= np.log(p)

            # Update state history
            last_action[s] = a
            last_reward[s] = r

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-adaptive softmax temperature with age-dependent adaptation rate.

    Summary
    - Tabular Q-learning with fixed learning rate alpha.
    - The inverse temperature beta is not fixed; for each state it adapts over time as a function
      of surprise (|TD error|). After confident predictions (small |delta|), beta increases;
      after surprising outcomes (large |delta|), beta decreases.
    - The adaptation learning rate depends on age group (younger vs older).
    - Set size reduces how quickly beta adapts: update magnitude is scaled by (3/set_size).
    - beta is lower-bounded by min_beta to avoid degenerate policies; initialized at beta0.
    - Returns negative log-likelihood of observed choices.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0-based).
    actions : array-like of int
        Chosen action per trial; valid {0,1,2}.
    rewards : array-like of int/float
        Outcome in {0,1}.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6), constant within a block.
    age : array-like (length 1)
        Participant age; <45 -> younger, >=45 -> older (selects eta).
    model_parameters : tuple/list of floats
        (alpha, beta0, eta_young, eta_old, min_beta)
        - alpha: Q-learning rate
        - beta0: initial inverse temperature per state at the start of each block
        - eta_young: adaptation rate of beta for younger participants
        - eta_old: adaptation rate of beta for older participants
        - min_beta: lower bound on beta (ensures non-zero exploration)

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, eta_y, eta_o, min_beta = model_parameters
    age_val = age[0]
    eta_age = eta_y if age_val < 45 else eta_o

    states = np.asarray(states)
    actions = np.asarray(actions)
    rewards = np.asarray(rewards)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)

    nll = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]
        if len(block_states) == 0:
            continue

        nS = int(np.max(block_states) + 1)
        Q = np.zeros((nS, nA))
        beta_state = np.ones(nS) * beta0

        ss = float(block_set_sizes[0])
        adapt_scale = 3.0 / ss  # slower adaptation under higher load

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Use current state's beta
            beta = max(beta_state[s], min_beta)

            # Softmax policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi = exp_logits / np.sum(exp_logits)

            if (a < 0) or (a >= nA) or (r < 0) or (r > 1) or (s < 0) or (s >= nS):
                nll -= np.log(1.0 / nA)
                continue

            p = np.clip(pi[a], 1e-12, 1.0)
            nll -= np.log(p)

            # TD update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Beta adaptation: increase with confidence (1 - |delta|), decrease with surprise
            # Centered so that when confidence = 0.5 there is no change.
            confidence = 1.0 - abs(delta)  # in [0,1]
            beta_state[s] = max(
                min_beta,
                beta_state[s] + adapt_scale * eta_age * (confidence - 0.5)
            )

    return nll