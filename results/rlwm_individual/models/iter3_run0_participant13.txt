def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise- and set-size-adaptive WM gating and WM decay.

    Core idea:
    - Arbitration: The effective WM weight increases with surprise (unsigned RL prediction error),
      but decreases with set size (higher load). This captures that people rely more on WM for
      surprising/diagnostic outcomes when load permits.
    - WM store: A decaying categorical memory that updates strongly after rewards and weakly after
      non-rewards. It decays toward a uniform prior within the block.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        gamma_size : float
            Set-size penalty scaling (>0). Larger values reduce WM influence more in nS=6 vs nS=3.
        pe_sens : float
            Sensitivity of WM gating to unsigned prediction error (>=0).
        wm_decay : float
            Per-trial decay rate of WM rows toward uniform prior (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gamma_size, pe_sens, wm_decay = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size modulation term (less WM trust at higher load)
        # Scales between ~1 (nS=3) and smaller values (nS=6) depending on gamma_size
        size_mod = 1.0 / (1.0 + gamma_size * max(0, nS - 3))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            # Surprise-adaptive WM weighting using unsigned prediction error
            pe = abs(r - Q_s[a])
            pe_gate = 1.0 - np.exp(-pe_sens * pe)  # in [0,1], rises with surprise
            eff_wm_weight = np.clip(wm_weight * size_mod * pe_gate, 0.0, 1.0)

            p_total = p_wm*eff_wm_weight + (1-eff_wm_weight)*p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: global decay then local update (reward > non-reward)
            # Decay entire matrix slightly toward uniform prior
            if wm_decay > 0:
                w = (1 - wm_decay) * w + wm_decay * w_0

            # Reward-strengthened categorical update
            alpha_pos = 0.9
            alpha_neg = 0.2
            if r > 0.0:
                w[s, :] = (1 - alpha_pos) * w[s, :]
                w[s, a] += alpha_pos
            else:
                w[s, :] = (1 - alpha_neg) * w[s, :]
                w[s, a] += alpha_neg

            # Normalize row to maintain a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with probabilistic WM availability and RL forgetting.

    Core idea:
    - Arbitration: WM is not always successfully retrieved; its availability increases with
      "capacity pressure" parameter and smaller set sizes. The effective mixture is hence
      wm_weight * p_avail(nS).
    - RL: Includes a small value forgetting/decay towards uniform (captures interference/memory drift).
    - WM store: Reinforced more strongly after rewards via wm_eta, with mild suppression on errors.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        theta_slots : float
            Capacity pressure (>0). Higher values increase WM availability at small set sizes
            but saturate for larger sets: p_avail = 1 - exp(-theta_slots / nS).
        wm_eta : float
            WM learning/update strength (0-1), applied more on rewards and less on non-rewards.
        q_decay : float
            Per-trial RL value decay toward uniform (0-1), modeling forgetting/interference.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, theta_slots, wm_eta, q_decay = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM availability as a function of set size
        p_avail = 1.0 - np.exp(-max(0.0, theta_slots) / max(1.0, nS))  # in [0,1]
        base_mix = np.clip(wm_weight * p_avail, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            # Mixture with retrieval availability
            eff_wm_weight = base_mix
            p_total = p_wm*eff_wm_weight + (1-eff_wm_weight)*p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            # RL update with forgetting toward uniform
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            if q_decay > 0:
                q = (1 - q_decay) * q + q_decay * (1.0/nA)

            # WM update: reward-weighted category update with wm_eta, smaller on errors
            pos = np.clip(wm_eta, 0.0, 1.0)
            neg = 0.25 * pos
            if r > 0.0:
                w[s, :] = (1 - pos) * w[s, :]
                w[s, a] += pos
            else:
                w[s, :] = (1 - neg) * w[s, :]
                w[s, a] += neg

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and WM leak.

    Core idea:
    - Arbitration: Weight on WM depends on relative certainty: higher when WM is certain
      (peaked distribution) and RL is uncertain (high entropy). A set-size penalty further
      reduces WM influence at larger loads.
    - WM store: Leaky integrator toward prior with reward-driven sharpening.
    - RL: Standard delta rule.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        k_arbit : float
            Arbitration gain (>0). Scales sensitivity to certainty differences between WM and RL.
        alpha_size : float
            Set-size penalty (>=0). Larger values reduce WM weight as nS grows: 1/(1+alpha_size*(nS-3)+).
        wm_leak : float
            Leak of WM toward uniform per trial (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, k_arbit, alpha_size, wm_leak = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size penalty
        size_pen = 1.0 / (1.0 + max(0.0, alpha_size) * max(0, nS - 3))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm*(W_s - W_s[a])))

            # Compute RL uncertainty (entropy of RL policy) and WM certainty (peakiness)
            pi_rl = np.exp(softmax_beta*(Q_s - np.max(Q_s)))
            pi_rl /= np.sum(pi_rl)
            # Entropy normalized to [0,1] via log(nA)
            H_rl = -np.sum(pi_rl * (np.log(pi_rl + eps))) / np.log(nA + eps)

            wm_peak = np.max(W_s)  # in [1/nA, 1]; higher means more certain WM
            wm_cert = (wm_peak - 1.0/nA) / (1.0 - 1.0/nA)  # map to [0,1]

            # Arbitration in [0,1]: stronger WM when WM is certain and RL is uncertain
            signal = wm_cert - H_rl  # positive favors WM
            gate = 1.0 / (1.0 + np.exp(-k_arbit * signal))
            eff_wm_weight = np.clip(wm_weight * size_pen * gate, 0.0, 1.0)

            p_total = p_wm*eff_wm_weight + (1-eff_wm_weight)*p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM leak toward prior then reward-based sharpening
            if wm_leak > 0:
                w = (1 - wm_leak) * w + wm_leak * w_0

            sharpen_pos = 0.8
            sharpen_neg = 0.2
            if r > 0.0:
                w[s, :] = (1 - sharpen_pos) * w[s, :]
                w[s, a] += sharpen_pos
            else:
                w[s, :] = (1 - sharpen_neg) * w[s, :]
                w[s, a] += sharpen_neg

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p