def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying working-memory (WM) cache with age- and load-dependent gating.

    The model mixes a standard RL system with a WM cache that stores the last rewarded action
    per state. WM availability decays with the time since last successful recall and is capacity
    limited relative to the set size. Younger participants have more effective WM (slower decay,
    higher effective capacity), while older show faster decay and reduced capacity. The mixture
    weight between WM and RL is state- and time-dependent, modulated by set size, decay, and age.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size corresponding to the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group (younger <45, older >=45).
    model_parameters : tuple/list of floats
        (alpha, beta, wm_beta, wm_decay_base, capacity_base, mix_bias)
        - alpha: RL learning rate (squashed to 0..1).
        - beta: inverse temperature for RL softmax (scaled internally).
        - wm_beta: inverse temperature for WM softmax over stored action (scaled internally).
        - wm_decay_base: base WM decay rate per trial; transformed and increased for older group.
        - capacity_base: controls effective WM capacity (in states), reduced in older group.
        - mix_bias: scales the WM mixture weight via a sigmoid; larger => more WM usage.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    nA = 3
    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    alpha, beta, wm_beta, wm_decay_base, capacity_base, mix_bias = model_parameters

    # Parameter transforms
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 15.0
    wm_beta = (1.0 / (1.0 + np.exp(-wm_beta))) * 20.0

    base_decay = 1.0 / (1.0 + np.exp(-wm_decay_base))  # 0..1
    decay_age_mult = 1.0 + 0.6 * older  # older -> faster decay
    # Cap decay to <1 per trial to avoid exploding lags
    decay_rate = min(base_decay * decay_age_mult, 0.95)

    K_base = 3.0 + 3.0 * (1.0 / (1.0 + np.exp(-capacity_base)))  # 3..6
    K_eff_age = max(0.0, K_base - 1.5 * older)  # older -> reduced effective capacity

    mix_gain = 1.0 / (1.0 + np.exp(-mix_bias))  # 0..1 scaling of mixture

    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM cache: store last rewarded action and last-seen (successful) trial index
        has_mem = np.zeros(nS, dtype=bool)
        mem_action = -np.ones(nS, dtype=int)
        last_success_t = -np.ones(nS, dtype=int)

        t_counter = 0
        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            prefs_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            pi_rl = np.exp(prefs_rl)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # WM policy: if memory exists, make it sharp on the stored action
            if has_mem[s] and mem_action[s] >= 0 and last_success_t[s] >= 0:
                lag = t_counter - last_success_t[s]
                p_rec = np.exp(-decay_rate * max(0, lag))
                # Softmax centered on the stored action
                wm_vals = np.zeros(nA)
                wm_vals[mem_action[s]] = 1.0
                prefs_wm = wm_beta * (wm_vals - np.max(wm_vals))
                pi_wm = np.exp(prefs_wm)
                pi_wm = pi_wm / (np.sum(pi_wm) + eps)
            else:
                # No memory -> uniform (equivalent to very low beta)
                p_rec = 0.0
                pi_wm = np.ones(nA) / nA

            # Capacity-based availability for this state given set size
            cap_frac = min(1.0, K_eff_age / max(1, nS))
            # Younger participants leverage WM slightly more when available
            age_fac = 1.0 + 0.2 * (1 - older)
            # Combined WM weight for this state at this time
            w_state = mix_gain * cap_frac * p_rec * age_fac
            w_state = max(0.0, min(1.0, w_state))

            pi = w_state * pi_wm + (1.0 - w_state) * pi_rl
            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store correct action on reward
            if r >= 0.5:
                has_mem[s] = True
                mem_action[s] = a
                last_success_t[s] = t_counter

            t_counter += 1

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with reward-/punishment-sensitivity and directed exploration (uncertainty bonus)
    modulated by age and set size.

    The model is a single RL system that:
      - Transforms outcomes with separate sensitivities to rewards (rho) and non-rewards (kappa).
      - Adds a directed exploration bonus that favors less-sampled actions (UCB-like), scaled
        by set size and age (older vs. younger).
      - Uses a softmax policy without perseveration or lapse.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions (0..nA-1).
    rewards : array-like of int
        Binary outcomes (0 or 1).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size corresponding to each trial's block.
    age : array-like of float
        Participant age; age[0] defines age group (older>=45).
    model_parameters : tuple/list of floats
        (alpha, beta, rho, kappa, explore_bonus, age_explore_mod)
        - alpha: learning rate (squashed to 0..1).
        - beta: inverse temperature (scaled internally).
        - rho: reward sensitivity (>0 increases impact of r==1).
        - kappa: punishing sensitivity (>0 increases impact of r==0 as negative signal).
        - explore_bonus: coefficient for uncertainty bonus (UCB-like).
        - age_explore_mod: multiplies exploration for older participants.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    nA = 3
    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    alpha, beta, rho, kappa, explore_bonus, age_explore_mod = model_parameters
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 15.0

    # Ensure positive sensitivities and exploration coefficients
    rho = np.exp(rho)  # >0
    kappa = np.exp(kappa)  # >0
    explore_bonus = np.exp(explore_bonus)  # >0
    age_bonus_mult = 1.0 + (np.tanh(age_explore_mod)) * older  # older modulates exploration

    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts

        # Set-size multiplier for exploration: larger set size -> stronger directed exploration
        size_mult = max(1.0, nS / 3.0)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Directed exploration bonus: higher for less tried actions
            bonus = explore_bonus * age_bonus_mult * size_mult / np.sqrt(N[s, :] + 1.0)

            prefs = Q[s, :] + bonus
            prefs = prefs - np.max(prefs)
            pi = np.exp(beta * prefs)
            pi = pi / (np.sum(pi) + eps)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Outcome transformation: reward vs non-reward sensitivities
            r_eff = rho * r - kappa * (1.0 - r)
            delta = r_eff - Q[s, a]
            Q[s, a] += alpha * delta

            N[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian action-belief model with forgetting and WSLS bias, with age- and load-dependent gain.

    The model maintains a Dirichlet belief over the correct action per state. Rewards
    increment evidence for the chosen action; non-rewards add weak evidence for alternatives.
    Beliefs undergo forgetting toward the prior, stronger in larger set sizes and for older adults.
    Choices are made by softmax over biased expected correctness (posterior mean), where a WSLS
    (win-stay/lose-shift) bias nudges choices based on the previous outcome in the same state.
    Younger adults in small set sizes have stronger decision gain; older adults in large set
    sizes have reduced gain.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions (0..nA-1).
    rewards : array-like of int
        Binary outcomes (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the trial's block.
    age : array-like of float
        Participant age; age[0] defines age group (older>=45).
    model_parameters : tuple/list of floats
        (beta, prior_conc, wsls_bias, age_gate, forget_rate)
        - beta: base inverse temperature for action selection (scaled internally).
        - prior_conc: prior Dirichlet concentration per action (>0 via softplus).
        - wsls_bias: WSLS preference magnitude added to last action depending on last outcome.
        - age_gate: modulates decision gain by age and set size: increases beta for younger in
                    small sets and decreases beta for older in large sets.
        - forget_rate: base forgetting rate driving beliefs toward the prior; stronger with
                       larger set sizes and older age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eps = 1e-12
    nA = 3
    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    beta, prior_conc, wsls_bias, age_gate, forget_rate = model_parameters

    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    prior_conc = np.log1p(np.exp(prior_conc)) + 1e-6  # softplus, >0
    wsls_bias = wsls_bias  # can be positive or negative
    age_gate = np.tanh(age_gate)  # -1..1
    f_base = 1.0 / (1.0 + np.exp(-forget_rate))  # 0..1

    nll = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_set_sizes = set_sizes[idx]
        nS = int(block_set_sizes[0])

        # Dirichlet concentrations per state-action
        C = np.ones((nS, nA)) * prior_conc

        # Track last action and its outcome per state for WSLS
        last_a = -np.ones(nS, dtype=int)
        last_r = np.zeros(nS) * np.nan

        # Forgetting modulation by set size and age
        size_mult = max(1.0, nS / 3.0)
        f_eff = min(0.9, f_base * size_mult * (1.0 + 0.5 * older))

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Apply forgetting toward the prior before computing policy
            C[s, :] = (1.0 - f_eff) * C[s, :] + f_eff * prior_conc

            # Posterior mean of correctness
            mean_p = C[s, :] / (np.sum(C[s, :]) + eps)

            # WSLS bias
            bias = np.zeros(nA)
            if last_a[s] >= 0:
                if last_r[s] >= 0.5:
                    bias[last_a[s]] += wsls_bias
                else:
                    bias[last_a[s]] -= wsls_bias

            # Age- and load-dependent gain on choice
            gain = 1.0
            if nS == 3 and older == 0:
                gain *= (1.0 + max(0.0, age_gate))  # strengthen for younger in small sets
            if nS == 6 and older == 1:
                gain *= (1.0 - max(0.0, -age_gate))  # weaken for older in large sets
            beta_eff = beta * max(0.1, gain)

            prefs = mean_p + bias
            prefs = prefs - np.max(prefs)
            pi = np.exp(beta_eff * prefs)
            pi = pi / (np.sum(pi) + eps)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Update Dirichlet counts based on outcome
            if r >= 0.5:
                C[s, a] += 1.0
            else:
                # Add weak evidence to alternatives when chosen action fails
                others = [aa for aa in range(nA) if aa != a]
                C[s, others] += 0.5

            last_a[s] = a
            last_r[s] = r

    return nll