def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with uncertainty- and set-size–scaled WM gating.

    Policy:
      - RL: tabular Q-learning with softmax.
      - WM: associative table with softmax; decays toward uniform; reward-gated strengthening.
      - Mixture: p = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.
      - Gating: wm_weight_eff scales down with set size (1/nS) and with RL uncertainty
        via state-wise RL entropy H(Q_s): wm_weight_eff = wm_base * (1/nS) * (1 - H_norm),
        where H_norm = H / log(nA).

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - softmax_beta: RL inverse temperature; internally scaled by 10.
      - wm_base: base WM mixture weight before scaling (0-1).
      - wm_beta_scale: scales the very deterministic WM temperature (>=0).
      - wm_decay: WM decay/learning step size toward uniform and toward rewarded one-hot (0-1).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, wm_beta_scale, wm_decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of the chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax with scaled high inverse temperature)
            beta_wm_eff = softmax_beta_wm * max(0.0, wm_beta_scale)
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Uncertainty (entropy) of RL policy at this state
            # Use a moderate-temperature softmax to create a distribution for entropy
            p_q = np.exp(Q_s - np.max(Q_s))
            p_q = p_q / np.sum(p_q)
            H = -np.sum(p_q * np.log(np.clip(p_q, 1e-12, 1.0)))
            H_norm = H / np.log(nA)

            # WM gating scales with set size and certainty (low entropy => more WM)
            wm_weight_eff = wm_base * (1.0 / float(nS)) * (1.0 - H_norm)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform each trial
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-gated WM strengthening toward a one-hot code for the chosen action
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value forgetting + WM with one-shot storage and interference-scaled decay.

    Policy:
      - RL: tabular Q-learning with softmax; Q-values globally forget toward uniform.
      - WM: one-shot storage upon reward; competitive decay scales up with set size (interference).
      - Mixture: constant wm_weight scaled down by interference from set size.

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - softmax_beta: RL inverse temperature; internally scaled by 10.
      - wm_weight: base WM mixture weight (0-1).
      - wm_one_shot: WM write-in strength upon reward (0-1).
      - interference: scales WM decay and weight reduction with set size (>=0).
      - q_forget: RL forgetting rate toward uniform each trial (0-1).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, wm_one_shot, interference, q_forget = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Interference-driven WM weight reduction with larger set sizes
        wm_weight_eff_block = wm_weight / (1.0 + interference * max(0.0, float(nS - 1)))
        wm_weight_eff_block = np.clip(wm_weight_eff_block, 0.0, 1.0)

        # Base WM decay per access grows with set size (more interference)
        base_wm_decay = np.clip(interference * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff_block * p_wm + (1 - wm_weight_eff_block) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL forgetting toward uniform (global), then standard update
            q[s, :] = (1 - q_forget) * q[s, :] + q_forget * (1.0 / nA)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM interference decay toward uniform
            w[s, :] = (1 - base_wm_decay) * w[s, :] + base_wm_decay * w_0[s, :]

            # One-shot WM write on reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_one_shot) * w[s, :] + wm_one_shot * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with novelty bonus + WM with fast encoding and a set-size–dependent lapse.

    Policy:
      - RL: tabular Q-learning with softmax over Q + novelty bonus b(s,a) = kappa_novel / sqrt(N_sa+1).
      - WM: fast encoding when rewarded; decays to uniform.
      - Mixture: fixed wm_weight (scaled by 3/nS), then mixed with a lapse to uniform that increases with set size.
      - Final: p = (1 - lapse) * [wm_mix] + lapse * (1/nA).

    Parameters (model_parameters):
      - lr: RL learning rate (0-1).
      - softmax_beta: RL inverse temperature; internally scaled by 10.
      - wm_weight: base WM mixture weight (0-1).
      - wm_eta: WM learning rate toward one-hot on reward (0-1).
      - lapse_base: base lapse probability scaled by set size (0-1).
      - kappa_novel: novelty bonus magnitude for untried/rare actions (>=0).

    Returns:
      - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, wm_eta, lapse_base, kappa_novel = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA))  # for novelty bonus

        # Set-size scaling for WM and lapse
        wm_weight_eff_block = np.clip(wm_weight * (3.0 / float(nS)), 0.0, 1.0)
        lapse = np.clip(lapse_base * (float(nS) / 6.0), 0.0, 0.5)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # Novelty bonus for RL policy
            bonus = kappa_novel / np.sqrt(visits[s, :] + 1.0)
            Q_eff = q[s, :] + bonus

            # RL policy using Q + novelty bonus
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy (deterministic softmax)
            W_s = w[s, :]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL/WM mixture then lapse to uniform
            p_mix = wm_weight_eff_block * p_wm + (1 - wm_weight_eff_block) * p_rl
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # Update visit counts for novelty
            visits[s, a] += 1.0

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]
            # WM fast encoding on reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_eta) * w[s, :] + wm_eta * target

        blocks_log_p += log_p

    return -blocks_log_p