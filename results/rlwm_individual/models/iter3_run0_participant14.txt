def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-adaptive WM mixture and recency-boosted WM usage.

    Idea:
    - A standard model-free RL learns Q-values via a single learning rate.
    - WM stores a high-precision associative policy for rewarded state-action pairs.
    - The policy at decision time is a mixture between RL and WM. The WM mixture weight
      is adapted by (i) set size (load) via a logistic slope, and (ii) current WM
      confidence (sharper WM distributions indicate stronger WM).
    - WM precision determines how sharp the one-shot encoded distribution is upon reward.
    - A recency boost increases the WM contribution when the WM distribution is confident.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: scalar >= 0, RL inverse temperature (internally scaled by 10).
    - mix_bias: real, baseline bias for WM mixture (logit space).
    - load_slope: real, how strongly set size modulates WM mixture (positive -> more WM in small sets).
    - wm_precision: scalar >= 0, sharpness of WM encoding (higher -> more one-hot).
    - recency_boost: scalar >= 0, scales the confidence-based boost of WM mixture.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, mix_bias, load_slope, wm_precision, recency_boost = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from current WM distribution
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM confidence to modulate its weight (entropy-based)
            Wc = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * Wc)
            pvec_wm = expW / np.sum(expW)
            entropy_wm = -np.sum(pvec_wm * np.log(pvec_wm + 1e-12))
            conf_wm = 1.0 - entropy_wm / np.log(nA)  # 0..1

            # Load-adaptive WM mixing: logistic transform of bias + load + confidence boost
            load_term = load_slope * (3.0 - nS) / 3.0  # positive when nS=3 if load_slope>0
            wm_weight_eff = 1.0 / (1.0 + np.exp(-(mix_bias + load_term + recency_boost * conf_wm)))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # On rewarded trials, encode a sharp distribution favoring the chosen action.
            # Target distribution T is a softmax over a one-hot with precision wm_precision.
            if r > 0:
                T = np.ones(nA)
                T[:] = 0.0
                T[a] = 1.0
                # Create a peaked target distribution using wm_precision
                logits = wm_precision * (T - 1.0 / nA)  # center so non-chosen are negative
                logits = logits - np.max(logits)
                exp_logits = np.exp(logits)
                T = exp_logits / np.sum(exp_logits)
                # Write toward the target distribution
                w[s, :] = T
            else:
                # No reward: leave WM unchanged (no anti-update), keeping precision advantage for rewarded pairs
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent WM decay, fast WM learning, and lapse noise.

    Idea:
    - Model-free RL with a single learning rate and softmax decision rule.
    - WM is a rapidly updated associative store with its own learning rate that moves
      toward a one-hot policy on rewarded trials and slightly diffuses otherwise.
    - WM globally decays toward uniform per trial; the decay increases with set size (load).
    - Final policy is a mixture of WM and RL, followed by a lapse component that injects
      uniform random choices with a small probability.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: scalar >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight_base: scalar in [0,1], base weight of WM in the policy mixture.
    - wm_learn_rate: scalar in [0,1], how strongly WM writes on each trial.
    - wm_decay_load: scalar >= 0, controls how fast WM decays per trial with set size.
    - lapse_rate: scalar in [0,1], probability of a random lapse choice.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_weight_base, wm_learn_rate, wm_decay_load, lapse_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent global decay per step (stronger decay when nS is larger)
        wm_decay = 1.0 - np.exp(-wm_decay_load * max(1.0, nS) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global WM decay toward uniform prior
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL with base WM weight
            p_mix = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl

            # Apply lapse noise towards uniform
            p_total = (1.0 - lapse_rate) * p_mix + lapse_rate * (1.0 / nA)
            log_p += np.log(p_total + 1e-12)

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rewarded trials: move WM toward one-hot for chosen action.
            # Unrewarded: slight diffusion toward uniform.
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_learn_rate) * w[s, :] + wm_learn_rate * target
            else:
                # Diffuse toward uniform slightly to avoid sticking to incorrect mappings
                w[s, :] = (1.0 - 0.5 * wm_learn_rate) * w[s, :] + (0.5 * wm_learn_rate) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with PE-gated WM influence and novelty-driven exploration in RL.

    Idea:
    - RL learns Q-values but is augmented with a novelty bonus based on action-specific visit counts
      within each state (less tried actions are favored).
    - WM contributes a fast, high-precision policy; its weight in the mixture is gated by recent
      surprise (absolute prediction error) and penalized by load.
    - WM writes strongly upon high surprise and rewards; on errors, WM partially resets toward uniform.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: scalar >= 0, RL inverse temperature (internally scaled by 10).
    - wm_base_weight: real, base logit for the WM mixture weight (before gating).
    - pe_sensitivity: scalar >= 0, scales how much recent |PE| increases WM mixture weight.
    - load_penalty: scalar >= 0, scales how much larger set size reduces WM mixture weight.
    - novelty_bonus: scalar >= 0, added to RL values as 1/sqrt(1+count) per action to encourage exploration.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_base_weight, pe_sensitivity, load_penalty, novelty_bonus = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track per-state action visit counts for novelty
        counts = np.zeros((nS, nA))
        # Track recent absolute PE per state to gate WM
        pe_trace = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Novelty bonus for RL: larger for less tried actions
            bonus_vec = novelty_bonus / np.sqrt(1.0 + counts[s, :])
            Q_s_bonus = Q_s + bonus_vec

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s_bonus - Q_s_bonus[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM mixture weight gated by recent surprise and penalized by load
            load_term = -load_penalty * (nS - 3.0) / 3.0  # reduces weight when nS=6
            wm_logit = wm_base_weight + pe_sensitivity * pe_trace[s] + load_term
            wm_weight_eff = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # Update counts after observing choice
            counts[s, a] += 1.0

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # PE-gated WM writing: stronger write when surprise is high and outcome is rewarding.
            abs_pe = np.abs(delta)
            if r > 0:
                write_strength = np.clip(abs_pe, 0.0, 1.0)
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * target
            else:
                # On errors, partially reset toward uniform to avoid locking in incorrect associations
                reset_strength = 0.5 * np.clip(abs_pe, 0.0, 1.0)
                w[s, :] = (1.0 - reset_strength) * w[s, :] + reset_strength * w_0[s, :]

            # Update PE trace for this state (no decay term needed within short blocks)
            pe_trace[s] = abs_pe

        blocks_log_p += log_p

    return -blocks_log_p