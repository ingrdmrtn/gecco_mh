def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + uncertainty-based WM arbitration with age- and load-modulated decay and lapse.

    Mechanism:
    - Model-free RL (Q-learning) updated each trial.
    - Simple WM store per state that caches the last rewarded action, with decay toward uniform.
    - Arbitration weight between RL and WM is based on relative uncertainty (entropy) of each system.
    - Learning rate and WM decay are modulated by set size (load) and age group.
    - Final decision includes a load- and age-modulated lapse.

    Parameters (model_parameters = [alpha0, beta, k_uncert, phi_decay, lapse_base]):
    - alpha0: base RL learning rate (0..1), decays within block with phi_decay and age.
    - beta: inverse temperature for softmax (>0).
    - k_uncert: slope controlling arbitration sensitivity to uncertainty difference.
    - phi_decay: scales within-block decay of RL learning rate and WM strength with load and age.
    - lapse_base: base lapse level scaled by load and age (0..1).

    Inputs:
    - states: array of int, state index on each trial within block.
    - actions: array of int, chosen action on each trial (0..2); out-of-range treated as lapses.
    - rewards: array of numeric, mapped to 0/1.
    - blocks: array of int, block index per trial (learning resets per block).
    - set_sizes: array of int, set size (cardinality of states) per trial (3 or 6).
    - age: array-like with one element, participant age in years.
    - model_parameters: list/tuple of 5 floats [alpha0, beta, k_uncert, phi_decay, lapse_base].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha0, beta, k_uncert, phi_decay, lapse_base = model_parameters
    is_older = 1 if age[0] >= 45 else 0

    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(b_set_sizes[0]) if len(b_set_sizes) > 0 else 0
        nS = max(nS, int(np.max(b_states)+1) if len(b_states) > 0 else 0)

        # Q-learning values
        Q = np.zeros((nS, nA))
        # WM distribution per state (probability over actions). Start as uniform.
        WM = np.ones((nS, nA)) / nA

        t_block = 0
        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = max(1, int(b_set_sizes[t]))
            t_block += 1

            # Effective learning rate decays within block, more under load and in older group
            alpha_eff = alpha0 / (1.0 + phi_decay * (t_block - 1) * (1.0 + 0.5 * is_older) * (load / 6.0))
            alpha_eff = max(0.0, min(1.0, alpha_eff))

            # RL policy
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM decay toward uniform depends on load and age via phi_decay
            decay = min(0.95, phi_decay * (load / 6.0) * (1.0 + 0.5 * is_older))
            WM[s, :] = (1.0 - decay) * WM[s, :] + decay * (np.ones(nA) / nA)

            # If previously rewarded, WM is peaked on that action (reflecting a cache)
            # We implement the cache by boosting the max entry if it exists (> uniform).
            if np.max(WM[s, :]) > (1.0 / nA) + 1e-6:
                p_wm = WM[s, :].copy()
            else:
                p_wm = WM[s, :].copy()

            # Compute entropies (uncertainties)
            def entropy(p):
                p_clip = np.clip(p, eps, 1.0)
                return -np.sum(p_clip * np.log(p_clip))
            u_rl = entropy(p_rl)
            u_wm = entropy(p_wm)

            # Arbitration: more WM weight when WM is less uncertain than RL
            w_wm = 1.0 / (1.0 + np.exp(-k_uncert * (u_rl - u_wm)))
            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl

            # Lapse increases with load and in older adults
            lapse = lapse_base * max(0.0, (load - 3.0) / 3.0) * (1.0 + 0.5 * is_older)
            lapse = max(0.0, min(0.49, lapse))
            p = (1.0 - lapse) * p_mix + lapse * (np.ones(nA) / nA)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # RL update
            if 0 <= a < nA:
                delta = r - Q[s, a]
                Q[s, a] += alpha_eff * delta

            # WM cache update: reward strengthens the remembered action
            if 0 <= a < nA:
                if r > 0.0:
                    # Move WM[s] toward one-hot on action a
                    one_hot = np.zeros(nA)
                    one_hot[a] = 1.0
                    WM[s, :] = (1.0 - decay) * one_hot + decay * (np.ones(nA) / nA)

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian action-value learner with age-dependent priors and exploration bonus (UCB-like).

    Mechanism:
    - For each state-action pair, maintain Bernoulli-Beta posterior over reward probability.
    - Decision values are posterior means plus an exploration bonus that shrinks with experience.
    - Age group determines prior concentration (older adults assumed more conservative priors).
    - Bonus is attenuated by set size (higher load) and by age.
    - Final policy includes a small lapse.

    Parameters (model_parameters = [beta, conc_young, conc_old, xi_bonus, lapse]):
    - beta: inverse temperature for softmax over decision values (>0).
    - conc_young: prior concentration (alpha=beta=conc_young) for younger group.
    - conc_old: prior concentration (alpha=beta=conc_old) for older group.
    - xi_bonus: scale of exploration bonus (>0).
    - lapse: base lapse probability (0..1), used directly.

    Inputs:
    - states, actions, rewards, blocks, set_sizes as described.
    - age: array-like with one element; used to select prior concentration.
    - model_parameters: list/tuple of 5 floats [beta, conc_young, conc_old, xi_bonus, lapse].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    beta, conc_y, conc_o, xi_bonus, lapse = model_parameters
    is_older = 1 if age[0] >= 45 else 0
    prior_conc = conc_o if is_older else conc_y
    prior_conc = max(prior_conc, 1e-6)

    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(b_set_sizes[0]) if len(b_set_sizes) > 0 else 0
        nS = max(nS, int(np.max(b_states)+1) if len(b_states) > 0 else 0)

        # Posterior parameters: successes (alpha) and failures (beta) per (s,a)
        suc = np.zeros((nS, nA)) + prior_conc  # alpha prior
        fail = np.zeros((nS, nA)) + prior_conc  # beta prior
        counts = np.zeros((nS, nA))  # for bonus

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = max(1, int(b_set_sizes[t]))

            # Posterior mean for reward probability
            mean_p = suc[s, :] / (suc[s, :] + fail[s, :] + eps)

            # Exploration bonus: larger when counts small; shrink with load and age
            # Using sqrt(log(1+total)/ (1+count)) form
            total_n = np.sum(counts[s, :]) + 1.0
            bonus_scale = xi_bonus * (1.0 - 0.3 * is_older) * (3.0 / max(3.0, float(load)))
            bonus = bonus_scale * np.sqrt(np.log(1.0 + total_n) / (1.0 + counts[s, :]))

            values = mean_p + bonus

            # Softmax with lapse
            v = values - np.max(values)
            p_soft = np.exp(beta * v)
            p_soft = p_soft / (np.sum(p_soft) + eps)
            lapse_clipped = max(0.0, min(0.49, lapse))
            p = (1.0 - lapse_clipped) * p_soft + lapse_clipped * (np.ones(nA) / nA)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # Update posterior
            if 0 <= a < nA:
                suc[s, a] += r
                fail[s, a] += (1.0 - r)
                counts[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with eligibility traces and age/load-modulated stickiness.

    Mechanism:
    - Standard Q-learning with replacing eligibility traces on (s,a).
    - Trace decay rate increases with set size and in older adults (shallower credit assignment).
    - Action perseveration bias (stickiness) is added to decision values and decays with the same
      effective rate; older adults under higher load retain stickiness longer/shorter depending on k.
    - No explicit WM module; all effects are captured via temporal credit assignment and bias.

    Parameters (model_parameters = [alpha, beta, tau_e, k_age_load, stick0]):
    - alpha: learning rate for Q updates (0..1).
    - beta: inverse temperature for softmax (>0).
    - tau_e: base trace decay rate per trial (>0); larger => faster decay.
    - k_age_load: scales decay with load and age; effective decay multiplier = exp(-tau_e * g),
                  where g = (load/6) * (1 + k_age_load * is_older).
    - stick0: base magnitude added to last-chosen action in policy (>0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes as described.
    - age: array-like with one element; used to modulate decay via is_older.
    - model_parameters: list/tuple of 5 floats [alpha, beta, tau_e, k_age_load, stick0].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, tau_e, k_age_load, stick0 = model_parameters
    is_older = 1 if age[0] >= 45 else 0
    eps = 1e-12
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(b_set_sizes[0]) if len(b_set_sizes) > 0 else 0
        nS = max(nS, int(np.max(b_states)+1) if len(b_states) > 0 else 0)

        Q = np.zeros((nS, nA))
        # Eligibility traces
        z = np.zeros((nS, nA))
        # Stickiness vector added to values at decision time
        stick = np.zeros(nA)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = max(1, int(b_set_sizes[t]))

            # Effective decay factor
            g = (float(load) / 6.0) * (1.0 + k_age_load * is_older)
            decay = np.exp(-tau_e * max(0.0, g))
            decay = max(0.0, min(1.0, decay))

            # Decay traces and stickiness each trial
            z *= decay
            stick *= decay

            # Decision values = Q[s] + stickiness bias
            vals = Q[s, :] + stick
            vals = vals - np.max(vals)
            p_soft = np.exp(beta * vals)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p_soft[a]
                nll -= np.log(max(p_a, eps))

            # Update eligibility trace (replacing)
            if 0 <= a < nA:
                z[s, :] = 0.0
                z[s, a] = 1.0

                # TD error
                delta = r - Q[s, a]
                # Q update through trace
                Q += alpha * delta * z

                # Update stickiness: add bias to chosen action
                stick *= decay  # apply extra decay before adding new bias
                stick[a] += stick0

    return nll