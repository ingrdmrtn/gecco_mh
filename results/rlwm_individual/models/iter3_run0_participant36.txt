def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited Working Memory (WM) with probabilistic gating and decay.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy.
    - WM system: stores up to 'capacity' states with one-hot correct actions when rewarded.
      Encoding is probabilistic via a gating mechanism that increases with surprise.
      WM items decay toward uniform when revisited without reward. WM contribution scales
      down with set size relative to capacity.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight_base: Base WM mixture weight when load <= capacity (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - capacity: WM capacity in number of states (0..6). Real-valued allowed; effective weight uses min(capacity/nS,1).
    - wm_decay: Leak of WM toward uniform on trials without reward (0..1).
    - gate_gain: Strength of surprise-dependent gating for WM writes (>=0). Higher => more likely to store when PE is large.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, capacity, wm_decay, gate_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track which states are currently stored in WM (for capacity management; LRU)
        stored_flags = np.zeros(nS, dtype=bool)
        lru_order = []  # list of state indices, most recent at the end

        # Load-adjusted WM weight: scales by how much of the set size can be covered by capacity
        cover = 0.0 if nS <= 0 else min(1.0, max(0.0, capacity) / float(nS))
        wm_weight_block = np.clip(wm_weight_base * cover, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values for this state
            # If the state is not stored, W_s will be near-uniform, producing near-uniform WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix RL and WM
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updates:
            # 1) If rewarded, probabilistically gate the write based on surprise |delta|
            #    Gate probability uses a saturating function of |delta| scaled by gate_gain.
            #    If written and over capacity, evict least-recently used state.
            # 2) If not rewarded, leak WM for this state toward uniform (wm_decay).
            if r > 0.0:
                # Surprise-dependent gating (bounded in [0,1])
                gate_prob = 1.0 - np.exp(-max(0.0, gate_gain) * abs(delta))
                if np.random.rand() < gate_prob:
                    # Write a one-hot memory for action a
                    onehot = np.zeros(nA)
                    onehot[a] = 1.0
                    w[s, :] = onehot

                    # Update LRU bookkeeping
                    if stored_flags[s]:
                        # Move s to the end (most recent)
                        if s in lru_order:
                            lru_order.remove(s)
                        lru_order.append(s)
                    else:
                        # Add s; evict if beyond capacity (use floor of capacity for discrete slots)
                        stored_flags[s] = True
                        lru_order.append(s)
                        max_slots = int(np.floor(max(0.0, capacity)))
                        if max_slots >= 0 and len([x for x in lru_order if stored_flags[x]]) > max_slots:
                            # Evict least recent still-stored state
                            for k in lru_order:
                                if stored_flags[k]:
                                    stored_flags[k] = False
                                    # Upon eviction, revert memory to uniform prior
                                    w[k, :] = w_0[k, :]
                                    lru_order.remove(k)
                                    break
                else:
                    # No write this time; keep current WM content
                    pass
            else:
                # Leak current state's WM toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-driven arbitration and WM leak.

    Overview:
    - RL: delta-rule Q-learning, softmax action selection.
    - WM: fast associative table that becomes peaked for rewarded actions and
      leaks toward uniform otherwise.
    - Arbitration: trial-wise WM weight increases when RL policy is confident
      (low entropy) and decreases with load (larger set size). A logistic
      transform ensures weight stays in [0,1].

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_bias: Baseline logit for WM weight (maps via sigmoid to ~baseline WM use).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - conf_gain: Gain on RL confidence (1 - entropy) to modulate WM weight (can be +/-).
    - wm_leak: WM leak toward uniform on non-reward trials (0..1).
    - load_penalty: Linear penalty per item above 3 on the WM weight logit (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_bias, softmax_beta, conf_gain, wm_leak, load_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy probabilities for entropy computation
            denom = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / max(denom, 1e-12)
            # Entropy in nats; confidence = 1 - normalized entropy (0..1)
            entropy = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            max_entropy = np.log(nA)
            confidence = 1.0 - (entropy / max_entropy if max_entropy > 0 else 0.0)

            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise WM weight: logistic(wm_bias + conf_gain*confidence - load_penalty*excess_load)
            excess_load = max(0, nS - 3)
            logit = wm_bias + conf_gain * confidence - max(0.0, load_penalty) * excess_load
            wm_weight_t = 1.0 / (1.0 + np.exp(-logit))
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward strengthens memory for chosen action; no-reward leaks toward uniform
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with binding errors under load and WM refresh strength.

    Overview:
    - RL: delta-rule Q-learning with softmax policy.
    - WM: stores last rewarded action per state as a peaked distribution.
      Retrieval suffers from binding errors that increase with set size: the
      retrieved WM signal for the current state is blended with memories from
      other states, diluting specificity.
    - WM update strength controls how sharply WM is written and how quickly it
      decays toward uniform on errors.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base mixture weight for WM (0..1), applied equally across loads.
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - bind_slope: Controls how binding error grows with set size beyond 3 (>=0).
    - wm_refresh: WM write/refresh rate: toward one-hot on reward and toward uniform on no-reward (0..1).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, bind_slope, wm_refresh = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM retrieval with binding error: blend current state's memory with others'
            if nS > 1:
                others_idx = [k for k in range(nS) if k != s]
                mean_others = np.mean(w[others_idx, :], axis=0) if len(others_idx) > 0 else w[s, :]
            else:
                mean_others = w[s, :]

            # Binding error grows with load above 3
            excess = max(0, nS - 3)
            bind_err = 1.0 - np.exp(-max(0.0, bind_slope) * excess)
            pref = (1.0 - bind_err) * w[s, :] + bind_err * mean_others

            # WM policy from blended preference
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (pref - pref[a])))

            # Mix
            wm_w = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward sharpens toward chosen action; no-reward decays toward uniform
            if r > 0.0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * onehot
            else:
                w[s, :] = (1.0 - wm_refresh) * w[s, :] + wm_refresh * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p