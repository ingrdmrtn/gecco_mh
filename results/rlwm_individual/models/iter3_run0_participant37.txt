def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates + capacity-scaled WM binding with interference.

    Model idea:
    - RL: standard Q-learning with separate learning rates for positive and negative prediction errors.
    - WM: one-shot binding updated toward a peaked distribution on rewarded choices; otherwise decays.
    - Capacity and set size: WM mixture weight is scaled by an effective capacity gate:
        gate = min(1, (capacity / nS) ** gamma). Larger sets reduce WM influence/interference increases.
    - WM interference/decay increases when set size is large (1 - gate).

    Parameters (model_parameters):
    - lr_pos: scalar in [0,1]. RL learning rate for positive prediction errors (r - Q > 0).
    - lr_neg: scalar in [0,1]. RL learning rate for negative prediction errors (r - Q < 0).
    - wm_base: scalar in [0,1]. Baseline contribution weight of WM before set-size gating.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    - capacity: scalar > 0. Effective WM capacity in number of state-action bindings.
    - gamma: scalar >= 0. Nonlinearity shaping how capacity scales with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_base, softmax_beta, capacity, gamma = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # capacity/set-size gate
        base_gate = min(1.0, (max(1e-9, capacity) / max(1.0, nS)) ** max(0.0, gamma))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: near-deterministic softmax over WM weights for current state
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture weight modulated by capacity vs set size, and local WM sharpness
            sharpness = max(0.0, np.max(W_s) - 1.0/nA) / (1.0 - 1.0/nA + 1e-12)
            wm_weight = np.clip(wm_base * base_gate * sharpness, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr*delta
            # WM updating: rewarded choices produce one-shot binding; otherwise interference/decay
            if r >= 1.0 - 1e-12:
                # Move distribution toward one-hot at chosen action proportional to gate
                target = np.zeros(nA)
                target[a] = 1.0
                eta = base_gate  # stronger update when effective capacity is high
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target
            else:
                # Interference/decay toward uniform scales with lack of capacity
                decay = 0.5 * (1.0 - base_gate)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # normalize for numerical safety
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + WM confidence gating by precision vs RL uncertainty.

    Model idea:
    - RL: Q-learning with global forgetting toward uniform.
    - WM: stores a peaked distribution after reward; confidence c[s] decays over time.
    - Arbitration: compute WM precision (c[s]) vs RL uncertainty (variance of Q_s);
      convert into a dynamic mixture weight via a sigmoid gate. Larger set sizes reduce WM confidence through decay.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    - wm_kappa0: scalar >= 0. Initial WM precision boost upon a rewarded binding.
    - wm_decay: scalar in [0,1]. Decay of WM confidence and WM distribution toward uniform per trial.
    - gate_temp: scalar > 0. Temperature of the arbitration sigmoid; larger -> softer gating.
    - rl_forget: scalar in [0,1]. Global RL forgetting toward uniform each trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_kappa0, wm_decay, gate_temp, rl_forget = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # WM confidence per state
        c = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL global forgetting toward uniform
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: near-deterministic softmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM precision vs RL uncertainty (variance of Q)
            rl_unc = np.mean((Q_s - np.mean(Q_s))**2)
            wm_prec = c[s]  # larger -> more precise WM
            # sigmoid gate on difference in precisions
            gate_input = (wm_prec - rl_unc) / max(1e-6, gate_temp)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating and confidence dynamics
            # decay confidence globally per trial for visited state
            c[s] = (1.0 - wm_decay) * c[s]
            # decay WM distribution toward uniform for current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r >= 1.0 - 1e-12:
                # store a strong binding and boost confidence
                target = np.zeros(nA); target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target  # sharpen upon success
                c[s] += wm_kappa0
            # normalize
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM delta-rule with novelty-driven WM weighting that decays with repetition.

    Model idea:
    - RL: standard Q-learning.
    - WM: delta-rule on a categorical distribution per state:
        reward -> move toward one-hot(chosen), no reward -> move toward uniform.
    - Arbitration: WM mixture weight starts higher for novel states (novelty_bonus) and
      decays toward wm_base with the number of visits to that state (controlled by wm_decay).
      Larger set sizes slow learning implicitly by distributing visits across more states.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate for Q-values.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - wm_lr: scalar in [0,1]. WM delta-rule learning rate.
    - wm_base: scalar in [0,1]. Baseline WM mixture weight after novelty decays.
    - wm_decay: scalar >= 0. Decay rate of novelty bonus per visit (exponential).
    - novelty_bonus: scalar in [0,1]. Extra WM weight for novel states on first encounters.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_lr, wm_base, wm_decay, novelty_bonus = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # visit counters to modulate novelty decay
        visits = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: near-deterministic softmax on WM distribution
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic WM weight: higher for novel states, decays with visits
            # visits[s] counts prior exposures; compute current effective weight
            novelty_factor = np.exp(-wm_decay * visits[s])
            wm_weight = np.clip(wm_base + novelty_bonus * novelty_factor, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM delta-rule update
            if r >= 1.0 - 1e-12:
                target = np.zeros(nA); target[a] = 1.0
            else:
                target = w_0[s, :]  # move toward uniform when feedback is 0
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

            # increment visits for this state after update
            visits[s] += 1

        blocks_log_p += log_p

    return -blocks_log_p