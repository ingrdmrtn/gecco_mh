def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with prediction-error-gated arbitration and load-modulated gating.

    Mechanism:
    - RL learns action values per state via a Rescorla-Wagner rule.
    - WM stores a supervised mapping for rewarded actions, decays toward uniform after non-reward.
    - Arbitration weight for WM is dynamically reduced by the absolute RL prediction error (|PE|),
      with stronger reduction under higher load (nS). This captures that surprising outcomes and
      higher cognitive load push the system to rely more on RL rather than WM.

    Parameters (total 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline WM arbitration weight in [0,1] before gating.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - pe_gate: Sensitivity of WM weight to absolute prediction error (>=0).
    - wm_decay: WM decay rate toward uniform after non-reward in [0,1].
    - wm_learn: WM supervised learning rate toward one-hot after reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, pe_gate, wm_decay, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # PE-gated arbitration, stronger under higher load
            # compute RL prediction error (based on current Q before update)
            pe = abs(r - Q_s[a])
            load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
            gate = pe_gate * (1.0 + load_term) * pe
            # map baseline wm_weight through logit and subtract gate, then back to [0,1]
            eps = 1e-8
            logit_w = np.log(np.clip(wm_weight, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight, eps, 1 - eps))
            wm_weight_t = 1.0 / (1.0 + np.exp(-(logit_w - gate)))

            p_total = wm_weight_t * p_wm + (1 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward strengthens one-hot association; no-reward decays toward uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # keep WM normalized and bounded
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with state-specific memory strength and load-dependent availability.

    Mechanism:
    - RL: Rescorla-Wagner Q-learning per state.
    - WM: supervised mapping updated by reward, but its influence is gated by a per-state
      memory strength m[s] that integrates recent reward and decays with load.
    - Arbitration: WM weight on each trial is wm_weight * m[s] / (1 + (nS - 1)^load_exp).
      Thus, larger set sizes reduce WM contribution, but states with stronger m[s] retain more WM control.

    Parameters (total 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: Base WM weight multiplier in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - rehearse_rate: Step size for updating m[s] toward the received reward in [0,1].
    - tau_wm: Controls WM learning strength scaling with memory strength (>=0).
    - load_exp: Exponent controlling how set size penalizes WM availability (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rehearse_rate, tau_wm, load_exp = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # per-state memory strength in [0,1]
        m = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-dependent availability and state-specific memory strength
            denom = 1.0 + np.power(max(nS - 1, 0), max(load_exp, 0.0))
            wm_weight_t = wm_weight * np.clip(m[s], 0.0, 1.0) / denom

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM supervised update scaled by memory strength
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # learning magnitude increases with m[s] via tau_wm
                alpha_wm_eff = 1.0 - np.exp(-tau_wm * (1.0 + m[s]))
                w[s, :] = (1.0 - alpha_wm_eff) * w[s, :] + alpha_wm_eff * target
            else:
                # decay toward uniform when no reward
                # decay speed modestly reduced when m[s] is strong (harder to forget)
                forget = (1.0 - np.exp(-tau_wm * (1.0 - m[s])))  # in [0,1)
                w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]

            # normalize WM row
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

            # Update memory strength m[s]: integrate reward and apply load-dependent decay
            # First, rehearsal toward r
            m[s] = (1.0 - rehearse_rate) * m[s] + rehearse_rate * r
            # Then, decay scaled by set size penalty; larger nS -> more decay
            decay = 1.0 - 1.0 / denom
            m[s] = (1.0 - decay) * m[s]
            m[s] = np.clip(m[s], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-driven swap interference across states.

    Mechanism:
    - RL: Rescorla-Wagner updating per state.
    - WM: rewarded actions are bound to states (supervised), but under load,
      bindings can swap/misattribute across states (interference). We approximate
      this by mixing the state's WM distribution with the mean WM distribution
      of other states with probability that increases with set size.
    - Arbitration: fixed wm_weight mixes WM and RL policies.

    Parameters (total 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: Weight of WM policy in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_learn: WM supervised learning rate toward one-hot after reward in [0,1].
    - swap_base: Baseline probability of swap interference in [0,1].
    - load_gain: How much swap increases from nS=3 to nS=6 (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_learn, swap_base, load_gain = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # precompute load-dependent swap probability
        load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
        swap_p = np.clip(swap_base + load_gain * load_term, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # Construct an interference-corrupted WM row for state s
            # by mixing with the average of other states' WM rows.
            if nS > 1:
                other_idx = [i for i in range(nS) if i != s]
                mean_other = np.mean(w[other_idx, :], axis=0)
            else:
                mean_other = w_0[s, :]

            W_s_eff = (1.0 - swap_p) * w[s, :] + swap_p * mean_other

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy using the interference-corrupted W_s_eff
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_eff - W_s_eff[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM supervised learning: only reward strengthens, no-reward leaves as is (rely on swap for degradation)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
                w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p