def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with prediction-error–adaptive learning and entropy-based arbitration.

    Mechanisms
    - RL: delta-rule with a learning rate that increases with unsigned prediction error
      (Pearce–Hall style), enabling rapid updating when surprising outcomes occur.
    - WM: item-specific memory trace that is set to a one-hot vector upon reward and
      otherwise decays toward a uniform prior across time-since-last-visit.
    - Arbitration: trial-wise mixture based on the relative entropy (uncertainty) of
      the RL vs WM policies: more weight goes to the less entropic (more confident)
      system. This naturally reduces WM impact in larger set sizes where WM is less precise.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr0 : Base RL learning rate in [0,1].
        pe_gain : Scales learning-rate boost with |prediction error| (>=0).
        wm_base : Base WM strength in [0,1], controls WM precision magnitude.
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        wm_forget : WM decay base in [0,1]; WM decays as (wm_forget)^dt toward uniform.
        gate_temp : Arbitration sensitivity to entropy difference (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr0, pe_gain, wm_base, softmax_beta, wm_forget, gate_temp = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # base deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            # RL choice probability (softmax in stable form given by template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Working memory effective trace with time-based forgetting toward uniform
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
                lam = wm_forget ** max(0, dt)
                W_s_eff = lam * w[s, :] + (1.0 - lam) * w_0[s, :]
            else:
                W_s_eff = w[s, :]

            # WM precision scales with a base strength; larger set sizes implicitly induce
            # higher entropy in W_s_eff unless reinforced; we modulate the logits directly.
            W_logits = wm_base * (W_s_eff - 1.0 / nA)
            # Convert to a softmax probability of the chosen action using the provided form
            # by mapping to an equivalent "Q-like" vector:
            W_fake = W_logits  # centered logits
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_fake - W_fake[a])))

            # Entropy of distributions to arbitrate: approximate from Q_s and W_fake
            # Build full distributions for entropy computation
            def softmax_prob(vec, beta):
                z = vec - np.max(vec)
                e = np.exp(beta * z)
                p = e / np.sum(e)
                return p

            p_rl_vec = softmax_prob(Q_s, softmax_beta)
            p_wm_vec = softmax_prob(W_fake, softmax_beta_wm)

            # Shannon entropy
            def entropy(p):
                p_clip = np.clip(p, 1e-12, 1.0)
                return -np.sum(p_clip * np.log(p_clip))

            H_rl = entropy(p_rl_vec)
            H_wm = entropy(p_wm_vec)

            # Arbitration weight favors the less entropic (more certain) policy
            # wm_mix = sigmoid(gate_temp * (H_rl - H_wm))
            wm_mix = 1.0 / (1.0 + np.exp(-gate_temp * (H_rl - H_wm)))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with PE-adaptive learning rate
            delta = r - q[s][a]
            lr_eff = np.clip(lr0 + pe_gain * abs(delta), 0.0, 1.0)
            q[s][a] += lr_eff * delta

            # WM update: reward writes a sharp one-hot; no-reward leaves trace as is
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with controlled forgetting + WM with cross-item interference and set-size–scaled precision.

    Mechanisms
    - RL: delta-rule with global Q-value forgetting toward uniform (useful under volatility).
    - WM: rewarded associations are stored as one-hot vectors; when any state is processed,
      other states' WM traces suffer interference-driven decay toward uniform.
    - Arbitration: fixed mixture weight between RL and WM, but WM precision scales down
      exponentially with set size.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        q_forget : RL forgetting rate toward uniform per trial in [0,1].
        wm_weight : Mixture weight for WM in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        wm_precision0 : Base WM precision scale (>=0).
        setsize_sens : Exponential sensitivity of WM precision to set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, q_forget, wm_weight, softmax_beta, wm_precision0, setsize_sens = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM precision declines with set size
        beta_wm_eff = softmax_beta_wm * np.exp(-setsize_sens * max(0, nS - 1))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s][a]
            q[s][a] += lr * delta
            # Global forgetting applied to all actions in the visited state
            q[s, :] = (1.0 - q_forget) * q[s, :] + q_forget * (1.0 / nA)

            # WM update with cross-state interference
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            # Interference: other states decay toward uniform (binding noise)
            if nS > 1:
                others = [i for i in range(nS) if i != s]
                for si in others:
                    w[si, :] = 0.9 * w[si, :] + 0.1 * w_0[si, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM (Win-Stay/Lose-Shift heuristic) with load-modulated WM weight and lapse.

    Mechanisms
    - RL: standard delta-rule with softmax.
    - WM: a heuristic policy that, for each state, prefers the last action after a win
      (win-stay) and avoids it after a loss (lose-shift). This is encoded as a WM value
      vector and converted to a softmax policy.
    - Arbitration: WM weight decreases with set size (capacity limits).
    - Lapse: with small probability, choice is uniform random (to capture inattention).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight0 : Base WM mixture weight in [0,1] for set size 1.
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        load_penalty : How quickly WM weight drops with set size (>=0).
        wsls_bias : Magnitude of WM preference for stay vs avoid (>=0).
        lapse : Lapse probability in [0,1] mixed with the final policy.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, load_penalty, wsls_bias, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # will hold WSLS logits after each visit
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1 * np.ones(nS, dtype=int)

        # WM weight decays with set size
        wm_weight = wm_weight0 / (1.0 + load_penalty * max(0, nS - 1))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Build WSLS WM vector for current state
            if last_action[s] >= 0 and last_reward[s] >= 0:
                logits = np.zeros(nA)
                if last_reward[s] > 0:  # win-stay
                    logits[last_action[s]] += wsls_bias
                else:  # lose-shift: penalize last action; others relatively preferred
                    logits[last_action[s]] -= wsls_bias
                # Convert logits to pseudo-values centered around uniform
                W_s = (1.0 / nA) + logits - np.mean(logits)
                # Normalize to avoid negative entries (softmax handles logits, but template uses values);
                # we will use logits directly by centering them.
                W_fake = logits
            else:
                # If no history, default to uniform (no WM guidance)
                W_s = w_0[s, :]
                W_fake = np.zeros(nA)

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_fake - W_fake[a])))

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: store last action/outcome for WSLS
            last_action[s] = a
            last_reward[s] = r

            # Keep a record in w as a soft preference vector (not strictly required by WM policy above)
            logits_store = np.zeros(nA)
            if r > 0:
                logits_store[a] += wsls_bias
            else:
                logits_store[a] -= wsls_bias
            # Map logits_store to a probability-like vector for potential future use
            z = logits_store - np.max(logits_store)
            e = np.exp(z)
            w[s, :] = e / np.sum(e)

        blocks_log_p += log_p

    return -blocks_log_p