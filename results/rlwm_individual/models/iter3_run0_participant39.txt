def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and WM retrieval that decays with set size and time since last visit.

    Mechanism
    - RL: Q-learning with separate learning rates for positive and negative feedback.
    - WM: on each trial, WM attempts to retrieve the correct response for the current state with a
      retrieval probability that decreases with (a) the time since the state was last seen (lag)
      and (b) set size (higher load reduces retrieval). When retrieval fails, WM policy devolves
      toward uniform choice. WM encoding is reward-gated: only rewarded choices are stored.
    - Mixture: action probabilities are a mixture of RL and WM policies using wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr_pos: RL learning rate for positive prediction errors (0..1).
        - lr_neg: RL learning rate for negative prediction errors (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_weight: mixture weight of WM vs RL (0..1).
        - wm_retrieval_base: baseline probability of WM retrieval at zero lag and set size 3 (0..1).
        - wm_decay_time: rate controlling the exponential decrease of WM retrieval with lag (>0).

    Set-size effects
    ----------------
    - WM retrieval is scaled by (3 / nS), so retrieval is harder at set size 6 than 3.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_retrieval_base, wm_decay_time = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last visit to compute lag-dependent retrieval
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM retrieval probability depends on lag and set size
            lag = 0 if last_seen[s] < 0 else (t - last_seen[s])
            scale_load = 3.0 / float(nS)
            p_retrieve = wm_retrieval_base * scale_load * np.exp(-wm_decay_time * float(lag))
            p_retrieve = np.clip(p_retrieve, 0.0, 1.0)

            # WM policy: with probability p_retrieve use sharp WM softmax; else approximate uniform
            p_wm_hit = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_miss = 1.0 / nA
            p_wm = p_retrieve * p_wm_hit + (1 - p_retrieve) * p_wm_miss

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s][a] += lr_pos * pe
            else:
                q[s][a] += lr_neg * pe

            # WM update: reward-gated encoding (store only if rewarded)
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Overwrite strongly toward the rewarded choice
                w[s] = onehot

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with constant learning and dynamic WM gating by confidence plus load-dependent competition.

    Mechanism
    - RL: standard Q-learning.
    - WM: stores item-action bindings with Hebbian updates on every trial, regardless of reward,
      but with stronger updates for rewarded trials. A global competition/normalization pressure
      pushes WM toward uniform across all states, mimicking limited capacity; this pressure scales
      with set size. Visiting a state refreshes its representation.
    - Dynamic mixture: the effective WM weight on each trial is a logistic function of the current
      state's WM confidence (max activation in that row), modulating around a baseline wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: baseline mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - gate_slope: sensitivity of the WM gate to confidence (higher = more gating).
        - comp_strength: strength (0..1) of global WM competition toward uniform per trial; scales with nS.
        - refresh_rate: rate (0..1) to sharpen the active state's WM row toward the chosen action.

    Set-size effects
    ----------------
    - WM competition strength is scaled by (nS / 6): higher load increases competition (more drift to uniform).
    - WM gating also implicitly depends on set size via lower confidence at higher load.
    """
    lr, wm_weight, softmax_beta, gate_slope, comp_strength, refresh_rate = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective competition per trial increases with load
        comp_eff = np.clip(comp_strength * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic gate based on WM confidence at state s
            max_conf = np.max(W_s)  # in [1/nA, 1]
            base_logit = np.log(np.clip(wm_weight, 1e-6, 1 - 1e-6)) - np.log(1 - np.clip(wm_weight, 1e-6, 1 - 1e-6))
            conf_centered = max_conf - (1.0 / nA)
            gate_logit = base_logit + gate_slope * conf_centered
            wm_gate = 1.0 / (1.0 + np.exp(-gate_logit))
            wm_gate = np.clip(wm_gate, 0.0, 1.0)

            # Mixture policy
            p_total = wm_gate * p_wm + (1 - wm_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s][a] += lr * pe

            # WM Hebbian/refresh update
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            # Refresh toward chosen action, with extra gain if rewarded
            gain = refresh_rate * (1.0 + r)  # doubles update when r=1
            w[s] = (1 - gain) * w[s] + gain * onehot

            # Global competition toward uniform across all states (capacity pressure)
            if comp_eff > 0:
                w = (1 - comp_eff) * w + comp_eff * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with experience-dependent learning-rate decay and WM with inhibitory (no-go) encoding and noise.

    Mechanism
    - RL: learning rate decays with the number of times a state-action pair has been chosen,
      modeling diminishing plasticity; decay is steeper under higher set size.
    - WM: reward leads to excitatory encoding toward the chosen action; non-reward leads to
      inhibitory encoding away from the chosen action (redistributing weight to alternatives).
      WM is also corrupted by zero-mean noise each trial, stronger under higher set size.
    - Mixture: fixed wm_weight combines WM and RL policies.

    Parameters
    ----------
    model_parameters : tuple
        - lr: base RL learning rate before decay (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - lr_decay_c: learning-rate decay coefficient (>0); effective decay scales with set size.
        - wm_noise: standard deviation of WM noise per trial (>=0); noise scales with set size.
        - wm_anti: strength (0..1) of inhibitory WM update on non-rewarded trials.

    Set-size effects
    ----------------
    - RL learning rate per (s,a): alpha_eff = lr / (1 + c_eff * N_sa), with c_eff = lr_decay_c * (nS/6).
    - WM noise per trial scales as wm_noise_eff = wm_noise * (nS/6).
    """
    lr, wm_weight, softmax_beta, lr_decay_c, wm_noise, wm_anti = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    rng = np.random.RandomState(0)  # deterministic noise for likelihood evaluation

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        N_sa = np.zeros((nS, nA))  # visit counts per (state, action)

        c_eff = lr_decay_c * (float(nS) / 6.0)
        wm_noise_eff = wm_noise * (float(nS) / 6.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with experience-dependent learning rate
            N_sa[s, a] += 1.0
            alpha_eff = lr / (1.0 + c_eff * N_sa[s, a])
            pe = r - Q_s[a]
            q[s][a] += alpha_eff * pe

            # WM update: excitatory if rewarded, inhibitory if not
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            if r > 0:
                # Move toward chosen action
                w[s] = (1 - alpha_eff) * w[s] + alpha_eff * onehot
            else:
                # Move away from chosen action: reduce its weight and renormalize toward others
                anti = wm_anti
                w[s][a] = (1 - anti) * w[s][a]
                # redistribute lost mass to other actions proportionally
                lost = 1.0 - np.sum(w[s])
                if lost != 0:
                    others = [i for i in range(nA) if i != a]
                    w[s][others] += lost / (nA - 1)

            # Add WM noise (Gaussian) and renormalize to simplex
            if wm_noise_eff > 0:
                noise = rng.normal(0.0, wm_noise_eff, size=nA)
                w[s] = w[s] + noise
                # Project to probabilities: clip and renormalize
                w[s] = np.clip(w[s], 1e-6, None)
                w[s] = w[s] / np.sum(w[s])

        blocks_log_p += log_p

    return -blocks_log_p