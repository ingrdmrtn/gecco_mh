def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with reward-gated success-trace bias and age- and load-dependent weighting plus lapse.

    Mechanism:
    - Baseline model-free Q-learning within each block.
    - In parallel, a "success-trace" bias H is maintained per state-action, which accumulates only after
      rewarded choices and decays otherwise. This captures a fast reinforcement of recently successful
      actions in a state, distinct from generic perseveration because it is reward-gated.
    - Policy uses softmax over Q + w_age_load * H, where w_age_load depends on age group and set size.
      This captures greater use of success traces at low load and group differences.
    - A lapse component mixes in a uniform random policy.

    Parameters
    ----------
    states : 1D array-like of int
        State indices on each trial (within a block; 0..set_size-1).
    actions : 1D array-like of int
        Chosen action indices on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Binary feedback for each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size per trial (constant within block; either 3 or 6).
    age : 1D array-like of float
        Participant age; age[0] is used to determine group (>=45 older, <45 younger).
    model_parameters : tuple/list
        (alpha, beta, lambda_success, load_sensitivity, lapse)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax over combined value (Q + w*H).
        - lambda_success: decay factor for success-trace H per encounter (0..1). Larger = more persistent bias.
        - load_sensitivity: controls how success-trace weight decreases with set size and increases for older group.
        - lapse: lapse probability for random responding (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, lambda_success, load_sensitivity, lapse = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize Q and success-trace H
        Q = (1.0 / nA) * np.ones((nS, nA))
        H = np.zeros((nS, nA))  # no initial bias

        # Age- and load-dependent weight on success trace.
        # Base scaling decreases with load (more load -> less reliance on success trace),
        # with an older-group boost (older adults may rely more on simple success tagging).
        # Scale in [0,1] by a squashing function.
        load_factor = 3.0 / float(max(1, nS))  # 1.0 at nS=3, 0.5 at nS=6
        base_w = load_sensitivity * load_factor
        age_boost = 0.5 * load_sensitivity if is_older else 0.0
        w_success = base_w + age_boost
        # squash to [0,1]
        w_success = 1.0 / (1.0 + np.exp(-(w_success - 0.5)))  # centered sigmoid

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax over combined value V = Q + w_success * H
            V_s = Q[s, :] + w_success * H[s, :]
            V_centered = V_s - np.max(V_s)
            expV = np.exp(beta * V_centered)
            p_vec = expV / np.sum(expV)
            p_choice = (1.0 - lapse) * p_vec[a] + lapse * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            total_log_p += np.log(p_choice)

            # Q-learning update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Success-trace update: decay then add reward-gated bump to chosen action
            H[s, :] *= lambda_success
            if r > 0.5:
                # Assign a unit pulse to the chosen action; no negative pulse on others
                H[s, a] += 1.0

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with directed exploration (uncertainty bonus, UCB-style) modulated by age and load, plus lapse.

    Mechanism:
    - Standard Q-learning per state-action.
    - Maintain visit counts N[s,a]. Add an exploration bonus B[s,a] = bonus_age * (3/nS) / sqrt(N[s,a] + 1).
      This yields stronger directed exploration for rarely tried actions, scaled down at higher set sizes,
      and with age-dependent magnitude.
    - Policy uses softmax over Q + B. Occasional lapses are modeled as uniform random choices.

    Parameters
    ----------
    states : 1D array-like of int
        State indices on each trial (within a block; 0..set_size-1).
    actions : 1D array-like of int
        Chosen action indices on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Binary feedback for each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size per trial (constant within a block).
    age : 1D array-like of float
        Participant age; age[0] is used. Age >= 45 -> older group; otherwise younger.
    model_parameters : tuple/list
        (alpha, beta, bonus_young, bonus_old, lapse)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax.
        - bonus_young: uncertainty bonus magnitude for younger group.
        - bonus_old: uncertainty bonus magnitude for older group.
        - lapse: lapse probability for random choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, bonus_young, bonus_old, lapse = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0
    bonus_age = bonus_old if is_older else bonus_young

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action

        # Load factor: directed exploration attenuates at larger set sizes
        load_scale = 3.0 / float(max(1, nS))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute UCB-style bonus for this state
            bonus = bonus_age * load_scale / np.sqrt(N[s, :] + 1.0)
            V_s = Q[s, :] + bonus

            V_centered = V_s - np.max(V_s)
            expV = np.exp(beta * V_centered)
            p_vec = expV / np.sum(expV)
            p_choice = (1.0 - lapse) * p_vec[a] + lapse * (1.0 / nA)
            p_choice = max(p_choice, 1e-12)
            total_log_p += np.log(p_choice)

            # Update counts and Q
            N[s, a] += 1.0
            td = r - Q[s, a]
            Q[s, a] += alpha * td

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with meta-controlled inverse temperature based on recent reward rate, with age-specific gain
    and load penalty.

    Mechanism:
    - Q-learning computes action values as usual.
    - Inverse temperature is dynamic: beta_t = beta0 + gain_age * (EMA_reward - 0.5) - load_penalty * ((nS-3)/3).
      Higher recent reward rate increases choice determinism; higher load reduces it. Gains differ by age group.
    - No lapse parameter here; stochasticity is captured by meta-adapted beta.

    Parameters
    ----------
    states : 1D array-like of int
        State indices on each trial (within a block; 0..set_size-1).
    actions : 1D array-like of int
        Chosen action indices on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Binary feedback for each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size per trial (constant within a block).
    age : 1D array-like of float
        Participant age; age[0] is used. Age >= 45 -> older group; otherwise younger.
    model_parameters : tuple/list
        (alpha, beta0, gain_young, gain_old, load_penalty)
        - alpha: learning rate for Q-learning (0..1).
        - beta0: baseline inverse temperature.
        - gain_young: meta-gain mapping reward EMA to beta for younger group.
        - gain_old: meta-gain for older group.
        - load_penalty: reduction in beta proportional to set size above 3 ((nS-3)/3 scaling).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, gain_young, gain_old, load_penalty = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0
    gain_age = gain_old if is_older else gain_young

    nA = 3
    total_log_p = 0.0

    # Exponential moving average smoothing for reward history within each block
    # We tie the EMA smoothing time constant to set size: larger sets -> slower EMA (more inertia).
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Reward EMA initialization at chance level 0.5; smoothing alpha depends on load
        # so that at higher load the estimate changes more slowly.
        ema = 0.5
        # Map set size to EMA smoothing: higher nS -> smaller ema_alpha
        ema_alpha = 0.6 * (3.0 / float(max(1, nS)))  # 0.6 at nS=3, 0.3 at nS=6

        # Precompute load penalty term
        load_term = load_penalty * max(0.0, (nS - 3) / 3.0)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Dynamic beta_t
            beta_t = beta0 + gain_age * (ema - 0.5) - load_term
            # Enforce non-negativity to avoid pathological inversions
            beta_t = max(0.0, beta_t)

            q_s = Q[s, :]
            q_centered = q_s - np.max(q_s)
            expq = np.exp(beta_t * q_centered)
            p_vec = expq / np.sum(expq)
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # Q update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Update EMA of reward
            ema = (1.0 - ema_alpha) * ema + ema_alpha * r

    return -float(total_log_p)