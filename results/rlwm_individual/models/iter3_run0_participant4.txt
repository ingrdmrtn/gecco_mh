def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and load-dependent decay.

    Mechanism
    - RL: Rescorla-Wagner update with single learning rate; softmax action selection.
    - WM: fast, reward-gated mapping that decays toward uniform. Decay speeds up with larger set size.
    - Arbitration: trial-wise WM weight is a logistic function of RL uncertainty (policy entropy)
      and set size (lower weight at higher load).

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr                : RL learning rate in (0,1].
        softmax_beta      : RL inverse temperature (scaled by *10 internally).
        wm_decay_base     : Base WM decay rate (>0); higher => faster decay toward uniform.
        arb_bias          : Arbitration bias (logit space), baseline WM reliance.
        arb_unc_slope     : Arbitration slope on RL uncertainty (positive => more WM when RL uncertain).
        wm_temp           : Scales WM sharpness (multiplies WM action values before WM softmax).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_decay_base, arb_bias, arb_unc_slope, wm_temp = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM retention: larger set size => more decay (smaller gamma)
        # gamma in [0,1]: retention multiplier per trial
        gamma = np.exp(-np.clip(wm_decay_base, 0.0, None) * (nS / 3.0))
        gamma = np.clip(gamma, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy and its uncertainty (entropy)
            # Compute full softmax probabilities for entropy
            z = softmax_beta * (Q_s - np.max(Q_s))
            pA = np.exp(z) / np.sum(np.exp(z))
            H = -np.sum(pA * np.log(np.clip(pA, eps, 1.0)))  # entropy
            H_norm = H / np.log(nA)  # in [0,1]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic softmax over WM values scaled by wm_temp
            W_eff = wm_temp * W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Arbitration: logistic of (bias + uncertainty term + load term)
            # Load term favors WM when load is small (3/nS near 1 for small set sizes)
            load_term = (3.0 / nS) - 0.5
            wm_logit = arb_bias + arb_unc_slope * H_norm + load_term
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]

            # WM update: reward-gated strengthening toward chosen action
            if r > 0:
                # Move distribution toward the chosen action's one-hot
                boost = 1.0 - w[s, a]
                w[s, a] += boost  # make chosen action dominant
                # Normalize to keep it a proper distribution (robustness)
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :]) + eps
            else:
                # Mild penalization of the chosen action in WM on negative outcome
                w[s, a] *= 0.5
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :]) + eps

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM slot-like encoding with load-dependent gating and interference.

    Mechanism
    - RL: Rescorla-Wagner with separate learning rates for positive and negative RPEs.
    - WM: On rewarded trials, the S-A pair is encoded into WM with probability determined by load.
      Encoding a new item causes interference that degrades other WM entries toward uniform.
    - Policy: mixture of WM softmax and RL softmax. The WM contribution is endogenous: stronger
      WM traces (more peaked) yield higher mixture weight for that state.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr_pos            : RL learning rate for positive prediction errors in (0,1].
        lr_neg            : RL learning rate for negative prediction errors in (0,1].
        softmax_beta      : RL inverse temperature (scaled by *10 internally).
        wm_encode_base    : Baseline WM encoding drive (logit space).
        wm_load_slope     : Load modulation of encoding (logit space per (3/nS)); positive => more encoding at small set size.
        wm_interference   : Interference strength in (0, +inf); higher => stronger global decay of other WM entries on encode.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_encode_base, wm_load_slope, wm_interference = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL likelihood for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM likelihood for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Endogenous WM mixture weight: stronger, peaked WM implies higher weight
            # Use "peakiness" = max(W_s) rescaled to [0,1]
            peak = (np.max(W_s) - (1.0 / nA)) / (1.0 - (1.0 / nA))
            wm_weight = np.clip(peak, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0 else lr_neg
            q[s][a] += eta * pe

            # WM encoding gate (reward-contingent)
            if r > 0:
                # Encoding probability as logistic of base + load effect
                load_factor = (3.0 / nS)
                logit_p = wm_encode_base + wm_load_slope * load_factor
                p_encode = 1.0 / (1.0 + np.exp(-logit_p))

                # Bernoulli sample is not possible in likelihood eval; instead, use expectation update:
                # Move W toward encoding target proportionally to p_encode.
                # First, interference on other states when encoding occurs in expectation:
                if np.any(w != w_0):
                    # Global interference scales with p_encode and WM size (stronger at larger sets)
                    inter = 1.0 - np.exp(-wm_interference * (nS / 3.0))
                    inter = np.clip(inter, 0.0, 1.0)
                    w = (1.0 - p_encode * inter) * w + (p_encode * inter) * w_0

                # Now move current state's row toward one-hot on action a
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - p_encode) * w[s, :] + p_encode * one_hot

                # Normalize for numerical stability
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :]) + eps
            else:
                # On negative outcome, slightly relax WM toward uniform for this state
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice persistence + episodic WM cache with time-based retrieval and load scaling.

    Mechanism
    - RL: Rescorla-Wagner with a single learning rate.
    - Choice persistence: adds a transient bias toward repeating the last action in a state
      when evaluating the RL policy (implemented as a bonus to that action's value).
    - WM: episodic cache of last rewarded action per state as a probability row; its retrieval probability
      decays with time since last reward and decreases with set size.
    - Policy: mixture of WM and RL where the WM weight equals the retrieval probability on that trial.

    Parameters
    ----------
    model_parameters : list/tuple of 5 floats
        lr                 : RL learning rate in (0,1].
        softmax_beta       : RL inverse temperature (scaled by *10 internally).
        wm_retrieval_base  : Base retrieval probability in [0,1].
        time_decay         : Time-based decay rate (>0) for WM retrieval with state-specific lag.
        choice_pers        : Choice persistence bonus added to the previously chosen action in that state.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_retrieval_base, time_decay, choice_pers = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # WM row will store the episodic distribution per state
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track per-state time since last rewarded encoding and previous action for persistence
        lag = np.full(nS, 1e6)  # large initial lag
        prev_action = -np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            # Increase lags each trial
            lag += 1

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL with choice persistence bonus on last action in this state
            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += choice_pers

            # RL chosen-action probability under biased Q
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from episodic cache
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Retrieval probability: higher when lag is short and load is small
            load_scale = (3.0 / nS)
            p_ret = wm_retrieval_base * np.exp(-np.clip(time_decay, 0.0, None) * lag[s]) * load_scale
            wm_weight = np.clip(p_ret, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update (without persistence in learning)
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: on reward, cache the chosen action strongly; otherwise decay toward uniform
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                lag[s] = 0  # reset lag for this state
            else:
                # Gentle decay toward uniform over time
                decay = 1.0 - np.exp(-time_decay * (nS / 3.0))
                decay = np.clip(decay, 0.0, 1.0)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Update previous action for persistence
            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p