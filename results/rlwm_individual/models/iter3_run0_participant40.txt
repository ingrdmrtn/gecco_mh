def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-modulated learning/exploration, novelty bonus, and load-dependent forgetting.

    The agent learns Q-values with a per-trial forgetting drift toward a uniform prior that increases
    with set size and in older adults. Action values are augmented by a novelty/exploration bonus
    that decreases with visit counts and is scaled by age sensitivity. Older adults show reduced
    learning rate and increased lapse compared to younger adults via a shared age-sensitivity parameter.

    Parameters
    ----------
    states : np.ndarray of int
        State index on each trial (0..nS-1 within block).
    actions : np.ndarray of int
        Chosen action on each trial (0..2). If action < 0, treated as lapse with uniform likelihood.
    rewards : np.ndarray of float
        Feedback per trial (0/1). Values < 0 indicate invalid/missing feedback (ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (typically 3 or 6); used for forgetting and exploration modulation.
    age : np.ndarray of float
        Participant age array (use age[0]); age >= 45 is older group.
    model_parameters : sequence of float
        [alpha_base, beta_base, kappa_age, lambda_forget, eps_base]
        - alpha_base: baseline RL learning rate (0..1).
        - beta_base: baseline inverse temperature for softmax.
        - kappa_age: age sensitivity controlling reduction in learning and scaling of exploration bonus;
                      applied only in older adults (>=45).
        - lambda_forget: base forgetting drift per trial toward uniform, amplified by set size and age.
        - eps_base: baseline lapse probability; increased in older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_base, beta_base, kappa_age, lambda_forget, eps_base = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize Q-values and visit counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Age- and load-adjusted parameters
            alpha = alpha_base / (1.0 + kappa_age * is_older)  # reduced learning in older adults
            beta = beta_base  # keep base inverse temperature; exploration via bonus
            epsilon = eps_base * (1.0 + 0.5 * kappa_age * is_older)  # more lapses in older adults

            # Load- and age-dependent forgetting drift toward uniform
            drift = lambda_forget * (ss / 6.0) * (1.0 + 0.5 * is_older)
            drift = np.clip(drift, 0.0, 1.0)
            # Apply forgetting to the current state's Q before action selection
            Q[s, :] = (1.0 - drift) * Q[s, :] + drift * (1.0 / nA)

            # Novelty/exploration bonus: larger for rarely tried actions; age scales its strength
            # bonus scales inversely with sqrt(1+count); set-size scales it downward mildly.
            bonus_scale_age = kappa_age * (0.5 + 0.5 * is_older)  # older rely slightly more on exploration bonus
            counts = N[s, :]
            novelty = 1.0 / np.sqrt(1.0 + counts)
            bonus = bonus_scale_age * novelty * (3.0 / max(3.0, float(ss)))  # reduce with load

            prefs = beta * (Q[s, :] + bonus)
            prefs = prefs - np.max(prefs)
            p = np.exp(prefs)
            p = p / np.sum(p)
            # Lapse mixture
            p = (1.0 - epsilon) * p + epsilon * (1.0 / nA)

            # Likelihood
            if a >= 0 and a < nA:
                total_logp += np.log(float(np.clip(p[a], 1e-12, 1.0)))
            else:
                # Invalid action -> uniform likelihood, no update
                total_logp += np.log(1.0 / nA)
                continue

            # Updates only if feedback is valid
            if r >= 0:
                # Increment visit count for exploration bonus computation
                N[s, a] += 1.0
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-biased learning and age/load-modulated choice stickiness.

    The agent updates Q-values with a single base learning rate modulated by a valence bias,
    producing different effective learning rates for rewards and non-rewards. The choice policy
    incorporates a perseverative bias toward repeating the last action taken in the same state.
    Stickiness increases with set size and is stronger in older adults.

    Parameters
    ----------
    states : np.ndarray of int
        State index on each trial (0..nS-1 within block).
    actions : np.ndarray of int
        Chosen action on each trial (0..2). If action < 0, treated as lapse with uniform likelihood.
    rewards : np.ndarray of float
        Feedback per trial (0/1); values < 0 denote invalid and are ignored for updating.
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), modulates stickiness.
    age : np.ndarray of float
        Participant age array (use age[0]); age >= 45 treated as older group.
    model_parameters : sequence of float
        [alpha, beta, valence_bias, stick_age, epsilon]
        - alpha: base learning rate.
        - beta: inverse temperature for softmax over Q + stickiness.
        - valence_bias: in (-1,1) approximately; sets alpha_pos = alpha*(1+valence_bias),
                        alpha_neg = alpha*(1-valence_bias).
        - stick_age: baseline stickiness strength; amplified by set size and in older adults.
        - epsilon: lapse probability (uniform action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, valence_bias, stick_age, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for stickiness; -1 indicates none yet
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Effective stickiness increases with load and in older adults
            stick_strength = stick_age * (ss / 3.0) * (1.0 + 0.5 * is_older)

            prefs = beta * Q[s, :]

            # Add stickiness toward the last action in this state
            if last_action[s] >= 0 and last_action[s] < nA:
                stick_vec = np.zeros(nA)
                stick_vec[last_action[s]] = 1.0
                prefs = prefs + stick_strength * stick_vec

            # Softmax with lapse
            prefs = prefs - np.max(prefs)
            p = np.exp(prefs)
            p = p / np.sum(p)
            p = (1.0 - epsilon) * p + epsilon * (1.0 / nA)

            # Likelihood
            if a >= 0 and a < nA:
                total_logp += np.log(float(np.clip(p[a], 1e-12, 1.0)))
            else:
                total_logp += np.log(1.0 / nA)
                # Do not update last_action or Q on invalid trials
                continue

            # Update last action after observing a valid choice
            last_action[s] = a

            # Q-learning update with valence bias if feedback is valid
            if r >= 0:
                alpha_pos = alpha * (1.0 + valence_bias)
                alpha_neg = alpha * (1.0 - valence_bias)
                eff_alpha = alpha_pos if r > 0.0 else alpha_neg
                delta = r - Q[s, a]
                Q[s, a] += eff_alpha * delta

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM gated mixture with age-dependent capacity and priority-based access.

    The agent combines incremental RL with a limited-capacity working memory (WM) that stores
    the most recently rewarded action per state as a one-hot policy. WM access probability
    depends on (i) effective capacity relative to set size and (ii) a priority distribution
    over states derived from recent rewards. Older adults have reduced effective capacity.

    Parameters
    ----------
    states : np.ndarray of int
        State index on each trial (0..nS-1 within block).
    actions : np.ndarray of int
        Chosen action on each trial (0..2). If action < 0, treated as lapse with uniform likelihood.
    rewards : np.ndarray of float
        Feedback per trial (0/1); values < 0 ignored for updating.
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6); modulates capacity fraction and WM leak.
    age : np.ndarray of float
        Participant age array (use age[0]); age >= 45 treated as older.
    model_parameters : sequence of float
        [alpha, beta, K_base, gate_temp, epsilon]
        - alpha: RL learning rate.
        - beta: inverse temperature for RL softmax.
        - K_base: baseline WM capacity in number of states effectively retrievable (can be fractional).
        - gate_temp: softmax temperature over state priorities (higher -> more selective WM access).
        - epsilon: lapse probability (uniform action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, K_base, gate_temp, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store as probability vectors per state (one-hot when confident)
        WM = (1.0 / nA) * np.ones((nS, nA))

        # Priority trace per state based on recent reward history
        priority = np.zeros(nS)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Effective capacity reduced in older adults
            K_eff = max(0.0, K_base - 0.8 * is_older)
            cap_frac = np.clip(K_eff / max(1.0, float(ss)), 0.0, 1.0)

            # Priority distribution over states via softmax
            pr = priority - np.max(priority)
            pr_exp = np.exp(gate_temp * pr)
            if np.sum(pr_exp) <= 0:
                pr_exp = np.ones_like(pr_exp)
            pr_dist = pr_exp / np.sum(pr_exp)

            # Probability WM governs current state = capacity fraction times priority mass at s
            w = cap_frac * pr_dist[s]
            w = float(np.clip(w, 0.0, 1.0))

            # Policies
            # RL softmax
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: use stored vector for state s
            p_wm = WM[s, :]
            p_wm = p_wm / np.sum(p_wm)

            # Mixture and lapse
            p_mix = w * p_wm + (1.0 - w) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            # Likelihood
            if a >= 0 and a < nA:
                total_logp += np.log(float(np.clip(p[a], 1e-12, 1.0)))
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Updates if feedback valid
            if r >= 0:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # Update priority trace (recent reward rate per state)
                # Use same alpha as a recency-weighted estimator
                priority[s] = (1.0 - alpha) * priority[s] + alpha * r

                # WM decay/leak toward uniform increases with load and with age
                leak = 0.15 * (ss / 6.0) + 0.15 * is_older
                leak = np.clip(leak, 0.0, 0.6)
                WM[s, :] = (1.0 - leak) * WM[s, :] + leak * (1.0 / nA)

                # On rewarded trials, write perfect memory for that action (one-hot)
                if r > 0.0:
                    wm_vec = np.zeros(nA)
                    wm_vec[a] = 1.0
                    WM[s, :] = wm_vec

    return -float(total_logp)