def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise-gated encoding and decay.
    WM stores state-action associations when RL experiences large prediction errors
    (surprise), with continuous strength based on how much the error exceeds a
    threshold. WM also decays toward uniform. Arbitration favors WM in small set sizes.

    Parameters
    ----------
    model_parameters : list or array of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Baseline WM mixture weight (0-1); scaled down as set size increases.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10 for appropriate range.
        surprise_gate : float
            Threshold on absolute TD-error at which WM encoding is engaged (0-1).
        wm_decay : float
            Per-visit WM decay toward uniform (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, surprise_gate, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM policy

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL policy (softmax expressed via difference trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM weights)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM weight scaled by set size (less WM in larger sets)
            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(wm_weight_base * size_scale, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform each visit
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Surprise-gated WM encoding: if |delta| exceeds threshold, strengthen memory
            abs_delta = np.abs(delta)
            if abs_delta > surprise_gate:
                # Normalize encoding strength between 0 and 1 as it exceeds threshold
                strength = (abs_delta - surprise_gate) / (1.0 - surprise_gate + 1e-8)
                strength = np.clip(strength, 0.0, 1.0)
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - strength) * w[s, :] + strength * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with per-visit forgetting + WM with set-size-driven interference and lapse.
    RL values decay toward uniform after each update, capturing forgetting under load.
    WM strengthens on rewarded trials but suffers interference proportional to set size.
    A small lapse mixture captures occasional random choices.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Baseline WM mixture weight (0-1); scaled down as set size increases.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        rl_forget : float
            Per-visit RL forgetting toward uniform (0-1).
        wm_interference : float
            Interference strength toward uniform WM increasing with set size (0-1).
        lapse : float
            Lapse probability mixing in uniform random choice (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, rl_forget, wm_interference, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM interference rate
        # 0 interference at nS=3, maximal at nS=6 scaled by wm_interference
        wm_int_rate = wm_interference * max(0.0, (nS - 3.0) / 3.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(wm_weight_base * size_scale, 0.0, 1.0)

            p_mix = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            # Lapse
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update then forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # WM interference toward uniform
            w[s, :] = (1.0 - wm_int_rate) * w[s, :] + wm_int_rate * w_0[s, :]

            # Reward-triggered WM strengthening (one-shot encoding)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Moderate consolidation to avoid saturating instantly
                w[s, :] = 0.5 * w[s, :] + 0.5 * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-adaptive temperature and confidence-based arbitration with WM.
    RL maintains a per-state uncertainty estimate from TD-error variance, which reduces
    effective inverse temperature (more exploration when uncertain). Arbitration weight
    favors the system (WM vs RL) with higher confidence derived from choice entropy.
    WM strengthens on reward and decays slightly each visit.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        softmax_beta : float
            Base RL inverse temperature; internally scaled by 10; reduced by uncertainty.
        wm_weight_base : float
            Baseline WM weight (0-1); scaled by set size and modulated by confidence.
        uncert_lr : float
            Step size for updating per-state uncertainty from squared TD error (0-1).
        wm_boost_rewarded : float
            WM consolidation strength toward chosen action on rewarded trials (0-1).
        entropy_temp : float
            Sensitivity of arbitration to confidence difference (higher => sharper).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, uncert_lr, wm_boost_rewarded, entropy_temp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    nA = 3
    eps = 1e-8

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask]
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Per-state uncertainty (initialized moderate)
        sigma = np.ones(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL policy with uncertainty-adaptive temperature
            beta_eff = softmax_beta / (1.0 + sigma[s])
            Q_s = q[s, :]
            logits_rl = beta_eff * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_rl)
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_wm)
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Confidence from entropy (normalized 0-1 by log(nA))
            H_rl = -np.sum(pvec_rl * np.log(np.clip(pvec_rl, eps, 1.0)))
            H_wm = -np.sum(pvec_wm * np.log(np.clip(pvec_wm, eps, 1.0)))
            rl_conf = np.clip(1.0 - H_rl / np.log(nA), 0.0, 1.0)
            wm_conf = np.clip(1.0 - H_wm / np.log(nA), 0.0, 1.0)

            # Base WM weight scaled by set size
            base_w = np.clip(wm_weight_base * (3.0 / float(nS)), 0.0, 1.0)
            # Confidence-modulated arbitration via logistic on difference (wm_conf - rl_conf)
            base_logit = np.log(np.clip(base_w, eps, 1.0 - eps)) - np.log(np.clip(1.0 - base_w, eps, 1.0))
            eff_logit = base_logit + entropy_temp * (wm_conf - rl_conf)
            eff_wm_weight = 1.0 / (1.0 + np.exp(-eff_logit))
            eff_wm_weight = np.clip(eff_wm_weight, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL learning and uncertainty update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            sigma[s] = (1.0 - uncert_lr) * sigma[s] + uncert_lr * (delta ** 2)

            # WM decay tied (weakly) to uncert_lr and reward-based strengthening
            wm_decay = 0.25 * uncert_lr
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                cons = np.clip(wm_boost_rewarded, 0.0, 1.0)
                w[s, :] = (1.0 - cons) * w[s, :] + cons * target

        blocks_log_p += log_p

    return -blocks_log_p