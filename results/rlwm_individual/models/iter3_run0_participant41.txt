def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL+WM with capacity-by-set-size arbitration, recency-gated WM availability, and choice stickiness.

    Idea:
    - RL: standard delta-rule update.
    - WM: stores action preferences per state; strengthened by reward and decays otherwise.
    - Arbitration: WM weight scales with (capacity / set size) and with recency of the state's last visit.
    - Stickiness: adds a bias toward repeating the previous action within a block, influencing both RL and WM policies.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM reliance (0..1), further modulated by capacity and recency.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_eta: WM learning rate (0..1) for updating WM on each trial.
    - recency_decay: controls how quickly WM availability decays with time since last visit (>0).
    - stickiness: choice perseveration weight added to the last chosen action (>=0).
    
    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_eta, recency_decay, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM policy is near-deterministic
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise last-visit tracker for recency
        last_visit_t = -1 * np.ones(nS, dtype=int)
        # Track previous action for stickiness
        prev_action = None

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Compute recency since last visit of this state
            if last_visit_t[s] < 0:
                isi = np.inf  # unseen yet: recency factor will be ~0
            else:
                isi = t - last_visit_t[s]

            recency_factor = 0.0 if np.isinf(isi) else np.exp(-recency_decay * float(isi))
            # Effective WM weight: baseline scaled by capacity (3/nS) and recency
            wm_weight_eff = np.clip(wm_weight * (3.0 / max(3.0, float(nS))) * recency_factor, 0.0, 1.0)

            # Stickiness bias vector (applied to both RL and WM)
            bias = np.zeros(nA)
            if prev_action is not None:
                bias[int(prev_action)] += stickiness

            # RL policy probability of chosen action
            Q_eff = q[s, :] + bias
            Qa = Q_eff[a]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Qa)))

            # WM policy probability of chosen action
            W_eff = w[s, :] + bias
            Wa = W_eff[a]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - Wa)))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = float(np.clip(p_total, eps, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: reward strengthens chosen action; no-reward decays toward uniform
            if r > 0.5:
                # Move W toward a one-hot on the chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # Decay toward uniform when not rewarded
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

            # Maintain trackers
            last_visit_t[s] = t
            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL+WM with asymmetric RL learning, uncertainty-weighted WM reliance, and PE-gated WM updating.

    Idea:
    - RL: asymmetric learning rates for positive vs. negative prediction errors.
    - WM: stores local action preferences; updated proportionally to a gating signal based on PE magnitude
      (surprise) and reward. When outcomes are surprising, WM is updated more.
    - Arbitration: WM weight scales with (3/nS) and increases when RL is uncertain (high entropy over actions).

    Parameters (tuple):
    - lr: RL learning rate for positive PEs (0..1).
    - wm_weight: baseline WM reliance (0..1), further modulated by RL uncertainty and set size.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - lr_neg: RL learning rate for negative PEs (0..1).
    - gate_sensitivity: scales how strongly |PE| drives WM update magnitude (>=0).
    - entropy_sensitivity: scales how strongly RL action entropy increases WM reliance (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, lr_neg, gate_sensitivity, entropy_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax action distribution over all actions for entropy
            Q_s = q[s, :]
            denom = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            pi = np.exp(softmax_beta * (Q_s - np.max(Q_s))) / max(denom, eps)
            # Shannon entropy
            entropy = -np.sum(pi * np.log(np.clip(pi, eps, 1.0)))

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: baseline scaled by set size and RL uncertainty
            unc_factor = 1.0 / (1.0 + np.exp(-entropy_sensitivity * (entropy - 0.5)))  # ~0..1 increasing with entropy
            wm_weight_eff = np.clip(wm_weight * (3.0 / max(3.0, float(nS))) * unc_factor, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = float(np.clip(p_total, eps, 1.0))
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            alpha = lr if pe >= 0.0 else lr_neg
            q[s, a] += alpha * pe

            # WM update gated by PE magnitude and reward
            gate_prob = 1.0 / (1.0 + np.exp(-gate_sensitivity * (abs(pe))))
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - gate_prob) * w[s, :] + gate_prob * target
            else:
                # If not rewarded, gentle decay toward uniform scaled by (1 - gate_prob)
                decay = (1.0 - gate_prob)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL+WM with RL decay, WM noise increasing with set size, and action perseveration.

    Idea:
    - RL: delta-rule plus global decay toward uniform after each trial (stronger decay parameter reduces retention).
    - WM: stores rewarded actions; independent per state but corrupted by noise toward uniform that increases
      with set size (load-dependent WM degradation).
    - Arbitration: WM reliance scales down with set size; both RL and WM policies include perseveration bias.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM reliance (0..1), scaled by (3/nS) each block.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - rl_decay: per-trial RL retention factor (0..1); lower means stronger decay toward uniform.
    - wm_noise: per-trial WM corruption strength at set size 6; scaled with nS (>=0).
    - perseveration: additive bias toward repeating the previous action within a block (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, rl_decay, wm_noise, perseveration = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Scale WM availability with set size (static per block)
        wm_weight_block = np.clip(wm_weight * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
        # Load-dependent WM noise strength
        wm_noise_eff = np.clip(wm_noise * (float(nS) / 6.0), 0.0, 1.0)

        prev_action = None
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Perseveration bias
            bias = np.zeros(nA)
            if prev_action is not None:
                bias[int(prev_action)] += perseveration

            # RL policy with bias
            Q_eff = q[s, :] + bias
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy with bias
            W_eff = w[s, :] + bias
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = float(np.clip(p_total, eps, 1.0))
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # RL decay toward uniform (applied globally each trial)
            q = rl_decay * q + (1.0 - rl_decay) * (1.0 / nA)

            # WM update: reward strengthens chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - 0.5) * w[s, :] + 0.5 * target  # moderate strengthening
            # Load-dependent noise corrupts WM toward uniform (all states)
            w = (1.0 - wm_noise_eff) * w + wm_noise_eff * w_0

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p