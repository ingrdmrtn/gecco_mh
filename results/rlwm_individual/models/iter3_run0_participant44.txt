def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited Working Memory with interference by set size.

    Idea:
    - RL learns Q-values with a single learning rate and softmax choice.
    - WM stores state-action mappings but suffers interference that grows with set size (nS).
    - Arbitration mixes WM and RL based on an effective WM weight that declines with set size relative to capacity.
    - WM decays toward uniform at a rate that increases with interference (nS vs capacity).
    - WM updates strongly on rewarded trials; weak avoidance on non-rewarded trials.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
      lr                : RL learning rate (0..1).
      wm_weight_base    : Base WM arbitration weight before capacity scaling (0..1).
      softmax_beta      : RL inverse temperature; internally scaled by 10.
      wm_capacity       : WM capacity (in "slots"); higher => less load penalty (>0).
      wm_interference   : Interference strength controlling WM decay with load (>=0).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_capacity, wm_interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM mix weight declines when nS exceeds capacity
        cap = max(wm_capacity, 1e-6)
        wm_weight_eff = wm_weight_base * min(1.0, cap / max(nS, 1))

        # Interference-driven decay rate increases with nS/capacity
        interf_factor = wm_interference * (nS / cap)
        wm_decay_rate = 1.0 - np.exp(-interf_factor)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY policy: softmax over WM row
            logits_wm = softmax_beta_wm * W_s
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mix RL and WM with capacity-scaled weight
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform due to interference
            w = (1.0 - wm_decay_rate) * w + wm_decay_rate * w_0

            # WM update: stronger on reward, mild avoidance on non-reward
            if r > 0.0:
                # Gain increases when capacity relative to set size is higher
                k = 1.0 - np.exp(-cap / max(nS, 1))
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - k) * w[s, :] + k * one_hot
            else:
                # Gently reduce probability of chosen action; redistribute to others
                k_neg = 0.5 * (1.0 - np.exp(-cap / max(nS, 1)))
                drop = k_neg * w[s, a]
                w[s, a] -= drop
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += drop / (nA - 1)
                # Normalize for numerical stability
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM gated by set-size load and learning strength; no lapses.

    Idea:
    - RL with single learning rate and softmax choice.
    - WM contributes with a weight that decays exponentially with set size.
    - WM learning strength controls how deterministically WM encodes rewarded associations.
    - WM also decays toward uniform faster under higher load (using the same gate parameter).

    Parameters
    ----------
    model_parameters : tuple/list of length 5
      lr             : RL learning rate (0..1).
      wm_weight      : Base WM arbitration weight (0..1).
      softmax_beta   : RL inverse temperature; internally scaled by 10.
      wm_gate_alpha  : Load sensitivity for WM weight/decay; larger => stronger size-based reduction (>=0).
      wm_learning    : WM learning strength toward one-hot on reward (0..1+), higher is stronger.

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, wm_gate_alpha, wm_learning = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Exponential gating by set size: larger nS => smaller WM weight
        wm_gate = np.exp(-wm_gate_alpha * max(nS - 1, 0))
        wm_mix_weight = np.clip(wm_weight * wm_gate, 0.0, 1.0)

        # Use the same gate to modulate decay
        wm_decay_rate = np.clip(1.0 - wm_gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic softmax on WM row
            logits_wm = softmax_beta_wm * W_s
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration
            p_total = wm_mix_weight * p_wm + (1.0 - wm_mix_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay due to load
            w = (1.0 - wm_decay_rate) * w + wm_decay_rate * w_0

            # WM update
            if r > 0.0:
                # Learn toward one-hot with strength wm_learning
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                eta = np.clip(wm_learning, 0.0, 2.0)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # Mild unlearning on errors: move a bit toward uniform
                eta_neg = np.clip(0.5 * wm_learning, 0.0, 1.0)
                w[s, :] = (1.0 - eta_neg) * w[s, :] + eta_neg * w_0[s, :]

            # Normalize row to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with threshold-based load gating and reliability adjustment.

    Idea:
    - RL: standard Q-learning and softmax.
    - WM: contributes more when set size <= threshold, and less otherwise.
    - WM contribution is further scaled by its current reliability (confidence) for the state.
      Reliability is the spread of the WM distribution: max(W_s) - min(W_s).
    - WM decays toward uniform at a fixed rate.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
      lr            : RL learning rate (0..1).
      wm_weight     : Base WM weight when below threshold (0..1).
      softmax_beta  : RL inverse temperature; internally scaled by 10.
      wm_threshold  : Set-size threshold; WM preferred when nS <= threshold (>=1).
      wm_decay      : WM decay rate toward uniform per trial (0..1).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, wm_threshold, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Threshold-based base WM weight
        base_weight = wm_weight if nS <= wm_threshold else 0.2 * wm_weight

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            logits_wm = softmax_beta_wm * W_s
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # WM reliability: spread in W_s (0..1)
            reliability = max(W_s) - min(W_s)
            wm_mix = np.clip(base_weight * reliability, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update
            if r > 0.0:
                # Strong commit to rewarded action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                # On error, softly reduce confidence in chosen action
                reduce = 0.25 * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += reduce / (nA - 1)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p