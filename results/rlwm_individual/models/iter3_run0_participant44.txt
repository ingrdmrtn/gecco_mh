def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + one-shot WM with decay and RPE-based arbitration, modulated by age and load.

    Model idea:
    - RL learns incrementally with a single learning rate.
    - WM stores a near-deterministic mapping after rewarded outcomes, with decay toward uniform otherwise.
    - Arbitration between WM and RL depends on: base WM weight, set size (load), age, and the size of the RL prediction error (RPE).
      Larger RPEs reduce reliance on WM (the system switches to RL when surprised).
    - Older adults have reduced WM influence via an explicit penalty and faster WM decay.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL policy (rescaled internally)
    - wm_decay: WM decay rate toward uniform per trial (0..1) [higher = faster forgetting]
    - arb_slope: sensitivity of arbitration to prediction error (>=0); higher = more switch to RL under large RPE
    - older_penalty: multiplicative penalty on WM mixture if older (0..1), applied as (1 - older_penalty)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, arb_slope, older_penalty = model_parameters
    softmax_beta *= 10  # RL inverse temperature scaling
    softmax_beta_wm = 50  # WM is near-deterministic when encoded
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax probability (near-deterministic when a strong item is stored)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight:
            # - load scaling: stronger WM for smaller sets (3/SS)
            # - RPE-based switching: larger absolute RPE => reduce WM reliance
            # - older penalty reduces WM influence in older adults
            rpe = r - Q_s[a]
            load_scale = 3.0 / float(nS)
            rpe_switch = 1.0 / (1.0 + np.exp(-arb_slope * (-abs(rpe))))  # increases with |RPE|
            wm_mix = wm_weight * load_scale * (1.0 - older_penalty * older) * (1.0 - rpe_switch)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward uniform (older adults decay faster)
            decay_rate = np.clip(wm_decay * (1.0 + 0.5 * older), 0.0, 1.0)
            w[s, :] = (1.0 - decay_rate) * w[s, :] + decay_rate * w_0[s, :]

            # One-shot encoding on reward: push probability mass to rewarded action
            if r > 0.5:
                encode_strength = 1.0  # deterministic write
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])  # renormalize

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with graded encoding and set-size interference; arbitration via uncertainty and age.

    Model idea:
    - RL updates with a single learning rate.
    - WM encodes graded information: reward strengthens the chosen action; non-reward suppresses it slightly.
    - WM suffers interference as set size grows; modeled as blending with uniform according to a sensitivity parameter.
    - Arbitration between WM and RL depends on:
        * base WM weight
        * set-size interference
        * RL uncertainty (entropy of softmax over Q): higher uncertainty -> rely more on WM
        * age: older adults have additional reduction in WM influence.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL policy (rescaled internally)
    - wm_noise: controls how strongly WM encoding is diluted by noise/interference (0..1)
    - setsize_sensitivity: how strongly set size increases interference (>=0)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_noise, setsize_sensitivity = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Interference factor increases with set size; older adults more susceptible via wm_noise
            interference = np.clip(setsize_sensitivity * (nS - 3) / 3.0, 0.0, 1.0)
            wm_blend = np.clip(wm_noise * (1.0 + 0.5 * older) * interference, 0.0, 1.0)
            W_eff = (1.0 - wm_blend) * W_s + wm_blend * w_0[s, :]
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # RL uncertainty from entropy of softmax over Q
            probs_q = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs_q = probs_q / np.sum(probs_q)
            entropy_q = -np.sum(probs_q * np.log(np.clip(probs_q, eps, 1.0)))
            entropy_q /= np.log(nA)  # normalize to [0,1]

            wm_mix = wm_weight * (1.0 - 0.4 * older) * (0.5 + 0.5 * entropy_q) * (3.0 / float(nS))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Graded WM update: reward strengthens chosen action; non-reward weakly suppresses it
            if r > 0.5:
                gain = 1.0 - 0.5 * wm_noise  # less noisy -> stronger write
                w[s, :] = (1.0 - gain) * w[s, :] + gain * w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                suppress = 0.2 + 0.3 * wm_noise
                w[s, a] = (1.0 - suppress) * w[s, a] + suppress * (1.0 / nA)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + WM recency trace and attention lapses modulated by load and age.

    Model idea:
    - RL updates with a single learning rate and includes value forgetting toward a baseline.
    - WM is a recency-weighted distribution that emphasizes the most recently rewarded action for each state.
    - Attention lapses introduce uniform random choice, increasing with load and age.
    - Arbitration combines WM and RL, attenuating WM under high load and for older adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixture weight (0..1)
    - softmax_beta: inverse temperature for RL policy (rescaled internally)
    - rl_forget: RL forgetting rate toward 1/nA (0..1)
    - wm_recency: WM recency gain when a reward occurs (>=0), larger -> more peaked WM
    - attn_lapse: baseline lapse rate (0..1), scaled by load and age

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value (participant age)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_recency, attn_lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy from current WM map
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: down-weight WM with load and age
            wm_mix = wm_weight * (3.0 / float(nS)) * (1.0 - 0.5 * older)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            # Attention lapse increases with load and age
            lapse = np.clip(attn_lapse * (float(nS) / 3.0) * (1.0 + 0.5 * older), 0.0, 1.0)
            p_model = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_model + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward 1/nA baseline
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Recency trace: reward boosts chosen action proportional to wm_recency; gentle decay otherwise
            if r > 0.5:
                boost = wm_recency
                w[s, :] = (1.0 - boost) * w[s, :] + boost * w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                decay = 0.2 + 0.2 * older
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p