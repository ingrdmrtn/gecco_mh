def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and set-size modulated exploration and value decay.

    Mechanism:
    - Q-learning with per-trial value decay (forgetting).
    - The inverse temperature (exploration) is scaled by age and set size:
        beta_eff = beta * exp(beta_age * I_older + beta_size * (3 - set_size))
      so small set sizes increase beta if beta_size > 0 (more exploitation),
      and older adults shift exploration via beta_age.
    - A global decay applied to all Q-values each trial:
        Q <- (1 - d_t) * Q
      where d_t = clip(decay_base * (set_size / 6.0) * (1 + 0.5 * I_older), 0, 0.99)
      thus larger set sizes and older adults forget more.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial; valid actions are {0,1,2}. Invalid values trigger uniform likelihood and no update.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Negative values indicate invalid/missing trials (uniform likelihood, no update).
    blocks : array-like of int
        Block index per trial; learning resets at block boundaries.
    set_sizes : array-like of int
        Set size per trial (constant within block).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 defines older group.
    model_parameters : tuple/list of floats
        (alpha, beta, beta_age, beta_size, decay_base)
        - alpha: learning rate (0..1).
        - beta: base inverse temperature (>0).
        - beta_age: additive shift on log(beta) for older adults.
        - beta_size: coefficient on (3 - set_size) for log(beta); >0 increases exploitation for small sets.
        - decay_base: base decay factor in [0,1); higher means more forgetting; scaled by set size and age.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, beta_age, beta_size, decay_base = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = float(block_set_sizes[t])

            # compute dynamic beta and decay at trial t
            log_beta_eff = np.log(max(beta, 1e-8)) + beta_age * is_older + beta_size * (3.0 - set_size_t)
            beta_eff = np.exp(np.clip(log_beta_eff, -20, 20))
            decay_t = decay_base * (set_size_t / 6.0) * (1.0 + 0.5 * is_older)
            decay_t = np.clip(decay_t, 0.0, 0.99)

            # apply decay to all Q values before observing choice/update
            Q *= (1.0 - decay_t)

            # handle invalid trials
            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # softmax over Q for current state
            logits = beta_eff * Q[s, :]
            logits -= np.max(logits)
            p = np.exp(logits)
            p_sum = np.sum(p)
            if p_sum <= 0:
                p = (1.0 / nA) * np.ones(nA)
            else:
                p /= p_sum

            total_loglik += np.log(np.clip(p[a], 1e-12, 1.0))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return -total_loglik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with one-shot consolidation (WM-like sharpening) gated by age and set size.

    Mechanism:
    - Standard Q-learning governs incremental updates.
    - When a reward is received, the model performs a one-shot consolidation for that state:
        Q[s, :] <- (1 - psi_t) * Q[s, :] + psi_t * onehot(a)
      This emulates WM capturing the correct action after reward with strength psi_t.
    - The consolidation gain psi_t is a logistic function of age group and set size:
        psi_t = sigmoid(psi0 + psi_age * I_older + psi_size * (3 - set_size))
      Larger (3 - set_size) favors consolidation in small sets if psi_size > 0.
      Age shifts consolidation up/down via psi_age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial; valid actions are {0,1,2}. Invalid values trigger uniform likelihood and no update.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Negative values indicate invalid/missing trials (uniform likelihood, no update).
    blocks : array-like of int
        Block index per trial; learning resets at block boundaries.
    set_sizes : array-like of int
        Set size per trial (constant within block).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 defines older group.
    model_parameters : tuple/list of floats
        (alpha, beta, psi0, psi_age, psi_size)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature (>0).
        - psi0: baseline logit for consolidation gain.
        - psi_age: additive logit shift for older adults (>=45).
        - psi_size: coefficient on (3 - set_size) for consolidation logit; >0 favors small sets.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, psi0, psi_age, psi_size = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = float(block_set_sizes[t])

            # handle invalid trials
            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p = np.exp(logits)
            p_sum = np.sum(p)
            if p_sum <= 0:
                p = (1.0 / nA) * np.ones(nA)
            else:
                p /= p_sum

            total_loglik += np.log(np.clip(p[a], 1e-12, 1.0))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # One-shot consolidation upon reward
            if r > 0:
                logit_psi = psi0 + psi_age * is_older + psi_size * (3.0 - set_size_t)
                psi = 1.0 / (1.0 + np.exp(-np.clip(logit_psi, -20, 20)))
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                Q[s, :] = (1.0 - psi) * Q[s, :] + psi * onehot

    return -total_loglik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and set-size dependent lapse probability (noisy gating).

    Mechanism:
    - Choices come from a mixture of a softmax over Q-values and a uniform random policy:
        p(a|s) = (1 - eps_t) * softmax(beta * Q[s,:]) + eps_t * 1/3
      where eps_t is a lapse probability capturing failures of control/WM.
    - The lapse probability is logistic in age group and set size:
        eps_t = sigmoid(l0 + l_age * I_older + l_size * (set_size - 3))
      Larger set sizes increase lapses when l_size > 0, and older adults lapse more when l_age > 0.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action per trial; valid actions are {0,1,2}. Invalid values trigger uniform likelihood and no update.
    rewards : array-like of float
        Feedback per trial (typically 0/1). Negative values indicate invalid/missing trials (uniform likelihood, no update).
    blocks : array-like of int
        Block index per trial; learning resets at block boundaries.
    set_sizes : array-like of int
        Set size per trial (constant within block).
    age : array-like of float
        Participant age; age[0] used. Age >= 45 defines older group.
    model_parameters : tuple/list of floats
        (alpha, beta, l0, l_age, l_size)
        - alpha: learning rate (0..1).
        - beta: inverse temperature (>0).
        - l0: baseline logit for lapse probability.
        - l_age: additive logit shift for older adults (>=45).
        - l_size: coefficient on (set_size - 3) for lapse logit; >0 increases lapses in larger sets.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, l0, l_age, l_size = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx]
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_size_t = float(block_set_sizes[t])

            # handle invalid trials
            if (a < 0) or (a >= nA) or (r < 0):
                total_loglik += -np.log(nA)
                continue

            # compute lapse probability
            logit_eps = l0 + l_age * is_older + l_size * (set_size_t - 3.0)
            eps = 1.0 / (1.0 + np.exp(-np.clip(logit_eps, -20, 20)))
            eps = np.clip(eps, 0.0, 1.0)

            # softmax policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p_soft = np.exp(logits)
            p_sum = np.sum(p_soft)
            if p_sum <= 0:
                p_soft = (1.0 / nA) * np.ones(nA)
            else:
                p_soft /= p_sum

            p = (1.0 - eps) * p_soft + eps * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            p /= np.sum(p)

            total_loglik += np.log(p[a])

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return -total_loglik