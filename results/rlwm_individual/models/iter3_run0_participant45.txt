def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with interference, age-slowed WM encoding, and choice stickiness.

    Idea:
    - RL updates action values with a standard learning rate.
    - WM stores rewarded state-action pairs via a fast Hebbian-like update, but suffers from
      load-dependent interference/decay toward uniform (stronger with larger set size).
    - Older adults have slower WM encoding (age_slow reduces WM update strength).
    - Choice stickiness biases both RL and WM policies toward the previously chosen action in that state.
    - Arbitration is confidence-weighted: WM weight is proportional to WM distinctiveness (margin between
      best and second-best) and penalized by load and age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_eta: WM encoding rate toward the rewarded action; older adults have reduced effective wm_eta.
    - interference: Strength of load-dependent WM interference/decay toward uniform (>=0).
    - age_slow: Factor reducing WM encoding for older adults (>=0; applied if age>=45).
    - stickiness: Choice perseveration bias added to the previous action in that state (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_eta, interference, age_slow, stickiness = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # highly deterministic WM
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Previous choice per state for stickiness
        prev_a = -np.ones(nS, dtype=int)

        # Age-modulated WM encoding rate
        wm_eta_eff = wm_eta / (1.0 + age_slow * is_older)

        # Load-dependent interference rate per trial (stronger with larger nS)
        wm_interf = np.clip(interference * (nS - 1.0) / max(1.0, nS), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Bias vectors for stickiness
            bias = np.zeros(nA)
            if prev_a[s] >= 0:
                bias[prev_a[s]] += stickiness

            # RL policy
            Q_s = q[s, :].copy()
            Q_s_biased = Q_s + bias
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s_biased - Q_s_biased[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :].copy()
            W_s_biased = W_s + bias
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_biased - W_s_biased[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Confidence-based arbitration weight from WM:
            # margin between best and second best, scaled by load and age
            sorted_W = np.sort(W_s_biased)[::-1]
            margin = max(0.0, sorted_W[0] - sorted_W[1]) if nA > 1 else 0.0
            load_factor = 3.0 / max(1.0, float(nS))  # down-weight WM under load
            age_factor = 1.0 / (1.0 + age_slow * is_older)
            wm_weight_t = np.clip(margin * load_factor * age_factor, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global interference/decay toward uniform
            w = (1.0 - wm_interf) * w + wm_interf * w_0

            # WM state-specific update:
            # Rewarded: move toward one-hot for chosen action; Unrewarded: slight relaxation to uniform
            target = np.full(nA, 1.0 / nA)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] = (1.0 - wm_eta_eff) * w[s, :] + wm_eta_eff * target

            # Update stickiness memory
            prev_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and surprise-gated WM use modulated by age and load.

    Idea:
    - RL updates with learning rate and per-visit decay (forgetting) toward uniform, capturing memory loss.
    - WM stores recent rewarded associations and is deterministic, but decays toward uniform over time.
    - Arbitration weight for WM is a logistic function of RL surprise (|prediction error|), penalized by
      higher load (log(nS)) and by age (older adults show lower WM gating).
    - This creates greater reliance on WM after surprising outcomes when load is low and in younger adults.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - rl_decay: Per-visit RL decay toward uniform for the current state (0..1).
    - gate_base: Baseline bias for using WM in arbitration (can be negative or positive).
    - surprise_gain: Sensitivity of WM gating to absolute RL surprise |PE| (>=0).
    - age_gate_penalty: Reduction in WM gating for older adults (>=0; applied if age>=45).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, rl_decay, gate_base, surprise_gain, age_gate_penalty = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # We keep a small WM decay proportional to rl_decay to capture general drift
        wm_decay = np.clip(0.5 * rl_decay, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # RL prediction error (before RL update)
            pe = r - Q_s[a]
            surprise = abs(pe)

            # Surprise-gated WM arbitration with age and load penalties
            load_penalty = np.log(max(2.0, float(nS)))  # stronger penalty at larger nS
            gate_input = gate_base + surprise_gain * surprise - age_gate_penalty * is_older - load_penalty
            p_use_wm = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_t = np.clip(p_use_wm, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL forgetting (toward uniform) for the visited state, then standard update
            q[s, :] = (1.0 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)
            q[s, a] += lr * (r - q[s, a])

            # WM decay toward uniform globally (light)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: on reward, encode chosen action strongly; on no-reward, slight relaxation to uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # encode with strength proportional to surprise to prioritize unexpected wins
                enc = np.clip(0.5 + 0.5 * surprise, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                # small drift to uniform when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + resource-rational WM with capacity K, rehearsal dynamics, age-reduced rehearsal, and WM lapses.

    Idea:
    - RL learns gradually via standard learning rule.
    - WM holds state-action associations, with effective access probability tied to capacity per item:
      p_use_wm(s) âˆ (K_total / nS) times an attention/rehearsal weight a_s that builds with visits.
    - Rehearsal/attention increases when a state is visited and decays otherwise; older adults rehearse less.
    - WM policy is near-deterministic but suffers lapses (epsilon mix with uniform).
    - Arbitration weight equals the current p_use_wm(s), reflecting resource allocation across states and load.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - K_total: Total WM capacity to be distributed across states (>=0).
    - rehearsal_rate: Increment to state-specific rehearsal/attention per visit (0..1).
    - age_reduction: Proportional reduction in rehearsal for older adults (0..1 applied if age>=45).
    - wm_lapse: Lapse probability within WM policy mixing with uniform (0..1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, K_total, rehearsal_rate, age_reduction, wm_lapse = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific rehearsal/attention weights a_s in [0,1]
        a = np.zeros(nS)

        # Age-modulated rehearsal increment
        rehearse_inc = rehearsal_rate * (1.0 - age_reduction * is_older)
        rehearse_inc = np.clip(rehearse_inc, 0.0, 1.0)

        # Per-trial passive decay of attention to spread resources (scaled by nS)
        base_decay = np.clip(rehearse_inc / max(1.0, float(nS)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a_obs = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a_obs])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy with lapse (epsilon) mixing with uniform
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a_obs])))
            p_wm_soft = 1.0 / max(denom_wm, eps)
            p_wm = (1.0 - wm_lapse) * p_wm_soft + wm_lapse * (1.0 / nA)

            # Resource-rational arbitration weight
            cap_per_item = np.clip(K_total / max(1.0, float(nS)), 0.0, 1.0)
            wm_weight_t = np.clip(cap_per_item * a[s], 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a_obs]
            q[s, a_obs] += lr * delta

            # Rehearsal dynamics: global decay, then increment for visited state
            a = (1.0 - base_decay) * a
            a[s] = np.clip(a[s] + rehearse_inc * (1.0 - a[s]), 0.0, 1.0)

            # WM update: reward-driven binding; otherwise relax toward uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a_obs] = 1.0
                # Encoding strength leverages current attention level a[s]
                enc = np.clip(0.5 * a[s] + 0.25, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                # Small relaxation to uniform when not rewarded
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p