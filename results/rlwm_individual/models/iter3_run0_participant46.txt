def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: PE-gated RL-WM mixture with load-dependent WM leak and reward-driven WM encoding
    - RL: tabular Q-learning with softmax policy (template).
    - WM: probability table over actions per state (w), decays to uniform with load-dependent leak,
          and encodes rewarded associations with a learning rate.
    - Arbitration: WM mixture weight is downregulated by absolute RL prediction error (|PE|) and by set size.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate for Q updates.
    - wm_weight_base: float in [0,1], baseline WM contribution at set size 3 with no PE.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - pe_gate: float >= 0, sensitivity of WM weight to absolute prediction error (larger -> stronger downweighting by large |PE|).
    - decay_ss: float in [0,1], base WM leak per trial at set size 3; scales up with set size.
    - wm_learn: float in [0,1], WM learning rate toward the rewarded action on positive feedback.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, pe_gate, decay_ss, wm_learn = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over WM weights (very sharp)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # PE-gated, load-scaled arbitration
            pe = abs(r - Q_s[a])
            gate = 1.0 / (1.0 + np.exp(pe_gate * (pe - 0.5)))  # large PE -> gate small
            load_scale = 3.0 / float(nS)                        # larger set -> smaller scale
            wm_weight = np.clip(wm_weight_base * gate * load_scale, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Load-dependent WM leak toward uniform
            leak = np.clip(decay_ss * (float(nS) / 3.0), 0.0, 1.0)
            w = (1.0 - leak) * w + leak * w_0

            # Reward-driven WM encoding with learning rate
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # On errors, slightly suppress the chosen action in WM (mild unlearning scaled by load)
                unlearn = 0.1 * (3.0 / float(nS))
                w[s, a] = max(0.0, w[s, a] - unlearn)
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Entropy-arbitrated RL-WM with WM sharpening and lapse
    - RL: tabular Q-learning with softmax.
    - WM: probabilistic table over actions per state; rewarded actions strengthened, errors mildly weakened.
          WM decision policy uses a "sharpening" exponent gamma_wm before the (fixed) WM softmax.
    - Arbitration: WM weight increases with RL policy entropy (uncertainty) and decreases with larger set size.
    - Lapse: mixture with uniform choice at rate epsilon.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate for Q updates.
    - wm_weight_base: float in [0,1], baseline WM mixture weight.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled by 10).
    - gamma_wm: float >= 0, WM sharpening exponent; gamma_wm>1 makes WM policy more peaked.
    - entropy_gain: float, scales effect of RL entropy on WM weight (positive -> more WM when RL is uncertain).
    - epsilon: float in [0,1], lapse rate mixing in a uniform policy.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, gamma_wm, entropy_gain, epsilon = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: sharpen WM distribution before softmax
            W_sharp = W_s.copy()
            W_sharp = np.power(np.clip(W_sharp, 1e-12, 1.0), max(0.0, gamma_wm))
            if np.sum(W_sharp) <= 0:
                W_sharp = w_0[s, :].copy()
            else:
                W_sharp = W_sharp / np.sum(W_sharp)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_sharp - W_sharp[a])))

            # RL policy distribution and entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits)
            pi_rl = pi_rl / np.sum(pi_rl)
            H = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))
            H_norm = H / np.log(nA)  # in [0,1]

            # Arbitration: base + entropy effect, scaled down by set size
            wm_weight = wm_weight_base + entropy_gain * (H_norm - 0.5)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)
            wm_weight *= (3.0 / float(nS))

            p_mix = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updates: strengthen on reward, mild weakening on error
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Move halfway toward one-hot to allow graded accumulation
                w[s, :] = 0.5 * w[s, :] + 0.5 * target
            else:
                # Mild penalty for chosen action; proportional to 1/(1+gamma_wm) to reuse parameter meaningfully
                unlearn = 0.2 / (1.0 + max(0.0, gamma_wm))
                w[s, a] = max(0.0, w[s, a] - unlearn)
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Capacity-limited committing WM with threshold and deterministic eviction
    - RL: tabular Q-learning with softmax.
    - WM: per-state strength m[s,a] for action associations. Upon sufficient evidence (rewarded repeats),
          a state "commits" to WM with a one-hot mapping. WM capacity declines with set size and there is
          deterministic eviction of the weakest committed states when capacity is exceeded.
    - Arbitration: if a state is committed, WM has higher weight (scaled by load); otherwise RL dominates.
      WM traces forget over time.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate for Q updates.
    - softmax_beta: float >=0, inverse temperature for RL softmax (internally scaled by 10).
    - wm_capacity: float >=0, nominal WM capacity at set size 3 (in number of items); effective capacity scales with 3/nS.
    - commit_thresh: float >=0, strength threshold to commit a state to WM.
    - forget_rate: float in [0,1], per-trial decay of both WM strengths and WM policy toward uniform; scales with set size.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_capacity, commit_thresh, forget_rate = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        m = np.zeros((nS, nA))            # WM evidence strengths
        committed = np.zeros(nS, dtype=bool)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective capacity declines with set size (at least 0)
            K_eff = int(max(0.0, np.floor(wm_capacity * (3.0 / float(nS)))))
            # Current committed indices and their strengths
            committed_idxs = np.where(committed)[0]
            max_m_per_state = np.max(m[committed_idxs, :], axis=1) if committed_idxs.size > 0 else np.array([])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy from current w
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: strong if committed; else weak; both scaled by load
            base_wm = 1.0 if committed[s] else 0.0
            wm_weight = base_wm * (3.0 / float(nS))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Global WM forgetting scaled by set size
            decay = np.clip(forget_rate * (float(nS) / 3.0), 0.0, 1.0)
            m *= (1.0 - decay)
            w = (1.0 - decay) * w + decay * w_0

            # Evidence update
            if r > 0.5:
                m[s, a] += 1.0
                # Make w one-hot-alike toward the rewarded action for this state
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Penalize evidence for the chosen action on error
                m[s, a] *= (1.0 - 0.5)

            # Check (re)commitment for state s
            m_max = np.max(m[s, :])
            if m_max >= commit_thresh:
                committed[s] = True
            # Enforce capacity via deterministic eviction: keep strongest K_eff
            if K_eff < np.sum(committed):
                if committed_idxs.size == 0:
                    committed_idxs = np.where(committed)[0]
                    max_m_per_state = np.max(m[committed_idxs, :], axis=1)
                # Build list and evict weakest until cap satisfied
                order = np.argsort(max_m_per_state)  # ascending strengths
                to_evict_count = int(np.sum(committed) - K_eff)
                # Evict weakest states
                for idx in order[:to_evict_count]:
                    s_evict = committed_idxs[idx]
                    committed[s_evict] = False
                    # On eviction, reset w for that state to uniform to reflect loss from WM
                    w[s_evict, :] = w_0[s_evict, :].copy()
            # If capacity is zero, ensure no commitments
            if K_eff == 0:
                if np.any(committed):
                    committed[:] = False
                    w = w_0.copy()

        blocks_log_p += log_p

    return -blocks_log_p