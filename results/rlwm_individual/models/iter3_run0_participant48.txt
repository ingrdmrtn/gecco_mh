def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with global leak + WM recall scaled by set size and WM confidence.

    Mechanism:
    - RL: tabular Q-learning with decay (leak) of all Q-values toward uniform each trial.
    - WM: item-specific store that becomes one-hot for rewarded state-action pairs and otherwise leaks toward uniform.
    - Arbitration: WM mixture weight is reduced exponentially with set size and further scaled by WM certainty on the current state.
      This yields strong WM influence in small set sizes and when WM is confident, but reduced influence under higher load or uncertainty.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_base: baseline WM mixture weight (0..1)
    - leak: global leak toward uniform for both Q and WM (0..1)
    - load_sensitivity: controls how quickly WM influence drops with set size (>=0)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_base, leak, load_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM downweighting for the block
        wm_block = wm_base * np.exp(-load_sensitivity * (nS - 1.0))  # decreases with nS

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM confidence scaling: 0 when uniform, 1 when one-hot
            max_w = np.max(W_s)
            certainty = (max_w - 1.0 / nA) / (1.0 - 1.0 / nA)
            certainty = max(0.0, min(1.0, certainty))
            wm_weight = wm_block * certainty

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update with global leak
            delta = r - Q_s[a]
            q *= (1.0 - leak)
            q += leak * (1.0 / nA)
            q[s, a] += lr * delta

            # WM leak toward uniform each trial
            w = (1.0 - leak) * w + leak * w_0
            # Reward-gated WM write
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice perseveration + recency-based WM and load-dependent interference.

    Mechanism:
    - RL: tabular Q-learning.
    - Choice perseveration: RL policy is biased toward repeating the previous action in that state (kappa).
    - WM: recency store that encodes the most recent action for each state.
      On rewarded trials, WM writes a one-hot for the chosen action.
      On unrewarded trials, WM still records recency but more weakly (partial bias).
    - Interference: WM representations decay toward uniform each trial by 'interference', which scales up with set size.
    - Arbitration: fixed WM mixture weight (wm_weight).

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: mixture weight for WM vs RL (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - kappa: choice perseveration strength added to the last action in state (>=0)
    - interference: baseline WM interference/decay toward uniform per trial (0..1), amplified by set size

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # per-state last choice

        # Load-amplified interference
        load_factor = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6
        eff_interf = min(1.0, interference * (1.0 + load_factor))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Choice perseveration bias in RL policy only
            if last_action[s] >= 0:
                Qe = Q_s.copy()
                Qe[last_action[s]] += kappa
            else:
                Qe = Q_s

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Qe - Qe[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update (no bias in value update)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM interference toward uniform
            w = (1.0 - eff_interf) * w + eff_interf * w_0

            # WM recency update
            if r > 0.0:
                # Strong, rewarded write
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Weak, unrewarded recency: partial bias to chosen action, then renormalize
                w[s, :] *= 0.5
                w[s, a] += 0.5
                w[s, :] /= np.sum(w[s, :])

            # Track last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Meta-cognitive arbitration: WM weight increases with RL uncertainty + load-dependent lapses.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: item-specific store with decay (wm_decay) toward uniform; on reward, WM writes a one-hot.
    - Arbitration: WM mixture weight is wm_weight0 scaled by the current RL uncertainty on the state:
        weight_wm = wm_weight0 * (H(Q_s))**wm_precision
      where H(Q_s) is the normalized entropy of the softmax over Q_s (0..1). Thus, when RL is uncertain (high entropy),
      WM has stronger influence; as RL becomes certain, the model relies more on RL.
    - Lapses: a set-size dependent lapse probability diverts choices to uniform random with probability epsilon(nS).

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight0: baseline WM mixture weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_precision: exponent controlling sensitivity of arbitration to RL uncertainty (>=0)
    - lapse_slope: slope controlling how lapse increases with set size (>=0)
    - wm_decay: WM decay rate toward uniform per trial (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_precision, lapse_slope, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent lapse (0 for 3 items; increases with nS)
        load_factor = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6
        epsilon = min(0.5, lapse_slope * load_factor)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL uncertainty (normalized entropy of softmax over Q_s)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            probs = np.exp(logits)
            probs /= np.sum(probs)
            H = -np.sum(probs * np.log(probs + 1e-15)) / np.log(float(nA))  # 0..1

            # WM arbitration weight increases with RL uncertainty
            wm_weight = wm_weight0 * (max(0.0, min(1.0, H)) ** max(0.0, wm_precision))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Lapse-augmented mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and reward-gated write
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p