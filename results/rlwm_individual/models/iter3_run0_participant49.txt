def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working-memory (WM) with age- and load-dependent encoding/maintenance.

    Mechanism
    - Two systems: model-free RL and a capacity-limited WM store of state->action mappings.
    - WM attempts to encode the currently correct action after rewarded trials.
      Encoding success depends on: a base encoding bias, available slots for the block's set size,
      and an age penalty (older adults have fewer effective slots).
    - Maintenance decays with time (trial-wise forgetting), reducing WM weights toward uniform.
    - Policy is a mixture of WM and RL; WM weight equals the current probability that the item
      is successfully stored and maintained.
    - RL uses standard delta rule.

    Parameters (6)
    ----------
    model_parameters : sequence of 6 floats
        alpha_rl       : Learning rate for RL values
        beta           : Softmax inverse temperature for both systems
        encode_bias    : Baseline encoding probability (0..1) when rewarded
        slots3         : Effective WM slots in small set-size blocks (0..3)
        slots6         : Effective WM slots in large set-size blocks (0..6)
        age_penalty    : Proportional reduction of effective slots for older adults (>=45)

    Inputs
    ------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block index per trial; learning and WM reset each block.
    set_sizes : array-like of int
        Set size per trial (3 or 6); constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) reduces effective WM capacity.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha_rl, beta, encode_bias, slots3, slots6, age_penalty = model_parameters

    # Parse age and older group flag
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: W holds preference for a single "stored" action per state; starts uniform
        W = (1.0 / nA) * np.ones((nS, nA))
        # Store flags indicate whether an item is believed stored for a state
        stored = np.zeros(nS, dtype=float)

        # Determine effective capacity for this block, factoring age
        if nS == 3:
            eff_slots = max(0.0, slots3 * (1.0 - age_penalty * is_older))
        else:
            eff_slots = max(0.0, slots6 * (1.0 - age_penalty * is_older))

        # Convert slots to per-item storage probability when encoding is attempted
        # If eff_slots >= nS => close to 1, else proportional
        per_item_store_prob = min(1.0, eff_slots / max(1.0, float(nS)))

        # Trial-wise WM decay rate: with more items (and with age), stronger decay
        base_decay = 0.10  # baseline forgetting per trial
        load_decay = 0.05 * (nS - 3)  # extra for set size 6
        age_decay = 0.05 * is_older
        decay = min(0.99, max(0.0, base_decay + load_decay + age_decay))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            pref_rl = Q[s, :].copy()
            pref_rl -= np.max(pref_rl)
            p_rl = np.exp(beta * pref_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM maintenance decay toward uniform each trial (state-wise)
            # Apply small decay to all states to reflect global maintenance pressure
            W = (1.0 - decay) * W + decay * (1.0 / nA)

            # WM policy: if stored[s] ~1, W[s] is peaked on stored action; otherwise near uniform
            pref_wm = W[s, :].copy()
            pref_wm -= np.max(pref_wm)
            p_wm = np.exp(beta * pref_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # WM gate for this state = probability item is stored and maintained
            wm_gate = np.clip(stored[s], 0.0, 1.0)

            # Mixture policy
            p_mix = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_a = p_mix[a]
            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM encoding after feedback: attempt only if reward present
            if r > 0.5:
                # Encoding attempt success prob = encode_bias * per_item_store_prob
                enc_success = encode_bias * per_item_store_prob
                enc_success = np.clip(enc_success, 0.0, 1.0)

                # Update stored flag toward success
                # We treat enc_success as probability mass moving stored[s] toward 1
                stored[s] = stored[s] + (1.0 - stored[s]) * enc_success

                # If stored, shape W to favor the chosen action
                strength = stored[s]
                W[s, :] = (1.0 - strength) * (1.0 / nA)
                W[s, a] += strength
                # Renormalize to a distribution
                W[s, :] = np.maximum(W[s, :], eps)
                W[s, :] = W[s, :] / np.sum(W[s, :])
            else:
                # No reward: reduce confidence that mapping is correct (forget faster)
                stored[s] *= (1.0 - 0.5 * enc_success) if 'enc_success' in locals() else (1.0 - 0.2)

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and win-stay/lose-shift (WSLS) bias; decision noise increases with load and age.

    Mechanism
    - RL values updated with eligibility traces (bandit setting, gamma=1). The trace allows
      recent choices to receive more credit over a few trials.
    - Policy uses softmax with an effective temperature that worsens (beta drops) with set size
      and for older adults.
    - WSLS bias shifts preferences in the current state based on the last outcome in that state:
      after a win, the last rewarded action is favored; after a loss, it is discouraged.
    - No cross-state stickiness is used; the bias is state-specific.

    Parameters (6)
    ----------
    model_parameters : sequence of 6 floats
        alpha         : Learning rate for RL updates
        beta0         : Baseline inverse temperature
        ws_bias       : Magnitude of WSLS preference shift
        noise_load    : Load sensitivity decreasing beta for larger set sizes
        noise_age     : Age sensitivity decreasing beta for older adults
        lambda_tr     : Eligibility trace decay parameter (0..1)

    Inputs
    ------
    states, actions, rewards, blocks, set_sizes, age : see above in model 1

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta0, ws_bias, noise_load, noise_age, lambda_tr = model_parameters

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        E = np.zeros((nS, nA))  # eligibility traces

        # State-specific memory of last outcome in this state
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS, dtype=float)

        # Effective beta for the block (worse with load and for older adults)
        load_term = noise_load * max(0.0, float(nS) - 3.0)
        age_term = noise_age * is_older
        beta_eff = beta0 * np.exp(-(load_term + age_term))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # WSLS preference shift in this state
            prefs = Q[s, :].copy()
            if last_action[s] >= 0:
                if last_reward[s] > 0.5:
                    # Win: favor repeating the last rewarded action
                    prefs[last_action[s]] += ws_bias
                else:
                    # Loss: discourage the last action (equivalently favor others)
                    prefs[last_action[s]] -= ws_bias

            # Softmax policy
            prefs -= np.max(prefs)
            p = np.exp(beta_eff * prefs)
            p = p / (np.sum(p) + eps)
            total_log_p += np.log(max(p[a], eps))

            # Eligibility traces update (bandit: gamma=1)
            E *= lambda_tr
            E[s, a] = 1.0

            # RL update with traces
            pe = r - Q[s, a]
            Q += alpha * pe * E

            # Update last outcome memory for WSLS
            last_action[s] = a
            last_reward[s] = r

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with Pearce-Hall associability and age/load-modulated learning rates plus lapse.

    Mechanism
    - Each state-action pair maintains a dynamic associability alpha_t (Pearce-Hall), which adapts
      to recent surprise: alpha_t moves toward |prediction error|.
    - Older age and larger set size reduce effective learning by scaling associability.
    - A small lapse process mixes a uniform policy with the softmax RL policy. Lapse increases for older adults.
    - Choice policy is softmax over Q values.

    Parameters (6)
    ----------
    model_parameters : sequence of 6 floats
        alpha0     : Initial associability for all state-actions
        beta       : Inverse temperature for softmax
        kappa_ph   : Update gain for associability (how fast alpha tracks |PE|)
        age_mod    : Multiplicative reduction of associability for older adults (>=45)
        load_mod   : Multiplicative reduction of associability with larger set size
        lapse      : Baseline lapse probability mixed with uniform; increased by age

    Inputs
    ------
    states, actions, rewards, blocks, set_sizes, age : see model 1

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha0, beta, kappa_ph, age_mod, load_mod, lapse = model_parameters

    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        A = alpha0 * np.ones((nS, nA))  # associability per state-action

        # Modulate associability by age and load in this block
        load_scale = 1.0 - load_mod * max(0.0, (float(nS) - 3.0) / 3.0)
        age_scale = 1.0 - age_mod * is_older
        assoc_scale = max(0.0, load_scale * age_scale)

        # Lapse increases for older adults
        lapse_eff = np.clip(lapse * (1.0 + 0.5 * is_older), 0.0, 0.5)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax RL policy
            prefs = Q[s, :].copy()
            prefs -= np.max(prefs)
            p_soft = np.exp(beta * prefs)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Mixture with lapse
            p_a = (1.0 - lapse_eff) * p_soft[a] + lapse_eff * (1.0 / nA)
            total_log_p += np.log(max(p_a, eps))

            # PE and associability update
            pe = r - Q[s, a]
            # Effective learning rate for this pair at this time
            lr_eff = assoc_scale * A[s, a]
            Q[s, a] += lr_eff * pe

            # Pearce-Hall: associability moves toward |PE|
            A[s, a] = (1.0 - kappa_ph) * A[s, a] + kappa_ph * abs(pe)
            # Keep within [0,1]
            A[s, a] = min(1.0, max(0.0, A[s, a]))

    return -float(total_log_p)