def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Mixture of Q-learning and Win-Stay/Lose-Shift with age- and set-size-adaptive mixing and perseveration.

    Idea:
    - A model-free Q-learner produces a softmax policy with a within-state perseveration bias.
    - A heuristic Win-Stay/Lose-Shift (WSLS) policy operates per state using the most recent outcome.
    - The mixture weight between WSLS and Q-learning adapts by set size (lower WSLS under higher load)
      and by age group (younger rely more on WSLS; older rely less).
    - Q-values are updated with a single learning rate when choices and rewards are valid.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions (<0 or >=3) are treated as random and do not update values.
    rewards : array-like of int or float
        Feedback 0/1. Invalid rewards (not 0/1) cause no learning.
    blocks : array-like of int
        Block index per trial. Value memory and WSLS memory reset per block.
    set_sizes : array-like of int
        Set size (3 or 6), used to adjust the WSLS mixture weight per trial.
    age : array-like of int or float
        Participant age; younger (<45) vs older (>=45) adjusts the WSLS mixture weight.
    model_parameters : tuple/list of floats
        (alpha, beta, ws_weight_base, persev, age_ws_bonus)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax over Q with perseveration bias.
        - ws_weight_base: base WSLS mixture propensity (on logit scale; passed through sigmoid).
        - persev: perseveration bias added to the last action's logit within a state.
        - age_ws_bonus: age modulation of WSLS mixture; younger add +bonus, older subtract -bonus.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the mixture policy.
    """
    alpha, beta, ws_weight_base, persev, age_ws_bonus = model_parameters
    nA = 3
    eps = 1e-12
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize Q-values and WSLS memory per state
        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # -1 means no prior action in this state
        last_reward = -np.ones(nS, dtype=int)  # -1 means unknown

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r_raw = block_rewards[t]
            r_valid = (r_raw == 0) or (r_raw == 1)
            r = 1.0 if r_raw == 1 else 0.0
            set_size = int(block_set_sizes[t])

            # Construct RL softmax with perseveration bias
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0 and last_action[s] < nA:
                logits[last_action[s]] += persev  # bias to repeat within the state
            pi_rl = np.exp(logits)
            pi_rl = pi_rl / np.sum(pi_rl)

            # Construct WSLS probabilities
            pi_ws = np.ones(nA) / nA  # default uniform if no prior for this state
            if last_action[s] >= 0 and last_action[s] < nA and last_reward[s] in (0, 1):
                if last_reward[s] == 1:
                    # Win-Stay: place probability mass on repeating last action
                    pi_ws = np.full(nA, eps)
                    pi_ws[last_action[s]] = 1.0 - (nA - 1) * eps
                else:
                    # Lose-Shift: distribute uniformly across other actions
                    pi_ws = np.full(nA, 0.0)
                    others = [aa for aa in range(nA) if aa != last_action[s]]
                    for aa in others:
                        pi_ws[aa] = 1.0 / (nA - 1)
                    # keep tiny epsilon everywhere to avoid zeros
                    pi_ws = (1 - nA * eps) * pi_ws + eps

            # Adaptive WSLS mixture weight: base -> sigmoid; modulated by age and set size
            base = 1.0 / (1.0 + np.exp(-ws_weight_base))
            age_shift = age_ws_bonus if younger == 1 else -age_ws_bonus
            # Harder set size reduces WSLS: subtract proportional to (set_size-3)
            ss_shift = -0.5 * max(0, set_size - 3)
            mix_logit = np.log(base + eps) - np.log(1 - base + eps) + age_shift + ss_shift
            ws_w = 1.0 / (1.0 + np.exp(-mix_logit))
            ws_w = min(max(ws_w, 0.0), 1.0)

            # Mixture policy
            pi = ws_w * pi_ws + (1.0 - ws_w) * pi_rl
            pi = pi / np.sum(pi)

            # Likelihood contribution
            if a < 0 or a >= nA:
                # invalid action treated as random
                total_log_p += np.log(max(1.0 / nA, eps))
            else:
                total_log_p += np.log(max(pi[a], eps))

            # Learning updates only if valid action and valid reward
            if (a >= 0 and a < nA) and r_valid:
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # Update WSLS memory
                last_action[s] = a
                last_reward[s] = int(r)
            else:
                # Even if no learning, update last action (for perseveration) if action is in-bounds
                if a >= 0 and a < nA:
                    last_action[s] = a
                # Do not set last_reward unless reward valid

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-sensitive Kalman Q-learning with age-modulated observation noise and set-size-scaled process noise.

    Idea:
    - Maintain Gaussian beliefs over each Q(s,a): mean (mu) and variance (var).
    - Update via a Kalman-like filter on valid feedback; larger set sizes increase process noise (harder memory).
    - Choice is uncertainty-sensitive: prefer actions with high mean and low uncertainty
      via softmax over mu / sqrt(var).
    - Age modulates both the effective temperature (beta) and the observation noise (older -> noisier, lower beta;
      younger -> less noisy, higher beta).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions are treated as random and skip learning.
    rewards : array-like of int or float
        Feedback 0/1. Invalid rewards (not 0/1) skip learning.
    blocks : array-like of int
        Block index per trial. Beliefs reset per block.
    set_sizes : array-like of int
        Set size (3 or 6), used to scale the process noise and thereby uncertainty.
    age : array-like of int or float
        Participant age; younger (<45) vs older (>=45) modulates beta and observation noise.
    model_parameters : tuple/list of floats
        (eta_proc, beta, sigma_obs, ss_scale, age_unc_gain)
        - eta_proc: base process noise scale per update (>=0).
        - beta: base inverse temperature for softmax over uncertainty-normalized means.
        - sigma_obs: base observation noise for rewards (>=0).
        - ss_scale: multiplicative factor controlling how much process noise grows with set size.
        - age_unc_gain: age modulation; younger: beta *= (1+age_unc_gain), sigma_obs *= (1-age_unc_gain);
                        older:  beta /= (1+age_unc_gain), sigma_obs *= (1+age_unc_gain).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta_proc, beta, sigma_obs, ss_scale, age_unc_gain = model_parameters
    nA = 3
    eps = 1e-12
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # Initialize Gaussian beliefs
        mu = np.zeros((nS, nA))
        var = np.ones((nS, nA))  # start with unit variance (uncertain)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r_raw = block_rewards[t]
            r_valid = (r_raw == 0) or (r_raw == 1)
            r = 1.0 if r_raw == 1 else 0.0
            set_size = int(block_set_sizes[t])

            # Age-adjusted choice temperature and observation noise
            if younger == 1:
                beta_eff = beta * (1.0 + age_unc_gain)
                sigma_eff = max(1e-6, sigma_obs * (1.0 - age_unc_gain))
            else:
                beta_eff = beta / (1.0 + age_unc_gain)
                sigma_eff = sigma_obs * (1.0 + age_unc_gain)

            # Choice policy: uncertainty-normalized softmax
            denom = np.sqrt(np.maximum(var[s, :], 1e-8))
            logits = beta_eff * (mu[s, :] / denom)
            logits = logits - np.max(logits)
            pi = np.exp(logits)
            pi = pi / np.sum(pi)

            if a < 0 or a >= nA:
                total_log_p += np.log(max(1.0 / nA, eps))
            else:
                total_log_p += np.log(max(pi[a], eps))

            # Time update (process noise) for uncertainty (applies every step)
            q = (eta_proc ** 2) * (1.0 + ss_scale * max(0, set_size - 3))
            var += q  # increase uncertainty globally before measurement update

            # Measurement update only if valid action and reward
            if (a >= 0 and a < nA) and r_valid:
                s_a_var = var[s, a]
                # Kalman gain
                K = s_a_var / (s_a_var + sigma_eff ** 2)
                # Update mean and variance
                mu[s, a] = mu[s, a] + K * (r - mu[s, a])
                var[s, a] = (1.0 - K) * s_a_var
            # If invalid reward or action, skip learning update (only process noise was applied)

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual learning rates by set size with decay and age-modulated epsilon-softmax exploration.

    Idea:
    - Model-free Q-learning with separate learning rates for small vs large set sizes.
    - Values decay over time (forgetting), with decay applied at each trial to the active state.
    - Action selection is an epsilon-softmax mixture:
        policy = (1-eps)*softmax(beta*Q) + eps*uniform.
      Epsilon increases with set size (harder memory load).
    - Age modulates exploration: older => larger epsilon; younger => smaller epsilon.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action per trial (0,1,2). Invalid actions treated as random; no learning.
    rewards : array-like of int or float
        Feedback 0/1. Invalid rewards skip learning.
    blocks : array-like of int
        Block index per trial. Q-values reset per block.
    set_sizes : array-like of int
        Set size (3 or 6), used to select the learning rate and scale exploration/decay.
    age : array-like of int or float
        Participant age; younger (<45) vs older (>=45) alters exploration propensity.
    model_parameters : tuple/list of floats
        (alpha_small, alpha_large, beta, decay, age_eps_gain)
        - alpha_small: learning rate when set size is small (3).
        - alpha_large: learning rate when set size is large (6).
        - beta: inverse temperature for softmax.
        - decay: forgetting factor applied to Q(s, :) at each visit (0..1), larger => more forgetting.
        - age_eps_gain: controls age- and load-driven exploration (epsilon).
          Epsilon per trial is:
             eps_t = sigmoid( g_age*age_eps_gain + (set_size-3)*age_eps_gain + 2*(decay-0.5) ),
          where g_age = +1 for older, -1 for younger. eps_t is clipped to [0, 0.5].

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_small, alpha_large, beta, decay, age_eps_gain = model_parameters
    nA = 3
    eps = 1e-12
    age_val = age[0]
    older = 1 if age_val >= 45 else 0
    g_age = 1 if older == 1 else -1

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r_raw = block_rewards[t]
            r_valid = (r_raw == 0) or (r_raw == 1)
            r = 1.0 if r_raw == 1 else 0.0
            set_size = int(block_set_sizes[t])

            # Per-visit decay of the active state (forgetting)
            decay_eff = max(0.0, min(1.0, decay / (1.0 + 0.5 * max(0, set_size - 3))))
            Q[s, :] = (1.0 - decay_eff) * Q[s, :]

            # Epsilon-softmax policy with age and set-size modulation
            # eps_t increases with set size and for older participants; also linked to decay
            mix_arg = g_age * age_eps_gain + (set_size - 3) * age_eps_gain + 2.0 * (decay - 0.5)
            eps_t = 1.0 / (1.0 + np.exp(-mix_arg))
            eps_t = max(0.0, min(0.5, eps_t))  # cap exploration

            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            pi_soft = np.exp(logits)
            pi_soft = pi_soft / np.sum(pi_soft)
            pi = (1.0 - eps_t) * pi_soft + eps_t * (np.ones(nA) / nA)

            if a < 0 or a >= nA:
                total_log_p += np.log(max(1.0 / nA, eps))
            else:
                total_log_p += np.log(max(pi[a], eps))

            # Learning update if valid
            if (a >= 0 and a < nA) and r_valid:
                alpha_eff = alpha_small if set_size <= 3 else alpha_large
                delta = r - Q[s, a]
                Q[s, a] += alpha_eff * delta
            # else: no update on invalid reward/action

    return -float(total_log_p)