def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + confidence- and load-gated WM with decay-to-baseline.

    Idea:
    - RL: standard delta learning.
    - WM stores near-deterministic associations, but its contribution is:
        (a) gated by within-state WM confidence (max probability in w[s,:])
        (b) down-weighted by set size (load).
    - WM representations decay toward a uniform baseline when not strongly reinforced.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline mixture weight for WM (0..1) before gating.
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - wm_leak: WM decay-to-baseline rate per trial for the visited state (0..1).
    - wm_conf_threshold: Threshold for WM confidence gating; higher means stricter requirement for WM dominance.
    - wm_load_sensitivity: How strongly WM is down-weighted by set size; 0=no load effect, higher=stronger down-weighting.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_leak, wm_conf_threshold, wm_load_sensitivity = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM scaling (smaller for larger sets)
        load_scale = 1.0 / (1.0 + wm_load_sensitivity * max(0.0, (float(nS) - 3.0) / 3.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM weights)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Confidence gating: use WM more when its within-state map is peaked
            conf = float(np.max(W_s))
            conf_gate = 1.0 / (1.0 + np.exp(-(conf - wm_conf_threshold) * 10.0))  # sharp sigmoid around threshold

            wm_weight_eff = wm_weight_base * conf_gate * load_scale

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward baseline, then reinforce on reward
            # Decay toward uniform baseline
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # If rewarded, imprint the association strongly
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Use the complement of leak as "write" strength so that larger leak leaves less room for sharp imprint
                write = max(0.0, 1.0 - wm_leak)
                w[s, :] = (1.0 - write) * w[s, :] + write * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM Hebbian accumulator with load-based arbitration.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM acts as a fast Hebbian accumulator over choices, strengthened by reward and weakened by non-reward.
    - Arbitration weight for WM decreases with set size (load) via a logistic function.
    - WM policy is a softmax over its accumulator vector.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - wm_weight: Baseline WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - wm_step: Size of WM Hebbian update per trial (accumulator step size).
    - load_slope: Slope of the logistic load penalty; higher means stronger down-weighting as set size increases.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_step, load_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM accumulator can take positive/negative values; initialize near-uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-based arbitration: WM weight decreases with set size via logistic around 3 items
        load_gate = 1.0 / (1.0 + np.exp(load_slope * (float(nS) - 3.0)))
        wm_weight_eff_block = wm_weight * load_gate

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over accumulator)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM Hebbian accumulator update:
            # - If rewarded: strengthen the chosen action.
            # - If not rewarded: weaken the chosen action slightly, and nudge others up toward baseline.
            if r > 0.0:
                w[s, a] += wm_step
            else:
                w[s, a] -= wm_step
                # redistribute a small portion to non-chosen to maintain competition
                redist = wm_step / (nA - 1)
                for a2 in range(nA):
                    if a2 != a:
                        w[s, a2] += redist

            # Optional mild drift toward baseline to prevent runaway (implicitly handled by competition).
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + slot-like WM capacity and retrieval noise.

    Idea:
    - RL: delta learning plus within-state forgetting toward uniform (to capture increased uncertainty in large sets).
    - WM: slot-like capacity K within each block; only states in the most recently visited K are actively maintained.
    - WM retrieval is noisy and becomes noisier as set size grows (scaled by wm_retrieval_noise).
    - Arbitration: WM contributes only when the current state is within the maintained set; otherwise RL dominates.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Maximum mixture weight for WM when available (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - rl_forget: RL forgetting rate toward uniform for the visited state (0..1 per trial).
    - wm_capacity: Approximate number of states that WM can actively maintain (slots).
    - wm_retrieval_noise: Scales retrieval noise with set size; higher means more noise for larger sets.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, rl_forget, wm_capacity, wm_retrieval_noise = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Slot-like WM: track recency of visited states; only the last K are in active WM
        K = int(np.clip(np.round(wm_capacity), 0, nS))
        recency_list = []

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Maintain recency list
            if s in recency_list:
                recency_list.remove(s)
            recency_list.insert(0, s)
            if len(recency_list) > K:
                recency_list = recency_list[:K]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM availability and retrieval noise increase with set size
            in_wm = 1.0 if (K > 0 and s in recency_list) else 0.0
            noise = 1.0 - np.exp(-wm_retrieval_noise * float(nS))  # 0 at small nS, approaches 1 as nS grows
            W_s_noisy = (1.0 - noise) * W_s + noise * w_0[s, :]

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            wm_weight_eff = wm_weight_base * in_wm

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL forgetting toward uniform for the visited state, then TD update
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update:
            if r > 0.0:
                # On reward, write a sharp one-hot trace
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                # On no reward, decay toward uniform baseline (stronger when retrieval is noisy)
                decay = 0.3 + 0.7 * noise  # between 0.3 and 1.0 depending on set size
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p