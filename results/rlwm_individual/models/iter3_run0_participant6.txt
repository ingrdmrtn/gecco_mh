def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise-weighted arbitration and load shift.

    Idea:
    - Choices are a mixture of model-free RL and WM.
    - WM influence is modulated by: (a) a load-dependent bias and (b) trial-wise RL surprise (unsigned PE).
    - WM traces decay toward uniform each trial; rewarded trials write a precise one-hot trace whose sharpness is
      controlled by wm_precision.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight_base: baseline WM mixture weight (mapped through a sigmoid internally).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_precision: sharpness of the WM write and sensitivity to surprise (higher = crisper WM + stronger surprise gain).
    - load_shift: linear shift of WM weight with set size (positive -> more WM under low load).
    - wm_decay: WM decay rate toward uniform (0..1) applied each trial.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, wm_precision, load_shift, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over WM weights; allow precision scaling via wm_precision
            p_wm = 1 / np.sum(np.exp((softmax_beta_wm * (1 + 0.5*(wm_precision-1)))*(W_s - W_s[a])))

            # Surprise-weighted arbitration with load bias
            # Base WM weight passed through sigmoid; add load shift favoring small set sizes
            base = wm_weight_base + load_shift * (max(0, 3.0 - (nS - 3)))  # increases when nS close to 3
            wm_w = 1 / (1 + np.exp(-base))
            # Compute unsigned PE from current RL estimates (before update)
            pe = abs(r - Q_s[a])
            wm_w = np.clip(wm_w + (wm_precision - 1) * 0.25 * pe, 0.0, 1.0)

            p_total = p_wm*wm_w + (1-wm_w)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0
            # Rewarded trials write a sharp one-hot memory; precision controls sharpness via convex mix
            if r > 0:
                sharp = np.zeros(nA)
                sharp[a] = 1.0
                w[s, :] = (1 - min(max(wm_precision - 0.5, 0.0), 1.0)) * w_0[s, :] + min(max(wm_precision - 0.5, 0.0), 1.0) * sharp

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with interference and refresh.

    Idea:
    - RL runs continuously.
    - WM stores up to K state-action associations with a strength that decays and suffers load-dependent interference.
    - Rewarded associations are stored (possibly evicting the weakest if at capacity).
    - Accessing a state refreshes its WM strength.
    - WM policy draws from the stored one-hot trace; if the state is not stored (strength ~ 0), WM offers uniform.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_capacity: number of WM slots (K) in [0, nS].
    - wm_refresh: refresh increment added to the strength of the accessed state on each trial (0..1).
    - interference_load_slope: scales global interference as set size increases (>=0).
      Higher values reduce WM strengths more in larger set sizes.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_capacity, wm_refresh, interference_load_slope = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # WM strength per state (0..1), 0 means not stored
        strengths = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Build effective WM vector from stored strength
            st = strengths[s]
            W_s = (1 - st) * w_0[s, :] + st * w[s, :]

            Q_s = q[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight increases with strength of WM for this state; also reduced by load via interference
            load_interf = 1.0 / (1.0 + interference_load_slope * max(0, nS - 3))
            wm_w = np.clip(st * load_interf, 0.0, 1.0)

            p_total = p_wm*wm_w + (1-wm_w)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Interference/decay on all strengths due to load
            strengths = np.clip(strengths * load_interf, 0.0, 1.0)

            # Refresh the accessed state's strength
            strengths[s] = np.clip(strengths[s] + wm_refresh*(1 - strengths[s]), 0.0, 1.0)

            # On reward, write a one-hot WM trace and ensure capacity
            if r > 0:
                # Write one-hot for this state
                w[s, :] = 0.0
                w[s, a] = 1.0
                # If not stored, add it; if over capacity, evict weakest other state
                if strengths[s] == 0:
                    strengths[s] = 0.5  # initialize mid strength
                # Enforce capacity by evicting weakest if needed
                active_idx = np.where(strengths > 0)[0]
                if len(active_idx) > wm_capacity:
                    # evict the smallest strength among states != s
                    candidates = [idx for idx in active_idx if idx != s]
                    if len(candidates) > 0:
                        weakest = candidates[np.argmin(strengths[candidates])]
                        strengths[weakest] = 0.0
                        w[weakest, :] = w_0[weakest, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent WM noise and a short-lived choice trace within WM.

    Idea:
    - RL contributes via softmax; WM contributes via a near-deterministic policy on a WM trace.
    - WM reliability degrades with set size (noisier WM distribution toward uniform).
    - A short-lived choice trace biases WM toward repeating the last chosen action at the current state
      (implemented within WM stream, not RL).
    - WM traces decay toward uniform each trial; rewarded trials overwrite the WM trace for that state.
    
    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: base WM mixture weight in [0,1].
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_noise: scales load-dependent corruption of WM (0..1); higher -> more noise under high load.
    - beta_load_gain: increases RL inverse temperature under higher load less; effective beta = beta*(1 + beta_load_gain/(nS))
    - choice_trace: magnitude (0..1) of a short-lived WM bias toward repeating the previous action at the same state.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_noise, beta_load_gain, choice_trace = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track last action per state for WM choice trace
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Load-dependent effective beta for RL (colder at high load if beta_load_gain is small)
            beta_eff = softmax_beta * (1.0 + beta_load_gain / max(1, nS))

            Q_s = q[s,:]
            # RL policy with load-modulated temperature
            p_rl = 1 / np.sum(np.exp(beta_eff*(Q_s-Q_s[a])))

            # WM distribution with load-dependent noise (mix with uniform)
            W_s = w[s,:].copy()
            noise_alpha = np.clip(wm_noise * max(0, nS - 3) / max(1, (6 - 3)), 0.0, 1.0)
            W_s = (1 - noise_alpha) * W_s + noise_alpha * w_0[s, :]

            # Add a short-lived choice trace within WM stream
            if last_action[s] >= 0:
                bias = np.zeros(nA)
                bias[last_action[s]] = choice_trace
                # renormalize after adding bias
                W_s = np.clip(W_s + bias, 1e-8, None)
                W_s = W_s / np.sum(W_s)

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Fixed WM mixture weight (could be expanded by load, but load already impacts WM via noise)
            wm_w = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm*wm_w + (1-wm_w)*p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM decay toward uniform
            w = (1 - 0.2) * w + 0.2 * w_0  # mild fixed decay to ensure wm_noise plays a role
            # On reward, write one-hot WM trace
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

            # Update last action trace
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p