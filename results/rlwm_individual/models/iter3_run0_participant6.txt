def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + capacity-limited episodic working memory (WM) with adaptive gating.

    Core idea
    - A model-free Q-learner learns state-action values via asymmetric learning rates.
    - A capacity-limited WM stores the last rewarded action per state with a strength.
    - Choice is a mixture of RL and WM policies, with a gating weight that increases
      when (a) set size is small, (b) the participant is younger, and (c) the trial
      is surprising (large |prediction error|). A small lapse mixes in uniform choice.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial.
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary feedback (0/1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for the block (3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha_pos, alpha_neg, beta, gate_gain, epsilon)
        - alpha_pos: learning rate for positive prediction errors (0..1).
        - alpha_neg: learning rate for negative prediction errors (0..1).
        - beta: inverse temperature for softmax (>0).
        - gate_gain: controls sensitivity of WM-vs-RL mixture weight to set size, age, and surprise.
        - epsilon: lapse probability mixed with uniform choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta, gate_gain, epsilon = model_parameters
    age_val = age[0]
    is_older = age_val >= 45

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM
        Q = (1.0 / nA) * np.ones((nS, nA))  # model-free values
        mem_action = -1 * np.ones(nS, dtype=int)  # WM stores the last rewarded action per state
        mem_strength = np.zeros(nS, dtype=float)  # strength of WM for that state (0..1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability for the chosen action
            prefs_rl = beta * Q[s, :]
            denom_rl = np.sum(np.exp(prefs_rl - prefs_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: if a memory exists, it strongly favors that action; else near-uniform
            if mem_action[s] >= 0 and mem_strength[s] > 1e-6:
                wm_vec = np.zeros(nA)
                wm_vec[mem_action[s]] = 1.0  # one-hot for stored action
                # scale preference by both beta and memory strength
                prefs_wm = (beta * (1.0 + mem_strength[s])) * wm_vec
                denom_wm = np.sum(np.exp(prefs_wm - prefs_wm[a]))
                p_wm = 1.0 / max(denom_wm, 1e-12)
            else:
                # no reliable memory => approximately uniform policy
                p_wm = 1.0 / nA

            # Prediction error
            pe = r - Q[s, a]
            # Mixture gating: favor WM when set size is small (3), participant is younger, and when |PE| is large
            size_term = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
            age_term = 1.0 if not is_older else -1.0
            surprise_term = np.abs(pe)  # 0..1
            gate_input = gate_gain * (size_term + age_term + surprise_term - 1.0)  # centered near 0
            w_wm = 1.0 / (1.0 + np.exp(-gate_input))
            w_wm = min(1.0, max(0.0, w_wm))

            # Final choice probability with lapse
            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_final = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            log_p += np.log(p_final)

            # RL update with asymmetric learning rates
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # WM update: store rewarded association; otherwise decay/forget more with load and age
            if r > 0.5:
                mem_action[s] = a
                # strengthen memory toward 1
                mem_strength[s] = 1.0 - (1.0 - mem_strength[s]) * (1.0 - 0.5 * (np.abs(pe)))  # faster when surprising
            else:
                # decay with interference: more decay if set size is large and/or participant is older
                base_decay = 0.10 + 0.15 * (float(nS) / 6.0) + (0.10 if is_older else 0.0)
                base_decay = min(0.9, max(0.0, base_decay))
                mem_strength[s] *= (1.0 - base_decay)
                # if strength gets tiny, clear memory
                if mem_strength[s] < 1e-4:
                    mem_action[s] = -1
                    mem_strength[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model-free RL with directed exploration bonus that adapts to set size and age.

    Core idea
    - Standard Q-learning for exploitation.
    - Directed exploration via a count-based bonus U(s,a) = xi_eff / sqrt(N(s,a)+1).
    - The effective exploration gain xi_eff increases when the set size is large (6) and/or the
      participant is older, capturing higher uncertainty and reliance on exploration under load/aging.
    - A small lapse mixes in uniform choice.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial.
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary feedback (0/1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for the block (3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha, beta, xi_base, k_age_size, epsilon)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax (>0).
        - xi_base: baseline directed exploration gain (>0).
        - k_age_size: modulation of exploration by set size and age.
                      Positive values increase exploration for larger sets and older age.
        - epsilon: lapse probability mixed with uniform choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, xi_base, k_age_size, epsilon = model_parameters
    age_val = age[0]
    is_older = age_val >= 45

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA), dtype=float)  # visit counts per state-action

        # Compute an exploration gain that depends on set size and age
        # Larger sets (nS=6) => lower 3/nS term; older => +1
        size_term = 1.0 - (3.0 / float(nS))  # 0.0 for 3-set, 0.5 for 6-set
        age_term = 1.0 if is_older else 0.0
        xi_eff = xi_base * (1.0 + k_age_size * (size_term + age_term))
        xi_eff = max(0.0, xi_eff)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Directed exploration bonus
            bonus = xi_eff / np.sqrt(N[s, :] + 1.0)

            prefs = beta * (Q[s, :] + bonus)
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_soft = 1.0 / max(denom, 1e-12)

            p_final = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            log_p += np.log(p_final)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Increment counts after choice
            N[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Confidence-weighted perseveration blended with RL, modulated by set size and age.

    Core idea
    - A model-free Q-learner tracks values.
    - A dynamic perseveration bias favors repeating the last action in a state, scaled by
      estimated state-action confidence that grows with rewards and shrinks with non-rewards.
    - The strength of perseveration is modulated by set size (weaker in large sets) and age
      (older show stronger perseveration), capturing working-memory constraints and strategy shifts.
    - Final policy is a softmax over (Q + bias) with a small lapse to uniform.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial.
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary feedback (0/1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for the block (3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha, beta, k_conf_pers, k_age_size, epsilon)
        - alpha: learning rate for Q updates (0..1).
        - beta: inverse temperature for softmax (>0).
        - k_conf_pers: base scaling from confidence to perseveration bias.
        - k_age_size: modulation of perseveration by age and set size.
                      Positive => more perseveration for older and/or larger sets.
        - epsilon: lapse probability mixed with uniform choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, k_conf_pers, k_age_size, epsilon = model_parameters
    age_val = age[0]
    is_older = age_val >= 45

    nA = 3
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state and confidence c[s,a] in [0,1]
        last_action = -1 * np.ones(nS, dtype=int)
        conf = np.zeros((nS, nA), dtype=float)

        # Compute perseveration gain factor given set size and age
        size_term = 1.0 - (3.0 / float(nS))  # 0 for 3-set, 0.5 for 6-set
        age_term = 1.0 if is_older else 0.0
        pers_gain = k_conf_pers * (1.0 + k_age_size * (size_term + age_term))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Build bias vector favoring last action in this state by its confidence
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                la = last_action[s]
                # Map confidence [0,1] to bias magnitude via pers_gain
                bias_mag = pers_gain * conf[s, la]
                bias[la] += bias_mag

            # Softmax over Q + bias for chosen action probability
            prefs = beta * (Q[s, :] + bias)
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_soft = 1.0 / max(denom, 1e-12)

            p_final = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            log_p += np.log(p_final)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update confidence: approach 1 with reward, 0 with no reward; slight forgetting with load
            conf_decay = 0.05 + 0.10 * (float(nS) / 6.0)  # more decay for larger sets
            conf[s, :] = (1.0 - conf_decay) * conf[s, :]
            if r > 0.5:
                conf[s, a] += (1.0 - conf[s, a]) * alpha
            else:
                conf[s, a] -= conf[s, a] * alpha

            # Older participants consolidate more on recent action (update last action more persistently)
            if r > 0.5 or is_older:
                last_action[s] = a
            else:
                # In younger participants without reward, occasionally reset last action in large sets
                if (not is_older) and (nS > 3) and (conf[s, a] < 0.1):
                    last_action[s] = -1

        blocks_log_p += log_p

    return -blocks_log_p