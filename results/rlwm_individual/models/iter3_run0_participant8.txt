def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL–limited-capacity WM with age- and set-size–dependent decay/arbitration.

    Mechanism
    - RL system: tabular Q-learning with softmax action selection.
    - WM system: stores the last rewarded action per state when capacity allows.
    - Arbitration: WM weight depends on (i) whether set size exceeds WM slots, and (ii) WM decay,
      which increases with set size and with age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). If out-of-range, treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Reward feedback (0/1 typical). If negative, treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        lr          : RL learning rate (0..1).
        beta        : Inverse temperature for RL softmax (>0).
        wm_slots    : WM capacity in items (>=0, can be fractional; higher -> more WM coverage).
        decay_base  : Baseline WM decay rate per trial (>=0).
        age_slope   : Additional WM decay per decade from 45 (can be +/-; used to modulate age effect).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    lr, beta, wm_slots, decay_base, age_slope = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    # Age-adjusted decay increment per trial: scale by decades from 45, bounded to reasonable range
    age_decay = age_slope * ((age_val - 45.0) / 10.0)

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        # RL values
        Q = np.zeros((nS, nA))
        # WM store: -1 means empty; otherwise holds the last rewarded action
        wm_store = -1 * np.ones(nS, dtype=int)
        # WM reliability (1 means confident, 0 means empty/forgotten)
        wm_strength = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Lapses or invalid indices -> uniform likelihood and skip learning
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # RL policy
            m = np.max(beta * Q[s, :])
            expv = np.exp(beta * Q[s, :] - m)
            p_rl = expv / np.sum(expv)

            # WM policy for this state
            if wm_store[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[wm_store[s]] = 1.0
                wm_rel = np.clip(wm_strength[s], 0.0, 1.0)
            else:
                p_wm = np.ones(nA) / nA
                wm_rel = 0.0

            # Capacity penalty: if set size exceeds slots, reduce WM influence
            cap_frac = np.clip(wm_slots / max(ss, 1.0), 0.0, 1.0)

            # Trialwise decay that increases with set size and age
            decay = np.clip(decay_base + age_decay + 0.1 * (ss - 3.0) * (1.0 + 0.5 * is_older), 0.0, 1.0)
            # Apply decay to all WM strengths each trial to reflect maintenance cost under load
            wm_strength *= (1.0 - decay)

            # Arbitration weight: combine capacity and current reliability
            wm_weight = np.clip(cap_frac * wm_rel, 0.0, 1.0)

            # Mixture policy
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning updates
            # RL TD update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # WM update: store last rewarded action with refreshed strength proportional to outcome
            if r > 0.0:
                wm_store[s] = a
                # Strength saturates toward 1, larger boost when set size is small
                boost = np.clip(0.7 - 0.05 * (ss - 3.0) - 0.1 * is_older, 0.1, 0.9)
                wm_strength[s] = np.clip(wm_strength[s] + boost, 0.0, 1.0)
            else:
                # Negative outcome weakens WM confidence if it mismatched
                if wm_store[s] == a:
                    wm_strength[s] *= (1.0 - 0.5)  # halve confidence if the remembered action failed

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian WM belief with hazard-driven forgetting, mixed with RL. Age and set size modulate hazard.

    Mechanism
    - WM belief: For each state, maintain a categorical belief over 3 actions (Dirichlet counts).
      After positive feedback, belief concentrates on the chosen action; after zero reward,
      belief shifts away from the chosen action. A hazard rate causes belief to revert toward uniform.
    - RL backup: tabular Q-learning with softmax.
    - Arbitration: WM weight = confidence (max probability minus uniform baseline).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Observed action (0..2). Out-of-range -> lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). If negative -> lapse (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of float
        Single-element array with participant age. Older group: age > 45.
    model_parameters : iterable of 5 floats
        lr        : RL learning rate (0..1).
        beta      : Inverse temperature for RL softmax (>0).
        h0        : Baseline WM hazard rate in logit space (can be any real).
        age_eff   : Additive shift on hazard for older adults (>=0 typically).
        set_eff   : Additive hazard increase per extra item beyond 3 (>=0 typically).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood.
    """
    lr, beta, h0, age_eff, set_eff = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = np.zeros((nS, nA))
        # WM Dirichlet counts for each state; start uninformative (1,1,1)
        counts = np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials -> uniform likelihood, skip learning
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Compute hazard for this trial (probability of reverting counts toward uniform)
            # hazard = sigmoid(h0 + age term + set size term)
            logit_h = h0 + age_eff * is_older + set_eff * (ss - 3.0)
            hazard = 1.0 / (1.0 + np.exp(-logit_h))
            hazard = np.clip(hazard, 0.0, 1.0)

            # Apply hazard-driven leak toward uniform prior for all states
            # counts <- (1-h) * counts + h * 1 (since prior is ones)
            counts = (1.0 - hazard) * counts + hazard * 1.0

            # WM policy: mean of Dirichlet
            wm_prob = counts[s, :] / np.sum(counts[s, :])

            # RL policy
            m = np.max(beta * Q[s, :])
            expv = np.exp(beta * Q[s, :] - m)
            p_rl = expv / np.sum(expv)

            # Arbitration: WM confidence measured as (max prob - 1/nA), clipped [0,1]
            conf = float(np.max(wm_prob) - (1.0 / nA))
            wm_weight = np.clip(3.0 * conf, 0.0, 1.0)  # scale up confidence to weight

            p_mix = wm_weight * wm_prob + (1.0 - wm_weight) * p_rl
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning updates
            # RL update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # WM counts update: reinforce rewarded action; if r=0, shift away from chosen
            if r > 0.0:
                counts[s, a] += 1.0
                # Optionally suppress alternatives mildly to sharpen belief
                others = [x for x in range(nA) if x != a]
                counts[s, others] = np.maximum(1.0, counts[s, others] - 0.25)
            else:
                # Negative/zero reward: reduce probability mass on chosen, increase alternatives
                counts[s, a] = max(1.0, counts[s, a] - 0.5)
                others = [x for x in range(nA) if x != a]
                counts[s, others] += 0.25

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Adaptive-temperature Q-learning with age- and set-size–dependent lapse and forgetting.

    Mechanism
    - RL: tabular Q-learning.
    - Softmax temperature adapts to cognitive load: higher set size and older age increase temperature
      (i.e., reduce inverse temperature), making choices noisier.
    - Lapse probability increases with age and set size (random action with uniform prob).
    - Q forgetting (value decay toward zero) scales with set size and age.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Observed action (0..2). If invalid, uniform likelihood and no learning.
    rewards : array-like of float
        Feedback (0/1 typical). If negative, uniform likelihood and no learning.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        lr         : Learning rate for Q updates (0..1).
        beta0      : Baseline inverse temperature (>0).
        temp_sens  : Sensitivity reducing inverse temperature per extra item and for older age (>=0).
        lapse0     : Baseline lapse logit (can be any real).
        age_lapse  : Additional lapse logit for older adults (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood.
    """
    lr, beta0, temp_sens, lapse0, age_lapse = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Inverse temperature adapts to load and age: beta_eff <= beta0
            # beta_eff = beta0 / (1 + temp_sens * ((ss-3) + 0.8*is_older))
            denom = 1.0 + temp_sens * ((ss - 3.0) + 0.8 * is_older)
            beta_eff = beta0 / max(denom, 1e-6)

            # Lapse probability depends on age and set size (logistic)
            lapse_logit = lapse0 + age_lapse * is_older + 0.5 * (ss - 3.0)
            lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
            lapse = np.clip(lapse, 0.0, 0.5)  # cap lapse to avoid degeneracy

            # Softmax policy
            m = np.max(beta_eff * Q[s, :])
            expv = np.exp(beta_eff * Q[s, :] - m)
            pi = expv / np.sum(expv)

            # Mixture with lapse
            p_a = (1.0 - lapse) * pi[a] + lapse * (1.0 / nA)
            p_a = np.clip(p_a, 1e-12, 1.0)
            total_log_p += np.log(p_a)

            # Learning with forgetting toward 0 (decay stronger with load and age)
            # Apply small decay to the selected state's action-values before update
            decay_rate = np.clip(0.02 * (1.0 + 0.5 * (ss - 3.0) + 0.8 * is_older), 0.0, 0.5)
            Q[s, :] *= (1.0 - decay_rate)

            # Standard Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

    return -float(total_log_p)