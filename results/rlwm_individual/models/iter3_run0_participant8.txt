def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-gated WM with set-size interference and graded WM learning.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: graded associative memory over actions per state, read out with a near-deterministic softmax.
    - The WM mixture weight is higher when:
        (a) set size is small (below a midpoint), and
        (b) the RL policy is uncertain (high entropy).
      This implements gating of WM by RL uncertainty and set-size capacity pressure.
    - WM suffers set-size-dependent interference (blend toward uniform as set size grows).
    - WM updates gradually toward the rewarded action when reward is delivered,
      and decays slightly toward uniform after non-reward.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_slope : float
            Slope for the sigmoidal dependence of WM weight on set size.
        wm_midpoint : float
            Midpoint set size for WM weight sigmoid (e.g., ~4.5 gives higher weight for nS < 4.5).
        interference : float
            Amplitude (0-1) of set-size-dependent WM interference toward uniform.
        wm_lr : float
            WM learning rate toward a one-hot mapping when reward is received.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, wm_slope, wm_midpoint, interference, wm_lr = model_parameters
    softmax_beta *= 10  # RL beta scaled
    softmax_beta_wm = 50  # near-deterministic WM readout

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL softmax distribution to get entropy-based uncertainty
            z = Q_s - np.max(Q_s)
            expz = np.exp(softmax_beta * z)
            p_soft = expz / np.sum(expz)
            # Normalized entropy (0=confident, 1=max uncertain)
            H = -np.sum(p_soft * (np.log(p_soft + 1e-12)))
            Hnorm = H / np.log(nA)
            rl_uncertainty = Hnorm  # gate WM more when RL is uncertain

            # Set-size factor for WM weight: sigmoid(wm_slope * (wm_midpoint - nS))
            ss_factor = 1.0 / (1.0 + np.exp(-wm_slope * (wm_midpoint - float(nS))))
            wm_weight_eff = np.clip(ss_factor * rl_uncertainty, 0.0, 1.0)

            # Set-size-dependent WM interference: blend W_s toward uniform
            inter_eff = np.clip(interference * (float(nS) / 6.0), 0.0, 0.99)
            W_eff = (1.0 - inter_eff) * W_s + inter_eff * (1.0 / nA)

            # WM policy for chosen action after interference
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_wm = p_wm_core

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: rewarded trials pull W toward one-hot; non-reward decays toward uniform
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * W_s + wm_lr * one_hot
            else:
                # gentle decay to uniform after errors
                w[s, :] = 0.9 * W_s + 0.1 * w_0[s, :]
            # Renormalize to be safe
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + slot-like WM capacity with deterministic recall and global decay.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: capacity-limited (K slots). Each stored state has a deterministic mapping after reward.
      Effective WM contribution scales with probability the state is stored: p_store = min(1, K/nS).
    - WM readout is near-deterministic; if not stored, WM yields uniform policy.
    - WM mixture weight is wm_use * p_store (state-independent within block).
    - WM traces for a state decay toward uniform after non-reward with rate `decay`.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        K : float
            WM capacity in number of items; effective WM weight scales with min(1, K/nS).
        wm_use : float
            Base WM mixture weight (0-1), multiplied by p_store.
        decay : float
            WM decay rate toward uniform after non-reward (0-1).
        epsilon : float
            WM lapse probability mixing in uniform choice in the WM policy (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, K, wm_use, decay, epsilon = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # deterministic WM for stored items

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Probability that a state is stored under capacity K
        p_store = min(1.0, float(K) / max(1.0, float(nS)))
        wm_weight_eff = np.clip(wm_use * p_store, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM readout: if stored, near-deterministic from W_s; else uniform.
            # We approximate storage by using p_store at the policy level.
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_stored = p_wm_core
            p_wm_unstored = 1.0 / nA
            p_wm_nomix = p_store * p_wm_stored + (1.0 - p_store) * p_wm_unstored
            # WM lapse epsilon
            p_wm = (1.0 - epsilon) * p_wm_nomix + epsilon * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - On reward: set a deterministic mapping (store this item).
            # - On non-reward: decay toward uniform (forgetting).
            if r > 0:
                w[s, :] = 1e-8
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]
                w[s, :] = np.clip(w[s, :], 1e-8, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + error-driven WM recruitment with set-size-dependent forgetting and WM precision.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: associative mapping read out via strong softmax, but its influence is recruited
      more on surprising (high |prediction error|) trials and less on predictable ones.
    - WM recruitment also down-weights with larger set sizes.
    - WM precision (effective determinism) degrades with set size via mixing with uniform.
    - WM updates:
        * Reward: move W toward one-hot of the chosen action, with forgetting competing.
        * No reward: decay toward uniform.

    Parameters
    ----------
    model_parameters : tuple/list with 6 entries
        lr : float
            RL learning rate (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_gain : float
            Maximum WM mixture gain (0-1).
        wm_forget_base : float
            Base WM forgetting rate that increases with set size (>=0).
        err_slope : float
            Slope controlling sensitivity of WM recruitment to absolute RL prediction error.
        wm_beta_base : float
            Base precision scaling for WM policy; lower values imply noisier WM (0-1, used as mixing toward uniform).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, softmax_beta, wm_gain, wm_forget_base, err_slope, wm_beta_base = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # base WM determinism used before precision mixing

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL prediction error BEFORE updating Q for error-driven recruitment
            pe = r - Q_s[a]
            abs_pe = abs(pe)
            # Error-driven WM recruitment (sigmoid of |PE|), down-weighted by set size
            recruit = 1.0 / (1.0 + np.exp(-err_slope * (abs_pe - 0.5)))
            ss_down = 3.0 / max(3.0, float(nS))  # 1.0 for nS=3, 0.5 for nS=6
            wm_weight_eff = np.clip(wm_gain * recruit * ss_down, 0.0, 1.0)

            # WM readout with set-size-dependent precision mixing
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Precision degrades with set size: increase uniform mixing as nS grows
            precision = np.clip(wm_beta_base * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
            p_wm = precision * p_wm_core + (1.0 - precision) * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating with set-size-dependent forgetting
            # Forgetting rate increases with set size
            forget = 1.0 - np.exp(-wm_forget_base * (float(nS) / 3.0))
            forget = np.clip(forget, 0.0, 1.0)

            if r > 0:
                # Move toward one-hot, but also leak toward uniform via forgetting
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w_prop = 0.85 * W_s + 0.15 * one_hot  # consolidate the rewarded mapping
                w[s, :] = (1.0 - forget) * w_prop + forget * w_0[s, :]
            else:
                # No reward: decay toward uniform
                w[s, :] = (1.0 - forget) * W_s + forget * w_0[s, :]

            # Normalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p