def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with state-wise action stickiness and load-dependent decay; stickiness amplified by age.

    Mechanism
    - Tabular Q-learning with learning rate alpha and softmax temperature beta.
    - Action stickiness within state: adds a positive bias to the previously chosen action's logit.
      Stickiness magnitude increases for older adults.
    - Load-dependent value decay: Q-values decay toward a uniform prior at a rate that increases
      with set size (reflecting higher load). This introduces forgetting that is stronger in large sets.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6). Determines the active state's cardinality.
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45)=0, older (>=45)=1.
    model_parameters : sequence of 5 floats
        [alpha, beta, kappa_base, age_stick_gain, decay_load]
        - alpha: Q-learning rate (sigmoid-mapped to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - kappa_base: baseline stickiness strength added to the previous action's logit (>=0).
        - age_stick_gain: multiplicative gain on stickiness for older adults (>=0).
        - decay_load: load-to-decay scaling factor; transformed by sigmoid to [0,1] and
                       scaled by relative set size to yield a per-trial decay rate.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, kappa_base, age_stick_gain, decay_load = model_parameters

    # Parameter transforms
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(1e-6, beta)
    kappa_base = np.maximum(0.0, kappa_base)
    age_stick_gain = np.maximum(0.0, age_stick_gain)
    decay_load = 1.0 / (1.0 + np.exp(-decay_load))  # base factor in [0,1]

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action in each state for stickiness
        last_act = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Load-dependent decay toward uniform prior
            # Scale by how much load exceeds the low-load baseline (3)
            load_scale = max(0.0, (float(nS_t) - 3.0) / 3.0)  # 0 for 3, 1 for 6
            d = decay_load * load_scale
            if d > 0:
                Q[s, :] = (1.0 - d) * Q[s, :] + d * (1.0 / nA)

            # Softmax with action stickiness bias on logits
            logits = beta * Q[s, :].copy()
            kappa = kappa_base * (1.0 + age_group * age_stick_gain)
            if last_act[s] >= 0:
                logits[last_act[s]] += kappa

            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            policy = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            p_a = np.clip(policy[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update stickiness memory
            last_act[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working-memory cache (reward-tagged), combined via precision-weighted logits.
    WM capacity depends on age group.

    Mechanism
    - RL: tabular Q-learning (alpha) with softmax (beta).
    - WM cache: stores the most recently rewarded action for up to K states in a block.
      If a state's last outcome was rewarded, WM provides a sharp preference for that action.
      Otherwise, WM is uniform.
    - Capacity K depends on age group (K_young vs K_old). If a new state is encountered and
      capacity is full, it is not added to WM (simple first-K policy).
    - Policy integration: combine RL and WM by summing precision-weighted logits:
        logits_total = beta * Q[s,:] + wm_beta * one_hot(last_rewarded_action)
      When WM not available for state s, the second term is zero.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45)=0, older (>=45)=1.
    model_parameters : sequence of 5 floats
        [alpha, beta, K_young, K_old, wm_beta]
        - alpha: RL learning rate (sigmoid to [0,1]).
        - beta: Inverse temperature for RL values (>=0).
        - K_young: WM capacity (number of states) in younger adults (>=0, clipped to [0, nS]).
        - K_old: WM capacity in older adults (>=0, clipped to [0, nS]).
        - wm_beta: precision/strength of WM logits when a rewarded action is cached (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, K_y, K_o, wm_beta = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(1e-6, beta)
    wm_beta = np.maximum(0.0, wm_beta)
    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Determine capacity for this participant
        K = K_o if age_group == 1 else K_y
        # Clip K to feasible range for this block's set size
        K_eff = int(np.clip(np.round(K), 0, nS))

        # WM data structures
        in_wm = np.zeros(nS, dtype=bool)  # whether state is stored in WM
        wm_count = 0
        last_rewarded_action = -np.ones(nS, dtype=int)  # -1 means no rewarded action cached

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # If first time seeing this state in the block, try to add to WM if capacity allows
            if not in_wm[s] and wm_count < K_eff:
                in_wm[s] = True
                wm_count += 1

            # RL logits
            logits = beta * Q[s, :].copy()

            # WM logits: only if state is in WM and last outcome was a reward and we know which action
            wm_logits = np.zeros(nA)
            if in_wm[s] and last_rewarded_action[s] >= 0:
                wm_logits[last_rewarded_action[s]] = wm_beta

            logits_total = logits + wm_logits
            logits_total -= np.max(logits_total)
            exp_logits = np.exp(logits_total)
            policy = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            p_a = np.clip(policy[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WM cache: store last rewarded action if received reward; if no reward, do not overwrite
            if r >= 0.5:
                last_rewarded_action[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and age-by-load lapse.

    Mechanism
    - RL: tabular Q-learning with separate learning rates for positive and negative RPEs.
    - Softmax choice with temperature beta.
    - Lapse (epsilon) mixture: with probability epsilon, choose uniformly at random (3 actions);
      otherwise choose according to RL softmax. Epsilon increases with set size (load)
      and is further amplified in older adults (age-by-load effect).

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0]. Age group: younger (<45)=0, older (>=45)=1.
    model_parameters : sequence of 5 floats
        [alpha_pos, alpha_neg, beta, lapse_base, age_load_gain]
        - alpha_pos: learning rate for positive prediction errors (sigmoid to [0,1]).
        - alpha_neg: learning rate for negative prediction errors (sigmoid to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - lapse_base: baseline log-odds of lapse vs. no lapse (free real).
        - age_load_gain: multiplicative gain applied to the load term for older adults (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha_pos, alpha_neg, beta, lapse_base, age_load_gain = model_parameters

    alpha_pos = 1.0 / (1.0 + np.exp(-alpha_pos))
    alpha_neg = 1.0 / (1.0 + np.exp(-alpha_neg))
    beta = np.maximum(1e-6, beta)
    age_load_gain = np.maximum(0.0, age_load_gain)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL softmax policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            p_rl = exp_logits / np.maximum(1e-12, np.sum(exp_logits))

            # Lapse probability: logistic of baseline + load + age*load*gain
            load = max(0.0, (float(nS_t) - 3.0) / 3.0)  # 0 for 3, 1 for 6
            lap_logit = lapse_base + load * (1.0 + age_group * age_load_gain)
            epsilon = 1.0 / (1.0 + np.exp(-lap_logit))
            epsilon = np.clip(epsilon, 0.0, 1.0)

            p_total = (1.0 - epsilon) * p_rl + epsilon * (1.0 / nA)
            p_a = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Asymmetric Q-learning update
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

    return nll