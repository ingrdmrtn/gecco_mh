def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Load- and age-dependent RL with Q-value forgetting and load-specific choice temperature.

    Mechanism:
    - Tabular Q-learning with per-trial, state-specific decay (forgetting) toward zero.
    - Decay magnitude increases with set size (higher load) and with age, via a logistic mapping.
    - Choice noise (inverse temperature) differs between low (3) and high (6) load blocks.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 within block).
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning resets per block.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of int or float
        Participant age; age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta_low, beta_high, forget_base, age_forget_bias]
          - alpha: RL learning rate (0..1).
          - beta_low: inverse temperature used when set size = 3.
          - beta_high: inverse temperature used when set size = 6.
          - forget_base: baseline forgetting logit controlling average decay per visit.
          - age_forget_bias: additive bias to forgetting logit applied as +bias if older, -bias if younger.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta_low, beta_high, forget_base, age_forget_bias = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))  # initialized to zero value

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            beta = beta_low if nS_t == 3 else beta_high

            # Softmax policy from Q
            Q_s = Q[s, :]
            Q_s_shift = Q_s - np.max(Q_s)
            expQ = np.exp(beta * Q_s_shift)
            p_vec = expQ / np.maximum(np.sum(expQ), eps)
            p = np.maximum(p_vec[a], eps)
            nll -= np.log(p)

            # Q forgetting (state-specific, per visit), logistic mapping
            # decay in [0,1): higher for larger set size and older age
            load_term = (nS_t - 3) / 3.0  # 0 for 3, 1 for 6
            age_term = age_forget_bias if is_older else -age_forget_bias
            decay_logit = forget_base + load_term + age_term
            decay = 1.0 / (1.0 + np.exp(-decay_logit))  # sigmoid
            # Apply decay toward zero
            Q[s, :] = (1.0 - decay) * Q[s, :]

            # Q update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-dependent lapse (state-inference failures).

    Mechanism:
    - Standard Q-learning controls goal-directed choices via softmax.
    - On each trial, with probability lapse_t, the participant lapses and responds randomly.
      Lapse probability increases with set size and is biased by age group.
      This can be interpreted as occasional state/attention failures under higher load or aging.
    - All parameters influence behavior on every trial.

    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning resets per block.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of int or float
        Participant age; age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, lapse_base, load_lapse_gain, age_lapse_shift]
          - alpha: learning rate (0..1)
          - beta: inverse temperature (>0)
          - lapse_base: baseline lapse logit
          - load_lapse_gain: multiplicative gain on load term in lapse logit
          - age_lapse_shift: additive shift to lapse logit (+ for older, - for younger)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, lapse_base, load_lapse_gain, age_lapse_shift = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL softmax choice
            Q_s = Q[s, :]
            Q_s_shift = Q_s - np.max(Q_s)
            expQ = np.exp(beta * Q_s_shift)
            p_rl_vec = expQ / np.maximum(np.sum(expQ), eps)
            p_rl = p_rl_vec[a]

            # Lapse probability depends on load and age via logistic function
            load_term = (nS_t - 3) / 3.0  # 0 for 3, 1 for 6
            age_term = age_lapse_shift if is_older else -age_lapse_shift
            lapse_logit = lapse_base + load_lapse_gain * load_term + age_term
            lapse_t = 1.0 / (1.0 + np.exp(-lapse_logit))  # in (0,1)

            # Mixture with random choice under lapse
            p = (1.0 - lapse_t) * p_rl + lapse_t * (1.0 / nA)
            p = np.maximum(p, eps)
            nll -= np.log(p)

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with capacity-limited recall boost: age- and load-dependent WM capacity enhances RL policy.

    Mechanism:
    - Baseline tabular RL controls learning.
    - A sparse working memory (WM) stores the last rewarded action for a limited number of states (capacity K).
    - When the current state is in WM, a recall bonus is added to that action's value before softmax.
    - Capacity K scales with age group (higher for younger, lower for older) and shrinks under higher load.

    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen action indices (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block indices; learning and WM reset per block.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like of int or float
        Participant age; age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, cap_base, age_cap_gain, recall_bonus]
          - alpha: learning rate (0..1)
          - beta: inverse temperature (>0)
          - cap_base: baseline WM capacity in "state slots" (positive, not necessarily integer)
          - age_cap_gain: multiplicative capacity factor (>0); applied as *gain for younger, /gain for older
          - recall_bonus: additive bonus added to the remembered action's value before softmax (>0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, cap_base, age_cap_gain, recall_bonus = model_parameters
    age_val = float(age[0]) if hasattr(age, "__len__") else float(age)
    is_older = age_val >= 45

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_set_sizes = set_sizes[mask]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        # WM store: mapping state -> action, with LRU order
        wm_map = {}
        wm_order = []  # list of states, oldest -> newest

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Capacity scaling: lower under higher load; age modulates capacity multiplicatively
            load_scale = 1.0 if nS_t == 3 else 0.5  # halve capacity at set size 6
            age_scale = (age_cap_gain if not is_older else (1.0 / max(age_cap_gain, eps)))
            K_eff = cap_base * load_scale * age_scale
            K_int = int(max(0, np.floor(K_eff + 1e-9)))  # effective integer capacity

            # Construct preference with WM recall bonus if applicable
            pref = Q[s, :].copy()
            if (s in wm_map) and (K_int > 0):
                a_mem = wm_map[s]
                pref[a_mem] += recall_bonus

            pref_shift = pref - np.max(pref)
            expP = np.exp(beta * pref_shift)
            p_vec = expP / np.maximum(np.sum(expP), eps)
            p = np.maximum(p_vec[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store last rewarded action; enforce capacity via LRU
            if r > 0.5:
                if s in wm_map:
                    wm_map[s] = a
                    # move s to end (most recent)
                    wm_order = [x for x in wm_order if x != s] + [s]
                else:
                    wm_map[s] = a
                    wm_order.append(s)
                # Enforce capacity limit
                while len(wm_order) > max(0, K_int):
                    oldest = wm_order.pop(0)
                    if oldest in wm_map:
                        del wm_map[oldest]

    return nll