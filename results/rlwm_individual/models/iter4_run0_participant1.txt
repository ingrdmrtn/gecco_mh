def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited chunked working memory (WM) with load-driven interference and age-dependent capacity.

    Concept
    - An RL system learns Q-values with a standard prediction-error rule.
    - A WM store caches stimulus-action associations when rewarded, but its effective coverage
      is limited by a small number of slots that depends on age (younger vs older).
    - Higher set size produces stronger interference/decay in WM, reducing its reliability.
    - Policy is a mixture of RL and WM, weighted by WM coverage and the confidence (max strength) in WM.
    - A small lapse probability mixes in uniform random choice.

    Parameters (model_parameters)
    - alpha_rl: RL learning rate in [0,1].
    - beta: inverse temperature for RL softmax (>0).
    - wm_slots_young: WM capacity (slots) if age<45 (>=0).
    - wm_slots_old: WM capacity (slots) if age>=45 (>=0).
    - interference: strength of WM interference/decay per unit load per trial (>=0).
    - lapse: lapse probability mixed with uniform choice in [0,0.2].

    Inputs
    - states: array of int, state index per trial (0..set_size-1 within block).
    - actions: array of int, chosen action per trial (0..2).
    - rewards: array of float/int, feedback per trial (0 or 1).
    - blocks: array of int, block index per trial.
    - set_sizes: array of int, set size for the trial's block (3 or 6).
    - age: array-like of length 1, participant age in years.
    - model_parameters: tuple/list of 6 floats as described above.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    alpha_rl, beta, wm_slots_young, wm_slots_old, interference, lapse = model_parameters

    nA = 3
    beta = max(beta, 1e-6) * 5.0
    lapse = np.clip(lapse, 0.0, 0.2)
    interference = max(0.0, interference)

    is_older = 1.0 if age[0] >= 45 else 0.0
    wm_slots = max(0.0, (1.0 - is_older) * wm_slots_young + is_older * wm_slots_old)

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM strengths per state-action, 0..1
        M = np.zeros((nS, nA))

        # For simple chunked mapping: when rewarded, store strong chunk for that action
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute load factor: 0 for 3, 1 for 6
            load = (max(nS, 3) - 3) / 3.0

            # RL policy
            logits = beta * Q[s, :]
            logits = logits - np.max(logits)
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)

            # WM recall
            # Decay/interference each trial scales with load
            if interference > 0.0 and load > 0.0:
                decay = np.clip(1.0 - interference * load, 0.0, 1.0)
                M *= decay

            # WM-based choice distribution derived from strengths
            if np.all(M[s, :] <= 1e-12):
                p_wm_vec = np.ones(nA) / nA
                wm_conf = 0.0
            else:
                # Normalize strengths to form a soft preference
                strengths = M[s, :].clip(0.0, 1.0)
                if np.sum(strengths) <= 1e-12:
                    p_wm_vec = np.ones(nA) / nA
                else:
                    p_wm_vec = strengths / np.sum(strengths)
                wm_conf = np.max(strengths)  # confidence in best cached action

            # WM coverage based on age-dependent slots and set size
            coverage = min(1.0, wm_slots / max(1.0, float(nS)))

            # Arbitration weight for WM = coverage * confidence
            w_wm = np.clip(coverage * wm_conf, 0.0, 1.0)

            # Mixture with lapse
            p_mix = (1.0 - lapse) * (w_wm * p_wm_vec + (1.0 - w_wm) * p_rl) + lapse * (1.0 / nA)
            p_choice = np.clip(p_mix[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            # RL update
            Q[s, a] += alpha_rl * (r - Q[s, a])

            # WM update: if rewarded, store a strong chunk for that state-action
            if r > 0.5:
                # strengthen chosen action and slightly suppress competitors
                M[s, a] = 1.0
                for a2 in range(nA):
                    if a2 != a:
                        # lateral inhibition proportional to interference and load
                        M[s, a2] = max(0.0, M[s, a2] * (1.0 - 0.5 * interference * load))

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with novelty bonus and simple recall, with age- and load-modulated exploration.

    Concept
    - Asymmetric RL updates (alpha_pos for positive PE, alpha_neg for negative PE).
    - Novelty/exploration bonus favors less-tried actions within a state.
    - A simple recall mechanism: if a state has a cached last rewarded action, with some probability
      (decaying with load and age), the model biases toward that action (direct access).
    - Age modulates both the effective inverse temperature and the novelty drive.
    - Set size reduces effective inverse temperature (more exploration under load).

    Parameters (model_parameters)
    - alpha_pos: learning rate for positive prediction errors in [0,1].
    - alpha_neg: learning rate for negative prediction errors in [0,1].
    - beta_base: base inverse temperature (>0).
    - novelty_bonus: scale of novelty bonus and recall propensity (>0).
    - age_explore_penalty: reduces inverse temperature and novelty when older (>=45) (>=0).
    - load_temp_penalty: reduces inverse temperature as set size increases (>=0).

    Inputs
    - states: array of int, state index per trial.
    - actions: array of int, chosen action per trial (0..2).
    - rewards: array of float/int, feedback (0/1).
    - blocks: array of int, block index per trial.
    - set_sizes: array of int, set size for each trial's block.
    - age: array-like of length 1, participant age in years.
    - model_parameters: tuple/list of 6 floats as described above.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_base, novelty_bonus, age_explore_penalty, load_temp_penalty = model_parameters

    nA = 3
    age_group = 1.0 if age[0] >= 45 else 0.0

    beta_base = max(beta_base, 1e-6) * 3.0
    novelty_bonus = max(novelty_bonus, 1e-6)
    age_explore_penalty = max(0.0, age_explore_penalty)
    load_temp_penalty = max(0.0, load_temp_penalty)

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Values and counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA))  # for novelty
        last_rewarded = -1 * np.ones(nS, dtype=int)  # last rewarded action per state; -1 if none

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            load = (max(nS, 3) - 3) / 3.0

            # Effective temperature decreases with age (older) and load penalty
            beta_eff = beta_base
            beta_eff *= np.exp(-load_temp_penalty * load)
            beta_eff *= np.exp(-age_group * age_explore_penalty)
            beta_eff = max(beta_eff, 1e-6) * 3.0

            # Novelty bonus for each action (smaller visits => larger bonus)
            # Age reduces novelty drive if older
            novelty_scale = novelty_bonus * (1.0 - 0.5 * age_group)  # younger preserve full novelty
            bon = novelty_scale / (1.0 + visits[s, :])

            # RL softmax with novelty
            logits = beta_eff * (Q[s, :] + bon)
            logits = logits - np.max(logits)
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)

            # Simple recall: bias to last rewarded action for this state
            # Probability of recall increases with novelty_scale but decays with load and age_explore_penalty
            if last_rewarded[s] >= 0:
                recall_base = 1.0 / (1.0 + np.exp(-novelty_scale))  # sigmoid of novelty scale
                recall_load = np.exp(-2.0 * load)  # more load, less recall
                recall_age = 1.0 / (1.0 + age_group * age_explore_penalty)  # older, less recall
                p_recall = np.clip(recall_base * recall_load * recall_age, 0.0, 1.0)
                p_recall_vec = np.ones(nA) / nA
                p_recall_vec[last_rewarded[s]] = max(1.0 - (nA - 1) * 1e-6, 1.0)  # near-point mass
                # mixture of recall and RL
                p_mix = p_recall * p_recall_vec + (1.0 - p_recall) * p_rl
            else:
                p_mix = p_rl

            p_choice = np.clip(p_mix[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            # Update counts and RL values
            visits[s, a] += 1.0

            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # Update last rewarded mapping
            if r > 0.5:
                last_rewarded[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Meta-controlled arbitration between RL and probabilistic WM, driven by volatility,
    with age-modulated arbitration inertia.

    Concept
    - RL system learns Q-values (prediction-error learning).
    - WM system learns a probabilistic association W[s,a] ~ P(reward|s,a) via delta rule (wm_learn).
    - A meta-controller maintains a WM weight w_t in each state that adapts to:
        - recent volatility in RL prediction errors (more volatility -> rely less on WM),
        - set size (larger load -> rely less on WM),
        - age bias (older -> lower WM reliance and more inertia).
      The weight is updated with inertia (low-pass dynamics), with inertia modulated by age.
    - Lapse mixes in uniform choice.

    Parameters (model_parameters)
    - alpha_rl: RL learning rate in [0,1].
    - beta: inverse temperature for RL softmax (>0).
    - wm_learn: learning rate for WM associative values in [0,1] (also used to update volatility).
    - arb_inertia: inertia parameter in [0,1]; higher -> slower changes in WM weight.
    - age_bias: additive bias shifting arbitration away from WM when older (>=45) (>=0).
    - lapse: lapse probability in [0,0.2].

    Inputs
    - states: array of int, state index per trial.
    - actions: array of int, chosen action per trial (0..2).
    - rewards: array of float/int, feedback (0/1).
    - blocks: array of int, block index per trial.
    - set_sizes: array of int, set size per trial's block (3 or 6).
    - age: array-like of length 1, participant age in years.
    - model_parameters: tuple/list of 6 floats as described above.

    Returns
    - Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta, wm_learn, arb_inertia, age_bias, lapse = model_parameters

    nA = 3
    beta = max(beta, 1e-6) * 5.0
    wm_learn = np.clip(wm_learn, 1e-6, 1.0)
    arb_inertia = np.clip(arb_inertia, 0.0, 1.0)
    age_bias = max(0.0, age_bias)
    lapse = np.clip(lapse, 0.0, 0.2)

    is_older = 1.0 if age[0] >= 45 else 0.0

    total_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM values
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = 0.5 * np.ones((nS, nA))  # WM estimates of reward probability

        # State-wise arbitration weight and volatility trackers
        w_wm = 0.5 * np.ones(nS)  # initial WM weight per state
        pe_avg = np.zeros(nS)     # running average of abs PE
        vol = np.zeros(nS)        # running volatility estimate

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            load = (max(nS, 3) - 3) / 3.0

            # Policies
            # RL policy
            logits = beta * Q[s, :]
            logits -= np.max(logits)
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy from W[s, :] as probabilities, soft-normalized
            wm_pref = W[s, :].clip(1e-6, 1.0)
            wm_pref = wm_pref / np.sum(wm_pref)
            p_wm_vec = wm_pref

            # Current arbitration weight for this state
            mix_w = np.clip(w_wm[s], 0.0, 1.0)

            # Lapse mixture
            p_mix = (1.0 - lapse) * (mix_w * p_wm_vec + (1.0 - mix_w) * p_rl) + lapse * (1.0 / nA)
            p_choice = np.clip(p_mix[a], 1e-12, 1.0)
            total_loglik += np.log(p_choice)

            # Learning updates
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM associative update toward actual reward probability
            W[s, a] += wm_learn * (r - W[s, a])

            # Volatility estimation from absolute PE dynamics
            abs_pe = abs(pe)
            # Update running avg abs PE and volatility (difference from previous avg)
            prev_avg = pe_avg[s]
            pe_avg[s] = (1.0 - wm_learn) * pe_avg[s] + wm_learn * abs_pe
            vol[s] = (1.0 - wm_learn) * vol[s] + wm_learn * abs(pe_avg[s] - prev_avg)

            # Target WM reliance decreases with volatility, load, and older age
            # Map to target via sigmoid
            z = -3.0 * vol[s] - 2.0 * load - is_older * age_bias
            target_w = 1.0 / (1.0 + np.exp(-z))

            # Update arbitration weight with inertia; older age increases inertia
            inertia_eff = np.clip(arb_inertia + 0.3 * is_older, 0.0, 1.0)
            w_wm[s] = inertia_eff * w_wm[s] + (1.0 - inertia_eff) * target_w

    return -float(total_loglik)