def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with uncertainty-based arbitration modulated by set size and age.

    Idea:
    - Two systems: model-free RL (Q-learning) and a lightweight WM trace that stores the last
      rewarded action per state with decay.
    - Arbitration weight increases when RL is uncertain (high entropy) and when the capacity
      pressure is low (smaller set sizes) and for younger adults; older adults rely more on RL.
    - WM trace decays toward uniform; rewarded trials write a one-hot memory for that state.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1].
    - beta: inverse temperature for RL softmax (>0).
    - phi_ret: WM retention (per-trial) in [0,1]; higher means slower decay.
    - gamma_arb: strength of uncertainty-based arbitration (>0).
    - age_bias: scalar in [0,1], increases RL reliance for older adults in arbitration and WM retention.

    Inputs:
    - states: integer array of states per trial (0..set_size-1 within each block).
    - actions: integer array of chosen actions per trial (0..2).
    - rewards: array of rewards per trial (0 or 1).
    - blocks: array of block identifiers per trial (learning resets per block).
    - set_sizes: array of set sizes per trial (3 or 6).
    - age: array-like with a single entry (participant age in years).
    - model_parameters: list/tuple with [alpha, beta, phi_ret, gamma_arb, age_bias].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, phi_ret, gamma_arb, age_bias = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        M = (1.0 / nA) * np.ones((nS, nA))  # WM policy table; rows sum to 1

        Hmax = np.log(nA)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(logits)
            p_rl_vec /= np.sum(p_rl_vec)

            # WM policy (from M table)
            m_s = M[s, :]
            m_s = m_s / np.sum(m_s)  # ensure normalization
            p_wm_vec = m_s

            # Arbitration: weight WM more when RL is uncertain (high entropy),
            # and when capacity pressure is low (small set size), and for younger adults.
            H_q = -np.sum(p_rl_vec * np.log(p_rl_vec + eps))
            unc = (H_q / Hmax)  # in [0,1]
            cap = (3.0 / float(curr_set))
            age_mod = (1.0 - age_bias * is_older)  # reduces WM weight if older
            w = 1.0 / (1.0 + np.exp(-gamma_arb * (unc * cap * age_mod - 0.5)))
            # w in (0,1) with midpoint at 0.5 of the combined drive

            p = w * p_wm_vec + (1.0 - w) * p_rl_vec
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # Update RL
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update WM: decay toward uniform; if rewarded, write one-hot memory
            # Effective retention also suffers under load and with age
            phi_eff = phi_ret * (3.0 / float(curr_set)) * (1.0 - 0.5 * age_bias * is_older)
            phi_eff = min(max(phi_eff, 0.0), 1.0)
            M[s, :] = phi_eff * M[s, :] + (1.0 - phi_eff) * (1.0 / nA)
            if r > 0.5:
                M[s, :] = 0.0
                M[s, a] = 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pearce-Hall–style adaptive learning rate with age and load modulation.

    Idea:
    - Model-free RL where the trial-wise learning rate alpha_t adapts to recent surprise (unsigned PE).
    - Sensitivity to surprise is amplified under higher set sizes (WM load) and for older adults.
    - Captures that under higher load or aging, participants may rely more on recent errors to update faster.

    Parameters (model_parameters):
    - alpha0: baseline learning rate in (0,1).
    - beta: inverse temperature for softmax (>0).
    - k_vol: gain of volatility/surprise control on alpha (>=0).
    - age_scale: multiplier (>0) scaling surprise sensitivity for older adults.
    - set_exp: exponent controlling how set size scales surprise sensitivity (can be >=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: per-trial arrays as described.
    - age: array-like with single value (years).
    - model_parameters: [alpha0, beta, k_vol, age_scale, set_exp].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha0, beta, k_vol, age_scale, set_exp = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    def clamp01(x):
        return min(1.0, max(0.0, x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Running estimate of surprise (unsigned PE); initialize moderate
        surprise = 0.5

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Softmax policy
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            p_vec = np.exp(logits)
            p_vec /= np.sum(p_vec)
            p_choice = max(p_vec[a], eps)
            nll -= np.log(p_choice)

            # Compute PE and update adaptive alpha
            delta = r - Q[s, a]
            # Update running surprise (EMA of unsigned PE) with a rate that grows with load
            # Use the same k_vol driver indirectly so that surprise adapts faster under higher load.
            ema_rate = 1.0 - np.exp(-k_vol * (curr_set / 3.0) ** max(0.0, set_exp))
            ema_rate = min(max(ema_rate, 0.0), 1.0)
            surprise = (1.0 - ema_rate) * surprise + ema_rate * abs(delta)

            # Age- and load-modulated alpha_t
            sens = k_vol * surprise * (1.0 + age_scale * is_older) * (curr_set / 3.0) ** max(0.0, set_exp)
            alpha_t = clamp01(alpha0 + sens)
            Q[s, a] += alpha_t * delta

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor–Critic with load- and age-dependent lapses.

    Idea:
    - Policy (actor) over actions for each state, updated by REINFORCE with a critic baseline.
    - The critic estimates state value; policy update uses advantage (TD error).
    - A lapse probability mixes the softmax policy with uniform random choice; lapses increase
      with set size (higher WM load) and are higher for older adults.

    Parameters (model_parameters):
    - alpha_p: learning rate for policy parameters (>0).
    - alpha_v: learning rate for critic value (>0).
    - beta: inverse temperature for softmax policy (>0).
    - lapse_base: base lapse probability in (0,1).
    - age_lapse_gain: additive increase of lapse in logit space for older adults (can be >=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes: per-trial arrays as described.
    - age: array-like with a single element (years).
    - model_parameters: [alpha_p, alpha_v, beta, lapse_base, age_lapse_gain].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_p, alpha_v, beta, lapse_base, age_lapse_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    def logit(p):
        p = min(max(p, 1e-6), 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        H = np.zeros((nS, nA))  # policy preferences
        V = np.zeros(nS)        # critic values

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Softmax policy
            logits = beta * (H[s, :] - np.max(H[s, :]))
            pi = np.exp(logits)
            pi /= np.sum(pi)

            # Lapse probability increases with load and for older adults (on logit scale)
            logit_eps = logit(lapse_base) + age_lapse_gain * is_older + (curr_set / 3.0 - 1.0)
            eps_t = sigmoid(logit_eps)
            eps_t = min(max(eps_t, 0.0), 1.0)

            p = (1.0 - eps_t) * pi + eps_t * (1.0 / nA)
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # TD error with zero discount (bandit-like within state)
            delta = r - V[s]
            V[s] += alpha_v * delta

            # Policy gradient update with advantage (delta)
            onehot = np.zeros(nA)
            onehot[a] = 1.0
            H[s, :] += alpha_p * delta * (onehot - pi)

    return nll