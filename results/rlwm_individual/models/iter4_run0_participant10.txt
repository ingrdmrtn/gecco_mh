def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Surprise-gated Working Memory with load-dependent decay.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores a distribution over actions per state; after reward it stores a sharp one-hot memory.
    - WM passively decays toward uniform. The decay rate increases with set size (load).
    - Arbitration: the mixture weight for WM on each trial is gated by surprise (absolute RL prediction error)
      via a logistic transform; surprising outcomes increase reliance on WM.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - gate_k: Sensitivity of WM gating to surprise (>=0)
    - gate_b: Bias/threshold for WM gating (can be positive/negative)
    - wm_decay: Base WM decay rate per trial (>=0). Effective decay increases with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, gate_k, gate_b, wm_decay = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent decay factor (higher for larger set sizes)
        phi_decay = 1.0 - np.exp(-wm_decay * float(nS))  # in [0,1)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Deterministic softmax over WM values
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Surprise-gated arbitration using absolute (pre-update) PE
            delta_pre = r - Q_s[a]
            gate_input = gate_k * np.abs(delta_pre) - gate_b
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Passive decay toward uniform (stronger under load)
            w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * w_0[s, :]

            # On rewarded outcomes, store a sharp one-hot
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On non-reward, reduce the weight on the chosen action slightly (avoid repeating mistake)
                alpha_neg = 0.5 * phi_decay  # tie to decay/load
                w[s, a] = max(1e-12, (1.0 - alpha_neg) * w[s, a])
                # renormalize
                w[s, :] = np.maximum(w[s, :], 1e-12)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Visit-based WM reliance with refreshing on re-visits.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores a distribution over actions per state. Rewarded actions shift WM toward one-hot.
    - WM is 'refreshed' upon re-visits to the same state: even without reward, the chosen action slightly
      strengthens in WM (models rehearsal), controlled by wm_refresh.
    - Arbitration: WM weight is high early in learning for each state and decreases with the number of
      visits to that state (per block), via an exponential decay controlled by k_visit and base weight beta_mix.
    - WM policy becomes noisier as the state is visited more often (less reliance on transient WM), by reducing
      its effective inverse temperature with visits.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - k_visit: Rate at which WM reliance decays with state visit count (>=0)
    - wm_refresh: Strength of WM refreshing on re-visit (0..1)
    - beta_mix: Base WM mixture weight (0..1), scaled down by visit-dependent decay

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, k_visit, wm_refresh, beta_mix = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state visit counters within the block
        visits = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Visit-based arbitration weight and WM determinism
            v = visits[s]
            wm_weight = beta_mix * np.exp(-k_visit * v)
            wm_weight = max(0.0, min(1.0, wm_weight))

            # WM becomes noisier with more visits
            eff_beta_wm = max(1.0, softmax_beta_wm / (1.0 + v))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            denom_wm = np.sum(np.exp(eff_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward strengthens one-hot memory
            if r > 0.5:
                w[s, :] = (1.0 - wm_refresh) * w[s, :]
                w[s, a] += wm_refresh
            else:
                # Even without reward, rehearsal/refresh toward the chosen action
                w[s, :] = (1.0 - 0.5 * wm_refresh) * w[s, :]
                w[s, a] += 0.5 * wm_refresh

            # Normalize to a proper probability distribution
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

            # Increment visit count after processing the trial
            visits[s] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Entropy- and PE-based arbitration with WM leak.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores a distribution over actions per state that leaks toward uniform each trial.
    - Arbitration weight for WM depends on:
        (a) WM certainty for the current state (measured as reduction in entropy vs. uniform),
        (b) Surprise (absolute RL PE before update).
      A logistic transform combines these signals: more certain WM and/or more surprise -> greater WM weight.
    - Load effect: larger set sizes indirectly reduce WM certainty (since leak pushes W toward uniform),
      thereby reducing WM weight via entropy.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - arbit_temp: Temperature/scaling of the arbitration logistic (>=0)
    - wm_leak: Leak rate of WM toward uniform per trial (0..1)
    - pe_sensitivity: Weight of absolute prediction error in arbitration (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, arbit_temp, wm_leak, pe_sensitivity = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute entropy of the uniform distribution
        H_uniform = -np.sum(w_0[0, :] * np.log(w_0[0, :]))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(1e-12, denom_wm)

            # Compute WM certainty via entropy reduction
            W_s_safe = np.maximum(W_s, 1e-12)
            W_s_safe = W_s_safe / np.sum(W_s_safe)
            H_W = -np.sum(W_s_safe * np.log(W_s_safe))
            certainty = H_uniform - H_W  # higher when WM is more peaked

            # Surprise signal (pre-update PE)
            delta_pre = r - Q_s[a]

            # Arbitration: logistic over combined certainty and surprise
            gate_input = arbit_temp * certainty + pe_sensitivity * np.abs(delta_pre)
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leak toward uniform, then Hebbian bump on chosen action proportional to reward
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            if r > 0.5:
                # Add a small sharp bump to the chosen action (kept modest to avoid collapse when leak is small)
                bump = min(0.5, 0.5 * (1.0 - wm_leak))
                w[s, a] += bump

            # Normalize and stabilize
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p