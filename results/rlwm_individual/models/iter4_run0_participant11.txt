def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce–Hall uncertainty-driven learning rate + capacity-limited WM with decay.

    Rationale:
    - RL learning rate adapts from trial-by-trial surprise (unsigned PE) via Pearce–Hall rule.
    - WM stores rewarded state-action mappings, but decays toward uniform (interference/forgetting).
    - WM contribution is down-weighted by load (3 vs 6) using a rational capacity factor (3/nS).

    Parameters (model_parameters):
    - lr0: Base RL learning rate (0..1) multiplied by Pearce–Hall associability
    - wm_weight: Base weight for WM policy in the mixture (0..1), scaled by capacity 3/nS
    - softmax_beta: Inverse temperature for RL softmax (scaled internally by 10)
    - ph_kappa: Pearce–Hall update rate for associability per state (0..1)
    - wm_alpha: WM encoding step on rewarded trials (0..1)
    - wm_decay: WM decay toward uniform on each trial (0..1)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block index per trial
    - set_sizes: set size per trial (all trials in a block share the same value)
    Returns: negative log-likelihood of observed choices under the model
    """
    lr0, wm_weight, softmax_beta, ph_kappa, wm_alpha, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL values, WM values, and state-wise associability (Pearce–Hall)
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        associability = np.ones(nS) * 0.5  # initial moderate associability

        # Load-based WM scaling
        capacity_scale = min(1.0, 3.0 / float(nS))
        wm_weight_block = wm_weight * capacity_scale
        wm_weight_block = max(0.0, min(1.0, wm_weight_block))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax over WM weights)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with Pearce–Hall associability
            delta = r - Q_s[a]
            associability[s] = (1.0 - ph_kappa) * associability[s] + ph_kappa * abs(delta)
            lr_eff = lr0 * max(0.0, min(1.0, associability[s]))
            q[s, a] += lr_eff * delta

            # WM update: decay toward uniform, then encode rewarded choice
            # Decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Encode if rewarded
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            # Normalize row to avoid numerical drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Error-gated WM arbitration + load-dependent exploration with RL.

    Rationale:
    - RL updates with fixed learning rate.
    - WM stores mappings when rewarded; weakly decays on errors.
    - Arbitration weight is higher when recent unsigned PE is large (need for WM) and lower under higher load.
    - RL exploration increases under higher load via load-dependent temperature.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM gating gain (controls sigmoid sensitivity to PE)
    - softmax_beta: Base inverse temperature (scaled internally by 10)
    - beta_load_slope: Load sensitivity of temperature; beta_eff = beta/(1+beta_load_slope*(nS-3))
    - wm_alpha: WM learning step on reward; on no-reward a small fraction is redistributed toward uniform
    - lapse: Lapse probability mixed uniformly across actions (0..1)

    Returns: negative log-likelihood of observed choices under the model
    """
    lr, wm_weight, softmax_beta, beta_load_slope, wm_alpha, lapse = model_parameters
    eps = 1e-12
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent RL temperature
        beta_eff = (softmax_beta * 10.0) / (1.0 + max(0.0, beta_load_slope) * max(0, (nS - 3)))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track recent unsigned PE per state for gating
        recent_upe = np.zeros(nS)

        # Load penalty for WM arbitration
        load_scale = min(1.0, 3.0 / float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with load-dependent temperature
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Error-gated WM weight via sigmoid of recent unsigned PE
            # recent_upe in [0,1]; wm_weight controls slope; load_scale reduces WM under high set size
            gate_input = recent_upe[s]
            wm_gate = 1.0 / (1.0 + np.exp(- (wm_weight * 5.0) * (gate_input - 0.5)))  # center at 0.5
            wm_weight_eff = load_scale * wm_gate
            wm_weight_eff = max(0.0, min(1.0, wm_weight_eff))

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update recent unsigned PE (simple EMA with lr as smoothing constant)
            recent_upe[s] = (1.0 - lr) * recent_upe[s] + lr * abs(delta)
            recent_upe[s] = max(0.0, min(1.0, recent_upe[s]))

            # WM update: strengthen on reward, slight redistribution toward uniform on no-reward
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # Gentle push toward uniform when error occurs
                leak = 0.5 * wm_alpha
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Reward-rate adapted exploration + WM encode/decay with load-sensitive arbitration.

    Rationale:
    - RL temperature adapts to running reward rate: more exploitative when recent reward rate is high.
    - RL learning rate fixed.
    - WM stores rewarded mappings and decays each trial; arbitration down-weights WM under higher load.
    - Provides a simple meta-learning account of exploration without extra RL parameters.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: Base WM mixture weight (0..1), scaled by load (3/nS)
    - softmax_beta: Base RL inverse temperature (scaled internally by 10), further adapted by reward-rate
    - wm_alpha: WM encoding step on reward (0..1)
    - temp_adapt: Reward-rate adaptation speed for both reward average and beta modulation (0..1)
    - wm_decay: WM decay toward uniform each trial (0..1)

    Returns: negative log-likelihood of observed choices under the model
    """
    lr, wm_weight, softmax_beta, wm_alpha, temp_adapt, wm_decay = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running reward rate per block
        rbar = 0.5

        # Load scaling for WM
        load_scale = min(1.0, 3.0 / float(nS))
        wm_weight_block = wm_weight * load_scale
        wm_weight_block = max(0.0, min(1.0, wm_weight_block))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Adapt beta with running reward rate: bias toward exploitation when rbar > 0.5
            beta_eff = base_beta * np.exp((rbar - 0.5) * 2.0)  # exponential scaling around 0.5

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update running reward average
            rbar = (1.0 - temp_adapt) * rbar + temp_adapt * r

            # WM decay then encode on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p