def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Gated, reward-triggered WM with load-sensitive gating and asymmetric RL learning.

    Idea:
    - Model-free RL with separate positive/negative learning rates.
    - WM is a win-stay associative store: when a trial is rewarded, the chosen action
      is encoded as a near one-hot vector for that state. Encoding is probabilistically
      gated, with the gate closing as set size (load) increases.
    - WM decays toward uniform with a forgetting rate.
    - Policy is a mixture of RL softmax and WM softmax, where WM policy is sharp.

    Parameters (model_parameters):
    - lr_pos: in [0,1], RL learning rate for positive prediction errors.
    - lr_neg: in [0,1], RL learning rate for negative prediction errors.
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_gate_base: real, base log-odds of WM gating (higher -> more likely to encode).
    - gate_slope: real, sensitivity of WM gating to load (more negative -> stronger suppression with larger set sizes).
    - wm_forget: in [0,1], per-trial WM decay toward uniform.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_gate_base, gate_slope, wm_forget = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-sensitive gate probability (logistic)
        # Higher set size -> lower p_gate if gate_slope < 0
        z = wm_gate_base + gate_slope * (nS - 3)  # center around low-load = 3
        p_gate = 1.0 / (1.0 + np.exp(-z))
        p_gate = np.clip(p_gate, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global WM decay toward uniform
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture weight is dynamically the current gating probability on this block
            wm_weight = p_gate
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM update: reward-triggered gated encoding as near one-hot
            if r > 0:
                if np.random.rand() < p_gate:
                    target = np.copy(w_0[s, :])
                    target[a] = 1.0
                    target = target / np.sum(target)
                    # strong write-in (overwrite-like)
                    w[s, :] = 0.0 * w[s, :] + 1.0 * target
            else:
                # No direct punishment encoding; rely on decay
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with recency-weighted WM reliability and load penalty.

    Idea:
    - Model-free RL with a single learning rate and softmax.
    - WM is a probabilistic policy store updated toward one-hot on reward and diffused otherwise.
    - The mixture weight is state- and time-dependent: WM reliability increases when the
      state was visited recently and is penalized by set size (load).
    - This captures that small set sizes and recent visits favor WM control.

    Parameters (model_parameters):
    - lr: in [0,1], RL learning rate.
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_bias: real, base log-odds for WM weight (positive favors WM).
    - recency_gain: >= 0, scales the effect of 1/(1 + time_since_last_visit) on WM weight.
    - load_gain: >= 0, penalizes WM weight linearly with set size.
    - wm_learn: in [0,1], WM learning rate toward one-hot on rewarded trials (and toward uniform otherwise).

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_bias, recency_gain, load_gain, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track recency: time since last visit per state
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Compute time since last visit for current state
            if last_seen[s] < 0:
                tsl = 1e6  # effectively infinite if never seen
            else:
                tsl = t - last_seen[s]
            last_seen[s] = t  # update last seen time

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic WM weight via logistic transform of features
            # Feature 1: recency score (higher if visited very recently)
            recency_score = 1.0 / (1.0 + tsl)
            # Feature 2: load penalty increases with set size
            z = wm_bias + recency_gain * recency_score - load_gain * (nS - 3)
            wm_weight = 1.0 / (1.0 + np.exp(-z))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: rewarded -> write toward one-hot, unrewarded -> drift toward uniform
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent Q-value decay + WM with imperfect encoding under interference.

    Idea:
    - RL learns Q-values but also suffers load-dependent decay toward uniform each trial,
      capturing that larger set sizes impair model-free retention across trials.
    - WM encodes rewarded associations with a certain write probability and imperfect
      precision: some probability "wm_confusion" spreads value to non-chosen actions.
    - The policy is a fixed mixture between RL and WM within a block; WM policy is sharp.

    Parameters (model_parameters):
    - lr: in [0,1], RL learning rate.
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight_base: in [0,1], baseline mixture weight on WM policy.
    - rl_decay_load: >= 0, controls per-trial RL decay toward uniform that scales with set size.
    - wm_write_prob: in [0,1], probability to write a rewarded action into WM on a given trial.
    - wm_confusion: in [0,1], fraction of WM write that is spread uniformly to non-chosen actions.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_weight_base, rl_decay_load, wm_write_prob, wm_confusion = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Compute RL decay rate based on load (bounded between 0 and 1)
        rl_decay = 1.0 - np.exp(-rl_decay_load * max(1.0, nS) / 3.0)
        rl_decay = np.clip(rl_decay, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Apply RL decay toward uniform before acting
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_base * p_wm + (1.0 - wm_weight_base) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update with imperfect encoding and interference
            if r > 0 and np.random.rand() < wm_write_prob:
                # Construct imprecise target: majority to chosen action, some spread to others
                target = np.full(nA, wm_confusion / max(1, (nA - 1)))
                target[a] = 1.0 - wm_confusion
                target = target / np.sum(target)
                w[s, :] = target
            else:
                # No reward: allow slight diffusion toward uniform due to interference
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p