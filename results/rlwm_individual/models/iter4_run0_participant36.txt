def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Leaky-Competing WM with load-dependent leak and fast write.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy.
    - WM system: a state-specific action distribution that:
        * rapidly writes toward a one-hot vector after feedback,
        * leaks toward uniform each trial (stronger leak under higher load).
      RL and WM policies are linearly mixed with a base WM weight that is reduced
      by load (set size).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Base mixture weight on the WM policy (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - wm_eta: WM learning rate (0..1) controlling how strongly feedback writes to WM.
    - wm_leak: Baseline WM leak toward uniform per visit (>=0).
    - load_slope: Increases WM leak and reduces WM weight as set size grows (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_eta, wm_leak, load_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-adjusted WM mixture weight and leak
        load_factor = max(0, nS - 3)
        wm_weight = wm_weight_base / (1.0 + max(0.0, load_slope) * load_factor)
        wm_weight = np.clip(wm_weight, 0.0, 1.0)
        wm_leak_eff = wm_leak * (1.0 + max(0.0, load_slope) * load_factor)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM leak toward uniform (load-adjusted)
            w[s, :] = (1.0 - wm_leak_eff) * w[s, :] + wm_leak_eff * w_0[s, :]

            # WM write: approach one-hot if rewarded; if not rewarded, drift back to uniform
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # Negative outcome weakens the current peak, nudging toward uniform
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with PE-gated arbitration and simple WM write/forget.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy.
    - WM system: maintains a distribution over actions per state that is sharpened
      toward the last experienced outcome. It forgets toward uniform each visit.
    - Arbitration: the WM mixture weight is dynamically gated by the absolute
      RL prediction error (PE). When |PE| is large (surprise), the model trusts WM less.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Base WM weight before PE gating (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - pe_temp: Sensitivity of WM weight to absolute PE (>=0). Higher => stronger reduction
               of WM trust under large PE.
    - wm_eta: WM learning rate (0..1) for writing after feedback.
    - wm_forget: Forgetting toward uniform per visit (0..1).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, pe_temp, wm_eta, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute PE before RL update for arbitration
            delta = r - Q_s[a]
            abs_pe = abs(delta)

            # PE-gated WM weight: larger |PE| => smaller WM reliance
            # wm_weight_t = wm_weight_base / (1 + exp(pe_temp*(|PE|-0.5)))
            wm_weight_t = wm_weight_base / (1.0 + np.exp(max(0.0, pe_temp) * (abs_pe - 0.5)))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s][a] += lr * delta

            # WM forgetting toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM write: reward strengthens selected action; non-reward weakens it
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # Move slightly away from chosen action toward uniform
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-adjusted RL temperature and recency-based WM retention with stochastic overwrite.

    Overview:
    - RL system: Q-learning with a block-level inverse temperature that decreases
      as set size increases (harder condition -> noisier RL choices).
    - WM system: when rewarded, the model overwrites WM for that state with a one-hot
      action representation with probability 'replace_prob'; retention decays with the
      time since last reward via 'recency_decay'. WM acts as a peaked distribution over
      the stored action, blended with uniform according to the retention.
    - Mixture: fixed WM weight mixes WM and RL policies.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight on WM policy (0..1).
    - softmax_beta_base: Base RL inverse temperature before load scaling (scaled by 10 internally).
    - beta_drop: Load sensitivity of RL temperature; larger values reduce RL beta more
                 strongly when set size > 3 (>=0).
    - replace_prob: Probability to store/overwrite WM with the chosen action on rewarded trials (0..1).
    - recency_decay: Per-trial retention factor for WM (0..1); higher means slower decay with time.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta_base, beta_drop, replace_prob, recency_decay = model_parameters
    softmax_beta = softmax_beta_base * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track stored action and last rewarded time for WM recency
        stored_action = -np.ones(nS, dtype=int)
        last_rew_t = -np.ones(nS, dtype=int)

        # Load-adjust RL beta
        load_factor = max(0, nS - 3)
        beta_block = softmax_beta / (1.0 + max(0.0, beta_drop) * load_factor)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with load-adjusted beta
            p_rl = 1.0 / np.sum(np.exp(beta_block * (Q_s - Q_s[a])))

            # Build WM preference based on recency retention of stored action
            if stored_action[s] >= 0:
                dt = t - last_rew_t[s] if last_rew_t[s] >= 0 else 1e6
                retain = max(0.0, min(1.0, (recency_decay ** max(0, dt))))
                pref = np.ones(nA) / nA
                pref[stored_action[s]] = retain + (1.0 - retain) / nA
                pseudo_W = pref
            else:
                pseudo_W = np.ones(nA) / nA

            # WM policy is softmax over the pseudo_W distribution
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (pseudo_W - pseudo_W[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: optional overwrite on reward, otherwise keep current state
            if r > 0.0:
                # Stochastic overwrite into WM
                if np.random.rand() < replace_prob:
                    stored_action[s] = a
                    last_rew_t[s] = t
                    onehot = np.zeros(nA)
                    onehot[a] = 1.0
                    w[s, :] = onehot
                else:
                    # Lightly nudge toward the rewarded action even if not overwritten
                    w[s, :] = 0.5 * w[s, :]
                    w[s, a] += 0.5
                    w[s, :] /= np.sum(w[s, :])
            else:
                # Drift WM content toward uniform when not rewarded
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p