def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with success-gated WM reliance and set-sizeâ€“dependent WM decay.

    Model idea:
    - RL: standard delta-rule Q-learning with softmax choice.
    - WM: stores action-probability distributions per state, concentrated on rewarded actions.
    - WM reliance (mixture weight) is increased by recent success with that state and reduced by set size.
    - WM traces decay faster as set size increases.

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate for Q updates.
    - wm_weight_base: [0,1] Baseline mixture weight of WM.
    - softmax_beta: >=0 RL inverse temperature (scaled internally by 10).
    - wm_success_boost: >=0 Strength of success-based gating; higher -> more reliance on WM when recently successful.
    - wm_decay_base: [0,1] Base WM decay towards uniform per visit (scaled by set size).
    - ss_sensitivity: >=0 Modulates how strongly set size increases WM decay and reduces WM weight.

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, wm_success_boost, wm_decay_base, ss_sensitivity = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track per-state running success to gate WM weight
        succ = np.zeros(nS)

        # Scaling factors by set size
        weight_scale = 1.0 / (1.0 + ss_sensitivity * max(0, nS - 3))
        decay_scale = 1.0 + ss_sensitivity * max(0, nS - 3)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM "values" (probabilities) for current state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Success-based gating: more WM if recently successful in this state,
            # and less WM as set size increases.
            # succ[s] in [0,1] (running average), map to [0, 1] gain: 1 + wm_success_boost*succ
            wm_w = wm_weight_base * weight_scale * (1.0 + wm_success_boost * succ[s])
            wm_w = max(0.0, min(1.0, wm_w))

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Rewarded: concentrate distribution on chosen action (one-shot increment)
            # - Unrewarded: decay towards uniform. Decay speeds up with set size.
            if r >= 1.0 - 1e-12:
                # Move w[s] towards one-hot at a
                target = np.full(nA, 0.0)
                target[a] = 1.0
                # Use a fast consolidation depending on base + success (capped at 1)
                eta = min(1.0, 0.5 + 0.5 * wm_success_boost)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target
                # Update running success (exponential moving average)
                succ[s] = 0.7 * succ[s] + 0.3 * 1.0
            else:
                # Decay towards uniform; faster with larger set size
                decay = max(0.0, min(1.0, wm_decay_base * decay_scale))
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                succ[s] = 0.7 * succ[s] + 0.3 * 0.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM with interference across states.

    Model idea:
    - RL: delta-rule with separate learning rates for positive vs negative prediction errors.
    - WM: stores probabilistic bindings per state; upon reward, updates toward one-hot on chosen action.
    - WM interference: as set size increases, memory for a given state is contaminated by other states'
      traces (confusability). Effective WM for state s is a mixture of its own trace and the mean of others.
    - Fixed mixture weight of WM vs RL (wm_weight), independent of set size, but WM itself degrades via interference.

    Parameters (model_parameters):
    - lr_pos: [0,1] RL learning rate for positive prediction errors.
    - lr_neg: [0,1] RL learning rate for negative prediction errors.
    - softmax_beta: >=0 RL inverse temperature (scaled internally by 10).
    - wm_weight: [0,1] Mixture weight of WM vs RL.
    - wm_binding_strength: [0,1] Learning rate for WM update after reward.
    - interference_sigma: >=0 Strength of WM interference across states (grows with set size).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_binding_strength, interference_sigma = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM interference: mix current state's WM trace with the average of others.
            if nS > 1:
                others_idx = [i for i in range(nS) if i != s]
                mean_others = np.mean(w[others_idx, :], axis=0)
            else:
                mean_others = w[s, :]

            gamma = min(1.0, interference_sigma * (nS - 1) / max(1.0, nS))  # more interference with larger set
            W_eff = (1.0 - gamma) * w[s, :] + gamma * mean_others

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            wm_w = max(0.0, min(1.0, wm_weight))
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM update
            if r >= 1.0 - 1e-12:
                # Move w[s] toward one-hot on a
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_binding_strength) * w[s, :] + wm_binding_strength * target
            else:
                # Mild decay toward uniform when unrewarded
                decay = 0.3 * wm_binding_strength
                decay = max(0.0, min(1.0, decay))
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Meta-cognitive arbitration using RL uncertainty vs WM sharpness; WM recency with capacity scaling.

    Model idea:
    - RL: standard delta-rule learning and softmax choice.
    - WM: stores peaked distributions after reward; decays toward uniform with recency-controlled decay.
    - Arbitration: mixture weight is a sigmoid of (WM sharpness - normalized RL policy entropy),
      scaled by a base weight and set-size-adjusted capacity.
      Intuition: rely on WM when it is sharp and RL is uncertain.
    - Capacity effect: effective WM base is scaled by min(1, capacity_slots / nS).

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate.
    - softmax_beta: >=0 RL inverse temperature (scaled internally by 10).
    - wm_base: [0,1] Baseline WM contribution before arbitration.
    - wm_temperature: >0 Slope of arbitration sigmoid (higher -> sharper gating).
    - recency_lambda: >=0 Controls WM decay toward uniform per visit.
    - capacity_slots: >0 Effective number of WM slots (scales WM base by slots / set size).

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_base, wm_temperature, recency_lambda, capacity_slots = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity scaling for WM base
        cap_scale = min(1.0, max(1.0, capacity_slots) / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from current memory trace
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL policy entropy (approx via softmax over Q)
            # Build the RL policy explicitly for arbitration only
            Q_shift = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * Q_shift)
            pi_rl = pi_rl / np.sum(pi_rl)
            pi_rl_safe = np.clip(pi_rl, 1e-12, 1.0)
            ent_rl = -np.sum(pi_rl_safe * np.log(pi_rl_safe))
            ent_rl_norm = ent_rl / np.log(nA)  # in [0,1]

            # WM sharpness via 1 - entropy(W_s)
            W_safe = np.clip(W_s, 1e-12, 1.0)
            ent_wm = -np.sum(W_safe * np.log(W_safe))
            sharp_wm = 1.0 - (ent_wm / np.log(nA))  # in [0,1]

            # Arbitration: sigmoid of (sharp_wm - ent_rl_norm)
            diff = sharp_wm - ent_rl_norm
            gate_arg = diff / max(1e-6, wm_temperature)
            sig = 1.0 / (1.0 + np.exp(-gate_arg))

            wm_w = wm_base * cap_scale * sig
            wm_w = max(0.0, min(1.0, wm_w))

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: rewarded => sharpen; otherwise decay toward uniform.
            if r >= 1.0 - 1e-12:
                # Consolidate toward one-hot
                target = np.zeros(nA)
                target[a] = 1.0
                eta_w = 1.0 - np.exp(-recency_lambda)  # in [0,1)
                w[s, :] = (1.0 - eta_w) * w[s, :] + eta_w * target
            else:
                # Recency-based decay; also mild global decay to avoid stale traces
                decay = 1.0 - np.exp(-recency_lambda * nS / max(1.0, capacity_slots))
                decay = max(0.0, min(1.0, decay))
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p