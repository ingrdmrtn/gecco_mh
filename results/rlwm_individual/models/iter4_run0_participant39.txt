def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM uncertainty arbitration with recency-weighted WM, load-sensitive arbitration.

    Mechanisms
    - RL: standard delta rule with inverse temperature softmax.
    - WM contents: per-state action distribution that moves toward the most recently
      rewarded action and otherwise decays toward uniform; recency is implemented implicitly
      by exponential smoothing (eta).
    - Arbitration: the WM mixture weight on each choice is a logistic function of
      (a) a base bias, (b) the RL policy entropy for the current state (higher entropy
      -> rely more on WM), and (c) a load penalty that grows with set size.
    - WM precision: softmax with fixed high beta, but we further reduce WM precision by
      current load through the arbitration (not by scaling beta itself).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_eta : WM update/decay rate in [0,1]; higher -> faster updating to rewarded action.
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        wm_bias : Arbitration bias term (can be negative or positive).
        wm_entropy_gain : Gain multiplying RL entropy (>=0); higher entropy -> more WM.
        wm_load_gain : Gain multiplying (set_size-1) penalty (>=0); larger sets -> less WM.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_eta, softmax_beta, wm_bias, wm_entropy_gain, wm_load_gain = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # base deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice probability of chosen action (stable trick)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute RL full policy to assess uncertainty (entropy)
            Qs_centered = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Qs_centered)
            pi_rl = expQ / np.sum(expQ)
            entropy_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, 1e-12, 1.0)))

            # WM policy: softmax over WM trace
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: logistic of bias + entropy_gain*entropy - load_gain*(nS-1)
            arb_linear = wm_bias + wm_entropy_gain * entropy_rl - wm_load_gain * max(0, nS - 1)
            wm_weight = 1.0 / (1.0 + np.exp(-arb_linear))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: move toward last rewarded action; otherwise decay toward uniform
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * onehot
            else:
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-adjusted exploration and WM caching strength arbitration.

    Mechanisms
    - RL: delta-rule with softmax whose inverse temperature decreases with load
      (more exploration at larger set sizes).
    - WM: a cache-like distribution per state that strengthens toward the action that
      produced reward and otherwise decays toward uniform (controlled by wm_decay).
      The "cache strength" is the max element of w[s], indicating how peaked the memory is.
    - Arbitration: WM mixture weight increases with the cache strength (confidence in WM)
      but has its own base bias; RL beta is explicitly scaled down with load.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        softmax_beta_base : Base RL inverse temperature (rescaled internally by *10).
        beta_load_gain : Gain > 0; RL beta scaled by 1/(1 + gain*(nS-1)).
        wm_weight0 : Arbitration intercept (base WM weight after logistic).
        wm_cache_gain : Gain multiplying cache strength in arbitration (>=0).
        wm_decay : WM decay/update rate in [0,1]; toward rewarded action if r=1, else toward uniform.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta_base, beta_load_gain, wm_weight0, wm_cache_gain, wm_decay = model_parameters
    softmax_beta = softmax_beta_base  # will be scaled then multiplied by 10 below
    softmax_beta *= 10  # base scaling per template
    softmax_beta_wm = 50  # very deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-adjust RL inverse temperature
        rl_beta_eff = softmax_beta / (1.0 + beta_load_gain * max(0, nS - 1))

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice probability under effective beta
            p_rl = 1.0 / np.sum(np.exp(rl_beta_eff * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute cache strength (confidence in WM for state s)
            cache_strength = np.max(W_s)  # in [1/nA, 1]

            # Arbitration: logistic of bias + gain * cache_strength
            arb_linear = wm_weight0 + wm_cache_gain * (cache_strength - (1.0 / nA))
            wm_weight = 1.0 / (1.0 + np.exp(-arb_linear))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: strengthen toward rewarded action, otherwise decay to uniform
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * onehot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with leaky values and WM win-stay/lose-shift heuristic, with set-size-dependent WM weighting.

    Mechanisms
    - RL: delta rule with global leaky forgetting toward uniform after each trial
      (captures interference in larger sets).
    - WM: heuristic distribution implementing win-stay/lose-shift for the current state.
      If last outcome in this state was a win, WM favors repeating that action; after a loss,
      WM spreads mass over alternative actions. A lapse mixes WM policy with uniform.
    - Arbitration: WM mixture weight is interpolated between two values for low vs. high load.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        softmax_beta : RL inverse temperature (rescaled internally by *10).
        wm_weight_low : WM mixture weight at low load (set size close to 3).
        wm_weight_high : WM mixture weight at high load (set size close to 6).
        q_leak : Leak rate in [0,1]; per-trial decay of all Q values toward uniform.
        wm_lapse : WM lapse rate in [0,1]; mixes WM policy with uniform to reflect slips.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_low, wm_weight_high, q_leak, wm_lapse = model_parameters
    softmax_beta *= 10  # higher upper bound
    softmax_beta_wm = 50  # deterministic WM

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Interpolate WM weight by load (assumes 3..6; generalizes linearly otherwise)
        if nS <= 3:
            wm_weight = wm_weight_low
        elif nS >= 6:
            wm_weight = wm_weight_high
        else:
            alpha = (nS - 3.0) / (6.0 - 3.0)
            wm_weight = (1 - alpha) * wm_weight_low + alpha * wm_weight_high

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # will hold WM heuristic distribution per state
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action and last reward per state to implement WSLS
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -1 * np.ones(nS, dtype=int)  # -1 means unknown

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # RL policy prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Construct WM heuristic distribution for current state
            if last_action[s] >= 0 and last_reward[s] >= 0:
                if last_reward[s] == 1:
                    # Win-stay: concentrate on last_action
                    W_s = np.full(nA, (1.0 - 1e-6) / (nA - 1))
                    W_s[last_action[s]] = 1e-6 + (1.0 - (nA - 1) * ((1.0 - 1e-6) / (nA - 1)))
                else:
                    # Lose-shift: spread probability on actions not last tried
                    W_s = np.full(nA, 0.0)
                    others = [i for i in range(nA) if i != last_action[s]]
                    W_s[others] = 1.0 / len(others)
            else:
                # No memory yet: uniform
                W_s = w_0[s, :].copy()

            # Apply WM lapse: mix with uniform
            W_s = (1.0 - wm_lapse) * W_s + wm_lapse * w_0[s, :]
            w[s, :] = W_s  # store for record (not strictly necessary but consistent)

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update (state-action)
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # RL leak toward uniform across all states and actions
            q = (1.0 - q_leak) * q + q_leak * (1.0 / nA)

            # Update WM history
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p