def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: Arbitration between model-free RL and Win-Stay/Lose-Shift (WSLS), 
             with age- and set-size-modulated arbitration weight.

    Rationale:
      - Choices arise from a mixture of model-free RL and a simple WSLS heuristic.
      - The arbitration weight favoring WSLS depends on age group and set size:
          older adults and smaller set sizes can rely more on WSLS (less load).
      - RL updates Q-values with a standard delta rule.
      - Likelihood is computed from the mixture policy.

    Parameters (model_parameters; total 5):
      - alpha: float in (0,1), RL learning rate.
      - beta: float > 0, inverse temperature for RL softmax (rescaled internally by 10).
      - wsls_bias: real, baseline logit bias toward WSLS arbitration.
      - age_wsls_boost: float >= 0, increases WSLS arbitration for older adults.
      - ss_wsls_penalty: float >= 0, decreases WSLS arbitration as set size increases.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha, beta, wsls_bias, age_wsls_boost, ss_wsls_penalty = model_parameters
    nA = 3
    beta_eff = max(1e-6, beta) * 10.0
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)

        # Track last action and reward for WSLS within each state
        last_action = -np.ones(nS, dtype=int)
        last_reward = np.zeros(nS, dtype=float)
        seen_state = np.zeros(nS, dtype=bool)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :].copy()
            logits_rl = beta_eff * (q_s - np.max(q_s))
            p_rl = np.exp(logits_rl)
            p_rl /= np.sum(p_rl)

            # WSLS policy for this state
            p_wsls = np.ones(nA, dtype=float) / nA
            if seen_state[s] and last_action[s] >= 0:
                if last_reward[s] > 0.0:
                    # Win: repeat last action
                    p_wsls[:] = 0.0
                    p_wsls[last_action[s]] = 1.0
                else:
                    # Lose: choose uniformly among other actions
                    p_wsls[:] = 0.0
                    others = [i for i in range(nA) if i != last_action[s]]
                    for oi in others:
                        p_wsls[oi] = 1.0 / len(others)

            # Arbitration weight favoring WSLS (logit link)
            # Larger set size reduces WSLS weight; older age boosts WSLS reliance.
            w_logit = wsls_bias + is_older * max(0.0, age_wsls_boost) - max(0.0, ss_wsls_penalty) * max(0.0, ss - 3.0)
            w_wsls = 1.0 / (1.0 + np.exp(-w_logit))
            w_wsls = np.clip(w_wsls, 0.0, 1.0)

            # Mixture policy
            p_mix = w_wsls * p_wsls + (1.0 - w_wsls) * p_rl
            pa = max(1e-12, p_mix[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 1e-6, 1.0) * pe

            # Update WSLS memory
            seen_state[s] = True
            last_action[s] = a
            last_reward[s] = r

    return float(neg_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Model-free RL with action-generalization across states, age-modulated generalization, 
             and load-sensitive forgetting.

    Rationale:
      - Standard Q-learning within each state.
      - After each trial, the same action in other states is also adjusted (generalization),
        scaled by set size (more dilution in larger sets) and amplified for older adults.
      - Q-values also decay toward 0 each trial, more so in larger set sizes (load-sensitive forgetting).

    Parameters (model_parameters; total 5):
      - alpha: float in (0,1), primary learning rate for the experienced state-action.
      - beta: float > 0, inverse temperature for softmax (rescaled internally by 10).
      - gen_gain: float >= 0, base gain for action-generalization to other states.
      - age_gen_boost: float >= 0, multiplicative boost to generalization for older adults.
      - rho_forget: float in [0,1), base forgetting rate per trial; scaled by set size.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of five floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha, beta, gen_gain, age_gen_boost, rho_forget = model_parameters
    nA = 3
    beta_eff = max(1e-6, beta) * 10.0
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Softmax policy
            q_s = Q[s, :].copy()
            logits = beta_eff * (q_s - np.max(q_s))
            p = np.exp(logits)
            p /= np.sum(p)

            pa = max(1e-12, p[a])
            neg_loglik -= np.log(pa)

            # Primary Q update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 1e-6, 1.0) * pe

            # Action-generalization to other states for the same action
            # Effective generalization gain depends on age and set size (dilution with larger ss).
            g_eff = max(0.0, gen_gain) * (1.0 + is_older * max(0.0, age_gen_boost)) / max(1.0, ss)
            if nS > 1:
                for s2 in range(nS):
                    if s2 == s:
                        continue
                    # Generalize proportionally to prediction error for the same action
                    Q[s2, a] += g_eff * pe

            # Load-sensitive forgetting toward 0 for all Qs
            # More forgetting when set size is larger.
            forget_rate = np.clip(rho_forget * (1.0 + max(0.0, ss - 3.0) / 3.0), 0.0, 0.999)
            Q *= (1.0 - forget_rate)

    return float(neg_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Capacity-limited working memory (WM) slots + model-free RL backup.

    Rationale:
      - A capacity-limited WM stores up to K effective stimulus-action associations with perfect recall,
        subject to a small execution noise. Larger set sizes increase competition for slots.
      - Older adults have effectively fewer WM slots (age shift).
      - If the current state is stored in WM, action selection follows WM with noise; otherwise RL softmax.
      - WM encoding occurs on rewarded trials with probability p_encode; RL updates always occur.

    Parameters (model_parameters; total 6):
      - alpha_rl: float in (0,1), RL learning rate.
      - beta_rl: float > 0, inverse temperature for RL softmax (rescaled internally by 10).
      - K_slots: float >= 0, baseline WM slot capacity.
      - age_slot_drop: float >= 0, reduction in effective slots for older adults.
      - eta_wm: float in [0,1], WM execution noise; if remembered, choose target with prob (1-eta_wm)+eta_wm/3.
      - p_encode: float in [0,1], probability to encode a rewarded association into WM.

    Inputs:
      - states: 1D int array of state indices per trial.
      - actions: 1D int array of chosen actions (0..2) per trial.
      - rewards: 1D float/int array of rewards (0/1) per trial.
      - blocks: 1D int array of block indices per trial.
      - set_sizes: 1D int array of set size (3 or 6) per trial.
      - age: array-like with a single number; age >= 45 -> older group.
      - model_parameters: sequence of six floats as described.

    Returns:
      - Negative log-likelihood of the observed choices under the model (float).
    """
    alpha_rl, beta_rl, K_slots, age_slot_drop, eta_wm, p_encode = model_parameters
    nA = 3
    beta_eff = max(1e-6, beta_rl) * 10.0
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    # Clamp parameters to valid ranges used in computations
    eta_wm = np.clip(eta_wm, 0.0, 1.0)
    p_encode = np.clip(p_encode, 0.0, 1.0)
    alpha_rl = np.clip(alpha_rl, 1e-6, 1.0)

    neg_loglik = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA), dtype=float)

        # Effective capacity per block, reduced for older adults
        K_eff = max(0.0, K_slots - is_older * max(0.0, age_slot_drop))
        K_eff_int = int(np.floor(K_eff + 1e-9))

        # WM store: mapping state -> (action, insertion_time)
        wm_store = {}
        t_counter = 0  # to manage FIFO eviction

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]
            ss = float(block_set_sizes[t])

            # Policy from RL
            q_s = Q[s, :].copy()
            logits = beta_eff * (q_s - np.max(q_s))
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # Policy from WM, if present
            if (s in wm_store) and (K_eff_int > 0):
                a_star = wm_store[s][0]
                p_wm = np.ones(nA, dtype=float) * (eta_wm / nA)
                p_wm[a_star] += (1.0 - eta_wm)
                p_mix = p_wm  # when remembered, rely on WM
            else:
                p_mix = p_rl  # fallback on RL when not remembered

            # Likelihood
            pa = max(1e-12, p_mix[a])
            neg_loglik -= np.log(pa)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # WM encoding/updating rule:
            # Encode with probability p_encode on rewarded trials. If full, evict oldest.
            if r > 0.0 and K_eff_int > 0:
                # Bernoulli "probability" handled deterministically in likelihood by mixing;
                # here we use expected update: scale encoding by p_encode.
                # Implement by maintaining a running probability of storage:
                # We approximate by encoding deterministically but weighting with p_encode in store logic:
                # Practical deterministic surrogate: attempt encode, but when at capacity,
                # probabilistically decide eviction via p_encode. To avoid stochasticity in the
                # likelihood function, we use expected behavior: encode if not present and capacity allows;
                # if full, evict the oldest with probability proportional to p_encode; approximate by
                # evicting only when p_encode > 0.5.
                attempt_encode = (p_encode > 0.5)

                if attempt_encode:
                    if s in wm_store:
                        # Update action for this state
                        wm_store[s] = (a, t_counter)
                    else:
                        if len(wm_store) < K_eff_int:
                            wm_store[s] = (a, t_counter)
                        else:
                            # Evict oldest entry (FIFO) and insert
                            oldest_state = None
                            oldest_time = np.inf
                            for st_key, (_, ins_t) in wm_store.items():
                                if ins_t < oldest_time:
                                    oldest_time = ins_t
                                    oldest_state = st_key
                            if oldest_state is not None:
                                del wm_store[oldest_state]
                                wm_store[s] = (a, t_counter)

            t_counter += 1

    return float(neg_loglik)