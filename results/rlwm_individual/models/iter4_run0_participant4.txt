def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + load-sensitive WM with rehearsal and interference.

    Mechanism
    - RL: standard Rescorla–Wagner update (fixed by template).
    - WM store: a fast, near-deterministic table w[s, a] that aims to be one-hot
      for the believed correct action. It decays toward uniform due to interference,
      with decay increasing under higher set sizes (load).
    - Policy: mixture of WM and RL. The WM mixture weight is downscaled as a
      function of load, and WM is sharpened by rehearsal after correct outcomes.

    Parameters
    ----------
    model_parameters : list/tuple of 5 floats
        lr                 : RL learning rate in (0, 1].
        wm_weight_base     : Base WM mixture weight in [0, 1] before load scaling.
        softmax_beta       : RL inverse temperature; scaled by *10 internally.
        wm_rehearsal       : WM encoding/rehearsal strength on rewarded trials (>0).
        wm_forget_load     : Load-dependent WM interference factor (>=0). Higher => stronger decay with larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_rehearsal, wm_forget_load = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-scaled WM weight: more load -> less effective WM
        load_factor = 1.0 / (1.0 + wm_forget_load * max(0, nS - 1))
        wm_weight_eff = np.clip(wm_weight_base * load_factor, 0.0, 1.0)

        # Load-scaled decay toward uniform (higher load -> faster decay)
        base_decay = np.clip(0.15 + 0.1 * wm_forget_load, 0.0, 1.0)
        wm_decay = np.clip(1.0 - load_factor, 0.0, 1.0)  # in [0,1]; larger with higher load
        gamma = np.clip(1.0 - (base_decay + wm_decay), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax on w row)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update (as specified)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform (interference)
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]

            # WM rehearsal/encoding
            if r > 0:
                # Move toward a one-hot code for the chosen action
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                eta = 1.0 - np.exp(-np.clip(wm_rehearsal, 0.0, 20.0))
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # Mild suppression of the chosen (incorrect) action
                suppress = 0.25 * (1.0 - load_factor)
                w[s, a] = (1.0 - suppress) * w[s, a]
                # Renormalize to sum to 1 to maintain probabilities
                w_sum = np.sum(w[s, :])
                if w_sum > 0:
                    w[s, :] /= w_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic WM retrieval with capacity law.

    Mechanism
    - RL: standard Rescorla–Wagner update (template).
    - WM: when a state-action receives reward, WM stores a near one-hot mapping.
      Retrieval succeeds with probability that decreases with set size according
      to a capacity law; on failed retrieval, the model falls back on RL.
    - Policy: mixture where WM weight equals the probability of successful WM retrieval.

    Parameters
    ----------
    model_parameters : list/tuple of 5 floats
        lr             : RL learning rate in (0,1].
        softmax_beta   : RL inverse temperature; scaled by *10 internally.
        K_effective    : Effective WM capacity (positive real; around 3–6).
        phi            : Capacity law exponent (>0). Larger => sharper drop with load.
        wm_noise       : WM imprecision (0=perfect one-hot; larger => noisier WM).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, K_effective, phi, wm_noise = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Probability of successful WM retrieval given load
        cap_ratio = np.clip(K_effective / max(1.0, nS), 0.0, 1.0)
        p_recall = np.clip(cap_ratio ** np.clip(phi, 0.01, 10.0), 0.0, 1.0)

        # Baseline decay for WM contents per trial, increasing with load
        gamma = np.clip(1.0 - 0.25 * (1.0 - cap_ratio), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with retrieval probability
            p_total = p_recall * p_wm + (1.0 - p_recall) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]

            # WM encoding: after reward, store a noisy one-hot; after no reward, slight suppression
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Noisy one-hot controlled by wm_noise
                eta = 1.0 / (1.0 + wm_noise)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # Penalize chosen action slightly
                w[s, a] *= (1.0 - 0.2 * (1.0 - cap_ratio))
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-based arbitration with load-dependent WM temperature.

    Mechanism
    - RL: standard Rescorla–Wagner update.
    - WM: fast associative map, decays with load. Its action distribution is
      computed with an effective temperature that worsens with load.
    - Arbitration: trial-wise WM weight increases with RL uncertainty (entropy)
      but is downscaled by load. Thus, when RL is uncertain and load is low,
      WM dominates; under high load, arbitration favors RL even if RL is uncertain.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr               : RL learning rate in (0,1].
        softmax_beta     : RL inverse temperature; scaled by *10 internally.
        wm_weight_base   : Max WM arbitration weight in [0,1].
        arb_slope        : Sensitivity of arbitration to RL uncertainty (>=0).
        wm_decay_base    : Baseline WM decay per trial in [0,1]; higher => faster decay.
        wm_temp_scale    : Load-driven WM noise scale (>=0); higher => lower WM precision at larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, arb_slope, wm_decay_base, wm_temp_scale = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50  # base; we'll compress it by load
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load modulation
        load_scale = 1.0 / (1.0 + wm_temp_scale * max(0, nS - 1))  # in (0,1]
        beta_wm_eff = max(1e-3, softmax_beta_wm * load_scale)

        # WM decay increases with load
        gamma = np.clip(1.0 - np.clip(wm_decay_base + (1.0 - load_scale) * 0.4, 0.0, 1.0), 0.0, 1.0)

        # RL softmax helper for entropy/uncertainty
        def softmax_probs(q_row, beta):
            z = q_row - np.max(q_row)
            p = np.exp(beta * z)
            p /= np.sum(p)
            return p

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy and uncertainty (entropy)
            # Compute full distribution to get entropy
            z = Q_s - np.max(Q_s)
            prl = np.exp(softmax_beta * z)
            prl /= np.sum(prl)
            p_rl = np.clip(prl[a], eps, 1.0)

            entropy = -np.sum(np.clip(prl, eps, 1.0) * np.log(np.clip(prl, eps, 1.0)))
            entropy_norm = entropy / np.log(nA)  # in [0,1]

            # WM policy with effective temperature
            z_wm = W_s - np.max(W_s)
            pwm = np.exp(beta_wm_eff * z_wm)
            pwm /= np.sum(pwm)
            p_wm = np.clip(pwm[a], eps, 1.0)

            # Arbitration: WM weight increases with RL uncertainty, downscaled by load
            wm_weight_unc = 1.0 / (1.0 + np.exp(-arb_slope * (entropy_norm - 0.5)))
            wm_weight_eff = np.clip(wm_weight_base * load_scale * wm_weight_unc, 0.0, 1.0)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = gamma * w[s, :] + (1.0 - gamma) * w_0[s, :]

            # WM encoding: potentiate chosen action on reward; mild suppression on no-reward
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Encoding strength benefits from low load
                eta = np.clip(0.8 * load_scale + 0.2, 0.0, 1.0)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # Reduce chosen action slightly; renormalize
                w[s, a] *= (1.0 - 0.15 * (1.0 - load_scale))
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p