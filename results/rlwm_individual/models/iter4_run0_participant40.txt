def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated time decay and exploration bonus (UCB-style).

    Mechanism:
    - Incremental model-free RL updates Q(s,a).
    - Between trials, Q-values decay toward a neutral prior due to memory leakage; decay is
      stronger for older adults and for larger set sizes.
    - Policy adds an uncertainty/exploration bonus that shrinks with experience via a
      1/sqrt(N) term; older adults and larger set sizes dampen this bonus.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If a<0, treated as lapse: uniform likelihood and no update.
    rewards : np.ndarray of float
        Feedback at each trial. Use r>=0 for learning; r<0 is ignored for updates.
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6).
    age : np.ndarray of float
        Participant age array; use age[0]. Age>=45 => older group.
    model_parameters : sequence of float
        [alpha, beta, decay_t, bonus0, epsilon]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax.
        - decay_t: base time-decay strength toward neutral prior (0..1 after logistic mapping inside).
        - bonus0: base exploration bonus scale for uncertainty (>=0).
        - epsilon: lapse probability: with prob epsilon, action is uniform random.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, decay_t, bonus0, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize Q and counts
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for UCB-like bonus

        # Neutral prior toward which Q decays
        q_prior = 0.0

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Compute per-trial decay rate: logistic-transformed base, then age+load scaling
            # Base in (0,1): stronger means faster forgetting
            base_decay = 1.0 / (1.0 + np.exp(-decay_t))
            # Older adults forget more; larger set sizes also increase forgetting
            age_mult = 1.25 if is_older else 1.0
            load_mult = 1.0 + 0.3 * max(0, ss - 3)
            decay_rate = np.clip(base_decay * age_mult * load_mult, 0.0, 1.0)

            # Apply decay toward prior before taking action
            Q[s, :] = (1.0 - decay_rate) * Q[s, :] + decay_rate * q_prior

            # Compute exploration bonus from uncertainty (count-based)
            # Smaller counts -> larger bonus. Age/load reduce bonus scale.
            bonus_scale = bonus0
            if is_older:
                bonus_scale *= 0.7
            if ss > 3:
                bonus_scale *= 0.7
            bonus = bonus_scale / np.sqrt(N[s, :] + 1.0)

            # Softmax over Q + bonus
            prefs = Q[s, :] + bonus
            prefs_center = prefs - np.max(prefs)
            p_soft = np.exp(beta * prefs_center)
            p_soft = p_soft / np.sum(p_soft)

            # Lapse mixture
            p = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)

            # Likelihood
            if 0 <= a < nA:
                total_logp += np.log(np.clip(p[a], 1e-12, 1.0))
            else:
                # invalid response treated as uniform emission; skip learning
                total_logp += np.log(1.0 / nA)
                continue

            # Update counts
            N[s, a] += 1.0

            # RL update if reward valid
            if r >= 0:
                Q[s, a] += alpha * (r - Q[s, a])

    return -float(total_logp)



def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL mixed with an age- and load-sensitive episodic cache (gated retrieval with drift).

    Mechanism:
    - Standard RL learns Q(s,a) incrementally.
    - Parallel episodic cache stores the most recently rewarded action for each state with
      a confidence value C(s). The cache is used with a gate probability that depends on
      C(s), reduced in older adults and with larger set sizes. Cache confidence drifts/decays
      over intervening trials; drift is stronger in older adults and at larger set sizes.
    - Policy mixes cache-derived choice distribution and RL softmax.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If a<0, treated as lapse: uniform likelihood and no update.
    rewards : np.ndarray of float
        Feedback at each trial. Use r>=0 for learning; r<0 is ignored for updates.
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6).
    age : np.ndarray of float
        Participant age array; use age[0]. Age>=45 => older group.
    model_parameters : sequence of float
        [alpha, beta, gate0, drift0, epsilon]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax.
        - gate0: base gate strength (logit space) controlling reliance on episodic cache.
        - drift0: base drift/decay rate of cache confidence per trial (logit space; mapped to (0,1)).
        - epsilon: lapse probability.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, gate0, drift0, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # Episodic cache: best action and confidence per state
        cache_a = -np.ones(nS, dtype=int)  # -1 indicates empty
        C = np.zeros(nS)  # confidence in [0,1]

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            prefs = q_s - np.max(q_s)
            p_rl = np.exp(beta * prefs)
            p_rl = p_rl / np.sum(p_rl)

            # Cache drift (confidence decays between uses); map drift0 with logistic to (0,1)
            base_drift = 1.0 / (1.0 + np.exp(-drift0))
            # Older and larger set size => stronger drift (more decay)
            drift = base_drift * (1.3 if is_older else 1.0) * (1.0 + 0.4 * max(0, ss - 3))
            drift = np.clip(drift, 0.0, 1.0)
            C[s] = (1.0 - drift) * C[s]

            # Cache-derived policy: if cache exists, put mass on cached action with sharpness proportional to C
            if cache_a[s] >= 0:
                sharp = 1.0 + 8.0 * np.clip(C[s], 0.0, 1.0)  # from near-uniform to sharp
                logits_cache = np.zeros(nA)
                logits_cache[cache_a[s]] = sharp
                logits_cache -= np.max(logits_cache)
                p_cache = np.exp(logits_cache)
                p_cache = p_cache / np.sum(p_cache)
            else:
                p_cache = np.ones(nA) / nA

            # Gate probability to use cache based on C, reduced by age and load
            gate_logit = gate0 + 3.0 * (C[s] - 0.5)
            if is_older:
                gate_logit -= 0.8
            if ss > 3:
                gate_logit -= 0.8
            w_cache = 1.0 / (1.0 + np.exp(-gate_logit))
            w_cache = np.clip(w_cache, 0.0, 1.0)

            # Mixture policy
            p_mix = w_cache * p_cache + (1.0 - w_cache) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            # Likelihood
            if 0 <= a < nA:
                total_logp += np.log(np.clip(p[a], 1e-12, 1.0))
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Learning updates when reward valid
            if r >= 0:
                # RL update
                Q[s, a] += alpha * (r - Q[s, a])

                # Cache update: on reward, store action and boost confidence; on no reward, slight decrease
                if r > 0.0:
                    cache_a[s] = a
                    C[s] = np.clip(C[s] + 0.6, 0.0, 1.0)
                else:
                    C[s] = np.clip(C[s] - 0.2, 0.0, 1.0)

    return -float(total_logp)



def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor–Critic with advantage-driven policy updates, eligibility decay, and age/load modulation.

    Mechanism:
    - Critic learns a state-value V(s).
    - Actor maintains action preferences H(s,a). After each choice, preferences are nudged
      in the direction of the advantage A = r - V(s). Preferences decay toward zero with
      a rate that depends on age and set size (older/larger -> faster decay).
    - The softmax policy is computed from H(s,·). This captures perseveration and win-stay/lose-shift
      tendencies via policy-gradient updates rather than direct value comparison.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If a<0, treated as lapse: uniform likelihood and no update.
    rewards : np.ndarray of float
        Feedback at each trial. Use r>=0 for learning; r<0 is ignored for updates.
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6).
    age : np.ndarray of float
        Participant age array; use age[0]. Age>=45 => older group.
    model_parameters : sequence of float
        [alpha_actor, alpha_critic, beta, trace_decay, epsilon]
        - alpha_actor: learning rate for actor preference updates.
        - alpha_critic: learning rate for critic V updates.
        - beta: inverse temperature for softmax over preferences.
        - trace_decay: base decay of actor preferences toward zero (logit-mapped to 0..1 inside).
        - epsilon: lapse probability.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    alpha_actor, alpha_critic, beta, trace_decay, epsilon = model_parameters
    age_val = float(age[0])
    is_older = age_val >= 45.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize actor preferences and critic values
        H = np.zeros((nS, nA))
        V = np.zeros(nS)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Preference decay toward zero (forgetting), stronger for older and larger set sizes
            base_decay = 1.0 / (1.0 + np.exp(-trace_decay))  # (0,1)
            decay = base_decay * (1.2 if is_older else 1.0) * (1.0 + 0.3 * max(0, ss - 3))
            decay = np.clip(decay, 0.0, 1.0)
            H[s, :] *= (1.0 - decay)

            # Policy from preferences
            logits = H[s, :] - np.max(H[s, :])
            p_pol = np.exp(beta * logits)
            p_pol = p_pol / np.sum(p_pol)

            p = (1.0 - epsilon) * p_pol + epsilon * (1.0 / nA)

            # Likelihood
            if 0 <= a < nA:
                total_logp += np.log(np.clip(p[a], 1e-12, 1.0))
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Learning if reward is valid
            if r >= 0:
                # Advantage signal
                A = r - V[s]

                # Critic update
                V[s] += alpha_critic * A

                # Actor policy-gradient style update:
                # Increase chosen action preference, decrease others proportional to their prob.
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                grad = onehot - p_pol  # baseline-removed gradient
                H[s, :] += alpha_actor * A * grad

    return -float(total_logp)