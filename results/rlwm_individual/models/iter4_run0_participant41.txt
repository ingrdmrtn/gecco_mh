def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited WM with reward-gated storage and decay; RL delta-rule.

    Idea:
    - RL: standard delta-rule.
    - WM: when rewarded, store the chosen action as a one-hot memory for that state (supervised); 
      otherwise softly decay toward uniform. WM traces also decay every trial.
    - Capacity limit: the contribution of WM is scaled by min(1, K / nS), mimicking slot-like capacity
      such that larger set sizes reduce WM influence.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline weight of WM contribution (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 to span a wide range.
    - K: WM capacity in number of items (>=0); effective weight scales as min(1, K/nS).
    - wm_decay: per-trial decay of WM toward uniform (0..1), higher means faster decay.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0  # RL inverse temperature with higher upper bound
    softmax_beta_wm = 50.0  # near-deterministic WM readout
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity scaling for this block
        cap_scale = min(1.0, float(K) / max(1.0, float(nS)))
        wm_avail = np.clip(wm_weight * cap_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over the WM weights for the current state
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_avail * p_wm + (1.0 - wm_avail) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - Positive feedback: store as one-hot (supervised)
            # - Negative feedback: gentle drift toward uniform
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # drift toward uniform when not rewarded
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Global per-trial decay of all WM traces toward uniform
            decay_all = 0.02  # small passive decay per trial
            w = (1.0 - decay_all) * w + decay_all * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Entropy-gated arbitration, asymmetric RL learning rates, and set-size WM noise.

    Idea:
    - RL: asymmetric delta-rule with separate learning rates for positive vs. negative outcomes.
    - WM: stores last rewarded action as a one-hot; otherwise drifts toward uniform.
    - WM noise: retrieval is corrupted toward uniform more strongly in larger set sizes.
    - Arbitration: WM weight is gated by RL certainty using the entropy of the RL policy;
      lower entropy (more certain Q) allows higher WM reliance (trusting consistent mappings).

    Parameters (tuple):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: baseline WM mixture weight (0..1).
    - wm_noise_base: base WM noise level (0..1); effective noise scales up with set size.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_noise_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy and its entropy for arbitration
            Q_s = q[s, :]
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi = np.exp(logits) / np.sum(np.exp(logits))
            p_rl = pi[a]

            # RL entropy (0..log(nA)); normalize to 0..1
            entropy = -np.sum(pi * (np.log(pi + eps)))
            entropy_norm = entropy / np.log(nA)

            # WM retrieval with set-size-dependent noise toward uniform
            W_s = w[s, :].copy()
            noise = np.clip(wm_noise_base * max(0.0, (nS - 3.0) / 3.0), 0.0, 1.0)
            W_eff = (1.0 - noise) * W_s + noise * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Arbitration: more certain RL (low entropy) increases WM reliance
            wm_weight = np.clip(wm_weight_base * (1.0 - entropy_norm), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetry
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0.0 else lr_neg
            q[s][a] += lr_use * delta

            # WM update: store on reward, drift on no reward
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                lam = 0.05
                w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

            # Mild passive decay of WM to account for time-based forgetting
            lam_all = 0.02
            w = (1.0 - lam_all) * w + lam_all * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Fast WM-like supervised learner plus slow RL, with set-size gating and response stickiness.

    Idea:
    - RL (slow): standard delta-rule capturing gradual value learning.
    - WM (fast): supervised delta to push the chosen action's value toward reward target:
        target = 1 for rewarded action, else 0 for non-reward; remaining mass normalized.
      This creates a rapid, but noisy, mapping akin to WM.
    - Stickiness: WM readout includes a bias toward repeating the previous action in the block.
    - Set-size gating: WM mixture weight is reduced as set size increases via a gating factor.

    Parameters (tuple):
    - lr_slow: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: baseline WM mixture weight (0..1).
    - wm_fast: WM supervised learning rate (0..1).
    - stickiness: additive WM bias for repeating previous action (>=0).
    - size_gate: strength of set-size gating (>=0); higher reduces WM reliance more in larger sets.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_slow, softmax_beta, wm_weight_base, wm_fast, stickiness, size_gate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Previous action tracker for stickiness (per block)
        prev_action = None

        # Set-size gating of WM weight
        gate = 1.0 / (1.0 + size_gate * max(0.0, (nS - 3.0) / 3.0))
        wm_weight = np.clip(wm_weight_base * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with stickiness bias on readout
            U = w[s, :].copy()
            if prev_action is not None:
                U[prev_action] += stickiness
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (U - U[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update (slow)
            delta = r - Q_s[a]
            q[s][a] += lr_slow * delta

            # WM supervised update (fast):
            # Move chosen action toward reward target and renormalize
            target = 1.0 if r > 0.5 else 0.0
            w[s, a] = (1.0 - wm_fast) * w[s, a] + wm_fast * target
            # Distribute remaining mass across other actions to keep a probability vector
            remaining = max(0.0, 1.0 - w[s, a])
            others = [aa for aa in range(nA) if aa != a]
            if len(others) > 0:
                share = remaining / len(others)
                for aa in others:
                    w[s, aa] = share
            # Mild passive decay across all states to prevent over-commitment
            lam_all = 0.01
            w = (1.0 - lam_all) * w + lam_all * w_0

            prev_action = a

        blocks_log_p += log_p

    return -blocks_log_p