def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age-modulated decay and load arbitration.

    Idea:
    - RL learns state-action values with softmax choice.
    - WM stores recently rewarded state-action mappings as near-deterministic, but decays toward uniform.
    - Arbitration weight for WM is proportional to effective capacity relative to load, with age reducing
      effective capacity and increasing decay.
    - Older adults (age>=45) show lower effective WM capacity and stronger decay, thus rely more on RL,
      especially under high load (nS=6).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 for a wider range.
    - wm_capacity: Baseline WM capacity (in "items"; >0). Arbitration scales with wm_capacity / nS.
    - wm_strength: Baseline WM arbitration gain (0..1) that scales the capacity ratio.
    - wm_decay_age: Base WM decay factor (0..1), further increased for older adults.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays describing the task.
    - age is an array; age[0] is used to determine age group (older if >=45).
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, softmax_beta, wm_capacity, wm_strength, wm_decay_age = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM capacity and decay for this block
        # Age reduces capacity linearly; higher nS increases load and reduces arbitration
        K_eff = max(0.0, wm_capacity - 0.8 * is_older)  # older adults lose ~0.8 items of capacity
        cap_ratio = min(1.0, K_eff / max(1.0, float(nS)))  # between 0 and 1

        # WM decay is higher for older adults
        wm_decay = np.clip(wm_decay_age + 0.2 * is_older, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            # RL policy: p(a|s)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: near-deterministic based on w[s]
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: WM weight scales with capacity ratio and a baseline strength
            wm_weight_t = np.clip(wm_strength * cap_ratio, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding on rewarded trials: store a one-hot association for (s,a)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Strong overwrite on reward; capacity is implicit in arbitration, not encoding strength
                enc = 0.9
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                # On error, nudge toward uniform for that state (no commitment)
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recency-gated WM arbitration with age-modulated recency sensitivity.

    Idea:
    - RL learns values with softmax choice.
    - WM stores recent state-action info; near-deterministic but subject to decay.
    - Arbitration weight depends on how recently the state was seen (state-specific recency trace):
      when a state was just seen, WM is more diagnostic; when it hasn't appeared recently, WM is less useful.
    - Older adults have reduced sensitivity to recency (weaker gating) and slightly higher decay,
      and larger set sizes naturally reduce recency (longer intervals between repeats).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - recency_scale: Strength of WM gating by state recency (>=0).
    - wm_bias: Baseline bias toward WM use (can be negative).
    - age_recency_drop: Reduction in recency-based gating for older adults (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays describing the task.
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, recency_scale, wm_bias, age_recency_drop = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific recency traces (0..1), higher when state was seen very recently
        recency = np.zeros(nS)
        # Recency decay per trial; larger nS implies longer gaps -> effectively lower recency
        base_decay = 0.6 + 0.3 * (nS / 6.0)  # ~0.6 for nS=3, ~0.9 for nS=6
        base_decay = np.clip(base_decay, 0.0, 1.0)
        # WM decay slightly higher for older adults
        wm_decay = np.clip(0.1 + 0.15 * is_older, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Recency-based arbitration with age penalty and load via recency dynamics
            rec = recency[s]
            gate_input = wm_bias + recency_scale * rec - age_recency_drop * is_older
            wm_weight_t = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM global decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding: reinforce chosen action more when rewarded, mild when not
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * target
            else:
                # Small nudge toward the chosen action to reflect short-lived echoic WM
                leak = 0.1
                w[s, :] = (1.0 - leak) * w[s, :]
                w[s, a] += leak
                # Renormalize to probabilities
                w[s, :] = np.clip(w[s, :], 0.0, None)
                if w[s, :].sum() > 0:
                    w[s, :] /= w[s, :].sum()
                else:
                    w[s, :] = w_0[s, :]

            # Update recency traces: decay all, set current state to 1
            recency = (1.0 - base_decay) * recency
            recency[s] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + reliability-based arbitration with age- and load-modulated lapse.

    Idea:
    - RL learns with softmax choice.
    - WM stores rewarded associations; its internal confidence is the margin between best and second-best.
    - Arbitration weight is based on relative reliabilities of WM vs RL:
        * WM reliability ~ confidence in WM distribution at state s.
        * RL reliability ~ inverse of running outcome variance for state s (using squared PE).
    - A lapse process mixes a small uniform choice probability, increasing with age and with load (nS),
      reflecting greater nonspecific noise for older adults and under high cognitive load.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_eta: WM encoding strength on reward (0..1).
    - rel_temp: Temperature scaling for reliability comparison (>=0).
    - lapse_age: Baseline lapse rate contribution for older adults (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays describing the task.
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_eta, rel_temp, lapse_age = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running estimate of RL uncertainty per state using squared PE (EWMA)
        rl_var = 0.25 * np.ones(nS)  # start moderately uncertain
        var_decay = 0.8  # EWMA decay for variance

        # WM decay modest; older adults forget faster
        wm_decay = np.clip(0.15 + 0.15 * is_older, 0.0, 1.0)

        # Lapse rate increases with age and load
        lapse = np.clip(0.01 + 0.04 * is_older + 0.02 * (nS - 3) / 3.0 + 0.5 * lapse_age * is_older, 0.0, 0.3)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Compute reliabilities
            # WM confidence: margin between top and second-best probability in W_s
            sorted_W = np.sort(W_s)[::-1]
            wm_conf = max(0.0, sorted_W[0] - (sorted_W[1] if len(sorted_W) > 1 else 0.0))
            wm_rel = wm_conf

            # RL reliability: inverse of estimated variance for this state
            rl_rel = 1.0 / max(rl_var[s], 1e-6)

            # Soft arbitration by relative reliabilities
            # wm_weight_t = softmax over (rel_temp * reliabilities)
            x_wm = rel_temp * wm_rel
            x_rl = rel_temp * rl_rel
            # Numerically stable 2-class softmax
            max_x = max(x_wm, x_rl)
            exp_wm = np.exp(x_wm - max_x)
            exp_rl = np.exp(x_rl - max_x)
            wm_weight_t = exp_wm / max(exp_wm + exp_rl, eps)
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Mixture policy with lapse to uniform
            p_mix = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))

            # RL update and variance tracking
            pe = r - q[s, a]
            q[s, a] += lr * pe
            rl_var[s] = var_decay * rl_var[s] + (1.0 - var_decay) * (pe ** 2)

            # WM global decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding on reward with strength wm_eta
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                enc = np.clip(wm_eta, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                # Slight unlearning on error for that state
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p