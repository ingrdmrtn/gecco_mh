def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Set-size–scaled WM temperature and exponentially load-attenuated arbitration.

    Idea:
      - RL: standard Q-learning with single learning rate.
      - WM: stores rewarded actions with decay toward uniform; selection is near-deterministic,
            but effective WM temperature cools with smaller sets and heats with larger sets.
      - Arbitration: mixture weight for WM decays exponentially with set size (capacity limit).

    Parameters:
      lr               : RL learning rate (0..1).
      wm_weight_base   : Base WM weight when set size is minimal (nS=3), before attenuation (0..1).
      softmax_beta     : RL inverse temperature; scaled internally by 10.
      alpha_load       : Attenuation slope for WM weight as set size increases (>=0).
                         Effective WM weight = wm_weight_base * exp(-alpha_load*(nS-3)).
      wm_decay         : WM decay toward uniform on each visit (0..1). Higher means faster forgetting.

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, alpha_load, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base deterministic WM inverse temperature
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–dependent arbitration and WM temperature scaling
        wm_weight_eff = np.clip(wm_weight_base * np.exp(-max(0.0, alpha_load) * max(0, nS - 3)), 0.0, 1.0)
        wm_temp_scale = 1.0 / max(1.0, (nS - 2))  # cooler for small sets, hotter for large sets
        beta_wm_eff = softmax_beta_wm * wm_temp_scale

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (wm_logits - wm_logits[a])))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform within-state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Reward-gated storage: push probability mass to chosen action if rewarded
            if r > 0:
                # move a fraction equal to wm_decay from others to the chosen action
                transfer = wm_decay * (1.0 - w[s, a])
                w[s, a] += transfer
                # renormalize strictly
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Entropy-based arbitration between WM and RL with leaky WM and adjustable WM gain.

    Idea:
      - RL: standard Q-learning (single learning rate).
      - WM: leaky store with reward-gated strengthening; WM softmax temperature adjustable.
      - Arbitration: WM weight adapts by the uncertainty of WM at the current state.
        Specifically, higher entropy of the WM distribution reduces WM reliance.
      - Set size acts implicitly by increasing WM entropy early in larger sets.

    Parameters:
      lr           : RL learning rate (0..1).
      softmax_beta : RL inverse temperature; scaled internally by 10.
      wm_base      : Base WM weight (0..1) when WM is perfectly certain.
      ent_slope    : Sensitivity of arbitration to WM entropy (>=0). Larger reduces WM weight when entropy is high.
      wm_leak      : WM leak toward uniform on each visit (0..1).
      wm_gain      : Reward-gated storage strength into WM (0..1).

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_base, ent_slope, wm_leak, wm_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (can be softened by entropy-based arbitration)
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Entropy of WM for state s
            W_s_safe = np.clip(W_s, eps, None)
            W_s_safe = W_s_safe / np.sum(W_s_safe)
            entropy = -np.sum(W_s_safe * np.log(W_s_safe + eps)) / np.log(nA)  # normalized 0..1

            # Arbitration weight decreases with entropy
            wm_weight_eff = np.clip(wm_base * np.exp(-max(0.0, ent_slope) * entropy), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            # Reward-dependent strengthening
            if r > 0:
                # Concentrate WM on chosen action with strength wm_gain
                w[s, :] = (1.0 - wm_gain) * w[s, :]
                w[s, a] += wm_gain
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Reward-rate–gated WM arbitration with asymmetric RL learning and WM decay.

    Idea:
      - RL: Q-learning with asymmetric learning rates for positive vs. negative prediction errors.
      - WM: leaky trace updated on reward; acts as a near-deterministic policy.
      - Arbitration: WM weight is gated by the running reward rate within the block (higher reward -> rely on WM),
                    and penalized by set size (implicit load effect).
        wm_weight = sigmoid(gate_bias + gate_slope*(reward_rate - 0.5) - gate_slope*((nS-3)/3))

    Parameters:
      lr_pos       : RL learning rate for positive prediction errors (0..1).
      lr_neg       : RL learning rate for negative prediction errors (0..1).
      softmax_beta : RL inverse temperature; scaled internally by 10.
      gate_bias    : Bias term of WM gate (real-valued).
      gate_slope   : Gain/slope of WM gate with respect to reward rate and set size (>=0).
      wm_decay     : WM decay toward uniform (0..1).

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, gate_bias, gate_slope, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Running reward rate estimate (incremental average)
        rew_rate = 0.5  # start neutral
        count = 0.0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Reward-rate–gated arbitration with set-size penalty
            # effective input to sigmoid
            set_penalty = (max(0, nS - 3) / 3.0)  # 0 at nS=3, 1 at nS=6
            gate_input = gate_bias + max(0.0, gate_slope) * (rew_rate - 0.5 - set_penalty)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr_use * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Rewarded actions are strengthened in WM
            if r > 0:
                # Shift a wm_decay fraction toward the chosen action
                mass = wm_decay * (1.0 - w[s, a])
                w[s, a] += mass
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

            # Update running reward rate
            count += 1.0
            rew_rate += (r - rew_rate) / count

        blocks_log_p += log_p

    return -blocks_log_p