def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + WM with asymmetric RL learning and set-size–dependent WM forgetting.

    Mechanism:
    - RL: tabular Q-learning with separate learning rates for positive vs. negative prediction errors.
    - WM: one-shot storage of rewarded state-action associations; passive forgetting increases with set size.
    - Arbitration: fixed WM mixture weight; WM reliability is effectively reduced at larger set sizes via stronger forgetting.

    Parameters:
    - lr_pos: learning rate for positive PE (0..1)
    - lr_neg: learning rate for negative PE (0..1), allows asymmetric learning
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_weight_base: baseline contribution of WM to choice (0..1)
    - wm_forget_base: base forgetting rate per trial (0..1); effective forgetting scales up with set size

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight_base, wm_forget_base = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent WM forgetting (higher nS -> larger per-trial forgetting)
        wm_forget = 1.0 - (1.0 - wm_forget_base) ** max(1, (nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration with fixed WM weight (forgetting already embeds load effects)
            wm_weight = wm_weight_base
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL learning with asymmetric learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr * delta

            # WM forgetting towards baseline
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            # WM update on rewarded outcomes (one-shot)
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-weighted arbitration with lapse increasing by set size.

    Mechanism:
    - RL: tabular Q-learning with single learning rate.
    - WM: one-shot, with mild built-in decay (small, set-size–dependent, no extra param).
    - Arbitration: weight WM vs RL according to their relative certainties on the current state.
      Certainty is 1 - normalized entropy of each system’s policy on that state.
    - Lapse: set-size–dependent lapse increases with load, mixing in a uniform policy.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_base: baseline WM influence (0..1) scaling the uncertainty arbitration
    - uncert_sensitivity: scales how strongly relative certainty (WM vs RL) shifts weight (>=0)
    - lapse0: baseline lapse probability (0..1)
    - lapse_slope: added lapse per unit increase in set size (>=0)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_base, uncert_sensitivity, lapse0, lapse_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Lapse increases with set size (capped at 0.49 to keep mixture valid)
        lapse = min(0.49, max(0.0, lapse0 + lapse_slope * (nS - 1)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute normalized entropies for arbitration
            # RL policy distribution
            prl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl /= np.sum(prl)
            ent_rl = -np.sum(prl * (np.log(prl + 1e-15)))
            ent_rl /= np.log(nA + 1e-15)
            conf_rl = 1.0 - ent_rl  # 0..1

            # WM policy distribution
            pwm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm /= np.sum(pwm)
            ent_wm = -np.sum(pwm * (np.log(pwm + 1e-15)))
            ent_wm /= np.log(nA + 1e-15)
            conf_wm = 1.0 - ent_wm  # 0..1

            # Uncertainty-weighted arbitration
            rel_conf = conf_wm - conf_rl
            wm_weight = wm_base / (1.0 + np.exp(-uncert_sensitivity * rel_conf))

            # Mix WM and RL, then apply lapse to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Mild WM decay toward baseline; slightly stronger for larger nS (no extra param)
            wm_decay = min(0.2, 0.03 * nS)
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM one-shot reinforcement on reward
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-thresholded WM gating with interference across states.

    Mechanism:
    - RL: tabular Q-learning with single learning rate.
    - WM: one-shot storage for rewarded pairs with cross-state interference that increases with set size.
    - Arbitration: WM mixture weight is gated by a sigmoidal function of set size, with threshold and slope parameters.
      This approximates a soft capacity limit: high WM influence when nS < theta, decreasing as nS exceeds theta.

    Parameters:
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_weight: maximum WM contribution to choice (0..1)
    - theta: set-size threshold for WM dominance (real-valued)
    - slope: steepness of the WM gating function (>=0)
    - interference: cross-state interference strength applied after WM updates (0..1)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight, theta, slope, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM gating as a function of set size (higher nS -> lower weight if theta is small)
        wm_gate = wm_weight / (1.0 + np.exp(slope * (nS - theta)))

        # Interference increases with set size (effective over-block strength)
        inter_eff = 1.0 - (1.0 - interference) ** max(1, (nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mix with set-size–gated WM weight
            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reinforce rewarded mapping
            if r > 0.0:
                # Set current state to one-hot for chosen action
                w[s, :] = 0.0
                w[s, a] = 1.0

                # Apply cross-state interference: push other states toward less discriminable codes
                # Here we pull other rows slightly toward uniform, eroding distinctiveness.
                for s2 in range(nS):
                    if s2 == s:
                        continue
                    w[s2, :] = (1.0 - inter_eff) * w[s2, :] + inter_eff * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p