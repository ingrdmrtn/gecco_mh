def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + one-shot working-memory recall with age- and load-dependent gating,
    plus a small lapse. WM stores the most recently rewarded action per state and,
    when recalled, chooses it deterministically; otherwise, RL softmax is used.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning and memory reset at block boundaries.
    set_sizes : array-like of int
        Set size per trial (3 or 6); assumed constant within a block.
    age : array-like or scalar
        Participant age; older (>=45) reduces WM recall gate.
    model_parameters : sequence of 6 floats
        alpha         : RL learning rate (0..1)
        beta          : Inverse temperature for RL softmax (scaled by 10 internally)
        recall_bias   : Intercept for WM recall probability (logit space)
        load_weight   : Sensitivity of WM recall to load (multiplied by 3 - set_size)
        age_penalty   : Reduction in WM recall weight for older adults (>=45)
        lapse         : Lapse probability mixing uniform policy

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, recall_bias, load_weight, age_penalty, lapse = model_parameters
    beta = beta * 10.0
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # RL values and WM memory per state
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM: for each state, store if we have a "cached correct action" and which one
        wm_has = np.zeros(nS, dtype=np.int32)      # 1 if a rewarded action is stored
        wm_act = np.zeros(nS, dtype=np.int32)      # which action is stored

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax policy
            prefs = Q[s, :].copy()
            prefs -= np.max(prefs)
            p_rl = np.exp(beta * prefs)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM policy (deterministic towards cached action if present)
            if wm_has[s] == 1:
                p_wm = np.zeros(nA)
                p_wm[int(wm_act[s])] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # Age- and load-dependent recall gate (probability WM is used)
            # Higher for smaller set size; reduced by age_penalty for older
            load_term = load_weight * (3.0 - float(nS))  # positive for set size=3 if load_weight>0
            z = recall_bias + load_term - age_penalty * is_older
            gate = 1.0 / (1.0 + np.exp(-z))
            gate = min(max(gate, 0.0), 1.0)

            # Mixture with lapse
            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (np.ones(nA) / nA)

            p_a = float(p_final[a])
            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: store the action if it was rewarded; if not rewarded, do not overwrite
            if r > 0.0:
                wm_has[s] = 1
                wm_act[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Volatility-adaptive RL with age/load-dependent exploration and perseveration.
    The learning rate increases with recent absolute prediction error (volatility proxy).
    Older age and larger set sizes reduce inverse temperature (increase exploration).
    A perseveration bias favors repeating the last chosen action.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6); assumed constant within block.
    age : array-like or scalar
        Participant age; older (>=45) increases exploration (reduces beta).
    model_parameters : sequence of 6 floats
        alpha0          : Base RL learning rate (0..1)
        k_vol           : Volatility integration rate (0..1) for abs(PE)
        beta0           : Base inverse temperature (scaled by 10 internally)
        age_beta_drop   : Beta reduction factor for older adults
        load_beta_drop  : Beta reduction factor per unit load (set_size-3)
        phi             : Perseveration bias added to last chosen action's preference

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha0, k_vol, beta0, age_beta_drop, load_beta_drop, phi = model_parameters
    beta0 = beta0 * 10.0
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        v = 0.0  # volatility proxy (running abs PE)
        last_action = -1

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Effective learning rate increases with volatility
            alpha_t = alpha0 * (1.0 + v)
            alpha_t = max(0.0, min(alpha_t, 1.0))

            # Effective beta reduced by age and load
            load_units = max(0.0, float(nS) - 3.0)  # 0 for 3, 3 for 6
            beta_eff = beta0 / (1.0 + age_beta_drop * is_older + load_beta_drop * load_units)
            beta_eff = max(1e-3, beta_eff)

            # Preferences with perseveration
            prefs = Q[s, :].copy()
            if last_action >= 0:
                prefs[last_action] += phi
            prefs -= np.max(prefs)

            p = np.exp(beta_eff * prefs)
            p = p / (np.sum(p) + eps)

            p_a = float(p[a])
            total_log_p += np.log(max(p_a, eps))

            # Update
            pe = r - Q[s, a]
            Q[s, a] += alpha_t * pe
            v = (1.0 - k_vol) * v + k_vol * abs(pe)

            last_action = a

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL combined with a decaying Bayesian WM estimate and uncertainty-based arbitration.
    WM stores decaying success/failure counts per state-action; RL tracks Q-values.
    Arbitration weight increases when WM is confident and RL is uncertain.
    Older age increases WM decay and shifts arbitration away from WM.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within each block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Reward per trial (0 or 1).
    blocks : array-like of int
        Block ID per trial; learning resets per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6); assumed constant within block.
    age : array-like or scalar
        Participant age; older (>=45) increases WM decay and reduces WM arbitration.
    model_parameters : sequence of 6 floats
        alpha        : RL learning rate (0..1)
        beta         : Shared inverse temperature for both systems (scaled by 10 internally)
        wm_decay     : Base decay rate (0..1) for WM counts per trial
        age_wm_drop  : Additional WM decay for older adults (>=45)
        arb_bias     : Intercept for arbitration gate (logit space)
        wm_weight    : Sensitivity of gate to WM confidence vs RL uncertainty

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, wm_decay, age_wm_drop, arb_bias, wm_weight = model_parameters
    beta = beta * 10.0
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM decaying counts (successes and trials) per state-action
        succ = np.ones((nS, nA)) * 0.5  # small prior to avoid zeros
        total = np.ones((nS, nA)) * 1.0

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # WM estimates: mean and confidence
            p_hat = succ[s, :] / np.maximum(total[s, :], 1e-8)
            # Confidence as total evidence magnitude (larger total => more confident)
            wm_conf = np.sum(total[s, :])

            # RL uncertainty proxy: dispersion of Q (low variance = certain). Use inverse entropy-like.
            q = Q[s, :]
            q_centered = q - np.mean(q)
            rl_uncert = 1.0 / (np.var(q_centered) + 1e-6)  # higher when values are similar -> uncertain

            # Normalize constructs
            wm_conf_norm = wm_conf / (wm_conf + 3.0)  # 0..1 increasing with evidence
            rl_uncert_norm = 1.0 / (1.0 + rl_uncert)  # 0..1 increasing with certainty; invert to uncertainty
            rl_uncert_norm = 1.0 - rl_uncert_norm     # now 0..1 uncertainty

            # Policies
            # RL softmax
            prefs_rl = q.copy()
            prefs_rl -= np.max(prefs_rl)
            p_rl = np.exp(beta * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM softmax over p_hat
            prefs_wm = p_hat.copy()
            prefs_wm -= np.max(prefs_wm)
            p_wm = np.exp(beta * prefs_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Arbitration: increase WM weight when WM_conf high and RL uncertainty high.
            # Age reduces WM weight via increased decay and via gate shift.
            signal = wm_conf_norm + rl_uncert_norm
            z = arb_bias + wm_weight * signal - 0.5 * is_older
            gate = 1.0 / (1.0 + np.exp(-z))
            gate = min(max(gate, 0.0), 1.0)

            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            p_a = float(p_mix[a])
            total_log_p += np.log(max(p_a, eps))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay and update (older increases decay)
            decay_eff = min(1.0, max(0.0, wm_decay + age_wm_drop * is_older))
            succ *= (1.0 - decay_eff)
            total *= (1.0 - decay_eff)
            succ[s, a] += r
            total[s, a] += 1.0

    return -float(total_log_p)