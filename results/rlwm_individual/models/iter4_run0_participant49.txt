def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, PE-gated Working Memory (WM).

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM contribution is capacity-limited: its effective weight scales as min(1, C / nS).
    - WM storage is gated by prediction error magnitude: larger surprise -> stronger encoding.
    - WM decays toward a uniform baseline within each state.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_mix: float in [0,1], baseline WM mixture weight before capacity scaling.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_capacity: float >= 0, WM capacity measured in number of state-action pairs.
                   Effective WM weight is scaled by min(1, wm_capacity / nS).
    - pe_gate: float >= 0, PE magnitude threshold for WM storage; higher -> less storage.
    - wm_forget: float in [0,1], WM decay toward uniform on each state encounter.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_mix, softmax_beta, wm_capacity, pe_gate, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Capacity-adjusted WM weight for this block
        cap_scale = min(1.0, max(0.0, wm_capacity) / max(1.0, float(nS)))
        wm_weight_eff_block = np.clip(wm_mix * cap_scale, 0.0, 1.0)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # Working memory policy: softmax over WM weights
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform for this state
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # PE-gated WM storage: larger |delta| relative to threshold -> stronger overwrite
            gate_strength = max(0.0, (abs(delta) - pe_gate)) / (abs(delta) + 1e-12) if abs(delta) > 0 else 0.0
            gate_strength = np.clip(gate_strength, 0.0, 1.0)

            if gate_strength > 0.0:
                # Move WM toward a one-hot on the chosen action proportional to gate_strength
                w[s, :] = (1.0 - gate_strength) * w[s, :] + gate_strength * w_0[s, :]
                w[s, a] += gate_strength
                # Re-normalize (numerical safety)
                w[s, :] = np.clip(w[s, :], 1e-8, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with entropy-sensitive exploration + leaky WM with reward-contingent storage.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - RL softmax inverse temperature is down-regulated by state-level uncertainty (entropy).
      When Q_s is flat (high entropy), exploration increases.
    - WM decays toward uniform each encounter; when rewarded, WM stores the chosen action
      with strength eta_store; when not rewarded, only decay happens.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_mix: float in [0,1], mixture weight for WM.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - xi_unc: float >= 0, sensitivity of RL exploration to Q-entropy; higher => more exploration under uncertainty.
              Effective beta: softmax_beta / (1 + xi_unc * H(Q_s)), where H is normalized entropy in [0,1].
    - wm_leak: float in [0,1], leak of WM toward uniform per state encounter.
    - eta_store: float in [0,1], strength of WM storage toward the chosen action when r=1.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_mix, softmax_beta, xi_unc, wm_leak, eta_store = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Entropy of softmax over Q_s at base beta=1 (scale-invariant approximation)
            q_probs = np.exp(Q_s - np.max(Q_s))
            q_probs = q_probs / np.sum(q_probs)
            H = -np.sum(q_probs * np.log(np.clip(q_probs, 1e-12, 1.0))) / np.log(nA)  # normalized to [0,1]

            beta_eff = softmax_beta / (1.0 + xi_unc * H)

            # RL policy
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture policy
            wm_weight_eff = np.clip(wm_mix, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # Reward-contingent WM storage
            if r > 0.0 and eta_store > 0.0:
                # Move W_s toward a one-hot on action a by eta_store
                w[s, :] = (1.0 - eta_store) * w[s, :]
                w[s, a] += eta_store
                # Normalize
                w[s, :] = np.clip(w[s, :], 1e-8, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-based arbitration and load-dependent lapses.

    Idea:
    - Arbitration: trial-wise WM vs RL weight is determined by their relative confidence.
      Confidence is the max softmax probability from each system. The difference is
      passed through a sigmoid (temperature conf_temp) and scaled by wm_strength.
    - Load-dependent lapses: larger set sizes induce more uniform lapses (epsilon),
      mixing the final policy with a uniform distribution.
    - WM decays toward uniform and stores rewarded actions.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_strength: float in [0,1], maximum WM influence under high WM confidence.
    - softmax_beta: float >= 0, RL inverse temperature; internally scaled by 10.
    - conf_temp: float >= 0, sharpness of arbitration based on confidence difference.
    - load_lapse: float >= 0, lapse growth per +3 items; epsilon = min(0.25, load_lapse * max(0, (nS-3)/3)).
    - wm_lambda: float in [0,1], WM decay toward uniform per state encounter.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_strength, softmax_beta, conf_temp, load_lapse, wm_lambda = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent lapse epsilon for this block
        eps = min(0.25, max(0.0, load_lapse) * max(0.0, (nS - 3) / 3.0))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy and confidence
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            p_rl = rl_probs[a]
            conf_rl = np.max(rl_probs)

            # WM policy and confidence
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            wm_probs = np.exp(wm_logits)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]
            conf_wm = np.max(wm_probs)

            # Confidence-based arbitration
            conf_diff = conf_wm - conf_rl  # positive -> favor WM
            if conf_temp > 0.0:
                # Sigmoid mapping in [0,1]
                arb = 1.0 / (1.0 + np.exp(-conf_temp * conf_diff))
            else:
                arb = 0.5
            wm_weight_eff = np.clip(wm_strength * arb, 0.0, 1.0)

            # Mixture policy
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Load-dependent lapse to uniform
            p_total = (1.0 - eps) * p_mix + eps * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay and reward-based storage
            w[s, :] = (1.0 - wm_lambda) * w[s, :] + wm_lambda * w_0[s, :]
            if r > 0.0:
                # Store deterministically toward chosen action
                w[s, :] = 0.0 * w[s, :] + w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p