Here are three standalone cognitive models that follow the requested template and constraints. Each returns the negative log-likelihood of the observed choices and uses all parameters meaningfully, with set-size-dependent WM effects.

Note: Assume numpy as np is already imported by the environment.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Cached-WM with interference-based forgetting, lapses, and state-wise perseveration.

    Mechanism
    - RL: slow delta-rule Q-values with softmax.
    - WM: caches the most recently rewarded action per state as a sharp distribution.
      WM decays toward uniform with a set-size-dependent interference rate.
      WM policy includes a lapse to uniform and a perseveration bonus on the last chosen action in the state.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Base mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - interference: float in [0,1]
            Base WM interference/forgetting rate; effective decay increases with set size.
        - lapse: float in [0,1]
            Lapse rate within WM policy mixing with uniform.
        - perseveration: float >= 0
            Additive bonus to WM preferences for the last chosen action in the same state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, interference, lapse, perseveration = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # WM policy-like table; will hold peaked caches
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for perseveration bias
        last_a = -np.ones(nS, dtype=int)

        # Set-size dependent decay (more interference at larger nS)
        decay = np.clip(interference * (nS - 1) / max(nS, 1), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # Start from current WM preferences for this state
            prefs = w[s, :].copy()

            # Add perseveration bonus to last chosen action in this state
            if last_a[s] >= 0:
                prefs[last_a[s]] += perseveration

            # Deterministic WM softmax then mix with lapse
            prefs_centered = prefs - np.mean(prefs)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_centered, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm_vec = (1 - lapse) * p_wm_vec + lapse * (1.0 / nA)

            # Probability assigned to the observed action
            p_wm = p_wm_vec[a]

            # Mixture policy
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global decay toward uniform (interference)
            w = (1 - decay) * w + decay * w_0

            # WM caching update
            if r > 0:
                # Store a sharp memory of the rewarded action for this state
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                # Renormalize to probabilities
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

            # Update last chosen action for perseveration
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Probabilistic recall WM (Dirichlet-count memory) with set-size-modulated recall.

    Mechanism
    - RL: slow delta-rule Q-values with softmax.
    - WM: maintains per-state Dirichlet-like counts of action correctness.
      When recalled, WM yields a near-deterministic policy from normalized counts.
      Probability of WM recall is set-size dependent via a logistic transform.
      Negative outcomes increment counts weakly (parameter rho_neg), modeling weak evidence.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Base mixture weight of WM in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - theta0: float
            Intercept controlling baseline WM recall probability.
        - thetaS: float
            Slope controlling how recall changes with effective load (1 / set size).
        - rho_neg: float in [0,1]
            Increment added to WM counts after negative feedback (vs. 1.0 after positive).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, theta0, thetaS, rho_neg = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM "counts" initialized to a symmetric weak prior (1 per action)
        w = np.ones((nS, nA))  # counts
        w_0 = np.ones((nS, nA))  # prior counts (for reference if needed)

        # Set-size dependent recall probability (higher load => lower recall via 1/nS)
        p_recall = sigmoid(theta0 + thetaS * (1.0 / max(nS, 1)))

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: normalized counts -> softmax with high beta
            counts = np.maximum(w[s, :], eps)
            probs = counts / np.maximum(np.sum(counts), eps)
            prefs = probs - np.mean(probs)
            wm_vec = np.exp(np.clip(softmax_beta_wm * prefs, -50, 50))
            wm_vec /= max(np.sum(wm_vec), eps)
            p_wm = wm_vec[a]

            # Effective WM weight scales with recall probability
            w_eff = wm_weight * p_recall
            p_total = w_eff * p_wm + (1 - w_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM counts update: positive feedback yields strong evidence; negative weak evidence
            if r > 0:
                w[s, a] += 1.0
            else:
                w[s, a] += np.clip(rho_neg, 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM eligibility traces with set-size interference and load-dependent lapses.

    Mechanism
    - RL: slow delta-rule Q-values with softmax.
    - WM: associative table W updated with a fast trace on (state, action), and globally
      decays toward uniform via set-size-dependent interference. WM policy is near-deterministic
      but mixes with a load-dependent lapse to uniform.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            RL inverse temperature (scaled internally by 10).
        - trace: float in [0,1]
            WM trace learning rate for the chosen action in the current state.
        - interference_slope: float in [0,1]
            Slope for WM global decay toward uniform scaling with set size.
        - epsilon_base: float in [0,1]
            Base lapse level; actual WM lapse increases with set size.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, trace, interference_slope, epsilon_base = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = np.zeros((nS, nA))  # WM associative strengths start neutral
        w_0 = (1 / nA) * np.ones((nS, nA))  # baseline to decay toward

        # Set-size dependent interference and lapse
        interference = np.clip(interference_slope * (nS - 1) / max(nS, 1), 0.0, 1.0)
        epsilon = np.clip(epsilon_base * (nS - 1) / max(nS, 1), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = rewards[blocks == b][t]  # equivalent to block_rewards[t] but explicit

            Q_s = q[s, :]

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from associative strengths with deterministic softmax, then lapse to uniform
            W_s_vals = w[s, :]
            prefs = W_s_vals - np.mean(W_s_vals)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm_vec = (1 - epsilon) * p_wm_vec + epsilon * (1.0 / nA)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Global WM interference (decay toward uniform)
            w = (1 - interference) * w + interference * (w_0 - (1.0 / nA))  # center around zero
            # Equivalent to moving strengths toward zero-centered baseline

            # WM trace update for current state-action
            # Move chosen action toward +1 if reward, toward -1 if no reward
            target = 1.0 if r > 0 else -1.0
            w[s, a] += trace * (target - w[s, a])
            # Optional mild suppression/normalization of other actions toward -target/nA to keep separation
            others = np.arange(nA) != a
            w[s, others] += trace * ((-target / (nA - 1)) - w[s, others])

        blocks_log_p += log_p

    return -blocks_log_p