def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-scaled learning plus perseveration bias.

    Mechanism
    - Tabular RL (Q-learning) with a learning rate scaled down by set size (load)
      and scaled by an age factor.
    - Choice perseveration ("stickiness") bias that increases under higher load and
      with age, capturing tendency to repeat prior action under cognitive load.
    - Invalid trials (action outside 0..2, or reward < 0) contribute uniform likelihood
      and do not update.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Observed actions (0..2). Out-of-range treated as lapse.
    rewards : array-like of float
        Reward feedback (e.g., 0/1). Negative indicates invalid trial.
    blocks : array-like of int
        Block index per trial; states reset each block.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like of float
        Participant age as a single-element array.
    model_parameters : iterable of 5 floats
        alpha_base  : Baseline RL learning rate (0..1).
        beta        : Inverse temperature for softmax (>0).
        kappa_stick : Baseline perseveration strength (>=0).
        age_bias    : Age scaling factor; modifies learning rate (and interacts with stickiness).
                      Positive values speed learning for younger (<45) and slow for older (>45).
        size_sens   : Load sensitivity (>=0) that reduces learning rate and increases stickiness
                      as set size increases from 3 to 6.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta, kappa_stick, age_bias, size_sens = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    age_center = (age_val - 45.0) / 10.0  # decades from 45 to modulate parameters

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        prev_action = -1  # initialize with no previous action

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials: uniform likelihood, no updates except resetting prev_action.
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                prev_action = a if (0 <= a < nA) else -1
                continue

            # Load- and age-scaled learning rate
            load_scale = 1.0 / (1.0 + size_sens * max(ss - 3.0, 0.0))
            age_scale = 1.0 / (1.0 + age_bias * age_center)  # if age_bias>0: older => slower lr
            alpha = np.clip(alpha_base * load_scale * age_scale, 0.0, 1.0)

            # Stickiness strength increases with load and (slightly) with age
            stick_strength = kappa_stick * (1.0 + size_sens * max(ss - 3.0, 0.0) + 0.3 * is_older * abs(age_bias))

            # Preferences: softmax over Q plus perseveration bonus for repeating last action
            pref = beta * Q[s, :].copy()
            if 0 <= prev_action < nA:
                pref[prev_action] += stick_strength

            # Softmax with numerical stability
            m = np.max(pref)
            expv = np.exp(pref - m)
            p = expv / np.sum(expv)

            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update prev_action for stickiness on next trial
            prev_action = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and a load/age-modulated choice kernel.

    Mechanism
    - Tabular Q-learning with trial-wise forgetting (decay toward zero), capturing interference.
    - A choice kernel (recency bias) that promotes repeating recent actions irrespective of reward.
      Its strength is increased under higher set sizes and in older participants.
    - Softmax temperature is also modulated by age and set size to capture increased randomness.

    Parameters
    ----------
    states : array-like of int
        State per trial within block (0..nS-1).
    actions : array-like of int
        Observed actions (0..2). Out-of-range treated as lapse.
    rewards : array-like of float
        Reward feedback. Negative indicates invalid trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial.
    age : array-like of float
        Participant age as a single-element array.
    model_parameters : iterable of 5 floats
        eta            : Learning rate for Q (0..1).
        inv_temp       : Baseline inverse temperature (>0).
        rho_forget     : Baseline forgetting rate per trial (0..1), applied to all Q-values.
        psi_rep        : Baseline choice-kernel strength (>=0).
        age_temp_gain  : Scaling factor that reduces effective inverse temperature with age/load
                         and strengthens the choice kernel (>=0).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    eta, inv_temp, rho_forget, psi_rep, age_temp_gain = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    age_center = (age_val - 45.0) / 10.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        # Choice kernel K over actions, shared across states within a block to capture global recency
        K = np.zeros(nA)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Effective forgetting increases with set size and age
            rho_eff = np.clip(rho_forget + 0.1 * max(ss - 3.0, 0.0) + 0.05 * is_older * (0.5 + abs(age_center)), 0.0, 1.0)

            # Apply forgetting to all Q-values (state-wise)
            Q[s, :] *= (1.0 - rho_eff)

            # Invalid trials: uniform likelihood, no learning but update kernel decay
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                # Decay the kernel even on invalid trials
                K *= (1.0 - 0.5 * np.clip(rho_eff, 0.0, 1.0))
                continue

            # Age and load reduce effective inverse temperature (more random under load/older)
            beta_eff = inv_temp / (1.0 + age_temp_gain * (0.5 * is_older + 0.5 * abs(age_center)) + 0.5 * age_temp_gain * max(ss - 3.0, 0.0))
            beta_eff = max(beta_eff, 1e-6)

            # Choice kernel strength scales with load and age
            psi_eff = psi_rep * (1.0 + 0.5 * max(ss - 3.0, 0.0) + age_temp_gain * (0.3 * is_older + 0.2 * abs(age_center)))

            # Preferences: Q plus choice kernel
            pref = beta_eff * Q[s, :] + psi_eff * K

            m = np.max(pref)
            expv = np.exp(pref - m)
            p = expv / np.sum(expv)

            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += eta * pe

            # Update choice kernel: decay then increment chosen action
            K *= (1.0 - 0.3)  # mild decay of recency influence
            K[a] += 1.0

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-driven exploration bonus modulated by age and set size.

    Mechanism
    - Tabular RL (Q-learning).
    - An exploration bonus based on stateâ€“action visit counts (UCB-like): bonus ~ c / sqrt(N[s,a]).
      The bonus magnitude increases with set size (more uncertainty) but is attenuated by age,
      capturing reduced directed exploration in older participants.
    - Invalid trials treated as uniform likelihood with no learning.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1) within block.
    actions : array-like of int
        Observed actions (0..2). Out-of-range treated as lapse.
    rewards : array-like of float
        Reward feedback. Negative indicates invalid trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array.
    model_parameters : iterable of 5 floats
        alpha     : RL learning rate (0..1).
        beta      : Inverse temperature for softmax (>0).
        bonus0    : Baseline uncertainty bonus magnitude (>=0).
        age_unc   : Age modulation of the exploration bonus; positive reduces bonus with age.
        size_unc  : Load modulation of the exploration bonus; positive increases bonus with set size.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, bonus0, age_unc, size_unc = model_parameters
    age_val = float(age[0])
    age_center = (age_val - 45.0) / 10.0
    is_older = 1.0 if age_val > 45.0 else 0.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = np.asarray(states)[idx]
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        N = np.ones((nS, nA))  # initialize at 1 to avoid division by zero

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials: no learning, uniform likelihood
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Compute exploration bonus
            load_factor = 1.0 + size_unc * max(ss - 3.0, 0.0)
            age_factor = 1.0 - age_unc * (0.5 * is_older + 0.5 * max(age_center, 0.0))  # reduce with older age
            age_factor = max(0.0, age_factor)

            c = bonus0 * load_factor * age_factor
            bonus = c / np.sqrt(N[s, :])

            pref = beta * (Q[s, :] + bonus)

            m = np.max(pref)
            expv = np.exp(pref - m)
            p = expv / np.sum(expv)

            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning update and count increment
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe
            N[s, a] += 1.0

    return -float(total_log_p)