def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + entropy-gated WM with set-size scaling and separate WM learn/forget rates.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: stores state-action associations as a probability vector W_s.
    - Arbitration: WM influence is down-weighted in larger set sizes and when WM is
      internally uncertain (high entropy). A logistic gate of WM entropy controls
      trial-by-trial WM usage.
    - WM updates: forgetting toward uniform each trial plus learning toward the chosen
      action on rewarded trials.

    Parameters
    ----------
    model_parameters : (lr, wm_weight, softmax_beta, wm_lr, wm_decay, gate_temp)
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Base mixture weight for WM; further scaled by set-size and WM entropy (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        wm_lr : float
            WM learning rate toward the chosen action on rewarded trials (0-1).
        wm_decay : float
            WM decay toward uniform each trial (0-1).
        gate_temp : float
            Entropy gating slope; higher sharpens the gate of WM usage by entropy.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, gate_temp = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM values)
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based gating of WM (higher entropy => less WM use)
            eps = 1e-12
            H = -np.sum(W_s * np.log(W_s + eps))  # entropy of WM for this state
            H_max = np.log(nA)
            H_mid = 0.5 * H_max
            gate = 1.0 / (1.0 + np.exp(gate_temp * (H - H_mid)))  # in (0,1)

            # Set-size scaling of WM weight (smaller sets => stronger WM)
            ss_scale = 3.0 / max(3.0, float(nS))
            wm_weight_eff = np.clip(wm_weight * ss_scale * gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm_core + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: decay toward uniform, then learn on reward
            # baseline decay
            w[s, :] = (1.0 - wm_decay) * W_s + wm_decay * w_0[s, :]
            # reward-consolidation toward chosen action
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * one_hot

            # renormalize and clip
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM one-shot encoding with interference and stickiness.

    Idea
    - RL: Q-learning with separate learning rates for positive vs. negative prediction errors.
    - WM: when rewarded, encode a near one-hot mapping; otherwise mainly rely on decay.
      WM suffers interference that increases with set size and exhibits a "stickiness"
      bias to repeat the last action in a state (capturing perseveration under load).
    - Arbitration: convex mixture; WM weight further downscaled with set size.

    Parameters
    ----------
    model_parameters : (lr_pos, wm_weight, softmax_beta, lr_neg, interference, stickiness)
        lr_pos : float
            RL learning rate for positive PEs (0-1).
        wm_weight : float
            Base mixture weight for WM (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        lr_neg : float
            RL learning rate for negative PEs (0-1).
        interference : float
            WM interference/decay rate toward uniform, scaled by set size (>=0).
        stickiness : float
            Weight of bias toward the last action taken in the same state (0-1)
            applied inside the WM policy.

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr_pos, wm_weight, softmax_beta, lr_neg, interference, stickiness = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        last_a = -np.ones(nS, dtype=int)  # last action per state, -1 if none

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Stickiness inside WM policy: bias toward last action in this state
            if last_a[s] >= 0:
                bias_vec = np.zeros(nA)
                bias_vec[last_a[s]] = 1.0
                W_bias = (1.0 - stickiness) * W_s + stickiness * bias_vec
            else:
                W_bias = W_s

            # WM policy softmax (with potential bias)
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_bias - W_bias[a])))

            # Downscale WM weight with set size (capacity limitation)
            ss_scale = 3.0 / max(3.0, float(nS))
            wm_weight_eff = np.clip(wm_weight * ss_scale, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm_core + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM update: interference scaled by set size; encode on reward
            # decay/interference
            lam = 1.0 - np.exp(-interference * (float(nS) / 3.0))
            lam = np.clip(lam, 0.0, 1.0)
            w[s, :] = (1.0 - lam) * w[s, :] + lam * w_0[s, :]

            # one-shot encoding on reward
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # high-fidelity encode, tempered by interference just applied
                encode_strength = 1.0 - 0.5 * lam
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * one_hot

            # renormalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

            # update last action for this state
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty/availability arbitration with delay-based WM decay and lapse.

    Idea
    - RL: standard Q-learning with softmax.
    - WM: stores mappings that decay with time since last rewarded visit to a state
      and with set size. WM policy readout is deterministic.
    - Arbitration: WM weight is product of a maximum WM capacity (wm_weight), a set-size
      factor, and a delay-based availability (shorter delays => stronger WM).
    - Lapse: small probability of random responding independent of policy.

    Parameters
    ----------
    model_parameters : (lr, wm_weight, softmax_beta, delay_decay, ss_exponent, lapse)
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Maximum WM mixture weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        delay_decay : float
            Rate of WM decay as a function of trials since last reward for that state (>=0).
        ss_exponent : float
            Exponent controlling set-size scaling: (3/nS)^ss_exponent (>=0).
        lapse : float
            Lapse probability to mix with uniform choice (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    lr, wm_weight, softmax_beta, delay_decay, ss_exponent, lapse = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track delay (trials since last rewarded visit) per state
        delay = np.zeros(nS, dtype=float)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Availability via delay since last rewarded visit
            avail = np.exp(-delay_decay * delay[s])
            # Set-size scaling via exponent
            ss_scale = (3.0 / max(3.0, float(nS))) ** max(0.0, ss_exponent)
            wm_weight_eff = np.clip(wm_weight * ss_scale * avail, 0.0, 1.0)

            p_mix = wm_weight_eff * p_wm_core + (1.0 - wm_weight_eff) * p_rl
            # Lapse mixture with uniform
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: mild decay each trial; stronger consolidation on reward
            # baseline decay over time (implicitly captured by delay but also decay content)
            base_forget = 1.0 - np.exp(-delay_decay * 0.5)  # small per-visit decay
            w[s, :] = (1.0 - base_forget) * W_s + base_forget * w_0[s, :]

            if r > 0:
                # consolidate mapping
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * one_hot
                delay[s] = 0.0  # reset delay on successful retrieval
            else:
                # no reward: let delay accumulate
                delay[s] += 1.0

            # renormalize
            w[s, :] = np.clip(w[s, :], 1e-8, None)
            w[s, :] /= np.sum(w[s, :])

            # increment delay for unvisited states this trial
            for s2 in range(nS):
                if s2 != s:
                    delay[s2] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p