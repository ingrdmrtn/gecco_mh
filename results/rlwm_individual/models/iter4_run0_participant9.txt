def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL(eligibility trace) + capacity-limited WM store with interference; set-size arbitration.

    Idea:
    - RL learns Q via a delta rule with eligibility traces, allowing credit to persist across trials.
    - WM is a fast, capacity-limited store: rewarded state-action pairs are stored as near-deterministic rows.
      Interference (stronger in larger set sizes) causes WM rows to drift back toward uniform.
    - Arbitration weight on WM scales with capacity utilization: wm_weight = base * min(1, capacity/nS).

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - lambda_et: Eligibility-trace persistence (0..1). Larger = longer-lasting eligibility.
    - wm_weight_base: Baseline arbitration weight for WM at capacity saturation (0..1).
    - wm_capacity: Number of states WM can hold near-deterministically (>=0). Effective weight drops when nS > capacity.
    - wm_interference: Per-trial interference driving WM rows back toward uniform (0..1). Scales up with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, lambda_et, wm_weight_base, wm_capacity, wm_interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility traces for RL
        e = np.zeros((nS, nA))
        # Memory strength per state (1 if recently stored, otherwise decays via interference)
        mem_strength = np.zeros(nS)

        # Arbitration: WM effective weight scaled by capacity utilization
        cap_scale = min(1.0, float(max(wm_capacity, 0.0)) / max(1, nS))
        wm_weight = float(np.clip(wm_weight_base * cap_scale, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: softmax over current WM row (near-deterministic if stored strongly)
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL learning with eligibility traces
            # Update eligibility: decay and set current (s,a) to 1
            e *= lambda_et
            e[s, :] *= 0.0
            e[s, a] = 1.0
            delta = r - q[s, a]
            q += lr * delta * e

            # WM interference: scale interference with set size (more load => more drift to uniform)
            interference = np.clip(wm_interference * (nS / 6.0), 0.0, 1.0)
            w = (1.0 - interference) * w + interference * w_0
            mem_strength = (1.0 - interference) * mem_strength

            # WM storage on positive feedback: store near one-hot; strength = 1
            if r > 0.5:
                w[s, :] = (1.0 - interference) * w[s, :]
                w[s, a] += interference  # push toward one-hot on the chosen action
                # Renormalize to maintain a probability distribution
                w[s, :] = np.clip(w[s, :], 0.0, None)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                mem_strength[s] = 1.0

            # Capacity pressure: if too many states are stored (mem_strength high),
            # weaken WM rows for the weakest states beyond capacity.
            k = int(max(0, np.floor(wm_capacity)))
            if k < nS and k >= 0:
                # Identify top-k by strength; others get additional drift to uniform
                idx_sorted = np.argsort(-mem_strength)
                keep = set(idx_sorted[:k].tolist())
                for s_idx in range(nS):
                    if s_idx not in keep:
                        # Extra drift for states beyond capacity
                        extra = 0.5 * interference
                        w[s_idx, :] = (1.0 - extra) * w[s_idx, :] + extra * w_0[s_idx, :]
                        mem_strength[s_idx] *= (1.0 - extra)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Win-Stay/Lose-Shift-style WM; set-size arbitration + lapse.

    Idea:
    - RL learns slowly via delta rule.
    - WM implements a heuristic: for each state, remember the last rewarded action and choose it with high probability.
      If no rewarded action is known, WM is near-uniform with controllable noise.
    - Arbitration weight for WM scales down with set size (3/nS).
    - A small lapse rate mixes in uniform random choice at the end.

    Parameters (5 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight0: Baseline WM weight at set size 3 (0..1); scaled by 3/nS.
    - wm_noise: Noise in WM policy (0..1). 0 -> deterministic choice of the remembered action; 1 -> uniform.
    - lapse: Final lapse probability mixing with uniform policy (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, wm_noise, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # not used directly here, but kept for consistency
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: last rewarded action per state; -1 if none
        last_rewarded = -1 * np.ones(nS, dtype=int)
        w = (1.0 / nA) * np.ones((nS, nA))  # maintained for interface; used to form WM policy rows
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size-scaled arbitration
        wm_weight = float(np.clip(wm_weight0 * (3.0 / max(1, nS)), 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy: if remembered, assign high prob to that action; else near-uniform.
            if last_rewarded[s] >= 0:
                w[s, :] = wm_noise * w_0[s, :]  # spread some mass
                w[s, last_rewarded[s]] += (1.0 - wm_noise)  # concentrate on remembered action
            else:
                w[s, :] = w_0[s, :]

            wm_row = w[s, :].copy()
            wm_row = np.clip(wm_row, eps, None)
            wm_row /= np.sum(wm_row)
            p_wm = wm_row[a]

            # Mixture + lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: store on reward; clear memory on consistent negative outcomes
            if r > 0.5:
                last_rewarded[s] = a
            else:
                # On repeated non-rewards, allow forgetting by stochastic erosion via wm_noise footprint
                # Move row slightly toward uniform on losses
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]
                # If it gets too flat, drop the memory
                if np.max(w[s, :]) - np.min(w[s, :]) < 0.05:
                    last_rewarded[s] = -1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size-specific temperature + WM delta learning with decay.

    Idea:
    - RL uses a single learning rate but different inverse temperatures depending on set size (beta_3 vs beta_6),
      capturing load-dependent exploration/exploitation.
    - WM learns a fast associative map via a delta rule toward a one-hot target on rewarded trials and
      decays toward uniform otherwise; decay scales up with set size, modeling interference.
    - Arbitration weight scales down with set size.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - beta_3: RL inverse temperature for set size 3; internally scaled by 10.
    - beta_6: RL inverse temperature for set size 6; internally scaled by 10.
    - wm_weight_base: Baseline WM arbitration weight at set size 3 (0..1); scales with 3/nS.
    - wm_decay: Base WM decay rate toward uniform (0..1).
    - phi_wm: WM learning rate toward one-hot on rewarded trials (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, beta_3, beta_6, wm_weight_base, wm_decay, phi_wm = model_parameters
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nS = int(block_set_sizes[0])
        # Choose RL beta by set size
        softmax_beta = (beta_3 if nS == 3 else beta_6) * 10.0

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Arbitration scaled by set size
        wm_weight = float(np.clip(wm_weight_base * (3.0 / max(1, nS)), 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            wm_logits = softmax_beta_wm * W_s
            wm_logits -= np.max(wm_logits)
            exp_wm = np.exp(wm_logits)
            pi_wm = exp_wm / max(np.sum(exp_wm), eps)
            p_wm = pi_wm[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Interference/decay grows with set size
            decay = np.clip(wm_decay * (nS / 6.0), 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # 2) Rapid learning toward a one-hot target on rewarded trials
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - phi_wm) * w[s, :] + phi_wm * target
                # Keep row normalized
                w[s, :] = np.clip(w[s, :], 0.0, None)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p