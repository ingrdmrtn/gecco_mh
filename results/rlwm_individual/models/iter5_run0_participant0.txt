def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-weighted arbitration and reward-contingent WM updates.

    Mechanism:
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: a fast, quasi-deterministic policy based on a state-specific action
      distribution (row w[s,:]), updated strongly on rewarded trials and
      weakly repelled on unrewarded trials. WM decays toward uniform at a
      rate that increases with set size (load).
    - Arbitration: the mixture weight of WM vs RL is adjusted by the relative
      certainty (1 - normalized entropy) of RL and WM within the current state.
      The parameter ent_weight controls how strongly entropy skews the base
      wm_weight toward the more certain system.

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] baseline WM mixture weight in the policy (before entropy adjustment).
    - softmax_beta: inverse temperature for RL (internally scaled by 10).
    - ent_weight: [0, +] strength of entropy-based arbitration (0 = fixed mixture).
    - wm_decay_rate: [0,1] base WM decay per trial toward uniform; scales up with load.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, ent_weight, wm_decay_rate = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    eps = 1e-12
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay multiplier: more decay when nS is larger
        load_factor = (nS - 1) / max(1, (nA + nS - 2))  # maps roughly to [0, ~1)
        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Entropy-based arbitration: compute normalized entropies
            def _norm_entropy(pvec):
                pvec = np.clip(pvec, eps, 1.0)
                pvec = pvec / np.sum(pvec)
                H = -np.sum(pvec * np.log(pvec))
                return H / np.log(len(pvec))

            # Reconstruct full choice probabilities for RL and WM
            # (softmax normalization using the same denominators)
            prl_full = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            prl_full = prl_full / np.sum(prl_full)
            pwm_full = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            pwm_full = pwm_full / np.sum(pwm_full)

            H_rl = _norm_entropy(prl_full)
            H_wm = _norm_entropy(pwm_full)
            C_rl = 1.0 - H_rl
            C_wm = 1.0 - H_wm

            # Blend baseline wm_weight using relative certainty
            # Effective WM weight tilts toward higher certainty system
            wm_weight_adj = wm_weight + ent_weight * (C_wm - C_rl)
            wm_weight_adj = np.clip(wm_weight_adj, 0.0, 1.0)

            p_total = wm_weight_adj * p_wm + (1.0 - wm_weight_adj) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform, stronger under higher load
            d = np.clip(wm_decay_rate * (1.0 + load_factor), 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # Reward-contingent WM update:
            # - If rewarded: attract W_s toward one-hot of chosen action.
            # - If not rewarded: repel W_s slightly from chosen action.
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0

            if r > 0.5:
                wm_alpha_pos = np.clip(0.75 * (1.0 - 0.5 * load_factor), 0.0, 1.0)
                w[s, :] = (1.0 - wm_alpha_pos) * w[s, :] + wm_alpha_pos * one_hot
            else:
                wm_alpha_neg = np.clip(0.25 * (1.0 - 0.5 * load_factor), 0.0, 1.0)
                # push mass away from chosen action and renormalize
                w[s, a] = (1.0 - wm_alpha_neg) * w[s, a]
                # distribute the removed mass equally to other actions
                redistribute = (wm_alpha_neg * 1.0) / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute
                # clamp and renormalize
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + capacity-limited WM slots with consolidation.

    Mechanism:
    - RL: delta-rule with softmax; includes global forgetting toward uniform.
    - WM: only a subset of states can be stored (slot capacity). When set size
      exceeds capacity, the probability that the current state is represented
      in WM is capacity_slots / nS. The effective WM mixture is scaled by this
      availability. WM consolidates strongly on rewarded trials and weakly
      anti-consolidates on errors.
    - Policy: mixture of RL and (availability-weighted) WM.

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] baseline WM mixture weight.
    - softmax_beta: inverse temperature for RL (scaled by 10).
    - capacity_slots: [0, +] number of WM slots available.
    - wm_consolidation: [0,1] WM consolidation strength when r=1 (and scaled for r=0).
    - q_forget: [0,1] per-trial RL forgetting rate toward uniform.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_slots, wm_consolidation, q_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    eps = 1e-12
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Probability that a given state can be in WM (slot availability)
        p_available = min(1.0, float(capacity_slots) / max(1, nS))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Effective WM weight scales with availability of WM for current state
            wm_weight_eff = wm_weight * p_available

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with forgetting toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Apply forgetting to all states softly each step
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA) * np.ones_like(q)

            # WM consolidation if the state is storable (in expectation)
            # We implement expected update by scaling consolidation by p_available.
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                alpha = wm_consolidation * p_available
                w[s, :] = (1.0 - alpha) * w[s, :] + alpha * one_hot
            else:
                # Anti-consolidation: push probability away from chosen action
                alpha = 0.5 * wm_consolidation * p_available
                w[s, a] = (1.0 - alpha) * w[s, a]
                inc = alpha / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += inc
                # keep valid simplex
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Load-adaptive temperatures and WM weight + leaky WM.

    Mechanism:
    - RL: delta-rule with load-dependent temperature; higher set size increases
      choice noise for RL via temp_load scaling.
    - WM: quasi-deterministic policy with precision scaled by wm_precision; WM
      decays (leaks) toward uniform each trial (wm_leak).
    - Arbitration: baseline WM weight (wm_weight0) is downscaled with set size.
      The same temp_load parameter controls both RL temperature inflation and
      WM mixture down-weighting as load increases.

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight0: [0,1] baseline WM mixture weight at lowest load.
    - softmax_beta: base inverse temperature for RL (scaled by 10 before load).
    - temp_load: [0, +] load sensitivity for both RL temperature and WM weight
                 (larger -> more RL noise and lower WM weight as set size increases).
    - wm_precision: [0, +] scales WM inverse temperature (higher -> sharper WM policy).
    - wm_leak: [0,1] WM leak per trial toward uniform (load-independent).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, temp_load, wm_precision, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    eps = 1e-12
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load scalars referenced inside the block
        load_norm = max(0.0, (nS - 3.0) / 3.0)  # 0 at nS=3, ~1 at nS=6
        beta_rl_eff = softmax_beta / (1.0 + temp_load * load_norm)
        wm_weight_eff_block = wm_weight0 / (1.0 + temp_load * load_norm)
        beta_wm_eff = max(1.0, softmax_beta_wm * max(0.0, wm_precision))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            denom_rl = np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform
            w = (1.0 - wm_leak) * w + wm_leak * w_0

            # Reward-gated WM sharpening
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                # modest, load-insensitive consolidation (already captured in arbitration)
                alpha = 0.4 * np.tanh(0.5 * wm_precision)
                w[s, :] = (1.0 - alpha) * w[s, :] + alpha * one_hot
            else:
                # slight penalization of chosen action
                alpha = 0.2 * np.tanh(0.5 * wm_precision)
                w[s, a] = (1.0 - alpha) * w[s, a]
                inc = alpha / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += inc
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p