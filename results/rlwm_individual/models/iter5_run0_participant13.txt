def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + episodic-recency retrieval with age- and load-modulated gating and lapses.

    The model assumes two systems:
      - RL: incremental value learning with softmax choice.
      - Episodic: retrieves the most recently rewarded action for the current state,
        weighted by how recent that reward was (exponential recency).

    Mixture:
      - The episodic system's contribution is modulated by set size (smaller sets favor episodic),
        and by age: older adults (>=45) have reduced episodic contribution (negative shift).
      - A lapse component mixes in uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) for each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of float
        Participant age; age[0] is used to define age group (older >= 45).
    model_parameters : tuple/list of floats
        (alpha, beta, epi_base, recency_decay, age_epi_shift, lapse)
        - alpha: RL learning rate (sigmoid to 0..1).
        - beta: inverse temperature for RL softmax (sigmoid to 0..12).
        - epi_base: baseline episodic mixture weight before load/age adjustments (sigmoid 0..1).
        - recency_decay: exponential decay rate of episodic trace with time (softplus >=0).
        - age_epi_shift: additive shift to episodic weight for older group (can be negative).
        - lapse: lapse rate mixing policy with uniform random (sigmoid to 0..0.3).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, epi_base, recency_decay, age_epi_shift, lapse = model_parameters

    # Parameter transforms
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    epi_base = 1.0 / (1.0 + np.exp(-epi_base))
    recency_decay = np.log(1.0 + np.exp(recency_decay))  # softplus
    lapse = (1.0 / (1.0 + np.exp(-lapse))) * 0.3

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # RL values
        Q = np.zeros((nS, nA))

        # Episodic memory: track most recent rewarded action and its trial index per state
        last_rewarded_action = -np.ones(nS, dtype=int)
        last_rewarded_time = -np.ones(nS, dtype=int)

        # Mixture weight: down-weight episodic under higher load (3/nS) and by age shift
        epi_w = epi_base * (3.0 / float(nS))
        epi_w = np.clip(epi_w + age_epi_shift * older, 0.0, 1.0)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL softmax policy
            q = Q[s, :]
            q = q - np.max(q)
            pi_rl = np.exp(beta * q)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # Episodic policy based on most recent rewarded action with recency weight
            pi_epi = np.ones(nA) / nA
            if last_rewarded_action[s] >= 0:
                dt = 0 if last_rewarded_time[s] < 0 else (t - last_rewarded_time[s])
                # Recency weight in [0,1], equals 1 at dt=0 and decays with dt
                w_rec = np.exp(-recency_decay * float(dt))
                a_star = last_rewarded_action[s]
                # Construct a peaked distribution with mass w_rec on a_star
                # Remaining mass is uniform on other actions
                pi_epi = np.ones(nA) * ((1.0 - w_rec) / (nA - 1))
                pi_epi[a_star] = w_rec

            # Mixture and lapse
            pi = epi_w * pi_epi + (1.0 - epi_w) * pi_rl
            pi = (1.0 - lapse) * pi + lapse * (1.0 / nA)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Episodic trace update upon reward
            if r > 0.0:
                last_rewarded_action[s] = a
                last_rewarded_time[s] = t

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Kalman-RL with uncertainty bonus (UCB) and age/load-modulated process noise plus lapses.

    Each state-action value is modeled as a latent mean with Gaussian uncertainty.
    Updates use a Kalman gain determined by current uncertainty and observation noise.
    Choice uses a softmax over mean plus an uncertainty bonus proportional to sqrt(variance).

    Age and set size increase the process noise, which maintains higher uncertainty and thus
    greater exploration via the UCB bonus. A lapse component mixes in uniform random choices.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1) each trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (older >= 45).
    model_parameters : tuple/list of floats
        (beta, obs_noise, bonus_scale, proc_noise_base, age_noise_shift, size_noise_scale)
        - beta: inverse temperature for softmax (sigmoid to 0..12).
        - obs_noise: observation noise std (softplus to >0).
        - bonus_scale: coefficient on sqrt(variance) exploration bonus (softplus to >=0).
        - proc_noise_base: base process noise std added after each update (softplus to >0).
        - age_noise_shift: additive increase to process noise for older adults (>=0 or <0 allowed).
        - size_noise_scale: multiplicative scaling of process noise by set size ratio (softplus).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    beta, obs_noise, bonus_scale, proc_noise_base, age_noise_shift, size_noise_scale = model_parameters

    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    obs_noise = np.log(1.0 + np.exp(obs_noise)) + 1e-6  # std
    bonus_scale = np.log(1.0 + np.exp(bonus_scale))
    proc_noise_base = np.log(1.0 + np.exp(proc_noise_base)) + 1e-6  # std
    size_noise_scale = np.log(1.0 + np.exp(size_noise_scale))  # >=0

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize means and variances
        mu = np.zeros((nS, nA))
        var = np.ones((nS, nA)) * 1.0  # prior variance

        # Process noise std for this block: base + age shift, scaled by set size
        # Larger sets -> multiply by (nS/3)^size_noise_scale
        size_multiplier = (float(nS) / 3.0) ** size_noise_scale
        proc_noise_std = np.maximum(proc_noise_base + age_noise_shift * older, 0.0) * size_multiplier
        proc_noise_var = proc_noise_std ** 2
        obs_var = obs_noise ** 2

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Compute decision values: mean + exploration bonus
            bonus = bonus_scale * np.sqrt(np.maximum(var[s, :], 1e-12))
            v = mu[s, :] + bonus

            # Softmax policy
            vv = v - np.max(v)
            pi = np.exp(beta * vv)
            pi = pi / (np.sum(pi) + eps)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # Kalman update for chosen action
            # Prediction: add process noise to all actions in the current state
            var[s, :] = var[s, :] + proc_noise_var

            # Observation update for chosen action
            K = var[s, a] / (var[s, a] + obs_var)
            mu[s, a] = mu[s, a] + K * (r - mu[s, a])
            var[s, a] = (1.0 - K) * var[s, a]

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and state-specific stickiness, with age/load-modulated lambda.

    The model uses SARSA(λ)-like eligibility traces within each block to propagate
    credit over repeated visits to the same state-action. The trace decay parameter λ
    is modulated by age group and set size:
      - Older adults (>=45) have altered λ (age shift).
      - Larger set sizes reduce λ (harder to maintain traces across items).

    A state-specific action stickiness bias is added to the softmax, favoring the last
    chosen action in that state; stickiness decays with the number of intervening trials
    since the state's last visit.

    Parameters
    ----------
    states : array-like of int
        State indices on each trial (0..nS-1 within block).
    actions : array-like of int
        Chosen actions on each trial (0..nA-1).
    rewards : array-like of int
        Binary rewards (0 or 1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) for the block of each trial.
    age : array-like of float
        Participant age; age[0] determines age group (older >= 45).
    model_parameters : tuple/list of floats
        (alpha_base, beta, lambda_base, age_lambda_shift, size_lambda_shift, stick_base)
        - alpha_base: learning rate baseline (sigmoid to 0..1).
        - beta: inverse temperature for softmax (sigmoid to 0..12).
        - lambda_base: baseline trace decay λ (sigmoid to 0..1).
        - age_lambda_shift: additive shift to λ for older adults (can be +/-; clipped to [0,1]).
        - size_lambda_shift: additive shift to λ per unit of (nS-3)/3 (can be +/-; clipped).
        - stick_base: base stickiness strength added to last action (softplus >=0), scaled by recency and load.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha_base, beta, lambda_base, age_lambda_shift, size_lambda_shift, stick_base = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha_base))
    beta = (1.0 / (1.0 + np.exp(-beta))) * 12.0
    lam0 = 1.0 / (1.0 + np.exp(-lambda_base))
    stick_base = np.log(1.0 + np.exp(stick_base))  # >=0

    age_val = float(age[0])
    older = 1 if age_val >= 45 else 0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Effective lambda for this block with age and size modulation
        size_factor = (float(nS) - 3.0) / 3.0  # 0 for nS=3, 1 for nS=6
        lam_eff = lam0 + age_lambda_shift * older + size_lambda_shift * size_factor
        lam_eff = np.clip(lam_eff, 0.0, 1.0)

        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))  # eligibility traces

        # Track last action and last visit time per state for stickiness
        last_action = -np.ones(nS, dtype=int)
        last_visit_time = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Construct softmax with stickiness bias
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                dt = 0 if last_visit_time[s] < 0 else (t - last_visit_time[s])
                # Recency-decayed stickiness; more decay with longer dt and larger set sizes
                # decay factor: 1/(1+dt)
                rec = 1.0 / (1.0 + float(dt))
                stick = stick_base * rec * (3.0 / float(nS))  # lower stickiness under higher load
                bias[last_action[s]] += stick

            logits = beta * Q[s, :] + bias
            logits = logits - np.max(logits)
            pi = np.exp(logits)
            pi = pi / (np.sum(pi) + eps)

            p = max(pi[a], eps)
            nll -= np.log(p)

            # TD error
            delta = r - Q[s, a]

            # Update eligibility traces: decay and set current to 1 (replacing traces)
            E *= lam_eff
            E[s, a] = 1.0

            # Value update with traces
            Q += alpha * delta * E

            # Update last action/visit time for stickiness
            last_action[s] = a
            last_visit_time[s] = t

    return nll