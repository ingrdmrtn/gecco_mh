Here are three distinct cognitive models, each as a standalone Python function that returns the negative log-likelihood of the observed choices. Each model mixes model-free RL and a working-memory (WM) component, with different mechanisms for how WM operates and interacts with set size (load). All parameters are used meaningfully and total parameter count per model is â‰¤ 6.

Note: Assume numpy as np is already imported. Small epsilons are added to guard against log(0).

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated WM and load-dependent WM precision.

    Idea:
    - Model-free RL with a single learning rate and softmax decision rule.
    - WM is a rapidly updated associative store that writes strongly on correct feedback
      and weakly suppresses incorrect actions on errors.
    - The effective weight of WM in the policy is gated by state-specific uncertainty
      (fewer visits => higher WM weight) and penalized by set size (higher load => lower WM weight).
    - WM precision also degrades with set size via a small drift toward uniform.

    Parameters (model_parameters):
    - lr: scalar in [0,1], RL learning rate.
    - softmax_beta: scalar >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight0: real, baseline WM mixture weight before gating (passed through sigmoid).
    - wm_gain_unc: >= 0, strength of uncertainty gating; higher means fewer visits give more WM weight.
    - wm_precision_load: >= 0, penalizes WM by set size (both weight gating and precision via drift).
    - wm_write_strength: in [0,1], strength of WM updates on each trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, wm_gain_unc, wm_precision_load, wm_write_strength = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # State-specific visit counts for uncertainty gating
        visits = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Gentle load-dependent WM drift toward uniform to reduce precision under high load
            drift = 1.0 - np.exp(-0.1 * wm_precision_load * max(1.0, nS) / 3.0)
            drift = np.clip(drift, 0.0, 1.0)
            w = (1.0 - drift) * w + drift * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability for the chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W_s (deterministic when peaked)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty gating for WM weight: fewer visits -> higher weight; scaled down by load
            unc = 1.0 / np.sqrt(visits[s] + 1.0)  # in (0,1]
            load_penalty = wm_precision_load * (max(1.0, nS) - 3.0) / 3.0
            gate_input = wm_weight0 + wm_gain_unc * unc - load_penalty
            wm_weight_eff = np.clip(sigmoid(gate_input), 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + eps)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: strong write on correct, mild suppression of chosen action on error
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_write_strength) * w[s, :] + wm_write_strength * target
            else:
                # Suppress the chosen action slightly, redistribute to others
                w_s = np.copy(w[s, :])
                w_s[a] = w_s[a] * (1.0 - 0.5 * wm_write_strength)
                # Renormalize by distributing removed mass uniformly
                removed = w[s, :].sum() - w_s.sum()
                w_s += removed * w_0[s, :]
                w_s = w_s / np.sum(w_s)
                w[s, :] = w_s

            visits[s] += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and WM capacity/refresh with interference and lapse.

    Idea:
    - Model-free RL with separate learning rates for positive and negative prediction errors.
    - WM has a capacity-limited refresh that depends on set size: larger sets reduce effective capacity.
    - Between trials, WM suffers interference that pushes all WM entries toward uniform (global interference).
    - Lapse component injects random choices with small probability.

    Parameters (model_parameters):
    - lr_pos: in [0,1], RL learning rate for rewarded trials (positive PE).
    - lr_neg: in [0,1], RL learning rate for unrewarded trials (negative PE).
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_capacity: in [0,1], base capacity; effective WM weight scales down with set size.
    - interference_rate: in [0,1], global WM interference per trial (stronger under larger set sizes).
    - lapse: in [0,1], probability of random choice.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_capacity, interference_rate, lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM capacity weight declines with set size
        cap_eff = wm_capacity * (3.0 / max(3.0, float(nS)))
        cap_eff = np.clip(cap_eff, 0.0, 1.0)

        # Interference magnitude scales with set size
        base_interf = interference_rate * (max(1.0, float(nS)) - 3.0) / 3.0
        base_interf = np.clip(base_interf, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global interference: WM drifts toward uniform each trial
            w = (1.0 - base_interf) * w + base_interf * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_mix = cap_eff * p_wm + (1.0 - cap_eff) * p_rl

            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(p_total + eps)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM refresh: if rewarded, write strongly toward one-hot; else partial reset
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - cap_eff) * w[s, :] + cap_eff * target
            else:
                # Partial forgetting on errors
                w[s, :] = (1.0 - 0.5 * cap_eff) * w[s, :] + (0.5 * cap_eff) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces and WM as a decaying episodic binding with load-scaled lifetime.

    Idea:
    - Model-free RL updated via eligibility traces (lambda), allowing extended credit assignment.
    - WM stores the last rewarded action per state as an episodic binding with a lifetime that
      decays each trial; lifetimes are shorter under high load (larger set size).
    - WM policy becomes more uniform as the remaining lifetime decreases; mixing weight is also
      penalized by load.

    Parameters (model_parameters):
    - lr: in [0,1], RL learning rate.
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - lam: in [0,1], eligibility trace decay; higher -> longer-lasting eligibility.
    - wm_weight_base: in [0,1], base WM contribution before load penalty.
    - life_base: >= 0, baseline WM lifetime after a correct outcome.
    - load_scale: >= 0, how strongly set size reduces both WM lifetime and weight.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, lam, wm_weight_base, life_base, load_scale = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility traces over state-action pairs
        e = np.zeros((nS, nA))

        # Load effects on WM lifetime and weight
        load_factor = (max(1.0, float(nS)) - 3.0) / 3.0
        weight_penalty = np.exp(-load_scale * load_factor)  # in (0,1]
        wm_weight_eff = np.clip(wm_weight_base * weight_penalty, 0.0, 1.0)

        # Per-trial multiplicative decay of WM lifetimes
        life_decay = np.exp(-load_scale * max(1.0, float(nS)) / 3.0)

        # Per-state WM lifetimes (continuous, >= 0)
        life = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Decay WM lifetimes each trial
            life = life * life_decay

            Q_s = q[s, :]
            # Construct a WM policy that mixes the stored one-hot with uniform by lifetime
            # Normalize lifetime into [0,1] sensitivity via 1 - exp(-life)
            Ls = 1.0 - np.exp(-life[s])  # increases with life, saturates below 1
            W_s = (1.0 - Ls) * w_0[s, :] + Ls * w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(p_total + eps)

            # RL with eligibility traces
            delta = r - Q_s[a]
            # Decay all eligibilities, then assign current one
            e = lam * e
            e[s, a] += 1.0
            q += lr * delta * e

            # WM update: on reward, store a strong one-hot and reset lifetime; on error, small diffusion
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                # Write deterministically to target
                w[s, :] = target
                # Reset lifetime with load-scaled base
                life[s] = life_base * weight_penalty
            else:
                # Mild diffusion toward uniform on errors
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p