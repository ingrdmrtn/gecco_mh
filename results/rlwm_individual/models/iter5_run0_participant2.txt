def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-gated WM with load-dependent leak.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores associative action preferences per state (w), expressed as a softmax policy.
    - WM mixture weight is capacity-gated: higher for small set sizes and lower for large set sizes.
      Specifically, effective WM weight = wm_base * sigmoid(gate_temp * (wm_capacity - nS)).
    - WM leaks toward uniform each trial at a rate wm_leak that does NOT change across blocks, but
      its behavioral influence changes via the capacity gate.
    - WM encodes the chosen action proportionally to the obtained reward (Hebbian, reward-gated).

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate.
    - wm_capacity: positive scalar. Effective capacity; larger values sustain WM at higher set sizes.
    - wm_base: scalar in (0,1). Base WM weight before capacity gating.
    - softmax_beta: positive scalar. Inverse temperature for RL softmax (internally scaled x10).
    - gate_temp: positive scalar. Steepness of the capacity gate (logistic slope).
    - wm_leak: scalar in (0,1). WM leak toward uniform on each trial.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_capacity, wm_base, softmax_beta, gate_temp, wm_leak = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-gated WM weight (constant within a block, depends on nS)
        gate = 1.0 / (1.0 + np.exp(-gate_temp * (wm_capacity - nS)))
        wm_eff_block = np.clip(wm_base * gate, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action a (deterministic-like)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_eff_block * p_wm + (1 - wm_eff_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # Reward-gated WM encoding (Hebbian push toward chosen action when rewarded)
            if r > 0.0:
                # Push distribution toward a one-hot on action a by replacing a fraction equal to r
                # Equivalent to convex combination with a delta vector on a
                w[s, :] = (1.0 - r) * w[s, :]
                w[s, a] += r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM with cross-state interference.

    Idea:
    - RL uses separate learning rates for positive vs negative prediction errors.
    - WM stores rewarded actions with high precision, but encoding interferes across states:
      when a correct action is stored for one state, a small fraction of that association
      "spills over" to the same action in other states, causing misbinding that scales with set size.
    - Choices are a mixture of RL and WM policies.

    Parameters (model_parameters):
    - alpha_pos: scalar in (0,1). RL learning rate for positive PE.
    - alpha_neg: scalar in (0,1). RL learning rate for negative PE.
    - wm_weight: scalar in (0,1). Mixture weight of WM relative to RL.
    - softmax_beta: positive scalar. Inverse temperature for RL softmax (internally scaled x10).
    - interference_rate: scalar in [0,1]. Fraction of WM encoding that diffuses to other states.
    - wm_precision: positive scalar. Scales WM policy sharpness by multiplying softmax_beta_wm.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, interference_rate, wm_precision = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50 * max(wm_precision, 1e-6)  # very deterministic, scaled by precision
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective per-block interference magnitude scales with number of potential distractor states
        # so the total diffusion to others is interference_rate * (nS-1)/nS.
        kappa_block = interference_rate * (nS - 1) / max(nS, 1)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            wm_eff = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_eff * p_wm + (1 - wm_eff) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update (asymmetric)
            pe = r - Q_s[a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            q[s][a] += lr * pe

            # WM decay mildly toward uniform to stabilize (tiny implicit leak via interference)
            # Not an explicit decay parameter; interference provides the main regularizer.

            # WM encoding on reward with interference across other states
            if r > 0.0:
                # First, strengthen correct action in current state
                w[s, :] *= (1.0 - r)
                w[s, a] += r

                # Then, diffuse a fraction of that encoding to the same action in other states
                # Distribute equally to other states and renormalize their rows
                if nS > 1 and kappa_block > 0:
                    spill = r * kappa_block / (nS - 1)
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        # Move other states slightly toward choosing action a
                        w[s2, :] = (1.0 - spill) * w[s2, :]
                        w[s2, a] += spill

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + win-stay WM with load-dependent retrieval.

    Idea:
    - RL learns Q-values; exploration via softmax.
    - WM implements a win-stay controller: if the last time a state was rewarded,
      WM stores that action and tends to repeat it. If not rewarded or never seen rewarded,
      WM is uninformative (uniform).
    - WM influence depends on a retrieval probability that decreases with set size using
      a saturating capacity function: p_recall = rho / (1 + (nS-1)/k_cap).
    - Effective mixture = wm_weight * p_recall.
    - WM representation w holds the last "rewarded action" per state as a one-hot distribution;
      absent a stored reward, the row is uniform.

    Parameters (model_parameters):
    - lr: scalar in (0,1). RL learning rate.
    - wm_weight: scalar in (0,1). Base WM weight before retrieval gating.
    - softmax_beta: positive scalar. Inverse temperature for RL softmax (internally scaled x10).
    - rho: scalar in (0,1]. Asymptotic retrieval probability at small set sizes.
    - k_cap: positive scalar. Capacity constant controlling how retrieval diminishes with nS.

    Returns:
    - Negative log-likelihood of the observed action sequence under the model.
    """
    lr, wm_weight, softmax_beta, rho, k_cap = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Retrieval probability as a function of set size
        p_recall = rho / (1.0 + (max(nS, 1) - 1.0) / max(k_cap, 1e-6))
        p_recall = float(np.clip(p_recall, 0.0, 1.0))
        wm_eff_block = np.clip(wm_weight * p_recall, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if a last rewarded action exists, W_s will be near one-hot; else uniform.
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_eff_block * p_wm + (1 - wm_eff_block) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: win-stay memory
            if r > 0.0:
                # Store last rewarded action as a one-hot distribution
                w[s, :] = 0.0 * w[s, :]
                w[s, a] = 1.0
            else:
                # If not rewarded, WM carries no informative content for this state (reset to uniform)
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p