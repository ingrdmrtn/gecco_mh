def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actor-Critic with load- and age-modulated decision temperature.

    Mechanism:
    - Policy is a softmax over learned action preferences (actor), with a state-value baseline (critic).
    - Learning uses an advantage signal: delta = r - V(s).
    - Decision temperature is reduced under higher set size (cognitive load) and for older adults.
    - Per-block learning (values reset each block).

    Parameters (model_parameters):
    - alpha_p: policy (actor) learning rate in [0,1]
    - alpha_v: value (critic) learning rate in [0,1]
    - beta0: base inverse temperature (>0)
    - slope_load_temp: scales how much larger set size reduces effective temperature (>=0)
    - age_temp_shift: fractional reduction of temperature for older adults (>=0)
      Effective beta = beta0 * exp(-slope_load_temp * (nS - 3)) * (1 - age_temp_shift * is_older)

    Inputs:
    - states: array of state indices (ints)
    - actions: array of chosen action indices (ints; out-of-range treated as lapses with uniform prob)
    - rewards: array of feedback (expects 0/1; values <=0 treated as 0, >0 as 1 for learning)
    - blocks: array indicating block index per trial
    - set_sizes: array indicating set size per trial (3 or 6)
    - age: array-like with a single value (years)
    - model_parameters: [alpha_p, alpha_v, beta0, slope_load_temp, age_temp_shift]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha_p, alpha_v, beta0, slope_load_temp, age_temp_shift = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Initialize actor (preferences) and critic (state values)
        P = np.zeros((nS, nA))
        V = np.zeros(nS)

        # Effective temperature for the block (load- and age-modulated)
        load_term = np.exp(-slope_load_temp * max(0, nS - 3))
        age_term = max(1e-3, 1.0 - age_temp_shift * is_older)
        beta_eff = beta0 * load_term * age_term

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = 1.0 if b_rewards[t] > 0 else 0.0

            # Action probabilities from preferences
            logits = beta_eff * (P[s, :] - np.max(P[s, :]))
            pi = np.exp(logits)
            pi = pi / (np.sum(pi) + eps)

            # Likelihood contribution
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # Skip learning on invalid actions
                continue
            else:
                p_a = pi[a]
                nll -= np.log(max(p_a, eps))

            # Critic update
            delta = r - V[s]
            V[s] += alpha_v * delta

            # Actor update (policy gradient with baseline)
            # Equivalent to increasing preference for chosen action and decreasing others
            for aa in range(nA):
                grad = (1.0 if aa == a else 0.0) - pi[aa]
                P[s, aa] += alpha_p * delta * grad

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited working memory (WM) with RL fallback and perseveration.

    Mechanism:
    - A limited number of state->action bindings are stored in WM when rewarded.
      Capacity is K_eff = clip(K_base - age_cap_shift * is_older, 0, 6).
    - Arbitration weight w = min(1, K_eff / set_size) (stronger reliance on WM when load <= capacity).
    - WM policy is deterministic for remembered states, otherwise uniform over actions.
    - RL Q-learning provides graded values with softmax choice.
    - Perseveration bias adds a stickiness bonus to the previously chosen action.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1]
    - beta: inverse temperature for RL softmax (>0)
    - K_base: baseline WM capacity (suggest 0..6)
    - age_cap_shift: capacity decrement for older adults (>=0)
    - stick: perseveration strength added to the last action's logit (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays as described above
    - age: array-like with a single value (years)
    - model_parameters: [alpha, beta, K_base, age_cap_shift, stick]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, K_base, age_cap_shift, stick = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: -1 indicates absent; otherwise stores the chosen action
        wm_binding = -np.ones(nS, dtype=int)
        # A simple FIFO list to enforce capacity
        wm_fifo = []

        # Effective capacity and WM weight
        K_eff = float(np.clip(K_base - age_cap_shift * is_older, 0.0, 6.0))
        w = min(1.0, K_eff / max(1.0, float(nS)))

        # Track previous action for perseveration
        prev_action = -1

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = 1.0 if b_rewards[t] > 0 else 0.0

            # WM policy
            if wm_binding[s] >= 0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_binding[s])] = 1.0
            else:
                p_wm = np.ones(nA) / nA

            # RL policy with perseveration bias
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if prev_action >= 0 and prev_action < nA:
                logits[prev_action] += stick
            p_rl = np.exp(logits)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Arbitration
            p_mix = w * p_wm + (1.0 - w) * p_rl

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                prev_action = a  # update even if invalid to reflect observed behavior
                continue
            else:
                p_a = p_mix[a]
                nll -= np.log(max(p_a, eps))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update on reward: store binding and enforce capacity
            if r > 0.0:
                if wm_binding[s] != a:
                    wm_binding[s] = a
                    # Add to FIFO (remove previous occurrence if any)
                    if s in wm_fifo:
                        wm_fifo.remove(s)
                    wm_fifo.append(s)
                    # Enforce capacity by evicting oldest
                    while len(wm_fifo) > int(np.floor(K_eff + 1e-9)):
                        s_old = wm_fifo.pop(0)
                        wm_binding[s_old] = -1

            prev_action = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Kalman Temporal-Difference (uncertainty-weighted RL) with uncertainty-driven exploration.
    
    Mechanism:
    - For each state-action, maintain a Gaussian belief over Q with mean and variance.
    - Observation noise increases with set size (load), process noise increases with age.
    - Kalman gain adaptively sets learning rate: K = S / (S + R).
    - Action selection uses a softmax over (mean Q + exploration bonus proportional to sqrt(variance)).

    Parameters (model_parameters):
    - beta: inverse temperature for softmax over augmented values (>0)
    - tau_p: process noise scale added to variance each trial (>=0)
    - sigma0: initial variance of Q estimates (>=0)
    - age_uncert_bias: multiplicative boost of process noise for older adults (>=0)
    - load_noise_scale: additive observation noise increase per extra item beyond 3 (>=0)

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays as described above
    - age: array-like with a single value (years)
    - model_parameters: [beta, tau_p, sigma0, age_uncert_bias, load_noise_scale]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    beta, tau_p, sigma0, age_uncert_bias, load_noise_scale = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Initialize means and variances
        Q_mean = np.zeros((nS, nA))
        Q_var = np.full((nS, nA), max(1e-6, sigma0))

        for t in range(len(b_states)):
            s = b_states[t]
            a = b_actions[t]
            r = 1.0 if b_rewards[t] > 0 else 0.0
            nS_t = float(b_set_sizes[t])

            # Add process noise (age-modulated) to all actions at this state
            proc = tau_p * (1.0 + age_uncert_bias * is_older)
            if proc > 0.0:
                Q_var[s, :] = Q_var[s, :] + proc

            # Observation noise grows with load (beyond 3)
            R = 1.0 + load_noise_scale * max(0.0, nS_t - 3.0)

            # Softmax over augmented values: mean + sqrt(variance)
            bonus = np.sqrt(np.maximum(Q_var[s, :], 1e-12))
            aug = Q_mean[s, :] + bonus
            logits = beta * (aug - np.max(aug))
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                continue
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # Kalman update for chosen action
            S_aa = Q_var[s, a]
            K_gain = S_aa / (S_aa + R)
            pe = r - Q_mean[s, a]
            Q_mean[s, a] = Q_mean[s, a] + K_gain * pe
            Q_var[s, a] = (1.0 - K_gain) * S_aa

    return nll