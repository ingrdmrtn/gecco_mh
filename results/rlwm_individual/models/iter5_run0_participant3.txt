def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and separate WM learning/decay.

    Mechanism:
    - RL: delta-rule Q-learning with softmax.
    - WM: a fast table of action weights per state, used via a sharp softmax.
    - Arbitration: dynamic mixture weight favors the policy with lower entropy
      (higher confidence). The baseline wm_weight is transformed via a logistic
      gain driven by the entropy difference between WM and RL.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_lr, wm_decay, entropy_temp)
        - lr: RL learning rate in [0,1].
        - wm_weight: baseline mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - wm_lr: WM learning rate toward one-hot upon reward in [0,1].
        - wm_decay: WM decay toward uniform when not rewarded in [0,1].
        - entropy_temp: sensitivity of arbitration to entropy difference (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, wm_decay, entropy_temp = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy via softmax and entropy-based arbitration
            p_vec_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            p_vec_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            p_vec_rl = p_vec_rl / np.sum(p_vec_rl)

            # Entropies (natural units)
            eps = 1e-12
            H_wm = -np.sum(p_vec_wm * np.log(np.clip(p_vec_wm, eps, 1.0)))
            H_rl = -np.sum(p_vec_rl * np.log(np.clip(p_vec_rl, eps, 1.0)))
            # Lower entropy => more confidence; shift baseline weight by (H_rl - H_wm)
            # so that WM weight increases when WM is more confident than RL.
            logit = lambda x: np.log(np.clip(x, eps, 1-eps)) - np.log(np.clip(1-x, eps, 1-eps))
            inv_logit = lambda z: 1.0 / (1.0 + np.exp(-z))
            wm_eff = inv_logit(logit(wm_weight) + entropy_temp * (H_rl - H_wm))

            p_total = p_wm*wm_eff + (1-wm_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating: rewarded -> learn one-hot; otherwise decay to uniform
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with outcome-gated WM reliance and unified WM update/decay rate.

    Mechanism:
    - RL: delta-rule Q-learning with softmax.
    - WM: fast softmax policy over a WM table.
    - Gating: a latent gate g accumulates outcome valence. Positive outcomes boost
      reliance on WM (win_boost), negative outcomes suppress it (loss_suppression).
      g integrates with rate kappa and modulates the effective WM weight via a
      logistic transform around baseline wm_weight.
    - WM update: if rewarded, WM moves toward one-hot with rate kappa; otherwise
      decays toward uniform with the same rate (unified update/decay).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, win_boost, loss_suppression, kappa)
        - lr: RL learning rate in [0,1].
        - wm_weight: baseline mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - win_boost: increment added to gate on reward (>=0).
        - loss_suppression: decrement applied to gate on no-reward (>=0).
        - kappa: integration/update rate in [0,1] for gate and WM updates.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, win_boost, loss_suppression, kappa = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Latent gate initialized at neutral (0)
        g = 0.0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy via softmax
            p_vec_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            # Outcome-gated WM reliance: update g then compute wm_eff
            target_g = (win_boost if r > 0.5 else -loss_suppression)
            g = (1.0 - kappa) * g + kappa * target_g

            eps = 1e-12
            logit = lambda x: np.log(np.clip(x, eps, 1-eps)) - np.log(np.clip(1-x, eps, 1-eps))
            inv_logit = lambda z: 1.0 / (1.0 + np.exp(-z))
            wm_eff = inv_logit(logit(wm_weight) + g)

            p_total = p_wm*wm_eff + (1-wm_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM updating: rewarded -> learn one-hot; else decay to uniform (both at rate kappa)
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - kappa) * w[s, :] + kappa * one_hot
            else:
                w[s, :] = (1.0 - kappa) * w[s, :] + kappa * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WSLS-style WM with load-dependent WM precision and lapse.

    Mechanism:
    - RL: delta-rule Q-learning with softmax.
    - WM policy: Win-Stay/Lose-Shift heuristic per state. If previous trial in
      state s was rewarded, repeat last action with prob ws; else choose any
      other action (not last) with prob ws split across the two alternatives.
      With 1-ws, choose uniformly at random (captures imperfect WM control).
    - Load effects: ws decreases with set size; an additional lapse mixes WM
      policy with uniform as set size increases.
    - WM update table w is maintained for completeness: it is nudged toward the
      one-hot of rewarded action and otherwise decays to uniform using ws_base
      as a step size.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, ws_base, load_slope, lapse0)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - ws_base: baseline probability (0..1) of following WSLS in set size 3.
        - load_slope: how strongly increased set size (to 6) reduces ws and
          increases lapse (>=0).
        - lapse0: baseline lapse (0..1) mixed with uniform in set size 3.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, ws_base, load_slope, lapse0 = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # not used directly; WM uses WSLS heuristic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Per-state memory for WSLS
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]  # maintained but not used directly in WSLS
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: WSLS with load-dependent ws and lapse
            # Map set size effect: when nS=3, delta=0; when nS=6, delta=1
            load_delta = max(0, (nS - 3) / 3.0)
            # Decrease ws with load via a logistic transform
            eps = 1e-12
            logit = lambda x: np.log(np.clip(x, eps, 1-eps)) - np.log(np.clip(1-x, eps, 1-eps))
            inv_logit = lambda z: 1.0 / (1.0 + np.exp(-z))
            ws = inv_logit(logit(ws_base) - load_slope * load_delta)
            lapse = np.clip(inv_logit(logit(lapse0) + load_slope * load_delta), 0.0, 0.49)

            # Construct WM choice probabilities under WSLS
            p_vec_wm = np.ones(nA) / nA  # default uniform
            if last_action[s] != -1:
                la = int(last_action[s])
                lrwd = last_reward[s]
                p_vec_wm = np.ones(nA) / nA  # start uniform
                if lrwd > 0.5:
                    # Win-Stay: with prob ws repeat la; else uniform
                    p_vec_wm = (1.0 - ws) * (np.ones(nA) / nA)
                    p_vec_wm[la] += ws
                else:
                    # Lose-Shift: with prob ws choose one of the two non-la (split equally)
                    p_vec_wm = (1.0 - ws) * (np.ones(nA) / nA)
                    others = [i for i in range(nA) if i != la]
                    for o in others:
                        p_vec_wm[o] += ws / 2.0
            # Add load-dependent lapse mixing with uniform
            p_vec_wm = (1.0 - lapse) * p_vec_wm + lapse * (np.ones(nA) / nA)
            p_wm = p_vec_wm[a]

            # Combine WM and RL
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM "value" updating (ancillary to WSLS memory): rewarded -> one-hot; else decay
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - ws_base) * w[s, :] + ws_base * one_hot
            else:
                w[s, :] = (1.0 - ws_base) * w[s, :] + ws_base * w_0[s, :]

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p