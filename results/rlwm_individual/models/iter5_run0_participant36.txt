def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: RL + capacity-limited WM gating with age- and set-size–dependent storage and decay

    Mechanism:
      - Model-free Q-learning within blocks.
      - Parallel capacity-limited working memory (WM) that stores a single action per state when reward=1.
      - Probability that a state's mapping is successfully stored (and maintained) depends on an effective capacity K
        and set size nS via a logistic gating control; older adults have reduced gating (age bias).
      - The decision policy mixes WM and RL according to the current probability that the state is in WM.
      - A small lapse puts probability mass on random actions.

    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Base inverse temperature for RL (>0). WM uses a scaled version for sharper recall.
    wm_slot_cap : float
        Latent capacity control; transformed to an effective capacity K in [1, 3].
    gate_gain : float
        Gain of the logistic gating; higher increases sensitivity to set size (interference).
    age_gate_bias : float
        Additive bias on gating log-odds for older adults (>=45). Negative -> weaker WM gating in older.
    lapse : float
        Lapse probability in [0, 0.2]; mixed with uniform to capture random slips.

    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial (typically 3 or 6); assumed constant within a block.
    age : array-like or scalar
        Participant age; older group defined as age >= 45.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, wm_slot_cap, gate_gain, age_gate_bias, lapse = model_parameters
    beta = max(beta, eps) * 5.0
    lapse = np.clip(lapse, 0.0, 0.2)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    # Map wm_slot_cap to an effective capacity K in [1, 3]
    K = 1.0 + 2.0 / (1.0 + np.exp(-wm_slot_cap))

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))

        # WM content: stored action per state (-1 if none), and probability of being available
        WM_action = -1 * np.ones(nS, dtype=int)
        WM_prob = np.zeros(nS, dtype=float)

        # Logistic gating probability baseline as a function of set size, capacity, and age
        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        gate_logit = gate_gain * (K - nS) + age_gate_bias * is_older
        p_gate = 1.0 / (1.0 + np.exp(-gate_logit))  # how likely WM succeeds (store/maintain) at this nS and age

        # Simple decay per trial increases with set size/interference using same control signal
        decay = np.clip(0.15 + 0.35 * ss_factor * (1.0 - p_gate), 0.0, 0.9)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # WM decay
            WM_prob *= (1.0 - decay)
            # If decay clears memory, mark as absent
            if WM_prob[s] < 1e-6:
                WM_action[s] = -1
                WM_prob[s] = 0.0

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl /= (np.sum(p_rl) + eps)
            p_rl = (1.0 - lapse) * p_rl + lapse * (1.0 / nA)

            # WM policy
            if WM_action[s] >= 0:
                p_wm = np.full(nA, eps)
                p_wm[WM_action[s]] = 1.0
            else:
                # If no stored action, WM is effectively uninformative (uniform with small noise)
                p_wm = np.ones(nA) / nA
            # Make WM sharper using a scaled beta but still apply lapse
            wm_beta = 3.0 * beta
            logits_wm = np.log(np.clip(p_wm, eps, 1.0))
            logits_wm -= np.max(logits_wm)
            p_wm = np.exp(wm_beta * logits_wm)
            p_wm /= (np.sum(p_wm) + eps)
            p_wm = (1.0 - lapse) * p_wm + lapse * (1.0 / nA)

            # Mixture by current WM availability for this state
            mix = np.clip(WM_prob[s], 0.0, 1.0)
            p = mix * p_wm + (1.0 - mix) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM encoding on reward with gating: boost availability and store action
            if r > 0.5:
                WM_action[s] = a
                WM_prob[s] = 1.0 - (1.0 - WM_prob[s]) * (1.0 - p_gate)  # move WM_prob toward 1 with strength p_gate

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Asymmetric RL with age-modulated sensitivity to negative outcomes + set-size–scaled UCB exploration

    Mechanism:
      - Q-learning with asymmetric learning rates for positive vs negative PEs.
      - Asymmetry is constructed from a base alpha and an age-based multiplicative boost for negative learning.
      - A directed exploration bonus (UCB-like) is added to action values at choice time:
        bonus ~ gain / sqrt(N_sa + 1), scaled up with larger set sizes.
      - A small lapse places mass on uniform.

    Parameters (model_parameters)
    --------------------------------
    alpha_base : float
        Base learning rate in [0,1]; used for positive PEs.
    beta : float
        Inverse temperature (>0).
    age_neg_boost : float
        Multiplicative boost applied to negative-PE learning in older adults (>=45). Positive -> stronger learning from errors.
    ucb_gain : float
        Strength of the uncertainty exploration bonus.
    ss_ucb_slope : float
        How much the UCB bonus increases with set size (relative to 3). Positive -> more directed exploration at nS=6.
    lapse : float
        Lapse probability in [0, 0.2].

    Inputs
    ------
    states : array-like (T,)
        State index per trial.
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial; constant within block.
    age : array-like or scalar
        Participant age; older group defined as age >= 45.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha_base, beta, age_neg_boost, ucb_gain, ss_ucb_slope, lapse = model_parameters
    beta = max(beta, eps) * 5.0
    lapse = np.clip(lapse, 0.0, 0.2)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        # Age-modulated negative learning rate
        alpha_pos = np.clip(alpha_base, 0.0, 1.0)
        alpha_neg = np.clip(alpha_base * (1.0 + age_neg_boost * is_older), 0.0, 1.0)
        # Set-size scaled UCB gain
        ucb_strength = max(0.0, ucb_gain) * (1.0 + ss_ucb_slope * ss_factor)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Choice policy: Q + UCB bonus
            q_s = Q[s, :].copy()
            bonus = ucb_strength / np.sqrt(N[s, :] + 1.0)
            logits = beta * (q_s + bonus - np.max(q_s + bonus))
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)
            p = (1.0 - lapse) * p + lapse * (1.0 / nA)

            total_logp += np.log(np.clip(p[a], eps, 1.0))

            # Update counts and Q
            N[s, a] += 1.0
            pe = r - Q[s, a]
            if pe >= 0.0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: Volatility-adaptive exploration via PE-driven temperature with age and set-size modulation

    Mechanism:
      - Standard Q-learning for values.
      - A scalar volatility signal v_t is updated from the magnitude of the reward prediction error.
      - The inverse temperature beta_t adapts online: beta_t = beta0 / (1 + v_t).
        Higher volatility -> lower beta_t (more exploration).
      - The volatility learning rate k is modulated by set size and age: larger sets and older adults track volatility faster,
        reflecting greater sensitivity/need for exploration under load.
      - Mild value forgetting increases with set size, ensuring interference pressure is represented.
      - A lapse puts small mass on uniform.

    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta0 : float
        Baseline inverse temperature (>0) when volatility is zero.
    tau_lr_base : float
        Base logit for volatility learning rate; passed through sigmoid to get k in (0,1).
    age_tau_shift : float
        Additive shift to tau_lr_base for older adults (>=45); positive -> faster volatility updates.
    forget_ss : float
        Strength with which set size increases both volatility learning and value forgetting.
    lapse : float
        Lapse probability in [0, 0.2].

    Inputs
    ------
    states : array-like (T,)
        State per trial.
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary reward per trial.
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size per trial; constant within a block.
    age : array-like or scalar
        Participant age; older group defined as age >= 45.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta0, tau_lr_base, age_tau_shift, forget_ss, lapse = model_parameters
    beta0 = max(beta0, eps) * 5.0
    lapse = np.clip(lapse, 0.0, 0.2)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))
        v = 0.0  # volatility proxy within block

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        # Volatility learning rate k in (0,1), increases with set size and age
        k = sigmoid(tau_lr_base + age_tau_shift * is_older)
        k = np.clip(k * (1.0 + np.clip(forget_ss, 0.0, 2.0) * ss_factor), 0.0, 0.99)

        # Value forgetting increases with set size via forget_ss
        f = np.clip(0.0 + 0.25 * ss_factor * max(0.0, forget_ss), 0.0, 0.25)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Compute adaptive beta
            beta_t = beta0 / (1.0 + v)

            # Policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p = np.exp(beta_t * q_s)
            p = p / (np.sum(p) + eps)
            p = (1.0 - lapse) * p + lapse * (1.0 / nA)

            total_logp += np.log(np.clip(p[a], eps, 1.0))

            # Update Q with forgetting
            Q *= (1.0 - f)
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update volatility from PE magnitude
            v = (1.0 - k) * v + k * np.abs(pe)

    return -float(total_logp)