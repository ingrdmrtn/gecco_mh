def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with load-dependent WM encoding strength and decay.
    
    Mechanism:
    - RL: standard Q-learning with learning rate lr and softmax choice rule (beta scaled by 10).
    - WM: a categorical memory per state w[s,:] interpreted as a choice probability distribution.
      The WM policy is p_wm = w[s,a] (direct readout).
      After rewarded choices, WM stores a one-hot-like distribution whose sharpness depends on set size
      (stronger for small sets, weaker for large sets). After non-reward, WM decays toward uniform,
      with faster decay at larger set sizes.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.
    
    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Mixture weight for WM policy.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Direct readout from WM distribution; optionally sharpened slightly to reduce ties.
            # Implement load-dependent effective WM certainty by blending with uniform before readout.
            # g_enc is higher at small set sizes (more reliable WM).
            g_enc = min(1.0, 3.0 / max(1.0, nS))
            W_eff = g_enc * W_s + (1.0 - g_enc) * (1.0 / nA)
            W_eff = np.clip(W_eff, 1e-12, 1.0)
            W_eff /= np.sum(W_eff)
            p_wm = W_eff[a]
            p_wm = float(np.clip(p_wm, 1e-12, 1.0))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Rewarded trials: store a one-hot-like distribution with load-dependent sharpness.
            if r >= 1.0 - 1e-12:
                sharp = min(1.0, 3.0 / max(1.0, nS))  # stronger binding at smaller set size
                w[s, :] = (1.0 - sharp) * (1.0 / nA)
                w[s, a] += sharp
            else:
                # Non-reward: decay toward uniform faster at larger load.
                # Decay factor increases with nS: when nS=3 -> mild decay; nS=6 -> stronger decay.
                decay = 0.3 + 0.4 * max(0.0, (nS - 3.0) / 3.0)  # in [0.3, 0.7] for nS in {3,6}
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # Normalize to guard against drift
            w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with interference-based WM and RL softmax; WM policy via softmax over cached responses.
    
    Mechanism:
    - RL: standard Q-learning with learning rate lr and softmax choice rule (beta scaled by 10).
    - WM: w[s,:] is a cached response strength vector. Policy uses a near-deterministic softmax over W_s.
      After reward, the chosen action strength is boosted; after non-reward, strengths spread toward uniform.
      Additionally, at each step, WM for the current state is contaminated by interference from other states'
      caches, with stronger interference at larger set sizes.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.
    
    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Mixture weight for WM policy.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy via softmax over cached strengths with high temperature.
            # Before computing policy, apply interference: blend current state's cache
            # with the average cache of other states; stronger at larger set sizes.
            if nS > 1:
                others_idx = [i for i in range(nS) if i != s]
                mean_other = np.mean(w[others_idx, :], axis=0)
            else:
                mean_other = w_0[s, :]
            interference = 0.1 + 0.3 * max(0.0, (nS - 3.0) / 3.0)  # 0.1 for 3-back, 0.4 for 6-back
            W_mix = (1.0 - interference) * W_s + interference * mean_other
            W_mix = np.clip(W_mix, 1e-6, None)
            W_mix /= np.sum(W_mix)

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_mix - W_mix[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward boosts chosen cache; non-reward spreads toward uniform.
            if r >= 1.0 - 1e-12:
                boost = 0.6 if nS == 3 else 0.4  # smaller boost at higher load
                w[s, :] = (1.0 - boost) * w[s, :] + boost * w_0[s, :]
                w[s, a] += boost  # add extra mass to chosen action
            else:
                spread = 0.4 if nS == 3 else 0.6  # spread more at higher load
                w[s, :] = (1.0 - spread) * w[s, :] + spread * w_0[s, :]
            # Renormalize
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with entropy-attenuated WM policy (implicit arbitration).
    
    Mechanism:
    - RL: standard Q-learning with lr and softmax choice rule (beta scaled by 10).
    - WM: w[s,:] is a memory distribution over actions. WM policy computed by first attenuating
      WM toward uniform based on its own sharpness and set size:
        W_eff = g * w[s,:] + (1 - g) * uniform, where g = (3/nS) * sharpness(w[s,:]).
      This makes WM policy less informative under high load or when memory is uncertain,
      even if the explicit mixture weight wm_weight is fixed.
      Policy then uses a near-deterministic softmax over W_eff.
    - Learning: on reward, set w[s,:] close to one-hot; on non-reward, decay toward uniform.
    
    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Mixture weight for WM policy.
    - softmax_beta: scalar >= 0. Inverse temperature for RL softmax (scaled internally by 10).
    
    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute WM sharpness via entropy; then attenuate by set size.
            W_safe = np.clip(W_s, 1e-12, 1.0)
            entropy = -np.sum(W_safe * np.log(W_safe))
            max_entropy = np.log(nA)
            sharp = 1.0 - (entropy / max_entropy)  # in [0,1]
            g = min(1.0, 3.0 / max(1.0, nS)) * sharp  # less weight at larger set size or high entropy
            W_eff = g * W_s + (1.0 - g) * (1.0 / nA)
            W_eff = np.clip(W_eff, 1e-12, None)
            W_eff /= np.sum(W_eff)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r >= 1.0 - 1e-12:
                # Write close to one-hot with load-sensitive sharpness
                write = 0.8 if nS == 3 else 0.5
                w[s, :] = (1.0 - write) * w_0[s, :]
                w[s, a] += write
            else:
                # Decay magnitude modulated by current uncertainty: more uncertain -> less change
                decay_base = 0.3 if nS == 3 else 0.6
                decay = decay_base * (0.5 + 0.5 * sharp)  # decay more when sharper (to forget errors)
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p