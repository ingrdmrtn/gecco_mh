Below are three standalone cognitive models that implement RL+WM mixtures per your template. Each function:
- Uses the required signature and returns negative log-likelihood
- Keeps the template structure and fills in the WM policy and updates where indicated
- Uses all parameters meaningfully (â‰¤ 6 parameters each)
- Incorporates a set-size (3 vs 6) effect in distinct ways
- Avoids repeating the exact parameter combinations you listed

Note: Assumes numpy as np is available globally (no imports inside functions).


def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with smooth WM binding and state-wise WM stickiness; WM impact decreases with set size.
    
    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta*10.
    - WM channel: softmax over WM weights W, with an additive stickiness bonus for the last chosen action in that state.
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.

    WM dynamics:
    - Smooth binding: after reward=1, W moves toward a one-hot code for the chosen action by rate wm_bind.
      After reward=0, W softly relaxes toward uniform by rate wm_bind/2.
    - WM stickiness: the last chosen action in a state receives a bias in WM logits (wm_stick).

    Set-size effect:
    - Effective WM weight decreases with set size: wm_weight_eff = wm_weight0 / (1 + size_penalty * max(0, nS - 3)).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight0: Baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_bind: WM binding rate toward target distributions (0..1).
    - wm_stick: Additive stickiness bias in WM logits for the last chosen action in a state (>=0).
    - size_penalty: Strength of set-size penalty on WM weight (>=0).
    """
    lr, wm_weight0, softmax_beta, wm_bind, wm_stick, size_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base WM precision
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_choice = -1 * np.ones(nS, dtype=int)

        wm_weight_eff = wm_weight0 / (1.0 + size_penalty * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy with stickiness
            W_s = w[s, :].copy()
            if last_choice[s] >= 0:
                W_s[last_choice[s]] += wm_stick  # additive bias in logits space

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: smooth binding
            if r > 0.5:
                # Move toward one-hot on chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_bind) * w[s, :] + wm_bind * target
            else:
                # Relax toward uniform
                w[s, :] = (1.0 - wm_bind / 2.0) * w[s, :] + (wm_bind / 2.0) * w_0[s, :]

            last_choice[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM mixture with probabilistic WM encoding, set-size-driven WM interference, and a global lapse.
    
    Policy:
    - RL channel: softmax over Q with inverse temperature beta*10.
    - WM channel: softmax over W with high precision.
    - Mixture with lapse: p_total = (1 - lapse) * [wm_weight * p_wm + (1 - wm_weight) * p_rl] + lapse * (1/nA).

    WM dynamics:
    - Interference increases with set size: at each trial, W_s is mixed with uniform by c = interf_strength * max(0, (nS-3)/3).
    - Probabilistic encoding when r=1: move a mass p_enc from W_s into the chosen action (keeps normalization).
    - Error handling when r=0: softly suppress the chosen action by redistributing a fraction (p_enc/2) of its current mass to others.

    Set-size effect:
    - Only through the interference coefficient c that grows from 0 (set size 3) to interf_strength (set size 6).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight of WM (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - p_enc: Probability/mass of WM encoding when feedback is positive (0..1).
    - interf_strength: Maximum WM interference mixing with uniform at set size 6 (0..1).
    - lapse: Probability of action-independent random lapse (0..1).
    """
    lr, wm_weight, softmax_beta, p_enc, interf_strength, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size driven interference coefficient (0 at size=3, up to interf_strength at size=6)
        c = interf_strength * max(0.0, (nS - 3) / 3.0)
        c = min(max(c, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM interference (mix with uniform)
            w[s, :] = (1.0 - c) * w[s, :] + c * w_0[s, :]

            # WM encoding/update
            if r > 0.5:
                # Move mass p_enc into chosen action (preserves normalization and bounds)
                mass = p_enc
                mass = min(max(mass, 0.0), 1.0)
                w[s, :] = (1.0 - mass) * w[s, :]
                w[s, a] += mass
            else:
                # Softly suppress chosen action by redistributing fraction of its mass
                frac = p_enc / 2.0
                frac = min(max(frac, 0.0), 0.5)
                take = frac * w[s, a]
                w[s, a] -= take
                redistribute = take / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and WM error-suppression; WM precision decreases with set size.
    
    Policy:
    - RL channel: softmax over Q with inverse temperature beta*10.
    - WM channel: softmax over scaled W, where WM precision drops with set size (scaling factor k_prec <= 1).
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    WM dynamics:
    - Correct feedback (r=1): deterministically store the chosen action (one-hot).
    - Error feedback (r=0): suppress the chosen action by multiplicative factor (1 - wm_suppress),
      and redistribute the removed mass equally to the other actions to keep normalization.

    RL dynamics:
    - Asymmetric learning: lr_pos for rewards, lr_neg for no rewards.

    Set-size effect:
    - WM precision scaling: k_prec = 1 / (1 + size_prec_penalty * max(0, nS - 3)),
      which shrinks WM logits at larger set sizes, making WM policy flatter.

    Parameters (tuple):
    - lr_pos: RL learning rate for positive outcomes (0..1).
    - lr_neg: RL learning rate for negative outcomes (0..1).
    - wm_weight: Mixture weight on WM (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_suppress: Degree of WM suppression after errors (0..1).
    - size_prec_penalty: Strength of set-size penalty on WM precision (>=0).
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_suppress, size_prec_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM precision scaling with set size (<=1)
        k_prec = 1.0 / (1.0 + size_prec_penalty * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy with reduced precision at larger set sizes
            W_s = w[s, :]
            # Scale logits by k_prec (k_prec <= 1 reduces WM precision)
            logits = softmax_beta_wm * k_prec * (W_s - W_s[a])
            denom_wm = np.sum(np.exp(logits))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            if r > 0.5:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            # WM update
            if r > 0.5:
                # Deterministic storage to one-hot
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Suppress chosen action, redistribute removed mass
                suppress = wm_suppress
                suppress = min(max(suppress, 0.0), 1.0)
                removed = suppress * w[s, a]
                w[s, a] -= removed
                if nA > 1:
                    add = removed / (nA - 1)
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] += add

        blocks_log_p += log_p

    return -blocks_log_p