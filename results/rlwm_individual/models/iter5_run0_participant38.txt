Below are three standalone cognitive models that follow your template structure, each with a distinct mechanism for RLâ€“WM arbitration and set-size effects. Each function returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-gated arbitration with set-size scaling + WM delta learning.

    Idea:
    - RL: standard delta rule.
    - WM: per-state action distribution updated toward chosen action on reward and away on non-reward.
    - Arbitration: trial-wise WM weight is modulated by RL uncertainty (entropy of RL policy).
      When RL is uncertain (high entropy), arbitration favors WM more if wm_weight_slope allows,
      but the base WM weight is penalized by set size.
    - Set-size effects:
      - Base WM weight is reduced as set size increases (linear penalty).
      - RL uncertainty (entropy) gates the WM weight (parameter arb_unc_temp controls sensitivity).

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[2] = wm_beta (float > 0): WM inverse temperature used for WM policy (fixed, deterministic-ish).
    - model_parameters[3] = wm_weight_base (float in [0,1]): Base mixture weight for WM.
    - model_parameters[4] = wm_weight_size_slope (float): Linear penalty on WM weight per extra item beyond 3.
    - model_parameters[5] = arb_unc_temp (float > 0): Arbitration sensitivity to RL entropy (higher -> more WM when RL uncertain).

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_beta, wm_weight_base, wm_weight_size_slope, arb_unc_temp = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = wm_beta  # use provided wm_beta directly

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size adjusted base WM weight (bounded to [0,1])
        wm_weight_block = wm_weight_base - wm_weight_size_slope * max(0, nS - 3)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Compute full RL policy to get entropy (uncertainty)
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            rl_policy = rl_exp / np.sum(rl_exp)
            rl_entropy = -np.sum(rl_policy * np.log(np.maximum(rl_policy, 1e-12)))

            # WM policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-gated arbitration: increase WM weight when RL is uncertain
            wm_weight_t = 1.0 / (1.0 + np.exp(-( (wm_weight_block - 0.5) * 6.0 + arb_unc_temp * rl_entropy )))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: move distribution toward chosen action if rewarded, else away
            if r > 0.5:
                # Shift mass toward chosen action
                gain = 0.5  # implicit fixed gain encoded via wm_beta controlling determinism; keep moderate update
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain
            else:
                # Shift mass away from chosen action
                loss = 0.25
                dec = min(loss, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)
            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    WM decay scaled by set size + WM store gain; RL is standard delta rule.

    Idea:
    - RL: standard delta rule.
    - WM: per-state probability table that decays each trial toward uniform, with a decay rate that increases
      with set size; when rewarded, the chosen action is stored strongly (store gain).
    - Arbitration: fixed mixture weight favoring WM depending on parameters.
    - Set-size effects:
      - WM decay rate increases linearly with set size (more items -> faster decay/interference).

    Parameters
    - model_parameters[0] = lr (float in [0,1]): RL learning rate.
    - model_parameters[1] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[2] = wm_weight (float in [0,1]): Mixture weight for WM policy.
    - model_parameters[3] = wm_decay_base (float in [0,1]): Base decay toward uniform per trial.
    - model_parameters[4] = wm_decay_size_slope (float in [0,1]): Incremental decay per item beyond 3.
    - model_parameters[5] = wm_store_gain (float in [0,1]): Strength toward chosen action on reward.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, softmax_beta, wm_weight, wm_decay_base, wm_decay_size_slope, wm_store_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM decay for this block
        wm_decay = wm_decay_base + wm_decay_size_slope * max(0, nS - 3)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy is near-deterministic on the stored row
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Global WM decay toward uniform every trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM storage on reward: push probability mass toward chosen action on the current state
            if r > 0.5:
                w[s, :] = (1.0 - wm_store_gain) * w[s, :]
                w[s, a] += wm_store_gain
                # Normalize the row
                row_sum = np.sum(w[s, :])
                if row_sum > 1e-12:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()
            else:
                # No special update on non-reward beyond decay
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Meta-RL learning rate via volatility + capacity-limited WM precision.

    Idea:
    - RL: learning rate adapts to recent surprise (absolute prediction error).
      lr_t = clip(lr_base + lr_vol_sens * |delta_prev|, 0, 1). This captures volatility adaptation.
    - WM: softmax with effective precision reduced by set size relative to capacity K.
      beta_wm_eff = wm_beta_base * min(1, K_capacity / nS).
      WM row is updated toward chosen action on reward and slightly away on non-reward.
    - Arbitration: fixed WM weight.

    Set-size effects:
    - WM precision drops when nS > K_capacity.
    - RL adapts to volatility but not directly to set size; the interaction is indirect via errors.

    Parameters
    - model_parameters[0] = lr_base (float in [0,1]): Base RL learning rate.
    - model_parameters[1] = lr_vol_sens (float in [0,1]): Sensitivity of RL learning rate to recent surprise.
    - model_parameters[2] = softmax_beta (float > 0): RL inverse temperature (internally scaled by 10).
    - model_parameters[3] = wm_weight (float in [0,1]): Mixture weight for WM policy.
    - model_parameters[4] = wm_beta_base (float > 0): Base WM inverse temperature.
    - model_parameters[5] = K_capacity (float > 0): Effective WM capacity in number of items.

    Returns
    - Negative log-likelihood of the observed choices under the model.
    """
    lr_base, lr_vol_sens, softmax_beta, wm_weight, wm_beta_base, K_capacity = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # placeholder; we will compute an effective WM beta each block

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM precision reduced by set size relative to capacity
        cap_ratio = min(1.0, max(1e-6, float(K_capacity) / float(nS)))
        wm_beta_eff = wm_beta_base * cap_ratio
        # Small floor to avoid too-flat WM
        wm_beta_eff = max(1e-3, wm_beta_eff)

        # Track previous absolute prediction error for meta-learning
        prev_abs_pe = 0.0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with effective precision
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with volatility-adaptive learning rate
            delta = r - Q_s[a]
            lr_t = np.clip(lr_base + lr_vol_sens * prev_abs_pe, 0.0, 1.0)
            q[s][a] += lr_t * delta
            prev_abs_pe = abs(delta)

            # WM update: toward chosen if rewarded, mild repulsion if not
            if r > 0.5:
                wm_gain = 0.5
                w[s, :] = (1.0 - wm_gain) * w[s, :]
                w[s, a] += wm_gain
            else:
                wm_away = 0.2
                dec = min(wm_away, w[s, a])
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Normalize WM row to be a valid distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 1e-12:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p