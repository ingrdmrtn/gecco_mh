def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity-gated encoding and global choice stickiness.

    Mechanism
    - RL: standard Q-learning.
    - WM: stores recent correct action per state, but encoding strength is capacity-gated by set size.
          Larger set sizes reduce WM encoding strength via a logistic gate.
    - Policy: mixture of RL and WM policies by wm_weight.
    - Stickiness: global tendency to repeat the last taken action, implemented as an additive bias to both RL and WM logits.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1) in the final policy.
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_enc_base: baseline term in WM encoding gate (logit units).
        - wm_enc_slope: slope on set size in WM encoding gate; negative values reduce encoding for larger sets.
        - stickiness: additive bias to the most recent action's logit (applied to both RL and WM; 0..1).

    Set-size effects
    ----------------
    - WM encoding gate: p_enc = sigmoid(wm_enc_base + wm_enc_slope*(nS-3)).
      When nS=6 and slope<0, WM encoding is weakened relative to nS=3.
    """
    lr, wm_weight, softmax_beta, wm_enc_base, wm_enc_slope, stickiness = model_parameters
    softmax_beta *= 10  # as specified
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # capacity-gated encoding probability (deterministic weight for update)
        p_enc = sigmoid(wm_enc_base + wm_enc_slope * (nS - 3))

        last_action_global = None

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # apply global stickiness to logits (bias last taken action)
            stick_vec = np.zeros(nA)
            if last_action_global is not None:
                stick_vec[last_action_global] = stickiness

            # RL policy
            Qe = Q_s + stick_vec
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Qe - Qe[a])))

            # WM policy
            We = W_s + stick_vec
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (We - We[a])))

            # mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: encode rewarded association with capacity-gated strength
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - p_enc) * w[s] + p_enc * onehot
            else:
                # If no reward, weak drift toward uniform (prevents runaway sharpening)
                w[s] = 0.98 * w[s] + 0.02 * w_0[s]

            # update last action
            last_action_global = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with rehearsal on revisit and load-dependent WM lapse.

    Mechanism
    - RL: standard Q-learning.
    - WM: maintains per-state associative weights. On reward, updates toward the chosen action.
          On revisit without reward, 'rehearses' the last stored action for that state (strength = rehearse).
    - WM lapse: with probability increasing with set size, WM policy lapses to uniform.
    - Policy: mixture of WM and RL.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - rehearse: WM rehearsal strength toward last stored action on revisit when not rewarded (0..1).
        - lapse0: baseline WM lapse log-odds (applied at nS=3).
        - lapse_slope: change in WM lapse log-odds per +3 items (positive => more lapses at nS=6).

    Set-size effects
    ----------------
    - WM lapse probability: lapse = sigmoid(lapse0 + lapse_slope*(nS-3)).
      Higher set size increases lapses, reducing WM reliability.
    """
    lr, wm_weight, softmax_beta, rehearse, lapse0, lapse_slope = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # track last stored (most likely) action per state for rehearsal
        last_stored = np.argmax(w, axis=1)

        lapse = sigmoid(lapse0 + lapse_slope * (nS - 3))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM base softmax prob
            p_wm_soft = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM lapse mixture with uniform
            p_wm = (1 - lapse) * p_wm_soft + lapse * (1.0 / nA)

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = 0.0 * w[s] + onehot  # overwrite on success for strong memory
                last_stored[s] = a
            else:
                # On unrewarded revisit, rehearse last stored action for this state
                reh_onehot = np.zeros(nA)
                reh_onehot[last_stored[s]] = 1.0
                w[s] = (1 - rehearse) * w[s] + rehearse * reh_onehot
                # small drift to uniform to avoid pathological sharpening
                w[s] = 0.995 * w[s] + 0.005 * w_0[s]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-diminished temperature and WM with probabilistic storage + decay.
    Arbitration weight depends on relative WM strength vs RL uncertainty.

    Mechanism
    - RL: standard Q-learning; effective RL temperature worsens with higher set size.
    - WM: on reward, updates toward one-hot of chosen action with storage strength proportional to
          wm_store_prob scaled by set size. WM traces decay toward uniform each trial by wm_decay.
    - Arbitration: dynamic wm_weight_dyn computed from WM strength (peak minus mean of W_s)
                   versus RL uncertainty (entropy of RL softmax). The base wm_weight parameter
                   sets the arbitration gain.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: arbitration gain scaling (0..1); higher values give more WM influence when strong.
        - softmax_beta: base RL inverse temperature; internally scaled by 10, then attenuated by load.
        - beta_rl_load: attenuation factor for RL beta with load; beta_eff = beta / (1 + beta_rl_load*(nS-3)).
        - wm_store_prob: base WM storage strength per rewarded trial at nS=3 (0..1); scaled by 3/nS.
        - wm_decay: per-trial WM decay toward uniform (0..1).

    Set-size effects
    ----------------
    - RL: higher nS reduces RL inverse temperature, making RL noisier.
    - WM: storage strength scales with 3/nS, weakening WM encoding at larger nS.
    """
    lr, wm_weight, softmax_beta, beta_rl_load, wm_store_prob, wm_decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0

    def softmax_probs(beta, vals):
        # stable softmax
        z = vals - np.max(vals)
        ez = np.exp(beta * z)
        return ez / np.sum(ez)

    def entropy(p):
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # load-modulated RL beta
        beta_rl_eff = softmax_beta / (1.0 + beta_rl_load * (nS - 3))

        # WM storage scaling with load
        store_strength = wm_store_prob * (3.0 / float(nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action using effective beta
            p_rl = 1 / np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))

            # WM policy probability
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic arbitration based on WM strength vs RL uncertainty
            rl_prob_vec = softmax_probs(beta_rl_eff, Q_s)
            rl_uncert = entropy(rl_prob_vec) / np.log(nA)  # normalized [0,1]
            wm_strength = max(0.0, np.max(W_s) - np.mean(W_s))  # [0,1] approx

            # map difference via sigmoid, scaled by wm_weight (gain)
            arb_signal = wm_weight * (wm_strength - rl_uncert)
            wm_weight_dyn = sigmoid(5.0 * arb_signal)  # steeper mapping for sensitivity

            p_total = wm_weight_dyn * p_wm + (1 - wm_weight_dyn) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            if wm_decay > 0:
                w = (1 - wm_decay) * w + wm_decay * w_0

            # WM storage on reward: fractional deterministic update with load scaling
            if r > 0 and store_strength > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - store_strength) * w[s] + store_strength * onehot

        blocks_log_p += log_p

    return -blocks_log_p