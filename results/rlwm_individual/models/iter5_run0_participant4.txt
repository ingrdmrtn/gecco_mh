def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-gated arbitration and load-sensitive refreshing.

    Mechanism
    - RL: standard Rescorla–Wagner update (template).
    - WM store: row-wise categorical memory over actions for each state (w).
      Positive feedback encodes a more one-hot mapping; between trials, the
      active state's WM decays toward uniform. The decay is stronger when
      set size is larger (simulating rehearsal limits).
    - Arbitration: the weight assigned to WM is wm_weight scaled by a
      confidence signal derived from the entropy of the WM row for the
      current state. Lower entropy -> higher WM confidence.
    - Policy: mixture of WM and RL.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr            : RL learning rate in (0,1].
        wm_weight     : Base mixture weight for WM in [0,1].
        softmax_beta  : RL inverse temperature; scaled by *10 internally.
        wm_refresh    : Base WM refresh-to-uniform rate per visit (>=0).
                        Larger => faster decay to uniform.
        entropy_temp  : Sensitivity of arbitration to WM confidence (>=0).
                        Higher => more deterministic gating by entropy.
        wm_precision  : Strength of WM encoding toward one-hot on reward
                        in [0,1], also scales WM policy precision.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_refresh, entropy_temp, wm_precision = model_parameters
    softmax_beta *= 10  # RL precision
    softmax_beta_wm = 50  # base WM precision
    eps = 1e-12

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-sensitive decay factor per visit (larger sets => more decay)
        # Map wm_refresh to an effective per-visit decay using an exponential form
        decay_visit = 1.0 - np.exp(-max(0.0, wm_refresh) * max(1, nS) / nA)
        decay_visit = np.clip(decay_visit, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax written as probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with effective precision scaled by wm_precision
            beta_wm_eff = softmax_beta_wm * max(0.0, wm_precision)
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Entropy-based arbitration: confidence = 1 - H(W_s)/H_max
            row = W_s.copy()
            row = np.clip(row, eps, 1.0)
            row /= np.sum(row)
            H = -np.sum(row * np.log(row))
            Hmax = np.log(nA)
            confidence = 1.0 - (H / Hmax if Hmax > 0 else 0.0)

            # Gate WM by sigmoid of confidence (centered at 0.5)
            gate = 1.0 / (1.0 + np.exp(-max(0.0, entropy_temp) * (confidence - 0.5)))
            wm_weight_eff = np.clip(wm_weight, 0.0, 1.0) * gate

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay of the visited state toward uniform (load-dependent)
            w[s, :] = (1.0 - decay_visit) * w[s, :] + decay_visit * w_0[s, :]

            # WM encoding on reward toward the chosen action (precision-controlled)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                encode_rate = np.clip(wm_precision, 0.0, 1.0)
                w[s, :] = (1.0 - encode_rate) * w[s, :] + encode_rate * one_hot
            # Normalize defensively
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size logistic arbitration and load-modulated forgetting.

    Mechanism
    - RL: standard Rescorla–Wagner update (template).
    - WM: rewarded actions are encoded toward one-hot; between visits the row
      decays to uniform with a rate that increases with set size (simulating
      interference).
    - Arbitration: WM weight follows a logistic function of set size with
      midpoint n50 and slope; scaled by wm_weight.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr            : RL learning rate in (0,1].
        wm_weight     : Base WM mixture weight in [0,1].
        softmax_beta  : RL inverse temperature; scaled by *10 internally.
        n50           : Set size at which WM and RL are equally weighted (>0).
        slope         : Positive slope of the logistic set-size effect (>=0).
        wm_decay      : Base WM decay parameter (>=0). Larger => more forgetting.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, n50, slope, wm_decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12

    slope = max(0.0, slope)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Logistic arbitration weight based on set size
        x = (nS - max(1e-6, n50)) * slope
        wm_weight_size = 1.0 / (1.0 + np.exp(x))
        wm_weight_size = np.clip(wm_weight_size * np.clip(wm_weight, 0.0, 1.0), 0.0, 1.0)

        # Load-modulated decay per visit
        decay_visit = 1.0 - np.exp(-max(0.0, wm_decay) * nS / nA)
        decay_visit = np.clip(decay_visit, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight_size * p_wm + (1.0 - wm_weight_size) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay and encoding
            w[s, :] = (1.0 - decay_visit) * w[s, :] + decay_visit * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Encode more strongly when memory is already focused (self-reinforcement)
                focus = np.max(w[s, :])
                encode_rate = np.clip(0.5 + 0.5 * focus, 0.0, 1.0)
                w[s, :] = (1.0 - encode_rate) * w[s, :] + encode_rate * one_hot

            # Normalize row
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM slots with probabilistic encoding and noise floor.

    Mechanism
    - RL: standard Rescorla–Wagner update (template).
    - WM: a capacity parameter C limits the probability that a state's mapping
      is available in WM within a block. When nS > C, the probability that the
      current state is "in WM" is approximately C/nS; when nS <= C it is ~1.
      Positive feedback encodes toward one-hot with strength encode_prob.
      Between visits, rows relax slightly toward uniform.
    - Arbitration: mixture weight equals wm_weight scaled by p_inmem = min(1, C/nS).
    - Additionally, choices include a small noise_floor that mixes in uniform choice.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr            : RL learning rate in (0,1].
        wm_weight     : Base WM mixture weight in [0,1].
        softmax_beta  : RL inverse temperature; scaled by *10 internally.
        C_capacity    : WM capacity (positive real; effective number of storable states).
        encode_prob   : Strength of WM encoding toward one-hot on reward in [0,1].
        noise_floor   : Probability of uniform-choice noise in [0,0.5).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, C_capacity, encode_prob, noise_floor = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    eps = 1e-12
    noise_floor = np.clip(noise_floor, 0.0, 0.499)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Probability that a given state is available in WM under capacity C
        p_inmem = np.clip(C_capacity / max(1.0, nS), 0.0, 1.0)
        wm_weight_eff = np.clip(wm_weight, 0.0, 1.0) * p_inmem

        # Mild baseline relaxation toward uniform per visit (not strongly load-dependent here)
        decay_visit = 1.0 - np.exp(-0.1 * max(1.0, nS) / nA)
        decay_visit = np.clip(decay_visit, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - noise_floor) * p_mix + noise_floor * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM relaxation toward uniform and encoding on reward
            w[s, :] = (1.0 - decay_visit) * w[s, :] + decay_visit * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                enc = np.clip(encode_prob, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * one_hot

            # Normalize row
            rs = np.sum(w[s, :])
            if rs > 0:
                w[s, :] /= rs
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p