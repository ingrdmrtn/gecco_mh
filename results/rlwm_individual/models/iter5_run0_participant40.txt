def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-weighted RL with age- and load-modulated exploration.

    The agent learns Q-values via incremental RL, while dynamically adapting choice
    temperature based on its certainty about the state's action values. Certainty is
    estimated from simple Bayesian success counts per state-action. Higher certainty
    (more evidence) sharpens the softmax; lower certainty encourages exploration.
    Older adults are modeled as having stronger symmetric priors (slower certainty
    accumulation) and lower effective softmax gain, especially under higher set sizes.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to modulate certainty accumulation and temperature.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha_q, beta_scale, prior_bias, age_mod, epsilon]
        - alpha_q: RL learning rate (0..1).
        - beta_scale: base softmax gain scaling (>=0).
        - prior_bias: symmetric prior pseudo-count per action at block start (>0).
        - age_mod: scales age and load effects on both prior strength and temperature.
        - epsilon: lapse probability (action replaced with uniform random with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha_q, beta_scale, prior_bias, age_mod, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize RL values and Bayesian counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Age-modulated symmetric prior: older -> stronger prior (slower certainty growth)
        prior = max(1e-6, prior_bias) * (1.0 + age_mod * is_older)
        counts = prior * np.ones((nS, nA))  # success counts per action

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL policy (will use dynamic beta)
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            # Bayesian certainty from counts: more total evidence -> more certain
            total_evidence = np.sum(counts[s, :])
            # Uncertainty proxy in [0,1): lower when evidence high. Normalize by nA to be scale-free.
            uncert = 1.0 / (1.0 + (total_evidence / (nA * max(1.0, ss))))  # larger sets stretch needed evidence

            # Age and load reduce effective temperature via age_mod; higher uncertainty reduces beta
            load_penalty = 1.0 + age_mod * (max(0, ss - 3))
            age_penalty = 1.0 + 0.5 * age_mod * is_older
            beta_eff = beta_scale * (1.0 - uncert) / (load_penalty * age_penalty)
            beta_eff = max(1e-6, beta_eff)

            p_rl = np.exp(beta_eff * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # Lapse mixture
            p = (1.0 - epsilon) * p_rl + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                # Invalid/missed response; assume uniform likelihood and skip updates
                total_logp += np.log(1.0 / nA)
                continue

            # Learning updates only if reward is valid
            if r >= 0.0:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha_q * delta

                # Bayesian success count update for observed action only
                # Older adults accrue evidence more cautiously under load
                accrual_scale = 1.0 / (1.0 + age_mod * is_older * max(1, ss - 2))
                counts[s, a] += accrual_scale * r

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Pearce–Hall associability RL with age- and load-dependent volatility tracking.

    The agent adapts a per-state associability that scales the effective learning rate
    and softmax gain. Associability increases with surprise (unsigned prediction error)
    and decays over time; older adults and larger set sizes induce stronger decay,
    limiting sustained high associability (slower adaptation under load and aging).

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to modulate associability decay and policy temperature.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha0, beta0, kappa_ph, decay_ctrl, epsilon]
        - alpha0: baseline learning rate scalar (0..1), scaled by associability A[s].
        - beta0: baseline softmax inverse temperature, scaled by A[s].
        - kappa_ph: gain for increasing associability with unsigned prediction error.
        - decay_ctrl: controls decay; larger -> faster decay with age and load.
        - epsilon: lapse probability (uniform random action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha0, beta0, kappa_ph, decay_ctrl, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Per-state associability (starts moderate)
        A = 0.5 * np.ones(nS)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Effective softmax gain increases with associability
            beta_eff = beta0 * (0.5 + 0.5 * A[s])
            beta_eff = max(1e-6, beta_eff)

            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_soft = np.exp(beta_eff * q_center)
            p_soft = p_soft / np.sum(p_soft)

            # Lapse mixture
            p = (1.0 - epsilon) * p_soft + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                continue

            if r >= 0.0:
                # State-dependent learning rate via associability
                alpha_eff = np.clip(alpha0 * A[s], 0.0, 1.0)
                delta = r - Q[s, a]
                Q[s, a] += alpha_eff * delta

                # Pearce–Hall associability update
                # Decay factor increases with age and set size (older + larger sets => faster decay)
                decay = 1.0 / (1.0 + np.exp(- ( -1.0 + decay_ctrl * (is_older + max(0, ss - 3)) )))
                # decay in (0,1); baseline around logistic(-1)=~0.27 when modifiers are 0
                A[s] = (1.0 - decay) * A[s] + kappa_ph * abs(delta)
                # Bound associability to [0,1]
                A[s] = float(np.clip(A[s], 0.0, 1.0))

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Win–stay/lose–explore prior combined with RL, with age-modulated action inertia.

    The policy blends two components:
      (1) Incremental RL softmax over Q-values.
      (2) A win–stay / lose–explore (WSLE) prior over actions at the current state,
          implemented as a soft bias favoring repeating the last action after rewards
          and diffusing away from it after non-rewards. Older adults exhibit stronger
          action inertia, amplifying the WSLE bias, especially under lower load; the
          bias is attenuated for larger set sizes.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). If action < 0 it is treated as a lapse (uniform likelihood).
    rewards : np.ndarray of float
        Feedback at each trial. Values in {0,1}; values < 0 indicate invalid/missing feedback (trial ignored for updating).
    blocks : np.ndarray of int
        Block id per trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6), used to scale WSLE bias.
    age : np.ndarray of float
        Array containing participant age (use age[0]); age>=45 considered older group.
    model_parameters : sequence of float
        [alpha, beta, ws_gain, inertia_age, epsilon]
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for RL softmax.
        - ws_gain: base gain of the WSLE prior (strength of stay vs explore signal).
        - inertia_age: scales how age modulates the WSLE prior; larger -> stronger bias in older adults.
        - epsilon: lapse probability (uniform random action with prob epsilon).

    Returns
    -------
    float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, ws_gain, inertia_age, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and outcome per state for WSLE
        last_a = -np.ones(nS, dtype=int)
        last_r = -np.ones(nS, dtype=float)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL softmax
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # WSLE prior over actions at state s
            ws_vec = np.zeros(nA)
            if last_a[s] >= 0 and last_a[s] < nA and last_r[s] >= 0.0:
                la = last_a[s]
                if last_r[s] > 0.0:
                    # Win-stay: boost last action
                    ws_vec[la] = 1.0
                else:
                    # Lose-explore: de-emphasize last action (boost others slightly)
                    ws_vec[:] = 1.0 / (nA - 1)
                    ws_vec[la] = 0.0
            else:
                # No prior info yet: uniform
                ws_vec[:] = 1.0 / nA

            # Convert WSLE vector into a soft bias distribution via temperature
            # Age increases inertia; set size reduces it
            inertia = 1.0 + inertia_age * is_older
            load_attn = 1.0 / (1.0 + 0.5 * max(0, ss - 3))
            ws_temp = ws_gain * inertia * load_attn
            # Translate ws_vec into logits: higher value gets boosted
            # To avoid zero entries, add a small floor and renormalize
            ws_safe = np.clip(ws_vec, 1e-6, None)
            ws_logits = ws_temp * (ws_safe - np.mean(ws_safe))
            p_ws = np.exp(ws_logits - np.max(ws_logits))
            p_ws = p_ws / np.sum(p_ws)

            # Combine RL and WSLE multiplicatively (product-of-experts), then renormalize
            p_comb = p_rl * p_ws
            p_comb = p_comb / np.sum(p_comb)

            # Lapse mixture
            p = (1.0 - epsilon) * p_comb + epsilon * (1.0 / nA)

            if a >= 0 and a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                # Do not update memory on invalid trials
                continue

            # Updates if reward is valid
            if r >= 0.0:
                # RL update
                delta = r - Q[s, a]
                Q[s, a] += alpha * delta

                # Update WSLE memory
                last_a[s] = a
                last_r[s] = r

    return -float(total_logp)