Below are three distinct cognitive models that follow your template, each with a different theory about how WM and RL interact across set sizes. All three functions:

- Take: states, actions, rewards, blocks, set_sizes, model_parameters
- Return: negative log-likelihood of the observed actions
- Keep the template lines unchanged and fill in the WM policy and WM learning
- Use all parameters meaningfully (≤ 6 per model)
- Let WM matter less under higher load (via block-level rescaling and/or interference)


def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + NoGo working memory (WM) that prioritizes encoding of incorrect actions for avoidance.

    Rationale
    - RL updates standard Q-values from reward prediction errors.
    - WM works as an avoidance map: it learns to suppress actions that led to no reward (NoGo),
      and modestly strengthens actions that led to reward. WM decays toward uniform each trial.
    - Arbitration: mixture between WM and RL; WM weight is downscaled by 3/nS.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1). Internally scaled by 3/nS per block.
        softmax_beta : float
            RL inverse temperature (scaled x10 internally).
        nogo_lr : float
            WM learning rate for encoding incorrect actions (0-1).
        wm_leak : float
            WM decay toward uniform on each visit to a state (0-1).
        nogo_scale : float
            Gain scaling of WM logits (>=0); higher means stronger avoidance/proclivity signals.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, nogo_lr, wm_leak, nogo_scale = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]
        nS = int(block_set_sizes[0])

        # Downscale WM by set size (more items -> less effective WM)
        wm_weight_eff = np.clip(wm_weight * (3.0 / float(nS)), 0.0, 1.0)

        q = (1 / nA) * np.ones((nS, nA))
        # WM initialized as uniform preference; lower values will reflect NoGo for those actions
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice prob for chosen action (as given in the template)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM-preference, with additional NoGo scaling.
            # We center WM values and scale by nogo_scale to boost avoidance signals.
            wm_logits = nogo_scale * (W_s - np.mean(W_s))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Mixture
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform on each visit to state s
            w[s, :] = (1 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            # WM updates: emphasize incorrect-action avoidance, modestly reinforce correct actions
            target = np.copy(W_s)
            if r <= 0:
                # Push chosen action down (avoid), redistribute a bit to others via normalization
                target[a] = (1 - nogo_lr) * target[a]
                # Renormalize to keep a proper distribution-like vector
                sum_rest = np.sum(target) - target[a]
                if sum_rest > 0:
                    target /= np.sum(target)
                else:
                    target = w_0[s, :].copy()
            else:
                # Reward: move toward one-hot on chosen action, but milder than NoGo updates
                alpha_pos = 0.5 * nogo_lr
                onehot = np.zeros(nA); onehot[a] = 1.0
                target = (1 - alpha_pos) * target + alpha_pos * onehot

            w[s, :] = target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with set-size–dependent RL decay + WM recency-gated encoding.

    Rationale
    - RL: standard delta-rule learning, but Q-values additionally decay toward uniform with a
      decay factor that increases with set size (larger nS -> more forgetting).
    - WM: encodes rewarded actions with a recency gate: the longer since last visit to a state,
      the weaker the WM update (decaying with lag via a time constant). WM also decays.
    - Arbitration: WM weight is scaled down by 3/nS.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), scaled by 3/nS per block.
        softmax_beta : float
            RL inverse temperature (scaled x10 internally).
        rl_decay_base : float
            Baseline RL decay toward uniform for visited states (0-1).
        rl_decay_size : float
            Additional RL decay per unit load beyond 3 (>=0).
        wm_recency_tau : float
            WM recency time constant (>0). Larger -> slower decay of recency effect.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, rl_decay_base, rl_decay_size, wm_recency_tau = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]
        nS = int(block_set_sizes[0])

        wm_weight_eff = np.clip(wm_weight * (3.0 / float(nS)), 0.0, 1.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last-visit time for each state to compute recency
        last_seen = -np.ones(nS, dtype=int)

        # Precompute RL decay factor for this block
        extra_load = max(0, nS - 3)
        rl_decay = np.clip(rl_decay_base + rl_decay_size * (extra_load / 3.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # RL decay toward uniform for the visited state (stronger decay at larger set sizes)
            q[s, :] = (1 - rl_decay) * q[s, :] + rl_decay * (1.0 / nA)

            # WM decay slightly toward uniform per visit (keeps WM from saturating)
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Recency-gated WM encoding: encode rewarded action more if state was seen recently
            lag = (t - last_seen[s]) if last_seen[s] >= 0 else 0
            recency_gate = 1.0 if last_seen[s] < 0 else np.exp(-float(lag) / max(1e-6, wm_recency_tau))

            if r > 0:
                onehot = np.zeros(nA); onehot[a] = 1.0
                alpha_wm = np.clip(recency_gate, 0.0, 1.0)
                w[s, :] = (1 - alpha_wm) * w[s, :] + alpha_wm * onehot

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with exploration bonus from visit counts + WM encoding gated by RL confidence, with
    load-dependent WM interference.

    Rationale
    - RL: standard delta-rule, but action values are augmented by an exploration bonus inversely
      proportional to sqrt of visit counts (count-based exploration).
    - WM: encodes rewarded actions only when RL is confident (value-gap threshold). WM suffers
      interference that grows with set size (stronger decay toward uniform at larger nS).
    - Arbitration: fixed mixture within a block but WM weight is scaled down by 3/nS.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), scaled by 3/nS per block.
        softmax_beta : float
            RL inverse temperature (scaled x10 internally).
        explore_bonus : float
            Weight for count-based exploration bonus added to Q before softmax (>=0).
        wm_interf_rate : float
            WM interference/decay rate toward uniform, multiplied by nS/6 (0-1).
        wm_encode_gap : float
            Minimum RL value gap (max - second max) required to commit a rewarded action to WM.

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, explore_bonus, wm_interf_rate, wm_encode_gap = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]
        nS = int(block_set_sizes[0])

        wm_weight_eff = np.clip(wm_weight * (3.0 / float(nS)), 0.0, 1.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Visit counts per state-action for exploration bonus
        counts = 0.5 * np.ones((nS, nA))  # start non-zero to avoid huge initial bonus

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # Add count-based exploration bonus to RL values before computing choice prob
            bonus = explore_bonus / np.sqrt(np.maximum(1e-6, counts[s, :]))
            Q_aug = Q_s + bonus

            # Compute RL choice probability for chosen action using augmented Q
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update on raw Q (no bonus in learning)
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # Update counts (after choice)
            counts[s, a] += 1.0

            # WM interference: decay toward uniform with strength that increases with set size
            interf = np.clip(wm_interf_rate * (float(nS) / 6.0), 0.0, 1.0)
            w[s, :] = (1 - interf) * w[s, :] + interf * w_0[s, :]

            # WM encoding gated by RL confidence (value gap)
            sorted_vals = np.sort(Q_s)[::-1]
            gap = sorted_vals[0] - sorted_vals[1] if nA > 1 else sorted_vals[0]
            if (r > 0) and (gap >= wm_encode_gap):
                onehot = np.zeros(nA); onehot[a] = 1.0
                w[s, :] = 0.2 * w[s, :] + 0.8 * onehot  # strong commit on confident reward

        blocks_log_p += log_p

    return -blocks_log_p