def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Q-decay + WM precision limited by set-size (capacity threshold).

    Mechanism:
    - RL: tabular Q-learning with per-trial decay toward uniform (q_decay), preventing overcommitment.
    - WM: fast one-shot encoding toward the rewarded action using the RL learning rate (lr) as WM gain.
      WM readout precision (softmax inverse temperature) is reduced when set size exceeds a capacity
      threshold (cap_threshold). WM precision and weight both shrink as nS rises above capacity.
    - Arbitration: fixed mixture by an effective WM weight scaled by set-size overload.

    Parameters (in order):
    - lr: RL learning rate in [0,1] and also used as WM encoding gain.
    - wm_weight: Baseline WM arbitration weight in [0,1].
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - q_decay: RL decay to uniform per trial in [0,1].
    - wm_precision: Baseline WM inverse temperature scale (>0); higher = more precise WM.
    - cap_threshold: Capacity threshold (e.g., 3-4); WM precision and weight drop when nS > cap_threshold.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, q_decay, wm_precision, cap_threshold = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # baseline, further scaled by wm_precision and set-size

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    lr = np.clip(lr, 0.0, 1.0)
    q_decay = np.clip(q_decay, 0.0, 1.0)
    wm_precision = max(1e-6, float(wm_precision))
    cap_threshold = float(cap_threshold)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size overload scaling: when nS exceeds capacity, reduce WM influence and precision
        overload = max(0.0, nS - cap_threshold)
        scale = 1.0 / (1.0 + overload)  # monotonic decrease with overload
        wm_weight_eff = np.clip(wm_weight * scale, 0.0, 1.0)
        beta_wm_eff = softmax_beta_wm * wm_precision * scale

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy (softmax on q)
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax on w) with capacity-limited precision
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # global decay
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA)

            # WM update: fast encoding on reward; gentle forgetting otherwise
            if r > 0:
                # move distribution toward chosen action using lr as gain
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                # partial reversion to uniform when not rewarded
                forget = 0.5 * lr
                w[s, :] = (1.0 - forget) * w[s, :] + forget * w_0[s, :]

            # normalize and clip to avoid numerical issues
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-driven exploration (UCB) + WM gated by recent reward rate.

    Mechanism:
    - RL: tabular Q-learning; action selection uses Q plus an exploration bonus proportional
      to 1/sqrt(visit count) (UCB-like). Learning rule is standard Q-learning.
    - WM: rewarded mappings cached strongly; otherwise decays toward uniform. WM arbitration
      weight is dynamically gated by the block's recent reward rate (EMA) and reduced with set size.
    - Arbitration: convex mixture with a sigmoid gate on WM weight.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline WM weight (center of the gate) in [0,1].
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - bonus: Uncertainty bonus amplitude (>=0) added to Q for choice via 1/sqrt(N).
    - gate_slope: Slope of the reward-rate gate mapping to WM weight.
    - gate_bias: Bias/offset of the reward-rate gate.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, bonus, gate_slope, gate_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    lr = np.clip(lr, 0.0, 1.0)
    bonus = max(0.0, float(bonus))
    gate_slope = float(gate_slope)
    gate_bias = float(gate_bias)

    # EMA step for recent reward rate (fixed small step)
    alpha_rr = 0.2

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA))  # for UCB bonus
        Rhat = 0.5  # recent reward rate

        # set-size downscaling of WM influence
        size_scale = 1.0 / (1.0 + max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with uncertainty bonus
            bonus_vec = bonus / np.sqrt(np.maximum(1.0, visits[s, :]))
            Q_s = q[s, :] + bonus_vec
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic-ish readout)
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Reward-rate-gated WM weight
            gate = sigmoid(gate_bias + gate_slope * (Rhat - 0.5))
            wm_weight_eff = np.clip(wm_weight * gate * size_scale, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update (no bonus in learning)
            Qa = q[s, a]
            delta = r - Qa
            q[s, a] = Qa + lr * delta

            # Update visit counts for UCB
            visits[s, a] += 1.0

            # WM update: cache on reward; decay otherwise
            if r > 0:
                # strong commit to chosen action
                w[s, :] = 1e-6  # avoid zeros
                w[s, a] = 1.0
            else:
                # soft decay to uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

            # Update recent reward rate
            Rhat = (1.0 - alpha_rr) * Rhat + alpha_rr * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall associability + WM with lateral inhibition and set-size lapse.

    Mechanism:
    - RL: Q-learning with dynamic associability (Pearce-Hall). Each (s,a) has an associability that
      tracks unsigned prediction error and scales the effective learning rate.
    - WM: reward-triggered boost for the chosen action with lateral inhibition to competitors
      (controlled by 'inhib'). WM influence reduced with set size; also a lapse parameter mixes
      in uniform responding on each trial.
    - Arbitration: convex mixture between WM and RL; final policy mixed with lapse.

    Parameters (in order):
    - base_lr: Base learning rate multiplied by associability (in [0,1]).
    - wm_weight: Baseline WM arbitration weight (in [0,1]).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - kappa: Associability update rate in [0,1]; larger = faster tracking of surprise.
    - inhib: Lateral inhibition strength applied to non-chosen WM actions when rewarded (>=0).
    - lapse: Lapse rate in [0,1] mixed with uniform choice after arbitration.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    base_lr, wm_weight, softmax_beta, kappa, inhib, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    base_lr = np.clip(base_lr, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    kappa = np.clip(kappa, 0.0, 1.0)
    inhib = max(0.0, float(inhib))
    lapse = np.clip(lapse, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        assoc = 0.5 * np.ones((nS, nA))  # initial associability

        # downscale WM with set size
        size_scale = 1.0 / (1.0 + max(0, nS - 3))
        wm_weight_eff = np.clip(wm_weight * size_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture + lapse
            p_mix = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall associability
            delta = r - Q_s[a]
            assoc[s, a] = (1.0 - kappa) * assoc[s, a] + kappa * abs(delta)
            lr_eff = base_lr * assoc[s, a]
            q[s, a] += lr_eff * delta

            # WM update with lateral inhibition on reward, mild decay otherwise
            if r > 0:
                gain = base_lr  # reuse base_lr as WM gain
                # enhance chosen
                w[s, a] += gain
                # inhibit others proportionally
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = max(1e-9, w[s, aa] - inhib * gain / (nA - 1))
            else:
                # decay toward uniform
                decay = 0.3 * base_lr
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # normalize row
            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p