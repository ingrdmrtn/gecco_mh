def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with confidence-weighted arbitration and age-related lapses.

    Idea:
    - RL updates Q-values with a single learning rate and softmax policy.
    - WM stores a probabilistic action code per state that sharpens on reward and decays on no-reward.
    - Arbitration weight for WM increases when:
        - set size is small (low load),
        - the RL policy is uncertain (low max softmax probability).
    - Older adults have added lapse noise: with some probability their choice is effectively random.
      This lapse is mixed into the final choice probability; it scales with age (older vs younger).
    
    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_base: baseline WM mixing weight (0..1)
    - softmax_beta: RL inverse temperature; internally scaled by 10
    - conf_gain: strength of confidence-based arbitration (>0); higher means rely more on WM when RL is uncertain
    - lapse_older: lapse probability added for older adults (0..1); zero effect for younger

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single participant age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, conf_gain, lapse_older = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of the observed action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of the observed action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL softmax max probability as a confidence proxy
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs /= np.sum(rl_probs)
            rl_conf = np.max(rl_probs)  # in [1/nA, 1]

            # Arbitration: rely more on WM when RL is uncertain (1 - rl_conf)
            load_scale = 3.0 / float(nS)  # 1.0 for set size 3; 0.5 for set size 6
            wm_mix = wm_base * load_scale * (1.0 + conf_gain * (1.0 - rl_conf))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            # Age-related lapse: mix with uniform choice
            lapse = lapse_older * older
            p_uniform = 1.0 / nA

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * p_uniform
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update:
            # - On reward, sharpen towards one-hot on chosen action; strength scales with RL uncertainty.
            # - On no reward, decay toward uniform prior.
            if r > 0.5:
                sharpen = np.clip(0.3 + 0.7 * (1.0 - rl_conf), 0.0, 1.0)
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                w[s, :] = (1.0 - sharpen) * W_s + sharpen * peaked
            else:
                decay = 0.4 + 0.2 * (nS - 3) / 3.0  # slightly stronger decay under load
                w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with binding errors that increase with load and age.

    Idea:
    - RL learns Q-values with softmax policy.
    - WM stores action probabilities per state, but suffers binding errors:
      on some trials the retrieved WM item is mis-bound to the wrong state,
      approximated as mixing the WM policy with a uniform distribution.
    - Binding error probability increases with set size and with being older.
    - Arbitration is a fixed WM weight mixed with RL.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight: baseline WM mixing weight (0..1)
    - softmax_beta: RL inverse temperature; internally scaled by 10
    - bind_base: base binding error logit (can be negative/positive)
    - age_bind_gain: additive increase to binding-error logit for older adults (>=0)
    - load_bind_gain: increase to binding-error logit per unit of load (>=0), where load = (nS-3)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single participant age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, bind_base, age_bind_gain, load_bind_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute binding error probability for this block (function of load and age)
        load = float(nS - 3)
        logit_pbind = bind_base + age_bind_gain * older + load_bind_gain * load
        # Logistic transform to [0,1]
        p_bind = 1.0 / (1.0 + np.exp(-logit_pbind))
        p_bind = np.clip(p_bind, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Ideal WM policy
            p_wm_ideal = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Binding error: with probability p_bind, retrieval is from the wrong state ~ uniform
            p_wm = (1.0 - p_bind) * p_wm_ideal + p_bind * (1.0 / nA)

            # Mix RL and WM
            wm_mix = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: reward => sharpen toward chosen; no-reward => decay to uniform
            if r > 0.5:
                alpha_w = 0.6  # fixed sharpening strength; binding errors are handled in policy, not learning
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                w[s, :] = (1.0 - alpha_w) * W_s + alpha_w * peaked
            else:
                decay = 0.5 + 0.3 * (nS - 3) / 3.0
                w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and age-adjusted temperature.

    Idea:
    - RL learns Q-values; WM stores action probabilities per state.
    - Arbitration relies more on WM when RL is uncertain (high entropy). Entropy sensitivity is a parameter.
    - WM traces decay over time toward uniform; decay is parameterized.
    - Older adults are assumed to have noisier RL policy (effective inverse temperature reduced).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: base RL inverse temperature; internally scaled by 10
    - wm_prior: baseline WM mixing weight (0..1)
    - entropy_beta: gain on RL entropy in arbitration (>=0); more weight to WM when entropy high
    - decay_wm: WM decay rate toward uniform on every trial of that state (0..1)
    - age_temp_drop: proportional drop in RL inverse temperature for older adults (0..1)

    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single participant age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_prior, entropy_beta, decay_wm, age_temp_drop = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    # Apply age-related temperature reduction
    eff_beta = base_beta * (1.0 - age_temp_drop * older)
    eff_beta = max(eff_beta, 1e-6)

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of the observed action
            p_rl = 1.0 / np.sum(np.exp(eff_beta * (Q_s - Q_s[a])))

            # WM policy probability of the observed action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL entropy as uncertainty measure
            rl_probs_full = np.exp(eff_beta * (Q_s - np.max(Q_s)))
            rl_probs_full /= np.sum(rl_probs_full)
            entropy = -np.sum(rl_probs_full * np.log(np.clip(rl_probs_full, eps, 1.0)))
            max_entropy = np.log(nA)
            norm_entropy = entropy / max_entropy  # in [0,1]

            # Arbitration: more WM when RL entropy is high; also scale by set-size load
            load_scale = 3.0 / float(nS)
            wm_mix = np.clip(wm_prior * load_scale * (1.0 + entropy_beta * norm_entropy), 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update:
            # - Passive decay toward uniform every time a state is visited.
            # - On reward, add a sharpening step toward the chosen action.
            # - On no-reward, just decay (already applied).
            # Passive decay
            w[s, :] = (1.0 - decay_wm) * W_s + decay_wm * w_0[s, :]
            if r > 0.5:
                sharpen = 0.7  # strong consolidation on reward
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                w[s, :] = (1.0 - sharpen) * w[s, :] + sharpen * peaked

        blocks_log_p += log_p

    return -blocks_log_p