Here are three standalone cognitive models that follow your template structure. Each model blends a delta-rule RL system with a working-memory (WM) component, but they differ in how WM is formed, how it degrades under load, and how it contributes to choice.

Note: As requested, all three functions:
- Take inputs (states, actions, rewards, blocks, set_sizes, model_parameters)
- Use all parameters meaningfully (≤ 6 per model)
- Return the negative log-likelihood of observed actions
- Fill in the WM policy and WM updating inside your provided template flow
- Do not import packages (assume numpy as np is already available)


def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, probabilistic-encoding WM with lapse.

    Mechanism
    - RL: standard delta-rule with softmax choice.
    - WM: stores near one-hot state-action associations when rewarded, but encoding
      probability is reduced by set-size relative to a slot capacity. WM also has
      a lapse component pushing it toward uniform.
    - Arbitration: fixed mixture weight (wm_weight) from the template; capacity,
      encoding strength, and lapses modulate p_wm internally.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Arbitration weight for WM in the fixed mixture (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        slots: float
            Effective WM slot capacity (>=0). Influence realized via slots / set size.
        p_bind: float
            Encoding strength toward the one-hot WM trace after reward (0..1).
        lapse: float
            WM lapse toward uniform per trial (0..1). Up-weighted when set size exceeds capacity.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, slots, p_bind, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM if perfectly encoded

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track graded memory strength per state (0..1), shaped by capacity and encoding
        mem_strength = np.zeros(nS)

        # Capacity factor: how much of the set can be held
        cap_factor = min(1.0, max(0.0, float(slots)) / max(1.0, float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # - Start with deterministic WM via softmax on W
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # - Memory availability scales by cap_factor and mem_strength[s]
            # - Lapse increases when load > capacity
            overload = max(0.0, 1.0 - cap_factor)
            lapse_eff = np.clip(lapse * (1.0 + 0.5 * overload), 0.0, 1.0)
            avail = np.clip(cap_factor * mem_strength[s], 0.0, 1.0)

            # Blend deterministic WM with uniform based on availability and lapses
            p_uniform = 1.0 / nA
            p_wm_avail = avail * p_wm_det + (1.0 - avail) * p_uniform
            p_wm = (1.0 - lapse_eff) * p_wm_avail + lapse_eff * p_uniform

            # Total policy via template mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            # - Move w[s] toward one-hot for chosen action when rewarded
            # - The update magnitude is reduced by capacity (cap_factor) and p_bind
            # - When not rewarded, decay toward uniform to avoid reinforcing errors
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            enc_strength = np.clip(p_bind * cap_factor, 0.0, 1.0)

            if r > 0.5:
                w[s, :] = (1.0 - enc_strength) * w[s, :] + enc_strength * one_hot
                mem_strength[s] = np.clip(mem_strength[s] + enc_strength * 0.8, 0.0, 1.0)
            else:
                decay_err = 0.5 * enc_strength
                w[s, :] = (1.0 - decay_err) * w[s, :] + decay_err * w_0[s, :]
                mem_strength[s] = np.clip(mem_strength[s] * (1.0 - 0.3 * enc_strength), 0.0, 1.0)

            # Normalize WM row to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + uncertainty-gated WM with load-sensitive precision and decay.

    Mechanism
    - RL: standard delta-rule with softmax choice.
    - WM: stores near one-hot associations when rewarded; decays toward uniform each trial.
      WM readout precision degrades with set size.
    - Arbitration inside WM policy: WM reference is blended with RL policy as a function
      of RL uncertainty (entropy) – when RL is uncertain, rely more on WM, otherwise less.
      Load increases RL's effective uncertainty scaling.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Arbitration weight for WM in the fixed mixture (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        chi_spread: float
            Sensitivity of WM-vs-RL blending to RL uncertainty (>=0).
        rho_decay: float
            Per-trial WM decay toward uniform (0..1).
        xi_noise: float
            Load sensitivity of WM readout noise; larger means less WM precision as set size grows (>=0).

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, chi_spread, rho_decay, xi_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # base precision of WM absent load

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Effective WM precision reduces with load
        load_scale = max(0.0, (nS - 3) / 3.0)  # 0 for size=3, 1 for size=6
        beta_wm_eff = softmax_beta_wm / (1.0 + xi_noise * load_scale)

        # Precompute uniform distribution
        p_uniform = 1.0 / nA

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL uncertainty via entropy of RL softmax policy
            # Compute full RL softmax probs (for entropy)
            q_logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(q_logits)
            pi_rl /= np.sum(pi_rl)
            H_rl = -np.sum(np.clip(pi_rl, 1e-12, 1.0) * np.log(np.clip(pi_rl, 1e-12, 1.0)))

            # Map uncertainty to WM blending inside p_wm, also scaled by load
            # Higher H_rl -> larger alpha_unc (more WM reliance)
            alpha_unc = 1.0 / (1.0 + np.exp(-chi_spread * (H_rl - (1.0 + 0.5 * load_scale))))
            alpha_unc = np.clip(alpha_unc, 0.0, 1.0)

            # WM readout policy (with load-degraded precision)
            p_wm_det = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Blend WM with RL based on RL uncertainty
            p_wm = alpha_unc * p_wm_det + (1.0 - alpha_unc) * p_rl
            p_wm = np.clip(p_wm, 1e-12, 1.0)

            # Total policy via template mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay every trial
            w = (1.0 - rho_decay) * w + rho_decay * w_0

            # WM reinforcement on reward
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strong overwrite toward the rewarded association
                lam = 1.0 - (1.0 - 0.85) * (1.0 - load_scale)  # ~0.85 to 1.0 across load
                w[s, :] = (1.0 - lam) * w[s, :] + lam * one_hot

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency-weighted WM with load-driven drift and precision control.

    Mechanism
    - RL: standard delta-rule with softmax choice.
    - WM: a recency-based trace per state that moves toward the most recent rewarded action.
      Each trial, WM also drifts toward uniform; drift increases with set size.
    - WM readout precision is tunable, and WM policy is internally mixed with uniform
      based on current load-driven drift to reflect reduced reliability in larger sets.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate.
        wm_weight: float
            Arbitration weight for WM in the fixed mixture (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        recency_lambda: float
            Step size toward one-hot trace for rewarded actions (0..1).
        load_gamma: float
            Scales WM drift toward uniform with set size (>=0).
        wm_precision: float
            Scales WM softmax inverse temperature (>=0), applied to a high base.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, recency_lambda, load_gamma, wm_precision = model_parameters
    softmax_beta *= 10.0

    # High base precision modulated by wm_precision
    softmax_beta_wm = 10.0 * max(0.0, wm_precision)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-driven drift parameter (0 for low load, increases with set size)
        load_scale = max(0.0, (nS - 3) / 3.0)  # 0 for 3, 1 for 6
        drift = np.clip(load_gamma * load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM readout with precision and added load-driven unreliability:
            # Blend WM softmax with uniform based on drift (more drift -> less reliable WM)
            p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_uniform = 1.0 / nA
            p_wm = (1.0 - drift) * p_wm_det + drift * p_uniform

            # Total policy via template mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM drift toward uniform each trial (stronger in larger set sizes)
            w = (1.0 - drift) * w + drift * w_0

            # Recency-based WM reinforcement when rewarded
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                lam = np.clip(recency_lambda, 0.0, 1.0)
                w[s, :] = (1.0 - lam) * w[s, :] + lam * one_hot

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p

How these models address load (3 vs 6):
- Model 1 (capacity + lapses): Working memory strength and encoding are explicitly reduced when set size exceeds slot capacity, with more lapses pushing WM toward uniform in large sets.
- Model 2 (uncertainty-gated): WM precision worsens with load and arbitration within p_wm leans more on WM when RL is uncertain; with larger sets, RL tends to be more uncertain early, but WM precision also deteriorates.
- Model 3 (recency WM + drift): WM drifts faster toward uniform in larger sets, reducing its reliability; WM precision is tunable via wm_precision, and drift is directly load-driven.