def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with age- and load-modulated arbitration and WM perseveration.

    Idea:
    - RL learns Q-values with a single learning rate.
    - WM stores recent rewarded state-action associations, read out via a deterministic softmax.
    - Arbitration weight (wm_weight) is adapted online from a base value: reduced by higher load (nS)
      and by age group (older >=45), increased after recent reward for that state (confidence).
    - WM policy includes a perseveration bias toward the most recent action taken in that state, modeling
      habit-like stickiness specifically within the WM system (older adults may rely on this more under load).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline arbitration weight for WM (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_confidence_boost: Increase to WM weight after reward in the same state (>=0).
    - stick_alpha: Strength of WM perseveration bias toward last chosen action (>=0).
    - age_load_penalty: Additional penalty to WM weight for older adults and larger set sizes (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: task arrays; age[0] is participant age.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_confidence_boost, stick_alpha, age_load_penalty = model_parameters

    softmax_beta *= 10.0  # RL precision upscaled per template
    softmax_beta_wm = 50.0  # WM is near-deterministic
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for WM perseveration
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax with perseveration bias toward last action in this state
            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                bias = np.zeros(nA)
                bias[last_action[s]] += stick_alpha
                W_s = W_s + bias  # additively bias WM values toward previous action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration weight adapted by load, age, and recent reward in state
            wm_weight = wm_weight_base
            # Penalize by load (larger sets reduce WM reliability)
            wm_weight = wm_weight / (1.0 + np.log(max(2.0, float(nS))))
            # Additional penalty for older adults that scales with load
            wm_weight = wm_weight * (1.0 / (1.0 + age_load_penalty * is_older * np.log(max(2.0, float(nS)))))
            # Confidence boost when last trial in this state was rewarded
            wm_weight = np.clip(wm_weight + wm_confidence_boost * last_reward[s], 0.0, 1.0)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Simple WM decay toward uniform, and overwrite on reward to one-hot
            decay = 0.05 + 0.1 * is_older  # older WM decays slightly more
            w = (1.0 - decay) * w + decay * w_0
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = target  # strong overwrite on rewarded association

            # Update state-wise traces
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-weighted arbitration and age-dependent WM noise.

    Idea:
    - RL updates with a standard learning rate.
    - WM stores recently rewarded pairs; readout is near-deterministic but subject to age-dependent noise.
    - Arbitration weight increases when RL is uncertain (high entropy over actions) and decreases with load.
      Older adults exhibit higher WM noise and thus lower effective WM contribution, especially at high load.
    - WM is refreshed on every trial for the current state (even without reward) with a small step toward chosen action.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline WM arbitration weight (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_refresh: Size of WM refresh toward the chosen action each trial in that state (0..1).
    - rl_uncertainty_gain: Scales the increase of WM weight with RL uncertainty (>=0).
    - age_wm_noise: Magnitude of additional noise injected into WM for older adults (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays; age[0] is participant age.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_refresh, rl_uncertainty_gain, age_wm_noise = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Age-dependent WM noise: blend W_s toward uniform for older adults
            W_s = w[s, :].copy()
            noise_strength = age_wm_noise * is_older * np.log(max(2.0, float(nS)))  # more noise with load
            if noise_strength > 0.0:
                W_s = (1.0 - noise_strength) * W_s + noise_strength * w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: increase WM weight with RL uncertainty, decrease with load
            # RL uncertainty via action entropy under RL policy for state s
            # Compute RL policy distribution pi_rl for state s (not just chosen prob)
            logits = softmax_beta * (Q_s - np.max(Q_s))
            pi_rl = np.exp(logits) / max(np.sum(np.exp(logits)), eps)
            entropy_rl = -np.sum(pi_rl * np.log(np.clip(pi_rl, eps, 1.0)))
            wm_weight = wm_weight_base
            wm_weight += rl_uncertainty_gain * entropy_rl
            wm_weight = wm_weight / (1.0 + np.log(max(2.0, float(nS))))  # load penalty
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay WM slightly toward uniform; reward-driven set; and small refresh toward chosen action
            w = 0.98 * w + 0.02 * w_0
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * target
            # Refresh toward the chosen action regardless of reward (strength wm_refresh)
            refresh_vec = (1.0 - wm_refresh) * w[s, :] + wm_refresh * np.eye(nA)[a]
            w[s, :] = refresh_vec

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited WM with age-biased action under high load.

    Idea:
    - RL standard learning.
    - WM has an effective capacity K: when set size exceeds K, WM traces are diluted (toward uniform).
    - Older adults show stronger dilution at high load and a small default bias toward a specific action
      (e.g., action 0) when WM is overloaded, capturing heuristic responding.
    - Arbitration weight is base-modulated by capacity pressure (nS/K) and age.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight_base: Baseline WM arbitration weight (0..1).
    - wm_capacity_k: WM capacity in number of items (>0).
    - wm_decay_rate: Global WM decay rate toward uniform per trial (0..1).
    - age_bias_to_action0: Size of additive WM bias toward action 0 when older and overloaded (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays; age[0] is participant age.
    - model_parameters: tuple/list of 6 parameters as above.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_capacity_k, wm_decay_rate, age_bias_to_action0 = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute overload factor: >1 means overloaded
        overload = max(1.0, float(nS) / max(1e-6, float(wm_capacity_k)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Capacity-limited WM: when overloaded, dilute W_s and add age-dependent bias to action 0
            W_s = w[s, :].copy()
            dilution = np.clip((overload - 1.0), 0.0, 10.0)  # 0 when under capacity, grows with overload
            dilution_strength = np.tanh(dilution)  # in (0,1)
            W_s = (1.0 - dilution_strength) * W_s + dilution_strength * w_0[s, :]

            if is_older and overload > 1.0:
                bias_vec = np.zeros(nA)
                bias_vec[0] = age_bias_to_action0 * np.log(overload + 1.0)  # stronger with overload
                W_s = W_s + bias_vec

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: reduce WM weight with overload and age
            wm_weight = wm_weight_base / (1.0 + np.log(max(2.0, float(nS))))
            wm_weight = wm_weight / (1.0 + 0.5 * is_older * (overload - 1.0))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay; on reward, strengthen the chosen action; on no reward, slight soft update
            w = (1.0 - wm_decay_rate) * w + wm_decay_rate * w_0
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Overwrite strength reduced when overloaded, more so for older
                overw = 0.9 / (1.0 + (overload - 1.0) * (1.0 + 0.5 * is_older))
                w[s, :] = (1.0 - overw) * w[s, :] + overw * target
            else:
                # Mild reinstatement toward uniform on error
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p