def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + fast-learning WM with set-size gated mixture (sigmoidal gating).

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM: a fast-learning associative table w[s,a] that tracks immediate stimulus-response
      contingencies with a high inverse temperature. WM is updated with its own learning rate.
    - Set-size gating: the mixture weight given to WM is reduced as set size increases via a
      sigmoidal gate centered at K_mid with slope > 0.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for Q-values.
        - wm_base: float in [0,1]
            Base WM weight before set-size gating.
        - softmax_beta: float
            Base inverse temperature for RL softmax (scaled internally by 10).
        - wm_alpha: float
            WM learning rate updating the w table.
        - K_mid: float
            Midpoint set size for the WM gating sigmoid (higher -> WM persists to larger sets).
        - slope: float
            Slope of the WM gating sigmoid (larger -> sharper transition with set size).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_base, softmax_beta, wm_alpha, K_mid, slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size gated WM weight (more WM for smaller sets)
        gate = 1.0 / (1.0 + np.exp(-slope * (K_mid - nS)))
        wm_weight_eff = np.clip(wm_base * gate, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over the WM table (deterministic)
            w_centered = W_s - np.mean(W_s)
            wm_logits = np.clip(softmax_beta_wm * w_centered, -50, 50)
            wm_probs = np.exp(wm_logits)
            wm_probs /= max(np.sum(wm_probs), eps)
            p_wm = wm_probs[a]

            # Mixture
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: fast delta-rule towards reward structure
            # Move toward a peaked distribution on rewarded action; otherwise shallow update.
            target = w_0[s, :].copy()
            if r > 0:
                target[:] = eps
                target[a] = 1.0
                target /= np.sum(target)

            w[s, :] = (1 - wm_alpha) * w[s, :] + wm_alpha * target
            # Normalize for numerical stability
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM heuristic (win-favor / lose-avoid) with set-size dependent decision noise.

    Mechanism
    - RL: delta-rule Q-learning with softmax; inverse temperature decreases with set size,
      capturing higher decision noise under high load.
    - WM: a heuristic policy that biases toward the last rewarded action in a state ("win-favor")
      and away from the last non-rewarded action ("lose-avoid"). WM maintains a policy-like row
      per state that is reset after each trial based on the most recent outcome.
    - Mixture: fixed WM weight combining the WM heuristic and RL softmax.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight for WM heuristic.
        - softmax_beta: float
            Base RL inverse temperature (scaled internally by 10 and divided by a set-size factor).
        - noise_slope: float >= 0
            Increases RL noise with set size: beta_rl = beta0 / (1 + noise_slope*(nS-1)).
        - win_bias: float
            Transformed to a probability via sigmoid for concentrating WM on last rewarded action.
        - lose_bias: float
            Transformed to a small probability via sigmoid for the last non-rewarded action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, noise_slope, win_bias, lose_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    sig = lambda x: 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1 / nA) * np.ones((nS, nA))

        # WM policy table; will be set after each trial based on outcome
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # State-wise memory of last action and outcome
        last_a = -np.ones(nS, dtype=int)
        last_r = np.zeros(nS)

        # Set-size-dependent RL inverse temperature
        beta_rl = softmax_beta / (1.0 + max(0.0, noise_slope) * max(0, nS - 1))

        # Map biases into probabilities
        p_win = np.clip(sig(win_bias), 0.0, 1.0)           # desired prob on last rewarded action
        p_lose = np.clip(sig(lose_bias), 0.0, 1.0) / nA    # small mass retained on last non-rewarded action

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))

            # WM policy from the current w row
            prefs = w[s, :] - np.mean(w[s, :])
            wm_logits = np.clip(softmax_beta_wm * prefs, -50, 50)
            wm_probs = np.exp(wm_logits)
            wm_probs /= max(np.sum(wm_probs), eps)
            p_wm = wm_probs[a]

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM heuristic update based on outcome just observed:
            # Reset the row to a distribution that favors (or avoids) the last chosen action.
            if r > 0:
                # Favor the chosen action strongly (prob ~ p_win), others share the remainder.
                w[s, :] = (1 - p_win) / (nA - 1)
                w[s, a] = p_win
            else:
                # Avoid the chosen action (assign small p_lose), others share remaining mass.
                w[s, :] = (1 - p_lose) / (nA - 1)
                w[s, a] = p_lose

            # Track last action/outcome (used implicitly via w)
            last_a[s] = a
            last_r[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-driven WM exploration with leaky counts and stay bonus.

    Mechanism
    - RL: delta-rule Q-values with softmax.
    - WM: maintains leaky counts of successes and trials per state-action to estimate
      Rhat(s,a) and adds a directed exploration bonus ~ eta / sqrt(N+1).
      A small stay bonus favors repeating the previous action in the state.
      Larger set sizes increase leak, making WM estimates more volatile under load.
    - Mixture: fixed WM weight combining WM policy and RL softmax.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight assigned to WM policy.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - eta_bonus: float >= 0
            Strength of directed exploration in WM via 1/sqrt(N+1).
        - leak: float in [0,1]
            Base leak of WM counts; effective leak increases with set size.
        - stay_bonus: float
            Additive preference in WM for repeating the last action in the same state.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, eta_bonus, leak, stay_bonus = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))  # not central to this model, but kept for template parity
        w_0 = (1 / nA) * np.ones((nS, nA))

        # WM: leaky counts of successes and trials
        succ = np.zeros((nS, nA))
        cnt = np.zeros((nS, nA))

        # Track last action for stay bonus
        last_a = -np.ones(nS, dtype=int)

        # Set-size dependent leak (more leak for larger sets)
        leak_eff = np.clip(leak * (nS / max(1, nS)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: compute values from leaky estimates + directed exploration + stay bonus
            # Rhat estimate
            Rhat = np.divide(succ[s, :], np.maximum(cnt[s, :], 1.0), out=np.zeros_like(succ[s, :]), where=np.maximum(cnt[s, :], 1.0) > 0)

            # Directed exploration bonus
            bonus = eta_bonus / np.sqrt(cnt[s, :] + 1.0)

            # Stay preference
            stay_vec = np.zeros(nA)
            if last_a[s] >= 0:
                stay_vec[last_a[s]] = stay_bonus

            wm_vals = Rhat + bonus + stay_vec
            wm_vals_centered = wm_vals - np.mean(wm_vals)
            wm_logits = np.clip(softmax_beta_wm * wm_vals_centered, -50, 50)
            wm_probs = np.exp(wm_logits)
            wm_probs /= max(np.sum(wm_probs), eps)
            p_wm = wm_probs[a]

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: leak counts, then add current observation
            succ *= (1.0 - leak_eff)
            cnt *= (1.0 - leak_eff)

            cnt[s, a] += 1.0
            succ[s, a] += r

            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p