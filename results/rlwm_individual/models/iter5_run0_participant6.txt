def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + one-shot working-memory store with load- and age-dependent encoding/decay and lapse.

    Idea
    - Model-free Q-learning runs in parallel.
    - A one-shot working-memory (WM) store per state captures a single action when rewarded,
      with a probability that drops under higher set size and for older adults.
    - The WM trace decays faster under higher set size and for older adults.
    - The policy blends WM (when available) and RL, with a lapse that increases with load and age.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Feedback per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; e.g., 3 or 6).
    age : array-like of float or int
        Participant age; age[0] is used to set age group.
    model_parameters : tuple/list
        (alpha, beta, p_store_base, k_load_age, lapse_base)
        - alpha: RL learning rate for Q updates (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - p_store_base: base probability to store a rewarded action into WM (0..1).
        - k_load_age: multiplicative penalty scaling for both load (larger set size)
                      and age on WM encoding probability and on WM decay.
                      Higher => worse WM under load/age.
        - lapse_base: base lapse probability mixed with uniform; scales up with load and age.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, p_store_base, k_load_age, lapse_base = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM
        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        wm_action = -1 * np.ones(nS, dtype=int)          # stored action per state (-1 = empty)
        wm_strength = np.zeros(nS, dtype=float)          # 0..1 strength per state

        # Precompute load/age terms
        load_term = (float(nS) - 3.0) / 3.0              # 0 for 3-set, 1 for 6-set
        age_term = is_older
        # Effective WM store probability decreases with load and age
        p_store_eff = p_store_base * np.exp(-k_load_age * (0.7 * load_term + 0.3 * age_term))
        p_store_eff = max(0.001, min(0.999, p_store_eff))

        # WM decay increases with load and age
        base_decay = 0.03
        wm_decay = base_decay + k_load_age * (0.5 * load_term + 0.5 * age_term)
        wm_decay = max(0.0, min(0.5, wm_decay))

        # Lapse increases with load and age
        lapse = lapse_base * (1.0 + 0.6 * load_term + 0.4 * age_term)
        lapse = max(1e-6, min(0.49, lapse))

        logp_block = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax probability of chosen action
            prefs = beta * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_rl = 1.0 / max(denom, 1e-12)

            # WM policy: if we have a stored action with strength m, use point mass; else uniform
            m = wm_strength[s]
            if wm_action[s] >= 0:
                p_wm_chosen = m if a == wm_action[s] else 0.0
                # To avoid zero-prob for WM, add tiny residual split across other actions:
                if wm_action[s] >= 0 and a != wm_action[s]:
                    p_wm_chosen = 0.0
            else:
                # No WM: act uniformly under WM policy
                p_wm_chosen = 1.0 / nA

            # Blend WM and RL by current WM strength
            p_mix = (1.0 - m) * p_rl + m * p_wm_chosen
            # Add lapse to uniform
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp_block += np.log(p_final)

            # Learning updates
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: decay, then potential one-shot store on reward
            wm_strength[s] = (1.0 - wm_decay) * wm_strength[s]
            if r > 0.5:
                # Attempt to store or refresh the mapping with probability p_store_eff
                # Stronger refresh if we store the same action, else overwrite with lower strength
                if np.random.rand() < p_store_eff:
                    if wm_action[s] == a:
                        wm_strength[s] = min(1.0, wm_strength[s] + (1.0 - wm_strength[s]) * 0.8)
                    else:
                        wm_action[s] = a
                        wm_strength[s] = max(wm_strength[s], 0.6)
            else:
                # Optional small weakening on negative feedback
                wm_strength[s] = max(0.0, wm_strength[s] - 0.1 * (0.5 + 0.5 * load_term))

            # If WM very weak, clear the slot
            if wm_strength[s] < 1e-3:
                wm_action[s] = -1

        total_logp += logp_block

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Load- and age-adaptive meta-control of learning rate and temperature with lapse.

    Idea
    - Standard Q-learning, but the learning rate (alpha_t) and inverse temperature (beta_t)
      adapt online to an estimate of within-block cognitive load (fraction of unique states seen).
    - Older adults show reduced effective learning rate and stronger temperature reduction under load.
    - A lapse increases with load and age, capturing attentional lapses.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Feedback per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; e.g., 3 or 6).
    age : array-like of float or int
        Participant age; age[0] is used to set age group.
    model_parameters : tuple/list
        (alpha_base, beta_base, k_load_temp, k_age_lr, epsilon)
        - alpha_base: base learning rate (0..1).
        - beta_base: base inverse temperature (>0).
        - k_load_temp: sensitivity of temperature to load; higher => colder (lower beta) under load/age.
        - k_age_lr: learning-rate reduction factor applied for older adults; also gates load impact on alpha.
        - epsilon: base lapse mixed with uniform that scales with load and age.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_base, beta_base, k_load_temp, k_age_lr, epsilon = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        seen = np.zeros(nS, dtype=bool)

        logp_block = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Update seen set and compute load estimate in [0,1]
            seen[s] = True
            frac_seen = np.sum(seen) / float(nS)  # 0..1
            load_effect = frac_seen * (float(nS) / 6.0)

            # Adaptive temperature: lower beta under higher load and for older
            temp_scale = 1.0 + k_load_temp * load_effect * (1.0 + 0.5 * is_older)
            beta_t = beta_base / max(1e-6, temp_scale)

            # Adaptive learning rate: lower under load and for older
            alpha_t = alpha_base * (1.0 - k_age_lr * (0.7 * is_older + 0.3 * load_effect))
            alpha_t = max(0.0, min(1.0, alpha_t))

            # Softmax with adaptive beta
            prefs = beta_t * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_soft = 1.0 / max(denom, 1e-12)

            # Lapse increases with load and age
            eps_t = epsilon * (1.0 + 0.5 * is_older + 0.5 * load_effect)
            eps_t = max(1e-6, min(0.49, eps_t))
            p_final = (1.0 - eps_t) * p_soft + eps_t * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp_block += np.log(p_final)

            # Q update with adaptive alpha
            pe = r - Q[s, a]
            Q[s, a] += alpha_t * pe

            # Mild load- and age-dependent forgetting of unchosen actions at this state (attentional drift)
            drift = 0.02 * (0.5 * load_effect + 0.5 * is_older)
            if drift > 0.0:
                for aj in range(nA):
                    if aj != a:
                        Q[s, aj] = (1.0 - drift) * Q[s, aj] + drift * (1.0 / nA)

        total_logp += logp_block

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Latent rule inference with hazard-driven forgetting, modulated by set size and age.

    Idea
    - For each state, the agent maintains a categorical belief over which action is the correct rule.
      Beliefs are represented as pseudo-counts and updated with reward feedback.
    - A hazard rate captures distraction/forgetting: with some probability, beliefs revert toward uniform.
      Hazard increases with set size and for older adults.
    - Choices follow a softmax over log-belief (equivalently, power transform on beliefs), with a lapse.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of float/int
        Feedback per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within block; e.g., 3 or 6).
    age : array-like of float or int
        Participant age; age[0] is used to set age group.
    model_parameters : tuple/list
        (beta, h_base, k_hs, alpha_belief, epsilon)
        - beta: inverse temperature applied to log-beliefs (>0).
        - h_base: base hazard toward uniform beliefs per trial (0..1).
        - k_hs: scaling of hazard by set size and age (higher => more forgetting under load/age).
        - alpha_belief: learning step for pseudo-count updates from feedback (>0).
        - epsilon: lapse probability mixed with uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    beta, h_base, k_hs, alpha_belief, epsilon = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize symmetric pseudo-counts (Dirichlet-like)
        counts = np.ones((nS, nA), dtype=float)

        # Hazard adjusted by set size and age
        load_term = (float(nS) - 3.0) / 3.0  # 0 for 3, 1 for 6
        h = h_base + k_hs * (0.6 * load_term + 0.4 * is_older)
        h = max(0.0, min(0.5, h))  # keep reasonable

        logp_block = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Apply hazard-driven drift toward uniform
            if h > 0.0:
                counts[s, :] = (1.0 - h) * counts[s, :] + h * 1.0

            # Current belief
            bvec = counts[s, :] / np.sum(counts[s, :])

            # Softmax over log-belief: p ~ b^beta
            logits = beta * np.log(np.maximum(bvec, 1e-12))
            denom = np.sum(np.exp(logits - logits[a]))
            p_rule = 1.0 / max(denom, 1e-12)

            p_final = (1.0 - epsilon) * p_rule + epsilon * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp_block += np.log(p_final)

            # Belief update from feedback
            if r > 0.5:
                counts[s, a] += alpha_belief
            else:
                # Non-reward: reduce confidence in chosen action by relatively increasing others
                delta = alpha_belief / (nA - 1)
                for aj in range(nA):
                    if aj != a:
                        counts[s, aj] += delta
                # To keep scale controlled, softly shrink the chosen count
                counts[s, a] = max(1e-6, counts[s, a] * (1.0 - 0.25 * alpha_belief))

        total_logp += logp_block

    return -total_logp