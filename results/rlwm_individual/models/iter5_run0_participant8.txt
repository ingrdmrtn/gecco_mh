def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid state-dependent RL and global habit system with age- and load-dependent arbitration.

    Mechanism
    - RL (state-action): tabular Q-learning per state.
    - Habit (action-only): global action values learned across all states.
    - Arbitration: mixture weight for the habit system depends on set size and age
      (older age and larger set sizes increase reliance on habit).
    - Choice policy: convex mixture of softmax over Q_s and softmax over H.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block (0..nS-1).
    actions : array-like of int
        Observed action (0..2). Out-of-range actions are treated as lapses (uniform likelihood).
    rewards : array-like of float
        Feedback (commonly 0/1). Negative rewards are treated as invalid trials (uniform likelihood).
    blocks : array-like of int
        Block index per trial; states reset between blocks.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array.
    model_parameters : iterable of 5 floats
        alpha_q  : Learning rate for state-action RL (0..1).
        beta     : Inverse temperature for softmax (>0) used for both systems.
        alpha_h  : Learning rate for the global habit values (0..1).
        omega0   : Baseline arbitration logit for habit weight.
        age_mod  : Age sensitivity in the arbitration logit (per decade from 45).

        Habit weight w_h = sigmoid(omega0 - log(set_size) + age_mod * ((age-45)/10)).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed actions under the model.
    """
    alpha_q, beta, alpha_h, omega0, age_mod = model_parameters
    age_val = float(age[0])

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Initialize state-action RL and global habit values
        Q = np.zeros((nS, nA))
        H = np.zeros(nA)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Lapse or invalid observation handling
            if (s < 0 or s >= nS) or (a < 0 or a >= nA) or (r < 0):
                total_log_p += np.log(1.0 / nA)
                continue

            # Softmax for RL (per state)
            m_q = np.max(beta * Q[s, :])
            p_rl = np.exp(beta * Q[s, :] - m_q)
            p_rl = p_rl / np.sum(p_rl)

            # Softmax for Habit (global action propensity)
            m_h = np.max(beta * H)
            p_h = np.exp(beta * H - m_h)
            p_h = p_h / np.sum(p_h)

            # Arbitration weight depends on set size and age
            # Larger set sizes and older age increase habit reliance
            age_term = (age_val - 45.0) / 10.0
            logit_w = omega0 - np.log(max(ss, 1.0)) + age_mod * age_term
            w_h = 1.0 / (1.0 + np.exp(-logit_w))
            w_h = np.clip(w_h, 0.0, 1.0)

            p_mix = w_h * p_h + (1.0 - w_h) * p_rl
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning updates
            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_q * pe

            # Habit update (global action value)
            pe_h = r - H[a]
            H[a] += alpha_h * pe_h

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Single-system RL with age- and load-modulated learning rate and action lapses.

    Mechanism
    - Q-learning per state with an effective learning rate that decreases with set size and age.
    - Lapse probability increases with set size and age; during lapses, choices are assumed uniform.
    - Softmax action selection with fixed inverse temperature.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1) within each block.
    actions : array-like of int
        Observed action (0..2). Out-of-range actions treated as lapses (uniform likelihood).
    rewards : array-like of float
        Reward feedback (0/1 typical). Negative rewards treated as invalid (uniform likelihood).
    blocks : array-like of int
        Block index per trial; Q-values reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array.
    model_parameters : iterable of 5 floats
        alpha0     : Baseline learning-rate logit (transformed via sigmoid).
        beta       : Inverse temperature for softmax (>0).
        zeta0      : Baseline lapse-rate logit (transformed via sigmoid).
        size_slope : Sensitivity of both LR and lapse to set size (per +3 items).
        age_slope  : Sensitivity of both LR and lapse to age (per decade from 45).

        Transformations:
        - Effective learning rate: lr_eff = sigmoid(alpha0 - size_slope*(ss-3) - age_slope*((age-45)/10))
        - Effective lapse:         zeta   = sigmoid(zeta0 + size_slope*(ss-3) + age_slope*((age-45)/10))

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed actions.
    """
    alpha0, beta, zeta0, size_slope, age_slope = model_parameters
    age_val = float(age[0])

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            if (s < 0 or s >= nS) or (a < 0 or a >= nA) or (r < 0):
                total_log_p += np.log(1.0 / nA)
                continue

            # Effective parameters given load and age
            age_term = (age_val - 45.0) / 10.0
            lr_eff = 1.0 / (1.0 + np.exp(-(alpha0 - size_slope * (ss - 3.0) - age_slope * age_term)))
            zeta = 1.0 / (1.0 + np.exp(-(zeta0 + size_slope * (ss - 3.0) + age_slope * age_term)))
            zeta = np.clip(zeta, 0.0, 0.5)  # keep lapse reasonable

            # Softmax over Q
            m = np.max(beta * Q[s, :])
            p_rl = np.exp(beta * Q[s, :] - m)
            p_rl = p_rl / np.sum(p_rl)

            # Mixture with uniform due to lapses
            p_mix = (1.0 - zeta) * p_rl + zeta * (np.ones(nA) / nA)
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += lr_eff * pe

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian associative memory with age- and load-dependent forgetting.

    Mechanism
    - For each state-action pair, maintain a Beta posterior over Bernoulli reward.
      Posterior parameters (success α_sa, failure β_sa) are updated with received outcomes.
    - Expected value for policy is the posterior mean: E[R|s,a] = α_sa / (α_sa + β_sa).
    - Global forgetting shrinks both α and β each trial toward zero; forgetting increases with set size and age.
    - Softmax over posterior means to produce action probabilities.

    Parameters
    ----------
    states : array-like of int
        State index per trial within each block (0..nS-1).
    actions : array-like of int
        Observed action (0..2). Out-of-range actions treated as lapses (uniform likelihood).
    rewards : array-like of float
        Reward feedback (0/1 typical). Negative rewards treated as invalid (uniform likelihood).
    blocks : array-like of int
        Block index per trial; posteriors reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array.
    model_parameters : iterable of 5 floats
        prior0    : Baseline symmetric Beta prior concentration per (s,a); α=β=prior0 (>0).
        beta      : Inverse temperature for softmax (>0).
        phi_base  : Baseline forgetting logit; transformed with sigmoid to [0,1] per trial.
        size_gain : Additive effect of set size on forgetting logit (per +3 items).
        age_gain  : Additive effect of age (per decade from 45) on forgetting logit.

        Forgetting per trial:
        phi = sigmoid(phi_base + size_gain*(ss-3) + age_gain*((age-45)/10))
        Posterior shrink: α <- (1-phi)*α, β <- (1-phi)*β before applying current-trial update.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed actions under the model.
    """
    prior0, beta, phi_base, size_gain, age_gain = model_parameters
    age_val = float(age[0])

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Beta posterior parameters for each state-action
        alpha_sa = np.full((nS, nA), prior0, dtype=float)
        beta_sa = np.full((nS, nA), prior0, dtype=float)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            if (s < 0 or s >= nS) or (a < 0 or a >= nA) or (r < 0):
                total_log_p += np.log(1.0 / nA)
                continue

            # Compute forgetting factor for this trial
            age_term = (age_val - 45.0) / 10.0
            phi = 1.0 / (1.0 + np.exp(-(phi_base + size_gain * (ss - 3.0) + age_gain * age_term)))
            phi = np.clip(phi, 0.0, 0.99)

            # Apply forgetting to all (s,a) parameters in the current state (or globally?)
            # Here we apply globally to encourage interference under load.
            alpha_sa *= (1.0 - phi)
            beta_sa *= (1.0 - phi)

            # Posterior means for current state
            means = alpha_sa[s, :] / np.clip(alpha_sa[s, :] + beta_sa[s, :], 1e-12, None)

            # Softmax over expected rewards
            m = np.max(beta * means)
            p = np.exp(beta * means - m)
            p = p / np.sum(p)

            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Update Beta posterior with observed outcome for chosen action
            alpha_sa[s, a] += r
            beta_sa[s, a] += (1.0 - r)

    return -float(total_log_p)