def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + episodic recall mixture with perseveration, and age-dependent recall.

    Mechanism
    - RL: Tabular Q-learning with softmax choice.
    - Episodic recall: If a state previously yielded a reward for a specific action,
      the agent can recall that action and choose it deterministically (otherwise uniform).
      The probability of using recall is a logistic function of a baseline plus a load term
      (reduced recall at larger set sizes) and an age penalty (reduced recall in older group).
    - Perseveration: Additive bias in the softmax for repeating the last chosen action,
      independent of outcome.
    - Mixture: Final policy is a convex combination of episodic policy and RL policy.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group (younger <45, older >=45).
    model_parameters : sequence of 5 floats
        [alpha, beta, recall_bias, age_recall_penalty, perseveration_weight]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta: inverse temperature for RL softmax (>=0).
        - recall_bias: baseline log-odds of using episodic recall vs RL.
        - age_recall_penalty: nonnegative penalty applied if older (reduces recall).
        - perseveration_weight: bias added to the last chosen action in softmax logits.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta, recall_bias, age_recall_penalty, perseveration_weight = model_parameters

    # Constrain parameters to sensible ranges
    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta = np.maximum(1e-6, beta)
    age_recall_penalty = np.maximum(0.0, age_recall_penalty)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        # Initialize RL values and episodic memory traces
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Episodic memory: store last rewarded action per state, -1 if none
        epi_action = -np.ones(nS, dtype=int)
        # Perseveration: last chosen action per state, -1 if none
        last_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # RL softmax with perseveration bias in logits
            logits = beta * Q[s, :].copy()
            if last_action[s] >= 0:
                logits[last_action[s]] += perseveration_weight
            logits -= np.max(logits)
            p_rl = np.exp(logits)
            p_rl /= np.maximum(1e-12, np.sum(p_rl))

            # Episodic recall policy
            pi_epi = np.ones(nA) / nA
            if epi_action[s] >= 0:
                pi_epi = np.zeros(nA)
                pi_epi[epi_action[s]] = 1.0

            # Recall probability: logistic of bias + load term - age penalty
            # Load term reduces recall at larger set sizes: (3 - nS)/3 in [-1,0] for nS>=3
            load_term = (3.0 - float(nS_t)) / 3.0
            recall_logit = recall_bias + load_term - age_group * age_recall_penalty
            w_recall = 1.0 / (1.0 + np.exp(-recall_logit))

            # Mixture policy
            p_total = w_recall * pi_epi + (1.0 - w_recall) * p_rl
            p_a = np.clip(p_total[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update episodic memory only on rewarded trials
            if r >= 0.5:
                epi_action[s] = a

            # Update perseveration trace
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with exploration bonus driven by uncertainty, modulated by load and age.

    Mechanism
    - RL: Tabular Q-learning with alpha.
    - Uncertainty bonus: Add an action-specific exploration bonus to the softmax logits
      that decays with experience (1/sqrt(N[s,a]+1)), encouraging directed exploration.
    - Load modulation: Larger set sizes increase exploration bonus (harder memory increases exploration).
    - Age modulation: Older group shifts exploration up or down (parameterized).
    - The final choice probabilities come from a softmax over Q + exploration bonuses.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group (younger <45, older >=45).
    model_parameters : sequence of 5 floats
        [alpha, beta0, xi_uncert, load_explore_gain, age_explore_shift]
        - alpha: Q-learning rate (sigmoid to [0,1]).
        - beta0: base inverse temperature for softmax (>=0).
        - xi_uncert: base strength of uncertainty bonus (>=0).
        - load_explore_gain: added to xi as set size increases (any real).
        - age_explore_shift: added to xi if older group (any real).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha, beta0, xi_uncert, load_explore_gain, age_explore_shift = model_parameters

    alpha = 1.0 / (1.0 + np.exp(-alpha))
    beta0 = np.maximum(1e-6, beta0)
    xi_uncert = np.maximum(0.0, xi_uncert)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visitation counts per state-action

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Construct exploration bonus scale for this trial
            # Load term increases with set size relative to 3: (nS-3)/3 in [0,1] for nS in {3,6}
            load_term = (float(nS_t) - 3.0) / 3.0
            xi_t = xi_uncert + load_explore_gain * load_term + age_group * age_explore_shift

            # Action-specific bonus decays with visits (uncertainty-driven)
            bonus = xi_t / np.sqrt(N[s, :] + 1.0)  # higher when less visited

            # Softmax over Q plus exploration bonus, scaled by beta0
            logits = beta0 * (Q[s, :] + bonus)
            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))

            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Update RL and counts
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta
            N[s, a] += 1.0

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with load- and age-modulated choice stickiness.

    Mechanism
    - RL: Tabular Q-learning with separate learning rates for positive and negative prediction errors.
    - Stickiness: Additive bias to repeat the previous action in the same state.
      The stickiness strength depends on both set size (stronger under higher load) and age group.
    - Policy: Softmax over Q plus stickiness bias.

    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback on each trial.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial (e.g., 3 or 6).
    age : array-like
        Participant age; uses age[0] to define age group (younger <45, older >=45).
    model_parameters : sequence of 5 floats
        [alpha_pos, alpha_neg, beta, stick_base, age_load_coeff]
        - alpha_pos: learning rate for positive prediction errors (sigmoid to [0,1]).
        - alpha_neg: learning rate for negative prediction errors (sigmoid to [0,1]).
        - beta: inverse temperature for softmax (>=0).
        - stick_base: baseline stickiness magnitude (can be any real).
        - age_load_coeff: scales how stickiness changes with load and age (any real).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed actions under the model.
    """
    alpha_pos, alpha_neg, beta, stick_base, age_load_coeff = model_parameters

    alpha_pos = 1.0 / (1.0 + np.exp(-alpha_pos))
    alpha_neg = 1.0 / (1.0 + np.exp(-alpha_neg))
    beta = np.maximum(1e-6, beta)

    age_group = 1 if age[0] >= 45 else 0
    nA = 3
    nll = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]
        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        prev_action = -np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_t = int(block_set_sizes[t])

            # Stickiness strength for this trial: combine load and age
            # Load term increases with set size relative to 3 in [0,1]; age_group in {0,1}
            load_term = (float(nS_t) - 3.0) / 3.0
            # Map combined factor through tanh to keep bounded influence
            kappa = stick_base * np.tanh(1.0 + age_load_coeff * (load_term + 0.5 * age_group))

            logits = beta * Q[s, :].copy()
            if prev_action[s] >= 0:
                logits[prev_action[s]] += kappa

            logits -= np.max(logits)
            p = np.exp(logits)
            p /= np.maximum(1e-12, np.sum(p))
            p_a = np.clip(p[a], 1e-12, 1.0)
            nll -= np.log(p_a)

            # Asymmetric RL update
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            prev_action[s] = a

    return nll