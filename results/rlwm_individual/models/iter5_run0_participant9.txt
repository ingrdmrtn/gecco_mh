def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and WM fast-store with set-size-scaled decay.

    Idea:
    - RL updates with separate learning rates for positive vs. negative prediction errors.
    - WM forms a sharp association for rewarded state-action pairs and decays toward uniform.
    - WM decay accelerates with larger set sizes via a power-law scaling (gamma_ns), reducing WM influence under high load.

    Parameters (6 total):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - wm_weight: Arbitration weight of WM in the mixture (0..1); RL gets (1 - wm_weight).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_decay_base: Baseline WM decay rate when nS=3 (0..1).
    - gamma_ns: Exponent controlling how WM decay scales with set size (>0). Effective decay = 1 - (1 - wm_decay_base)^( (nS/3)^gamma_ns ).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay_base, gamma_ns = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # set-size-scaled WM decay
        scale = (nS / 3.0) ** max(0.0, gamma_ns)
        wm_decay_eff = 1.0 - (1.0 - np.clip(wm_decay_base, 0.0, 1.0)) ** max(scale, 1e-6)
        wm_decay_eff = float(np.clip(wm_decay_eff, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (from template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W_s with high inverse temperature
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy (from template)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_use = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_use * delta

            # WM global decay toward uniform
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

            # WM rewarded association strengthening (fast-store on reward)
            if r > 0.5:
                # concentrate mass on the rewarded action in the visited state
                w[s, :] = (1.0 - wm_decay_eff) * w[s, :]
                w[s, a] += wm_decay_eff

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Q-forgetting and slot-like WM capacity (K) with stochastic storage.

    Idea:
    - RL learns via delta rule but also forgets toward uniform each trial (captures load/interference).
    - WM stores up to K distinct states with high-fidelity associations; other states are near-uniform.
    - Storage into WM is stochastic (wm_store_prob) and contingent on reward. When capacity is exceeded,
      the oldest stored state is evicted (FIFO).
    - Set-size effects emerge when nS > K: many states will not be stored in WM, reducing WM's contribution.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight: Arbitration weight of WM in the mixture (0..1); RL gets (1 - wm_weight).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - q_forget: RL forgetting rate toward uniform each trial (0..1).
    - K_capacity: WM capacity in number of states (>=0, real-valued; effectively rounded down).
    - wm_store_prob: Probability to store/update WM on rewarded trials (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, q_forget, K_capacity, wm_store_prob = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM maintained set and policy targets
        maintained_states = []
        stored_action = {}  # maps state -> action believed correct

        K = int(max(0, np.floor(K_capacity)))
        q_forget = float(np.clip(q_forget, 0.0, 1.0))
        wm_store_prob = float(np.clip(wm_store_prob, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Construct WM state's distribution: sharp if maintained, else uniform-ish
            if s in stored_action:
                # make W_s a peaked distribution on stored action
                W_s = np.copy(w[s, :])
                W_s[:] = 0.0
                W_s[stored_action[s]] = 1.0
                # write back for policy (keeps w as an internal buffer, but we'll not rely on its value directly)
                w[s, :] = W_s
            else:
                W_s = w[s, :]  # near-uniform initial

            Q_s = q[s, :]

            # RL policy (from template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: deterministic softmax over W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # global Q forgetting (toward uniform prior)
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA)

            # WM update: on reward, possibly store s with action a
            if r > 0.5:
                if np.random.rand() < wm_store_prob:
                    # insert/update
                    stored_action[s] = a
                    if s not in maintained_states:
                        maintained_states.append(s)
                    # evict if over capacity
                    if len(maintained_states) > K:
                        evict = maintained_states.pop(0)
                        if evict in stored_action:
                            stored_action.pop(evict, None)
                        w[evict, :] = w_0[evict, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-gated WM precision and WM learning via error-driven update + decay.

    Idea:
    - Arbitration is fixed by wm_weight in the mixture, but WM precision (its softmax temperature) is reduced
      under higher set sizes and higher WM uncertainty (entropy of W_s).
    - RL uses a standard delta rule.
    - WM updates toward a one-hot of the chosen action proportional to outcome r (wm_eta), and decays to uniform.
      This lets negative outcomes reduce the peak (noisy anti-learning).

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight: Arbitration weight of WM in the mixture (0..1); RL gets (1 - wm_weight).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_decay: Per-trial WM decay toward uniform (0..1).
    - wm_eta: WM learning rate toward the chosen action, scaled by reward (0..1).
    - kappa: Sensitivity controlling how WM inverse temperature shrinks with set size and entropy (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_eta, kappa = model_parameters
    softmax_beta *= 10.0
    base_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute set-size penalty on WM precision
        size_penalty = 1.0 + kappa * max(0.0, (nS - 3.0) / 3.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (from template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM entropy to modulate precision
            W_safe = np.clip(W_s, eps, 1.0)
            W_safe /= np.sum(W_safe)
            entropy = -np.sum(W_safe * np.log(W_safe + eps)) / np.log(nA)  # normalized 0..1
            # Effective WM beta shrinks with set size and entropy
            beta_wm_eff = base_beta_wm / (size_penalty * (1.0 + kappa * entropy))
            beta_wm_eff = max(1.0, beta_wm_eff)  # keep some determinism

            # WM policy with adaptive precision
            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM error-driven update toward chosen action, scaled by reward
            target = np.zeros(nA)
            target[a] = 1.0
            w[s, :] = (1.0 - wm_eta * r) * w[s, :] + (wm_eta * r) * target

        blocks_log_p += log_p

    return -blocks_log_p