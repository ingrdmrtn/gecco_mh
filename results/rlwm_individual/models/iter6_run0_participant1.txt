def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL with uncertainty- and load-gated WM recall and decay.

    The model blends model-free RL with a decaying working-memory (WM) store for each state.
    A meta-gate determines the mixture weight between WM and RL on each trial based on:
    - Cognitive load (set size; higher load reduces WM use),
    - Policy uncertainty (entropy of RL policy; higher uncertainty increases WM use),
    - Age group (younger vs older bias).

    WM stores the most recently rewarded action for each state with a confidence value that
    decays after unrewarded experience, and yields a peaked policy over the stored action.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback received on each trial.
    blocks : array-like of int
        Block index for each trial (resets memory/values per block).
    set_sizes : array-like of int
        Set size condition at each trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0] with group split: younger (<45) vs older (>=45).
    model_parameters : list or array
        [alpha, beta, wm_gain, wm_forget, age_wm_bias]
        - alpha (0..1): RL learning rate.
        - beta (>0): Inverse temperature for RL policy (scaled internally by 10).
        - wm_gain (real): Strength of WM gate; larger increases WM engagement at low load.
        - wm_forget (0..1): WM confidence decay factor applied on unrewarded trials.
        - age_wm_bias (real): Additive gate bias for younger vs older:
                              gate += age_wm_bias * (younger=+1, older=-1).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, wm_gain, wm_forget, age_wm_bias = model_parameters
    beta = max(1e-8, beta) * 10.0
    wm_forget = np.clip(wm_forget, 0.0, 1.0)

    age_val = age[0]
    younger = 1.0 if age_val < 45 else 0.0
    older = 1.0 - younger
    age_sign = younger - older  # +1 for younger, -1 for older

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: preferred action and its confidence
        wm_pref = -np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            pol_rl = np.exp(logits)
            pol_rl = pol_rl / (np.sum(pol_rl) + eps)

            # RL uncertainty: entropy of RL policy
            H_rl = -np.sum(pol_rl * np.log(np.clip(pol_rl, eps, 1.0)))

            # WM policy from stored preference and confidence
            if wm_pref[s] >= 0 and wm_conf[s] > 0.0:
                # Sharpened one-hot with softness controlled by confidence
                # epsilon_wm decreases with confidence
                eps_wm = np.exp(-np.abs(wm_conf[s]))  # in (0,1]
                pol_wm = np.ones(nA) * (eps_wm / nA)
                pol_wm[wm_pref[s]] += (1.0 - eps_wm)
            else:
                pol_wm = np.ones(nA) / nA

            # Gate WM vs RL using load, uncertainty, and age
            # More WM under low load (3/set), high uncertainty, and age_bias
            gate_drive = (wm_gain * (3.0 / float(set_t))) + (H_rl) + (age_wm_bias * age_sign)
            p_wm = 1.0 / (1.0 + np.exp(-gate_drive))
            p_wm = np.clip(p_wm, 0.0, 1.0)

            pol = p_wm * pol_wm + (1.0 - p_wm) * pol_rl

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: reward strengthens stored action/confidence; no-reward decays it
            if r > 0.5:
                wm_pref[s] = a
                wm_conf[s] = wm_conf[s] + 1.0  # accumulate confidence
            else:
                wm_conf[s] = wm_conf[s] * (1.0 - wm_forget)
                # If confidence goes near zero, clear preference
                if wm_conf[s] < 1e-6:
                    wm_pref[s] = -1

        neg_log_like += -log_p

    return float(neg_log_like)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with entropy- and load-dependent lapse control modulated by age group.

    Choices are generated by a mixture of:
    - Model-free RL softmax policy,
    - A uniform random lapse process.

    The lapse probability increases with set size (cognitive load) and with the entropy
    of the RL policy (uncertainty), and is modulated by age group.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback received on each trial.
    blocks : array-like of int
        Block index for each trial (resets values per block).
    set_sizes : array-like of int
        Set size condition at each trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0] with group split: younger (<45) vs older (>=45).
    model_parameters : list or array
        [alpha, beta, k_lapse, age_lapse_bias, k_entropy]
        - alpha (0..1): RL learning rate.
        - beta (>0): Inverse temperature for RL policy (scaled internally by 10).
        - k_lapse (real): Gain on load (set size) for lapse drive.
        - age_lapse_bias (real): Additive bias on lapse for older vs younger
                                 (applied as + for older, - for younger).
        - k_entropy (real): Gain on policy entropy for lapse drive.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, k_lapse, age_lapse_bias, k_entropy = model_parameters
    beta = max(1e-8, beta) * 10.0

    age_val = age[0]
    is_younger = 1.0 if age_val < 45 else 0.0
    is_older = 1.0 - is_younger
    age_term = age_lapse_bias * (is_older - is_younger)  # +bias for older, -bias for younger

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # RL softmax policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            pol_rl = np.exp(logits)
            pol_rl = pol_rl / (np.sum(pol_rl) + eps)

            # Policy entropy
            H_rl = -np.sum(pol_rl * np.log(np.clip(pol_rl, eps, 1.0)))

            # Lapse probability: increases with load and entropy, modulated by age
            load_term = k_lapse * np.log(max(1.0, set_t / 3.0))
            lapse_drive = load_term + (k_entropy * H_rl) + age_term
            xi = 1.0 / (1.0 + np.exp(-lapse_drive))
            xi = np.clip(xi, 0.0, 1.0)

            pol = (1.0 - xi) * pol_rl + xi * (np.ones(nA) / nA)

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

        neg_log_like += -log_p

    return float(neg_log_like)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with kernel-based state generalization whose width depends on load and age.

    The model learns Q-values with updates that spread to neighboring states within a block
    via a Gaussian kernel over state index distance. Kernel width increases with set size
    (encouraging more generalization under load) and is modulated by age group.

    A small global decay is applied to all Q-values on each trial to capture forgetting.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback received on each trial.
    blocks : array-like of int
        Block index for each trial (resets values per block).
    set_sizes : array-like of int
        Set size condition at each trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0] with group split: younger (<45) vs older (>=45).
    model_parameters : list or array
        [alpha, beta, gen_gain, age_gen_shift, decay]
        - alpha (0..1): Learning rate for TD updates.
        - beta (>0): Inverse temperature for softmax policy (scaled internally by 10).
        - gen_gain (>0): Base kernel width gain controlling spread of generalization.
        - age_gen_shift (real): Multiplicative shift on kernel width for older vs younger
                                (applied as factor (1 + age_gen_shift) for older and
                                 (1 - age_gen_shift) for younger).
        - decay (0..1): Per-trial forgetting toward zero for all Q-values.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, gen_gain, age_gen_shift, decay = model_parameters
    beta = max(1e-8, beta) * 10.0
    gen_gain = max(1e-8, gen_gain)
    decay = np.clip(decay, 0.0, 1.0)

    age_val = age[0]
    is_younger = 1.0 if age_val < 45 else 0.0
    is_older = 1.0 - is_younger
    # Width multiplier: older wider if age_gen_shift>0, younger narrower
    width_mult = (1.0 + age_gen_shift) * is_older + (1.0 - age_gen_shift) * is_younger
    width_mult = max(1e-8, width_mult)

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # Softmax policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            pol = np.exp(logits)
            pol = pol / (np.sum(pol) + eps)

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # TD error
            pe = r - Q[s, a]

            # Global forgetting
            Q *= (1.0 - decay)

            # Kernel width scales with load and age
            sigma = gen_gain * width_mult * (float(set_t) / 3.0)
            sigma = max(sigma, 1e-6)

            # Apply kernel-based update to neighbors in same block
            # Gaussian over state index distance; normalized to sum to 1
            dists = np.arange(nS) - s
            kern = np.exp(-0.5 * (dists / sigma) ** 2)
            kern = kern / (np.sum(kern) + eps)

            # Update only the chosen action's values across states
            Q[:, a] += alpha * pe * kern

        neg_log_like += -log_p

    return float(neg_log_like)