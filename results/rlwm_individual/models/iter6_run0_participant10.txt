def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with value forgetting and action stickiness, both modulated by set size and age.

    Idea:
    - Model-free Q-learning with per-trial value forgetting toward uniform for the current state.
    - A perseveration (stickiness) bias to repeat the last action taken in the same state.
    - Both forgetting and stickiness are modulated by working-memory load (set size) and age group:
      older adults forget more and rely less on stickiness; larger set sizes increase forgetting
      and reduce stickiness.

    Parameters (model_parameters):
    - alpha: float in [0,1], Q-learning rate for reward prediction error.
    - beta: float > 0, inverse temperature for softmax choice.
    - eta_forget: float in [0,1], base forgetting rate toward uniform for the current state.
    - kappa_stick: float, base stickiness strength added to the chosen action's logit if it
                   matches the previous action taken in the same state.
    - age_load_gain: float >= 0, scales the effect of age and load on forgetting and stickiness.

    Inputs:
    - states: array-like of ints, state id on each trial (0..set_size-1 within block).
    - actions: array-like of ints in {0,1,2}, chosen action per trial.
    - rewards: array-like of {0,1}, feedback per trial.
    - blocks: array-like of block ids; learning resets per block.
    - set_sizes: array-like of ints (e.g., 3 or 6), set size per trial/block (constant within a block).
    - age: array-like with a single element: participant age in years.
    - model_parameters: list/tuple [alpha, beta, eta_forget, kappa_stick, age_load_gain].

    Returns:
    - Negative log-likelihood (float) of the observed choices under the model.
    """
    alpha, beta, eta_forget, kappa_stick, age_load_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Track last action per state for stickiness; -1 means no previous action yet.
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            # Effective stickiness and forgetting under load and age
            load_scale = 3.0 / float(curr_set)  # 1 at set=3, 0.5 at set=6
            # Older -> less stickiness; higher load -> less stickiness
            kappa_eff = kappa_stick * load_scale * (1.0 - age_load_gain * is_older)
            # Older -> more forgetting; higher load -> more forgetting
            eta_eff = eta_forget * (1.0 / load_scale) * (1.0 + 0.5 * age_load_gain * is_older)
            eta_eff = min(max(eta_eff, 0.0), 1.0)

            # Softmax with stickiness bias to repeat last action in this state
            logits = beta * (Q[s, :] - np.max(Q[s, :]))
            if last_action[s] >= 0:
                logits[last_action[s]] += kappa_eff
            p = np.exp(logits)
            p /= np.sum(p)

            # Choice likelihood
            nll -= np.log(max(p[a], eps))

            # Q forgetting toward uniform for the current state
            Q[s, :] = (1.0 - eta_eff) * Q[s, :] + eta_eff * (1.0 / nA)

            # Q-learning update for chosen action
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Update last action for this state
            last_action[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-limited hypothesis-elimination (WM-like) decision-making with age- and load-modulated
    retention and lapses.

    Idea:
    - For each state, the agent maintains a belief distribution over the 3 possible correct actions.
    - Reward=1 sets belief to the chosen action (one-shot learning for deterministic rules).
    - Reward=0 eliminates the chosen action from the candidate set for that state.
    - Working-memory capacity/retention is limited: beliefs decay toward uniform with strength
      that depends on set size and age group (older adults and larger sets have weaker retention).
    - A lapse probability adds uniform noise to choices; lapse increases with age and set size.

    Parameters (model_parameters):
    - beta: float > 0, inverse temperature applied to the log-beliefs for choice.
    - phi_base: float in [0,1], baseline retention (higher -> stronger memory, less decay).
    - cap_gain: float >= 0, controls how much set size and age reduce retention.
    - lapse_base: float in [0,1), baseline lapse rate (random responding).
    - age_lapse_gain: float >= 0, increases lapse for older adults.

    Inputs:
    - states: array-like of ints, state id on each trial (0..set_size-1 within block).
    - actions: array-like of ints in {0,1,2}, chosen action per trial.
    - rewards: array-like of {0,1}, feedback per trial.
    - blocks: array-like of block ids; beliefs reset per block.
    - set_sizes: array-like of ints (e.g., 3 or 6), set size per trial/block (constant within a block).
    - age: array-like with a single element: participant age in years.
    - model_parameters: list/tuple [beta, phi_base, cap_gain, lapse_base, age_lapse_gain].

    Returns:
    - Negative log-likelihood (float) of the observed choices under the model.
    """
    beta, phi_base, cap_gain, lapse_base, age_lapse_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12
    H = np.log(nA)  # not used in equations; kept for potential diagnostics

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        # Belief over correct action per state
        B = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            load_scale = 3.0 / float(curr_set)  # 1.0 for set=3, 0.5 for set=6

            # Effective retention: reduced by load and age
            phi_eff = phi_base * load_scale * (1.0 - cap_gain * 0.5 * is_older)
            phi_eff = min(max(phi_eff, 0.0), 1.0)

            # Decay beliefs toward uniform before choice (capacity limitation)
            B[s, :] = phi_eff * B[s, :] + (1.0 - phi_eff) * (1.0 / nA)

            # Choice policy: softmax over log-beliefs with lapses
            b_s = B[s, :].copy()
            b_s = np.clip(b_s, eps, 1.0)
            logits = beta * (np.log(b_s) - np.max(np.log(b_s)))
            p_soft = np.exp(logits)
            p_soft /= np.sum(p_soft)

            # Lapse increases with age and load
            lapse = lapse_base + age_lapse_gain * is_older + (1.0 - load_scale) * 0.5 * lapse_base
            lapse = min(max(lapse, 0.0), 0.99)
            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)

            nll -= np.log(max(p[a], eps))

            # Update beliefs based on deterministic feedback
            if r >= 0.5:
                # Reward confirms correct action: set one-hot
                B[s, :] = 0.0
                B[s, a] = 1.0
            else:
                # Eliminate the chosen action for this state
                B[s, a] = 0.0
                sum_rest = np.sum(B[s, :])
                if sum_rest <= eps:
                    # If all eliminated (shouldn't happen often), reset to uniform
                    B[s, :] = (1.0 / nA)
                else:
                    B[s, :] /= sum_rest

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with optimistic exploration and adaptive inverse temperature modulated by load and age.

    Idea:
    - Model-free Q-learning drives values.
    - Optimistic exploration bonus applied more to rarely tried state-action pairs; this bonus
      is stronger under lower load and for younger adults, and decays as pairs are sampled.
    - The inverse temperature beta adapts online to difficulty and recent feedback:
      higher under low load and after reward, reduced under high load and for older adults.

    Parameters (model_parameters):
    - alpha: float in [0,1], Q-learning rate.
    - beta_base: float > 0, baseline inverse temperature.
    - beta_gain: float >= 0, scales modulation of beta by load and recent outcome.
    - age_explore_bias: float in [0,1], reduces both optimistic bonus and beta for older adults.
    - q_bonus: float >= 0, magnitude of optimistic bonus for rarely sampled pairs.

    Inputs:
    - states: array-like of ints, state id on each trial (0..set_size-1 within block).
    - actions: array-like of ints in {0,1,2}, chosen action per trial.
    - rewards: array-like of {0,1}, feedback per trial.
    - blocks: array-like of block ids; learning resets per block.
    - set_sizes: array-like of ints (e.g., 3 or 6), set size per trial/block (constant within a block).
    - age: array-like with a single element: participant age in years.
    - model_parameters: list/tuple [alpha, beta_base, beta_gain, age_explore_bias, q_bonus].

    Returns:
    - Negative log-likelihood (float) of the observed choices under the model.
    """
    alpha, beta_base, beta_gain, age_explore_bias, q_bonus = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts per state-action pair
        prev_r = 0.5  # neutral initial recent outcome

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            curr_set = int(block_set_sizes[t])

            load_scale = 3.0 / float(curr_set)  # 1 for set=3, 0.5 for set=6

            # Optimistic bonus decays with visits; reduced for older adults and higher load
            bonus_scale = load_scale * (1.0 - age_explore_bias * is_older)
            bonus = q_bonus * bonus_scale / (1.0 + N[s, :])

            # Adaptive beta: increase with low load and after reward; reduced for older adults
            beta_t = beta_base * (1.0 + beta_gain * (load_scale * (0.5 + 0.5 * prev_r)))
            beta_t *= (1.0 - 0.5 * age_explore_bias * is_older)
            beta_t = max(beta_t, eps)

            # Softmax over Q plus optimistic bonus
            q_eff = Q[s, :] + bonus
            logits = beta_t * (q_eff - np.max(q_eff))
            p = np.exp(logits)
            p /= np.sum(p)

            nll -= np.log(max(p[a], eps))

            # Update counts and prev reward
            N[s, a] += 1.0
            prev_r = r

            # Q-learning update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

    return nll