def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with reliability arbitration via RL volatility and WM sharpness scaling by set size.

    Core idea:
    - RL learns with a standard delta rule.
    - WM forms a sharp distribution over the most recent rewarded action, with decay toward uniform.
    - Arbitration: WM influence increases when RL is unreliable (high volatility/PE) and decreases with larger set size.
      WM sharpness controls how deterministic the WM policy is, and also reduces with larger set size.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            RL learning rate (0-1).
        wm_bias : float
            Base WM mixture bias (0-1), scaled by RL reliability and set size.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        rl_volatility : float
            Smoothing for running RL volatility estimate (0-1). Larger -> volatility tracks PE faster.
        wm_sharpness : float
            Controls WM determinism and resistance to decay. Higher -> sharper, more persistent WM.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_bias, softmax_beta, rl_volatility, wm_sharpness = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # upper cap for WM determinism; will be modulated
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Running RL volatility estimate per state
        vol = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax with set-size adjusted determinism
            # WM becomes less sharp as set size increases; scaled by wm_sharpness
            wm_beta_eff = softmax_beta_wm * wm_sharpness * (3.0 / max(1.0, nS))
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Arbitration: WM weight increases when RL volatility is high, decreases with set size
            # Compute a bounded reliability proxy from volatility
            rl_rel = 1.0 - np.tanh(vol[s])  # in (0,1], higher = more reliable
            size_scale = min(1.0, 3.0 / max(1.0, nS))
            eff_wm_weight = wm_bias * (1.0 - rl_rel) * size_scale

            # Mixture probability
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update RL volatility (EWMA of absolute PE)
            vol[s] = (1.0 - rl_volatility) * vol[s] + rl_volatility * abs(delta)

            # WM decay toward uniform; decay slows as wm_sharpness increases and set size decreases
            base_decay = 0.10
            size_penalty = (nS - 3) * 0.05 if nS > 3 else 0.0
            decay = min(0.5, max(0.0, base_decay + size_penalty) / max(1e-6, wm_sharpness))
            w = (1.0 - decay) * w + decay * w_0

            # WM reward-contingent imprint: stronger after reward
            alpha_pos = min(1.0, 0.75 * wm_sharpness)   # more sharpness -> stronger overwrite
            alpha_neg = min(0.5, 0.25 * wm_sharpness)
            if r > 0.0:
                w[s, :] = (1.0 - alpha_pos) * w[s, :]
                w[s, a] += alpha_pos
            else:
                w[s, :] = (1.0 - alpha_neg) * w[s, :]
                w[s, a] += alpha_neg

            # Normalize WM row to keep a valid distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with surprise-gated WM updates and set-sizeâ€“penalized gating.

    Core idea:
    - RL updates on every trial with softmax choice.
    - WM updates are probabilistically gated by surprise (|PE|), with a penalty for larger set sizes.
      When the gate opens, WM imprints the chosen action more strongly.
    - Arbitration: WM weight equals a base weight times the gating probability on that trial.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            RL learning rate (0-1).
        wm_base : float
            Base WM mixture weight (0-1), scaled by gating.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        surprise_sens : float
            Sensitivity of WM gate to surprise (|PE|). Higher -> more likely to gate on large PE.
        size_penalty : float
            Linear penalty on the gate for each extra item beyond 3 in set size.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, surprise_sens, size_penalty = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic baseline for WM (will still be softmaxed)
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen-action probability
            # Keep WM sharp but constant; mixture weight will be reduced by gating and size penalty
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL PE to determine surprise
            delta = r - Q_s[a]

            # Gate probability: logistic over surprise with set-size penalty
            # gate_prob in (0,1); reduces as set size grows
            size_term = size_penalty * max(0.0, (nS - 3))
            gate_input = surprise_sens * abs(delta) - size_term
            # Numerically stable logistic
            gate_prob = 1.0 / (1.0 + np.exp(-gate_input))

            # Effective WM weight is base * gate probability
            eff_wm_weight = wm_base * gate_prob

            # Mixture probability
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * delta

            # WM global decay increases with set size
            base_decay = 0.08
            decay = min(0.5, base_decay + 0.04 * max(0, (nS - 3)))
            w = (1.0 - decay) * w + decay * w_0

            # WM gated update: when gate is strong, imprint more
            # Use the probabilistic gate_prob as a continuous update strength
            alpha_wm = gate_prob  # bounded in (0,1)
            # Reward modulates the imprint slightly
            if r > 0.0:
                alpha_eff = min(1.0, alpha_wm * 1.0)
            else:
                alpha_eff = min(0.6, alpha_wm * 0.6)

            w[s, :] = (1.0 - alpha_eff) * w[s, :]
            w[s, a] += alpha_eff

            # Normalize WM state row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Dual-rate RL (fast + slow) combined into the RL policy, mixed with WM via confidence-weighted WM.

    Core idea:
    - Two RL systems run in parallel: fast (high learning rate) and slow (low learning rate).
      Their policies are blended based on their decisiveness (inverse entropy), controlled by an attention temperature.
    - WM stores recent associations with decay; WM influence increases when WM is confident (peaked distribution)
      and decreases with larger set size.
    - Final choice probability is a mixture of the blended RL policy and WM policy.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr_fast : float
            Learning rate for fast RL (0-1).
        lr_slow : float
            Learning rate for slow RL (0-1).
        wm_base : float
            Base WM mixture weight (0-1), further scaled by WM confidence and set size.
        softmax_beta : float
            Base inverse temperature for RL (scaled by 10 internally).
        att_temp : float
            Attention temperature controlling how strongly decisiveness (inverse entropy) biases toward fast vs slow.
        q_decay : float
            Per-trial decay of both Q tables toward uniform (0-0.2), modeling forgetting/interference.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr_fast, lr_slow, wm_base, softmax_beta, att_temp, q_decay = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic baseline for WM
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Dual Q tables
        q_fast = (1.0 / nA) * np.ones((nS, nA))
        q_slow = (1.0 / nA) * np.ones((nS, nA))

        # WM
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Qf_s = q_fast[s, :]
            Qs_s = q_slow[s, :]
            W_s = w[s, :]

            # Fast and slow RL chosen-action probabilities
            p_rl_fast = 1.0 / np.sum(np.exp(softmax_beta * (Qf_s - Qf_s[a])))
            p_rl_slow = 1.0 / np.sum(np.exp(softmax_beta * (Qs_s - Qs_s[a])))

            # Compute decisiveness via inverse entropy of each policy
            def softmax_probs(vec, beta):
                z = np.exp(beta * (vec - np.max(vec)))
                z_sum = np.sum(z)
                return z / z_sum if z_sum > 0 else np.ones_like(vec) / len(vec)

            pf = softmax_probs(Qf_s, softmax_beta)
            ps = softmax_probs(Qs_s, softmax_beta)

            def entropy(p):
                p_safe = np.clip(p, 1e-12, 1.0)
                return -np.sum(p_safe * np.log(p_safe))

            Hf = entropy(pf)
            Hs = entropy(ps)
            # Lower entropy -> higher decisiveness; convert to weights via softmax over negative entropy
            logits = np.array([-Hf, -Hs]) * att_temp
            logits = logits - np.max(logits)
            exps = np.exp(logits)
            att_w_fast = exps[0] / np.sum(exps)
            att_w_slow = 1.0 - att_w_fast

            # Blended RL chosen-action probability
            p_rl = att_w_fast * p_rl_fast + att_w_slow * p_rl_slow

            # WM chosen-action probability with set-size reduced sharpness
            wm_beta_eff = softmax_beta_wm * (3.0 / max(1.0, nS))
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # WM confidence via spread; stronger in small set size
            spread = np.max(W_s) - np.min(W_s)
            size_scale = min(1.0, 3.0 / max(1.0, nS))
            eff_wm_weight = wm_base * spread * size_scale

            # Mixture probability
            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL updates with decay toward uniform
            delta_fast = r - Qf_s[a]
            delta_slow = r - Qs_s[a]
            q_fast[s, a] += lr_fast * delta_fast
            q_slow[s, a] += lr_slow * delta_slow

            if q_decay > 0.0:
                q_uniform = 1.0 / nA
                q_fast = (1.0 - q_decay) * q_fast + q_decay * q_uniform
                q_slow = (1.0 - q_decay) * q_slow + q_decay * q_uniform

            # WM decay stronger as set size increases
            base_decay = 0.10
            decay = min(0.5, base_decay + 0.05 * max(0, (nS - 3)))
            w = (1.0 - decay) * w + decay * w_0

            # WM reward-based imprint
            if r > 0.0:
                alpha_pos = 0.9
                w[s, :] = (1.0 - alpha_pos) * w[s, :]
                w[s, a] += alpha_pos
            else:
                alpha_neg = 0.3
                w[s, :] = (1.0 - alpha_neg) * w[s, :]
                w[s, a] += alpha_neg

            # Normalize WM row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p