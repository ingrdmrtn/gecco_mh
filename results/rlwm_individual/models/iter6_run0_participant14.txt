def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with asymmetric RL learning, load-sensitive WM decay, and recency-weighted WM writing.

    Idea:
    - Model-free RL with separate learning rates for positive and negative prediction errors.
    - WM is a policy-like store that decays toward uniform faster under higher set sizes (load).
    - WM writes strongly to the currently visited state on reward, with a recency-weighted update.
    - Mixture weight of WM vs RL is attenuated under load.

    Parameters (model_parameters):
    - lr_pos: in [0,1], RL learning rate for positive PE (r - Q > 0).
    - lr_neg: in [0,1], RL learning rate for negative PE (r - Q < 0).
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_base: in [0,1], base mixture weight for WM in low-load blocks (set size ~3).
    - recency_lambda: >= 0, controls both WM global decay and how sharply WM writes on reward.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr_pos, lr_neg, softmax_beta, wm_base, recency_lambda = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-sensitive WM decay per trial (faster decay for larger nS)
        wm_decay = 1.0 - np.exp(-recency_lambda * max(1.0, nS) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        # Load attenuates effective WM mixture weight
        wm_weight_load = wm_base * np.exp(-(max(1.0, nS) - 3.0) / 6.0)

        # Convert recency_lambda to a bounded WM write strength
        wm_write = 1.0 - np.exp(-recency_lambda)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy (deterministic-softmax on the WM store)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with load-attenuated WM weight
            p_total = p_wm * wm_weight_load + (1 - wm_weight_load) * p_rl
            log_p += np.log(p_total + 1e-12)
      
            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM update: reward-driven focused write; non-reward leaves only global decay
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_write) * w[s, :] + wm_write * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + probabilistic WM encoding with load-sensitive interference.

    Idea:
    - Standard RL updates action values.
    - WM encodes the currently visited state-action on each trial with a probability
      that decreases with set size (load). We use the expected (non-sampling) update.
    - WM globally diffuses toward uniform with an interference rate that increases with load.
    - When WM has encoded a state recently, it exerts stronger influence on choice via
      a higher mixture weight; otherwise WM contribution is smaller.

    Parameters (model_parameters):
    - lr: in [0,1], RL learning rate.
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_strength: in [0,1], base WM influence on choice when an item is encoded.
    - encode_bias: real, baseline logit controlling WM encoding probability per visit.
    - interference_rate: >= 0, controls load-driven WM diffusion toward uniform.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_strength, encode_bias, interference_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-sensitive global interference/decay
        wm_decay = 1.0 - np.exp(-interference_rate * max(1.0, nS) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        # Small baseline WM influence when not encoded
        wm_min_influence = 0.1 * wm_strength

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global WM diffusion
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy from current WM store
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Expected encoding probability decreases with load (logistic function)
            # logit(p_enc) = encode_bias - (nS-3) term
            logit_p_enc = encode_bias - (max(1.0, nS) - 3.0)
            p_enc = 1.0 / (1.0 + np.exp(-logit_p_enc))

            # Mixture weight increases when WM is likely encoded for this state
            wm_weight = wm_min_influence + (wm_strength - wm_min_influence) * p_enc

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Expected WM update: apply with strength scaled by p_enc and reward
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - p_enc) * w[s, :] + p_enc * target
            else:
                # On errors, a mild drift back to uniform proportional to p_enc
                w[s, :] = (1.0 - 0.5 * p_enc) * w[s, :] + (0.5 * p_enc) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-based arbitration and load-dependent forgetting.

    Idea:
    - RL learns Q with a single learning rate.
    - WM stores a policy that is sharpened by reward and globally forgets faster under higher load.
    - Arbitration is dynamic: the mixture weight depends on the relative confidence of WM vs RL.
      Confidence is computed as the margin between the best action and the average for each system,
      scaled by system-specific precision and an arbitration temperature.

    Parameters (model_parameters):
    - lr: in [0,1], RL learning rate.
    - softmax_beta: >= 0, RL inverse temperature (internally scaled by 10).
    - wm_precision: >= 0, scales WM confidence and WM learning strength.
    - arb_temp: >= 0, arbitration temperature; higher makes gating more sensitive to confidence differences.
    - forget_base: >= 0, baseline WM forgetting rate, scaled up with set size.
    - wm_bias: real, bias term favoring WM (positive) or RL (negative) in the arbitration logit.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_precision, arb_temp, forget_base, wm_bias = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent forgetting
        wm_decay = 1.0 - np.exp(-forget_base * max(1.0, nS) / 3.0)
        wm_decay = np.clip(wm_decay, 0.0, 1.0)

        # WM learning strength from precision (bounded via 1 - exp transform)
        wm_learn = 1.0 - np.exp(-wm_precision)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global WM forgetting
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            Q_s = q[s,:]
            W_s = w[s,:]

            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute confidence margins
            rl_margin = np.max(Q_s) - np.mean(Q_s)
            wm_margin = np.max(W_s) - (1.0 / nA)

            # Arbitration weight via logistic on confidence difference
            logit_w = wm_bias + arb_temp * (wm_precision * wm_margin - rl_margin)
            wm_weight = 1.0 / (1.0 + np.exp(-logit_w))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-driven sharpening scaled by wm_precision
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # On non-reward, slight diffusion beyond the global forgetting
                w[s, :] = (1.0 - 0.25 * wm_learn) * w[s, :] + (0.25 * wm_learn) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p