def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with uncertainty-gated mixture and set-size dependent WM decay.

    Idea:
    - RL learns Q-values with a standard delta rule.
    - WM maintains a normalized action-probability vector per state that is updated when rewarded.
    - WM decays toward uniform faster for larger set sizes (higher load).
    - The mixture between WM and RL is not fixed: it is adaptively increased when RL is uncertain for the current state.
      Uncertainty is approximated by the spread of Q-values in the current state.
    
    Parameters (model_parameters):
    - lr: scalar (0,1). RL learning rate.
    - wm_base: scalar (0,1). Baseline mixture weight for WM vs RL.
    - softmax_beta: positive scalar. RL softmax inverse temperature (internally scaled x10).
    - mix_slope: positive scalar. Strength with which RL uncertainty gates WM weight.
    - wm_learn: scalar (0,1). Strength of WM encoding toward the rewarded action.

    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    lr, wm_base, softmax_beta, mix_slope, wm_learn = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay: faster decay for larger set sizes (3 -> weak, 6 -> stronger)
        # Here phi ranges approximately from ~0.15 (nS=3) to ~0.3 (nS=6)
        phi = np.clip(0.05 + 0.05 * (nS - 3), 0.0, 0.5)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL uncertainty for current state: smaller spread => more uncertain.
            q_spread = np.max(Q_s) - np.min(Q_s)
            uncert = 1.0 - q_spread  # in [approx 0,1], higher => more uncertain
            uncert = np.clip(uncert, 0.0, 1.0)

            # Adaptive mixture: push weight toward WM when RL is uncertain
            wm_w = 1.0 / (1.0 + np.exp(-(np.log(wm_base + eps) - np.log(1.0 - wm_base + eps) + mix_slope * (uncert - 0.5))))
            wm_w = np.clip(wm_w, 0.0, 1.0)

            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (load-dependent)
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM encoding: move probability mass toward chosen action when rewarded
            if r > 0.5:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with meta-learned mixture and load-modulated WM forgetting.

    Idea:
    - RL learns standard Q-values.
    - WM holds a state-specific action distribution updated by reward; it forgets toward uniform faster with higher load.
    - The mixture weight between WM and RL is not fixed; it is meta-learned online to favor the source that better predicts outcomes.
      Specifically, we maintain a latent logit z of wm_weight and update it via a delta rule using the advantage of WM over RL.

    Parameters (model_parameters):
    - lr: scalar (0,1). RL learning rate.
    - wm_weight0: scalar (0,1). Initial WM mixture weight at block start.
    - softmax_beta: positive scalar. RL softmax inverse temperature (internally scaled x10).
    - meta_lr: scalar (0,1). Meta-learning rate for adapting the WM mixture weight.
    - wm_forget: scalar (0,1). Base WM forgetting rate; effective forgetting increases with set size.

    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    lr, wm_weight0, softmax_beta, meta_lr, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-modulated forgetting: scale with nS
        phi = np.clip(wm_forget * (nS / 3.0), 0.0, 0.95)

        # Initialize mixture weight logit per block
        z = np.log(wm_weight0 + eps) - np.log(1.0 - wm_weight0 + eps)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM action probabilities for the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_w = 1.0 / (1.0 + np.exp(-z))
            p_total = wm_w * p_wm + (1.0 - wm_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM encode when rewarded
            if r > 0.5:
                # one-shot push toward chosen action
                gain = min(1.0, 0.5 + 0.5 * (3.0 / nS))  # slightly weaker at higher load
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain

            # Meta-learn the mixture weight using advantage signal
            # If WM predicted the chosen action better than RL, increase wm_w, else decrease.
            advantage = (p_wm - p_rl)
            z += meta_lr * advantage  # gradient-ascent on log-likelihood proxy

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL+WM with interference across states, load-dependent lapses, and reward-gated WM encoding.

    Idea:
    - RL learns Q-values; action selection uses RL softmax.
    - WM tracks action distributions per state; when rewarded, WM encodes to the active state and
      partially intrudes (interferes) into other states' WM representations (more realistic interference).
    - There is a load-dependent lapse probability that mixes in a uniform random choice, capturing attentional lapses under high load.
    - Final action probability is a mixture of WM and RL, followed by a lapse mixture.

    Parameters (model_parameters):
    - lr: scalar (0,1). RL learning rate.
    - wm_weight: scalar (0,1). Mixture weight between WM and RL.
    - softmax_beta: positive scalar. RL softmax inverse temperature (internally scaled x10).
    - wm_encode: scalar (0,1). Strength of WM encoding toward the rewarded action.
    - chi_interf: scalar (0,1). Degree of WM interference spread from the current state to other states upon reward.
    - lapse: scalar (0,1). Base lapse rate; effective lapse scales with set size.

    Returns:
    - Negative log-likelihood of the observed action sequence.
    """
    lr, wm_weight, softmax_beta, wm_encode, chi_interf, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Lapse increases with load
        lapse_eff = np.clip(lapse * (nS / 3.0), 0.0, 0.5)

        # Mild baseline WM decay that increases with load
        phi = np.clip(0.02 * (nS / 3.0), 0.0, 0.2)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM-RL mixture
            p_mix = np.clip(wm_weight, 0.0, 1.0) * p_wm + (1.0 - np.clip(wm_weight, 0.0, 1.0)) * p_rl

            # Lapse mixture with uniform policy
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform (load-dependent)
            w[s, :] = (1.0 - phi) * w[s, :] + phi * w_0[s, :]

            # WM encoding with interference when rewarded
            if r > 0.5:
                # Encode to current state
                w[s, :] = (1.0 - wm_encode) * w[s, :]
                w[s, a] += wm_encode
                # Interference: spread a fraction of the updated pattern to other states
                if nS > 1 and chi_interf > 0.0:
                    spread = chi_interf / max(1, nS - 1)
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        w[s2, :] = (1.0 - spread) * w[s2, :] + spread * w[s, :]

        blocks_log_p += log_p

    return -blocks_log_p