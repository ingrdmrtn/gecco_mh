def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity-limited reliance, probabilistic encoding, and decay.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM policy: fast softmax over a WM value table W.
    - Capacity-limited arbitration: the effective WM weight decreases with set size
      via a hyperbolic capacity function f(nS) = 1 / (1 + nS/C50).
      wm_eff = wm_weight_base * f(nS).
    - WM encoding: with probability p_enc, a rewarded item is encoded (moved toward one-hot).
      Otherwise WM at that state decays toward uniform at rate lambda_wm.
      On non-rewarded trials, the stateâ€™s WM also decays toward uniform.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight_base, softmax_beta, C50, lambda_wm, p_enc)
        - lr: RL learning rate in [0,1].
        - wm_weight_base: baseline reliance on WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - C50: capacity scale (>0) controlling how quickly WM reliance falls with set size.
        - lambda_wm: WM decay/update rate in [0,1] toward target (one-hot or uniform).
        - p_enc: probability of encoding into WM on rewarded trials in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, C50, lambda_wm, p_enc = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-limited effective WM weight (fixed within block)
        cap_factor = 1.0 / (1.0 + (nS / max(C50, 1e-6)))
        wm_eff_block = np.clip(wm_weight_base * cap_factor, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values (high precision)
            p_vec_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            # Mixture policy with capacity-limited WM reliance
            p_total = p_wm * wm_eff_block + (1 - wm_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: probabilistic encoding on reward, otherwise decay
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                # Encode with probability p_enc, else decay toward uniform
                if np.random.rand() < p_enc:
                    w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * one_hot
                else:
                    w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * w_0[s, :]
            else:
                # No reward: decay toward uniform
                w[s, :] = (1.0 - lambda_wm) * w[s, :] + lambda_wm * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and WM with cross-state interference.

    Mechanism
    - RL: two learning rates for positive/negative prediction errors.
    - WM policy: fast softmax over a WM table.
    - WM update:
        - On reward: update toward one-hot with rate wm_alpha.
        - On no reward: decay toward uniform with rate wm_alpha.
        - Interference: a fraction xi_interf of the WM update for the current state
          bleeds into all other states (uniformly), modeling confusion that grows
          more problematic with larger set sizes (since it spreads thinly but
          touches more states).
    - Arbitration: fixed wm_weight mixture (independent of set size), letting
      performance degrade under high load via interference rather than gating.

    Parameters
    ----------
    model_parameters : tuple
        (lr_pos, lr_neg, wm_weight, softmax_beta, xi_interf, wm_alpha)
        - lr_pos: RL learning rate for positive PE in [0,1].
        - lr_neg: RL learning rate for negative PE in [0,1].
        - wm_weight: mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - xi_interf: interference strength in [0,1], fraction of WM update that
          bleeds into other states.
        - wm_alpha: WM update/decay rate in [0,1].

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, xi_interf, wm_alpha = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_vec_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0 else lr_neg
            q[s][a] += lr * pe

            # WM update at current state
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w_update = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                w_update = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

            # Interference: part of the change spills over to other states
            delta_w = w_update - w[s, :]
            spill = xi_interf * delta_w
            retain = (1.0 - xi_interf) * delta_w

            # Apply to current state
            w[s, :] = w[s, :] + retain

            # Distribute spill equally to all other states
            if nS > 1:
                for s_other in range(nS):
                    if s_other == s:
                        continue
                    w[s_other, :] = w[s_other, :] + spill / (nS - 1)

            # Renormalize rows to sum to 1 to keep them as probability distributions
            row_sums = np.sum(w, axis=1, keepdims=True)
            row_sums[row_sums == 0] = 1.0
            w = w / row_sums

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-driven arbitration and load-based WM suppression.

    Mechanism
    - RL: delta-rule Q-learning with softmax; policy entropy reflects uncertainty.
    - WM: fast softmax over a WM table, updated quickly on reward and decaying otherwise.
    - Arbitration: the effective WM weight is modulated by
        wm_eff = sigmoid( logit(wm_weight_base * exp(-lambda_load*(nS-1))) + kappa_uncertainty * (H_rl - H_ref) )
      where H_rl is the entropy of the RL choice probabilities in the current state,
      and H_ref = log(nA) is maximal entropy. Thus, when RL is uncertain (high H_rl),
      reliance on WM increases relative to baseline; larger set sizes suppress WM via
      the exp(-lambda_load*(nS-1)) factor.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight_base, softmax_beta, kappa_uncertainty, lambda_load, wm_decay)
        - lr: RL learning rate in [0,1].
        - wm_weight_base: baseline WM weight in [0,1] before uncertainty/load modulation.
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - kappa_uncertainty: sensitivity of WM reliance to RL entropy (can be +/-).
        - lambda_load: load suppression strength (>=0) applied to set size nS.
        - wm_decay: WM update/decay rate in [0,1] toward target (one-hot on reward, uniform otherwise).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, kappa_uncertainty, lambda_load, wm_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute baseline WM weight adjusted for load
        eps = 1e-12
        base_load_factor = np.exp(-max(lambda_load, 0.0) * (nS - 1))
        wm_base_block = np.clip(wm_weight_base * base_load_factor, eps, 1 - eps)
        logit_base = np.log(wm_base_block) - np.log(1 - wm_base_block)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL softmax full distribution and chosen prob
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)
            p_rl = rl_probs[a]

            # RL entropy (uncertainty)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))
            H_ref = np.log(nA)

            # WM policy distribution
            W_s = w[s, :]
            p_vec_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            # Uncertainty-driven arbitration around load-suppressed baseline
            wm_eff = 1.0 / (1.0 + np.exp(-(logit_base + kappa_uncertainty * (H_rl - H_ref))))
            p_total = p_wm * wm_eff + (1 - wm_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: rewarded -> one-hot; unrewarded -> decay toward uniform
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * one_hot
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p