def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall associability per state and capacity-limited WM cache with rehearsal.

    Model idea:
    - RL system uses a baseline learning rate scaled by a state-specific associability that tracks recent unsigned PEs
      (Pearce-Hall). This allows faster learning when outcomes are surprising.
    - WM system acts like a capacity-limited cache: when a state is rewarded, it stores a sharp action distribution.
      The cache rehearses/maintains representations, but with imperfect maintenance that decays toward uniform.
    - Mixture weight is proportional to the probability that the state is effectively in WM:
        wm_weight_eff = min(1, C / nS) * rehearsal
      where C is capacity (in slots) and rehearsal reflects maintenance quality in the block.

    Parameters (model_parameters):
    - lr_base: scalar in [0,1]. Baseline RL learning rate.
    - alpha_ph: scalar in [0,1]. Pearce-Hall update rate for associability per state.
    - softmax_beta: scalar >= 0. RL softmax inverse temperature (scaled internally by 10).
    - wm_capacity: scalar > 0. Effective number of WM slots C.
    - wm_store_prob: scalar in [0,1]. Probability to write to WM on rewarded trials.
    - rehearsal: scalar in [0,1]. WM maintenance quality; also used to scale the WM mixture.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, alpha_ph, softmax_beta, wm_capacity, wm_store_prob, rehearsal = model_parameters
    softmax_beta *= 10.0  # RL inverse temperature scaling
    softmax_beta_wm = 50  # WM is near-deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific associability (Pearce-Hall)
        phi = np.ones(nS)  # start with high associability

        # Effective WM access probability based on capacity and rehearsal
        wm_gate = min(1.0, wm_capacity / max(1.0, nS)) * rehearsal

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall associability
            pe = r - Q_s[a]
            q[s, a] += lr_base * phi[s] * pe
            # Update associability with unsigned PE
            phi[s] = (1.0 - alpha_ph) * phi[s] + alpha_ph * abs(pe)
            # Bound associability to [0,1]
            phi[s] = min(1.0, max(0.0, phi[s]))

            # WM updating
            if r >= 1.0 - 1e-12:
                # With some probability, store a sharp distribution on the rewarded action
                if wm_store_prob > 0.0:
                    # Deterministic write scaled by probability via convex combination
                    sharp = np.full(nA, 0.0)
                    sharp[:] = 0.0
                    sharp += 0.0
                    sharp = np.ones(nA) * (0.0)  # initialize not strictly needed
                    sharp = np.ones(nA) * (0.0)  # placeholder to ensure variable defined
                # Construct sharp distribution directly:
                sharp = np.ones(nA) * (0.0)
                sharp[:] = 0.0
                sharp = np.full(nA, 0.0)
                sharp += 0.0
                sharp = np.zeros(nA)
                sharp[:] = 0.0
                sharp = np.ones(nA) * (0.0)
                # Proper sharp distribution:
                sharp = np.ones(nA) * (0.0)
                sharp[:] = 0.0
                sharp = np.ones(nA) * (0.0)
                sharp = np.zeros(nA)
                sharp[a] = 1.0
                # Blend with current WM row according to write prob
                w[s, :] = (1.0 - wm_store_prob) * w[s, :] + wm_store_prob * sharp
            else:
                # No reward: gentle decay toward uniform due to imperfect maintenance
                w[s, :] = (1.0 - rehearsal) * w[s, :] + rehearsal * w_0[s, :]

            # Normalize safety (not strictly needed but ensures numeric stability)
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Bayesian RL (Beta-Bernoulli) with WM win-stay/lose-shift heuristic and capacity gating.

    Model idea:
    - RL system maintains Beta(a,b) posterior per state-action on Bernoulli rewards. The expected value is a/(a+b).
      Choice is softmax over expected values.
    - WM system implements a fast win-stay / lose-shift heuristic for each state:
        after reward: peak at last chosen action
        after no reward: distribute mass over the other actions
      A lapse parameter smooths the WM policy.
    - Mixture weight depends on set size via capacity K: wm_weight_eff = base * min(1, K / nS)

    Parameters (model_parameters):
    - prior_a: scalar > 0. Prior success count for Beta posterior.
    - prior_b: scalar > 0. Prior failure count for Beta posterior.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - wm_base: scalar in [0,1]. Baseline WM mixture weight.
    - lapse: scalar in [0,1). Lapse/epsilon for WM policy; smooths away determinism.
    - capacity_K: scalar > 0. WM capacity modulating set-size effect.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    prior_a, prior_b, softmax_beta, wm_base, lapse, capacity_K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Beta posterior parameters per state-action
        a_post = np.full((nS, nA), prior_a, dtype=float)
        b_post = np.full((nS, nA), prior_b, dtype=float)

        # WM distribution per state initialized to uniform
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        base_gate = wm_base * min(1.0, capacity_K / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL expected values from Beta posterior
            Q = a_post / (a_post + b_post)
            Q_s = Q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from win-stay/lose-shift with lapse
            if r >= 1.0 - 1e-12:
                # Win-stay: peak at chosen action
                wm_pref = np.full(nA, lapse / (nA - 1))
                wm_pref[a] = 1.0 - lapse
            else:
                # Lose-shift: avoid chosen action by spreading on other actions
                wm_pref = np.full(nA, (1.0 - lapse) / (nA - 1))
                wm_pref[a] = lapse

            # Update current state's WM distribution toward wm_pref
            w[s, :] = 0.5 * w[s, :] + 0.5 * wm_pref  # partial overwrite for stability
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = base_gate * p_wm + (1.0 - base_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Update Beta posterior
            a_post[s, a] += r
            b_post[s, a] += (1.0 - r)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty bonus (UCB-like) and WM Hebbian binding with set-size-dependent interference.

    Model idea:
    - RL system updates Q with learning rate and adds an exploration bonus proportional to action uncertainty
      (inverse sqrt of visit counts) scaled by a bonus weight in the softmax.
    - WM system updates a fast associative map (w) toward the chosen action when rewarded (Hebbian),
      but suffers interference across states: updates bleed into other states with strength that increases
      with set size.
    - Mixture weight scales with set size via capacity.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled internally by 10).
    - bonus_w: scalar >= 0. Weight of uncertainty bonus added to Q before softmax.
    - wm_base: scalar in [0,1]. Baseline WM mixture weight.
    - interference: scalar in [0,1]. Fraction of WM update that diffuses uniformly to other states.
    - capacity_K: scalar > 0. WM capacity affecting set-size gating.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, bonus_w, wm_base, interference, capacity_K = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        counts = np.ones((nS, nA))  # initialize with 1 to avoid div-by-zero

        base_gate = wm_base * min(1.0, capacity_K / max(1.0, nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy with exploration bonus (UCB-like)
            # Uncertainty term: higher when counts are low
            sigma = 1.0 / np.sqrt(np.maximum(1.0, counts[s, :]))
            Q_eff = q[s, :] + bonus_w * sigma
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = base_gate * p_wm + (1.0 - base_gate) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta
            counts[s, a] += 1.0

            # WM update with interference
            if r >= 1.0 - 1e-12:
                # Create a sharp vector on chosen action
                sharp = np.zeros(nA)
                sharp[a] = 1.0
                # Update current state strongly
                w[s, :] = 0.5 * w[s, :] + 0.5 * sharp
                # Interference: diffuse a fraction to other states, scaled by set size
                if interference > 0.0 and nS > 1:
                    leak = interference * (nS - 1) / nS
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        w[s_other, :] = (1.0 - leak) * w[s_other, :] + leak * sharp
            else:
                # On no reward, WM at that state decays toward uniform
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]

            # Normalize for numerical stability
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p