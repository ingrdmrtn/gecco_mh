def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with success/error streak-gated arbitration (load-insensitive WM policy).

    Mechanism:
    - RL: Rescorla-Wagner learning of Q(s,a).
    - WM: Supervised mapping updated toward the rewarded action; small decay after errors.
    - Arbitration: The WM mixture weight is dynamically modulated by state-specific success and error streaks:
        * Success streaks increase reliance on WM (succ_boost).
        * Error streaks decrease reliance on WM (err_suppr).
      This captures confidence accumulation in WM vs. retreat to RL after repeated errors.

    Parameters (total 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline WM weight in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - err_suppr: Suppression strength on WM weight per consecutive error (>=0).
    - succ_boost: Enhancement strength on WM weight per consecutive success (>=0).
    - wm_learn: WM supervised learning rate toward one-hot after reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, err_suppr, succ_boost, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track per-state success and error streaks
        success_streak = np.zeros(nS, dtype=float)
        error_streak = np.zeros(nS, dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL softmax probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax probability of chosen action
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic arbitration: baseline + streak-based adjustment (logit space)
            eps = 1e-8
            base_logit = np.log(np.clip(wm_weight, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight, eps, 1 - eps))
            adj = succ_boost * success_streak[s] - err_suppr * error_streak[s]
            wm_weight_t = 1.0 / (1.0 + np.exp(-(base_logit + adj)))

            p_total = wm_weight_t * p_wm + (1 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            if r > 0.5:
                # Supervised push to one-hot of the chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
                success_streak[s] += 1.0
                error_streak[s] = 0.0
            else:
                # Mild decay back toward uniform after an error
                decay = 0.5 * wm_learn
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
                error_streak[s] += 1.0
                success_streak[s] = 0.0

            # Normalize and clip WM
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference-by-time and load modulation.

    Mechanism:
    - RL: Rescorla-Wagner learning of Q(s,a).
    - WM: Supervised mapping updated toward rewarded action.
    - Interference: Effective WM policy decays with the number of intervening trials since a state was last seen,
      scaled by set size (load). This produces worse WM reliance under higher load and longer lags.
    - Arbitration: Fixed baseline WM weight.

    Parameters (total 6):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline WM weight in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - interference_rate: Base interference rate per intervening trial (>=0).
    - load_factor: Scales interference with load: effective rate * (1 + load_factor * load), load=(nS-3)/3 in [0,1].
    - wm_learn: WM supervised learning rate toward one-hot after reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, interference_rate, load_factor, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last seen trial index per state (within block)
        last_seen = -1 * np.ones(nS, dtype=int)

        # Precompute load term in [0,1]
        load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply interference to the effective WM policy for the current state based on lag
            lag = 0 if last_seen[s] < 0 else (t - last_seen[s])
            eff_rate = interference_rate * (1.0 + load_factor * load_term)
            # Fraction decayed toward uniform after lag steps
            decay_frac = 1.0 - np.exp(-eff_rate * lag)
            # Effective WM distribution for policy computation (do not overwrite memory yet)
            W_s_base = w[s, :]
            W_s_eff = (1.0 - decay_frac) * W_s_base + decay_frac * w_0[s, :]

            Q_s = q[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability from decayed distribution
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s_eff - W_s_eff[a])))

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (supervised on reward; small passive decay on non-reward)
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # Passive small decay after non-reward
                decay = 0.25 * wm_learn
                w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

            # Update last seen
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (asymmetric learning rates) + WM with entropy-gated arbitration and load interaction.

    Mechanism:
    - RL: Rescorla-Wagner with separate learning rates for positive and negative PE.
    - WM: Probabilistic action map; updates toward one-hot after reward; decays toward uniform after non-reward.
    - Arbitration: WM weight is reduced as WM entropy increases (uncertain WM), with stronger reduction under higher load.

    Parameters (total 6):
    - lr_pos: RL learning rate for positive PE in [0,1].
    - lr_neg: RL learning rate for negative PE in [0,1].
    - wm_weight: Baseline WM weight in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - entropy_gate: Strength of entropy-based suppression of WM (>=0).
    - wm_decay: WM decay rate toward uniform on non-reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, entropy_gate, wm_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load term for gating
        load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy of WM (normalized to [0,1] by dividing by log nA)
            eps = 1e-12
            H = -np.sum(W_s * np.log(np.clip(W_s, eps, 1.0))) / np.log(nA)

            # Gate WM weight down by entropy and load
            base_logit = np.log(np.clip(wm_weight, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight, eps, 1 - eps))
            gate = entropy_gate * H * (1.0 + load_term)
            wm_weight_t = 1.0 / (1.0 + np.exp(-(base_logit - gate)))

            p_total = wm_weight_t * p_wm + (1 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr_use = lr_pos if pe >= 0 else lr_neg
            q[s, a] += lr_use * pe

            # WM update
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Use (1 - wm_decay) as a learn rate complement to ensure wm_decay is meaningful
                wm_learn = 1.0 - np.clip(wm_decay, 0.0, 1.0)
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p