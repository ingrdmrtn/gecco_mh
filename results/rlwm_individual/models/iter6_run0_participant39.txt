def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Pearce-Hall adaptive learning and WM with load-dependent decay.

    Mechanism
    - RL: Q-learning where the effective learning rate adapts via Pearce-Hall attention:
          alpha_t(s) = clip(lr_base + att_gain * |delta_prev(s)|, 0..1),
          where delta_prev(s) is the previous trial's unsigned prediction error for state s.
    - WM: item-specific store of rewarded actions with decay toward uniform. Decay increases with set size.
    - Mixture: WM and RL policies are mixed by wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr_base: base RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL in policy (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10 for numerical range.
        - att_gain: scaling (>=0) of attention gain from unsigned PE in Pearce-Hall (higher -> more adaptive).
        - wm_decay_load: base WM decay rate (0..1); effective decay scales with set size (stronger at nS=6).
        - wm_alpha_rew: WM encoding strength toward the chosen action on rewarded trials (0..1).

    Set-size effects
    ----------------
    - WM decay: wm_decay_eff = wm_decay_load * (nS / 6), causing stronger WM decay under higher load.
    """
    lr_base, wm_weight, softmax_beta, att_gain, wm_decay_load, wm_alpha_rew = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last unsigned PE per state for adaptive attention
        last_abs_pe = np.zeros(nS)

        # Load-dependent WM decay
        wm_decay_eff = np.clip(wm_decay_load * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with Pearce-Hall adaptive learning rate
            delta = r - Q_s[a]
            alpha_t = np.clip(lr_base + att_gain * last_abs_pe[s], 0.0, 1.0)
            q[s][a] += alpha_t * delta
            last_abs_pe[s] = abs(delta)

            # WM decay toward uniform (load-dependent)
            if wm_decay_eff > 0:
                w = (1 - wm_decay_eff) * w + wm_decay_eff * w_0

            # WM update: encode rewarded action
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_alpha_rew) * w[s] + wm_alpha_rew * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL plus a global choice-kernel as WM proxy with load-sensitive interference.

    Mechanism
    - RL: standard Q-learning with softmax policy.
    - WM: a global action preference kernel (state-independent) that captures recency/perseveration
          and interference across items; implemented by applying the same WM vector to all states.
          The kernel decays and is updated toward the last chosen action, with decay increasing under load.
    - Mixture: WM kernel policy and RL policy mixed by wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - ck_lr: learning rate for the choice kernel toward the chosen action (0..1).
        - ck_decay: base decay rate of the choice kernel toward uniform (0..1).
        - load_sens: multiplicative factor (>=0) scaling kernel decay with set size (higher -> more interference).

    Set-size effects
    ----------------
    - WM kernel decay: ck_decay_eff = clip(ck_decay * load_sens * (nS / 6), 0..1),
      so at higher load the global kernel is pulled more strongly toward uniform, reflecting interference.
    """
    lr, wm_weight, softmax_beta, ck_lr, ck_decay, load_sens = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # Initialize WM matrix, but treat it as a global kernel replicated across states
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        ck_decay_eff = np.clip(ck_decay * load_sens * (float(nS) / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]  # all rows identical; serves as global action kernel

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy based on global choice kernel
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Global kernel decay toward uniform (load-sensitive)
            if ck_decay_eff > 0:
                w = (1 - ck_decay_eff) * w + ck_decay_eff * w_0

            # Global kernel update toward chosen action across all states
            onehot = np.zeros((nS, nA))
            onehot[:, a] = 1.0
            w = (1 - ck_lr) * w + ck_lr * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RLâ€“WM arbitration: dynamic mixture based on uncertainty and load, with WM learning and decay.

    Mechanism
    - RL: standard Q-learning.
    - WM: item-specific store updated toward the chosen action when rewarded, with decay toward uniform.
    - Arbitration: mixture weight is computed dynamically from (i) RL uncertainty (entropy of RL policy),
      (ii) WM reliability (margin between best WM action and average), and (iii) set size (load penalty).
      Base bias comes from wm_weight.

      wm_weight_t = sigmoid( k * [ wm_weight - load_penalty + (wm_rel - rl_uncert) ] )
      where load_penalty = arb_load * (nS - 3) / 3, rl_uncert in [0,1], wm_rel in [0,1], and k=5.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: base bias for arbitration toward WM (can be negative or positive).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_alpha: WM learning rate toward one-hot on rewarded trials (0..1).
        - wm_decay: WM decay toward uniform (0..1); effective decay scales with set size.
        - arb_load: load penalty coefficient (>=0) that reduces WM reliance as set size increases.

    Set-size effects
    ----------------
    - WM decay: wm_decay_eff = clip(wm_decay * (nS / 6), 0..1).
    - Arbitration load penalty: load_penalty = arb_load * (nS - 3) / 3 decreases WM reliance at nS=6.
    """
    lr, wm_weight, softmax_beta, wm_alpha, wm_decay, arb_load = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        wm_decay_eff = np.clip(wm_decay * (float(nS) / 6.0), 0.0, 1.0)
        load_penalty = arb_load * (float(nS) - 3.0) / 3.0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL policy distribution for uncertainty computation
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_exp = np.exp(rl_logits)
            rl_pi = rl_exp / np.sum(rl_exp)
            # Normalized entropy in [0,1]
            rl_uncert = -np.sum(rl_pi * (np.log(rl_pi + 1e-12))) / np.log(nA)

            # WM policy and reliability
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            wm_best = np.max(W_s)
            wm_rel = np.clip((wm_best - np.mean(W_s)) / max(1e-12, (1 - 1.0 / nA)), 0.0, 1.0)

            # Dynamic arbitration weight via sigmoid; k=5 sharpens sensitivity
            k = 5.0
            mix_input = wm_weight - load_penalty + (wm_rel - rl_uncert)
            wm_mix = 1.0 / (1.0 + np.exp(-k * mix_input))

            # Mixture of policies
            p_total = wm_mix * p_wm + (1 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            if wm_decay_eff > 0:
                w = (1 - wm_decay_eff) * w + wm_decay_eff * w_0

            # WM update: reward-dependent strengthening; slight anti-learning otherwise
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_alpha) * w[s] + wm_alpha * onehot
            else:
                # move slightly toward uniform when not rewarded
                anti = 0.5 * wm_alpha
                w[s] = (1 - anti) * w[s] + anti * w_0[s]

        blocks_log_p += log_p

    return -blocks_log_p