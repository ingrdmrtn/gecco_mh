def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration with age- and set-size-modulated WM gating, WM noise, and lapses.

    Mechanism:
    - RL: Q-learning over state-action values.
    - WM: a simple associative store per state for the last rewarded action with confidence.
    - An arbitration weight controls how strongly WM vs. RL drives choice. This weight increases
      with better gating but decreases with larger set size and in older adults.
    - WM confidence decays as a function of a noise parameter that grows with set size and age.
    - A lapse component (modulated by age) adds uniform random choice.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block indices per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age (age[0] is used). Age >=45 treated as older.
    model_parameters : tuple/list of floats
        (lr, beta, gate_base, wm_noise, lapse_age)
        - lr: RL learning rate for Q-values.
        - beta: Inverse temperature for RL softmax.
        - gate_base: Baseline WM gating propensity (higher -> more WM reliance).
        - wm_noise: Baseline WM noise/decay controlling confidence loss.
        - lapse_age: Baseline lapse rate that is amplified in older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, beta, gate_base, wm_noise, lapse_age = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_sizes = set_sizes[idx]

        nS = int(block_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: last rewarded action and a confidence weight per state
        wm_action = -1 * np.ones(nS, dtype=int)
        wm_conf = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_sizes[t])

            # Effective WM noise/decay rises with set size and in older adults
            noise_eff = wm_noise * (ss / 3.0) * (1.0 + 0.5 * is_older)
            noise_eff = np.clip(noise_eff, 0.0, 1.0)

            # Decay WM confidence each trial
            wm_conf[s] = (1.0 - noise_eff) * wm_conf[s]

            # RL policy
            q_s = Q[s, :]
            pref = beta * (q_s - np.max(q_s))
            pi_rl = np.exp(pref)
            pi_rl = pi_rl / (np.sum(pi_rl) + eps)

            # WM policy: if we have a stored action, use a peaked distribution
            pi_wm = np.ones(nA) / nA
            if wm_action[s] >= 0:
                peaked = np.full(nA, eps)
                peaked[wm_action[s]] = 1.0
                peaked = peaked / np.sum(peaked)
                # Blend peaked target with uniform according to current confidence
                pi_wm = wm_conf[s] * peaked + (1.0 - wm_conf[s]) * (np.ones(nA) / nA)

            # Arbitration weight: logistic transform of gate_base with penalties
            # Larger sets and older age reduce WM reliance.
            gate_lin = gate_base - 0.8 * (ss / 3.0 - 1.0) - 0.6 * is_older
            w_wm = 1.0 / (1.0 + np.exp(-gate_lin))
            w_wm = np.clip(w_wm, 0.0, 1.0)

            # Lapse increases with age
            lapse = np.clip(lapse_age * (1.0 + 0.5 * is_older), 0.0, 0.5)

            # Total policy
            pi = (1.0 - lapse) * (w_wm * pi_wm + (1.0 - w_wm) * pi_rl) + lapse * (np.ones(nA) / nA)
            total_loglik += np.log(pi[a] + eps)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + lr * pe

            # WM update: on reward, store the action with confidence reset to 1; on no reward, confidence decays
            if r > 0.0:
                wm_action[s] = a
                wm_conf[s] = 1.0
            else:
                wm_conf[s] = (1.0 - noise_eff) * wm_conf[s]

    return -float(total_loglik)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and set-size-modulated exploration and state-wise perseveration.

    Mechanism:
    - Q-learning with an effective inverse temperature that decreases with set size and in older adults.
    - A state-wise perseveration bias that favors repeating the last chosen action in that state;
      this bias grows with set size and in older adults.
    - Learning rate is reduced in older adults, especially under higher set sizes.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age (age[0] used). Age >=45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, beta_base, explore_boost, persev0, age_weight)
        - alpha: base RL learning rate.
        - beta_base: base inverse temperature.
        - explore_boost: strength of exploration increase with load/age (reduces effective beta).
        - persev0: base perseveration strength (bias to repeat last action in a state).
        - age_weight: scales how strongly age reduces learning rate and increases perseveration.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta_base, explore_boost, persev0, age_weight = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_sizes = set_sizes[idx]

        nS = int(block_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_act = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_sizes[t])

            # Effective beta: exploration increases as set size and age increase
            beta_eff = beta_base * np.exp(-explore_boost * ((ss / 3.0 - 1.0) + 0.7 * is_older))
            beta_eff = max(beta_eff, 1e-6)

            # Perseveration bias: stronger with larger set size and in older adults
            pers_strength = persev0 * (1.0 + 0.5 * (ss / 3.0 - 1.0) + age_weight * 0.5 * is_older)
            bias_vec = np.zeros(nA)
            if last_act[s] >= 0:
                bias_vec[last_act[s]] = pers_strength

            # Softmax with bias
            q_s = Q[s, :]
            prefs = beta_eff * q_s + bias_vec
            prefs -= np.max(prefs)
            pi = np.exp(prefs)
            pi = pi / (np.sum(pi) + eps)

            total_loglik += np.log(pi[a] + eps)

            # Learning rate is reduced in older adults and under higher set size
            alpha_eff = alpha * (1.0 - 0.4 * age_weight * is_older) * (1.0 - 0.2 * (ss / 3.0 - 1.0))
            alpha_eff = np.clip(alpha_eff, 0.0, 1.0)

            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha_eff * pe

            last_act[s] = a

    return -float(total_loglik)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Capacity-weighted RL with focus modulation and age-related default bias.

    Mechanism:
    - Q-learning where both choice and learning are weighted by a "focus" factor representing
      whether the current state falls within a limited-capacity focus of control.
    - Focus decreases with set size and in older adults; switching between states incurs a
      multiplicative penalty on focus.
    - When focus is low, choices gravitate toward a uniform (default) policy. Older adults have
      an added default bias toward action 0.

    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..set_size-1).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int/float
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (e.g., 3 or 6).
    age : array-like of float
        Participant age (age[0] used). Age >=45 treated as older.
    model_parameters : tuple/list of floats
        (alpha, beta, cap_prop, switch_pen, bias0)
        - alpha: RL learning rate (scaled by focus).
        - beta: Inverse temperature for focused softmax.
        - cap_prop: Baseline capacity proportion for focus (0..1).
        - switch_pen: Penalty on focus when the state changes from previous trial.
        - bias0: Default bias toward action 0 that is expressed more when unfocused and in older adults.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, cap_prop, switch_pen, bias0 = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    eps = 1e-12
    total_loglik = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_sizes = set_sizes[idx]

        nS = int(block_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        prev_state = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_sizes[t])

            # Base focus declines with load and in older adults
            base_focus = cap_prop * (3.0 / ss) * (1.0 - 0.4 * is_older)
            base_focus = np.clip(base_focus, 0.0, 1.0)

            # Switching penalty if state changes
            if prev_state is None:
                focus = base_focus
            else:
                focus = base_focus * np.exp(-switch_pen * (1.0 if s != prev_state else 0.0))
            focus = np.clip(focus, 0.0, 1.0)

            # Focused softmax policy
            q_s = Q[s, :]
            prefs = beta * q_s
            prefs -= np.max(prefs)
            pi_focus = np.exp(prefs)
            pi_focus = pi_focus / (np.sum(pi_focus) + eps)

            # Unfocused default policy with age-related bias toward action 0
            default = np.ones(nA) / nA
            if bias0 != 0.0:
                # Add a small logit bias toward action 0 that is stronger in older adults
                bias_vec = np.zeros(nA)
                bias_vec[0] = bias0 * (1.0 + 0.5 * is_older)
                logits_def = bias_vec - np.max(bias_vec)
                pi_default = np.exp(logits_def)
                pi_default = pi_default / (np.sum(pi_default) + eps)
            else:
                pi_default = default

            # Mixture by focus
            pi = focus * pi_focus + (1.0 - focus) * pi_default
            total_loglik += np.log(pi[a] + eps)

            # Learning scaled by focus
            alpha_eff = np.clip(alpha * focus, 0.0, 1.0)
            pe = r - Q[s, a]
            Q[s, a] = Q[s, a] + alpha_eff * pe

            prev_state = s

    return -float(total_loglik)