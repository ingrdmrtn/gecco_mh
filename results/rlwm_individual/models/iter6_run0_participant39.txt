def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with rehearsal and asymmetric RL learning.

    Mechanisms
    - RL: delta-rule with separate learning rates for positive vs negative outcomes.
    - WM: capacity-limited cache of state-action mappings. If a state is in cache,
      WM policy is precise; otherwise WM is diffuse (uniform prior).
      Cache admits mappings only when rewarded (store the chosen action).
    - Rehearsal: between visits, cached WM traces decay toward uniform unless
      they are probabilistically rehearsed, reducing decay.
    - Arbitration: mixture of RL and WM policies; WM contributes only if the state
      is currently in cache, and with a base weight wm_weight0. Larger set sizes
      reduce the chance that a state is in cache (due to limited capacity).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr_pos : RL learning rate for rewarded outcomes in [0,1].
        lr_neg : RL learning rate for unrewarded outcomes in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        wm_weight0 : Base mixture weight for WM contribution when the state is in cache [0,1].
        capacity_K : WM capacity (real-valued; internally rounded/clipped to [1,nS]).
        rehearsal_rate : Probability that a cached memory is rehearsed (0..1); higher reduces decay.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight0, capacity_K, rehearsal_rate = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # base deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Capacity-limited cache of states; FIFO replacement policy
        K = int(np.clip(np.round(capacity_K), 1, nS))
        cache = []  # list of states currently in WM cache, most-recent at end

        # Track last-visited time for decay dynamics
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Rehearsal-based decay: for states not seen at this trial, approximate
            # decay since last seen; if rehearsed, reduce decay.
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
            else:
                dt = 1

            # Apply decay toward uniform for this state's WM trace if it's in cache
            if s in cache and dt > 0:
                # Effective retention per step with rehearsal: retain = rehearsal + (1-rehearsal)*0.5
                # So decay factor per step = (1 - retain)
                retain_per_step = rehearsal_rate + (1 - rehearsal_rate) * 0.5
                retain_per_step = np.clip(retain_per_step, 0.0, 1.0)
                retain = retain_per_step ** dt
                w[s, :] = retain * w[s, :] + (1 - retain) * w_0[s, :]

            last_seen[s] = t

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: precise if in cache, otherwise diffuse (uniform prior)
            in_cache = 1.0 if (s in cache) else 0.0
            beta_wm_eff = softmax_beta_wm if in_cache > 0.5 else 0.0
            if beta_wm_eff > 0:
                p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            else:
                # Uniform policy if not in cache
                p_wm = 1.0 / nA

            # Arbitration: WM contributes only when state is in cache
            p_total = (wm_weight0 * in_cache) * p_wm + (1.0 - wm_weight0 * in_cache) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s][a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM update rule: rewarded outcomes admit to cache and store one-hot;
            # unrewarded outcomes push toward uniform (erode wrong association).
            if r > 0.5:
                # Store/update WM trace as one-hot for chosen action
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot

                # Maintain cache capacity with FIFO replacement
                if s in cache:
                    # move s to most recent position
                    cache = [c for c in cache if c != s] + [s]
                else:
                    if len(cache) >= K:
                        # evict oldest
                        cache = cache[1:] + [s]
                    else:
                        cache.append(s)
            else:
                # Negative feedback: erode any existing bias for the chosen action
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + graded WM with uncertainty-based arbitration via entropy comparison.

    Mechanisms
    - RL: standard delta rule with softmax.
    - WM: graded value store updated by a learning rate wm_eta toward the target
      distribution (one-hot if rewarded; diffuse away from chosen if not).
    - Arbitration: trial-wise weight determined by relative uncertainty (entropy)
      of RL vs WM policies: higher weight on the lower-entropy (more certain)
      controller. A soft gain and temperature modulate sensitivity.
    - WM precision: base precision wm_beta0 for softmax; not fixed at 50.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        wm_eta : WM learning rate in [0,1] for value updates.
        wm_beta0 : Base WM inverse temperature (scales the template's WM precision 50).
        arb_gain : Slope for converting entropy difference to WM weight.
        entropy_temp : Temperature scaling for entropy difference before sigmoid (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_eta, wm_beta0, arb_gain, entropy_temp = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # base deterministic WM policy; will be scaled by wm_beta0

    entropy_temp = max(entropy_temp, 1e-6)  # avoid division by zero
    beta_wm_eff_scale = max(wm_beta0, 0.0)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with effective precision
            beta_wm_eff = softmax_beta_wm * beta_wm_eff_scale
            if beta_wm_eff > 0:
                p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            else:
                p_wm = 1.0 / nA

            # Build full action distributions for entropy computation
            # Numerically stable softmax for RL
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)

            # WM softmax
            if beta_wm_eff > 0:
                wm_logits = beta_wm_eff * (W_s - np.max(W_s))
                wm_probs = np.exp(wm_logits)
                wm_probs = wm_probs / np.sum(wm_probs)
            else:
                wm_probs = np.ones(nA) / nA

            # Entropies (natural log)
            H_rl = -np.sum(np.clip(rl_probs, 1e-12, 1.0) * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            H_wm = -np.sum(np.clip(wm_probs, 1e-12, 1.0) * np.log(np.clip(wm_probs, 1e-12, 1.0)))

            # Arbitration weight favoring the more certain (lower entropy) system
            x = (H_rl - H_wm) / entropy_temp
            wm_weight = 1.0 / (1.0 + np.exp(-arb_gain * x))

            # Mixture policy likelihood of chosen action
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update toward target distribution:
            # If reward: target is one-hot on chosen action.
            # If no reward: move away from chosen action toward uniform.
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                # Diffuse: reduce probability on chosen, increase others equally
                target = w_0[s, :].copy()
            w[s, :] = w[s, :] + wm_eta * (target - w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-dependent RL forgetting and WM decay in precision.

    Mechanisms
    - RL: delta-rule with per-visit forgetting toward uniform that grows with set size.
    - WM: fast overwriting toward one-hot on rewarded trials; otherwise partial decay.
      WM softmax precision declines with set size.
    - Arbitration: fixed WM mixture weight (wm_weight0) across trials within a block.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        softmax_beta0 : Base RL inverse temperature (rescaled internally by *10).
        wm_weight0 : Fixed WM mixture weight in [0,1].
        rl_forget : Base RL forgetting rate per visit in [0,1]; scaled by load.
        wm_decay_base : Base WM decay toward uniform after non-reward in [0,1].
        beta_load_gain : Gain > 0 controlling how WM precision declines with set size.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta0, wm_weight0, rl_forget, wm_decay_base, beta_load_gain = model_parameters
    softmax_beta = softmax_beta0 * 10  # higher upper bound as in template
    softmax_beta_wm = 50  # base deterministic WM policy

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute load-dependent factors
        # Scale forgetting with load (relative to minimal load 3)
        load_factor = max(0, nS - 3)
        rl_forget_eff = np.clip(rl_forget * (1 + 0.5 * load_factor), 0.0, 1.0)

        # WM precision declines exponentially with set size
        beta_wm_eff = softmax_beta_wm * np.exp(-beta_load_gain * load_factor)
        beta_wm_eff = max(0.0, beta_wm_eff)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL likelihood of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM likelihood of chosen action
            if beta_wm_eff > 0:
                p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            else:
                p_wm = 1.0 / nA

            # Mixture
            p_total = wm_weight0 * p_wm + (1.0 - wm_weight0) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL forgetting toward uniform on the visited state
            q[s, :] = (1.0 - rl_forget_eff) * q[s, :] + rl_forget_eff * (1.0 / nA)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: reward consolidates one-hot; non-reward decays toward uniform
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
            else:
                w[s, :] = (1.0 - wm_decay_base) * w[s, :] + wm_decay_base * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p