def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + entropy-gated WM mixture with load scaling and decay.

    Mechanism
    - RL: Rescorla–Wagner update (provided in template).
    - WM store: For each state, WM stores a near one-hot preference for the
      rewarded action, with precision controlled by wm_precision. WM decays
      toward uniform at rate wm_decay.
    - Arbitration: Trial-wise WM weight is the product of
        (a) a base weight (wm_weight),
        (b) a load factor (3 / nS), and
        (c) an entropy gate that increases WM reliance when RL is confident
            (low entropy), and decreases it when RL is uncertain (high entropy).
      This captures that under higher load (nS=6) and/or higher uncertainty,
      the system relies less on WM.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr            : RL learning rate in (0,1].
        wm_weight     : Base mixture weight for WM in [0,1].
        softmax_beta  : RL inverse temperature; scaled by *10 internally.
        wm_decay      : Per-trial WM decay toward uniform in [0,1].
        entropy_gain  : Sensitivity of arbitration to RL entropy (>=0).
        wm_precision  : Strength of WM encoding toward one-hot (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, entropy_gain, wm_precision = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    # Ensure parameters are within reasonable numeric ranges
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    entropy_gain = max(0.0, entropy_gain)
    wm_precision = max(1e-3, wm_precision)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load factor reduces WM influence when set size grows
        load_factor = 3.0 / max(1.0, nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability (from template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy of RL policy (approx via softmax with current beta)
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs /= np.sum(rl_probs)
            entropy_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))

            # Entropy-gated dynamic WM weight
            # Gate uses a logistic transform of negative entropy: lower entropy => larger gate
            # gate = sigmoid(entropy_gain * (H0 - entropy)), with H0 ~ log(nA)
            H0 = np.log(nA)
            gate = 1.0 / (1.0 + np.exp(-entropy_gain * (H0 - entropy_rl)))
            wm_weight_dyn = np.clip(wm_weight * load_factor * gate, 0.0, 1.0)

            # Mixture policy (from template)
            p_total = p_wm * wm_weight_dyn + (1.0 - wm_weight_dyn) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update (template)
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM encoding: on reward, move strongly toward one-hot with precision
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                eta = 1.0 - np.exp(-wm_precision)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # On no reward, slightly flatten distribution (increase uncertainty)
                flatten = 0.25 * wm_decay
                w[s, :] = (1.0 - flatten) * w[s, :] + flatten * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + statewise WM success-tracking arbitration with forgetting.

    Mechanism
    - RL: Rescorla–Wagner update (provided).
    - WM: stores a near one-hot mapping for rewarded state-action; subject to
      forgetting (wm_forget) toward uniform each trial.
    - Arbitration: Maintains a per-state WM-reliability trace m[s] that
      estimates whether WM is helpful in that state. m[s] increases when reward
      arrives (successful encoding) and decays otherwise. Mixture weight equals
      wm_weight * m[s] scaled by load (3/nS).

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr           : RL learning rate in (0,1].
        wm_weight    : Base WM mixture weight in [0,1].
        softmax_beta : RL inverse temperature; scaled by *10 internally.
        wm_alpha     : Learning rate for WM reliability trace m[s] in (0,1].
        wm_forget    : Forgetting rate of WM store toward uniform in [0,1].
        bias_low     : Baseline reliability boost in low information trials (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, wm_forget, bias_low = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    wm_alpha = np.clip(wm_alpha, 0.0, 1.0)
    wm_forget = np.clip(wm_forget, 0.0, 1.0)
    bias_low = max(0.0, bias_low)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Per-state WM reliability trace
        m = np.zeros(nS)

        load_factor = 3.0 / max(1.0, nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Dynamic WM weight from per-state reliability trace
            # Add a small baseline bias_low that helps early trials when RL is uninformative
            wm_weight_dyn = np.clip(wm_weight * load_factor * np.tanh(bias_low + m[s]), 0.0, 1.0)

            p_total = p_wm * wm_weight_dyn + (1.0 - wm_weight_dyn) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM forgetting toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM encoding on reward: push toward one-hot; on no reward, slight flatten
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * one_hot
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Update WM reliability trace m[s]
            # Target signal: 1 on reward, 0 otherwise
            m[s] += wm_alpha * ((1.0 if r > 0 else 0.0) - m[s])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM novelty/error map with load-dependent arbitration.

    Mechanism
    - RL: Rescorla–Wagner update (provided).
    - WM: tracks a fast error map per state that penalizes recently failed
      actions and slightly boosts rewarded ones. This acts like a short-term
      "avoid-known-wrong" controller, useful especially at small set sizes.
    - Arbitration: Load-dependent sigmoid mapping from set size to WM weight:
      more WM under small sets, less under large sets.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr            : RL learning rate in (0,1].
        wm_weight     : Maximum WM mixture weight in [0,1].
        softmax_beta  : RL inverse temperature; scaled by *10 internally.
        load_mid      : Set-size midpoint where WM weight is half-max (>0).
        load_steep    : Steepness of the load-to-weight sigmoid (>0).
        error_boost   : Strength of WM error penalization (>0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, load_mid, load_steep, error_boost = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    eps = 1e-12

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    load_mid = max(1e-3, load_mid)
    load_steep = max(1e-3, load_steep)
    error_boost = max(1e-3, error_boost)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM here is an error map: lower values for recently failed actions
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM weight via sigmoid on set size
        # Higher set size => smaller WM weight
        load_input = (nS - load_mid) / max(1e-6, load_mid)
        load_gate = 1.0 / (1.0 + np.exp(load_steep * load_input))
        wm_weight_by_load = np.clip(wm_weight * load_gate, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over W_s. Since W encodes "avoid errors", we
            # keep it as a preference vector (higher is better).
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture with load-dependent WM weight
            wm_weight_dyn = wm_weight_by_load
            p_total = p_wm * wm_weight_dyn + (1.0 - wm_weight_dyn) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update as an error/novelty map:
            # - On error: penalize chosen action and redistribute mass to others.
            # - On reward: modest boost to chosen action (since it's likely correct).
            # - Apply small decay toward uniform to prevent saturation.
            decay = 0.05
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            if r > 0:
                boost = 0.2
                w[s, a] = w[s, a] + boost * (1.0 - w[s, a])
                # normalize row to keep bounded
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
            else:
                penalty = np.clip(error_boost, 0.0, 5.0)
                # move probability mass away from the failed action
                drop = penalty * w[s, a]
                w[s, a] = max(0.0, w[s, a] - drop)
                # redistribute equally to other actions
                redistribute = drop / (nA - 1.0)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute
                # renormalize to avoid drift
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p