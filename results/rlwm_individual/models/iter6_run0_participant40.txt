def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Asymmetric RL + WM with set-size-dependent interference and same-state refresh.

    Mechanism
    - RL: delta-rule with asymmetric learning rates for positive vs. negative prediction errors.
    - WM: a fast, quasi-deterministic store that encodes rewarded S->A mappings; it
      suffers from cross-state interference that grows with set size (nS).
      If the same state repeats on the next trial, WM is refreshed (sharpened).
    - Arbitration: fixed WM weight mixed with RL policy. WM policy is softmax with very
      high inverse temperature (near-deterministic).

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr_pos : float
            RL learning rate for positive prediction errors (0-1).
        lr_neg : float
            RL learning rate for negative prediction errors (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_interference : float
            Amount by which encoding in one state interferes with other states,
            scaled by 1/nS (0-1).
        wm_refresh : float
            Extra sharpening of WM on immediate repetitions of the same state (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_interference, wm_refresh = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        prev_state = -1
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            logits_Q = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_Q - np.max(logits_Q))
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy
            W_s = w[s, :]
            logits_W = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_W - np.max(logits_W))
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Arbitration (fixed wm_weight)
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update (asymmetric learning rates)
            delta = r - q[s][a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM baseline decay toward uniform each trial
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # WM encoding on reward
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Encode strongly in current state
                w[s, :] = 0.7 * w[s, :] + 0.3 * target

                # Cross-state interference that grows with set size
                if nS > 1:
                    interf = (wm_interference / float(nS))
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        # Interference pushes other states slightly toward the encoded action
                        w[s_other, :] = (1 - interf) * w[s_other, :] + interf * ((1 - 1.0/nA) * w_0[s_other, :] + (1.0/nA) * np.eye(nA)[a])

            # Same-state immediate repetition refresh (sharpen existing WM)
            if prev_state == s and wm_refresh > 0:
                # Sharpen distribution by moving toward its own argmax (without hard one-hot)
                a_star = int(np.argmax(w[s, :]))
                sharpen = np.zeros(nA); sharpen[a_star] = 1.0
                w[s, :] = (1 - wm_refresh) * w[s, :] + wm_refresh * sharpen

            prev_state = s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-weighted WM arbitration and WM leak.

    Mechanism
    - RL: standard delta-rule learning with fixed inverse temperature.
    - WM: encodes rewarded actions; leaks toward uniform each trial.
    - Arbitration: WM weight increases as RL policy entropy increases (RL is unsure),
      and decreases with set size (WM is burdened). Weight = sigmoid(base + slope*H) * (3/nS).

    Parameters
    ----------
    model_parameters : list or array of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Base logit for WM mixture weight (unbounded; passed through sigmoid).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_entropy_slope : float
            Slope controlling how much RL entropy increases reliance on WM.
        wm_decay : float
            WM decay toward uniform per visit (0-1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_entropy_slope, wm_decay = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3
    log_nA = np.log(nA)

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            logits_Q = softmax_beta * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits_Q - np.max(logits_Q))
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # RL entropy (natural log)
            H_rl = -np.sum(pvec_rl * (np.log(pvec_rl + 1e-12))) / log_nA  # normalized 0..1

            # WM policy
            W_s = w[s, :]
            logits_W = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_W - np.max(logits_W))
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Arbitration: uncertainty-weighted and set-size downscaled
            base_weight = sigmoid(wm_weight_base + wm_entropy_slope * H_rl)
            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(base_weight * size_scale, 0.0, 1.0)

            p_total = p_wm * eff_wm_weight + (1 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: decay, then encode on reward
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                target = np.zeros(nA); target[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited WM with perseveration bias in RL.

    Mechanism
    - RL: delta-rule learning with a perseveration bias toward repeating the last
      action taken in the same state (irrespective of outcome).
    - WM: encodes rewarded actions and decays toward uniform; arbitration weight is
      scaled by min(1, C/nS) to capture limited-capacity WM that is diluted in larger sets.

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight_base : float
            Base WM mixture weight (0-1) before capacity scaling.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_capacity_C : float
            Effective WM capacity in number of items; weight scaled by min(1, C/nS).
        wm_decay : float
            WM decay toward uniform per visit (0-1).
        perseveration : float
            Additive logit bias for repeating the last action in the same state (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_capacity_C, wm_decay, perseveration = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    nA = 3

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        last_action = -np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with perseveration bias
            Q_s = q[s, :].copy()
            logits_Q = softmax_beta * (Q_s - np.max(Q_s))
            if last_action[s] >= 0:
                logits_Q[last_action[s]] += perseveration
            exp_rl = np.exp(logits_Q - np.max(logits_Q))
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy
            W_s = w[s, :]
            logits_W = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_W - np.max(logits_W))
            pvec_wm = exp_wm / np.sum(exp_wm)
            p_wm = pvec_wm[a]

            # Capacity-limited WM arbitration
            cap_scale = np.minimum(1.0, wm_capacity_C / float(nS))
            eff_wm_weight = np.clip(wm_weight_base * cap_scale, 0.0, 1.0)

            p_total = p_wm * eff_wm_weight + (1 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update: decay then encode on reward
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                target = np.zeros(nA); target[a] = 1.0
                w[s, :] = 0.6 * w[s, :] + 0.4 * target

            # Track last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p