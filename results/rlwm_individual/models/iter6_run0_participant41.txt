def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated hybrid RL + fast WM with age- and set-size-dependent capacity, plus uncertainty-driven omissions.

    Idea:
    - RL: tabular Q-learning with softmax.
    - WM: fast cache per state that concentrates probability on the most recently rewarded action; it decays over time.
    - Arbitration: action policy is a mixture of WM and RL. The WM weight is determined by an age- and set-size-
      scaled capacity parameter. Uncertainty in WM increases omission probability.
    - Lapses: a small fraction of trials are random choices among available actions (not omissions).

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial. Valid actions {0,1,2}; -2 denotes omission/timeout.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative reward indicates invalid trial (e.g., omission).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial. Constant within a block.
    age : array-like of float/int
        Participant age; older group (>=45) reduces effective WM capacity and increases omission tendencies.
    model_parameters : list/tuple of length 5
        [alpha, beta, cap_base, eta_unc2om, lapse_rand]
        - alpha: RL learning rate (0-1).
        - beta: inverse temperature (>0) for RL softmax.
        - cap_base: baseline WM capacity weight (0-1), scaled by set size and age to form mixture weight.
        - eta_unc2om: gain converting WM uncertainty into omission probability.
        - lapse_rand: random-choice lapse probability (0-1) applied to actions (not omissions).

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, cap_base, eta_unc2om, lapse_rand = model_parameters
    age_val = age[0]
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    # Age effects: older adults have reduced effective WM capacity and higher omission drive
    age_cap_mult = 0.6 if older else 1.0
    age_omit_mult = 1.3 if older else 1.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM
        Q = (1.0 / nA) * np.ones((nS, nA))
        WM = (1.0 / nA) * np.ones((nS, nA))  # fast cache
        WM_conf = np.zeros(nS)               # confidence [0..1] based on recent correct storage

        # Set-size dependent WM weighting: fewer items -> more effective WM
        # Scale cap_base by relative capacity 3/nS and age factor
        wm_weight = np.clip(cap_base * (3.0 / nS) * age_cap_mult, 0.0, 1.0)

        # WM decay per trial: harder with larger set sizes and older adults
        wm_decay = 0.05 * (1.0 + 0.5 * older) * (1.0 + (nS - 3) / 3.0)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            Qs = Q[s].copy()
            Qs -= np.max(Qs)
            rl_logits = beta * Qs
            rl_logits -= np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs = rl_probs / np.sum(rl_probs)

            # WM policy
            WM_s = WM[s].copy()
            WM_s = np.maximum(WM_s, eps)
            WM_s /= np.sum(WM_s)
            # Make WM more peaked when confidence is high
            beta_wm = 1.0 + 6.0 * WM_conf[s]  # 1..7
            wm_logits = beta_wm * (WM_s - np.max(WM_s))
            wm_logits -= np.max(wm_logits)
            wm_probs = np.exp(wm_logits)
            wm_probs = wm_probs / np.sum(wm_probs)

            # Uncertainty of WM: 1 - max prob
            wm_unc = 1.0 - np.max(WM_s)

            # Omission probability driven by WM uncertainty, age, and set size
            p_omit = np.clip(eta_unc2om * wm_unc * age_omit_mult * (nS / 3.0), 0.0, 1.0)

            # Arbitration between WM and RL for action (non-omission) choices
            mix_probs = wm_weight * wm_probs + (1.0 - wm_weight) * rl_probs
            mix_probs = np.maximum(mix_probs, eps)
            mix_probs /= np.sum(mix_probs)

            # Lapse: random action choice (not omission)
            p_rand = lapse_rand

            # Final choice probability
            if a == -2:
                p_choice = p_omit
                p_choice = max(p_choice, eps)
            elif 0 <= a < nA:
                # Non-omit: combine arbitration and lapses; ensure total mass (1 - p_omit)
                p_act = (1.0 - p_omit)
                p_action = (1.0 - p_rand) * mix_probs[a] + p_rand * (1.0 / nA)
                p_choice = p_act * p_action
                p_choice = max(p_choice, eps)
            else:
                p_choice = eps

            neg_log_lik -= np.log(p_choice)

            # Learning updates only on valid non-omission trials with valid reward
            if (a >= 0) and (r >= 0):
                # RL update
                pe = r - Q[s, a]
                Q[s, a] += alpha * pe

                # WM update: rewarded action strengthens cache; non-reward weakens confidence
                if r > 0.5:
                    # Move WM toward a one-hot on action a
                    gain = 0.7
                    WM[s, a] = WM[s, a] + gain * (1.0 - WM[s, a])
                    for aa in range(nA):
                        if aa != a:
                            WM[s, aa] *= (1.0 - gain)
                    WM_conf[s] = np.clip((1.0 - wm_decay) * WM_conf[s] + 0.6 * (1.0 - WM_conf[s]), 0.0, 1.0)
                else:
                    # Mild flattening and confidence decay
                    flatten = 0.2
                    WM[s] = (1.0 - flatten) * WM[s] + flatten * (np.ones(nA) / nA)
                    WM_conf[s] = (1.0 - wm_decay) * WM_conf[s]

                # Passive decay toward uniform
                WM[s] = (1.0 - wm_decay) * WM[s] + wm_decay * (np.ones(nA) / nA)
                WM[s] = np.maximum(WM[s], eps)
                WM[s] /= np.sum(WM[s])

    return neg_log_lik


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Decaying Dirichlet belief model over correct action per state with age- and set-size-adaptive temperature and omissions.

    Idea:
    - The participant maintains a categorical belief for each state over which action is correct.
      Beliefs are Dirichlet counts that decay over time within a block.
    - Choice: softmax over the current belief mean (normalized counts), with temperature depending on set size and age.
    - Update: rewarded action increments its count; non-rewarded actions get a small decrement via decay only.
    - Omissions: a fixed omission propensity modulated by set size and age; when omitting, no update occurs.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial. Valid actions {0,1,2}; -2 denotes omission/timeout.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative reward indicates invalid trial (e.g., omission).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial. Constant within a block.
    age : array-like of float/int
        Participant age; older group (>=45) reduces action precision and increases omission propensity.
    model_parameters : list/tuple of length 5
        [beta3, beta6, conc0, decay, omit_p]
        - beta3: inverse temperature for set size 3.
        - beta6: inverse temperature for set size 6.
        - conc0: symmetric Dirichlet prior concentration (>0).
        - decay: per-trial decay rate of Dirichlet counts toward the prior (0-1).
        - omit_p: base omission probability; scaled up by set size and age.

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed choices.
    """
    beta3, beta6, conc0, decay, omit_p = model_parameters
    age_val = age[0]
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    # Age-modulated precision and omission factors
    beta_age_mult = 0.85 if older else 1.0  # older -> lower precision
    omit_age_mult = 1.25 if older else 1.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Temperature by set size and age
        beta = (beta3 if nS <= 3 else beta6) * beta_age_mult

        # Initialize Dirichlet counts per state
        counts = np.ones((nS, nA)) * (conc0 / nA)

        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Current mean belief over actions
            c_s = counts[s].copy()
            c_s = np.maximum(c_s, eps)
            bel = c_s / np.sum(c_s)

            # Omission probability depends on set size and age
            p_omit = np.clip(omit_p * omit_age_mult * (nS / 3.0), 0.0, 1.0)

            # Action probabilities given no omission
            logits = beta * (bel - np.max(bel))
            logits -= np.max(logits)
            probs = np.exp(logits)
            probs = probs / np.sum(probs)

            if a == -2:
                p_choice = max(p_omit, eps)
            elif 0 <= a < nA:
                p_choice = (1.0 - p_omit) * probs[a]
                p_choice = max(p_choice, eps)
            else:
                p_choice = eps

            neg_log_lik -= np.log(p_choice)

            # Belief update: only on valid, non-omission trials with valid reward
            # Apply decay toward prior before incrementing
            counts[s] = (1.0 - decay) * counts[s] + decay * (conc0 / nA) * np.ones(nA)

            if (a >= 0) and (r >= 0):
                if r > 0.5:
                    # Reward strengthens belief that action a is correct
                    counts[s, a] += 1.0
                else:
                    # No explicit penalty; decay already reduces spurious certainty
                    pass

            # Keep counts positive
            counts[s] = np.maximum(counts[s], eps)

    return neg_log_lik


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Q-learning with interference across actions, age- and set-size-dependent forgetting, and a default-response pathway.

    Idea:
    - RL: tabular Q-learning with softmax.
    - Interference: when updating the chosen action's value, competing actions in the same state are inhibited
      proportionally to the prediction error (captures misbinding/interference).
    - Forgetting: per-trial decay of Q toward a neutral baseline; stronger with larger set size and in older adults.
    - Default pathway: with probability that grows with uncertainty and set size, behavior is driven by a default
      response tendency. Default tendency is toward action 0 if default_bias > 0, or toward omission if default_bias < 0.
      Older age increases reliance on the default pathway.

    Parameters
    ----------
    states : array-like of int
        State index per trial within block (0..nS-1).
    actions : array-like of int
        Chosen action per trial. Valid actions {0,1,2}; -2 denotes omission/timeout.
    rewards : array-like of float/int
        Feedback per trial. 0 or 1; negative reward indicates invalid trial (e.g., omission).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6) per trial. Constant within a block.
    age : array-like of float/int
        Participant age; older group (>=45) increases forgetting and default-pathway reliance.
    model_parameters : list/tuple of length 5
        [alpha, beta, xi_interf, forget, default_bias]
        - alpha: learning rate (0-1) for Q updates.
        - beta: inverse temperature (>0) for softmax over Q.
        - xi_interf: interference strength (0-1): fraction of prediction error applied to inhibit unchosen actions.
        - forget: baseline forgetting rate (0-1) toward neutral baseline each trial.
        - default_bias: controls the default pathway. Magnitude in [0,1] is the base strength; sign determines target:
            >0 -> bias toward action 0; <0 -> bias toward omission. Older adults amplify its effect.

    Returns
    -------
    neg_log_lik : float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, xi_interf, forget, default_bias = model_parameters
    age_val = age[0]
    older = 1 if age_val >= 45 else 0

    nA = 3
    eps = 1e-12
    neg_log_lik = 0.0

    # Age effects
    age_forget_mult = 1.3 if older else 1.0
    age_default_mult = 1.4 if older else 1.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q
        Q = (1.0 / nA) * np.ones((nS, nA))
        neutral = 1.0 / nA

        # Effective forgetting for this block
        forget_eff = np.clip(forget * age_forget_mult * (nS / 3.0), 0.0, 1.0)

        # Default pathway base strength
        base_def = np.clip(abs(default_bias), 0.0, 1.0) * age_default_mult * (nS / 3.0)
        target_is_action0 = (default_bias >= 0.0)

        # Track last action per state to compute uncertainty (via Q entropy)
        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Softmax over Q for action policy
            Qs = Q[s].copy()
            Qs -= np.max(Qs)
            logits = beta * Qs
            logits -= np.max(logits)
            soft = np.exp(logits)
            soft = soft / np.sum(soft)

            # Uncertainty via normalized entropy of Q-policy
            p_soft = np.maximum(soft, eps)
            ent = -np.sum(p_soft * np.log(p_soft))
            ent_norm = ent / np.log(nA)  # 0..1

            # Default pathway probability increases with uncertainty
            p_def = np.clip(base_def * ent_norm, 0.0, 0.95)

            # Combine default and softmax pathways
            if target_is_action0:
                # Default mass goes to action 0
                act_probs = (1.0 - p_def) * soft
                act_probs[0] += p_def
                p_omit = 0.0
            else:
                # Default mass goes to omission
                act_probs = (1.0 - p_def) * soft
                p_omit = p_def

            # Choice likelihood
            if a == -2:
                p_choice = max(p_omit, eps)
            elif 0 <= a < nA:
                p_choice = max(act_probs[a], eps)
            else:
                p_choice = eps

            neg_log_lik -= np.log(p_choice)

            # Learning updates on valid non-omission trials with valid reward
            # Apply forgetting first (toward neutral baseline)
            Q[s] = (1.0 - forget_eff) * Q[s] + forget_eff * neutral

            if (a >= 0) and (r >= 0):
                pe = r - Q[s, a]
                # Update chosen action
                Q[s, a] += alpha * pe
                # Interference: inhibit other actions
                if xi_interf > 0.0:
                    leak = xi_interf * alpha * pe / (nA - 1)
                    for aa in range(nA):
                        if aa != a:
                            Q[s, aa] -= leak

    return neg_log_lik