def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + decaying Working Memory with load-dependent arbitration.

    Mechanism:
    - RL: tabular Q-learning with a single learning rate.
    - WM: fast delta-rule toward the most recent outcome in each state, with trial-wise decay
          back to uniform (interference/forgetting).
    - Arbitration: convex mixture of RL and WM policies. WM contribution is down-weighted
          under higher set size via an exponential load penalty.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline mixture weight on WM in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10 for a wide range.
    - wm_eta: WM learning rate in [0,1]; governs how strongly WM updates on each outcome.
    - wm_decay: WM decay-to-uniform per trial in [0,1]; larger means faster forgetting/interference.
    - load_slope: Nonnegative coefficient for set-size-dependent down-weighting of WM via
                  wm_weight_eff = wm_weight * exp(-load_slope * (nS - 3)).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_eta, wm_decay, load_slope = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic baseline WM inverse temperature

    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    lr = np.clip(lr, 0.0, 1.0)
    wm_eta = np.clip(wm_eta, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    load_slope = max(0.0, float(load_slope))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent arbitration weight
        wm_weight_eff = wm_weight * np.exp(-load_slope * max(0, nS - 3))
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: softmax probability of the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values (high precision by default)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy with load-dependent WM influence
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: delta-rule toward outcome; distributes mass accordingly and decays
            # Move chosen action toward r, and the other actions toward (1-r)/(nA-1) to keep normalization
            target = np.ones(nA) * ((1.0 - r) / (nA - 1))
            target[a] = r
            w[s, :] += wm_eta * (target - w[s, :])

            # Trial-wise decay of the WM store toward uniform to capture interference
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Ensure numerical stability and normalization per state-row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with error-gated arbitration and choice stickiness expressed via WM.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: cached action tendency per state that strengthens on reward and weakens on error.
    - Error-gated arbitration: WM weight increases when WM is confident (high max entry in w[s,:])
      and decreases when it is uncertain; confidence is sigmoidal-gated and also attenuated by load (1/nS).
    - Choice stickiness: the last chosen action in a state transiently boosts WM for that action,
      creating short-run perseveration.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline WM mixture weight in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - stickiness: Additive boost to WM for the chosen action after each choice (>=0).
    - gate_slope: Steepness of the confidence gate for arbitration (>=0).
    - wm_eta: WM plasticity in [0,1]; strength of updating WM on feedback (reward and error).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, stickiness, gate_slope, wm_eta = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic

    lr = np.clip(lr, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    stickiness = max(0.0, float(stickiness))
    gate_slope = max(0.0, float(gate_slope))
    wm_eta = np.clip(wm_eta, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Compute RL policy
            Q_s = q[s, :]

            # WM confidence measured as the max weight in the state
            W_s = w[s, :]
            wm_conf = float(np.max(W_s))
            # Confidence gate in [0,1], baseline at uniform (1/nA), scaled by set size
            centered_conf = wm_conf - (1.0 / nA)
            gate = 1.0 / (1.0 + np.exp(-gate_slope * centered_conf))
            # Load attenuation (higher nS -> lower WM weight)
            wm_weight_eff = wm_weight * gate / max(1, nS)
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Reward: push chosen action up; others down.
            # - No reward: pull the state vector toward uniform (forget the mapping).
            if r > 0.0:
                # Hebbian-like strengthening for chosen action; normalization by redistribution
                w[s, :] *= (1.0 - wm_eta)
                w[s, a] += wm_eta
            else:
                # Error-driven forgetting toward uniform
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

            # Choice stickiness: after acting, transiently bias WM toward the chosen action
            if stickiness > 0.0:
                w[s, a] += stickiness
                # Normalize to keep a valid distribution
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-sensitive WM precision and directed exploration via UCB in WM.

    Mechanism:
    - RL: tabular Q-learning.
    - WM: fast associative store whose softmax precision decreases with set size (load).
      WM precision: beta_wm = psi0 / (1 + (nS - 1)^load_exp).
    - Directed exploration: an Upper-Confidence-Bound (UCB) bonus encourages sampling
      less-tried actions in the WM policy by augmenting WM logits proportionally to
      1/sqrt(N[s,a]+1), where N are within-block visit counts.
    - Arbitration: convex mixture controlled by a fixed wm_weight.

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Mixture weight on WM in [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - ucb_beta: Strength of UCB bonus (>=0) added to WM logits to promote exploration.
    - psi0: Baseline WM precision scale (>0).
    - load_exp: Exponent controlling how sharply WM precision decreases with set size (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, ucb_beta, psi0, load_exp = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # base; will be replaced by load-sensitive precision

    lr = np.clip(lr, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    ucb_beta = max(0.0, float(ucb_beta))
    psi0 = max(1e-6, float(psi0))
    load_exp = max(0.0, float(load_exp))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # visit counts for UCB

        # Load-sensitive WM precision
        beta_wm_eff = psi0 / (1.0 + max(0, nS - 1) ** load_exp)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Increment visit count for state s (for all actions, only chosen will increase below)
            # We'll use current N before increment to compute exploration bonus.
            Q_s = q[s, :]
            W_s = w[s, :]
            N_s = N[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with UCB: augment WM logits by a bonus favoring less-tried actions.
            # Implement by adding a bonus to the effective logits before softmax.
            # The softmax probability trick used below expects we pass (logits) as values divided by beta,
            # so we add (ucb_beta / beta_wm_eff) * bonus so that after multiplying by beta_wm_eff
            # the effective additive term equals ucb_beta * bonus.
            bonus = 1.0 / np.sqrt(N_s + 1.0)
            W_eff = W_s + (ucb_beta / max(beta_wm_eff, 1e-6)) * bonus

            p_wm = 1.0 / np.sum(np.exp(beta_wm_eff * (W_eff - W_eff[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-dependent strengthening, otherwise mild decay toward uniform
            if r > 0.0:
                # Increase chosen action; renormalize
                inc = 0.5  # implicit internal gain via fixed fraction of mass; maintain use of parameters elsewhere
                # Use lr to scale mild coupling between systems: greater lr reduces WM reliance incrementally
                gain = min(1.0, 0.5 + 0.5 * (1.0 - lr))
                w[s, :] *= (1.0 - gain)
                w[s, a] += gain
            else:
                # Gentle drift toward uniform on errors
                drift = 0.2
                w[s, :] = (1.0 - drift) * w[s, :] + drift * w_0[s, :]

            # Normalize WM row
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

            # Update visit counts
            N[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p