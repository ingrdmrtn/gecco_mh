Below are three standalone cognitive models that follow your template and constraints. Each model implements a distinct WM mechanism and a load-dependent arbitration/update policy. All models return the negative log-likelihood of observed choices.

Note: These functions assume numpy as np is already imported, as requested.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-gated arbitration and load-scaled WM decay.

    Idea:
    - RL learns via delta-rule and softmax.
    - WM stores a one-shot policy for rewarded actions (item-specific binding).
    - Arbitration weight decreases with set size (cognitive load).
    - WM decays faster under higher load.
    
    Parameters (6 total):
    - lr: RL learning rate in [0,1].
    - wm_weight: Base WM arbitration weight in [0,1].
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - wm_encode: WM encoding strength on rewarded trials in [0,1].
    - wm_decay_load: Base WM decay toward uniform; scaled by load in [0,1].
    - beta_load_arb: Load sensitivity (>0) controlling how strongly load reduces WM arbitration.
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_encode, wm_decay_load, beta_load_arb = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute load-dependent decay
        wm_decay = np.clip(wm_decay_load * (1.0 + load), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy via softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-gated arbitration: higher load => lower effective WM weight
            # wm_weight_eff = wm_weight * sigmoid(-beta_load_arb * load)
            wm_weight_eff = wm_weight / (1.0 + np.exp(beta_load_arb * load))
            wm_weight = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r >= 0.5:
                # Encode rewarded action as one-shot binding
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_encode) * w[s, :] + wm_encode * target
            else:
                # On non-reward: decay toward uniform
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-gated arbitration and cross-state interference.

    Idea:
    - RL learns via delta-rule and softmax.
    - WM encodes rewarded actions with a learning rate.
    - Under higher load, WM suffers cross-state interference (global decay across all states),
      modeling limited capacity and confusion among items.
    - Arbitration weight decreases with load.

    Parameters (6 total):
    - lr: RL learning rate in [0,1].
    - wm_weight: Base WM arbitration weight in [0,1].
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - wm_alpha: WM learning rate for rewarded trials in [0,1].
    - cross_talk: Base global interference strength in [0,1]; scaled by load each trial.
    - load_gate: Load sensitivity (>0) reducing WM arbitration with increasing load.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, cross_talk, load_gate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Standard WM softmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-gated arbitration (higher load => less WM)
            wm_weight_eff = wm_weight / (1.0 + load_gate * load)
            wm_weight = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Encode on reward
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target

            # 2) Apply cross-state interference proportional to load
            # This models capacity limitations that blur all WM entries toward uniform
            xi = np.clip(cross_talk * load, 0.0, 1.0)
            if xi > 0:
                w = (1.0 - xi) * w + xi * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with win-stay encoding strength and load-dependent lapses.

    Idea:
    - RL uses delta-rule softmax.
    - WM implements a win-stay mechanism: stronger one-shot encoding on rewarded trials
      with tunable strength (stay_bonus).
    - On losses, WM shifts slightly away from the chosen action (anti-binding).
    - Decision lapses increase with load, reducing the determinism of WM policy.

    Parameters (6 total):
    - lr: RL learning rate in [0,1].
    - wm_weight: WM arbitration weight in [0,1].
    - softmax_beta: RL inverse temperature (scaled internally x10).
    - stay_bonus: Encoding gain (>0). Larger => more decisive win-stay in WM.
    - epsilon0: Baseline lapse rate in [0,1].
    - load_lapse: Load slope for lapse (>=0). Effective lapse = epsilon0 + load_lapse*load (clipped [0,1]).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, stay_bonus, epsilon0, load_lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute effective lapse
        epsilon = np.clip(epsilon0 + load_lapse * load, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax over W_s
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Inject load-dependent lapse into WM policy: p = (1-eps)*p_wm_core + eps*(1/nA)
            p_wm = (1.0 - epsilon) * p_wm_core + epsilon * (1.0 / nA)

            # Use provided wm_weight directly (no change) for arbitration
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Win-stay encoding: scale by stay_bonus via a sigmoid to keep [0,1]
            enc = 1.0 / (1.0 + np.exp(-stay_bonus))
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                # Lose-shift: move a fraction of mass away from chosen action toward uniform
                anti = 0.5 * enc  # smaller than win-stay
                # Reduce chosen action prob and renormalize toward uniform baseline
                w[s, :] = (1.0 - anti) * w[s, :] + anti * w_0[s, :]
                # Additionally, slightly penalize the chosen action to emphasize shift
                penal = min(anti, 0.49)  # keep stability
                w[s, a] = (1.0 - penal) * w[s, a] + penal * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p