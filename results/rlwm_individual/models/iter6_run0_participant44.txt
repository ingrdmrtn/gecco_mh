def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with entropy-based arbitration and valence-asymmetric learning; age reduces WM influence.

    Idea:
    - RL uses separate learning rates for positive/negative prediction errors (valence asymmetry).
    - WM stores a probability distribution per state; on reward it sharpens toward the chosen action,
      on no reward it decays toward uniform.
    - Arbitration between WM and RL depends on relative certainty: more weight to the lower-entropy
      (more certain) policy. Also gives more WM weight at low set size and penalizes WM for older adults.
    - Action probabilities are a convex mixture of RL softmax and WM softmax.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0..1)
    - lr_neg: RL learning rate for negative prediction errors (0..1)
    - wm_base: baseline WM weight before arbitration (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - entropy_slope: slope controlling how much entropy difference gates WM (>=0)
    - age_wm_drop: how much to reduce WM weight for older adults (0..1)
    
    Inputs:
    - states, actions, rewards: per-trial arrays
    - blocks: block index per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single participant age
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_base, softmax_beta, entropy_slope, age_wm_drop = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (softmax vector)
            Qc = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(softmax_beta * Qc)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM policy (softmax vector over WM distribution)
            Wc = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Wc)
            p_wm_vec /= np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            # Entropy-based arbitration with load and age adjustments
            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, eps, 1.0)))
            H_wm = -np.sum(p_wm_vec * np.log(np.clip(p_wm_vec, eps, 1.0)))
            load_scale = 3.0 / float(nS)  # 1 for set size 3, 0.5 for set size 6
            # logistic gating mixing entropy difference (lower entropy -> greater weight)
            raw_gate = (wm_base * load_scale) + entropy_slope * (H_rl - H_wm) - age_wm_drop * older
            wm_mix = 1.0 / (1.0 + np.exp(-raw_gate))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - q[s, a]
            lr_eff = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_eff * pe

            # WM update: reinforce on reward, decay on no-reward
            if r > 0.5:
                # Sharpen toward one-hot; stronger in small set sizes
                alpha_w = 0.5 * (3.0 / float(nS)) + 0.25  # 0.75 at nS=3; 0.5 at nS=6
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                w[s, :] = (1.0 - alpha_w) * W_s + alpha_w * peaked
            else:
                # Forget toward uniform faster for larger set sizes
                decay = 0.3 + 0.3 * ((nS - 3) / 3.0)  # 0.3 (nS=3) to 0.6 (nS=6)
                w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + recency-based WM (stickiness) and age-adjusted exploration.

    Idea:
    - RL learns with a single learning rate.
    - WM tracks a recency-weighted distribution over the most recent chosen action per state ("stickiness"),
      strengthened by rewards. This creates a tendency to repeat recent successful actions.
    - Arbitration leans more on WM at low set size; RL exploration increases (lower beta) for older adults,
      especially under high load (set size 6).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - softmax_beta: base RL inverse temperature (scaled internally by 10)
    - stickiness: recency gain for WM update (0..1)
    - age_explore_boost: increases exploration in older adults (0..1; reduces RL beta proportional to load)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, stickiness, age_explore_boost = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Age- and load-adjusted RL temperature: older adults explore more, esp. at high load
            load_factor = (nS - 3) / 3.0  # 0 (nS=3), 1 (nS=6)
            beta_rl_eff = softmax_beta * (1.0 - older * age_explore_boost * load_factor)
            beta_rl_eff = max(beta_rl_eff, 1e-3)

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL choice prob
            Qc = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(beta_rl_eff * Qc)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM choice prob (recency/stickiness)
            Wc = W_s - np.max(W_s)
            p_wm_vec = np.exp(softmax_beta_wm * Wc)
            p_wm_vec /= np.sum(p_wm_vec)
            p_wm = p_wm_vec[a]

            # Arbitration: more WM at low load; slight age penalty to WM
            wm_mix = wm_weight_base * (3.0 / float(nS)) * (1.0 - 0.3 * older)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: recency-weighted with reward gain
            # Base recency push toward chosen action
            kappa = stickiness * (0.5 + 0.5 * r)  # stronger if rewarded
            onehot = np.zeros(3)
            onehot[a] = 1.0
            w[s, :] = (1.0 - kappa) * W_s + kappa * onehot

            # Passive decay to uniform stronger under high load and in older adults
            decay = 0.1 + 0.2 * load_factor + 0.1 * older * load_factor
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with limited slots that depend on age; only slot-eligible states get WM support.

    Idea:
    - RL learns action values via a single learning rate.
    - WM has a limited number of active "slots" (wm_slots). Only states encountered early up to this
      capacity receive WM encoding; others default to uniform and rely on RL.
    - Older adults have fewer effective slots (age_slot_penalty), reducing WM support especially at set size 6.
    - Arbitration weight is proportional to whether the current state has a WM slot and to low cognitive load.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_weight_base: baseline WM mixture weight (0..1)
    - wm_slots: nominal number of WM slots (1..6)
    - age_slot_penalty: slots reduction applied if older (0..wm_slots)

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight_base, wm_slots, age_slot_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    older = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Effective slots after age penalty
        slots_eff = int(np.clip(np.round(wm_slots - older * age_slot_penalty), 1, nS))

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track order of first encounter to assign WM slots
        seen_order = []
        in_slot = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Assign slot on first encounter up to capacity
            if s not in seen_order:
                seen_order.append(s)
                if np.sum(in_slot) < slots_eff:
                    in_slot[s] = True

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL softmax
            Qc = Q_s - np.max(Q_s)
            p_rl_vec = np.exp(softmax_beta * Qc)
            p_rl_vec /= np.sum(p_rl_vec)
            p_rl = p_rl_vec[a]

            # WM softmax; if no slot, WM is effectively uniform
            if in_slot[s]:
                Wc = W_s - np.max(W_s)
                p_wm_vec = np.exp(softmax_beta_wm * Wc)
                p_wm_vec /= np.sum(p_wm_vec)
            else:
                p_wm_vec = (1.0 / nA) * np.ones(nA)
            p_wm = p_wm_vec[a]

            # Arbitration: WM contributes only if in slot and low load
            wm_mix = wm_weight_base * (3.0 / float(nS)) * (1.0 if in_slot[s] else 0.0)
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update only if state is in a slot
            if in_slot[s]:
                if r > 0.5:
                    # Write rewarded action strongly
                    alpha = 0.7
                    onehot = np.zeros(nA)
                    onehot[a] = 1.0
                    w[s, :] = (1.0 - alpha) * W_s + alpha * onehot
                else:
                    # Gentle decay to uniform
                    decay = 0.3
                    w[s, :] = (1.0 - decay) * W_s + decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p