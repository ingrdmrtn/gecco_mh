def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-gated WM with load-sensitive arbitration.

    Mechanism
    - RL: delta-rule learning with softmax policy.
    - WM: associative table w that moves toward one-hot on rewards and softly
      downweights erroneous associations on losses; decays toward uniform each trial.
    - Arbitration: trial-wise weight favors the source (RL vs WM) with higher
      confidence (spread between best and worst action values). Arbitration additionally
      penalizes WM under higher set sizes.

    Parameters
    ----------
    model_parameters: tuple/list of 5 floats
        lr: float
            Learning rate for RL and WM table updates (0..1).
        wm_weight: float
            Baseline arbitration weight favoring WM (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        kappa_meta: float
            Confidence sensitivity; higher values shift arbitration toward the higher-spread (more confident) system.
        lambda_load: float
            Load penalty on WM under larger set sizes; also mildly increases WM decay as nS grows.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, kappa_meta, lambda_load = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM is near-deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay increases slightly with load
        decay_wm = np.clip(0.025 + 0.075 * lambda_load * max(0, nS - 3) / 3.0, 0.0, 1.0)

        # Precompute base arbitration in logit space for stability
        eps = 1e-12
        base_logit = np.log((wm_weight + eps) / (1.0 - wm_weight + eps))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence-based arbitration with load penalty on WM
            conf_rl = np.max(Q_s) - np.min(Q_s)
            conf_wm = np.max(W_s) - np.min(W_s)
            wm_signal = kappa_meta * (conf_wm - conf_rl) - lambda_load * max(0, nS - 3)

            wm_mix = 1.0 / (1.0 + np.exp(-(base_logit + wm_signal)))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM update: reward-congruent sharpening and loss-softening
            if r > 0.5:
                # Move distribution toward one-hot on chosen action
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                # On loss, dampen chosen action toward uniform
                w[s, a] = (1.0 - lr) * w[s, a] + lr * (1.0 / nA)

            # Normalize to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Probabilistic WM gating with recency-based decay.

    Mechanism
    - RL: delta-rule learning with softmax policy.
    - WM: updated primarily on rewarded trials via a probabilistic gate that closes
      as set size increases; WM decays toward uniform as a function of time since last
      visit (recency) and set size.
    - Arbitration: fixed WM mixture weight (wm_weight), reflecting a constant reliance
      on WM vs RL within a block.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Constant WM arbitration weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        gate_bias: float
            Baseline logit bias for WM gating on rewarded trials (higher -> more likely to store).
        gate_load_slope: float
            Load-induced reduction in gating; larger values reduce gating as set size increases.
        decay_rate: float
            Base WM recency decay rate per trial; stronger decay for longer inter-visit intervals and larger set sizes.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_bias, gate_load_slope, decay_rate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM is near-deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last-visit times for recency-based decay
        last_seen = -1 * np.ones(nS, dtype=int)

        # Fixed arbitration weight per block
        wm_mix = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply recency-based decay for the current state before computing policy
            if last_seen[s] >= 0:
                dt = (t - last_seen[s])
                # Effective decay increases with dt and set size
                eff_decay = 1.0 - np.exp(-decay_rate * dt * (1.0 + max(0, nS - 3) / 3.0))
                eff_decay = np.clip(eff_decay, 0.0, 1.0)
                w[s, :] = (1.0 - eff_decay) * w[s, :] + eff_decay * w_0[s, :]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM gating on reward; expected (mean-field) update without sampling
            # Gate probability decreases with set size
            gate_logit = gate_bias - gate_load_slope * max(0, nS - 3)
            p_gate = 1.0 / (1.0 + np.exp(-gate_logit))
            p_gate = np.clip(p_gate, 0.0, 1.0)

            if r > 0.5:
                # Expected-strength update toward one-hot with magnitude p_gate
                w[s, :] = (1.0 - p_gate) * w[s, :]
                w[s, a] += p_gate
            else:
                # On loss, gently pull chosen action toward uniform
                w[s, a] = (1.0 - lr) * w[s, a] + lr * (1.0 / nA)

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

            # Update last seen time
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WSLS-style WM with load-attenuated reliance and recency drift.

    Mechanism
    - RL: delta-rule learning with softmax policy.
    - WM: implements a within-state win-stay/lose-shift preference signal m(s,a).
      On reward, the chosen action gets a positive boost; on loss, a negative boost.
      The WM preference decays over time and is more fragile at larger set sizes.
      WM is converted to a probability table w via a softmax over m.
    - Arbitration: base WM weight reduced linearly with set size.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight: float
            Base WM arbitration weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        wsls_gain: float
            Magnitude of WM preference change on win/loss.
        recency_lambda: float
            Per-trial decay of WM preference magnitudes (0..1).
        load_penalty: float
            Linear reduction of WM arbitration weight and additional preference decay as set size increases.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wsls_gain, recency_lambda, load_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # WM readout is near-deterministic

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM probability table derived from preferences m via softmax
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        m = np.zeros((nS, nA))  # WM preference map

        # Load-scaled arbitration and decay multiplier
        load_scale = max(0.0, (nS - 3) / 3.0)
        wm_mix = np.clip(wm_weight * (1.0 - load_penalty * load_scale), 0.0, 1.0)
        # Additional decay of preferences with load
        pref_decay = np.clip(recency_lambda + load_penalty * 0.5 * load_scale, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Convert preferences to WM probabilities for current state
            # Stable softmax over m[s,:]
            m_s = m[s, :]
            m_s_centered = m_s - np.max(m_s)
            exp_m = np.exp(m_s_centered)
            w[s, :] = exp_m / np.sum(exp_m)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM preference decay (global per trial) and load-induced fragility
            m *= (1.0 - pref_decay)

            # WSLS update localized to the visited state-action
            if r > 0.5:
                m[s, a] += wsls_gain
            else:
                m[s, a] -= wsls_gain

            # Optional weak anchoring toward uniform in w-space (via m centering)
            # Keep preferences bounded implicitly by decay + updates

        blocks_log_p += log_p

    return -blocks_log_p