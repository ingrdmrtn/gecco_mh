def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with slot-precision and interference; age x load reduces WM arbitration.

    Idea:
    - RL: standard delta rule with softmax choice.
    - WM: stores rewarded state-action pairs as near one-hot distributions with a tunable precision,
      but suffers from global interference that pushes WM toward uniform as the set gets larger.
    - Arbitration: a baseline WM weight is passed through a logistic gate penalized by age and load.
      Older adults and larger set sizes reduce reliance on WM.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: baseline WM arbitration weight before age/load gating (real-valued).
    - softmax_beta: RL inverse temperature; scaled up internally by 10.
    - slot_precision: WM encoding strength for rewarded pairs (>=0).
    - interference_rate: global WM interference/decay rate per trial (0..1).
    - age_load_slope: penalty scaling on WM weight for older adults and load (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays of equal length.
    - age: array-like with a single value; older group defined as age>=45.
    - Returns negative log-likelihood of the observed choices.
    """
    lr, wm_weight_base, softmax_beta, slot_precision, interference_rate, age_load_slope = model_parameters

    softmax_beta *= 10.0  # make RL sharper within a modest parameter range
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Compute a constant arbitration bias per block influenced by age and load
        load_pen = np.log(max(2.0, float(nS)))
        gate_input_const = wm_weight_base - age_load_slope * (is_older + load_pen)
        wm_weight_const = 1.0 / (1.0 + np.exp(-gate_input_const))  # in [0,1]

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy (current state's memory distribution)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Combine with arbitration weight (constant within block due to stationary load/age)
            wm_weight_t = np.clip(wm_weight_const, 0.0, 1.0)
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM global interference toward uniform (stronger effective impact at larger nS because of more states)
            w = (1.0 - interference_rate) * w + interference_rate * w_0

            # WM encoding: on reward, push toward a sharpened one-hot; on no-reward, mild decay toward uniform
            if r > 0.5:
                # One-hot target with controlled precision
                target = np.full(nA, 0.0)
                target[a] = 1.0
                # Precision is scaled by load so larger sets are harder to encode precisely
                enc_strength = slot_precision / (1.0 + load_pen + 0.5 * is_older)
                enc_strength = np.clip(enc_strength, 0.0, 1.0)
                w[s, :] = (1.0 - enc_strength) * w[s, :] + enc_strength * target
            else:
                # Mild forgetting on errors
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates; WM gated by RL entropy and age; WM decay increases with load and age.

    Idea:
    - RL: separate learning rates for positive vs negative outcomes; softmax choice.
    - WM: deterministic when confident, but decays toward uniform with a rate that increases with set size
      and in older adults.
    - Arbitration: WM weight is a logistic function of a bias plus RL entropy (higher entropy => more WM),
      with an age penalty on the gate.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewards (0..1).
    - lr_neg: RL learning rate for non-rewards (0..1).
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_gate_bias: baseline bias to rely on WM in arbitration (real-valued).
    - entropy_sensitivity: scaling of RL entropy in WM gate (>=0).
    - age_decay_bonus: added WM decay when older (>=0), also penalizes WM gating indirectly.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - Returns negative log-likelihood.
    """
    lr_pos, lr_neg, softmax_beta, wm_gate_bias, entropy_sensitivity, age_decay_bonus = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay increases with set size and in older adults
        base_decay = 0.05 + 0.1 * np.log(max(2.0, float(nS)))
        wm_decay = np.clip(base_decay + age_decay_bonus * is_older, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            # Full softmax to compute entropy for gating
            logits = softmax_beta * Q_s
            logits -= np.max(logits)
            probs = np.exp(logits)
            probs /= np.sum(probs)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # RL entropy as uncertainty measure
            H = -np.sum(probs * np.log(np.clip(probs, eps, 1.0)))

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Arbitration: bias + entropy, with age penalty
            gate_input = wm_gate_bias + entropy_sensitivity * H - 0.5 * age_decay_bonus * is_older
            wm_weight_t = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            lr_use = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr_use * pe

            # WM decay each trial
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding: reward strengthens one-hot; no-reward weakly decays
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                enc = np.clip(0.5 + 0.25 * (H), 0.0, 1.0)  # encode a bit more when RL is uncertain
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Reliability-based arbitration: WM precision vs RL dispersion; age-related lapses and load penalty.

    Idea:
    - RL: standard delta rule.
    - WM: forms sharp one-hot codes on reward with precision parameter; small passive decay.
    - Arbitration: compute a reliability signal for each system:
        * WM confidence = max(W_s) - second_max(W_s).
        * RL dispersion = max(Q_s) - mean(Q_s) as a proxy for RL confidence.
      The weight on WM is a logistic function of (WM_confidence - RL_confidence),
      penalized by set size and an age-related lapse term.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_precision: strength of WM one-hot encoding on rewarded trials (0..1).
    - lapse_old: additional penalty term for older adults in the arbitration gate (>=0).
    - load_penalty_wm: penalty scaling for larger set sizes in the gate (>=0).
    - mix_slope: slope converting confidence difference into WM weight (>=0).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age: arrays for the task.
    - Returns negative log-likelihood.
    """
    lr, softmax_beta, wm_precision, lapse_old, load_penalty_wm, mix_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = 1 if age_val >= 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Small passive WM decay each trial; modestly increases with load
        wm_decay = np.clip(0.02 + 0.03 * np.log(max(2.0, float(nS))), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            # WM policy
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)

            # Reliability-based arbitration
            # WM confidence: gap between best and second best action probs in WM table for state s
            sorted_W = np.sort(W_s)[::-1]
            wm_conf = max(0.0, sorted_W[0] - sorted_W[1])

            # RL confidence: how peaked Q is (max - mean)
            rl_conf = max(0.0, np.max(Q_s) - np.mean(Q_s))

            # Gate input with load and age penalties
            load_term = load_penalty_wm * np.log(max(2.0, float(nS)))
            age_term = lapse_old * is_older
            gate_input = mix_slope * (wm_conf - rl_conf) - load_term - age_term
            wm_weight_t = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM encoding: reward makes near-one-hot with precision; no-reward mild decay
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                enc = np.clip(wm_precision, 0.0, 1.0)
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target
            else:
                w[s, :] = 0.97 * w[s, :] + 0.03 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p