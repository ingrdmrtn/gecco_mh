def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: Set-size–gated WM via logistic arbitration + WM leak + global lapse.
    - RL: standard Q-learning with single learning rate.
    - WM: probability map per state that decays toward uniform; reward-driven storage.
    - Arbitration: WM weight is a logistic function of set size (nS), controlled by
      midpoint and slope parameters (higher nS -> lower WM weight for positive slope).
    - Lapse: mixture with a uniform policy to capture random responding.

    Parameters:
      lr:             RL learning rate (0..1).
      softmax_beta:   RL inverse temperature (scaled internally by 10; >=0).
      wm_midpoint:    Set-size value at which WM and RL are equally weighted.
      wm_slope:       Slope of logistic gating over set size (positive -> less WM at large sets).
      wm_decay:       WM decay/leak toward uniform on each encounter (0..1).
      lapse:          Trial-wise lapse rate mixing with uniform choice (0..1).

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_midpoint, wm_slope, wm_decay, lapse = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Logistic arbitration based on set size (constant within block)
        # wm_weight_eff in [0,1]
        wm_weight_eff = 1.0 / (1.0 + np.exp(wm_slope * (nS - wm_midpoint)))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax over WM map)
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Mixture with lapse
            p_mix = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Rewarded storage: sharpen chosen action's WM probability
            if r > 0:
                # Move mass toward chosen action while keeping normalization
                gain = 1.0  # implicit gain; encoded via the deterministic softmax and decay
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain

                # Normalize and stabilize
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: Choice stickiness in RL + outcome-gated WM arbitration + WM leak.
    - RL: Q-learning with single learning rate; action stickiness adds a transient bias
      toward repeating the last chosen action within the same state.
    - WM: decays toward uniform; storage on reward.
    - Arbitration: WM weight is trial-wise and depends on the most recent outcome:
      higher after reward, lower after no-reward, controlled by a slope parameter and base.

    Parameters:
      lr:               RL learning rate (0..1).
      softmax_beta:     RL inverse temperature (scaled internally by 10; >=0).
      stickiness:       Strength of choice repetition bias in RL policy (>=0).
      wm_base:          Baseline WM weight (0..1).
      wm_outcome_slope: Slope controlling how outcome (reward 0/1) tilts WM weight.
                        Effective weight: sigmoid(logit(wm_base) + wm_outcome_slope*(r_prev-0.5))
                        with r_prev = last outcome for the same state (initialized 0.5).
      wm_decay:         WM decay/leak toward uniform (0..1).

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, stickiness, wm_base, wm_outcome_slope, wm_decay = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # For stickiness: store last action per state as a one-hot bias
        last_a_onehot = np.zeros((nS, nA))
        # For outcome-gated WM: store last outcome per state (initialize to 0.5 neutral)
        last_outcome = 0.5 * np.ones(nS)

        # Convert wm_base to logit space for smooth additive modulation
        wm_base = np.clip(wm_base, 1e-6, 1 - 1e-6)
        base_logit = np.log(wm_base / (1 - wm_base))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]

            # Add stickiness bias to Q-values transiently
            bias = stickiness * last_a_onehot[s, :]
            Q_eff = Q_s + bias

            # RL policy with stickiness-augmented values
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM policy
            W_s = w[s, :]
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Outcome-gated WM weight based on last outcome in this state
            z = base_logit + wm_outcome_slope * (last_outcome[s] - 0.5)
            wm_weight_eff = 1.0 / (1.0 + np.exp(-z))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage on reward
            if r > 0:
                w[s, :] = 0.0 * w[s, :]
                w[s, a] = 1.0  # store deterministically; policy temperature handles softness

            # Update stickiness traces and last outcome
            last_a_onehot[s, :] = 0.0
            last_a_onehot[s, a] = 1.0
            last_outcome[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Fixed WM weight with set-size–scaled WM temperature + cross-state WM interference.
    - RL: Q-learning with single learning rate.
    - WM: state-local probability maps; when rewarded, chosen action is reinforced.
      Cross-state interference spreads weakening to non-current states to mimic capacity limits.
    - Arbitration: fixed WM weight, but WM stochasticity increases with set size by lowering
      WM inverse temperature linearly with nS.

    Parameters:
      lr:               RL learning rate (0..1).
      softmax_beta:     RL inverse temperature (scaled internally by 10; >=0).
      wm_weight:        Fixed WM mixture weight (0..1).
      wm_temp0:         Baseline WM inverse temperature at set size 0 (>=0).
      wm_temp_slope:    Linear decrease per item in set size: beta_wm = max(1, wm_temp0 - wm_temp_slope*nS).
      wm_xtalk:         Cross-state interference (0..1): fraction pulling all non-current states'
                        WM toward uniform on each rewarded encoding in current state.

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, wm_temp0, wm_temp_slope, wm_xtalk = model_parameters

    softmax_beta *= 10
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM temperature (bounded below by 1 for numerical stability)
        softmax_beta_wm = max(1.0, wm_temp0 - wm_temp_slope * nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with set-size–scaled temperature
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Fixed arbitration
            wm_w = np.clip(wm_weight, 0.0, 1.0)
            p_total = wm_w * p_wm + (1 - wm_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM: mild spontaneous normalization to keep stability
            # (no additional decay parameter; governed by xtalk upon reward)
            # Renormalize just in case
            w[s, :] = np.clip(w[s, :], 1e-12, None)
            w[s, :] /= np.sum(w[s, :])

            # WM storage on reward with cross-state interference
            if r > 0:
                # Sharpen current state's chosen action
                w[s, :] = 0.0
                w[s, a] = 1.0
                # Cross-state interference: pull other states slightly toward uniform
                if wm_xtalk > 0:
                    for s2 in range(nS):
                        if s2 == s:
                            continue
                        w[s2, :] = (1.0 - wm_xtalk) * w[s2, :] + wm_xtalk * w_0[s2, :]
                        # Keep normalized
                        w[s2, :] = np.clip(w[s2, :], 1e-12, None)
                        w[s2, :] /= np.sum(w[s2, :])

        blocks_log_p += log_p

    return -blocks_log_p