def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with gap-gated arbitration and load-dependent WM noise.

    Idea:
    - Choices are a mixture of a model-free RL policy and a WM policy derived from a fast, symbolic store.
    - WM reliability decreases with load (set size) through an increase in retrieval noise.
    - Arbitration weight for WM depends on the discriminability (gap between best and second-best WM entries) and is penalized by load.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - mix_bias: real, baseline logit for WM weight in the arbitration (higher => more WM reliance).
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - wm_noise_load: float >= 0, scales how much load degrades WM precision (larger => noisier WM at larger set sizes).
      Effective WM inverse temperature: beta_wm_eff = 50 / (1 + wm_noise_load * load_scaled).
    - wm_gain: float >= 0, sensitivity of the arbitration to the WM “gap” (best vs. second-best entry).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, mix_bias, softmax_beta, wm_noise_load, wm_gain = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # baseline WM precision (will be reduced by load)
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load_scaled = max(0.0, (nS - 3) / 3.0)  # 0 for nS=3, 1 for nS=6

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-dependent noise (lower precision with higher load)
            beta_wm_eff = softmax_beta_wm / (1.0 + wm_noise_load * load_scaled)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # WM discriminability (gap between best and second-best entries)
            order = np.argsort(W_s)[::-1]
            best = W_s[order[0]]
            second = W_s[order[1]] if nA > 1 else W_s[order[0]]
            gap = max(0.0, best - second)

            # Arbitration: WM weight increases with gap, penalized by load
            wm_logit = mix_bias + wm_gain * gap - load_scaled
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating:
            # 1) Load-dependent drift toward uniform (retrieval interference)
            gamma = max(0.0, min(1.0, wm_noise_load * load_scaled))
            w[s, :] = (1.0 - gamma) * w[s, :] + gamma * w_0[s, :]

            # 2) Reward-based overwriting: if rewarded, store as one-hot; if not, slightly de-emphasize chosen action
            if r > 0.0:
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0
            else:
                # Soft suppression of the non-rewarded chosen action to promote exploration within WM
                w[s, a] = (1.0 - 0.5 * gamma) * w[s, a] + (0.5 * gamma) * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM cache with load-dependent persistence.

    Idea:
    - WM acts as a one-shot cache: when a state-action is rewarded, it is stored and then decays over time.
    - The persistence (time constant) of the WM cache decreases with load (set size).
    - Arbitration uses the current cache strength for the state as the WM mixture weight.
    - WM policy is concentrated on the cached action with strength-dependent sharpness.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - wm_persist_base: float > 0, baseline persistence time constant (higher => longer lasting cache).
    - wm_persist_load: float >= 0, how much load reduces persistence: tau_eff = wm_persist_base / (1 + wm_persist_load * load_scaled).
    - wm_hit_bonus: float >= 0, boosts WM sharpness as a function of cache strength:
      beta_wm_eff = 50 * (1 + wm_hit_bonus * strength).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_persist_base, wm_persist_load, wm_hit_bonus = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load_scaled = max(0.0, (nS - 3) / 3.0)
        tau_eff = wm_persist_base / (1.0 + wm_persist_load * load_scaled)
        tau_eff = max(tau_eff, 1e-6)

        # Cache tracking: per-state cached action and its age
        cached_action = -1 * np.ones(nS, dtype=int)
        cached_age = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Update ages (only for the current state to reflect time since last rewarded mapping seen)
            cached_age[s] += 1.0

            # Compute current cache strength for state s
            if cached_action[s] >= 0:
                strength = np.exp(-cached_age[s] / tau_eff)
                strength = float(max(0.0, min(1.0, strength)))
            else:
                strength = 0.0

            Q_s = q[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy constructed from cache
            if cached_action[s] >= 0:
                onehot = np.zeros(nA)
                onehot[cached_action[s]] = 1.0
                # Compose WM vector as a blend of uniform and the cached one-hot, weighted by strength
                W_s = (1.0 - strength) * w_0[s, :] + strength * onehot
            else:
                W_s = w_0[s, :].copy()

            # WM softmax precision scales with strength (sharper when cache is fresh)
            beta_wm_eff = softmax_beta_wm * (1.0 + wm_hit_bonus * strength)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: use the cache strength directly as WM weight
            wm_weight = strength

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM updating (cache mechanics):
            # - If reward delivered, set/refresh cache with chosen action; reset age.
            # - Otherwise, just let it decay (handled by age increment).
            if r > 0.0:
                cached_action[s] = a
                cached_age[s] = 0.0

            # Maintain w as the current WM vector representation used for state s (for interpretability)
            w[s, :] = W_s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with cross-state generalization and load-dependent forgetting.

    Idea:
    - WM stores rewarded state-action mappings as one-hot vectors.
    - When a mapping is stored, WM also generalizes to other states by boosting the same action,
      modeling chunking strategies; this generalization is attenuated by load.
    - WM forgets faster under higher load via drift toward uniform.
    - Arbitration weight depends on a baseline bias and is reduced by load; WM has its own temperature.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - mix_bias: real, baseline logit for WM mixture weight (penalized by load).
    - gen_strength: float in [0,1], how strongly a rewarded action generalizes to other states
      (effective boost scaled by (1 - load_scaled)).
    - forget_load: float >= 0, scales WM forgetting toward uniform as a function of load per encounter.
    - temp_wm: float >= 0, scales WM inverse temperature: beta_wm_eff = 50 * (1 + temp_wm).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, mix_bias, gen_strength, forget_load, temp_wm = model_parameters
    softmax_beta *= 10.0
    base_softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load_scaled = max(0.0, (nS - 3) / 3.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with its own temperature
            beta_wm_eff = base_softmax_beta_wm * (1.0 + temp_wm)
            denom_wm = np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: baseline WM bias penalized by load
            wm_logit = mix_bias - load_scaled
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting: load-dependent drift toward uniform for the current state
            forget_rate = max(0.0, min(1.0, forget_load * load_scaled))
            w[s, :] = (1.0 - forget_rate) * w[s, :] + forget_rate * w_0[s, :]

            # WM reward-based storage with cross-state generalization
            if r > 0.0:
                # Store exact mapping for current state
                w[s, :] = w_0[s, :].copy()
                w[s, a] = 1.0

                # Generalize to other states: boost the rewarded action
                if nS > 1:
                    gen_eff = max(0.0, min(1.0, gen_strength * (1.0 - load_scaled)))
                    if gen_eff > 0.0:
                        for s2 in range(nS):
                            if s2 == s:
                                continue
                            # Move w[s2, a] toward 1 by gen_eff, and renormalize row to be a probability vector
                            w[s2, a] = (1.0 - gen_eff) * w[s2, a] + gen_eff * 1.0
                            # Light normalization to keep distribution valid
                            row = w[s2, :]
                            row = np.maximum(row, 1e-12)
                            row /= np.sum(row)
                            w[s2, :] = row

        blocks_log_p += log_p

    return -blocks_log_p