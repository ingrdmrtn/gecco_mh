def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian WM (Dirichlet counts) with load-adaptive arbitration.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM: keeps Dirichlet counts over actions per state; WM policy is the
      posterior-predictive (normalized counts). Counts start from a symmetric
      prior and accumulate rewards for the chosen action.
    - Arbitration: WM mixture weight decreases with set size (load) via a logistic slope.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate for Q updates.
        - wm_base: float in (0,1)
            Baseline WM contribution to the mixture at low load (nS=3).
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - alpha0: float > 0
            Symmetric Dirichlet prior concentration for WM counts.
        - weight_slope: float
            Slope controlling how fast WM weight decreases as set size increases.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, alpha0, weight_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    # Helper: logistic
    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1 / nA) * np.ones((nS, nA))

        # WM "counts" initialized to symmetric Dirichlet prior
        w = np.ones((nS, nA)) * alpha0
        w_0 = np.ones((nS, nA)) * alpha0  # prior baseline for reference

        log_p = 0
        # Compute a block-specific WM weight based on set size (load)
        # Convert wm_base to logit then shift by size effect so it remains in (0,1)
        base_logit = np.log(np.clip(wm_base, eps, 1 - eps)) - np.log(1 - np.clip(wm_base, eps, 1 - eps))
        wm_weight_eff = logistic(base_logit - weight_slope * (nS - 3))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL policy probability for chosen action using "difference trick"
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: posterior predictive from Dirichlet counts
            counts = w[s, :].copy()
            counts = np.clip(counts, eps, None)
            p_wm_vec = counts / np.sum(counts)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: add reward evidence to chosen action's count
            # (rewarded choices strengthen the cached action; no change when r=0)
            if r > 0:
                w[s, a] += r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + RPE-gated, decaying one-shot WM with load-scaled decay.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM: stores a sharp distribution favoring the most recently "surprisingly" rewarded
      action in each state. The stored WM distribution decays toward uniform with a set-size
      dependent rate. Update strength is gated by the signed reward-prediction error (RPE):
      larger positive RPEs produce stronger WM caching; non-reward or negative RPE produce
      little to no caching.
    - Arbitration: fixed base WM weight across a block.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight0: float in [0,1]
            Mixture weight of WM policy in the final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - gate_slope: float
            Steepness of the logistic gating that maps RPE to WM update strength.
        - wm_decay_base: float in [0,1]
            Baseline WM decay toward uniform per trial at set size 3. Effective decay scales with load.
        - wm_boost: float in (0,1]
            Maximal sharpness of the WM cache when a strongly positive RPE occurs.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, gate_slope, wm_decay_base, wm_boost = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1 / nA) * np.ones((nS, nA))

        # WM distribution table initialized to uniform and has the same shape
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-scaled decay per step
        # Larger sets accelerate decay/interference relative to base
        size_scale = (nS - 3) / max(1, (6 - 3))  # 0 at nS=3, 1 at nS=6
        decay = np.clip(wm_decay_base * (1 + size_scale), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL policy probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from cached distribution
            prefs = w[s, :].copy()
            prefs_centered = prefs - np.mean(prefs)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_centered, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture with fixed base weight
            p_total = wm_weight0 * p_wm + (1 - wm_weight0) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform each step
            w = (1 - decay) * w + decay * w_0

            # RPE-gated WM caching: positive, surprising rewards create sharp caches
            # Gate maps RPE to [0,1]; stronger for large positive RPE
            gate = logistic(gate_slope * (r - Q_s[a]))
            if r > 0:
                # Create a peaked target distribution on the chosen action
                target = w_0[s, :].copy()
                target[a] = 1.0
                target = np.clip(target, eps, None)
                target /= np.sum(target)
                # Blend current WM row toward the target based on gate and wm_boost
                w[s, :] = (1 - gate * wm_boost) * w[s, :] + (gate * wm_boost) * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + fast value-based WM with uncertainty- and load-adaptive arbitration.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM: fast leaky value learner per state-action that tracks recent outcomes with a
      set-size dependent learning rate (faster at low load). WM policy is a softmax over WM values.
    - Arbitration: WM mixture weight increases when RL is uncertain (high entropy) and decreases
      with set size (load). The balance is computed trial-wise from RL state's entropy
      and a load term.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_max: float in (0,1]
            Maximum possible WM weight in the mixture.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - entropy_sensitivity: float
            Scales how strongly RL entropy modulates WM reliance (higher means more WM at high RL uncertainty).
        - size_bias: float
            Controls both WM learning rate and a multiplicative down-weighting of WM with larger set sizes.
        - wm_beta: float
            Inverse temperature for the WM softmax over WM values.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_max, softmax_beta, entropy_sensitivity, size_bias, wm_beta = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = max(1e-3, wm_beta)  # allow user to set WM determinism
    blocks_log_p = 0
    eps = 1e-12

    def softmax_probs(x, beta):
        x = x - np.max(x)  # stabilize
        z = np.exp(np.clip(beta * x, -50, 50))
        return z / max(np.sum(z), eps)

    def logistic(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1 / nA) * np.ones((nS, nA))

        # WM values: start neutral at 1/nA
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM learning rate (faster when nS is small)
        # Map (3 -> higher lr_wm, 6 -> lower lr_wm) via logistic on (3 - nS)
        wm_lr = logistic(size_bias * (3 - nS))

        # Load penalty for WM weight
        load_factor = logistic(size_bias * (3 - nS))  # in (0,1]; shrinks with larger nS

        log_p = 0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # RL chosen-action prob via difference trick
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values
            p_wm_vec = softmax_probs(w[s, :], softmax_beta_wm)
            p_wm = p_wm_vec[a]

            # RL uncertainty (entropy) for this state from its own softmax
            rl_pi = softmax_probs(Q_s, max(1e-3, softmax_beta / 10.0))  # slightly tempered for smoother entropy
            entropy = -np.sum(rl_pi * np.log(np.clip(rl_pi, eps, 1.0)))
            max_entropy = np.log(nA)
            # Center entropy around half of max to avoid ceiling effects
            centered_H = entropy - (0.5 * max_entropy)

            # Arbitration: more WM when RL is uncertain; less WM when set size large
            wm_weight = wm_max * logistic(entropy_sensitivity * centered_H) * load_factor

            # Mixture probability
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: fast leaky tracking toward recent outcomes
            # Move chosen action's WM value toward r, and softly decay others toward baseline
            w[s, a] = (1 - wm_lr) * w[s, a] + wm_lr * r
            # Gentle leakage of non-chosen actions toward baseline
            not_a = [aa for aa in range(nA) if aa != a]
            w[s, not_a] = (1 - 0.25 * wm_lr) * w[s, not_a] + (0.25 * wm_lr) * w_0[s, not_a]

        blocks_log_p += log_p

    return -blocks_log_p