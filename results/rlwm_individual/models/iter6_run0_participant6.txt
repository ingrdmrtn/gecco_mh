def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-gated working memory with age- and set-size-dependent encoding.

    Idea
    - A standard Q-learner runs in the background.
    - A simple working-memory (WM) store encodes the currently hypothesized correct action for a state
      when the gate opens. The gate opening probability increases when capacity slots exceed set size
      and is higher for younger adults; it decreases for older adults and larger set sizes.
    - If WM has a strong entry for the current state, the policy mixes WM's focused choice with RL via softmax.
    - A small lapse to uniform is included.

    Parameters
    ----------
    states : array-like of int
        State indices per trial (0..set_size-1 in each block).
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int
        Binary rewards (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    age : array-like (length >= 1)
        Participant age; age[0] used to define age group.
    model_parameters : tuple/list
        (alpha, beta, slots, gate_sigma, lapse)
        - alpha: RL learning rate (0..1).
        - beta: inverse temperature for softmax (>0).
        - slots: effective WM capacity (e.g., around 3â€“4 in young adults).
        - gate_sigma: slope for the WM gate sigmoid; higher -> sharper gating.
        - lapse: lapse probability mixed with uniform choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed actions.
    """
    alpha, beta, slots, gate_sigma, lapse = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    # Helper sigmoid
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM stores
        Q = (1.0 / nA) * np.ones((nS, nA))
        wm_action = -1 * np.ones(nS, dtype=int)       # stored action hypothesis per state
        wm_strength = np.zeros(nS, dtype=float)       # confidence/strength in stored hypothesis (0..1)

        # Gate bias: younger encode more readily; older less. Also, larger set sizes reduce encoding.
        # Gate input = (slots - nS) - age_penalty; positive -> higher encode probability.
        age_penalty = 0.8 * is_older  # older adults have lower gating
        # Precompute static gate term per block
        gate_input = (slots - float(nS)) - age_penalty
        p_encode_block = sigmoid(gate_sigma * gate_input)  # probability of encoding on a given trial

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax prob for chosen action
            prefs = beta * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_rl = 1.0 / max(denom, 1e-12)

            # WM policy: if we have a stored action, put mass on it proportional to strength
            p_wm = 1.0 / nA
            if wm_action[s] >= 0:
                # Deterministic WM preference on stored action; convert to probability via softmax-like spike
                wm_pref = np.zeros(nA)
                wm_pref[wm_action[s]] = 1.0
                # Convert to probability vector: mostly on stored action, small split across others
                eps_wm = max(1e-6, 1.0 - wm_strength[s])
                p_vec = (eps_wm / nA) * np.ones(nA)
                p_vec[wm_action[s]] += (1.0 - eps_wm) * (nA - 1) / nA
                p_wm = p_vec[a]
            else:
                p_wm = 1.0 / nA

            # Mixture weight of WM depends on current strength and block encode probability
            wm_weight = p_encode_block * wm_strength[s]
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp += np.log(p_final)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM updates: try to encode/update when reward arrives or occasionally by gate
            # Encode with probability p_encode_block; if rewarded, strengthen more
            # Strength decay with set size (more decay when larger nS) and with age
            decay = 0.05 + 0.05 * (nS / 6.0) + 0.05 * is_older
            wm_strength[s] = (1.0 - decay) * wm_strength[s]
            will_encode = (np.random.rand() < p_encode_block)  # stochastic gate for internal state evolution
            if r > 0.5:
                # On reward, set hypothesis to chosen and boost strength strongly
                wm_action[s] = a
                wm_strength[s] += (1.0 - wm_strength[s]) * (0.6 + 0.3 * (1.0 - is_older))
            elif will_encode:
                # If not rewarded but gate opens, weakly store/update hypothesis
                wm_action[s] = a
                wm_strength[s] += (1.0 - wm_strength[s]) * (0.2 - 0.1 * is_older)

            # If repeated non-reward on same hypothesis, reduce strength further
            if (r < 0.5) and (wm_action[s] == a):
                wm_strength[s] *= (1.0 - (0.2 + 0.1 * (nS / 6.0)))

            wm_strength[s] = min(max(wm_strength[s], 0.0), 1.0)

        total_logp += logp

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Error-adaptive learning rate with age- and load-modulated volatility, plus softmax choice.

    Idea
    - Model-free Q-learning where the learning rate adapts per state based on recent prediction errors.
    - A running error trace increases the effective learning rate when recent errors are high.
    - Older adults and larger set sizes act as additional volatility, increasing sensitivity to errors
      but also adding mild choice noise through a dynamic temperature scaling.
    - No explicit perseveration; adaptation is through eta_s(t).

    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int
        Binary feedback (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial/block (3 or 6).
    age : array-like
        Participant age; age[0] used to define age group.
    model_parameters : tuple/list
        (eta0, inv_temp, err_gain, age_load_gain)
        - eta0: baseline learning rate (0..1) before adaptation.
        - inv_temp: base inverse temperature (>0).
        - err_gain: scaling from error trace to learning rate increase.
        - age_load_gain: modulation of both learning rate and temperature by age and set size.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta0, inv_temp, err_gain, age_load_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        # Error trace per state (EMA of absolute PE)
        err_trace = np.zeros(nS)

        # Volatility factor from age and load: higher for older and larger set sizes
        load = (nS - 3) / 3.0  # 0 for 3-set, 1 for 6-set
        vol_factor = 1.0 + age_load_gain * (is_older + 0.5 * load)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Dynamic inverse temperature: higher volatility -> lower effective beta
            beta_eff = inv_temp / (1.0 + 0.5 * vol_factor * (0.5 + err_trace[s]))
            prefs = beta_eff * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_a = 1.0 / max(denom, 1e-12)
            logp += np.log(max(p_a, 1e-12))

            # Compute PE and update learning rate for this state
            pe = r - Q[s, a]
            # Update error trace (EMA), faster when load is large
            trace_decay = 0.7 - 0.2 * load  # keep in (0.3..0.7)
            err_trace[s] = trace_decay * err_trace[s] + (1.0 - trace_decay) * abs(pe)

            # Effective learning rate for this state/time
            eta_eff = eta0 + err_gain * err_trace[s]
            # Age/load increase nominal eta but cap at 1
            eta_eff *= min(1.5, 1.0 + 0.5 * (vol_factor - 1.0))
            eta_eff = min(max(eta_eff, 0.0), 1.0)

            Q[s, a] += eta_eff * pe

        total_logp += logp

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian hypothesis testing blended with RL, with age- and load-dependent arbitration.

    Idea
    - For each state, maintain a simple Dirichlet-multinomial belief over which action is correct.
      Belief counts update with reward: rewarded actions increase their count; non-reward softly
      redistributes mass to alternatives.
    - Run a parallel model-free RL learner.
    - Arbitration weight favoring Bayesian belief increases in small set sizes and in younger adults;
      older adults and larger loads shift weight toward RL.
    - Final choice is a softmax over a convex combination of belief policy and RL values with a lapse.

    Parameters
    ----------
    states : array-like of int
        State indices per trial.
    actions : array-like of int
        Chosen actions per trial (0..2).
    rewards : array-like of int
        Binary outcomes (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per block (3 or 6).
    age : array-like
        Participant age; age[0] used to determine age group.
    model_parameters : tuple/list
        (alpha_rl, beta, w0_bayes, k_arbitration, lapse)
        - alpha_rl: Q-learning rate.
        - beta: inverse temperature for softmax over combined preferences.
        - w0_bayes: baseline arbitration weight on Bayesian belief (0..1).
        - k_arbitration: sensitivity of arbitration to age and load.
        - lapse: lapse probability mixed with uniform choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_rl, beta, w0_bayes, k_arbitration, lapse = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    # Helper functions
    def clip_prob(p):
        return min(max(p, 1e-12), 1.0 - 1e-12)

    def inv_logit(x):
        return 1.0 / (1.0 + np.exp(-x))

    # Convert baseline weight to logit space for smooth modulation
    w0 = clip_prob(w0_bayes)
    w0_logit = np.log(w0) - np.log(1.0 - w0)

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize RL values and Bayesian counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        counts = np.ones((nS, nA))  # symmetric Dirichlet(1,1,1) prior

        # Arbitration term: favor Bayes for smaller sets and younger adults
        load_term = (3.0 / float(nS)) - 0.5  # +0.5 for 3-set, 0.0 for 6-set
        age_term = -0.5 * is_older          # younger -> +0, older -> negative shift
        arb_logit = w0_logit + k_arbitration * (load_term + age_term)
        w_bayes = inv_logit(arb_logit)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Bayesian policy: posterior mean over actions
            post = counts[s, :] / np.sum(counts[s, :])
            # Map belief to a pseudo-preference using log probabilities for softmax compatibility
            belief_prefs = np.log(np.maximum(post, 1e-12))

            # RL preferences
            rl_prefs = Q[s, :]

            # Combine preferences (convex combination in preference space)
            prefs = beta * (w_bayes * belief_prefs + (1.0 - w_bayes) * rl_prefs)
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_choice = 1.0 / max(denom, 1e-12)
            p_final = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            logp += np.log(max(p_final, 1e-12))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha_rl * pe

            # Bayesian count updates
            if r > 0.5:
                counts[s, a] += 1.0
            else:
                # On non-reward, softly increase alternatives to reflect "this action less likely"
                penalty = 0.5
                counts[s, a] = max(1.0, counts[s, a] - penalty)
                redistribute = penalty
                for aa in range(nA):
                    if aa != a:
                        counts[s, aa] += redistribute / (nA - 1)

        total_logp += logp

    return -total_logp