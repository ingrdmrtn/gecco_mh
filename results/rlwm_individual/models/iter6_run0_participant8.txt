def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and set-sizeâ€“modulated perseveration and lapse mixture.

    Mechanism
    - Tabular Q-learning with softmax choice.
    - Perseveration: tendency to repeat the last chosen action in the same state.
      Strength decreases with larger set size and with age.
    - Lapse: small probability of random choice, increasing with set size and age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). Negative values treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age in years as a single-element array; older group if >45.
    model_parameters : iterable of 5 floats
        alpha     : RL learning rate (0..1).
        beta      : Inverse temperature for softmax (>0).
        kappa0    : Baseline perseveration strength (>0).
        lapse0    : Baseline lapse rate in [0,1].
        age_slope : Age modulation of both perseveration (negative) and lapse (positive); can be +/-.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    alpha, beta, kappa0, lapse0, age_slope = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    nA = 3

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # last chosen action per state

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials: uniform likelihood, no learning
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Perseveration bias vector (favor last action in this state)
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = 1.0

            # Effective perseveration strength: decreases with set size and with age
            # kappa_eff = kappa0 * (3/ss) * (1 - age_slope*is_older) clipped at >=0
            kappa_eff = kappa0 * (3.0 / max(ss, 1.0)) * max(0.0, 1.0 - age_slope * is_older)

            # Softmax with perseveration as additive bias to logits
            q = Q[s, :]
            m = np.max(beta * q + kappa_eff * bias)
            expv = np.exp((beta * q + kappa_eff * bias) - m)
            p_choice = expv / np.sum(expv)

            # Lapse mixture: epsilon increases with set size and with age
            # eps = lapse0 * (ss/6) * (1 + age_slope*is_older), clipped to [0, 0.5]
            eps = lapse0 * (ss / 6.0) * (1.0 + max(0.0, age_slope) * is_older)
            eps = np.clip(eps, 0.0, 0.5)

            p_mix = (1.0 - eps) * p_choice + eps * (np.ones(nA) / nA)
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update perseveration memory
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Hybrid RL + Bayesian Working Memory with limited capacity (LRU) and age-modulated WM noise.

    Mechanism
    - RL: tabular Q-learning with softmax.
    - WM: for up to K states (capacity), maintain Dirichlet counts over actions, updated by feedback.
      For states beyond capacity, use LRU replacement. Age increases WM noise via extra pseudo-count.
    - Arbitration: weight given by WM certainty (1 - normalized entropy) scaled by capacity coverage.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). Negative values treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age in years as a single-element array; older group if >45.
    model_parameters : iterable of 5 floats
        alpha     : RL learning rate (0..1).
        beta      : Inverse temperature for RL softmax (>0).
        K0        : WM capacity in number of states (>=0, can be fractional).
        cons      : Consolidation increment to WM counts after reward (>=0).
        age_noise : Additional WM pseudo-count per action for older group; scaled by age distance.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    alpha, beta, K0, cons, age_noise = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    # Age-based noise magnitude: increases with distance from 45 in decades, only if older
    noise_mag = max(0.0, (age_val - 45.0) / 10.0) * age_noise * is_older

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))

        # WM structures
        wm_counts = np.ones((nS, nA))  # start with symmetric Dirichlet(1,...,1) baseline
        wm_active = np.zeros(nS, dtype=bool)
        wm_recency = np.zeros(nS)  # larger means more recent
        recency_tick = 0.0
        capacity = int(np.floor(min(K0, nS)))  # number of states WM can actively maintain

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])  # used implicitly via nS and capacity

            # Invalid trials: uniform likelihood, no learning
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Ensure WM capacity management (LRU replacement)
            recency_tick += 1.0
            if not wm_active[s]:
                if np.sum(wm_active) < capacity:
                    wm_active[s] = True
                    wm_counts[s, :] = 1.0  # reset to uninformative prior for a newly stored state
                else:
                    # Replace least recent active state
                    active_idx = np.where(wm_active)[0]
                    lru = active_idx[np.argmin(wm_recency[active_idx])]
                    wm_active[lru] = False
                    wm_counts[s, :] = 1.0
                    wm_active[s] = True
            wm_recency[s] = recency_tick

            # RL softmax policy
            q = Q[s, :]
            m = np.max(beta * q)
            expv = np.exp(beta * q - m)
            p_rl = expv / np.sum(expv)

            # WM predictive distribution over actions (posterior mean of Dirichlet)
            if wm_active[s]:
                # Add age-dependent noise pseudo-counts
                counts = wm_counts[s, :] + noise_mag
                p_wm = counts / np.sum(counts)

                # WM certainty via entropy
                entropy = -np.sum(p_wm * np.log(np.clip(p_wm, 1e-12, 1.0)))
                entropy_max = np.log(nA)
                certainty = 1.0 - (entropy / max(entropy_max, 1e-12))  # in [0,1]
            else:
                p_wm = np.ones(nA) / nA
                certainty = 0.0

            # Arbitration weight scales with both certainty and capacity coverage
            coverage = 0.0 if capacity <= 0 else min(1.0, capacity / float(nS))
            w = np.clip(certainty * coverage, 0.0, 1.0)

            p_mix = w * p_wm + (1.0 - w) * p_rl
            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # RL learning
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update: reinforce chosen action; reward-driven consolidation
            if wm_active[s]:
                # Base observation update
                wm_counts[s, a] += 1.0
                # Additional consolidation on reward
                if r > 0.0:
                    wm_counts[s, a] += cons
                # Normalize growth a bit to avoid explosion (optional soft normalization)
                scale = np.sum(wm_counts[s, :]) / (nA + cons + 1.0)
                if scale > 1e3:  # crude taming
                    wm_counts[s, :] /= scale

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and age-modulated novelty exploration bonus.

    Mechanism
    - Q-learning with replacing eligibility traces across state-actions.
    - Novelty bonus: transient additive bonus to the current state's action values based on
      inverse visit count, encouraging exploration; scaled up in larger set sizes.
    - Age effect: older adults show reduced trace persistence (lower lambda) and modified
      exploration bonus.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). Out-of-range treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Feedback (0/1 typical). Negative values treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age in years as a single-element array; older group if >45.
    model_parameters : iterable of 5 floats
        alpha       : Learning rate for TD updates (0..1).
        beta        : Inverse temperature (>0).
        lambda_tr   : Trace decay parameter (0..1), baseline for younger.
        novelty     : Base magnitude of novelty bonus (>=0).
        age_explore : Age modulation of both trace and novelty (can be +/-).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    alpha, beta, lambda_tr, novelty, age_explore = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    nA = 3

    total_log_p = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Eligibility traces per (state, action)
        E = np.zeros((nS, nA))
        # Visit counts per state to drive novelty
        visits = np.zeros(nS) + 1e-6  # avoid div by zero

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            # Invalid trials: uniform likelihood, no learning or trace updates
            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Age-modulated trace decay (older -> lower effective lambda)
            lam_eff = np.clip(lambda_tr * (1.0 - 0.5 * age_explore * is_older), 0.0, 1.0)

            # Novelty bonus for current state: larger in larger set sizes; age modulates strength
            # bonus = novelty * (1/sqrt(visits[s])) * (ss/6) * (1 + age_explore*(1-is_older) - age_explore*is_older)
            size_scale = ss / 6.0
            age_scale = 1.0 + age_explore * (1.0 - 2.0 * is_older)  # increase for younger if age_explore>0, decrease for older
            bonus_mag = novelty * size_scale * age_scale / np.sqrt(visits[s])
            bonus_vec = np.zeros(nA) + bonus_mag  # uniform bonus to all actions in the state
            # Optionally, slight extra for least tried actions could be added, but keep within param budget

            # Softmax with bonus on current state
            logits = beta * (Q[s, :] + bonus_vec)
            m = np.max(logits)
            expv = np.exp(logits - m)
            p = expv / np.sum(expv)

            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # TD error and eligibility trace updates (replacing traces)
            # Decay traces
            E *= lam_eff
            # Set current SA trace to 1 (replacing)
            E[s, :] *= 0.0
            E[s, a] = 1.0

            # TD update
            delta = r - Q[s, a]
            Q += alpha * delta * E

            # Increment visits for state
            visits[s] += 1.0

    return -float(total_log_p)