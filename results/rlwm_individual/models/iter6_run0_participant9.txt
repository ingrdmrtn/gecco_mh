def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility trace + WM mixture with set-size logistic arbitration.

    Idea:
    - RL: delta-rule learning with an eligibility trace (within-block), allowing credit to
      propagate to recent state-action pairs.
    - WM: fast, decay-prone table that stores rewarded associations.
    - Arbitration: WM weight decreases with set size via a logistic function, capturing load effects.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight0: Base scale for WM contribution (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 for sensitivity.
    - wm_decay: Per-trial WM decay toward uniform (0..1).
    - lambda_et: Eligibility trace decay (0..1), applied per trial; gamma is implicitly 1.
    - ns_alpha: Set-size sensitivity for WM arbitration; larger values reduce WM weight faster with nS.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay, lambda_et, ns_alpha = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # highly deterministic WM channel
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])
        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Eligibility trace table
        e = np.zeros((nS, nA))

        # Set-size dependent arbitration (logistic falloff with nS)
        # Larger nS -> smaller wm_weight
        wm_weight = wm_weight0 / (1.0 + np.exp(ns_alpha * (nS - 3.0)))
        wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (from template trick: probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax policy for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with eligibility trace
            delta = r - Q_s[a]
            # Decay eligibility traces and increment for the chosen state-action
            e *= lambda_et
            e[s, a] += 1.0
            q += lr * delta * e

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # If rewarded, reinforce the chosen association in WM for this state
            if r > 0.5:
                # Move the state's WM toward a one-hot on the chosen action by allocating the decay mass
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-weighted WM arbitration + RL; WM with decay; lapse probability.

    Idea:
    - RL: standard delta-rule learning.
    - WM: fast, decaying table; encodes rewarded associations.
    - Arbitration: trial-wise WM weight scales with WM certainty (margin between top-2 WM action values)
      and penalizes larger set sizes with a power-law factor.
    - Lapse: small probability to choose uniformly at random, independent of value signals.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight0: Base WM weight scaling (0..1).
    - wm_decay: WM decay toward uniform each trial (0..1).
    - phi_ns: Set-size penalty exponent; WM weight multiplied by (3/nS)^phi_ns.
    - lapse: Lapse rate mixed with uniform policy (0..0.2 typical).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, wm_decay, phi_ns, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        size_penalty = (3.0 / max(1, nS)) ** max(0.0, phi_ns)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty-weighted WM arbitration:
            # Compute WM reliability as margin between best and second-best WM values in this state.
            sorted_W = np.sort(W_s)[::-1]
            wm_margin = max(0.0, sorted_W[0] - sorted_W[1]) if nA >= 2 else 0.0
            wm_weight = wm_weight0 * wm_margin * size_penalty
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with decay, plus reward-gated strengthening
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Time- and set-size-gated episodic WM with aging + RL.

    Idea:
    - RL: standard delta-rule.
    - WM: episodic cache-like table that strongly encodes the last rewarded action for each state,
      but the effective WM weight decays with the time since last reward for that state (aging).
    - Arbitration weight combines a base term, a set-size bonus for small sets, and a penalty for aging.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight0: Baseline WM weight (0..1).
    - wm_learn: WM learning rate to move W_s toward a one-hot when rewarded (0..1).
    - ns_bias: Multiplier for (3 - nS); positive values boost WM when nS is small, reduce when large.
    - time_decay: Penalty per trial since last reward for that state; larger means faster WM down-weighting.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, wm_learn, ns_bias, time_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track "age" since last reward for each state
        age = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # State-specific WM weight with aging and set size
            # Base + set-size term + aging penalty
            setsize_term = ns_bias * (3.0 - nS)
            aging_penalty = time_decay * age[s]
            wm_weight = wm_weight0 + setsize_term - aging_penalty
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Aging increases for all states; reset on reward for that state
            age += 1.0
            if r > 0.5:
                age[s] = 0.0
                # Move WM for this state toward a one-hot on chosen action with wm_learn
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
            else:
                # Gentle return toward uniform when not rewarded (forgetting)
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p