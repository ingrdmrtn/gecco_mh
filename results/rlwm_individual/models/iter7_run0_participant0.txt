def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity-gated precision and Hebbian WM updates.

    Mechanism:
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: a fast, Hebbian-like store that encodes the most recent correct action
      per state as a near one-hot distribution (row w[s,:]), but its precision is
      capacity-gated by set size. When set size exceeds the capacity_units, WM
      becomes less precise (lower temperature) and forgets more to uniform.
    - Policy: mixture of WM and RL with fixed wm_weight (from template).

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] mixture weight for WM in the policy.
    - softmax_beta: RL inverse temperature (internally scaled by 10).
    - capacity_units: effective WM capacity (in number of states). Higher than nS
      boosts WM precision; lower than nS reduces it and increases forgetting.
    - gating_bias: bias term shifting the capacity gating function (sigmoid).
    - hebb_rate: [0,1] Hebbian WM learning rate for reinforcing the chosen action
      in WM upon feedback (with slight anti-Hebbian weakening when r=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_units, gating_bias, hebb_rate = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity gating factor g in [0,1]: higher when nS <= capacity_units
        # g controls WM precision and the amount of WM forgetting.
        x = (float(capacity_units) - float(nS)) + float(gating_bias)
        g = 1.0 / (1.0 + np.exp(-x))  # sigmoid

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / max(np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))), 1e-12)

            # WM policy: precision scaled by capacity gating g
            W_s = w[s, :].copy()
            beta_wm_eff = softmax_beta_wm * g
            p_wm = 1.0 / max(np.sum(np.exp(beta_wm_eff * (W_s - W_s[a]))), 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting toward uniform increases when capacity is exceeded (g small)
            decay = 1.0 - g  # in [0,1]
            w = (1.0 - decay) * w + decay * w_0

            # Hebbian WM update on the chosen state
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.0:
                # Strengthen chosen action representation
                w[s, :] = (1.0 - hebb_rate) * w[s, :] + hebb_rate * one_hot
            else:
                # Mild anti-Hebbian: weaken chosen action a and softly renormalize
                weaken = hebb_rate * 0.5
                row = w[s, :].copy()
                row[a] = (1.0 - weaken) * row[a]
                # redistribute the removed mass uniformly among non-chosen options
                loss = w[s, a] - row[a]
                for a2 in range(nA):
                    if a2 != a:
                        row[a2] += loss / (nA - 1)
                w[s, :] = row

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + resource-rational WM with load-dependent precision, interference, and resets.

    Mechanism:
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: precision declines with set size per a resource parameter (resource_k).
      WM suffers load-dependent interference blending toward uniform, and a
      deterministic "reset" toward baseline that increases with load.
    - Policy: mixture of WM and RL with fixed wm_weight (from template).

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] mixing weight of WM in the policy.
    - softmax_beta: RL inverse temperature (internally scaled by 10).
    - resource_k: positive resource parameter; larger resource_k buffers WM precision
      against increases in set size.
    - interference_p: [0,1] magnitude of load-driven interference toward uniform.
    - reset_prob: [0,1] expected per-trial reset strength toward baseline (scaled
      by overload), implemented deterministically as a blend each trial.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, resource_k, interference_p, reset_prob = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load metrics
        max_set = max(1, int(np.max(set_sizes[blocks == b]))) if np.any(blocks == b) else nS
        load_ratio = float(nS) / float(max(1, max_set))  # ~ 0.5 for nS=3 when max_set=6, else 1
        overload = max(0.0, (nS - 3.0) / max(1.0, (max_set - 3.0))) if max_set > 3 else 0.0

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / max(np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))), 1e-12)

            # WM policy: precision decreases with load via resource_k
            # Higher resource_k keeps temperature high; small resource_k reduces it as nS grows
            beta_wm_eff = softmax_beta_wm / (1.0 + float(nS) / max(1e-6, float(resource_k)))

            # Interference: blend WM toward uniform as load grows
            interf = np.clip(interference_p * load_ratio, 0.0, 1.0)
            W_s = (1.0 - interf) * w[s, :] + interf * (1.0 / nA) * np.ones(nA)

            p_wm = 1.0 / max(np.sum(np.exp(beta_wm_eff * (W_s - W_s[a]))), 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Deterministic expected reset toward baseline increases with overload
            reset_rate = np.clip(reset_prob * overload, 0.0, 1.0)
            w = (1.0 - reset_rate) * w + reset_rate * w_0

            # WM learning: load-dependent rate (weaker when nS is large vs resource_k)
            wm_alpha = 0.5 / (1.0 + float(nS) / max(1e-6, float(resource_k)))
            wm_alpha = np.clip(wm_alpha, 0.0, 1.0)

            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.0:
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot
            else:
                # Small corrective move away from the chosen action when r=0
                row = w[s, :].copy()
                decrease = wm_alpha * 0.25
                row[a] = (1.0 - decrease) * row[a]
                # redistribute mass uniformly to non-chosen actions
                loss = w[s, a] - row[a]
                for a2 in range(nA):
                    if a2 != a:
                        row[a2] += loss / (nA - 1)
                w[s, :] = row

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + context-binding WM with similarity-based generalization and decay.

    Mechanism:
    - RL: delta-rule with softmax policy (beta scaled by 10).
    - WM: maintains per-state action distributions w[s,:] but also binds across
      states using a similarity kernel over state indices. At decision time,
      the effective WM policy for state s blends its own memory with a
      similarity-weighted average of other states, capturing interference and
      generalization in larger set sizes. WM also decays toward uniform.
    - Policy: mixture of WM and RL with fixed wm_weight (from template).

    Parameters:
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] mixing weight of WM in the policy.
    - softmax_beta: RL inverse temperature (internally scaled by 10).
    - binding_strength: [0,1] weight of similarity-based cross-state binding
      in the WM policy (0=no binding; 1=full reliance on similar states).
    - tau_similarity: positive scale of the similarity kernel; larger values
      produce broader generalization across states.
    - wm_decay: [0,1] base WM decay to uniform per trial (scaled by load).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, binding_strength, tau_similarity, wm_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute similarity kernel across states for this block
        S = np.zeros((nS, nS))
        for i in range(nS):
            for j in range(nS):
                dist = abs(i - j)
                S[i, j] = np.exp(-float(dist) / max(1e-6, float(tau_similarity)))
        # Normalize each row to sum to 1 (avoid self-only dominance when tau small)
        S /= np.maximum(S.sum(axis=1, keepdims=True), 1e-12)

        # Load scaling for decay
        load_scale = float(nS) / 6.0  # assuming max set size 6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / max(np.sum(np.exp(softmax_beta * (Q_s - Q_s[a]))), 1e-12)

            # Similarity-based binding for WM policy
            local = w[s, :].copy()
            neighbor = np.sum(S[s, :].reshape(-1, 1) * w, axis=0)
            W_eff = (1.0 - binding_strength) * local + binding_strength * neighbor

            p_wm = 1.0 / max(np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a]))), 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform increases with load
            d = np.clip(wm_decay * load_scale, 0.0, 1.0)
            w = (1.0 - d) * w + d * w_0

            # WM learning on the chosen state, reward-modulated
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            eta_plus = 1.0 - d  # higher effective learning when decay is smaller
            eta_minus = 0.25 * (1.0 - d)

            if r > 0.0:
                w[s, :] = (1.0 - eta_plus) * w[s, :] + eta_plus * one_hot
            else:
                row = w[s, :].copy()
                row[a] = (1.0 - eta_minus) * row[a]
                loss = w[s, a] - row[a]
                for a2 in range(nA):
                    if a2 != a:
                        row[a2] += loss / (nA - 1)
                w[s, :] = row

        blocks_log_p += log_p

    return -blocks_log_p