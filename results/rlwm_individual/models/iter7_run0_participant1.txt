def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and age- and load-modulated perseveration and learning rate.

    The model uses model-free Q-learning with replacing eligibility traces and a softmax choice rule.
    Choice probabilities are biased toward repeating the last action taken in the same state
    (perseveration). Perseveration strength is modulated by age group. Learning rate is modulated
    by cognitive load (set size).

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback received on each trial.
    blocks : array-like of int
        Block index for each trial (resets memory/values per block).
    set_sizes : array-like of int
        Set size condition at each trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0] with group split: younger (<45) vs older (>=45).
    model_parameters : list or array
        [alpha_base, beta, lambda_elig, persev_base, age_persev_slope, size_lr_gain]
        - alpha_base (0..1): Base learning rate.
        - beta (>0): Inverse temperature for softmax (scaled internally by 10).
        - lambda_elig (0..1): Eligibility trace decay factor (replacing traces).
        - persev_base (real): Baseline perseveration bias added to the last action in a state.
        - age_persev_slope (real): Additive change to perseveration for younger vs older
                                   (younger=+1, older=-1).
        - size_lr_gain (real): Exponent controlling how load scales learning rate via (3/set_size)^size_lr_gain.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_base, beta, lambda_elig, persev_base, age_persev_slope, size_lr_gain = model_parameters
    beta = max(1e-8, beta) * 10.0
    lambda_elig = np.clip(lambda_elig, 0.0, 1.0)

    age_val = age[0]
    younger = 1.0 if age_val < 45 else 0.0
    older = 1.0 - younger
    age_sign = younger - older  # +1 for younger, -1 for older

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        Q = np.zeros((nS, nA), dtype=float)
        E = np.zeros((nS, nA), dtype=float)  # eligibility traces
        last_action_state = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # Load-modulated learning rate
            load_scale = (3.0 / float(set_t)) ** size_lr_gain
            alpha = np.clip(alpha_base * load_scale, 0.0, 1.0)

            # Perseveration bias (state-specific)
            kappa = persev_base + age_persev_slope * age_sign
            bias = np.zeros(nA)
            if last_action_state[s] >= 0:
                bias[last_action_state[s]] += kappa

            # Softmax over Q plus perseveration bias
            logits = beta * (Q[s, :] - np.max(Q[s, :])) + bias
            pol = np.exp(logits - np.max(logits))
            pol = pol / (np.sum(pol) + eps)

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # TD error and replacing eligibility traces
            delta = r - Q[s, a]
            E *= lambda_elig
            E[s, :] = 0.0
            E[s, a] = 1.0

            # Q update
            Q += alpha * delta * E

            # Update last action memory
            last_action_state[s] = a

        neg_log_like += -log_p

    return float(neg_log_like)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited episodic recall with age-dependent capacity and decay.

    The model blends model-free RL with an episodic memory module that stores
    state-action associations when rewarded. The episodic system has a limited
    capacity (in slots) that depends on age group, and recall probability scales
    inversely with set size. Episodic traces decay with use and over trials.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback received on each trial.
    blocks : array-like of int
        Block index for each trial (resets memory/values per block).
    set_sizes : array-like of int
        Set size condition at each trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0] with group split: younger (<45) vs older (>=45).
    model_parameters : list or array
        [alpha, beta, cap_y, cap_o, recall_tau, decay_eps]
        - alpha (0..1): RL learning rate.
        - beta (>0): RL softmax inverse temperature (scaled internally by 10).
        - cap_y (>0): Episodic capacity (in equivalent "slots") for younger group.
        - cap_o (>0): Episodic capacity for older group.
        - recall_tau (>0): Inverse temperature for episodic policy over stored strengths.
        - decay_eps (0..1): Per-trial proportional decay of episodic strengths
                            (scaled by 1/set_size so larger sets decay faster).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, cap_y, cap_o, recall_tau, decay_eps = model_parameters
    beta = max(1e-8, beta) * 10.0
    recall_tau = max(1e-8, recall_tau)
    decay_eps = np.clip(decay_eps, 0.0, 1.0)

    age_val = age[0]
    younger = 1.0 if age_val < 45 else 0.0
    older = 1.0 - younger
    cap = cap_y * younger + cap_o * older

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        # Episodic strengths per state-action: higher means more confident memory
        M = np.zeros((nS, nA), dtype=float)

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # RL policy
            logits_rl = beta * (Q[s, :] - np.max(Q[s, :]))
            pol_rl = np.exp(logits_rl)
            pol_rl = pol_rl / (np.sum(pol_rl) + eps)

            # Episodic policy: if nothing stored, uniform; else softmax over M
            if np.all(M[s, :] <= 0.0):
                pol_epi = np.ones(nA) / nA
            else:
                logits_epi = recall_tau * (M[s, :] - np.max(M[s, :]))
                pol_epi = np.exp(logits_epi)
                pol_epi = pol_epi / (np.sum(pol_epi) + eps)

            # Recall probability scales with capacity and inversely with load
            p_recall = np.clip(cap / float(set_t), 0.0, 1.0)

            # Mixture policy
            pol = p_recall * pol_epi + (1.0 - p_recall) * pol_rl

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Episodic updates:
            # 1) Global decay scaled by load (larger set => faster decay)
            decay_factor = 1.0 - decay_eps * (1.0 / float(set_t))
            decay_factor = np.clip(decay_factor, 0.0, 1.0)
            M *= decay_factor

            # 2) If rewarded, strengthen the chosen action memory and suppress alternatives
            if r > 0.5:
                # Promote chosen association
                M[s, a] = 1.0
                # Suppress competing actions (promote selectivity)
                for aa in range(nA):
                    if aa != a:
                        M[s, aa] = min(M[s, aa], 0.0)
            else:
                # If not rewarded, slightly weaken this specific association
                M[s, a] *= 0.5

        neg_log_like += -log_p

    return float(neg_log_like)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated directed exploration via uncertainty bonus (UCB-like).

    The model uses Q-learning for reward expectation and augments action values with an
    information bonus proportional to action-specific uncertainty. Uncertainty is proxied
    by the inverse square root of visit counts, scaled by set size (higher load boosts
    exploration). Age modulates the strength of the information bonus.

    Parameters
    ----------
    states : array-like of int
        State index at each trial (0..set_size-1 within a block).
    actions : array-like of int
        Chosen action (0..2).
    rewards : array-like of float {0,1}
        Feedback received on each trial.
    blocks : array-like of int
        Block index for each trial (resets memory/values per block).
    set_sizes : array-like of int
        Set size condition at each trial (3 or 6).
    age : array-like of float
        Participant age; uses age[0] with group split: younger (<45) vs older (>=45).
    model_parameters : list or array
        [alpha, beta, info_gain, age_info_shift, size_unc_slope]
        - alpha (0..1): Q-learning rate.
        - beta (>0): Inverse temperature for softmax (scaled internally by 10).
        - info_gain (real): Baseline weight on uncertainty bonus.
        - age_info_shift (real): Additive change to info_gain for younger vs older
                                 (younger=+1, older=-1).
        - size_unc_slope (real): Exponent controlling how load-scaled uncertainty
                                 maps into the bonus.

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, info_gain, age_info_shift, size_unc_slope = model_parameters
    beta = max(1e-8, beta) * 10.0

    age_val = age[0]
    younger = 1.0 if age_val < 45 else 0.0
    older = 1.0 - younger
    age_sign = younger - older  # +1 for younger, -1 for older
    bonus_scale = info_gain + age_info_shift * age_sign

    nA = 3
    eps = 1e-12
    neg_log_like = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        block_sets = set_sizes[idx].astype(int)
        nS = int(block_sets[0])

        Q = np.zeros((nS, nA), dtype=float)
        N = np.full((nS, nA), 1e-6, dtype=float)  # visit counts to estimate uncertainty

        log_p = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            set_t = int(block_sets[t])

            # Uncertainty per action (higher when less visited)
            unc = 1.0 / np.sqrt(np.maximum(N[s, :], 1e-12))
            # Load-boosted uncertainty and exponent shaping
            load_boost = (float(set_t) / 3.0) * unc
            shaped_unc = np.power(np.clip(load_boost, 1e-12, None), size_unc_slope)

            # Directed exploration bonus
            bonus = bonus_scale * shaped_unc

            logits = beta * (Q[s, :] + bonus - np.max(Q[s, :] + bonus))
            pol = np.exp(logits)
            pol = pol / (np.sum(pol) + eps)

            p_choice = np.clip(pol[a], eps, 1.0)
            log_p += np.log(p_choice)

            # Learn from outcome
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update counts
            N[s, a] += 1.0

        neg_log_like += -log_p

    return float(neg_log_like)