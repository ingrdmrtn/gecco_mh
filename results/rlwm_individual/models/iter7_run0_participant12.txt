def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity- and recency-gated one-shot Working Memory (WM)

    Description:
    - RL: tabular Q-learning with fixed learning rate.
    - WM: one-shot storage of rewarded associations with recency decay and load-limited
      accessibility. If WM has a reliable mapping for the current state, it chooses it
      almost deterministically; otherwise it falls back on RL.
      Accessibility combines (a) recency-based memory strength and (b) capacity pressure
      from set size.
    - Mixture: fixed base mixture, but the actual contribution of WM on each trial is
      attenuated by the state-specific availability probability.

    Parameters (tuple):
    - lr0: RL learning rate (0..1).
    - mix_base: Base mixture weight scaling the WM contribution (0..1).
    - softmax_beta: RL inverse temperature; scaled internally by x10.
    - recency: Per-trial retention factor for WM traces (0..1). Higher = slower decay.
    - cap_scale: Capacity scale controlling how WM availability declines with set size (>0).
    - lapse: Lapse probability blended into WM policy toward uniform (0..0.5).
    """
    lr0, mix_base, softmax_beta, recency, cap_scale, lapse = model_parameters

    softmax_beta *= 10.0  # RL temperature scaling
    softmax_beta_wm = 50.0  # highly deterministic WM if available
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))
        # WM store: for each state, store a preferred action index and a strength in [0,1]
        # We'll encode WM preference in w as a probability vector; strength separately.
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        wm_strength = np.zeros(nS)  # 0 means nothing stored; 1 fully fresh

        # Helper: availability as function of strength and load
        # More items (nS) reduce availability via exp(-nS/cap_scale)
        load_term = np.exp(-max(1, nS) / max(eps, cap_scale))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action a
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM availability for this state combines strength and load
            avail = wm_strength[s] * load_term
            avail = np.clip(avail, 0.0, 1.0)

            # WM policy: near-deterministic to the stored action distribution, with lapse
            wm_pref = (1.0 - lapse) * W_s + lapse * w_0[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (wm_pref - wm_pref[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Mixture: effective WM weight is gated by availability
            wm_weight = np.clip(mix_base * avail, 0.0, 1.0)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr0 * delta

            # WM update:
            # - Decay the strength each time the state is encountered (recency-based)
            wm_strength[s] *= np.clip(recency, 0.0, 1.0)

            if r > 0.5:
                # Store or refresh a one-hot preference for the rewarded action
                w[s, :] = 0.0
                w[s, a] = 1.0
                wm_strength[s] = 1.0  # fully refreshed trace
            else:
                # If unrewarded and the currently stored action matches the chosen one,
                # weaken the trace; otherwise, do nothing.
                if np.argmax(W_s) == a and wm_strength[s] > 0.0:
                    wm_strength[s] *= np.clip(recency, 0.0, 1.0)
                    # soften the one-hot toward uniform as it weakens
                    w[s, :] = wm_strength[s] * w[s, :] + (1.0 - wm_strength[s]) * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with load-dependent value leak + WM error-tracking policy

    Description:
    - RL: tabular Q-learning with a load-dependent leak toward the uninformative prior
      (uniform). Larger set sizes increase leak, modeling interference under load.
    - WM: maintains an error-tracking bias per state: recent errors penalize the
      corresponding action; recent rewards give it a bonus. WM policy is a softmax over
      these biases, acting as a fast-acting heuristic to avoid repeated mistakes and
      exploit recent successes.
    - Mixture: fixed weight between WM and RL.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM policy (0..1).
    - softmax_beta: RL inverse temperature; scaled internally by x10.
    - leak_base: Base leak per item/load (0..1). 0=no leak, 1=full pull to uniform.
    - load_mult: Multiplier on set-size effect (>0). Effective leak = 1 - (1 - leak_base)^(nS*load_mult).
    - err_bias: Magnitude of WM bias change after feedback (>0).
    """
    lr, wm_weight, softmax_beta, leak_base, load_mult, err_bias = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM bias table: starts neutral (uniform). We'll treat w as a logit-like preference vector.
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute load-dependent leak toward uniform
        leak_eff = 1.0 - (1.0 - np.clip(leak_base, 0.0, 1.0)) ** max(1.0, nS * max(load_mult, eps))
        leak_eff = np.clip(leak_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy from error-tracking biases
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with leak toward uniform prior each visit
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # Apply leak to entire state's Q-values on each trial with that state
            q[s, :] = (1.0 - leak_eff) * q[s, :] + leak_eff * (1.0 / nA)

            # WM update: reinforce success, penalize error, then renormalize to a prob. vector
            if r > 0.5:
                w[s, a] += err_bias
            else:
                w[s, a] -= err_bias
            # Keep values centered by subtracting mean then softmax-like normalization
            # Convert arbitrary scores to probabilities via a softmax with high beta:
            w_centered = w[s, :] - np.mean(w[s, :])
            w_probs = np.exp(softmax_beta_wm * (w_centered - np.max(w_centered)))
            w_probs = w_probs / max(eps, np.sum(w_probs))
            w[s, :] = w_probs

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with UCB-like uncertainty bonus + WM directed exploration (novelty seeking)

    Description:
    - RL: tabular Q-learning augmented with an uncertainty (novelty) bonus based on
      inverse square root of visit counts per (state, action), encouraging exploration
      when uncertain. This affects only the policy, not the value update.
    - WM: tracks counts of action usage within each state and favors lesser-tried
      actions (novelty). WM policy temperature is load-dampened: higher set sizes
      flatten WM policy.
    - Mixture: fixed mixture between WM and RL.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - mix: Mixture weight for WM policy (0..1).
    - softmax_beta: RL inverse temperature; scaled internally by x10.
    - ucb_bonus: Scale of the uncertainty bonus added to RL preferences (>0).
    - wm_temp: Base inverse temperature for WM novelty softmax (>0).
    - load_shift: Load sensitivity scaling that reduces WM sharpness with set size (>=0).
    """
    lr, mix, softmax_beta, ucb_bonus, wm_temp, load_shift = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # will be overridden effectively by load
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        # WM uses counts; we store them in w as pseudo-counts initialized at 1 (Dirichlet-like)
        w = np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # RL visit counts for UCB bonus
        counts = np.ones((nS, nA))  # start at 1 to avoid div-by-zero

        # Load-dampened WM inverse temperature
        wm_beta_eff = wm_temp / (1.0 + load_shift * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # UCB-like bonus: bonus = ucb_bonus / sqrt(N_sa)
            bonus = ucb_bonus / np.sqrt(np.maximum(1.0, counts[s, :]))
            Q_aug = Q_s + bonus

            # RL policy using augmented preferences
            denom_rl = np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM novelty policy: prefer lesser-tried actions (lower counts)
            # Transform counts into preferences: smaller count => larger preference
            inv_counts = 1.0 / np.maximum(1.0, w[s, :])
            # Use an adjustable beta; if wm_beta_eff is very small, approach uniform
            if wm_beta_eff <= 0:
                p_wm = 1.0 / nA
            else:
                pref = inv_counts
                denom_wm = np.sum(np.exp(wm_beta_eff * (pref - pref[a])))
                p_wm = 1.0 / max(eps, denom_wm)

            p_total = mix * p_wm + (1.0 - mix) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update (no bonus in the update, only in policy)
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: increment usage counts irrespective of reward
            w[s, a] += 1.0

            # Update RL counts for UCB
            counts[s, a] += 1.0

        blocks_log_p += log_p

    return -blocks_log_p