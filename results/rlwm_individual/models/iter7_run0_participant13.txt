def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with size-sensitive WM precision and forgetting.

    Policy: A mixture of RL and WM. RL uses a softmax over Q-values (beta scaled by 10).
    WM uses a softmax over a working-memory matrix w, but its effective precision drops
    as set size increases. The mixture weight is wm_weight (provided by the template).
    
    WM update: Each trial, WM leaks toward a uniform prior (forgetting), then is updated
    toward a one-hot code for the chosen action. Positive feedback yields a stronger,
    more selective update than negative feedback. Both forgetting and size impact how
    sharp WM remains.

    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1); larger values favor WM policy.
        softmax_beta : float
            RL inverse temperature (internally scaled by 10).
        wm_forgetting : float
            Per-trial WM forgetting rate (0-1). Higher means faster decay of WM to uniform.
        size_scaler : float
            Controls how strongly WM precision decreases with set size: effective WM beta
            is divided by (1 + size_scaler * max(0, nS-3)).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_forgetting, size_scaler = model_parameters

    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Reduce WM precision as set size increases
            size_penalty = 1.0 + size_scaler * max(0.0, float(nS) - 3.0)
            eff_beta_wm = softmax_beta_wm / size_penalty
            p_wm = 1.0 / np.sum(np.exp(eff_beta_wm * (W_s - W_s[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Global forgetting/leak toward uniform
            w = (1.0 - wm_forgetting) * w + wm_forgetting * w_0
            # 2) Reward-dependent write into WM at state s
            #    Stronger, more selective write on reward; softer on no-reward
            alpha_pos = 1.0
            alpha_neg = 0.3
            alpha = alpha_pos if r > 0 else alpha_neg
            # Move row s toward a one-hot for action a
            w[s, :] = (1.0 - alpha) * w[s, :]
            w[s, a] += alpha
            # Normalize row to keep a distribution
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with PE-gated WM writing and size-dependent WM noise.

    Policy: Mixture of RL and WM. RL softmax as usual. WM policy is computed from a noisy
    WM representation: the WM row is blended with uniform noise whose strength increases
    with set size, degrading WM in larger-NS conditions.

    WM update: Writing into WM is gated by the absolute RL prediction error |PE|.
    When feedback is surprising (large |PE|), WM writes more strongly (salient trial).
    An eligibility parameter shapes how selective the WM update is (how much non-chosen
    actions are suppressed vs. preserved).

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1).
        softmax_beta : float
            RL inverse temperature (internally scaled by 10).
        wm_eligibility : float
            In [0,1]. Higher values make WM updates more selective to the chosen action
            (stronger suppression of unchosen actions).
        wm_noise_scale : float
            Scales the increase in WM noise with set size; higher values make WM less
            reliable at larger set sizes.
        pe_gate : float
            Sensitivity of WM write strength to absolute prediction error; controls how
            strongly surprise boosts WM encoding.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_eligibility, wm_noise_scale, pe_gate = model_parameters

    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Add size-dependent noise to WM representation
            size_factor = max(0.0, float(nS) - 3.0) / 3.0  # 0 at 3, ~1 at 6
            wm_noise = np.clip(wm_noise_scale * size_factor, 0.0, 0.95)
            W_noisy = (1.0 - wm_noise) * W_s + wm_noise * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_noisy - W_noisy[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM write strength is gated by surprise (|PE|) via a smooth sigmoid
            pe = float(abs(delta))
            gate = 1.0 / (1.0 + np.exp(-pe_gate * (pe - 0.5)))  # near 0 for small PE, ~1 for large PE

            # Eligibility controls suppression of unchosen actions
            # Move the row toward one-hot on a, with strength = gate
            w[s, :] = (1.0 - gate) * w[s, :] + gate * ((1.0 - wm_eligibility) * w[s, :])
            w[s, a] += gate * wm_eligibility

            # Normalize row
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM mixture with rehearsal-driven WM sharpening (chunking) and size cost.

    Policy: Mixture of RL and WM. WM precision is reduced by set size (size_cost),
    while within-trial rehearsal (chunking) sharpens the currently stored WM distribution
    by amplifying its current mode, simulating chunking/rehearsal that is more effective
    in smaller set sizes.

    WM update: After a basic reward-dependent write, apply a rehearsal-driven sharpening
    step that increases the peakiness of the WM distribution at the visited state.
    The sharpening intensity is scaled down by set size so that it is more impactful
    in small-N conditions.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Mixture weight on WM policy (0-1).
        softmax_beta : float
            RL inverse temperature (internally scaled by 10).
        chunking_rate : float
            Strength of rehearsal-driven sharpening (0-1).
        rehearsal_gain : float
            Additional gain applied to sharpening following rewarded trials.
        size_cost : float
            Penalty on WM precision with larger set size: WM beta is divided by
            (1 + size_cost * max(0, nS-3)).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, chunking_rate, rehearsal_gain, size_cost = model_parameters

    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM precision reduced by set size
            eff_beta_wm = softmax_beta_wm / (1.0 + size_cost * max(0.0, float(nS) - 3.0))
            p_wm = 1.0 / np.sum(np.exp(eff_beta_wm * (W_s - W_s[a])))
            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Basic reward-dependent write
            base_alpha = 0.4 if r <= 0 else 0.9
            w[s, :] = (1.0 - base_alpha) * w[s, :]
            w[s, a] += base_alpha
            w[s, :] /= np.sum(w[s, :])

            # Rehearsal-driven sharpening (chunking): amplify the current mode
            # Stronger after reward, and scaled down by set size
            mode_a = int(np.argmax(w[s, :]))
            size_scale = 1.0 / (1.0 + max(0.0, float(nS) - 3.0))  # more effect at small set size
            sharpen_strength = chunking_rate * size_scale * (1.0 + rehearsal_gain * (1.0 if r > 0 else 0.0))
            if sharpen_strength > 0.0:
                w[s, :] = (1.0 - sharpen_strength) * w[s, :]
                w[s, mode_a] += sharpen_strength
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p