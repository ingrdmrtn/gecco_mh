def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with capacity- and confidence-gated arbitration.

    Idea:
    - Model-free RL with a single learning rate and softmax decision rule.
    - WM is a fast associative store updated toward a one-hot policy when rewarded and
      softly diffused otherwise.
    - Arbitration weight for WM is gated by two factors:
        (a) an explicit capacity factor that penalizes WM under higher set size,
        (b) a state-specific confidence signal (from recent reward history).
      The effective WM precision further modulates WM's softmax determinism.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_capacity: float >= 0, effective WM capacity in number of items; reduces WM weight when nS > wm_capacity.
    - wm_precision: float >= 0, scales WM inverse temperature (higher = more precise WM policy).
    - mix_base: float in [0,1], base mixture weight for WM before capacity/confidence gating.
    - conf_decay: float in [0,1], rate at which WM confidence updates toward recent rewards and decays otherwise.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_capacity, wm_precision, mix_base, conf_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # State-specific confidence that WM mapping is reliable; initialized at chance.
        conf = 0.5 * np.ones(nS)

        # Capacity penalty as a function of load
        cap_factor = min(1.0, wm_capacity / max(1.0, float(nS)))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (precision-scaled)
            beta_wm_eff = softmax_beta_wm * max(0.0, wm_precision)
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # Confidence gating: map conf[s] in [0,1] to [0,1] via sigmoid-like shaping
            # Use a smooth S-shaped transform without extra parameters:
            conf_gate = conf[s] ** 2 / (conf[s] ** 2 + (1 - conf[s]) ** 2 + 1e-12)

            wm_weight = cap_factor * mix_base * conf_gate
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward strengthens one-hot mapping; non-reward softly diffuses toward uniform
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                # Fast overwrite blended by conf-dependent learning: higher confidence -> stronger write
                write_gain = 0.5 + 0.5 * conf[s]
                w[s, :] = (1.0 - write_gain) * w[s, :] + write_gain * target
            else:
                # Diffuse toward uniform slightly (small forgetting)
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # Confidence update toward recent reward outcome with decay
            conf[s] = (1.0 - conf_decay) * conf[s] + conf_decay * r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration and global WM forgetting.

    Idea:
    - Model-free RL with a single learning rate and softmax decision rule.
    - WM is a fast associative store that writes on reward and softly forgets otherwise.
    - Arbitration is dynamic and state-wise: it depends on the difference in entropy
      between RL and WM policies at the current state, as well as a load bias.
      If WM is more certain (lower entropy), its weight increases, and vice versa.
    - WM globally forgets toward uniform each trial, controlled by a forgetting rate.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - arb_sensitivity: float >= 0, scales influence of entropy difference on mixing.
    - load_bias: float (can be negative), additive bias on WM weight per item beyond 3.
    - wm_learn: float in [0,1], the WM learning rate when reward is received.
    - wm_forget: float in [0,1], global WM forgetting rate each trial toward uniform.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, arb_sensitivity, load_bias, wm_learn, wm_forget = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute load-dependent bias term
        load_term = load_bias * max(0.0, float(nS) - 3.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Global WM forgetting
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            # Compute full softmax distribution for entropy
            logits_rl = softmax_beta * (Q_s - np.max(Q_s))
            probs_rl = np.exp(logits_rl) / np.sum(np.exp(logits_rl))
            p_rl = probs_rl[a]

            # WM policy
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            probs_wm = np.exp(logits_wm) / np.sum(np.exp(logits_wm))
            p_wm = probs_wm[a]

            # Entropy-based arbitration (lower entropy => higher weight)
            H_rl = -np.sum(probs_rl * np.log(probs_rl + 1e-12))
            H_wm = -np.sum(probs_wm * np.log(probs_wm + 1e-12))
            # Weight toward WM increases if WM entropy is lower than RL
            wm_weight = 1.0 / (1.0 + np.exp(-arb_sensitivity * (H_rl - H_wm) - load_term))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                # Mild drift toward uniform when not rewarded (in addition to global forgetting above)
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with novelty bonus + WM with load-dependent binding errors.

    Idea:
    - RL includes an additive novelty bonus for low-visit state-action pairs, encouraging exploration.
    - WM stores recent rewarded mappings but suffers binding errors that increase with set size
      (i.e., sometimes the WM policy is applied to the wrong state). We approximate this by mixing
      the WM policy with uniform noise in a load-dependent manner.
    - The final policy is a mixture of WM (with binding error) and RL.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight: float in [0,1], baseline contribution of WM in the mixture.
    - novelty_bonus: float >= 0, strength of novelty added to RL Q for less-visited pairs.
    - bind_base: float in [0,1], baseline binding error rate, scaled upward by set size.
    - wm_learn: float in [0,1], learning rate for WM when reward is received.

    Returns:
    - Negative log-likelihood of the observed actions under the model.
    """
    lr, softmax_beta, wm_weight, novelty_bonus, bind_base, wm_learn = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Count visits to implement novelty bonus in RL
        visits = np.zeros((nS, nA))

        # Binding error increases with load; saturates toward 1 as nS grows
        bind_err = np.clip(bind_base * (float(nS) / max(1.0, float(nS) + 2.0)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # Novelty bonus: add higher bonus to less-visited actions in the state
            bonus_s = novelty_bonus / (1.0 + visits[s, :])
            logits_rl = softmax_beta * (Q_s + bonus_s - np.max(Q_s + bonus_s))
            probs_rl = np.exp(logits_rl) / np.sum(np.exp(logits_rl))
            p_rl = probs_rl[a]

            # WM policy for current state
            W_s = w[s, :]
            logits_wm = softmax_beta_wm * (W_s - np.max(W_s))
            probs_wm = np.exp(logits_wm) / np.sum(np.exp(logits_wm))
            # Binding error approximation: with probability bind_err, WM acts as if mismatched -> uniform
            probs_wm_bound = (1.0 - bind_err) * probs_wm + bind_err * (1.0 / nA)
            p_wm = probs_wm_bound[a]

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Increment visits for novelty accounting
            visits[s, a] += 1.0

            # WM update: reward writes mapping; non-reward diffuses slightly
            if r > 0:
                target = np.copy(w_0[s, :])
                target[a] = 1.0
                target = target / np.sum(target)
                w[s, :] = (1.0 - wm_learn) * w[s, :] + wm_learn * target
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p