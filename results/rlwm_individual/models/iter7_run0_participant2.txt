def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with adaptive arbitration by RL uncertainty and set size, and WM delta-encoding.

    Idea:
    - RL learns action values with a standard delta rule and softmax policy.
    - WM stores a fast, prototypical mapping: after reward, it moves toward a one-hot code for the chosen action;
      after no-reward, it relaxes toward uniform (to avoid perseveration).
    - Arbitration between RL and WM is adaptive:
        wm_weight = sigmoid(k0 + k1*(3 - nS) + k2*H_rl)
      where H_rl is the entropy of the RL softmax (higher when RL is uncertain).
      This captures more WM reliance at smaller set sizes and when RL is uncertain.

    Parameters (model_parameters): 6 params
    - lr: (0,1) RL learning rate.
    - softmax_beta: >0 RL inverse temperature (internally scaled x10 for dynamic range).
    - wm_alpha: (0,1) WM encoding/relaxation step size (fast delta-like update per trial).
    - k0: real. Baseline arbitration bias toward WM (positive favors WM).
    - k1: real. Set-size arbitration slope (positive increases WM for smaller sets).
    - k2: real. Uncertainty arbitration slope (positive increases WM when RL entropy is high).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_alpha, k0, k1, k2 = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            # Full RL softmax for entropy
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = p_rl_vec[a]

            # RL entropy (uncertainty), normalized by log(nA)
            H_rl = -np.sum(p_rl_vec * (np.log(p_rl_vec + eps)))
            H_rl /= np.log(nA)

            # WM policy (deterministic softmax over WM values)
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = p_wm_vec[a]

            # Adaptive arbitration by set size and RL uncertainty
            wm_weight = 1.0 / (1.0 + np.exp(-(k0 + k1 * (3 - nS) + k2 * H_rl)))
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # Move toward one-hot for rewarded action; toward uniform for no-reward
            target = np.copy(w_0[s, :])
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target
            # Keep normalized (numerical safety)
            w[s, :] = np.clip(w[s, :], 0.0, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, recency-weighted WM with load-dependent storage and decay.

    Idea:
    - RL as standard delta rule + softmax.
    - WM behaves like a recency-weighted pointer to the last rewarded action per state,
      with a strength m[s] in [0,1] that decays over time and grows upon successful reward.
    - Storage probability decreases with set size: store_prob = sigmoid(cap_strength * (1 - nS/6)).
    - WM policy is near-deterministic over a mixture: w[s] = m[s]*onehot(last_rewarded) + (1-m[s])*uniform.
    - Mixture between RL and WM is controlled by wm_mix (constant across trials).

    Parameters (model_parameters): 5 params
    - lr: (0,1) RL learning rate.
    - softmax_beta: >0 RL inverse temperature (internally scaled x10).
    - wm_mix: (0,1) Mixture weight for WM relative to RL.
    - cap_strength: real. Governs how rapidly storage probability falls with larger set sizes.
    - decay_tau: (0,1) Base decay rate of WM strength per trial; effective decay scales with set size.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_mix, cap_strength, decay_tau = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # WM strength per state and last rewarded action memory
        m = np.zeros(nS)  # strength in [0,1]
        last_rewarded = -1 * np.ones(nS, dtype=int)

        # Load-dependent decay per step
        eff_decay = np.clip(decay_tau * (nS / 6.0), 0.0, 0.999)

        # Storage probability as a function of set size (lower for larger nS)
        store_prob = 1.0 / (1.0 + np.exp(-cap_strength * (1.0 - nS / 6.0)))
        store_prob = np.clip(store_prob, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Build WM policy from current m[s] and last rewarded
            if last_rewarded[s] >= 0:
                w[s, :] = (1.0 - m[s]) * w_0[s, :]
                w[s, last_rewarded[s]] += m[s]
            else:
                w[s, :] = w_0[s, :]

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = p_wm_vec[a]

            # Mixture
            wm_weight = np.clip(wm_mix, 0.0, 1.0)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM strength decay (load-dependent)
            m[s] *= (1.0 - eff_decay)

            # WM storage/update upon reward, with load-dependent storage probability
            if r > 0.5:
                # Store with probability store_prob by increasing m toward 1
                m[s] += (1.0 - m[s]) * store_prob
                last_rewarded[s] = a
            else:
                # No reward: slight weakening toward 0 to avoid perseveration
                m[s] *= (1.0 - 0.5 * eff_decay)

            # Keep w normalized next time (already constructed from convex mixture)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM retrieval noise increasing with set size.

    Idea:
    - RL uses separate learning rates for positive vs negative outcomes and a softmax policy.
    - WM stores a running associative map updated toward the chosen action when rewarded.
    - Retrieval from WM is noisy and more imprecise under higher set size:
        W_retrieved = (1 - gamma)*w[s] + gamma*uniform with gamma = sigmoid(noise_gain*(nS-3))
      yielding a near-uniform WM policy at high load.
    - Choices are a mixture of RL and (noisy) WM policies with constant wm_mix.

    Parameters (model_parameters): 5 params
    - rl_alpha_pos: (0,1) RL learning rate for rewarded outcomes.
    - rl_alpha_neg: (0,1) RL learning rate for non-rewarded outcomes.
    - softmax_beta: >0 RL inverse temperature (internally scaled x10).
    - wm_mix: (0,1) Mixture weight for WM policy.
    - noise_gain: real. Controls how quickly WM retrieval noise rises with set size.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    rl_alpha_pos, rl_alpha_neg, softmax_beta, wm_mix, noise_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent retrieval noise parameter gamma in [0,1]
        gamma = 1.0 / (1.0 + np.exp(-noise_gain * (nS - 3.0)))
        gamma = np.clip(gamma, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            Q_shift = Q_s - np.max(Q_s)
            expQ = np.exp(softmax_beta * Q_shift)
            p_rl_vec = expQ / np.sum(expQ)
            p_rl = p_rl_vec[a]

            # WM retrieval with load-dependent noise
            W_clean = w[s, :]
            W_retrieved = (1.0 - gamma) * W_clean + gamma * w_0[s, :]

            # WM policy from noisy retrieval
            W_shift = W_retrieved - np.max(W_retrieved)
            expW = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = expW / np.sum(expW)
            p_wm = p_wm_vec[a]

            # Mixture
            wm_weight = np.clip(wm_mix, 0.0, 1.0)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL asymmetric update
            alpha = rl_alpha_pos if r > 0.5 else rl_alpha_neg
            delta = r - Q_s[a]
            q[s, a] += alpha * delta

            # WM update: Hebbian increment on reward, mild decay otherwise
            if r > 0.5:
                # Push toward one-hot of chosen action
                w[s, :] = 0.9 * w[s, :]  # mild compression
                w[s, a] += 0.1
            else:
                # Slight relaxation toward uniform if non-rewarded
                w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

            # Renormalize
            w[s, :] = np.clip(w[s, :], 0.0, None)
            w_sum = np.sum(w[s, :])
            if w_sum > 0:
                w[s, :] /= w_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p