def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, age-sensitive working memory (WM) arbitration with WM decay.

    Mechanism:
    - RL: tabular Q-learning per state with softmax action selection.
    - WM: stores the last rewarded action per state. WM retrieval may fail due to decay
      that increases with load (set size) and age.
    - Arbitration: mixture of WM policy and RL policy. The WM weight is a capacity term
      w_cap = min(1, C / set_size), where C depends on age group (younger vs. older).
      When WM has a valid binding and retrieval succeeds, it proposes a deterministic action.
      Otherwise, WM policy is uniform.
    - Age: influences both the WM capacity (C) and WM decay with load.

    Parameters (model_parameters):
    - alpha: RL learning rate (0..1)
    - beta: inverse temperature for RL softmax (>0)
    - cap_y: WM capacity for younger group (in number of items; >0)
    - cap_o: WM capacity for older group (in number of items; >0)
    - decay_k: WM decay rate constant (>0); effective p_forget = 1 - exp(-decay_k * load * age_factor)

    Inputs:
    - states: array of state indices (0..set_size-1 within block)
    - actions: array of chosen actions (integers; valid 0..2). Invalid actions handled as lapses.
    - rewards: array of rewards (typically 0/1). Invalid trials (e.g., action <0) are ignored in learning.
    - blocks: array of block indices
    - set_sizes: array of set size for each trial (3 or 6)
    - age: array-like with one element (participant's age in years)
    - model_parameters: list/tuple [alpha, beta, cap_y, cap_o, decay_k]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    alpha, beta, cap_y, cap_o, decay_k = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Initialize RL and WM structures
        Q = np.zeros((nS, nA))
        # WM binding: stores last rewarded action; -1 means no binding
        wm_bind = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load_t = float(b_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM retrieval with decay depending on load and age
            # Higher load and being older increase forgetting probability
            age_factor = 1.0 + 0.5 * is_older
            p_forget = 1.0 - np.exp(-decay_k * load_t * age_factor)
            have_binding = 1.0 if wm_bind[s] >= 0 else 0.0
            retrieved = have_binding * (1.0 - p_forget)

            if have_binding > 0.0 and retrieved > 0.0:
                p_wm = np.zeros(nA)
                p_wm[int(wm_bind[s])] = 1.0
            else:
                p_wm = np.ones(nA) / nA  # no reliable WM

            # Capacity-based arbitration weight (age-sensitive)
            C = cap_o if is_older > 0.5 else cap_y
            w_cap = min(1.0, max(0.0, C / max(1.0, load_t)))
            # Effective WM weight additionally conditioned on retrieval success
            w_eff = w_cap * retrieved

            # Final policy
            p = w_eff * p_wm + (1.0 - w_eff) * p_rl

            # Likelihood and updates
            if a < 0 or a >= nA:
                # Invalid action: treat as lapse/uniform; no learning
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                continue

            p_a = p[a]
            nll -= np.log(max(p_a, eps))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update: store last rewarded action
            if r > 0.0:
                wm_bind[s] = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Actorâ€“Critic with load-driven forgetting and age-sensitive exploration temperature.

    Mechanism:
    - Policy is represented by state-action preferences H(s,a); action selection via softmax.
    - Critic V(s) estimates state value. TD error drives both actor and critic.
    - Load-driven forgetting: higher set size increases preference decay toward zero each trial.
    - Age alters effective exploration: higher load interacts with an age bias to reduce inverse
      temperature more strongly for older adults.
    - All learning occurs only on valid actions.

    Parameters (model_parameters):
    - alpha_c: critic learning rate (0..1)
    - alpha_a: actor learning rate (0..1)
    - invtemp: base inverse temperature for softmax (>0)
    - rho_forget: base forgetting rate for preferences (>0); also scales load impact on temperature
    - age_temp_bias: age sensitivity of temperature scaling (>=0); affects only older group

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as in model1.
    - model_parameters: [alpha_c, alpha_a, invtemp, rho_forget, age_temp_bias]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha_c, alpha_a, invtemp, rho_forget, age_temp_bias = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        H = np.zeros((nS, nA))  # preferences
        V = np.zeros(nS)        # state values

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r_raw = b_rewards[t]
            # Map rewards to [0,1] for learning; invalid negative rewards are treated as 0
            r = 1.0 if r_raw > 0 else 0.0
            load_t = float(b_set_sizes[t])

            # Effective inverse temperature decreases with load and more so for older adults
            temp_scale = 1.0 + rho_forget * (load_t / 6.0) * (1.0 + age_temp_bias * is_older)
            invtemp_eff = invtemp / temp_scale

            # Policy
            prefs = H[s, :] - np.max(H[s, :])
            p = np.exp(invtemp_eff * prefs)
            p = p / (np.sum(p) + eps)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # No learning on invalid action
                continue

            p_a = p[a]
            nll -= np.log(max(p_a, eps))

            # TD error
            delta = r - V[s]
            # Critic update
            V[s] += alpha_c * delta

            # Actor update (compatible function approximation for softmax policy gradient)
            for aa in range(nA):
                if aa == a:
                    H[s, aa] += alpha_a * delta * (1.0 - p[aa])
                else:
                    H[s, aa] -= alpha_a * delta * p[aa]

            # Load-driven forgetting of preferences (toward 0, i.e., more stochasticity)
            decay = min(0.99, rho_forget * (load_t / 6.0))
            H[s, :] *= (1.0 - decay)

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian evidence accumulation over "correct action" with leak, plus load- and age-sensitive lapses.

    Mechanism:
    - For each state, maintains Dirichlet pseudo-counts over the three actions representing
      beliefs that an action is the correct one.
    - Update rule:
        * If reward=1: add evidence to the chosen action.
        * If reward=0: add distributed counter-evidence to the non-chosen actions.
      Between trials, counts leak toward the symmetric prior, with a leak rate that increases
      with set size and age.
    - Choice: softmax over expected correctness probabilities from the Dirichlet posterior.
    - Lapse: uniform random responding increases with load and age.

    Parameters (model_parameters):
    - prior_k: symmetric Dirichlet prior concentration per action (>0)
    - beta: inverse temperature for softmax (>0)
    - leak_base: base leak rate toward prior per trial (0..1)
    - age_load_slope: scales how much load increases leak for older adults (>=0)
    - lapse0: base lapse rate (0..1), modulated by load and age

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age as above.
    - model_parameters: [prior_k, beta, leak_base, age_load_slope, lapse0]

    Returns:
    - Negative log-likelihood of observed choices.
    """
    prior_k, beta, leak_base, age_load_slope, lapse0 = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]

        nS = int(b_set_sizes[0])

        # Initialize Dirichlet counts per state
        counts = np.ones((nS, nA)) * prior_k

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r_raw = b_rewards[t]
            r = 1.0 if r_raw > 0 else 0.0
            load_t = float(b_set_sizes[t])

            # Leak toward prior: counts <- (1 - leak)*counts + leak*prior
            # Leak increases with load; older adults have stronger load-leak slope
            leak = np.clip(leak_base * (1.0 + (load_t / 6.0) * (1.0 + age_load_slope * is_older)), 0.0, 0.99)
            counts[s, :] = (1.0 - leak) * counts[s, :] + leak * prior_k

            # Posterior mean over actions
            probs_corr = counts[s, :] / (np.sum(counts[s, :]) + eps)
            logits = beta * (probs_corr - np.max(probs_corr))
            p_soft = np.exp(logits)
            p_soft = p_soft / (np.sum(p_soft) + eps)

            # Lapse increases with load and age
            lapse = np.clip(lapse0 * (load_t / 6.0) * (1.0 + 0.5 * is_older), 0.0, 0.49)
            p = (1.0 - lapse) * p_soft + lapse * (np.ones(nA) / nA)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

                # Evidence update
                if r > 0.0:
                    counts[s, a] += 1.0
                else:
                    # distribute weak counter-evidence to non-chosen actions
                    for aa in range(nA):
                        if aa != a:
                            counts[s, aa] += 1.0 / (nA - 1)

    return nll