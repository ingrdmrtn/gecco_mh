def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: Actor-Critic with eligibility traces and age/set-size modulated choice temperature.
    
    Mechanism:
      - Actor-Critic learner per block with preferences H[s,a] (actor) and state values V[s] (critic).
      - Eligibility traces on actor preferences to capture within-state credit assignment.
      - Decision temperature (inverse) is reduced under higher cognitive load (set size = 6) and further
        reduced in older adults, capturing noisier choices under load and aging.
    
    Parameters (model_parameters):
      alpha_v : float
          Critic learning rate in [0,1] for state value V.
      alpha_h : float
          Actor learning rate in [0,1] for policy preferences H.
      beta0 : float
          Baseline inverse temperature (>0) before modulation.
      lam : float
          Eligibility trace decay lambda in [0,1] for actor preferences.
      load_temp_boost : float
          Positive factor that increases temperature (i.e., reduces inverse temperature) with set size.
      age_temp_boost : float
          Positive factor that increases temperature (reduces inverse temperature) for older adults.
    
    Inputs:
      states : array-like (T,)
          State index per trial (0..nS-1 within each block).
      actions : array-like (T,)
          Chosen action per trial (0..2).
      rewards : array-like (T,)
          Binary rewards (0/1).
      blocks : array-like (T,)
          Block index per trial.
      set_sizes : array-like (T,)
          Set size per trial in its block (3 or 6).
      age : array-like or scalar
          Participant age; older group defined as age >= 45.
    
    Returns:
      nll : float
          Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12
    alpha_v, alpha_h, beta0, lam, load_temp_boost, age_temp_boost = model_parameters

    # Determine age group
    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize actor (preferences) and critic (values)
        H = np.zeros((nS, nA))
        V = np.zeros(nS)

        # Eligibility traces for actor preferences
        E = np.zeros((nS, nA))

        # Set-size factor: 0 for 3, 1 for 6
        ss_factor = (nS - 3) / 3.0

        # Effective inverse temperature decreases with load and in older adults
        beta_eff = beta0 / (1.0 + max(0.0, load_temp_boost) * ss_factor + max(0.0, age_temp_boost) * is_older)
        beta_eff = max(beta_eff, eps)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Policy from actor preferences
            pref = H[s, :].copy()
            pref = pref - np.max(pref)  # stabilize
            p = np.exp(beta_eff * pref)
            p = p / (np.sum(p) + eps)

            # Likelihood contribution
            total_logp += np.log(np.clip(p[a], eps, 1.0))

            # TD error with state value (bandit-like within state)
            delta = r - V[s]

            # Update critic
            V[s] += alpha_v * delta

            # Update eligibility traces: decay and set current (s,a) to 1
            E *= lam
            E[s, a] += 1.0

            # Actor update with eligibility
            H += alpha_h * delta * E

            # Optional: baseline correction to keep preferences centered per state
            H[s, :] -= np.mean(H[s, :])

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Q-learning mixed with a state-specific win-stay/lose-shift heuristic.
    
    Mechanism:
      - Standard Q-learning per state-action within blocks.
      - Parallel heuristic policy that repeats the last action in a state after a win (win-stay),
        and suppresses it after a loss (lose-shift). This heuristic operates on action logits.
      - The mixture weight toward the heuristic increases in younger adults and smaller set sizes
        (i.e., older adults and larger set sizes rely more on the RL policy).
    
    Parameters (model_parameters):
      eta : float
          Learning rate for Q-learning in [0,1].
      beta : float
          Inverse temperature for the RL softmax (>0).
      ws_gain : float
          Logit boost for repeating the last action after a win (win-stay strength).
      ls_gain : float
          Logit penalty for repeating the last action after a loss (lose-shift strength).
      age_ws_bias : float
          Age modulation of heuristic mixture weight; positive values increase heuristic weight in older adults.
      ss_ws_slope : float
          Set-size modulation (relative to 3) of heuristic mixture weight; positive reduces heuristic under larger set sizes.
    
    Inputs:
      states : array-like (T,)
          State index per trial (0..nS-1 within each block).
      actions : array-like (T,)
          Chosen action per trial (0..2).
      rewards : array-like (T,)
          Binary rewards (0/1).
      blocks : array-like (T,)
          Block index per trial.
      set_sizes : array-like (T,)
          Set size per trial in its block (3 or 6).
      age : array-like or scalar
          Participant age; older group defined as age >= 45.
    
    Returns:
      nll : float
          Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12
    eta, beta, ws_gain, ls_gain, age_ws_bias, ss_ws_slope = model_parameters
    beta = max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))

        # Track last action and last reward per state for heuristic
        last_act = -1 * np.ones(nS, dtype=int)
        last_rew = np.zeros(nS)

        ss_factor = (nS - 3) / 3.0

        # Mixture weight toward heuristic (logistic); tuned by age and set size
        logit_omega = 0.1 * ws_gain + age_ws_bias * is_older - ss_ws_slope * ss_factor
        omega = 1.0 / (1.0 + np.exp(-logit_omega))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Heuristic policy logits
            h_logits = np.zeros(nA)
            if last_act[s] >= 0:
                la = int(last_act[s])
                if last_rew[s] >= 0.5:
                    # Win-stay: boost last action
                    h_logits[la] += ws_gain
                else:
                    # Lose-shift: penalize last action, distribute boost to alternatives
                    h_logits[la] -= ls_gain
                    for a2 in range(nA):
                        if a2 != la:
                            h_logits[a2] += ls_gain / (nA - 1)

            # Convert heuristic logits to probabilities
            h_logits = h_logits - np.max(h_logits)
            p_h = np.exp(h_logits)
            p_h = p_h / (np.sum(p_h) + eps)

            # Mixture
            p = omega * p_h + (1.0 - omega) * p_rl
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += eta * pe

            # Update heuristic memory
            last_act[s] = a
            last_rew[s] = r

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL mixed with a Dirichlet-count "mapping memory" whose precision is modulated by age and set size.
    
    Mechanism:
      - Model-free Q-learning per state-action.
      - Parallel Bayesian-like memory that tracks action counts per state (Dirichlet parameters),
        strongly reinforcing actions that yielded reward. Larger set sizes and older age increase
        the baseline concentration (i.e., flatter, less informative priors), reducing the influence
        of the memory policy.
      - The policy is a convex combination of the RL softmax and the Dirichlet-induced categorical
        distribution (normalized counts). The mixture weight itself depends on age and set size.
    
    Parameters (model_parameters):
      alpha : float
          Learning rate for Q-learning in [0,1].
      beta : float
          Inverse temperature for RL softmax (>0).
      conc0 : float
          Baseline Dirichlet concentration per action (>0).
      age_conc_shift : float
          Additive increase in concentration for older adults (>=45); positive -> flatter memory prior.
      ss_conc_slope : float
          Additive increase in concentration with set size (relative to 3); positive -> flatter memory prior in larger sets.
      wm_mix0 : float
          Baseline logit for mixing weight toward the memory policy.
    
    Inputs:
      states : array-like (T,)
          State index per trial (0..nS-1 within each block).
      actions : array-like (T,)
          Chosen action per trial (0..2).
      rewards : array-like (T,)
          Binary rewards (0/1).
      blocks : array-like (T,)
          Block index per trial.
      set_sizes : array-like (T,)
          Set size per trial in its block (3 or 6).
      age : array-like or scalar
          Participant age; older group defined as age >= 45.
    
    Returns:
      nll : float
          Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12
    alpha, beta, conc0, age_conc_shift, ss_conc_slope, wm_mix0 = model_parameters
    beta = max(beta, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        ss_factor = (nS - 3) / 3.0

        # Dirichlet concentration per action for this block
        conc = max(conc0 + age_conc_shift * is_older + ss_conc_slope * ss_factor, eps)

        # Initialize counts with concentration (symmetric Dirichlet prior)
        C = conc * np.ones((nS, nA))
        Q = np.zeros((nS, nA))

        # Mixing weight toward memory policy; older and larger sets reduce/increase weight via wm_mix0
        logit_w = wm_mix0 - 0.5 * ss_factor + 0.5 * is_older
        w = 1.0 / (1.0 + np.exp(-logit_w))  # in (0,1)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Memory policy from Dirichlet counts (normalized)
            counts = C[s, :].copy()
            p_mem = counts / (np.sum(counts) + eps)

            # Mixture
            p = w * p_mem + (1.0 - w) * p_rl
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            total_logp += np.log(p[a])

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update Dirichlet-like counts: reward strengthens chosen action;
            # small negative evidence spreads to alternatives on non-reward.
            if r >= 0.5:
                C[s, a] += 1.0
            else:
                # Distribute a small fraction to alternatives to reflect "not-a" evidence
                leak = 0.5
                for a2 in range(nA):
                    if a2 != a:
                        C[s, a2] += leak / (nA - 1)

    return -float(total_logp)