def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + leaky, asymmetric WM with capacity-scaled gating.

    Idea:
    - RL: standard delta-rule Q-learning (fixed by template).
    - WM: a fast leaky preference table per state that:
        * decays each trial toward a uniform prior,
        * is asymmetrically boosted by rewards (stronger potentiation on wins; milder suppression on losses).
      The WM policy is a near-deterministic softmax over WM preferences.
    - Mixture: WM contribution is down-weighted in larger set sizes by a power-law capacity gate.

    Parameters:
    - lr: scalar in [0,1]. RL learning rate.
    - wm_base: scalar in [0,1]. Baseline WM mixture weight before capacity scaling.
    - softmax_beta: scalar >= 0. RL inverse temperature (internally scaled by 10).
    - wm_decay: scalar in [0,1]. Trial-wise leak of WM preferences toward uniform.
    - wm_asym_gain: scalar in [0,1]. Step size for WM asymmetric update after feedback.
                     On reward: push chosen action up by this amount; on no-reward: suppress chosen action by half this amount.
    - capacity_gamma: scalar >= 0. Exponent controlling how fast WM weight drops with set size: wm_weight = wm_base * (3/nS)^capacity_gamma.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, wm_decay, wm_asym_gain, capacity_gamma = model_parameters
    softmax_beta *= 10.0  # higher upper bound as specified
    softmax_beta_wm = 50  # very deterministic WM policy as specified
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM mixture weight
        wm_weight = wm_base * (3.0 / max(1.0, nS))**max(0.0, capacity_gamma)
        wm_weight = min(max(wm_weight, 0.0), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL choice probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (softmax over WM preferences)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM asymmetric update based on outcome
            if r >= 1.0 - 1e-12:
                # Potentiate chosen action, pull others down proportionally
                add = wm_asym_gain
                w[s, :] *= (1.0 - add)
                w[s, a] += add
            else:
                # Suppress chosen action mildly and redistribute to others
                sub = wm_asym_gain * 0.5
                give = sub / (nA - 1)
                w[s, a] = max(0.0, w[s, a] - sub)
                others = [k for k in range(nA) if k != a]
                w[s, others] += give

            # Renormalize WM row to be a proper distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency-based episodic WM with entropy- and set-size–gated arbitration.

    Idea:
    - RL: standard delta-rule Q-learning (fixed by template).
    - WM: stores a recency-weighted trace of the most recent rewarded action per state.
      On reward, WM for that state moves toward a one-hot vector at the chosen action;
      on no-reward, the trace decays toward uniform.
    - Arbitration: WM weight increases with RL uncertainty (higher policy entropy)
      and decreases with set size. Entropy is computed from the RL softmax policy.

    Parameters:
    - lr: scalar in [0,1]. RL learning rate.
    - softmax_beta: scalar >= 0. RL inverse temperature (internally scaled by 10).
    - wm_recency: scalar in [0,1]. Fractional update toward one-hot on reward; also sets decay toward uniform on no-reward.
    - wm_strength: scalar controlling slope of arbitration sigmoid with respect to entropy signal.
    - entropy_temp: scalar >= 0. Temperature applied when forming RL policy for entropy; larger temp flattens policy before entropy.
    - ss_penalty: scalar >= 0. Linear penalty on mixture as set size increases.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_recency, wm_strength, entropy_temp, ss_penalty = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy and chosen probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Build full RL policy (with entropy_temp to compute entropy)
            logits = Q_s * max(1e-12, entropy_temp)
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            prl_vec = exp_logits / np.sum(exp_logits)

            # Entropy of RL policy
            entropy = -np.sum(prl_vec * np.log(np.clip(prl_vec, 1e-12, 1.0)))

            # Set-size– and entropy-gated WM weight via sigmoid
            gate_input = wm_strength * (entropy - ss_penalty * np.log(max(1.0, nS)))
            wm_weight = 1.0 / (1.0 + np.exp(-gate_input))
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            # WM policy for chosen action
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-driven recency, otherwise decay to uniform
            if r >= 1.0 - 1e-12:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_recency) * w[s, :] + wm_recency * target
            else:
                w[s, :] = (1.0 - wm_recency) * w[s, :] + wm_recency * w_0[s, :]

            # Normalize to distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + noisy binding WM with set-size–dependent interference and precision-scaled WM temperature.

    Idea:
    - RL: standard delta-rule Q-learning (fixed by template).
    - WM: a fast delta-rule that tracks action correctness per state, but suffers from
      binding interference that grows with set size. Interference adds noise to WM values.
      WM softmax temperature is scaled by a 'binding precision' parameter.
    - Mixture: WM weight starts at wm_weight0 and decays exponentially with set size.

    Parameters:
    - lr: scalar in [0,1]. RL learning rate.
    - softmax_beta: scalar >= 0. RL inverse temperature (internally scaled by 10).
    - wm_eta: scalar in [0,1]. WM learning rate toward a one-hot target on reward and away-from target on no-reward.
    - wm_weight0: scalar in [0,1]. Baseline WM mixture weight at minimal set size.
    - interference: scalar >= 0. Scales both weight decay with set size and additive WM noise per trial.
    - binding_precision: scalar > 0. Scales WM softmax determinism: beta_wm_eff = softmax_beta_wm * binding_precision.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_eta, wm_weight0, interference, binding_precision = model_parameters
    softmax_beta *= 10.0
    base_wm_beta = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size–dependent WM mixture weight (exponential decay with set size)
        wm_weight = wm_weight0 * np.exp(-interference * max(0.0, nS - 3.0))
        wm_weight = min(max(wm_weight, 0.0), 1.0)

        # Effective WM temperature scales with binding precision
        softmax_beta_wm = base_wm_beta * max(1e-6, binding_precision)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM delta-rule toward/away from one-hot target
            if r >= 1.0 - 1e-12:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # Move slightly away from chosen action; redistribute to others
                anti = np.full(nA, 1.0 / (nA - 1))
                anti[a] = 0.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * anti

            # Add interference noise that grows with set size
            noise_scale = interference * max(0.0, nS - 3.0)
            if noise_scale > 0:
                # Zero-mean noise; keep within [0,1] after renorm
                eps = (np.random.rand(nA) - 0.5) * 2.0 * noise_scale
                w[s, :] = np.clip(w[s, :] + eps, 0.0, None)

            # Renormalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p