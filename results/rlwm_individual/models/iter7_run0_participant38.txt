def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-interference, conditional encoding, and choice-stickiness within WM.

    Mechanism:
    - RL: standard Rescorla-Wagner value learning with softmax policy (already defined in template).
    - WM: state-action probability table that encodes rewarded actions (supervised) and decays to uniform.
      Interference increases with load (nS), and a small choice-stickiness term biases WM toward the last
      chosen action to capture short-term motor/attentional persistence.
    - Arbitration: baseline WM weight is downscaled as a deterministic function of load.

    Parameters (total 6):
    - lr: RL learning rate [0,1].
    - wm_weight0: Baseline WM arbitration weight before load penalty [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - load_interf: Load interference strength (>=0). Higher values increase WM decay and reduce WM weight as nS grows.
    - wm_forget: Baseline WM decay toward uniform per trial/state in [0,1].
    - choice_stick: Magnitude of WM update toward the chosen action on every trial (small, >=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, load_interf, wm_forget, choice_stick = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute a load-dependent arbitration weight (down-weight WM under high load).
            load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
            wm_weight = wm_weight0 * np.exp(-load_interf * load_term)

            # Deterministic WM policy from current WM distribution
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-gated encoding toward one-hot of chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Stronger encoding when rewarded; interference increases decay below
                encode_rate = 1.0  # one-shot overwrite on reward
                w[s, :] = (1.0 - encode_rate) * w[s, :] + encode_rate * target
            # Decay toward uniform each trial, amplified by load interference
            decay_eff = np.clip(wm_forget + load_interf * load_term * (1.0 - r), 0.0, 1.0)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            # Choice stickiness: small supervised nudge toward the chosen action every trial
            if choice_stick > 0.0:
                stick_vec = np.zeros(nA)
                stick_vec[a] = 1.0
                w[s, :] = (1.0 - choice_stick) * w[s, :] + choice_stick * stick_vec

            # Renormalize and clip
            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with one-shot WM encoding, load-driven misbinding, and load-reduced arbitration.

    Mechanism:
    - RL: standard value learning and softmax policy (already provided).
    - WM: on reward, encodes a one-hot target but suffers misbinding noise that redistributes
      a fraction of mass uniformly to other actions. Misbinding increases with load (nS).
      On non-reward, WM mildly relaxes toward uniform.
    - Arbitration: baseline WM weight reduced linearly with load.

    Parameters (total 6):
    - lr: RL learning rate [0,1].
    - wm_weight_base: Baseline WM contribution to choice [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_one_shot: Strength of WM one-shot update on reward in [0,1].
    - misbind_gain: Scales misbinding as a function of load (>=0).
    - relax_rate: WM relaxation rate to uniform on non-reward in [0,1].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_one_shot, misbind_gain, relax_rate = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Load-reduced arbitration weight
            load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
            wm_weight = np.clip(wm_weight_base * (1.0 - load_term), 0.0, 1.0)

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # One-shot encoding with misbinding increasing with load
                misbind = np.clip(misbind_gain * load_term, 0.0, 1.0)
                target = np.zeros(nA)
                target[a] = 1.0
                # First move toward one-hot
                w[s, :] = (1.0 - wm_one_shot) * w[s, :] + wm_one_shot * target
                # Then apply misbinding by blending with uniform
                w[s, :] = (1.0 - misbind) * w[s, :] + misbind * w_0[s, :]
            else:
                # On non-reward, relax toward uniform
                w[s, :] = (1.0 - relax_rate) * w[s, :] + relax_rate * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence-based arbitration and load-amplified WM noise.

    Mechanism:
    - RL: standard Q-learning with softmax (provided).
    - WM: probability table that is updated toward one-hot on reward and decays otherwise.
      WM confidence is defined as max(W_s) - second_max(W_s); higher confidence increases arbitration.
      Load amplifies WM noise (blends toward uniform), which lowers confidence.
    - Arbitration: wm_weight_t is a logistic transform of a baseline plus a confidence term. The term
      uses a noise parameter that is explicitly amplified by load.

    Parameters (total 6):
    - lr: RL learning rate [0,1].
    - wm_weight0: Baseline WM weight before confidence modulation [0,1].
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - conf_gain: Gain on WM confidence for arbitration (can be positive).
    - decay_wm: WM decay toward uniform on non-reward [0,1].
    - noise_wm: Base WM noise strength blended each trial, amplified by load (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, conf_gain, decay_wm, noise_wm = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute WM confidence and load-amplified noise
            sorted_W = np.sort(W_s)[::-1]
            conf = float(sorted_W[0] - sorted_W[1]) if len(sorted_W) >= 2 else 0.0

            load_term = np.clip((nS - 3.0) / 3.0, 0.0, 1.0)
            noise_eff = noise_wm * (1.0 + load_term)

            # Blend WM state with uniform due to noise (reduces effective precision under load)
            W_eff = (1.0 - np.clip(noise_eff, 0.0, 1.0)) * W_s + np.clip(noise_eff, 0.0, 1.0) * w_0[s, :]

            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Confidence-based arbitration (logistic transform)
            eps = 1e-8
            base_logit = np.log(np.clip(wm_weight0, eps, 1 - eps)) - np.log(1 - np.clip(wm_weight0, eps, 1 - eps))
            wm_weight = 1.0 / (1.0 + np.exp(-(base_logit + conf_gain * (conf - noise_eff))))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Moderate supervised move toward one-hot
                alpha_w = 0.7
                w[s, :] = (1.0 - alpha_w) * w[s, :] + alpha_w * target
            else:
                w[s, :] = (1.0 - decay_wm) * w[s, :] + decay_wm * w_0[s, :]

            # Apply the same noise blending used in policy to the stored WM (to capture ongoing degradation)
            w[s, :] = (1.0 - np.clip(noise_eff, 0.0, 1.0)) * w[s, :] + np.clip(noise_eff, 0.0, 1.0) * w_0[s, :]

            w[s, :] = np.clip(w[s, :], 1e-8, 1.0)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p