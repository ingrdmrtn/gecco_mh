def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + confidence-tracking WM with set-size-limited WM arbitration.

    Mechanisms
    - RL: standard delta rule with softmax choice.
    - WM: maintains leaky "confidence" over actions per state; reward pushes toward
      a one-hot code for the chosen action, non-reward pulls toward a uniform prior.
    - Arbitration: base WM weight is downregulated by set size via a logistic
      capacity curve (fewer items -> stronger WM contribution).

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within a block).
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback per trial.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (constant within a block).
    model_parameters : tuple/list of 5 floats
        lr : RL learning rate in [0,1].
        wm_weight0 : Baseline arbitration weight for WM in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        wm_conf_leak : WM confidence leak/update step size in [0,1]; larger -> faster WM updates.
        capacity_alpha : Steepness of the logistic set-size penalty on WM; >0 reduces WM at larger set sizes.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_conf_leak, capacity_alpha = model_parameters
    softmax_beta *= 10  # higher upper bound as in template
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM weight (capacity-limited)
        # More items => smaller WM weight; centered around set size 4.
        wm_weight_block = wm_weight0 / (1.0 + np.exp(capacity_alpha * (nS - 4.0)))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action (keep stable form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over leaky confidence W_s, high precision
            # To avoid overflow, subtract chosen value as in stable form
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Use the set-size adjusted WM weight
            wm_weight = wm_weight_block

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leaky confidence update: reward -> toward one-hot; no reward -> toward uniform
            target = np.ones(nA) / nA
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            w[s, :] = (1.0 - wm_conf_leak) * w[s, :] + wm_conf_leak * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning + WM as win-stay/lose-shift and entropy-based arbitration.

    Mechanisms
    - RL: delta rule with separate learning rates for positive and negative outcomes.
    - WM: win-stay/lose-shift (WSLS) map per state implemented as a probabilistic table.
      Reward shifts WM distribution toward chosen action; non-reward shifts it toward the other actions.
    - Arbitration: dynamic, based on predicted choice entropies; WM gets higher weight when WM
      is more decisive than RL (and vice versa), controlled by arb_tau.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    model_parameters : tuple/list of 6 floats
        lr_pos : RL learning rate after reward in [0,1].
        lr_neg : RL learning rate after no-reward in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        arb_tau : Arbitration gain (>0); larger values make arbitration more sensitive to entropy differences.
        wm_ws_strength : WM WSLS update step size in [0,1].
        noise_eps : Small smoothing to avoid zero-probability WM states and stabilize entropies (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, arb_tau, wm_ws_strength, noise_eps = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: compute full WM distribution (softmax at high precision)
            # Add small noise smoothing to avoid degenerate entropies
            W_logit = softmax_beta_wm * W_s
            W_logit = W_logit - np.max(W_logit)
            wm_probs = np.exp(W_logit)
            wm_probs = wm_probs / np.sum(wm_probs)
            wm_probs = (1 - noise_eps) * wm_probs + noise_eps * (1.0 / nA)
            wm_probs = wm_probs / np.sum(wm_probs)

            p_wm = wm_probs[a]

            # Compute RL full distribution for entropy-based arbitration
            Q_logit = softmax_beta * Q_s
            Q_logit = Q_logit - np.max(Q_logit)
            rl_probs = np.exp(Q_logit)
            rl_probs = rl_probs / np.sum(rl_probs)
            rl_probs = np.clip(rl_probs, 1e-12, 1.0)

            # Entropy-based arbitration: WM gets higher weight when its entropy is lower
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            H_wm = -np.sum(wm_probs * np.log(np.clip(wm_probs, 1e-12, 1.0)))
            wm_weight = 1.0 / (1.0 + np.exp(-arb_tau * (H_rl - H_wm)))  # sigmoid of entropy difference

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s][a] += lr_eff * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WSLS update: reward -> move toward chosen action; no reward -> move toward other actions
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = np.ones(nA) / (nA - 1)
                target[a] = 0.0
            w[s, :] = (1.0 - wm_ws_strength) * w[s, :] + wm_ws_strength * target
            # Smooth to avoid exact zeros/ones, consistent with policy smoothing
            if noise_eps > 0.0:
                w[s, :] = (1 - noise_eps) * w[s, :] + noise_eps * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency- and load-sensitive WM with distraction.

    Mechanisms
    - RL: standard delta rule with softmax.
    - WM: stores a sharp code for recently rewarded actions; its effective precision
      decays with time since last visit to the state and with higher set size (distraction).
    - Arbitration: base WM weight scaled by a recency factor; when a state is recently
      visited, WM dominates; otherwise RL dominates.

    Parameters
    ----------
    states : array-like of int
    actions : array-like of int
    rewards : array-like of {0,1}
    blocks : array-like of int
    set_sizes : array-like of int
    model_parameters : tuple/list of 6 floats
        lr : RL learning rate in [0,1].
        wm_weight0 : Baseline WM arbitration weight in [0,1].
        softmax_beta : Base inverse temperature for RL (rescaled internally by *10).
        recency_lambda : Recency retention (0..1); WM recency factor is lambda^dt for dt elapsed trials.
        setsize_boundary : Set-size reference around which distraction increases (e.g., 4.0).
        distraction_rate : Strength by which larger set sizes reduce WM precision/weight (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, recency_lambda, setsize_boundary, distraction_rate = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last-visit time and last-reward time per state for recency
        last_seen = -1 * np.ones(nS, dtype=int)
        last_rewarded = -1 * np.ones(nS, dtype=int)

        # Precompute load-based distraction factor for this block
        load_factor = 1.0 / (1.0 + distraction_rate * max(0.0, nS - setsize_boundary))

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Recency factor based on time since last visit; if unseen, use 0
            if last_seen[s] >= 0:
                dt = t - last_seen[s]
                rec = recency_lambda ** dt
            else:
                rec = 0.0

            # Effective WM distribution blends stored WM with uniform due to distraction and recency
            W_eff = rec * load_factor * W_s + (1.0 - rec * load_factor) * w_0[s, :]
            # WM policy from effective WM distribution
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_eff - W_eff[a])))

            # Dynamic arbitration: recency-upweighted WM weight
            wm_weight = wm_weight0 * rec * load_factor

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Update WM: after reward, store a sharp one-hot; otherwise, slight decay toward uniform
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s, :] = onehot
                last_rewarded[s] = t
            else:
                # mild forgetting toward uniform to reflect interference
                forget_rate = min(1.0, 1.0 - load_factor)  # more forgetting under high load
                w[s, :] = (1.0 - forget_rate) * w[s, :] + forget_rate * w_0[s, :]

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p