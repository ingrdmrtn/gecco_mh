Below are three standalone cognitive models that follow your template and constraints. Each model blends reinforcement learning (RL) with a working-memory (WM) channel and makes the WM contribution depend on set size (load) in distinct ways.

Note: Assume numpy is already imported as np. Each function returns the negative log-likelihood of the observed choices.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with set-size interference.

    Idea:
    - RL: standard delta-rule with a single learning rate.
    - WM: one-shot encoding toward a one-hot associative trace upon reward,
      but storage strength is limited by capacity and suffers interference that grows with set size.
    - Arbitration: WM weight is scaled down by (capacity / set_size) and by an interference factor tied to load.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight in [0..1].
    - softmax_beta: RL inverse temperature baseline; scaled by 10 internally.
    - wm_capacity_slots: effective WM capacity (e.g., around 3-4).
    - interference_rate: increases interference with larger set sizes (>=0).
    - decay_rate: per-trial WM decay toward uniform (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_capacity_slots, interference_rate, decay_rate = model_parameters
    softmax_beta *= 10.0  # RL has higher effective inverse temperature
    softmax_beta_wm = 50.0  # near-deterministic WM policy
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Block-level load factors
        cap_effect = min(1.0, float(wm_capacity_slots) / max(1.0, float(nS)))
        load = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 at 3, up to 1 at 6
        inter_noise = np.clip(interference_rate * load, 0.0, 1.0)
        lam = np.clip(decay_rate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: reduce WM influence by capacity and interference
            wm_gate = np.clip(wm_weight * cap_effect * (1.0 - inter_noise), 0.0, 1.0)
            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - global decay toward uniform
            w = (1.0 - lam) * w + lam * w_0

            # - reward-contingent encoding with capacity/interference-limited strength
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                one_hot = one_hot / np.sum(one_hot)
                alpha_enc = np.clip(cap_effect * (1.0 - inter_noise), 0.0, 1.0)
                w[s, :] = (1.0 - alpha_enc) * w[s, :] + alpha_enc * one_hot
            else:
                # On error, apply mild smoothing away from the chosen action (interference-like)
                leak = 0.25 * inter_noise
                w[s, a] = (1.0 - leak) * w[s, a]
                w[s, :] += leak / (nA - 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning and entropy-based arbitration with WM.

    Idea:
    - RL: separate learning rates for positive and negative prediction errors.
    - WM: fast one-shot learning when rewarded; leaky forgetting otherwise.
    - Arbitration: WM reliance increases when WM policy is sharper than RL policy,
      quantified by an entropy difference, and decreases with set size.

    Parameters (tuple):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - wm_weight_base: baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature baseline; scaled by 10 internally.
    - ent_weight_gain: gain on entropy-difference gating (>=0).
    - wm_forget: per-trial WM forgetting toward uniform (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, wm_weight_base, softmax_beta, ent_weight_gain, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def softmax_probs(x, beta):
        z = beta * (x - np.max(x))
        ez = np.exp(z)
        s = ez / np.sum(ez)
        return s

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        size_penalty = 3.0 / max(3.0, float(nS))  # 1 at 3, 0.5 at 6
        lam = np.clip(wm_forget, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # Compute full policies to derive entropies
            probs_rl = softmax_probs(Q_s, softmax_beta)
            probs_wm = softmax_probs(W_s, softmax_beta_wm)

            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))
            H_wm = -np.sum(probs_wm * np.log(np.clip(probs_wm, eps, 1.0)))

            # Chosen-action probabilities (as in template)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration, scaled down by set size
            gate_signal = wm_weight_base + ent_weight_gain * (H_rl - H_wm)
            wm_gate = 1.0 / (1.0 + np.exp(-gate_signal))  # logistic in [0,1]
            wm_gate *= size_penalty  # reduce WM influence for larger set sizes
            wm_gate = np.clip(wm_gate, 0.0, 1.0)

            p_total = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # WM forgetting (toward uniform)
            w = (1.0 - lam) * w + lam * w_0

            # WM encoding: strong on reward; mild suppression of chosen action on error
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                one_hot = one_hot / np.sum(one_hot)
                w[s, :] = 0.0 * w[s, :] + one_hot  # one-shot reset for this state
            else:
                # Slightly reduce chosen action's weight to reflect error memory
                gamma = 0.1
                w[s, a] = (1.0 - gamma) * w[s, a]
                redistribute = gamma / (nA - 1.0)
                for k in range(nA):
                    if k != a:
                        w[s, k] += redistribute
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Surprise- and size-gated WM arbitration with lapse.

    Idea:
    - RL: standard delta-rule.
    - WM: fast, reward-triggered one-shot encoding; otherwise decays toward uniform.
    - Arbitration: WM reliance is a logistic function of "low surprise" (small |PE|),
      scaled down by set size. A lapse rate mixes uniform choice into the final policy.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1), before gating.
    - softmax_beta: RL inverse temperature baseline; scaled by 10 internally.
    - pe_thresh: threshold term in logistic gating; larger favors WM when |PE| is small.
    - size_penalty: scales down WM reliance with set size (>=0).
    - lapse: lapse probability mixing in uniform random choices (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, pe_thresh, size_penalty, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Initialization: unknown PE magnitude per state is high to start
        last_pe_abs = np.ones(nS)
        size_scale = np.exp(-size_penalty * max(0.0, (float(nS) - 3.0) / 3.0))  # 1 at 3, decays with size

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise- and size-gated WM arbitration
            gate_input = pe_thresh - last_pe_abs[s]
            wm_gate = wm_weight * (1.0 / (1.0 + np.exp(-gate_input)))  # higher when surprise is low
            wm_gate *= size_scale
            wm_gate = np.clip(wm_gate, 0.0, 1.0)

            p_mix = wm_gate * p_wm + (1.0 - wm_gate) * p_rl
            # Lapse: mix with uniform choice
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update and record surprise
            pe = r - q[s, a]
            q[s, a] += lr * pe
            last_pe_abs[s] = abs(pe)

            # WM update: reward-locked one-shot encoding, mild decay otherwise
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Mild decay toward uniform for the visited state
                lam_state = 0.2
                w[s, :] = (1.0 - lam_state) * w[s, :] + lam_state * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p