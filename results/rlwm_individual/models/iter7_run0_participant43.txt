def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with novelty-gated arbitration and load-modulated WM decay.

    Idea:
    - RL learns action values via delta rule and softmax.
    - WM stores a one-shot preference for the last rewarded action in a state.
    - WM decays faster under higher load (set size 6 vs 3).
    - Arbitration favors WM more when a state is novel (few visits), and favors RL more
      as the state becomes familiar and under higher load.

    Parameters:
    - lr: [0,1] RL learning rate.
    - arb_bias: baseline arbitration bias toward WM (real-valued; mapped via sigmoid).
    - softmax_beta: RL inverse temperature; internally scaled x10.
    - wm_decay: [0,1] baseline WM decay rate toward uniform; scaled up by load.
    - novelty_slope: >=0, controls how strongly novelty (1/(1+visits)) drives arbitration and encoding.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, arb_bias, softmax_beta, wm_decay, novelty_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)  # 0 for 3, 1 for 6

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        visits = np.zeros(nS)

        # Effective decay increases with load
        wm_decay_eff = np.clip(wm_decay * (1.0 + load), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Novelty-based arbitration: more WM when state is novel, less under load
            novelty = 1.0 / (1.0 + max(0.0, visits[s]))
            arb_input = arb_bias + novelty_slope * novelty - load
            wm_weight = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            # Mixture policy
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # WM encoding: if rewarded, store the chosen action more strongly when novel
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                encode_strength = np.clip(novelty_slope * novelty, 0.0, 1.0)
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * target

            visits[s] += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated arbitration and load-dependent WM precision.

    Idea:
    - RL learns via delta rule; we track state-wise uncertainty as an EMA of squared TD errors.
    - Arbitration favors WM when RL is uncertain (high recent TD error variance).
    - Under higher load, WM precision (temperature) drops.
    - WM is updated with a dedicated learning rate toward a one-hot action when rewarded,
      and decays otherwise.

    Parameters:
    - lr: [0,1] RL learning rate.
    - arb_base: baseline arbitration bias toward WM (real-valued; mapped via sigmoid).
    - softmax_beta: RL inverse temperature; internally scaled x10.
    - uncertainty_scale: (0,1] smoothing for uncertainty EMA and also scales arbitration input.
    - wm_temp_load: >=0 controls how much load reduces WM inverse temperature (precision).
    - wm_lr: [0,1] learning rate for WM updates toward one-hot after reward; also used for decay.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, arb_base, softmax_beta, uncertainty_scale, wm_temp_load, wm_lr = model_parameters
    softmax_beta *= 10
    base_wm_beta = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track uncertainty per state: EMA of squared TD errors
        uncert = np.zeros(nS)

        # Load reduces WM precision
        softmax_beta_wm_eff = base_wm_beta / (1.0 + wm_temp_load * load)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with load-reduced temperature
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm_eff * (W_s - W_s[a])))

            # Arbitration: more WM when RL uncertainty is high; less WM under higher load
            unc_term = np.sqrt(max(uncert[s], 0.0))
            arb_input = arb_base + (uncertainty_scale * unc_term) - load
            wm_weight = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Update uncertainty (EMA of squared TD error)
            uncert[s] = (1.0 - uncertainty_scale) * uncert[s] + uncertainty_scale * (delta ** 2)

            # WM update: reward-driven strengthening; otherwise decay toward uniform
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target
            else:
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with forgetting + WM chunking with load-driven competition.

    Idea:
    - RL values decay toward uniform each trial (forgetting), which can model interference across items.
    - WM encodes rewarded actions (chunking) with a gain that is reduced by load.
    - Global WM competition: under higher load, all WM states are pulled toward uniform (interference),
      reducing WM efficacy.
    - Arbitration is driven by a bias toward WM countered by load-driven competition.

    Parameters:
    - lr: [0,1] RL learning rate.
    - arb_bias: baseline arbitration bias toward WM (real-valued; mapped via sigmoid).
    - softmax_beta: RL inverse temperature; internally scaled x10.
    - rl_decay: [0,1] per-trial decay of RL Q-values toward uniform.
    - chunk_gain: [0,1] WM encoding gain applied on rewards (reduced by load).
    - load_comp: >=0 strength by which load both reduces WM gain and increases global WM interference.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, arb_bias, softmax_beta, rl_decay, chunk_gain, load_comp = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])
        load = max(0.0, (nS - 3.0) / 3.0)

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute RL decay matrix toward uniform
        rl_decay = np.clip(rl_decay, 0.0, 1.0)
        # Load-modulated WM encoding and competition
        wm_encode = np.clip(chunk_gain / (1.0 + load_comp * load), 0.0, 1.0)
        wm_competition = np.clip(load_comp * load, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply RL forgetting toward uniform before choice (global interference)
            q = (1.0 - rl_decay) * q + rl_decay * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: WM bias reduced by load-driven competition
            arb_input = arb_bias - wm_competition
            wm_weight = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with delta rule
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global competition/interference toward uniform
            w = (1.0 - wm_competition) * w + wm_competition * w_0

            # WM chunking on reward: encode chosen action strongly for the current state
            if r >= 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_encode) * w[s, :] + wm_encode * target

        blocks_log_p += log_p

    return -blocks_log_p