def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with asymmetric WM learning and load-dependent decay.

    Mechanism
    - RL: delta-rule learning with softmax policy.
    - WM: associative table W that is updated with asymmetric learning rates
      for rewarded vs unrewarded outcomes; additionally decays toward uniform with
      a rate that increases with set size (load).
    - Arbitration: fixed baseline WM weight that is attenuated by set size.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_mix_base: float
            Baseline WM mixture weight (0..1) before load attenuation.
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        alpha_wm_plus: float
            WM learning rate for rewarded actions (0..1).
        alpha_wm_minus: float
            WM learning rate for unrewarded actions (0..1).
        decay_base: float
            Baseline WM decay toward uniform per trial (0..1), scaled up by load.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_mix_base, softmax_beta, alpha_wm_plus, alpha_wm_minus, decay_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled decay and WM weight attenuation
        load_scale = max(0, nS - 3) / 3.0  # 0 at nS=3, 1 at nS=6
        decay_wm = np.clip(decay_base * (1.0 + load_scale), 0.0, 1.0)
        wm_weight_block = np.clip(wm_mix_base * (1.0 - 0.5 * load_scale), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of the observed action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM table
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_weight = wm_weight_block
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM update (asymmetric)
            if r > 0.5:
                # strengthen chosen action toward 1, reduce others proportionally
                w[s, :] = (1.0 - alpha_wm_plus) * w[s, :]
                w[s, a] += alpha_wm_plus
            else:
                # push chosen action toward uniform
                w[s, a] = (1.0 - alpha_wm_minus) * w[s, a] + alpha_wm_minus * (1.0 / nA)

            # row renormalization for numerical stability
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with valence-split learning + capacity-limited WM with interference.

    Mechanism
    - RL: separate learning rates for positive vs negative prediction errors.
    - WM: capacity-limited associative table that is more effective at small set sizes.
      Global decay toward uniform scales with both set size and an interference parameter.
    - Arbitration: WM mixture weight attenuated by load via a capacity-strength parameter.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        alpha_pos_rl: float
            RL learning rate for positive prediction errors (0..1).
        alpha_neg_rl: float
            RL learning rate for negative prediction errors (0..1).
        wm_base: float
            Baseline WM mixture weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        cap_strength: float
            Sensitivity of WM mixture to set size (higher reduces WM under larger sets).
        interference: float
            WM decay multiplier driven by load-related interference (>=0).

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    alpha_pos_rl, alpha_neg_rl, wm_base, softmax_beta, cap_strength, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM mixture weight shaped by capacity under load (sigmoid-like attenuation)
        load_term = (nS - 3) / 3.0  # 0 at nS=3, 1 at nS=6
        wm_weight_block = np.clip(wm_base / (1.0 + cap_strength * load_term), 0.0, 1.0)

        # Interference-driven decay increases with set size
        decay_wm = np.clip(1.0 - np.exp(-interference * max(0.0, load_term)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_weight = wm_weight_block
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence-split learning rates
            pe = r - Q_s[a]
            if pe >= 0:
                q[s, a] += alpha_pos_rl * pe
            else:
                q[s, a] += alpha_neg_rl * pe

            # WM decay (global interference)
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM update: rewarded trials imprint strongly, errors soften association
            if r > 0.5:
                # move row toward a one-hot on the chosen action with moderate sharpening
                imprint = 0.7
                w[s, :] = (1.0 - imprint) * w[s, :]
                w[s, a] += imprint
            else:
                # lose-shift tendency: reduce chosen action and redistribute to others
                leak = 0.3
                reduce = min(leak, w[s, a])
                w[s, a] -= reduce
                redistribute = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            # Row renormalization
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Bayesian WM (Dirichlet counts) with precision-based arbitration and lapse.

    Mechanism
    - RL: delta-rule learning with a recency bias via a choice-trace bonus added
      to Q-values when computing policy.
    - WM: per-state Dirichlet counts (stored in w), converted to probabilities W_s.
      Counts decay by a load-dependent factor. Reward increases the chosen action's
      count; non-reward adds small counts to non-chosen actions.
    - Arbitration: WM mixture weight scales with WM precision (sum of counts) and is
      penalized by load. A small lapse probability mixes in uniform choice.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_prior: float
            Initial Dirichlet prior magnitude per action (>0).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        zeta_trace: float
            Choice-trace bonus added to the previously chosen action in a state.
        wm_load_gain: float
            Strength of load-dependent WM decay and arbitration penalty.
        lapse: float
            Lapse probability mixing in uniform random choice (0..1).

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_prior, softmax_beta, zeta_trace, wm_load_gain, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12
    lapse = np.clip(lapse, 0.0, 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values and choice traces
        q = (1.0 / nA) * np.ones((nS, nA))
        trace = np.zeros((nS, nA))

        # WM counts initialized by prior
        w = (wm_prior) * np.ones((nS, nA))
        w_0 = (wm_prior) * np.ones((nS, nA))

        # Load effects: decay and arbitration penalty
        load_term = max(0, nS - 3) / 3.0  # 0..1
        decay_wm = np.clip(wm_load_gain * load_term, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Effective Q includes a choice-trace bonus
            Q_s = q[s, :] + zeta_trace * trace[s, :]
            # Convert WM counts to probabilities
            counts_s = w[s, :]
            total_counts = np.sum(counts_s)
            if total_counts <= 0:
                W_s = np.ones(3) / 3.0
            else:
                W_s = counts_s / total_counts

            # RL policy prob of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from probabilities via near-deterministic softmax
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Precision-based WM weight with load penalty
            precision = total_counts  # higher when more certain
            c_load = 1.0 + wm_load_gain * load_term
            wm_weight = precision / (precision + c_load)
            wm_weight = np.clip(wm_weight, 0.0, 1.0)

            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Update choice trace: decay others slightly and boost chosen action
            trace[s, :] *= (1.0 - 0.5)  # moderate decay of old trace
            trace[s, a] += 1.0

            # WM decay of counts toward the prior (forgetfulness)
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM update of counts
            if r > 0.5:
                # reward: add one count to chosen action
                w[s, a] += 1.0
            else:
                # non-reward: allocate small evidence to non-chosen actions
                penalty = 0.5
                add_each = penalty / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += add_each

        blocks_log_p += log_p

    return -blocks_log_p