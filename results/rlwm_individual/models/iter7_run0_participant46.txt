def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + capacity-limited WM with lapse and perseveration bias

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: an associative matrix that encodes rewarded state-action pairs using the same learning rate as RL,
      and globally decays toward uniform. Decay increases with set size due to limited capacity.
    - Arbitration: mixture of WM and RL with an effective WM weight that shrinks hyperbolically with set size.
    - Choice noise: a lapse parameter mixes the final policy with a uniform random choice.
    - Perseveration: adds a bias toward repeating the last chosen action in the current state.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate; also used as WM encoding strength for simplicity.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight_base: float in [0,1], base WM mixture weight before set-size scaling.
    - capacity_C: float > 0, WM capacity constant; larger C reduces decay and preserves WM under higher load.
                   Effective WM weight scales as wm_weight_base * C / (C + (nS-1)).
                   Decay per trial scales as (nS) / (nS + C).
    - lapse: float in [0,1], lapse probability mixing the final policy with uniform.
    - perseveration: float >= 0, strength of same-state perseveration bias (applied as additive bias on W).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, capacity_C, lapse, perseveration = model_parameters
    softmax_beta *= 10  # RL beta higher upper bound
    softmax_beta_wm = 50  # deterministic WM
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration bias
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy probability of chosen action (use template-compatible stable form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: add perseveration bias in the WM score space before softmax
            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                # Additive bias to the previously chosen action within the state
                W_s[last_action[s]] += perseveration

            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Effective WM weight shrinks with set size (hyperbolic capacity)
            wm_weight_eff = wm_weight_base * (capacity_C / (capacity_C + max(1.0, float(nS) - 1.0)))
            wm_weight_eff = float(np.clip(wm_weight_eff, 0.0, 1.0))

            # Mixture (template)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl

            # Lapse to uniform
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM global decay toward uniform; stronger decay under larger set size
            decay = float(nS) / (float(nS) + max(1e-6, capacity_C))
            decay = float(np.clip(decay, 0.0, 1.0))
            w = (1.0 - decay) * w + decay * w_0

            # WM encoding: rewarded pairs move toward one-hot; unrewarded slightly down-weighted
            if r > 0.5:
                # Move row toward one-hot on action a
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                # Penalize chosen action slightly
                w[s, a] = (1.0 - lr) * w[s, a]
                # Re-normalize the row to keep it a distribution
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

            # Update perseveration memory
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL + entropy-gated WM with load-dependent decay and set-size bias on arbitration

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: associative matrix updated toward one-hot for reward, with global decay toward uniform
      that increases with set size.
    - Arbitration: WM weight is modulated both by a base weight and by the RL policy entropy (uncertainty).
      When the RL policy is uncertain (high entropy), arbitration favors WM more. Additionally, a set-size
      bias term directly reduces WM weight as set size grows.
    - All parameters are used: entropy_slope scales the sensitivity to RL entropy; wm_decay_base controls decay;
      ss_bias penalizes WM weight as a function of set size.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate; also used as WM encoding strength.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight0: float in [0,1], base WM weight before gating.
    - entropy_slope: float >= 0, scales effect of RL policy entropy on WM weight (more entropy -> higher WM weight).
    - wm_decay_base: float in [0,1], baseline WM decay per trial; scaled up by set size.
    - ss_bias: float >= 0, set-size penalty on WM weight: wm_weight *= exp(-ss_bias*(nS-3)).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight0, entropy_slope, wm_decay_base, ss_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL softmax probs to compute entropy of current state
            logits = softmax_beta * Q_s
            # subtract max for stability
            z = logits - np.max(logits)
            expz = np.exp(z)
            rl_probs = expz / np.sum(expz)
            # Probability of chosen action
            p_rl = rl_probs[a]

            # RL entropy (natural units)
            entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration gate: increase WM weight with RL entropy, penalize with set size
            wm_weight = wm_weight0 * np.exp(-ss_bias * max(0, nS - 3))
            wm_weight *= 1.0 / (1.0 + np.exp(-entropy_slope * (entropy - np.log(nA)/2.0)))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            # Mixture (template)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform increases with set size
            decay = np.clip(wm_decay_base * (float(nS) / 3.0), 0.0, 1.0)
            w = (1.0 - decay) * w + decay * w_0

            # WM encoding
            if r > 0.5:
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            else:
                w[s, a] = (1.0 - lr) * w[s, a]
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: RL + recency- and surprise-gated WM with time-based decay and rehearsal

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: associative matrix that decays as a function of time-since-last-visit for each state (recency-based),
      updated toward one-hot on reward, and occasionally rehearsed even without reward.
    - Arbitration: WM weight is the product of a base weight, a recency factor exp(-rho_time * dt), and a surprise
      gate based on absolute RL prediction error. This makes WM strongest for recently seen and surprising items.
    - Rehearsal: with probability p_reh (treated deterministically as a strength here), we add a small boost toward
      the currently chosen action regardless of reward, capturing strategic rehearsal in low load.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight0: float in [0,1], base WM mixture weight.
    - rho_time: float >= 0, time-based WM decay rate; larger values reduce recency factor faster.
    - p_reh: float in [0,1], rehearsal strength pushing WM toward the chosen action on every visit.
    - pe_slope: float >= 0, slope of logistic surprise gate over |prediction error|.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight0, rho_time, p_reh, pe_slope = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track last time each state was seen (initialize to None -> large dt on first visit)
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute recency factor from time since last seen
            if last_seen[s] < 0:
                dt = 1e6  # effectively very old
            else:
                dt = max(1, t - last_seen[s])
            recency = np.exp(-rho_time * float(dt))

            # Surprise gate from absolute prediction error
            pe = abs(r - Q_s[a])
            surprise_gate = 1.0 / (1.0 + np.exp(-pe_slope * (pe - 0.5)))  # center near 0.5

            # Load scaling: dampen WM under higher set size
            load_scale = 3.0 / float(nS)

            wm_weight = np.clip(wm_weight0 * recency * surprise_gate * load_scale, 0.0, 1.0)

            # Mixture (template)
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Time-based decay applied globally each step toward uniform
            # Approximate by mixing entire matrix with w_0; stronger mixing if many states not recently seen
            # Here, we apply a modest global decay proportional to average recency deficit
            avg_dt = np.mean([max(1, t - last_seen[i]) if last_seen[i] >= 0 else 10 for i in range(nS)])
            global_decay = 1.0 - np.exp(-rho_time * float(avg_dt) / max(1.0, nS))
            global_decay = float(np.clip(global_decay, 0.0, 1.0))
            w = (1.0 - global_decay) * w + global_decay * w_0

            # WM encoding: rewarded move toward one-hot
            if r > 0.5:
                w[s, :] = (1.0 - lr) * w[s, :]
                w[s, a] += lr
            # Rehearsal: always add a smaller push toward the chosen action
            w[s, :] = (1.0 - p_reh) * w[s, :]
            w[s, a] += p_reh

            # Row normalization safeguard
            row_sum = np.sum(w[s, :])
            if row_sum <= 0:
                w[s, :] = w_0[s, :].copy()
            else:
                w[s, :] /= row_sum

            # Update recency for this state
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p