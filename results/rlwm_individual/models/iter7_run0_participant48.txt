def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited WM with load-dependent decay and intrusion + RL hybrid.

    Mechanism:
    - RL: tabular Q-learning with single learning rate.
    - WM: builds one-shot mappings for rewarded state-action pairs; memory decays toward uniform
      and suffers "intrusion" (blending toward uniform) when set size exceeds capacity.
    - Arbitration: fixed mixture weight (wm_weight), but its effective influence is scaled by
      capacity relative to set size (min(1, K_slots/nS)) so WM matters less under high load.

    Parameters:
    - lr: RL learning rate in [0,1].
    - wm_weight: baseline WM mixture weight in [0,1].
    - softmax_beta: RL inverse temperature; scaled internally by 10 for numerical range.
    - decay_base: base WM decay rate per trial in [0,1]; effective decay increases with load.
    - K_slots: WM capacity (effective number of state-action pairs) in [0,6].
    - intrude_gain: additional uniform intrusion when nS > K_slots (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, decay_base, K_slots, intrude_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-scaled WM decay and capacity scaling
        cap_scale = min(1.0, max(1e-6, K_slots) / float(nS))
        # Translate base decay to an effective decay that increases as K_slots < nS
        wm_forget = 1.0 - (1.0 - decay_base) ** max(1.0, (float(nS) / max(1e-6, K_slots)))
        # Intrusion toward uniform when beyond capacity
        intrusion = intrude_gain * max(0.0, (float(nS) - K_slots)) / max(1.0, float(nS))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (high precision softmax)
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration with capacity scaling
            omega_eff = wm_weight * cap_scale
            p_total = omega_eff * p_wm + (1.0 - omega_eff) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            # WM encoding: after reward, set a strong one-hot trace with possible intrusion
            if r > 0.0:
                new_row = np.zeros(nA)
                new_row[a] = 1.0
                # intrusion blends the memory toward uniform when capacity is exceeded
                w[s, :] = (1.0 - intrusion) * new_row + intrusion * (1.0 / nA)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Dual-learning-rate RL with load-gated WM and valence-dependent WM editing.

    Mechanism:
    - RL: tabular Q-learning with separate learning rates for positive/negative outcomes.
    - WM:
        * Decays toward uniform; decay accelerates with set size.
        * Positive feedback: store a near-deterministic one-hot mapping for (state, chosen).
        * Negative feedback: "anti-store" by reducing weight on the chosen action (push toward
          competing actions via a normalized redistribution).
    - Arbitration: WM mixture weight is attenuated as set size increases (load-gated).

    Parameters:
    - lr_pos: RL learning rate for rewards (0..1).
    - lr_neg: RL learning rate for non-rewards (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10).
    - wm_decay: base WM decay per trial (0..1), load-amplified with set size.
    - gate_slope: load gating slope (>=0) controlling how much load reduces WM influence.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, gate_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-dependent WM decay and arbitration gating
        wm_forget = 1.0 - (1.0 - wm_decay) ** max(1, nS)
        omega_eff = wm_weight / (1.0 + gate_slope * max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            p_total = omega_eff * p_wm + (1.0 - omega_eff) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update with dual learning rates
            pe = r - q[s, a]
            if pe >= 0.0:
                q[s, a] += lr_pos * pe
            else:
                q[s, a] += lr_neg * pe

            # WM global decay toward uniform
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            # WM editing by outcome valence
            if r > 0.0:
                # Reward: store one-hot for chosen action
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # Non-reward: push probability mass away from chosen action
                # Set chosen action low and redistribute mass uniformly to the others
                anti = np.ones(nA) / (nA - 1)
                anti[a] = 0.0
                w[s, :] = anti

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Error-triggered WM arbitration with load-sensitive WM decay.

    Mechanism:
    - RL: standard Q-learning.
    - WM: stores rewarded state-action pairs; decays toward uniform with rate that increases with set size.
    - Arbitration: baseline WM mixture weight (wm_weight) is dynamically boosted by an error-trace
      that accumulates after non-rewarded choices within each state, then decays over time.
      This reflects strategic reliance on WM following recent failures, especially under low load.

    Parameters:
    - lr: RL learning rate (0..1).
    - wm_weight: baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10).
    - err_gain: scaling of error-trace contribution to WM weight (>=0).
    - wm_decay: base WM decay per trial (0..1) for WM values; load-amplified.
    - ss_sensitivity: load sensitivity (>0) modulating both WM decay and error-trace decay.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, err_gain, wm_decay, ss_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-wise error traces
        e_trace = np.zeros(nS)

        # Load-sensitive WM decay and error-trace decay
        wm_forget = 1.0 - (1.0 - wm_decay) ** max(1.0, ss_sensitivity * nS)
        err_decay = (1.0 - 0.5) ** max(1.0, ss_sensitivity * (nS - 1))  # faster decay under higher load

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration with error-triggered boost
            omega_eff = wm_weight + err_gain * e_trace[s]
            omega_eff = min(1.0, max(0.0, omega_eff))
            p_total = omega_eff * p_wm + (1.0 - omega_eff) * p_rl
            log_p += np.log(p_total + 1e-15)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            # WM encoding on reward only
            if r > 0.0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update error trace for the visited state and decay globally
            if r <= 0.0:
                e_trace[s] = min(1.0, e_trace[s] + 1.0)  # accumulate after error
            # global decay of traces each trial
            e_trace *= err_decay

        blocks_log_p += log_p

    return -blocks_log_p