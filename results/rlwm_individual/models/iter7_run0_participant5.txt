def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Fast WM with reward-gated learning and set-size- plus time-driven forgetting.

    Mechanism
    - RL: standard delta-rule Q-values with softmax.
    - WM: distribution over actions per state that sharpens quickly after reward and
      decays toward uniform with a rate that increases with set size and an overall time leak.
      WM also learns weakly from non-reward (toward uniform) to reduce false caches.
    - Arbitration: fixed base WM mixture weight (wm_weight) but effective WM precision is
      controlled by its own learning and forgetting dynamics.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - wm_lr: float in [0,1]
            WM learning rate toward a one-hot after reward; toward uniform after no-reward.
        - forget_base: float in [0,1]
            Base WM forgetting rate toward uniform; scaled up by set size.
        - time_leak: float in [0,1]
            Additional global WM leak per trial irrespective of set size.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_lr, forget_base, time_leak = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size and time-driven forgetting each step
        base_decay = np.clip(forget_base * (nS / 6.0) + time_leak, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM preferences (here the probability vector itself acts as preferences)
            prefs_wm = W_s - np.mean(W_s)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_wm, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Global WM decay toward uniform (set-size and time dependent)
            w = (1 - base_decay) * w + base_decay * w_0

            # Reward-gated WM learning:
            # - if rewarded: move distribution toward one-hot on chosen action
            # - if not rewarded: move toward uniform (unlearn wrong cache)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :]

            w[s, :] = (1 - wm_lr) * w[s, :] + wm_lr * target
            # Normalize defensively
            w[s, :] = np.clip(w[s, :], eps, None)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with state-wise choice stickiness + WM with PE-driven forgetting.

    Mechanism
    - RL: standard delta-rule with softmax applied to Q-values augmented by a
      state-wise stickiness bonus for the last chosen action in that state.
    - WM: rapidly updates toward the rewarded action; if an outcome is surprising
      (large unsigned prediction error), WM decays more (to reduce reliance on incorrect caches).
      Forgetting also scales with set size.
    - Arbitration: fixed wm_weight mixture between RL and WM policies.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_weight: float in [0,1]
            Mixture weight of WM policy in final choice probability.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - stickiness: float >= 0
            Additive bonus to the last chosen action within a state for RL policy.
        - wm_lr: float in [0,1]
            WM learning rate toward rewarded one-hot.
        - pe_forget: float in [0,1]
            Scales WM forgetting by unsigned RL prediction error (surprise).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, stickiness, wm_lr, pe_forget = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last chosen action per state for RL stickiness
        last_a = -np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            # RL with stickiness for last chosen action in the same state
            Q_s = q[s, :].copy()
            if last_a[s] >= 0:
                Q_s[last_a[s]] += stickiness

            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from cached distribution
            prefs_wm = W_s - np.mean(W_s)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_wm, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s][a] += lr * delta

            # PE-driven WM forgetting, scaled by set size
            pe = abs(delta)
            forget = np.clip(pe_forget * pe * (nS / 6.0), 0.0, 1.0)
            w = (1 - forget) * w + forget * w_0

            # Reward-driven WM learning toward one-hot
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_lr) * w[s, :] + wm_lr * target
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

            # Update last chosen action for stickiness
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration modulated by set size.

    Mechanism
    - RL: standard delta-rule softmax.
    - WM: fast rule table that learns toward one-hot on reward and decays toward uniform with a mild base rate.
    - Arbitration: dynamic WM mixture weight computed from:
        * a base bias (wm_bias),
        * set-size advantage for WM in small sets (k_size),
        * relative certainty (negative entropy) of WM vs RL in the current state (k_uncert).
      The resulting effective weight is passed through a sigmoid, then used in the mixture.

    Parameters
    ----------
    model_parameters: tuple
        - lr: float
            RL learning rate.
        - wm_bias: float
            Baseline log-odds bias favoring WM in the arbitration.
        - softmax_beta: float
            Inverse temperature for RL softmax (scaled internally by 10).
        - k_size: float
            Weight of set-size term; positive values favor WM more in small sets.
        - k_uncert: float
            Weight of certainty difference term (WM certainty minus RL certainty).
        - wm_lr: float in [0,1]
            WM learning rate toward one-hot on reward; also sets mild decay per trial.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_bias, softmax_beta, k_size, k_uncert, wm_lr = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    eps = 1e-12

    def entropy(p):
        p = np.clip(p, eps, 1.0)
        return -np.sum(p * np.log(p))

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-np.clip(x, -50, 50)))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Mild baseline decay tied to wm_lr so that larger wm_lr implies more plastic but also more labile WM
        base_decay = np.clip(0.25 * wm_lr * (nS / 6.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy vector from WM table
            prefs_wm = W_s - np.mean(W_s)
            p_wm_vec = np.exp(np.clip(softmax_beta_wm * prefs_wm, -50, 50))
            p_wm_vec /= max(np.sum(p_wm_vec), eps)
            p_wm = p_wm_vec[a]

            # Compute dynamic arbitration weight based on certainty and set size
            # Approximate RL policy vector by softmax over Q
            Q_centered = Q_s - np.mean(Q_s)
            p_rl_vec = np.exp(np.clip(softmax_beta * Q_centered, -50, 50))
            p_rl_vec /= max(np.sum(p_rl_vec), eps)

            H_wm = entropy(p_wm_vec)
            H_rl = entropy(p_rl_vec)
            cert_diff = (np.log(nA) - H_wm) - (np.log(nA) - H_rl)  # = H_rl - H_wm

            size_term = k_size * (3.0 - (nS - 3.0)) / 3.0  # + for small set (3), - for large (6)
            arbit_logits = wm_bias + size_term + k_uncert * cert_diff
            wm_weight_eff = sigmoid(arbit_logits)

            # Mixture
            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM baseline decay
            w = (1 - base_decay) * w + base_decay * w_0

            # Reward-driven WM learning toward one-hot
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1 - wm_lr) * w[s, :] + wm_lr * target
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p