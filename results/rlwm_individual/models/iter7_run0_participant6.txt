def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working-memory cache with age- and load-modulated capacity and lapses.

    Idea
    - Model-free RL updates Q-values.
    - In parallel, a simple working-memory (WM) cache stores the last rewarded action per state
      and a strength z_s that increases on reward and decays otherwise.
    - The effective WM weight depends on an internal capacity that shrinks with set size and with age,
      and on the current cache strength z_s for that state.
    - Final choice policy is a mixture of RL softmax and a WM softmax that strongly favors the cached action.
    - Lapse probability mixes in a uniform choice.

    Parameters
    ----------
    states : array-like (int)
        State index for each trial.
    actions : array-like (int)
        Chosen action per trial (0..2).
    rewards : array-like (0/1)
        Outcome feedback per trial.
    blocks : array-like (int)
        Block index per trial.
    set_sizes : array-like (int)
        Set size for each trial's block (e.g., 3 or 6).
    age : array-like (float)
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha, beta, capacity_base, k_age_load, lapse)
        - alpha: RL learning rate (also used as WM cache learning/decay rate).
        - beta: inverse temperature for RL softmax.
        - capacity_base: baseline WM capacity (in "slots" relative to the set size).
        - k_age_load: how much age (older) and set size reduce effective WM capacity.
        - lapse: lapse probability mixed with uniform choice.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, capacity_base, k_age_load, lapse = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM cache: cached action per state (-1 means empty) and its strength z in [0,1]
        cached = -1 * np.ones(nS, dtype=int)
        z = np.zeros(nS, dtype=float)

        # Effective capacity term: declines with set size and with age
        # size_term: 0 for 3-set, 0.5 for 6-set
        size_term = 1.0 - (3.0 / float(nS))
        capacity_eff = max(0.0, capacity_base - k_age_load * (size_term + is_older))
        # Convert capacity to a weight relative to set size
        base_wm_weight = min(1.0, capacity_eff / float(nS))  # in [0,1]

        logp = 0.0
        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # RL policy
            prefs_rl = beta * Q[s, :]
            denom_rl = np.sum(np.exp(prefs_rl - prefs_rl[a]))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: if cached, heavily bias towards cached action; scale by z[s]
            if cached[s] >= 0:
                # Strong but soft preference toward cached action; scaled by cache strength
                beta_wm = 10.0 * (0.5 + 0.5 * z[s])  # in [5,10], stronger when confident
                wm_vec = np.zeros(nA)
                wm_vec[cached[s]] = 1.0
                prefs_wm = beta_wm * wm_vec
                denom_wm = np.sum(np.exp(prefs_wm - prefs_wm[a]))
                p_wm = 1.0 / max(denom_wm, 1e-12)
            else:
                # No cache -> uniform WM
                p_wm = 1.0 / nA

            # State-specific WM mixture weight: base capacity scaled by current cache strength
            w_s = np.clip(base_wm_weight * z[s], 0.0, 1.0)

            p_mix = w_s * p_wm + (1.0 - w_s) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp += np.log(p_final)

            # Learning
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update WM cache: reward writes/strengthens, non-reward decays
            if r > 0.5:
                cached[s] = a
                z[s] = z[s] + alpha * (1.0 - z[s])  # move toward 1
            else:
                z[s] = (1.0 - alpha) * z[s]         # decay toward 0

        total_logp += logp

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric learning rates and perseveration, jointly modulated by age and load.

    Idea
    - Model-free RL with separate learning rates for positive vs. negative outcomes.
    - A perseveration bias pushes repeating the most recent action within the state.
    - Age and set size jointly modulate: decrease beta and alpha_pos, increase alpha_neg and perseveration.

    Parameters
    ----------
    states : array-like (int)
        State index for each trial.
    actions : array-like (int)
        Chosen action per trial (0..2).
    rewards : array-like (0/1)
        Outcome feedback.
    blocks : array-like (int)
        Block index per trial.
    set_sizes : array-like (int)
        Set size (3 or 6).
    age : array-like (float)
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha_pos_base, alpha_neg_base, beta_base, k_mod, lambda_base)
        - alpha_pos_base: baseline learning rate for positive prediction errors.
        - alpha_neg_base: baseline learning rate for negative prediction errors.
        - beta_base: baseline inverse temperature.
        - k_mod: modulation strength applied to age and load.
        - lambda_base: baseline perseveration strength added to preference of last action.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha_pos_base, alpha_neg_base, beta_base, k_mod, lambda_base = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Last action per state for perseveration
        last_a = -1 * np.ones(nS, dtype=int)

        # Age-load factor: 0 for small set, 0.5 for large; add 1 if older
        size_term = 1.0 - (3.0 / float(nS))
        factor = (size_term + is_older)

        # Modulate parameters
        # Larger factor -> lower beta, lower alpha_pos, higher alpha_neg, stronger perseveration
        beta_eff = max(1e-3, beta_base * (1.0 - 0.5 * k_mod * factor))
        alpha_pos = np.clip(alpha_pos_base * (1.0 - 0.5 * k_mod * factor), 1e-4, 1.0)
        alpha_neg = np.clip(alpha_neg_base * (1.0 + 0.5 * k_mod * factor), 1e-4, 1.0)
        kappa = lambda_base * (1.0 + k_mod * factor)

        logp = 0.0
        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Perseveration bias
            bias = np.zeros(nA)
            if last_a[s] >= 0:
                bias[last_a[s]] += kappa

            prefs = beta_eff * Q[s, :] + bias
            denom = np.sum(np.exp(prefs - prefs[a]))
            p = 1.0 / max(denom, 1e-12)
            logp += np.log(max(p, 1e-12))

            # Update
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Update last action
            last_a[s] = a

        total_logp += logp

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    TD(λ) with surprise-adaptive temperature and age/load-modulated eligibility decay.

    Idea
    - Model-free RL with eligibility traces to propagate credit to recent actions in the same state.
    - The trace decay λ is reduced by age and set size, modeling diminished binding/maintenance.
    - Decision noise increases with surprise (|PE|): higher surprise -> lower inverse temperature.
    - This captures sharper policies when outcomes are expected and more exploratory choices after surprises.

    Parameters
    ----------
    states : array-like (int)
        State index per trial.
    actions : array-like (int)
        Chosen action per trial (0..2).
    rewards : array-like (0/1)
        Feedback per trial.
    blocks : array-like (int)
        Block index per trial.
    set_sizes : array-like (int)
        Set size (3 or 6).
    age : array-like (float)
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha, beta_base, lam_base, k_age_load, noise_gain)
        - alpha: learning rate for value updates.
        - beta_base: base inverse temperature when surprise is zero.
        - lam_base: baseline eligibility trace decay (0..1).
        - k_age_load: modulation factor that decreases λ with age and load.
        - noise_gain: strength of surprise-to-temperature effect.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta_base, lam_base, k_age_load, noise_gain = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask].astype(int)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        E = np.zeros((nS, nA))  # eligibility traces

        # Age-load modulation of λ
        size_term = 1.0 - (3.0 / float(nS))  # 0 for 3-set, 0.5 for 6-set
        lam_eff = np.clip(lam_base * (1.0 - k_age_load * (size_term + is_older)), 0.0, 1.0)

        logp = 0.0
        prev_abs_pe = 0.0
        for t in range(len(block_states)):
            s = block_states[t]
            a = block_actions[t]
            r = block_rewards[t]

            # Surprise-adaptive temperature: reduce inverse temp when previous surprise was high
            beta_t = max(1e-3, beta_base / (1.0 + noise_gain * prev_abs_pe))

            prefs = beta_t * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p = 1.0 / max(denom, 1e-12)
            logp += np.log(max(p, 1e-12))

            # PE
            pe = r - Q[s, a]

            # Eligibility update: decay all traces, then set current to 1 (accumulating traces)
            E *= lam_eff
            E[s, a] = 1.0

            # TD(λ) update
            Q += alpha * pe * E

            prev_abs_pe = abs(pe)

        total_logp += logp

    return -total_logp