def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with confidence- and load-weighted arbitration and WM precision decay with load.

    Idea:
    - Choices are a mixture of model-free RL and a WM policy over state-action associations.
    - WM precision declines with set size; arbitration weight increases with WM "confidence"
      (how peaked the WM distribution is) and also declines with load.
    - WM traces decay toward a uniform prior each trial; rewarded trials overwrite the WM trace for that state.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight_base: Base mixture weight scaling WM influence (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_precision_base: base inverse temperature (precision) for WM policy.
    - load_sensitivity: controls how WM precision and arbitration weight decline with set size (>0 means more decline).
    - wm_decay: WM decay rate toward uniform (0..1) applied each trial.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight_base, softmax_beta, wm_precision_base, load_sensitivity, wm_decay = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (kept but not directly used; we use wm_precision_base)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load factor: >1 for larger sets, 1 for set size 3; damps WM precision and weight
        load_factor = 1 + max(0, nS - 3) * load_sensitivity
        wm_beta_eff = wm_precision_base / load_factor
        weight_damping = 1 / load_factor  # down-weight WM arbitration under load

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax with effective WM precision that declines with load
            p_wm = 1 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            # Confidence from WM distribution (how peaked it is)
            # Use difference between max and mean of W_s; pass through a squashing nonlinearity
            wm_conf = max(0.0, np.max(W_s) - np.mean(W_s))
            wm_weight_eff = wm_weight_base * weight_damping * (1 / (1 + np.exp(-5 * (wm_conf - 0.0))))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward uniform
            w = (1 - wm_decay) * w + wm_decay * w_0
            # Rewarded trials: store one-shot association
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (valence-asymmetric) + WM with prediction-error-based gating and load-dependent WM forgetting.

    Idea:
    - RL uses separate learning rates for positive and negative prediction errors.
    - WM contributes a near-deterministic policy from remembered associations.
    - Arbitration weight for WM is down-regulated when current unsigned prediction error is large
      (trust RL exploration when surprised).
    - WM forgetting increases with set size (higher load -> faster decay).

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive PE (0..1).
    - lr_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_weight: baseline WM mixture weight (0..1).
    - pe_sensitivity: scales how strongly unsigned PE suppresses WM weight (>0).
    - wm_forgetting: base WM forgetting rate (0..1) that is further increased by load.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, pe_sensitivity, wm_forgetting = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-scaled forgetting: more forgetting when nS larger
        load_scale = max(1.0, nS / 3.0)
        wm_forgetting_eff = np.clip(wm_forgetting * load_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: near-deterministic (softmax with high beta)
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute unsigned prediction error BEFORE updating RL
            pe = r - Q_s[a]
            wm_weight_eff = wm_weight * np.exp(-pe_sensitivity * np.abs(pe))

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))
      
            # RL update with valence-asymmetric learning rates
            if pe >= 0:
                q[s][a] += lr_pos * pe
            else:
                q[s][a] += lr_neg * pe

            # WM decay toward uniform with load-dependent forgetting
            w = (1 - wm_forgetting_eff) * w + wm_forgetting_eff * w_0
            if r > 0:
                # reinforce the remembered mapping on reward
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited probabilistic WM encoding + lapse.

    Idea:
    - WM encodes rewarded state-action pairs with probability that declines with set size due to capacity limits.
    - If encoded, WM yields a near-deterministic choice; otherwise WM defaults to uniform.
    - Arbitration between RL and WM uses the (capacity- and load-adjusted) encoding probability.
    - A small lapse mixes in uniform choice to capture occasional random responses.
    - WM traces decay toward uniform each trial.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_capacity: effective WM capacity in number of items (>0); controls load effect.
    - enc_prob: baseline probability of encoding a rewarded item into WM (0..1).
    - recency_decay: WM decay rate toward uniform (0..1) applied each trial.
    - lapse_eps: lapse rate mixing uniform choice into final policy (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_capacity, enc_prob, recency_decay, lapse_eps = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective probability that a rewarded item is available in WM at any trial,
        # scaled by capacity relative to set size
        cap_factor = min(1.0, max(0.0, wm_capacity) / max(1.0, float(nS)))
        enc_prob_eff = np.clip(enc_prob * cap_factor, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: a mixture capturing probabilistic availability of WM trace
            # If WM holds the mapping: near-deterministic; if not: uniform
            p_wm_if_encoded = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_if_not = 1.0 / nA
            p_wm = enc_prob_eff * p_wm_if_encoded + (1 - enc_prob_eff) * p_wm_if_not

            # Use enc_prob_eff as arbitration weight between WM and RL
            p_total_core = p_wm * enc_prob_eff + (1 - enc_prob_eff) * p_rl

            # Add lapse to capture occasional random choices
            p_total = (1 - lapse_eps) * p_total_core + lapse_eps * (1.0 / nA)

            log_p += np.log(max(p_total, 1e-12))
      
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM recency decay toward uniform
            w = (1 - recency_decay) * w + recency_decay * w_0
            # On reward, (probabilistically, but in likelihood we reflect it via the policy)
            # we store the one-shot mapping deterministically in the trace; the uncertainty
            # about whether it is present is handled by enc_prob_eff above.
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p