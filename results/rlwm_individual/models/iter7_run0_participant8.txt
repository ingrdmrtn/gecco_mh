def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated forgetting plus action stickiness.

    Mechanism
    - Tabular Q-learning with per-state action values.
    - Value forgetting toward zero that increases with set size and with age.
    - Action stickiness: bias to repeat the last action chosen in the same state; reduced under higher load and in older age group.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). If out-of-range, treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Reward feedback (0/1 typical). If negative, treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        lr0          : Base Q-learning rate (0..1).
        inv_temp     : Inverse temperature for softmax (>0).
        forget_rate  : Baseline per-trial forgetting rate toward zero (0..1).
        stick_base   : Baseline action stickiness weight added to the chosen action (>0).
        age_slope    : Scales how much forgetting increases per decade from 45 (can be +/-).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    lr0, inv_temp, forget_rate, stick_base, age_slope = model_parameters

    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    age_deviation = (age_val - 45.0) / 10.0  # decades from 45

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])
        Q = np.zeros((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Compute effective forgetting rate (more with larger set size and higher age)
            size_factor = 1.0 + 0.3 * (ss - 3.0)
            age_factor = 1.0 + age_slope * age_deviation
            forget_eff = np.clip(forget_rate * size_factor * age_factor, 0.0, 0.99)

            # Apply forgetting on the current state's action values
            Q[s, :] *= (1.0 - forget_eff)

            # Stickiness (repetition bias for the last action in this state)
            stick_weight = stick_base * (1.0 - 0.3 * (ss - 3.0)) * (1.0 - 0.25 * is_older)
            stick_weight = max(0.0, stick_weight)
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = stick_weight

            # Softmax policy with stickiness bias
            pref = inv_temp * Q[s, :] + bias
            m = np.max(pref)
            expv = np.exp(pref - m)
            p = expv / np.sum(expv)
            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += lr0 * pe

            # Update last action memory
            last_action[s] = a

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-sensitive (Kalman) RL with age/load-modulated process noise and directed exploration.

    Mechanism
    - For each state-action, track a Gaussian posterior over expected reward (mean and variance).
    - Per-trial process noise (volatility) increases with set size and with age.
    - Choice uses softmax over mean values plus an uncertainty bonus proportional to sqrt(variance),
      with bonus weight reduced in older adults and scaled by set size.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). If out-of-range, treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Reward feedback (0/1 typical). If negative, treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        inv_temp  : Inverse temperature for the softmax (>0).
        q_base    : Baseline process noise (>=0) driving learning rate (higher -> faster learning).
        r_base    : Observation noise (>=0) controlling learning rate (higher -> slower learning).
        age_gain  : Scales how process noise and exploration change per decade from 45 (can be +/-).
        size_gain : Scales how process noise and exploration change per +3 items in set size (can be +/-).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    inv_temp, q_base, r_base, age_gain, size_gain = model_parameters

    age_val = float(age[0])
    is_older = 1.0 if age_val > 45.0 else 0.0
    age_dev = (age_val - 45.0) / 10.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Initialize mean and variance for each state-action
        mu = np.zeros((nS, nA))
        var = np.ones((nS, nA))  # start with uncertainty

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Effective process noise increases with set size and with age
            q_eff = q_base * (1.0 + size_gain * (ss - 3.0)) * (1.0 + age_gain * age_dev)
            q_eff = max(0.0, q_eff)
            r_eff = max(1e-6, r_base)  # avoid zero division

            # Directed exploration weight from uncertainty, reduced in older age
            explore_w = 0.5 * (1.0 + size_gain * (ss - 3.0)) * (1.0 - 0.3 * is_older + 0.3 * age_gain * age_dev)
            # Compute preferences with exploration bonus
            bonus = np.sqrt(np.clip(var[s, :], 1e-12, None))
            pref = inv_temp * (mu[s, :] + explore_w * bonus)

            m = np.max(pref)
            expv = np.exp(pref - m)
            p = expv / np.sum(expv)
            pa = np.clip(p[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Kalman update for the chosen action
            # Predict step: add process noise to chosen pair
            var[s, a] = var[s, a] + q_eff
            # Gain
            K = var[s, a] / (var[s, a] + r_eff)
            # Update mean and variance
            mu[s, a] = mu[s, a] + K * (r - mu[s, a])
            var[s, a] = (1.0 - K) * var[s, a]

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian outcome counting with age/load-modulated lapses.

    Mechanism
    - For each state-action, maintain a Beta prior/posterior over reward probability via success/failure counts.
    - The expected reward (posterior mean) drives a softmax choice.
    - Lapse probability mixes the softmax with a uniform policy; lapses increase with set size and with age.

    Parameters
    ----------
    states : array-like of int
        State index per trial (0..nS-1 within block).
    actions : array-like of int
        Observed action (0..2). If out-of-range, treated as lapse (uniform likelihood, no learning).
    rewards : array-like of float
        Reward feedback (0/1 typical). If negative, treated as invalid (uniform likelihood, no learning).
    blocks : array-like of int
        Block index per trial; states reset per block.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age as a single-element array. Older group if age > 45.
    model_parameters : iterable of 5 floats
        prior_conc : Symmetric Beta prior concentration per action (>=0). alpha=beta=prior_conc/2.
        inv_temp   : Inverse temperature for the softmax (>0).
        lapse_base : Baseline logit of lapse probability (can be +/-).
        age_mod    : Additive effect on lapse logit per decade from 45 (can be +/-).
        size_mod   : Additive effect on lapse logit per +3 items in set size (can be +/-).

    Returns
    -------
    neg_log_likelihood : float
        Negative log-likelihood of the observed action sequence.
    """
    prior_conc, inv_temp, lapse_base, age_mod, size_mod = model_parameters

    age_val = float(age[0])
    age_dev = (age_val - 45.0) / 10.0

    total_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = np.asarray(actions)[idx]
        block_rewards = np.asarray(rewards)[idx]
        block_states = np.asarray(states)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]

        nS = int(block_set_sizes[0])

        # Beta prior parameters per state-action: alpha (successes) and beta (failures)
        alpha_succ = np.full((nS, nA), max(1e-6, 0.5 * prior_conc))
        beta_fail = np.full((nS, nA), max(1e-6, 0.5 * prior_conc))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            ss = float(block_set_sizes[t])

            if not (0 <= s < nS) or not (0 <= a < nA) or r < 0:
                total_log_p += np.log(1.0 / nA)
                continue

            # Posterior mean estimate of reward probability
            denom = alpha_succ[s, :] + beta_fail[s, :]
            q_est = alpha_succ[s, :] / np.clip(denom, 1e-12, None)

            # Softmax over expected reward
            pref = inv_temp * q_est
            m = np.max(pref)
            expv = np.exp(pref - m)
            p_soft = expv / np.sum(expv)

            # Lapse probability as logistic function of age and set size
            logit_eps = lapse_base + age_mod * age_dev + size_mod * (ss - 3.0)
            eps = 1.0 / (1.0 + np.exp(-logit_eps))
            p_mix = (1.0 - eps) * p_soft + eps * (np.ones(nA) / nA)

            pa = np.clip(p_mix[a], 1e-12, 1.0)
            total_log_p += np.log(pa)

            # Update counts with observed outcome
            if r >= 0.0:
                alpha_succ[s, a] += r
                beta_fail[s, a] += (1.0 - r)

    return -float(total_log_p)