def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with dynamic attention (Pearce-Hall style) and reliability-based WM arbitration.

    Idea:
    - RL: A baseline learning rate is modulated online by unsigned prediction error using
      an attention filter with inertia (rho). This adapts learning to surprise.
    - WM: A fast, decaying associative matrix. Reward strengthens the chosen action with
      confidence wm_conf; decay increases with set size to capture load/interference.
    - Arbitration: On each trial, compute WM vs RL "reliability" as distinctiveness
      (max - second-max) and map the difference to a mixture weight via a sigmoid
      scaled by wm_weight0.

    Parameters (6 total):
    - lr_base: Baseline RL learning rate (0..1).
    - kappa_attn: Scales the contribution of unsigned prediction error to attention (0..1).
    - rho_attn: Attention inertia (0..1); higher means slower changes in attention.
    - softmax_beta: RL inverse temperature; internally scaled by 10 for sensitivity.
    - wm_weight0: Arbitration gain; larger values favor WM when its reliability > RL's.
    - wm_conf: Strength of WM updates on rewarded trials (0..1).
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, kappa_attn, rho_attn, softmax_beta, wm_weight0, wm_conf = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # highly deterministic WM channel
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Attention/learning-rate tracker per state-action
        alpha_sa = lr_base * np.ones((nS, nA))

        # WM decay scales with set size to capture load/interference
        # Larger nS -> stronger drift toward uniform each trial
        wm_decay_ns = (1.0 - rho_attn) * (nS / (nS + 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute reliability via distinctiveness (margin)
            def margin(vec):
                vmax = np.max(vec)
                # second maximum
                idx = np.argmax(vec)
                vtmp = vec.copy()
                vtmp[idx] = -1e9
                v2 = np.max(vtmp)
                return vmax - v2

            d_wm = margin(W_s)
            d_rl = margin(Q_s)
            # Arbitration: map reliability difference via sigmoid with gain wm_weight0
            wm_weight = 1.0 / (1.0 + np.exp(-wm_weight0 * (d_wm - d_rl)))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with attention-modulated learning rate
            delta = r - Q_s[a]
            abs_pe = abs(delta)
            # Update local attention for (s,a)
            alpha_sa[s, a] = rho_attn * alpha_sa[s, a] + (1.0 - rho_attn) * (kappa_attn * abs_pe + (1.0 - kappa_attn) * lr_base)
            alpha_eff = np.clip(alpha_sa[s, a], 0.0, 1.0)
            q[s, a] += alpha_eff * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay_ns) * w + wm_decay_ns * w_0
            # Reward-driven WM sharpening of the chosen action
            if r > 0.5:
                # contract others slightly and boost chosen by wm_conf
                w[s, :] = (1.0 - wm_conf) * w[s, :]
                w[s, a] += wm_conf

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Kalman RL (adaptive learning rate) with probabilistic WM storage scaled by set size.

    Idea:
    - RL: Each state-action pair has an uncertainty P; learning rate alpha_t is computed
      as in a steady-state Kalman filter with process noise omega and observation noise sigma.
    - WM: Fast store-on-reward with probability that decreases with set size (capacity limit).
      We implement this deterministically as a convex combination weighted by p_store.
    - Arbitration: WM contribution equals wm_weight0 times the store probability (thus
      decreasing with set size); RL handles the rest.

    Parameters (6 total):
    - omega: Process noise controlling volatility/learning rate floor (>=0).
    - sigma: Observation noise (>=0).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight0: Base weight scaling WM's influence in the mixture (0..1+).
    - wm_decay: Per-trial WM decay toward uniform (0..1).
    - c_store: Capacity constant; WM store probability p_store = min(1, c_store / nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    omega, sigma, softmax_beta, wm_weight0, wm_decay, c_store = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Uncertainty per state-action for Kalman RL
        P = np.ones((nS, nA))

        # WM store probability decreases with set size
        p_store = min(1.0, float(c_store) / max(1.0, nS))
        p_store = max(0.0, p_store)

        # Mixture weight scales with p_store (less WM under high load)
        wm_weight = float(np.clip(wm_weight0 * p_store, 0.0, 1.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # Kalman RL update
            # Predictive uncertainty update
            P[s, a] = P[s, a] + omega
            # Adaptive learning rate
            alpha_t = P[s, a] / (P[s, a] + sigma + eps)
            delta = r - Q_s[a]
            q[s, a] += alpha_t * delta
            # Posterior uncertainty
            P[s, a] = (1.0 - alpha_t) * P[s, a]

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # Deterministic "expected" storage on reward
            if r > 0.5:
                # move row toward a one-hot on chosen action with weight p_store
                w[s, :] = (1.0 - p_store) * w[s, :]
                w[s, a] += p_store

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with decay and exploration bonus, WM recency-gated influence scaled by set size.

    Idea:
    - RL: Values decay toward uniform at rate decay_q (forgetting/interference) and are
      updated by a standard delta-rule; policy includes an uncertainty-driven bonus
      inversely proportional to the square root of visit counts.
    - WM: Time-based decay with timescale tau_wm; reward produces a one-shot boost for
      the chosen action in the current state.
    - Arbitration: WM weight increases with state recency and decreases with set size
      by a factor (3 / nS), reflecting load.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - decay_q: Per-trial Q-value decay toward uniform (0..1).
    - bonus_eta: Coefficient for exploration bonus (>=0).
    - wm_weight0: Base WM influence scale (0..1+).
    - tau_wm: WM time constant for recency gating and decay (>0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, decay_q, bonus_eta, wm_weight0, tau_wm = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Visit counts for exploration bonus
        N = np.zeros((nS, nA)) + 1e-6

        # Recency tracker per state: last seen trial index within block
        last_seen = -1e9 * np.ones(nS)

        # WM per-trial decay factor from tau
        gamma_wm = np.exp(-1.0 / max(tau_wm, 1e-6))  # in (0,1)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL decay toward uniform
            q = (1.0 - decay_q) * q + decay_q * (1.0 / nA)

            Q_s = q[s, :].copy()
            # Add exploration bonus to policy (but not to learning target)
            bonus = bonus_eta / np.sqrt(N[s, :] + 1e-12)
            Qs_policy = Q_s + bonus

            # RL policy with bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Qs_policy - Qs_policy[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Recency-based arbitration weight and set-size penalty
            gap = t - last_seen[s]
            recency = np.exp(-gap / max(tau_wm, 1e-6)) if last_seen[s] > -1e8 else 0.0
            wm_weight = wm_weight0 * recency * (3.0 / max(1.0, nS))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            N[s, a] += 1.0

            # WM decay toward uniform
            w = gamma_wm * w + (1.0 - gamma_wm) * w_0
            # Reward-driven WM boost for current state-action
            if r > 0.5:
                w[s, :] = gamma_wm * w[s, :]
                w[s, a] += (1.0 - gamma_wm)

            # Update recency
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p