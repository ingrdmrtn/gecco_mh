def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + Bayesian Working Memory (Dirichlet-Beta) with capacity-dependent decay and age-modulated arbitration.

    Idea:
    - Two systems:
      1) Model-free RL (Q-learning) with softmax.
      2) A Bayesian WM store that tracks, per state and action, a Beta posterior over "correctness"
         using pseudo-counts (successes/failures). The WM policy is the posterior mean over actions.
    - WM precision decays each trial toward the prior; decay is stronger with larger set sizes and for older adults.
    - Arbitration weight for WM depends on WM precision (posterior concentration), load (set size), and age.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1].
    - beta: inverse temperature for RL (>0).
    - prior_strength: symmetric prior pseudo-counts for WM Beta distributions (>0).
    - decay_base: base per-trial WM decay rate in [0,1]; higher = faster decay.
    - age_wm_bias: weight in [0,1] controlling how much older adults down-weight WM and increase decay.

    Inputs:
    - states: array of state indices per trial (0..set_size-1 within each block).
    - actions: array of chosen actions per trial (0..2).
    - rewards: array of binary rewards per trial (0 or 1).
    - blocks: array of block identifiers (learning resets per block).
    - set_sizes: array of set sizes (e.g., 3 or 6).
    - age: array-like with a single value: participant age in years.
    - model_parameters: list/tuple [alpha, beta, prior_strength, decay_base, age_wm_bias].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, prior_strength, decay_base, age_wm_bias = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # RL Q-values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM pseudo-counts: Beta(a,b) per state-action. We keep successes and failures.
        # Initialize with symmetric prior.
        succ = prior_strength * np.ones((nS, nA))
        fail = prior_strength * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            curr_set = float(block_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # WM posterior means per action for current state
            m_s = succ[s, :] / (succ[s, :] + fail[s, :] + eps)
            # Convert to a proper probability distribution over actions by normalizing
            wmlogits = m_s - np.max(m_s)
            p_wm = np.exp(wmlogits)
            p_wm /= np.sum(p_wm)

            # WM concentration (precision) for arbitration: sum of pseudo-counts over actions
            conc = np.sum(succ[s, :] + fail[s, :])
            # Normalize concentration by a nominal scale so weight remains in (0,1)
            # Larger set sizes reduce effective WM contribution, older age reduces it further.
            conc_norm = conc / (conc + nA * 2.0)  # in (0,1)
            load_factor = 3.0 / curr_set  # more weight at small set sizes
            age_factor = 1.0 - 0.6 * age_wm_bias * is_older
            wm_weight = np.clip(conc_norm * load_factor * age_factor, 0.0, 1.0)

            # Mixture policy
            p = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay: pseudo-counts move toward prior strength; stronger decay with load and for older adults
            # Effective decay this trial:
            decay_eff = decay_base * (curr_set / 6.0) * (1.0 + 0.8 * age_wm_bias * is_older)
            decay_eff = np.clip(decay_eff, 0.0, 1.0)
            succ[s, :] = (1.0 - decay_eff) * succ[s, :] + decay_eff * prior_strength
            fail[s, :] = (1.0 - decay_eff) * fail[s, :] + decay_eff * prior_strength

            # WM write: increment success/failure for chosen action based on reward
            if r > 0.5:
                succ[s, a] += 1.0
            else:
                fail[s, a] += 1.0

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-adaptive exploration and action stickiness via a choice kernel.

    Idea:
    - Model-free RL with softmax choice.
    - The inverse temperature is dynamically interpolated between beta_min and beta_max
      based on set size (load) and age: larger sets and older adults reduce beta (more exploration).
    - A choice kernel K biases repeating the last chosen action (stickiness), with strength modulated by age.
      Younger adults rely less on stickiness; older adults more.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1].
    - beta_min: lower bound on inverse temperature (>0) under high load/older age.
    - beta_max: upper bound on inverse temperature (>beta_min) under low load/younger age.
    - stick_base: base stickiness gain (can be >0), modulated by age to increase with age.
    - age_mix: mixing weight in [0,1] controlling how strongly age pushes toward beta_min and increases stickiness.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial.
    - age: array-like with single value (years).
    - model_parameters: [alpha, beta_min, beta_max, stick_base, age_mix].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta_min, beta_max, stick_base, age_mix = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        # Choice kernel (recency of chosen actions), shared across states to capture motor/response habits
        K = np.zeros(nA)

        last_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            curr_set = float(block_set_sizes[t])

            # Load factor: 1 at set size 3, 0 at set size 6
            load_fac = (6.0 - curr_set) / 3.0  # 1 for 3, 0 for 6, linear in between
            # Age factor reduces beta toward beta_min for older adults
            beta_weight = np.clip(0.5 * load_fac + (1.0 - age_mix * is_older) * 0.5, 0.0, 1.0)
            beta_eff = beta_weight * beta_max + (1.0 - beta_weight) * beta_min

            # Stickiness gain increases with age
            stick_gain = stick_base * (1.0 + 0.8 * age_mix * is_older)

            q_s = Q[s, :].copy()
            stick_vec = np.zeros(nA)
            if last_action is not None:
                stick_vec[last_action] = 1.0

            # Combine Q-values with stickiness as additive logit bias scaled by stick_gain
            logits = beta_eff * q_s + stick_gain * (K + stick_vec)
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)

            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update choice kernel to favor recently chosen actions; small decay to keep bounded
            K *= 0.8
            K[a] += 1.0

            # Track last action globally (captures simple perseveration)
            last_action = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with load- and age-dependent cross-state generalization via a shared action preference.

    Idea:
    - Standard per-state Q-learning.
    - Plus a global action preference G[a] that captures generalization across states (e.g., response habits
      or structure learned within a block). This preference is updated by the reward prediction error and
      acts as a bias term shared across states.
    - Generalization strength increases with load (set size 6) and for younger adults (assuming they leverage
      structure to compensate for load), while older adults rely less on cross-state generalization.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1].
    - beta: inverse temperature (>0).
    - gen_strength: base strength of cross-state generalization (>0).
    - load_slope: scales how much set size amplifies generalization (>0).
    - age_factor: in [0,1], reduces generalization for older adults and mildly boosts for younger adults.

    Inputs:
    - states, actions, rewards, blocks, set_sizes: arrays per trial.
    - age: array-like with single value (years).
    - model_parameters: [alpha, beta, gen_strength, load_slope, age_factor].

    Returns:
    - Negative log-likelihood of observed choices.
    """
    alpha, beta, gen_strength, load_slope, age_factor = model_parameters
    age_val = age[0]
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        G = np.zeros(nA)  # global action preference shared across states in the block

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])
            curr_set = float(block_set_sizes[t])

            # Effective generalization gain this trial:
            # increases with load (higher for set size 6), reduced for older adults
            load_gain = 1.0 + load_slope * ((curr_set - 3.0) / 3.0)  # 1 at 3, 1+load_slope at 6
            age_mod = 1.0 - 0.8 * age_factor * is_older + 0.2 * age_factor * (1.0 - is_older)
            g_eff = gen_strength * load_gain * age_mod

            logits = beta * Q[s, :] + g_eff * G
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p /= np.sum(p)

            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Global preference update shares the PE signal across actions:
            # Increase preference for chosen action, slight decay for others to keep bounded.
            G *= 0.9
            G[a] += pe  # signed update: rewarded choice boosts its global attractiveness

    return nll