def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-scaled Working Memory (WM).

    Idea:
    - RL learns action values per state via standard delta rule.
    - WM stores the most recent rewarded action as a sharp distribution, but it leaks toward uniform.
    - WM contribution is scaled by an effective capacity parameter relative to set size: if nS > capacity,
      WM is down-weighted and also leaks more (interference).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: Base mixture weight of WM vs RL (0..1)
    - softmax_beta: Inverse temperature for RL choice (scaled internally by 10)
    - k_cap: Effective WM capacity in number of states (>0). WM influence scales roughly as k_cap/nS.
    - wm_leak: Per-trial WM leak toward uniform (0..1). Also scaled by load (nS/k_cap).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight0, softmax_beta, k_cap, wm_leak = model_parameters

    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-scaled WM mixture and leak
        cap_ratio = min(1.0, max(0.0, (k_cap / max(1.0, float(nS)))))
        wm_weight_block = wm_weight0 * cap_ratio
        eff_wm_leak = min(1.0, wm_leak * (float(nS) / max(1.0, k_cap)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights (sharp when WM is confident)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture
            p_total = wm_weight_block * p_wm + (1.0 - wm_weight_block) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leak toward uniform; if rewarded, encode chosen action as a one-hot
            w[s, :] = (1.0 - eff_wm_leak) * w[s, :] + eff_wm_leak * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            # normalize to avoid numerical drift
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy- and load-based arbitration.

    Idea:
    - RL learns via delta rule; its policy uncertainty (entropy) varies across states.
    - WM stores last rewarded actions but leaks toward uniform.
    - Arbitration: when RL is uncertain (high entropy), increase WM weight; also down-weight WM under high load (large set size).
      A parameter eta_ent controls how strongly entropy vs. load drives the mixture.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: Inverse temperature for RL choice (scaled internally by 10)
    - wm_weight0: Base WM mixture weight (0..1)
    - eta_ent: Arbitration weight for RL entropy (0..1). Higher -> more WM reliance when RL is uncertain.
    - wm_leak: WM leak toward uniform per trial (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight0, eta_ent, wm_leak = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-based penalty term (0..1), decreasing with set size
        load_term = min(1.0, max(0.0, 3.0 / float(nS)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL action prob vector for entropy
            logits = softmax_beta * Q_s
            logits -= np.max(logits)
            exp_logits = np.exp(logits)
            pi_rl = exp_logits / np.sum(exp_logits)

            # RL probability of chosen action
            p_rl = pi_rl[a]

            # RL entropy (normalized 0..1)
            eps = 1e-12
            H = -np.sum(pi_rl * np.log(np.maximum(pi_rl, eps)))
            H_norm = H / np.log(nA)

            # Arbitration: combine entropy cue and load penalty
            wm_weight = wm_weight0 * (eta_ent * H_norm + (1.0 - eta_ent) * load_term)
            wm_weight = min(1.0, max(0.0, wm_weight))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: leak toward uniform; encode if rewarded
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated WM encoding under load.

    Idea:
    - RL learns via a delta rule.
    - WM encoding is gated by positive prediction error (PE): surprising rewards trigger stronger WM updates.
    - Higher set size reduces both WM mixture weight and the effective encoding strength.
    - WM leaks toward uniform each trial.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: Inverse temperature for RL choice (scaled internally by 10)
    - wm_weight0: Base WM mixture weight (0..1)
    - pe_thresh: Threshold on positive PE for WM encoding (>=0). Larger -> harder to encode.
    - load_kappa: Load sensitivity (>=0). Larger -> stronger reduction of WM with set size.
    - wm_forget: WM leak toward uniform per trial (0..1)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_weight0, pe_thresh, load_kappa, wm_forget = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load attenuation factor for WM mixture and encoding
        load_att = np.exp(-load_kappa * max(0.0, float(nS) - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture: WM weight reduced by load
            wm_weight = wm_weight0 * load_att
            wm_weight = min(1.0, max(0.0, wm_weight))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(1e-12, min(1.0, p_total))
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM leak toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # PE-gated WM encoding strength for positive surprises
            pe_pos = max(0.0, delta)
            # Smooth gating around threshold; scaled by load attenuation
            gate = 1.0 / (1.0 + np.exp(-10.0 * (pe_pos - pe_thresh)))
            encode_strength = gate * load_att

            if encode_strength > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * one_hot

            # Normalize for numerical stability
            w[s, :] = np.maximum(w[s, :], 1e-12)
            w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p