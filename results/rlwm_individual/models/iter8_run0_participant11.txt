def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hebbian WM with load-dependent precision and leaky normalization.

    Rationale:
    - RL uses a standard delta-rule and softmax.
    - WM stores rewarded stimulus-action associations via Hebbian increments and
      decays toward uniform (leaky memory). Under higher loads (nS=6), WM's policy
      becomes noisier via a load-dependent temperature inflation.
    - Arbitration is a fixed mixture weight for a given block, set by wm_weight but
      WM's effective precision is reduced by load.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight of WM policy in choice (0..1)
    - softmax_beta: Base RL inverse temperature (scaled by 10 internally)
    - hebb: WM Hebbian encoding strength for rewarded actions (>=0)
    - wm_decay: WM leak toward uniform each trial (0..1)
    - load_noise: Increases WM stochasticity as set size grows (>=0).
                  Effective WM beta = softmax_beta_wm / (1 + load_noise*(nS-3))

    Returns: negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, hebb, wm_decay, load_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM precision reduction
        wm_beta_eff = softmax_beta_wm / (1.0 + load_noise * max(0, (nS - 3)))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax over W_s with load-degraded precision
            p_wm = 1.0 / np.sum(np.exp(wm_beta_eff * (W_s - W_s[a])))

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(p_total, eps))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: leaky Hebbian toward rewarded action, decay toward uniform
            # Leak/decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Hebbian potentiation if rewarded, mild depression otherwise
            if r > 0.5:
                # Potentiate chosen action and normalize row
                w[s, a] += hebb
            else:
                # Mild redistribution away from chosen action
                reduce = 0.5 * hebb
                w[s, a] = max(0.0, w[s, a] - reduce)
                inc = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += inc

            # Renormalize WM row to a valid distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Uncertainty-based arbitration: RL uncertainty vs WM confidence with forgetting.

    Rationale:
    - RL learns Q by delta-rule; additionally tracks state-action uncertainty via
      a running mean squared prediction error (MSPE). Higher RL uncertainty reduces
      RL's contribution.
    - WM stores recent state-action mappings and a confidence weight; confidence grows
      with rewarded repetition and decays (forgetting) otherwise.
    - Arbitration combines base wm_weight with a sigmoid of (WM confidence - RL uncertainty),
      and incorporates load by diluting WM confidence when nS is larger.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight0: Baseline WM weight (0..1)
    - softmax_beta: Base RL inverse temperature (scaled by 10 internally)
    - rl_unc_temp: Temperature scaling for RL uncertainty in arbitration (>=0)
    - wm_conf_slope: Slope mapping WM confidence into arbitration (>=0)
    - wm_forget: WM confidence forgetting rate per trial (0..1)

    Returns: negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight0, softmax_beta, rl_unc_temp, wm_conf_slope, wm_forget = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # RL uncertainty tracker: per (s,a) running MSPE initialized high-ish/uniform
        mspe = 0.5 * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: deterministic softmax on W_s
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute arbitration weight via uncertainty vs confidence
            pe = r - Q_s[a]
            # RL uncertainty proxy (bounded to [0,1])
            mspe[s, a] = 0.9 * mspe[s, a] + 0.1 * (pe**2)
            rl_uncertainty = max(0.0, min(1.0, mspe[s, a]))

            # WM confidence for current state: margin between best and 2nd best
            sorted_W = np.sort(W_s)[::-1]
            wm_conf = max(0.0, sorted_W[0] - (sorted_W[1] if len(sorted_W) > 1 else 0.0))
            # Dilute confidence by load (3/nS factor)
            wm_conf *= (3.0 / float(nS))

            # Map to arbitration weight using a logistic transform
            x = wm_conf_slope * wm_conf - rl_unc_temp * rl_uncertainty
            wm_weight_t = 1.0 / (1.0 + np.exp(-(x)))  # in (0,1)
            # Blend with baseline
            wm_weight_t = 0.5 * wm_weight_t + 0.5 * max(0.0, min(1.0, wm_weight0))

            p_total = p_wm*wm_weight_t + (1-wm_weight_t)*p_rl
            log_p += np.log(max(p_total, eps))
      
            # RL update
            q[s][a] += lr*(pe)

            # WM update with confidence dynamics and forgetting:
            # - On reward: move W_s toward a one-hot on chosen a (confidence increase)
            # - On no reward: gentle forgetting and redistribution
            # Forget toward uniform
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            if r > 0.5:
                # Move probability mass toward chosen action proportionally to current deficit
                deficit = 1.0 - w[s, a]
                gain = deficit * 0.5  # half-way pull
                w[s, a] += gain
                # Normalize row
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
            else:
                # Small penalization of chosen action, redistribute to others
                reduce = 0.2 * (w[s, a])
                w[s, a] -= reduce
                inc = reduce / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += inc

            # Final normalization safety
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces and WM rehearsal on state repetitions plus lapse.

    Rationale:
    - RL uses a delta-rule with eligibility traces (recency credit assignment).
    - WM rehearses when the same state repeats: boosts the chosen action in WM for repeated states,
      scaled by rehearsal_gain. Under higher load, repetitions are rarer; thus WM impact is implicitly reduced.
    - A small lapse parameter mixes in uniform random choice.
    - Arbitration uses a fixed wm_weight; WM policy is softmax over its map.

    Parameters:
    - lr: RL learning rate (0..1)
    - wm_weight: Mixture weight of WM policy (0..1)
    - softmax_beta: RL inverse temperature (scaled by 10 internally)
    - lambda_trace: Eligibility trace decay (0..1)
    - rehearsal_gain: WM rehearsal increment when state repeats (>=0)
    - lapse: Lapse probability mixing in uniform choice (0..1)

    Returns: negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, lambda_trace, rehearsal_gain, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility traces per (s,a)
        e = np.zeros((nS, nA))
        last_state = None

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture of WM and RL with lapse
            p_mix = (wm_weight * p_wm + (1 - wm_weight) * p_rl)
            p_total = (1 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, eps))
      
            # RL update with eligibility traces
            pe = r - Q_s[a]
            # Decay all eligibilities
            e *= lambda_trace
            # Set current (s,a) eligibility to 1
            e[s, a] = 1.0
            # Update all Q with TD error weighted by eligibility
            q += lr * pe * e

            # WM update:
            # 1) Gentle normalization toward uniform (prevents runaway)
            w[s, :] = 0.95 * w[s, :] + 0.05 * w_0[s, :]

            # 2) Rehearsal if the same state repeats
            if last_state is not None and s == last_state:
                w[s, a] += rehearsal_gain
            else:
                # If not a repetition but reward occurred, lightly reinforce
                if r > 0.5:
                    w[s, a] += 0.5 * rehearsal_gain

            # 3) Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

            last_state = s

        blocks_log_p += log_p

    return -blocks_log_p