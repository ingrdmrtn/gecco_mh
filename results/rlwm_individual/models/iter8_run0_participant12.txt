def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-gated mixture and leaky WM storage

    Description:
    - RL: tabular Q-learning with standard delta-rule.
    - WM: fast delta-rule storage toward a reward-contingent target, with leakage toward uniform.
    - Policy mixture: The WM/RL mixture is dynamically gated by the entropy of the RL policy:
      higher RL entropy (greater uncertainty) shifts weight toward WM. The baseline mixture
      is wm_base; the sensitivity to RL entropy is ent_slope. This allows WM to assist
      primarily when RL is uncertain, and the gate can flex with load via the change in
      RL entropy that typically occurs with larger set sizes.
    - Load influence: Though no explicit load parameter, the entropy gate naturally yields
      less WM reliance when RL becomes confident (lower entropy), which tends to happen
      more in small set sizes.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - wm_base: Baseline WM mixture weight before entropy gating (0..1)
    - softmax_beta: RL inverse temperature; internally scaled x10
    - wm_lr: WM learning rate for delta update toward a target (0..1)
    - wm_leak: WM leakage toward uniform per trial (0..1)
    - ent_slope: Sensitivity of WM mixture to RL policy entropy (can be positive/negative)
      Effective mixture weight = sigmoid(logit(wm_base) + ent_slope*(H_rl - ln(nA)/2))

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, wm_base, softmax_beta, wm_lr, wm_leak, ent_slope = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, eps, 1.0 - eps)
        return np.log(p) - np.log(1.0 - p)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy probability of chosen action (softmax written as denom trick)
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy probability (deterministic softmax with high beta)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Compute RL policy distribution to estimate its entropy
            rl_logits = softmax_beta * Q_s
            rl_logits -= np.max(rl_logits)
            rl_probs = np.exp(rl_logits)
            rl_probs /= np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(np.clip(rl_probs, eps, 1.0)))  # nats

            # Entropy-gated mixture weight
            mix_logit = logit(np.clip(wm_base, 0.0, 1.0)) + ent_slope * (H_rl - 0.5 * np.log(nA))
            wm_weight_eff = sigmoid(mix_logit)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: leak toward uniform then learn toward reward-contingent target
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :].copy()  # no-reward: forget toward uniform
            # Leak
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
            # Learn toward target
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Pearce-Hall–style adaptive RL learning rate + WM with load-dependent swaps

    Description:
    - RL: Q-learning with an adaptive, state-specific learning rate modulated by a
      Pearce-Hall–like attention trace that tracks unsigned prediction error.
      alpha_s = clip(lr0 * (1 + att_gain * att_s), 0..1), with
      att_s <- (1 - att_decay)*att_s + att_decay*|PE|.
    - WM: On reward, WM stores a near one-hot distribution but with load-dependent
      swap interference that mis-assigns a fraction of mass to other actions.
      swap_eff = min(1, swap_base * (nS - 1)) increases with set size.
      On no reward, WM drifts toward uniform.
    - Mixture: fixed mixture weight wm_weight between WM and RL policies.

    Parameters (tuple):
    - lr0: Base RL learning rate (0..1)
    - wm_weight: Mixture weight for WM policy (0..1)
    - softmax_beta: RL inverse temperature; internally scaled x10
    - att_gain: Gain scaling from attention trace to effective learning rate (>=0)
    - att_decay: Attention trace update rate (0..1)
    - swap_base: Base swap interference per added item in set (0..1)

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr0, wm_weight, softmax_beta, att_gain, att_decay, swap_base = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        att = np.zeros(nS)  # attention trace per state

        # Load-dependent swap interference
        swap_eff = min(1.0, max(0.0, swap_base) * max(0, nS - 1))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(eps, p_total))

            # RL update with adaptive learning rate
            pe = r - q[s, a]
            att[s] = (1.0 - att_decay) * att[s] + att_decay * abs(pe)
            alpha = np.clip(lr0 * (1.0 + att_gain * att[s]), 0.0, 1.0)
            q[s, a] += alpha * pe

            # WM update: reward => one-hot with swap interference; no reward => uniform
            if r > 0.5:
                target = np.ones(nA) * (swap_eff / (nA - 1.0))
                target[a] = 1.0 - swap_eff
            else:
                target = w_0[s, :].copy()
            w[s, :] = (1.0 - alpha) * w[s, :] + alpha * target  # couple learning rates: more surprising => faster WM update

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Load-gated mixture + spacing-sensitive WM decay + RL Q-learning

    Description:
    - RL: standard Q-learning with fixed learning rate.
    - WM: delta-rule toward one-hot on reward, and toward uniform when unrewarded.
      WM representations decay as a function of time (trials) since the state was last
      seen: w[s] <- (1 - d)*w[s] + d*uniform, where d = 1 - exp(-time_sens * delta_t).
      This captures spacing-dependent forgetting, more pronounced with longer lags.
    - Mixture: lapse-regularized mixture. The WM/RL weight is a sigmoid function of
      set size: mix = sigmoid(mix_slope * (nS_ref - nS)), so WM dominates in small sets
      and RL dominates in large sets. A lapse parameter mixes in a uniform policy.

    Parameters (tuple):
    - lr: RL learning rate (0..1)
    - mix_slope: Slope controlling how sharply mixture shifts with set size
    - softmax_beta: RL inverse temperature; internally scaled x10
    - wm_lr: WM learning rate for delta update (0..1)
    - time_sens: Sensitivity of WM decay to time since last state presentation (>=0)
    - lapse: Lapse rate mixing with uniform random choice (0..1)

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, mix_slope, softmax_beta, wm_lr, time_sens, lapse = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask].astype(float)
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_seen = -np.ones(nS, dtype=int)

        # Load-gated mixture weight (constant within block given fixed set size)
        nS_ref = 4.5  # pivot between 3 and 6
        wm_weight = sigmoid(mix_slope * (nS_ref - nS))
        uniform_p = 1.0 / nA

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Spacing-dependent WM decay before computing policy
            if last_seen[s] >= 0:
                delta_t = max(1, t - last_seen[s])
                d = 1.0 - np.exp(-max(0.0, time_sens) * float(delta_t))
                w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(eps, denom_rl)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(eps, denom_wm)

            # Lapse-regularized mixture
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * uniform_p
            log_p += np.log(max(eps, p_total))

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update toward target
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
            else:
                target = w_0[s, :].copy()
            w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * target

            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p