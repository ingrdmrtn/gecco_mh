def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited WM with load-dependent precision and lapse, plus WM decay.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM policy: softmax over a WM table with an effective precision that decreases
      with set size, and a load-dependent lapse to uniform responding.
    - WM update: global decay toward uniform that grows with load, and reward-based
      encoding for the current state.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_beta0, eps_load, wm_decay)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - wm_beta0: base WM inverse temperature (precision) at low load.
        - eps_load: controls load-dependent WM lapse and precision drop (>0).
        - wm_decay: WM decay/encoding rate in [0,1]; also scaled by load for global decay.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_beta0, eps_load, wm_decay = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic (upper bound if no load)
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Load-dependent precision: drops as set size increases.
            beta_wm_eff = wm_beta0 / (1.0 + eps_load * max(0, nS - 3))
            # Stable computation of softmax over WM table
            Wc = W_s - np.max(W_s)
            p_vec_wm = np.exp(beta_wm_eff * Wc)
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            # Load-dependent lapse to uniform responding
            eps = 1.0 - np.exp(-eps_load * nS)
            eps = np.clip(eps, 0.0, 0.5)
            p_wm = (1.0 - eps) * p_vec_wm[a] + eps * (1.0 / nA)

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward uniform increases with set size
            decay_rate = wm_decay * (nS / 6.0)
            decay_rate = np.clip(decay_rate, 0.0, 1.0)
            w = (1.0 - decay_rate) * w + decay_rate * w_0

            # Reward-based encoding for the current state
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            enc_rate = np.clip(2.0 * wm_decay, 0.0, 1.0)
            if r > 0.5:
                w[s, :] = (1.0 - enc_rate) * w[s, :] + enc_rate * one_hot
            else:
                # On error, no extra encoding beyond global decay
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with cross-state interference and decay.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM policy: softmax over WM with adjustable gain.
    - WM update:
        - Rewarded trials strengthen the (state, action) association.
        - A fraction of the update "spills over" to other states (binding interference),
          which worsens selectivity as set size grows.
        - Global decay toward uniform, scaled by load.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_alpha, spill_xi, decay_lambda)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - wm_alpha: WM learning rate (encoding strength) in [0,1].
        - spill_xi: fraction of WM update that spills into other states in [0,1].
        - decay_lambda: base global WM decay rate in [0,1]; scaled by load.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_alpha, spill_xi, decay_lambda = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Adjustable WM gain modulates sharpness of WM policy.
            beta_wm_eff = softmax_beta_wm * max(1e-3, wm_alpha)
            Wc = W_s - np.max(W_s)
            p_vec_wm = np.exp(beta_wm_eff * Wc)
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global decay toward uniform increases with set size
            decay_rate = decay_lambda * (nS / 6.0)
            decay_rate = np.clip(decay_rate, 0.0, 1.0)
            w = (1.0 - decay_rate) * w + decay_rate * w_0

            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                # Strengthen the correct association in the current state
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * one_hot

                # Spillover interference: update leaks into other states
                if nS > 1 and spill_xi > 0:
                    leak = (spill_xi * wm_alpha) / max(1, nS - 1)
                    for k in range(nS):
                        if k == s:
                            continue
                        w[k, :] = (1.0 - leak) * w[k, :] + leak * one_hot
            else:
                # On errors, no targeted encoding beyond global decay
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty- and load-scaled WM precision; adaptive WM update.

    Mechanism
    - RL: delta-rule Q-learning with softmax.
    - WM policy: softmax over WM with inverse temperature reduced by:
        - Entropy of the WM state distribution (higher uncertainty -> lower precision).
        - Set size (higher load -> lower precision).
    - WM update: rewarded trials move WM toward one-hot with wm_lr; unrewarded trials
      move WM toward uniform with wm_lr.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, wm_lr, tau_entropy_slope, tau_load_slope)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - wm_lr: WM learning rate in [0,1] for both strengthen and decay.
        - tau_entropy_slope: scales entropy-driven reduction of WM precision (>=0).
        - tau_load_slope: scales set-size-driven reduction of WM precision (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_lr, tau_entropy_slope, tau_load_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # baseline deterministic bound
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute entropy of WM distribution for the current state
            eps = 1e-12
            P = np.clip(W_s / np.sum(W_s), eps, 1.0)
            P = P / np.sum(P)
            H = -np.sum(P * np.log(P))  # in nats, max ~ log(nA)

            # Effective inverse temperature decreases with entropy and load
            denom = 1.0 + tau_entropy_slope * H + tau_load_slope * max(0, nS - 3)
            beta_wm_eff = softmax_beta_wm / max(denom, 1e-6)

            Wc = W_s - np.max(W_s)
            p_vec_wm = np.exp(beta_wm_eff * Wc)
            p_vec_wm = p_vec_wm / np.sum(p_vec_wm)
            p_wm = p_vec_wm[a]

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            if r > 0.5:
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * one_hot
            else:
                w[s, :] = (1.0 - wm_lr) * w[s, :] + wm_lr * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p