def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, age-sensitive working memory with decay.

    Mechanism:
    - Q-learning tracks action values per state.
    - A parallel working-memory (WM) store keeps a single best-known action per state when a
      reward is obtained. The WM's influence depends on available capacity relative to load.
    - WM capacity depends on age group (younger vs. older). WM entries decay over trials.
    - The policy is a mixture of a WM policy (if bound and retrieved) and an RL softmax policy.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - beta: inverse temperature (>0) for RL softmax
    - K_y: effective WM capacity for younger participants (>=0)
    - K_o: effective WM capacity for older participants (>=0)
    - decay: WM decay rate per trial in [0,1]; higher => faster forgetting of WM bindings

    Inputs:
    - states: array of state indices (0..nS-1 within each block)
    - actions: array of chosen actions (0..2); out-of-range treated as lapses (uniform)
    - rewards: array of feedback values (arbitrary numeric; mapped to r_bin = 1 if >0 else 0)
    - blocks: array indicating block membership for each trial
    - set_sizes: array giving the set size of the current block at each trial (3 or 6)
    - age: array-like with single value, participant age in years
    - model_parameters: [lr, beta, K_y, K_o, decay]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr, beta, K_y, K_o, decay = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        # RL values
        Q = np.zeros((nS, nA))

        # WM store: for each state, the bound action and its current strength w in [0,1]
        wm_bind = -np.ones(nS, dtype=int)
        wm_strength = np.zeros(nS, dtype=float)

        # Select capacity by age group
        K = K_o if is_older > 0 else K_y
        # Capacity relative to load (probability WM is available for a given state)
        # Clamp to [0,1]; scales down under higher load.
        capacity_factor = min(1.0, max(0.0, K / float(nS)))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0

            # RL softmax
            q_s = Q[s, :] - np.max(Q[s, :])
            prl = np.exp(beta * q_s)
            prl = prl / (np.sum(prl) + eps)

            # WM policy for this state
            if wm_bind[s] >= 0 and wm_strength[s] > 0:
                pwm = np.zeros(nA)
                pwm[int(wm_bind[s])] = 1.0
                # Effective retrieval probability combines capacity and current strength
                p_retrieve = capacity_factor * wm_strength[s]
                p_retrieve = max(0.0, min(1.0, p_retrieve))
            else:
                pwm = np.ones(nA) / nA
                p_retrieve = 0.0

            # Mixture policy
            p = p_retrieve * pwm + (1.0 - p_retrieve) * prl

            # Likelihood contribution
            if a < 0 or a >= nA:
                # Treat out-of-range actions as uniform-lapse observations; no learning
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # WM global decay step
                wm_strength *= (1.0 - decay)
                continue
            else:
                p_a = p[a]
                nll -= np.log(max(p_a, eps))

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += lr * delta

            # WM update and decay
            # First decay all WM strengths slightly each trial
            wm_strength *= (1.0 - decay)
            # If rewarded, refresh binding for this state to the chosen action
            if r > 0.0:
                wm_bind[s] = a
                wm_strength[s] = 1.0  # full strength upon successful reward

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age-by-load adaptive learning rate, exploration bonus, and state-wise choice kernel.

    Mechanism:
    - Q-learning with trial-wise learning rate reduced under higher set size, especially for older adults.
    - Exploration is encouraged via an uncertainty-guided bonus that depends on (a) state uncertainty
      and (b) action-specific novelty (fewer visits -> larger bonus).
    - Choice kernel (state-wise stickiness) biases repeating the last action in that state.

    Parameters (model_parameters):
    - lr0: base learning rate in [0,1]
    - beta0: inverse temperature (>0) for choice
    - bonus: scale (>0) for exploration bonus
    - eta_age_load: sensitivity (>0) of learning rate to age-by-load (reduces lr for older and high load)
    - stick: choice kernel weight (can be positive or negative)

    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2); out-of-range treated as lapses (uniform)
    - rewards: array of feedback values (mapped to r_bin = 1 if >0 else 0)
    - blocks: array of block indices
    - set_sizes: array of set sizes (3 or 6)
    - age: array-like with single value, participant age in years
    - model_parameters: [lr0, beta0, bonus, eta_age_load, stick]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    lr0, beta0, bonus, eta_age_load, stick = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        Q = np.zeros((nS, nA))
        # Visit counts per state-action for novelty bonus
        N = np.zeros((nS, nA))
        # Last action per state for choice kernel (-1 means none yet)
        last_a = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            # Age-by-load adjusted learning rate (older and higher load -> lower lr)
            # lr_eff = lr0 * sigmoid( - eta_age_load * is_older * (load - 3) )
            # which is in (0, lr0); use a smooth saturating decrease
            mod = 1.0 / (1.0 + np.exp(eta_age_load * is_older * (load - 3.0)))
            lr_eff = lr0 * mod

            # Exploration bonus:
            # - State uncertainty via entropy of softmax(Q) (normalized to [0,1])
            q_s = Q[s, :]
            q_soft = np.exp(q_s - np.max(q_s))
            q_soft = q_soft / (np.sum(q_soft) + eps)
            H = -np.sum(q_soft * np.log(q_soft + eps))  # entropy in nats
            H_max = np.log(nA)
            u_state = H / (H_max + eps)  # in [0,1]

            # - Action novelty: b[a] = 1/sqrt(N_sa+1)
            novelty = 1.0 / np.sqrt(N[s, :] + 1.0)

            # Augmented values
            Q_aug = Q[s, :] + bonus * u_state * novelty

            # Choice kernel (stickiness on last action in this state)
            bias = np.zeros(nA)
            if last_a[s] >= 0:
                bias[int(last_a[s])] = stick

            logits = beta0 * Q_aug + bias
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            # Likelihood
            if a < 0 or a >= nA:
                # Treat as lapse (uniform); skip learning
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                continue
            else:
                nll -= np.log(max(p[a], eps))

            # Learning updates
            delta = r - Q[s, a]
            Q[s, a] += lr_eff * delta
            N[s, a] += 1.0
            last_a[s] = a

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian habit learner with age-dependent priors, load-sensitive lapses, and leaky consolidation.

    Mechanism:
    - For each state, maintain a Dirichlet posterior over the 3 actions indicating which action is
      correct. The prior concentration differs by age group.
    - Choice uses a softmax over log posterior means (akin to Thompson/logit rule).
    - Lapses increase with set size and age.
    - After each trial, apply leaky consolidation toward the prior (forgetting), then update the
      chosen action's count if reward is positive.

    Parameters (model_parameters):
    - beta: inverse temperature (>0) applied to log posterior means
    - conc_y: prior concentration per action for younger adults (>0)
    - conc_o: prior concentration per action for older adults (>0)
    - lam_forget: leak toward prior per update in [0,1]
    - lapse0: base lapse rate in [0,1], scaled by load and age

    Inputs:
    - states: array of state indices
    - actions: array of chosen actions (0..2); out-of-range treated as lapses (uniform)
    - rewards: array of feedback values (mapped to r_bin = 1 if >0 else 0)
    - blocks: array of block indices
    - set_sizes: array of set sizes (3 or 6)
    - age: array-like with single value, participant age in years
    - model_parameters: [beta, conc_y, conc_o, lam_forget, lapse0]

    Returns:
    - Negative log-likelihood of observed choices under the model.
    """
    beta, conc_y, conc_o, lam_forget, lapse0 = model_parameters
    is_older = 1.0 if age[0] >= 45 else 0.0

    nll = 0.0
    eps = 1e-12
    nA = 3

    # Choose prior based on age
    prior_conc = float(conc_o) if is_older > 0 else float(conc_y)

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask]
        b_actions = actions[mask]
        b_rewards = rewards[mask]
        b_set_sizes = set_sizes[mask]
        nS = int(b_set_sizes[0])

        # Dirichlet parameters per state-action
        Alpha = np.ones((nS, nA)) * prior_conc
        Alpha0 = np.ones(nA) * prior_conc  # for forgetting target

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = 1.0 if b_rewards[t] > 0 else 0.0
            load = float(b_set_sizes[t])

            # Posterior mean over actions
            mean_p = Alpha[s, :] / (np.sum(Alpha[s, :]) + eps)

            # Softmax over log means (equivalent to power transform of means)
            logits = beta * np.log(mean_p + eps)
            logits = logits - np.max(logits)
            p = np.exp(logits)
            p = p / (np.sum(p) + eps)

            # Load- and age-sensitive lapse
            # Scale from base: higher load and older -> larger lapse, capped below 0.5
            load_scale = (load / 3.0) ** (1.0 + 0.5 * is_older)
            lapse_t = min(0.49, max(0.0, lapse0 * load_scale))
            p_final = (1.0 - lapse_t) * p + lapse_t * (np.ones(nA) / nA)

            # Likelihood
            if a < 0 or a >= nA:
                p_a = 1.0 / nA
                nll -= np.log(max(p_a, eps))
                # Skip learning for invalid actions
                continue
            else:
                nll -= np.log(max(p_final[a], eps))

            # Forgetting (leak toward prior on this state's counts)
            Alpha[s, :] = (1.0 - lam_forget) * Alpha[s, :] + lam_forget * Alpha0

            # Reward-driven update: increment only if reward is positive
            if r > 0.0:
                Alpha[s, a] += 1.0

    return nll