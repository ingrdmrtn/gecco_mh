def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 1: Q-learning plus one-shot rule evidence, with age- and set-size-modulated consolidation and mixing.

    Mechanism:
      - Model-free Q-learning within blocks (state-action specific).
      - Parallel evidence accumulator E for "rule-like" mapping: when a rewarded action occurs in a state,
        evidence for that state-action increases; otherwise evidence decays slightly.
      - Decision policy mixes the rule-based softmax over E and the RL softmax over Q.
      - Consolidation and mixture weight depend on set size (weaker at larger set sizes) and age group.

    Parameters (model_parameters)
    --------------------------------
    alpha_q : float
        Q-learning rate in [0,1].
    beta_q : float
        Inverse temperature for Q softmax (>0); scaled internally for stability.
    cons_rate : float
        Base consolidation rate for evidence increments in [0,1].
    conf_slope : float
        Inverse temperature for rule evidence softmax (>0).
    age_conf_shift : float
        Reduces consolidation and mixture weight in older adults (>=45).
    mix_logit0 : float
        Baseline logit controlling rule-vs-RL mixture weight.

    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial in its block (e.g., 3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    model_parameters : sequence
        See parameter list above.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha_q, beta_q, cons_rate, conf_slope, age_conf_shift, mix_logit0 = model_parameters
    beta_q = 5.0 * max(beta_q, eps)
    conf_slope = 5.0 * max(conf_slope, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize values and evidence at block start
        Q = np.zeros((nS, nA))
        E = np.zeros((nS, nA))  # rule evidence

        # Set-size and age modulations
        ss_scale = 3.0 / float(nS)  # 1 for nS=3, 0.5 for nS=6
        cons_eff = np.clip(cons_rate * ss_scale * (1.0 - age_conf_shift * is_older), 0.0, 1.0)

        # Mixture weight baseline adjusted by set size and age
        mix_logit = mix_logit0 + (ss_scale - 1.0) - age_conf_shift * is_older
        # mixture weight is state-specific via addition of mean evidence, computed each trial

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta_q * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Rule policy from evidence
            e_s = E[s, :].copy()
            e_s -= np.max(e_s)
            p_rule = np.exp(conf_slope * e_s)
            p_rule = p_rule / (np.sum(p_rule) + eps)

            # Mixture weight depends on baseline + current state's mean evidence
            mean_e = float(np.mean(E[s, :])) if nA > 0 else 0.0
            w = 1.0 / (1.0 + np.exp(-(mix_logit + mean_e)))
            p = w * p_rule + (1.0 - w) * p_rl
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # Update Q
            pe = r - Q[s, a]
            Q[s, a] += alpha_q * pe

            # Update evidence E: rewarded trials strengthen, non-reward weakly decay
            if r > 0.5:
                # strengthen evidence for chosen action, mild competitive suppression
                E[s, :] *= (1.0 - 0.25 * cons_eff)
                E[s, a] += cons_eff
            else:
                # decay evidence slightly when not rewarded
                E[s, :] *= (1.0 - 0.10 * cons_eff)

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 2: Q-learning with a global action-valence kernel, modulated by set size and age.

    Mechanism:
      - State-action Q-learning within blocks.
      - A global action kernel G (shared across states) tracks the overall valence of each action
        based on recent rewards, capturing a heuristic bias to repeat globally successful responses.
      - Decision logits combine state-specific Q and global G with separate temperatures.
      - Learning rate for the global kernel scales with set size (more reliance when sets are larger)
        and kernel impact is amplified in older adults.

    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Inverse temperature for Q softmax (>0); scaled internally.
    bias_lr : float
        Base learning rate for the global action kernel in [0,1].
    bias_temp : float
        Temperature weight for the global kernel contribution (>0).
    age_bias_shift : float
        Multiplicative boost of bias_temp in older adults (>=45).
    ss_bias_boost : float
        Scales bias_lr with set size; effective bias_lr *= (1 + ss_bias_boost * (nS-3)/3).

    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial in its block (e.g., 3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    model_parameters : sequence
        See parameter list above.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, bias_lr, bias_temp, age_bias_shift, ss_bias_boost = model_parameters
    beta = 5.0 * max(beta, eps)
    bias_temp = 5.0 * max(bias_temp, eps)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        # Initialize
        Q = np.zeros((nS, nA))
        G = np.zeros(nA)  # global action valence kernel

        # Set-size and age modulations
        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        bias_lr_eff = np.clip(bias_lr * (1.0 + ss_bias_boost * ss_factor), 0.0, 1.0)
        bias_temp_eff = bias_temp * (1.0 + age_bias_shift * is_older)

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Combined logits: state-specific Q and global kernel G
            logits = beta * (Q[s, :] - np.max(Q[s, :])) + bias_temp_eff * (G - np.max(G))
            # Softmax
            exp_logits = np.exp(logits)
            p = exp_logits / (np.sum(exp_logits) + eps)
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # Update Q-learning
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update global action kernel
            # Small decay towards zero to keep kernel bounded
            G *= (1.0 - 0.10 * bias_lr_eff)
            # Reinforce chosen action proportional to reward prediction (simple valence)
            delta_g = (2.0 * r - 1.0)  # +1 if reward, -1 if no reward
            G[a] += bias_lr_eff * delta_g

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Model 3: RL with uncertainty-driven lapses modulated by set size and age.

    Mechanism:
      - Standard Q-learning for state-action values.
      - Choice policy is a mixture of the RL softmax and a uniform random policy.
      - The lapse probability increases with decision uncertainty (entropy of the RL policy),
        with additional increases at larger set sizes and in older adults.

    Parameters (model_parameters)
    --------------------------------
    alpha : float
        Q-learning rate in [0,1].
    beta : float
        Inverse temperature for RL softmax (>0); scaled internally.
    lapse0 : float
        Baseline lapse probability in (0,1) represented as a probability (internally converted to logit).
    ent_gain : float
        Sensitivity of lapse log-odds to RL policy entropy (>=0).
    age_lapse_shift : float
        Additive shift on lapse log-odds for older adults (>=45).
    ss_lapse_gain : float
        Gain scaling of lapse log-odds with set size (relative to 3).

    Inputs
    ------
    states : array-like (T,)
        State index per trial (0..nS-1 within a block).
    actions : array-like (T,)
        Chosen action per trial (0..2).
    rewards : array-like (T,)
        Binary rewards (0/1).
    blocks : array-like (T,)
        Block index per trial.
    set_sizes : array-like (T,)
        Set size for each trial in its block (e.g., 3 or 6).
    age : array-like or scalar
        Participant age; older group defined as age >= 45.
    model_parameters : sequence
        See parameter list above.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    nA = 3
    eps = 1e-12

    alpha, beta, lapse0, ent_gain, age_lapse_shift, ss_lapse_gain = model_parameters
    beta = 5.0 * max(beta, eps)
    lapse0 = np.clip(lapse0, 1e-6, 1.0 - 1e-6)

    age_val = age[0] if hasattr(age, "__len__") and not np.isscalar(age) else age
    is_older = 1.0 if age_val >= 45 else 0.0

    def logit(p):
        p = np.clip(p, 1e-12, 1.0 - 1e-12)
        return np.log(p) - np.log(1.0 - p)

    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_states = states[idx].astype(int)
        block_actions = actions[idx].astype(int)
        block_rewards = rewards[idx].astype(float)
        nS = int(set_sizes[idx][0])

        Q = np.zeros((nS, nA))

        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # RL softmax
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)
            p_rl = np.clip(p_rl, eps, 1.0)
            p_rl /= np.sum(p_rl)

            # Entropy of RL policy (uncertainty)
            H = -np.sum(p_rl * np.log(np.clip(p_rl, eps, 1.0)))

            # Lapse probability from entropy, set size, and age
            lapse_logit = logit(lapse0) + ent_gain * H + age_lapse_shift * is_older + ss_lapse_gain * ss_factor
            lapse_prob = 1.0 / (1.0 + np.exp(-lapse_logit))
            lapse_prob = np.clip(lapse_prob, 0.0 + 1e-6, 1.0 - 1e-6)

            # Mixture of RL and uniform random
            p = (1.0 - lapse_prob) * p_rl + lapse_prob * (np.ones(nA) / nA)
            p = np.clip(p, eps, 1.0)
            p /= np.sum(p)

            total_logp += np.log(p[a])

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

    return -float(total_logp)