def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + capacity-limited, decaying WM.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy (provided by template).
    - WM system: per-state action strengths that are (i) updated by reward,
      (ii) decay toward uniform, and (iii) diluted by set-size-dependent capacity.
    - Arbitration: fixed mixture weight (wm_weight from template line); capacity is folded
      into WM policy by mixing WM with uniform when nS exceeds capacity.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_weight: Mixture weight assigned to WM policy in p_total (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - wm_learn: WM learning rate towards one-hot when rewarded (>=0).
    - wm_decay: Per-trial decay of WM strengths toward uniform (0..1).
    - capacity_k: Effective WM capacity (in number of states). For nS > capacity_k,
                  WM policy is partially replaced by uniform.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_learn, wm_decay, capacity_k = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Softmax over WM strengths, then capacity-adjusted shrinkage toward uniform.
            wm_pref = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_pref = wm_pref / np.sum(wm_pref)
            # Capacity gate: when nS > capacity_k, mix with uniform
            cap_gate = min(1.0, max(0.0, capacity_k) / max(1.0, nS))
            wm_probs = cap_gate * wm_pref + (1.0 - cap_gate) * (1.0 / nA)
            p_wm = max(1e-12, wm_probs[a])

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(1e-12, p_total))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay all actions for this state toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.0:
                # Write toward one-hot on chosen action
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
            else:
                # If unrewarded, slight redistribution toward alternatives
                if nA > 1:
                    leak = wm_learn / (nA - 1)
                    for aa in range(nA):
                        if aa != a:
                            w[s, aa] += leak
                    w[s, a] = max(0.0, w[s, a] - wm_learn)

            # Normalize to keep w a valid distribution-like strength vector
            denom = np.sum(w[s, :])
            if denom > 0:
                w[s, :] /= denom

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + entropy-gated WM (uncertainty- and load-dependent access).

    Overview:
    - RL: delta-rule Q-learning with softmax (template).
    - WM: per-state action strengths updated by reward; policy access is gated
      by current choice uncertainty (entropy of RL policy) and set size.
    - Arbitration: mixture weight fixed in template; here the WM policy itself
      is adaptively mixed with uniform based on a gate that increases when RL
      is uncertain and decreases with load.

    Parameters:
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM in p_total (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - gate_base: Intercept for WM access gate (can be negative/positive).
    - gate_pe: Sensitivity of WM access to RL uncertainty (>=0). Higher -> more WM when RL is uncertain.
    - load_gain: Sensitivity of WM access to set size (>=0). Higher -> less WM at larger nS.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_base, gate_pe, load_gain = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute RL policy distribution to measure uncertainty (entropy)
            rl_pref = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_pref / np.sum(rl_pref)
            entropy = -np.sum(rl_probs * np.log(np.clip(rl_probs, 1e-12, 1.0)))
            # Normalize entropy to [0,1] relative to maximum entropy log(nA)
            entropy_norm = entropy / np.log(nA)

            # Base WM policy from WM strengths
            wm_pref = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_pref = wm_pref / np.sum(wm_pref)

            # Gate WM access by uncertainty (higher entropy -> stronger WM) and penalize by load
            gate_lin = gate_base + gate_pe * entropy_norm - load_gain * max(0, nS - 3)
            gate = 1.0 / (1.0 + np.exp(-gate_lin))  # sigmoid to [0,1]

            wm_probs = gate * wm_pref + (1.0 - gate) * (1.0 / nA)
            p_wm = max(1e-12, wm_probs[a])

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(1e-12, p_total))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward-dependent strengthening with gentle decay toward uniform
            decay = 0.1  # implicit small decay each trial; tied to gate to use params
            # Make decay scale with (1 - gate): less WM access -> more forgetting
            decay = np.clip((1.0 - gate), 0.0, 1.0)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            if r > 0.0:
                # Reinforce chosen action; amount scaled by gate (more access -> stronger write)
                alpha_w = np.clip(0.5 * gate, 0.0, 1.0)
                w[s, :] = (1.0 - alpha_w) * w[s, :]
                w[s, a] += alpha_w
            else:
                # On error, soften the chosen action and nudge others up a bit
                beta_w = np.clip(0.25 * (1.0 - gate), 0.0, 1.0)
                w[s, a] = (1.0 - beta_w) * w[s, a]
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] = (1.0 - beta_w) * w[s, aa] + beta_w / (nA - 1)

            # Renormalize
            denom = np.sum(w[s, :])
            if denom > 0:
                w[s, :] /= denom

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + recency-weighted WM with load-induced interference.

    Overview:
    - RL: delta-rule Q-learning with softmax (template).
    - WM: action bindings that (i) strengthen on reward (binding strength),
      (ii) decay with time since last visit to the state (recency),
      and (iii) suffer global interference that grows with set size.
    - Arbitration: template mixture; WM policy is additionally mixed with uniform
      based on a recency gate (recently seen states -> stronger WM).

    Parameters:
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM in p_total (0..1).
    - softmax_beta: RL inverse temperature (scaled by 10 internally).
    - bind_strength: WM learning rate toward one-hot on reward (>=0).
    - interference: Global interference magnitude per trial scaled by load (>=0).
    - recency: Time constant controlling recency gate; larger -> slower decay (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, bind_strength, interference, recency = model_parameters
    softmax_beta *= 10 # beta has a higher upper bound
    softmax_beta_wm = 50 # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Track last-seen time for each state to compute recency gating
        last_seen = -1 * np.ones(nS, dtype=int)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Compute recency gate based on time since last seen
            if last_seen[s] < 0 or recency <= 0:
                rec_gate = 0.0 if last_seen[s] < 0 else 1.0
            else:
                dt = max(1, t - last_seen[s])
                rec_gate = np.exp(-dt / max(1e-6, recency))
                rec_gate = np.clip(rec_gate, 0.0, 1.0)

            # Base WM preference from strengths
            wm_pref = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            wm_pref = wm_pref / np.sum(wm_pref)

            # Mix with uniform based on recency gate: older state -> more uniform
            wm_probs = rec_gate * wm_pref + (1.0 - rec_gate) * (1.0 / nA)
            p_wm = max(1e-12, wm_probs[a])

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            log_p += np.log(max(1e-12, p_total))
      
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global load-induced interference: move all states slightly toward uniform
            if nS > 0:
                gamma = np.clip(interference * max(0, nS - 3) / max(1, nS), 0.0, 1.0)
                w = (1.0 - gamma) * w + gamma * w_0

            # State-specific update
            if r > 0.0:
                # Bind rewarded action strongly
                w[s, :] = (1.0 - bind_strength) * w[s, :]
                w[s, a] += bind_strength
            else:
                # If error, slight recency-based decay to uniform for that state
                beta = np.clip((1.0 - rec_gate) * 0.5, 0.0, 1.0)
                w[s, :] = (1.0 - beta) * w[s, :] + beta * w_0[s, :]

            # Renormalize row s
            denom = np.sum(w[s, :])
            if denom > 0:
                w[s, :] /= denom

            # Update last seen time for this state
            last_seen[s] = t

        blocks_log_p += log_p

    return -blocks_log_p