Below are three standalone cognitive models that implement different mechanisms for combining reinforcement learning (RL) and working memory (WM) in the rlwm task. Each function follows the requested signature, returns the negative log-likelihood of observed choices, uses all parameters meaningfully, and includes set-size effects (3 vs 6). No imports are included (assume numpy as np is available).

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Confidence-gated WM encoding with set-size-scaled WM mixture.

    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta (scaled by 10 internally).
    - WM channel: softmax over WM weights with high inverse temperature (deterministic tendency).
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl.

    RL dynamics:
    - Single learning rate alpha.

    WM dynamics:
    - Decay toward uniform at rate wm_decay on each visit to state s.
    - Probabilistic encoding gate that favors rewarding, low-surprise outcomes:
        p_encode = sigmoid(conf_gate * (r - 0.5 - |PE|)),
      where PE = r - Q_s[a]. If encoding occurs, overwrite WM for that state with a one-hot vector at action a.

    Set-size effects:
    - WM mixture weight decays exponentially with set size:
        wm_weight_eff = wm_mix3 * exp(-mix_size_drop * (nS - 3)).

    Parameters (tuple):
    - alpha: RL learning rate (0..1).
    - wm_mix3: Baseline WM mixture weight at set size 3 (0..1).
    - softmax_beta: Inverse temperature for RL (scaled by 10 internally).
    - wm_decay: WM decay rate toward uniform (0..1).
    - conf_gate: Slope for confidence-based WM encoding gate (>=0).
    - mix_size_drop: Strength of set-size penalty on WM mixture (>=0).
    """
    alpha, wm_mix3, softmax_beta, wm_decay, conf_gate, mix_size_drop = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_weight_eff = wm_mix3 * np.exp(-max(0, nS - 3) * mix_size_drop)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Confidence-gated encoding: higher for rewarded, low-surprise outcomes
            p_encode = 1.0 / (1.0 + np.exp(-conf_gate * (r - 0.5 - abs(pe))))
            if np.random.rand() < p_encode:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates and set-size-depressed precision + WM with set-size-driven interference.

    Policy:
    - RL channel: softmax over Q-values with inverse temperature beta_eff,
      where beta_eff = beta * exp(-size_beta_drop * (nS - 3)).
    - WM channel: softmax over WM weights with high inverse temperature.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    RL dynamics:
    - Asymmetric learning rates for positive and negative prediction errors:
        alpha_pos for PE > 0, alpha_neg for PE < 0.

    WM dynamics:
    - Rewarded overwriting: if reward=1, store a one-hot vector at chosen action.
    - Interference toward uniform that increases with set size:
        gamma_eff = 1 - (1 - wm_interference)^(1 + (nS - 3)),
        w[s,:] <- (1 - gamma_eff) * w[s,:] + gamma_eff * uniform.

    Set-size effects:
    - RL precision is reduced at larger set sizes via beta_eff above.
    - WM interference increases with set size via gamma_eff above.

    Parameters (tuple):
    - alpha_pos: RL learning rate for positive PE (0..1).
    - alpha_neg: RL learning rate for negative PE (0..1).
    - softmax_beta: Baseline inverse temperature for RL (scaled by 10 internally).
    - wm_weight: Constant WM mixture weight (0..1).
    - wm_interference: Baseline interference toward uniform (0..1).
    - size_beta_drop: Strength of set-size penalty on RL precision (>=0).
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight, wm_interference, size_beta_drop = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        beta_eff = softmax_beta * np.exp(-max(0, nS - 3) * size_beta_drop)
        gamma_eff = 1.0 - (1.0 - wm_interference) ** (1 + max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            alpha_t = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += alpha_t * pe

            # Interference toward uniform
            w[s, :] = (1.0 - gamma_eff) * w[s, :] + gamma_eff * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty-adaptive precision + WM with recall failures that grow with set size.

    Policy:
    - RL channel: softmax with state-specific, uncertainty-adaptive inverse temperature:
        beta_state_eff = beta / (1 + u_s),
      where u_s is a running uncertainty estimate for state s (higher uncertainty -> lower precision).
    - WM channel: softmax over WM with high inverse temperature, but subject to recall failures:
        p_wm_eff = p_recall * p_wm + (1 - p_recall) * (1/nA).
    - Mixture: p_total = wm_weight_eff * p_wm_eff + (1 - wm_weight_eff) * p_rl.

    RL dynamics:
    - Learning rate alpha, with prediction error PE = r - Q_s[a].
    - Uncertainty u_s is updated by an exponential moving average of unsigned PE:
        u_s <- (1 - vol_sensitivity) * u_s + vol_sensitivity * |PE|.

    WM dynamics:
    - If reward=1, store a one-hot vector at action a (overwrite).
    - No decay; instead, retrieval (recall) can fail.

    Set-size effects:
    - WM recall probability decreases with set size:
        p_recall = recall_base * exp(-recall_size_drop * (nS - 3)).
    - WM mixture weight can also diminish with set size:
        wm_weight_eff = wm_weight_base * exp(-recall_size_drop * (nS - 3)).

    Parameters (tuple):
    - alpha: RL learning rate (0..1).
    - softmax_beta: Baseline inverse temperature for RL (scaled by 10 internally).
    - wm_weight_base: Baseline WM mixture weight at set size 3 (0..1).
    - recall_base: Baseline WM recall probability at set size 3 (0..1).
    - recall_size_drop: Set-size penalty on both WM recall and WM mixture (>=0).
    - vol_sensitivity: Step size for uncertainty (unsigned-PE) update (0..1).
    """
    alpha, softmax_beta, wm_weight_base, recall_base, recall_size_drop, vol_sensitivity = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        u = np.zeros(nS)  # state-wise uncertainty from unsigned PE

        size_factor = np.exp(-max(0, nS - 3) * recall_size_drop)
        wm_weight_eff = wm_weight_base * size_factor
        p_recall = recall_base * size_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            beta_state_eff = softmax_beta / (1.0 + max(u[s], 0.0))
            denom_rl = np.sum(np.exp(beta_state_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            W_s = w[s, :]
            denom_wm_core = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_core = 1.0 / max(denom_wm_core, 1e-12)
            p_wm = p_recall * p_wm_core + (1.0 / nA) * (1.0 - p_recall)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            pe = r - Q_s[a]
            q[s, a] += alpha * pe

            # Update uncertainty from unsigned PE
            u[s] = (1.0 - vol_sensitivity) * u[s] + vol_sensitivity * abs(pe)

            # WM overwrite on reward
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p

Notes:
- All three models keep the specified mixture line intact; any additional mechanisms (e.g., recall failures) are folded into the computed p_wm before the mixture.
- Numerical guards (1e-12) prevent log(0).
- Each parameter is used and tied to interpretable cognitive mechanisms, with explicit set-size dependencies.