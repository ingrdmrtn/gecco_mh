def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Decaying WM bindings with load-dependent interference and sigmoid gating.

    Model idea:
    - RL: standard Q-learning per state-action with learning rate lr, softmax choice with beta (scaled internally).
    - WM: stores a sharp binding to the last rewarded action per state as a one-hot vector that decays over time.
      A load-dependent interference blends the WM distribution toward uniform when set size increases.
    - Mixture: WM and RL are mixed using a sigmoid gate over set size (lower WM weight at larger set sizes).

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Baseline WM weight before gating by set size.
    - softmax_beta: scalar >= 0. RL inverse temperature, scaled internally by 10.
    - wm_decay: scalar in [0,1]. Per-trial decay of WM bindings toward uniform.
    - interference_k: scalar >= 0. Strength of load-induced WM interference toward uniform.
    - load_slope: scalar (can be +/-). Controls how steeply WM weight drops with set size via sigmoid.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, interference_k, load_slope = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM interference toward uniform
        # 0 at nS=3, increases with set size
        load_factor = max(0.0, (nS - 3) / max(1.0, (6 - 3)))
        interference = interference_k * load_factor

        # Sigmoid gate for WM weight based on set size
        # Higher set size -> smaller weight (if load_slope > 0)
        ss_center = 4.5
        gate = 1.0 / (1.0 + np.exp(load_slope * (nS - ss_center)))
        wm_weight_eff = wm_weight * gate

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy:
            # 1) Decay WM trace toward uniform
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) If rewarded, bind strongly to the chosen action (one-hot impulse)
            if r >= 1.0 - 1e-12:
                impulse = np.zeros(nA)
                impulse[a] = 1.0
                # Move a substantial portion toward the impulse while respecting interference
                w[s, :] = (1 - interference) * (0.7 * w[s, :] + 0.3 * impulse) + interference * w_0[s, :]

            # 3) Normalize just in case of numerical drift
            w[s, :] = np.clip(w[s, :], 1e-9, 1.0)
            w[s, :] /= np.sum(w[s, :])

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updating already applied above via decay + impulse

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Recency-based WM with uncertainty-gated arbitration.

    Model idea:
    - RL: standard Q-learning with lr and softmax(beta).
    - WM: maintains a recency-weighted action distribution per state (exponential trace).
      After reward, it shifts probability toward the chosen action; after no reward, it shifts away.
      Controlled by a recency parameter lambda_wm.
    - Arbitration: WM weight is modulated by RL uncertainty for the current state.
      When RL is uncertain (high entropy), the model relies more on WM; when certain, less.
      A logistic transform with bias and slope maps entropy to a weight.
      This is also scaled down under higher set sizes by a simple 1/nS factor.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Baseline WM mixture weight.
    - softmax_beta: scalar >= 0. RL inverse temperature, scaled internally by 10.
    - gate_bias: scalar. Bias term for the entropy-to-weight logistic mapping.
    - gate_slope: scalar. Slope for the entropy-to-weight logistic mapping.
    - lambda_wm: scalar in [0,1]. Recency parameter: how fast WM traces track recent outcomes.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate_bias, gate_slope, lambda_wm = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: exponential recency trace updated by outcome
            # Decay toward uniform a bit each trial for stability
            w[s, :] = (1 - 0.1 * (1 - lambda_wm)) * w[s, :] + 0.1 * (1 - lambda_wm) * w_0[s, :]

            if r >= 1.0 - 1e-12:
                # Reward: move probability toward chosen action
                target = np.copy(w[s, :])
                target[a] = 1.0
                target = np.clip(target, 1e-9, None)
                target /= np.sum(target)
                w[s, :] = (1 - lambda_wm) * w[s, :] + lambda_wm * target
            else:
                # No reward: move probability away from chosen action, redistribute to others
                redistribute = np.copy(w[s, :])
                # shift mass from chosen action to others uniformly
                take = min(w[s, a], lambda_wm * 0.5)
                redistribute[a] -= take
                add = take / (nA - 1)
                for ai in range(nA):
                    if ai != a:
                        redistribute[ai] += add
                redistribute = np.clip(redistribute, 1e-9, None)
                redistribute /= np.sum(redistribute)
                w[s, :] = redistribute

            w[s, :] = np.clip(w[s, :], 1e-9, None)
            w[s, :] /= np.sum(w[s, :])
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: entropy of RL state value distribution
            # Convert Q to softmax probabilities at a mild temperature to compute entropy
            temp = max(1e-6, 1.0)
            probs = np.exp(Q_s / temp)
            probs /= np.sum(probs)
            entropy = -np.sum(probs * np.log(np.clip(probs, 1e-12, 1.0)))
            # Normalize entropy by log(nA)
            entropy /= np.log(nA)

            gate_input = gate_bias + gate_slope * entropy
            gate_w_unc = 1.0 / (1.0 + np.exp(-gate_input))  # in (0,1)

            # Scale down under load (simple capacity dilution)
            gate_w = wm_weight * gate_w_unc * (3.0 / max(3.0, float(nS)))

            p_total = gate_w * p_wm + (1 - gate_w) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # WM update was applied above

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Dirichlet WM posterior with load-tuned concentration.

    Model idea:
    - RL: standard Q-learning with lr and softmax(beta).
    - WM: per state, a Dirichlet distribution over the 3 actions reflecting the hypothesized correct mapping.
      On reward, increase the concentration for the chosen action; on non-reward, add mass to the other actions.
      The effective concentration is dampened by set size to reflect WM precision loss at higher load.
      The WM policy is the mean of the Dirichlet (i.e., normalized counts).
    - Mixture: fixed wm_weight but the WM posterior itself becomes less peaked at higher set sizes
      via a load temperature that blends counts toward uniform.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Mixture weight for WM policy.
    - softmax_beta: scalar >= 0. RL inverse temperature, scaled internally by 10.
    - alpha0: scalar > 0. Base Dirichlet prior concentration for WM per action.
    - reward_gain: scalar > 0. Amount of concentration added to chosen action on reward.
    - nonreward_spread: scalar >= 0. Total concentration spread across unchosen actions on non-reward.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, alpha0, reward_gain, nonreward_spread = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Dirichlet counts for WM posterior
        counts = np.full((nS, nA), alpha0, dtype=float)

        # Load-tuned blending toward uniform (higher nS -> more blending)
        load_blend = (nS - 3) / max(1.0, (6 - 3))
        load_blend = np.clip(load_blend, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM posterior mean from Dirichlet counts, with load blending toward uniform
            wm_mean = counts[s, :] / np.sum(counts[s, :])
            wm_mean = (1 - load_blend) * wm_mean + load_blend * w_0[s, :]

            # Keep w as the current WM policy distribution for softmax-based likelihood
            w[s, :] = np.clip(wm_mean, 1e-9, None)
            w[s, :] /= np.sum(w[s, :])

            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM Dirichlet updates
            if r >= 1.0 - 1e-12:
                counts[s, a] += reward_gain
            else:
                # spread mass to the two non-chosen actions
                spread_each = nonreward_spread / (nA - 1) if nA > 1 else 0.0
                for ai in range(nA):
                    if ai != a:
                        counts[s, ai] += spread_each

        blocks_log_p += log_p

    return -blocks_log_p