Below are three standalone cognitive models that follow your template structure, return negative log-likelihood, and incorporate distinct RLâ€“WM mechanisms with set-size effects. Each model uses at most six parameters and uses all parameters meaningfully.

Note: No imports are included; assume numpy as np is already available.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Confidence-gated WM policy (threshold depends on set size)

    Idea:
    - RL: standard delta rule.
    - WM: maintains action probabilities per state.
      The WM policy is only used when WM confidence (max probability in the WM row)
      exceeds a threshold. Otherwise WM outputs a uniform (uninformative) policy.
    - Arbitration: fixed WM mixture weight.
    - Set size effect: WM confidence threshold increases with set size, making WM harder to use in larger sets.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: mixture weight for WM policy in [0,1].
    - softmax_beta: RL softmax inverse temperature (>0, internally scaled by 10).
    - wm_thresh0: base WM confidence threshold in [0,1].
    - wm_thresh_slope: slope controlling how threshold increases with set size (positive -> stricter at larger sets).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, wm_thresh0, wm_thresh_slope = parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size dependent WM confidence threshold
        # Normalize size effect by using deviation from 3 (small set)
        wm_threshold = np.clip(wm_thresh0 + wm_thresh_slope * (nS - 3) / 3.0, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy is confidence-gated: if confidence (max prob) < threshold, output uniform
            wm_conf = np.max(W_s)
            if wm_conf >= wm_threshold:
                p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            else:
                p_wm = 1.0 / nA  # uniform/unconfident WM output

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Simple associative WM update: push row toward chosen action if rewarded,
            # otherwise push away from chosen action toward uniform.
            if r > 0.5:
                # Increase chosen action's probability and renormalize
                inc = min(1.0 - W_s[a], 0.5)  # bounded increment for stability
                w[s, a] += inc
                # proportional decrease from others
                if nA > 1:
                    w[s, :] -= inc / (nA - 1)
                    w[s, a] += inc / (nA - 1)  # undo the decrease for chosen
            else:
                # Decrease chosen action's probability and redistribute to others
                dec = min(W_s[a], 0.5)
                w[s, a] -= dec
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += dec / (nA - 1)

            # Renormalize to a proper distribution and clamp
            row = w[s, :]
            row[row < 1e-8] = 1e-8
            w[s, :] = row / np.sum(row)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WSLS-augmented WM with set-size-dependent forgetting

    Idea:
    - RL: standard delta rule.
    - WM: per-state action distribution that is nudged by outcomes (reward -> toward chosen; no reward -> away).
    - WSLS augmentation: if last outcome in that state was rewarded, boost last rewarded action's logit;
      if unrewarded, penalize repeating that action (lose-shift).
    - Forgetting: WM decays toward uniform with rate increasing with set size.
    - Arbitration: fixed WM mixture weight.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1].
    - wm_weight: mixture weight for WM policy in [0,1].
    - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
    - wsls_strength: strength added to WM logits for win-stay / lose-shift (>0).
    - wm_decay: base WM decay rate toward uniform in [0,1].
    - size_influence: scales how much decay increases with set size (>=0). Effective decay = wm_decay * (1 + size_influence*(nS-3)/3).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, wsls_strength, wm_decay, size_influence = parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action and last reward per state for WSLS
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = -np.ones(nS)  # -1 means no history yet

        # Set-size dependent decay
        decay_eff = wm_decay * (1.0 + size_influence * (nS - 3) / 3.0)
        decay_eff = np.clip(decay_eff, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WSLS-augmented WM logits: base on W_s, add/subtract strength to last action depending on last outcome
            wm_logits = softmax_beta_wm * W_s.copy()
            if last_action[s] >= 0:
                if last_reward[s] > 0.5:
                    wm_logits[last_action[s]] += wsls_strength
                else:
                    wm_logits[last_action[s]] -= wsls_strength

            # Transform to probability of the chosen action via softmax denominator trick
            chosen_logit = wm_logits[a]
            p_wm = 1.0 / np.sum(np.exp(wm_logits - chosen_logit))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward uniform (set-size dependent)
            w[s, :] = (1.0 - decay_eff) * w[s, :] + decay_eff * w_0[s, :]

            # Outcome-based WM nudge: reward -> toward chosen; no reward -> away from chosen
            if r > 0.5:
                gain = 0.5  # bounded step for stability
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain
            else:
                # Move mass away from chosen to other actions
                shift = min(0.5, w[s, a])
                w[s, a] -= shift
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += shift / (nA - 1)

            # Renormalize
            row = w[s, :]
            row[row < 1e-8] = 1e-8
            w[s, :] = row / np.sum(row)

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Resource-rational RL learning rate + WM with lateral inhibition (set-size scaled)

    Idea:
    - RL: standard delta rule but with an effective learning rate that decreases with set size
      (resource-rational scaling).
    - WM: per-state distribution updated by outcome; incorporates lateral inhibition after reward:
      when rewarded, probability mass is transferred from non-chosen actions to the chosen one.
    - WM leak: global leak toward uniform each trial.
    - Arbitration: fixed WM mixture weight.

    Set size effects:
    - RL effective lr = lr / (1 + rr_scale * (nS - 1)) -> lower learning when more items.
    - WM lateral inhibition strength is scaled by nS (more competition at larger set sizes).

    Parameters (model_parameters):
    - lr: base RL learning rate in [0,1].
    - wm_weight: mixture weight for WM policy in [0,1].
    - softmax_beta: RL inverse temperature (>0, internally scaled by 10).
    - rr_scale: resource-rational scaling factor for RL (>=0).
    - wm_inhib: base lateral inhibition strength in [0,1].
    - wm_leak: leak toward uniform in [0,1].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    parameters = model_parameters
    lr, wm_weight, softmax_beta, rr_scale, wm_inhib, wm_leak = parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size-dependent RL learning rate
        lr_eff = lr / (1.0 + rr_scale * max(nS - 1, 0))
        lr_eff = np.clip(lr_eff, 0.0, 1.0)

        # Set-size-scaled lateral inhibition
        inhib_eff = np.clip(wm_inhib * (nS / 3.0), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with resource-rational lr
            delta = r - Q_s[a]
            q[s][a] += lr_eff * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Leak toward uniform
            w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]

            if r > 0.5:
                # Lateral inhibition: move mass from non-chosen to chosen
                give = inhib_eff * np.sum(w[s, :])  # scale by current mass (which is 1 after norm)
                give = min(give, 0.5)  # keep bounded
                # Take proportionally from non-chosen actions
                non_idx = [aa for aa in range(nA) if aa != a]
                take_total = 0.0
                for aa in non_idx:
                    take = min(w[s, aa], give / len(non_idx))
                    w[s, aa] -= take
                    take_total += take
                w[s, a] += take_total
            else:
                # If not rewarded, soften the chosen action slightly (anti-perseveration)
                soften = min(0.2, w[s, a])
                w[s, a] -= soften
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += soften / (nA - 1)

            # Renormalize/clamp
            row = w[s, :]
            row[row < 1e-8] = 1e-8
            w[s, :] = row / np.sum(row)

        blocks_log_p += log_p

    return -blocks_log_p

Notes on set-size impacts:
- Model 1: WM threshold increases with set size, reducing WM usability in larger sets.
- Model 2: WM forgetting increases with set size, making WSLS influence and WM traces degrade more in larger sets.
- Model 3: RL learning rate decreases with set size (resource-rational), and WM lateral inhibition increases with set size, enhancing competition under higher load.