def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated WM with load-dependent maintenance.

    Mechanism
    - RL: standard Rescorla–Wagner update (template).
    - WM policy: softmax over a working-memory map w[s,:].
    - Arbitration: uses the template's wm_weight mixture, but we adapt wm_weight
      online by penalizing large set sizes and scaling by current WM strength.
    - WM update: 
        • Maintenance decay toward uniform that increases with set size.
        • PE-gated encoding after reward: larger unsigned PE => stronger update.
        • Weak penalization of chosen action on errors.

    Parameters
    ----------
    model_parameters : list/tuple of 5 floats
        lr              : RL learning rate in (0,1].
        wm_weight       : Baseline WM arbitration weight in [0,1].
        softmax_beta    : RL inverse temperature; scaled by *10 internally.
        wm_decay_rate   : Base WM decay rate toward uniform (per choice).
        load_penalty    : How much larger sets reduce WM weight/maintenance (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay_rate, load_penalty = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    eps = 1e-12

    def clip01(x):
        return np.clip(x, 0.0, 1.0)

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    def inv_logit(z):
        return 1 / (1 + np.exp(-z))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute effective load factor for this block
        load_factor = max(0.0, nS - 3.0)
        # Penalize WM arbitration by load (in logit space)
        base_w = inv_logit(logit(clip01(wm_weight)) - load_penalty * load_factor)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability (probability assigned to chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy using a sharp softmax; add small epsilon for numerical stability
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Adapt wm_weight by current WM strength (0=no info, 1=perfect one-hot)
            mem_strength = np.max(W_s) - 1.0 / nA  # in [0, 1-1/nA]
            mem_strength = clip01(mem_strength / (1.0 - 1.0 / nA + eps))
            wm_weight = clip01(base_w * mem_strength)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Maintenance decay toward uniform, stronger with load
            # decay in [0,1): increase with set size via load_penalty
            decay = 1.0 - np.exp(-wm_decay_rate * (1.0 + load_penalty * load_factor))
            decay = clip01(decay)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # 2) Outcome-driven update
            if r > 0:
                # PE-gated encoding: larger unsigned PE => stronger push to one-hot
                eta = clip01(np.abs(delta))  # in [0,1]
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # On error, gently suppress the chosen action within WM
                suppress = 0.2 * (1.0 + 0.25 * load_factor)  # mild load-dependent penalty
                w[s, a] = clip01(w[s, a] * (1.0 - suppress))
                # Renormalize row to sum to 1
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL (valence-asymmetric) + consistency-gated WM with load-driven interference.

    Mechanism
    - RL: separate learning rates for positive vs negative prediction errors.
    - WM policy: softmax over WM map with adjustable sharpness.
    - Arbitration: template mixture uses wm_weight; we adapt wm_weight online by:
        • Increasing weight when the state's WM is behaviorally consistent.
        • Decreasing weight as set size increases (interference).
    - WM update:
        • On reward: push toward the chosen action (encoding).
        • On no reward: downweight chosen action (avoidance).
        • Global decay toward uniform to model interference in larger sets.
    - Consistency trace: per-state variable that grows when reward follows the WM-
      preferred action and decays otherwise; modulates arbitration.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr_pos           : RL learning rate for positive PEs (>0).
        wm_weight        : Baseline WM arbitration weight in [0,1].
        softmax_beta     : RL inverse temperature; scaled by *10 internally.
        wm_consistency   : Gain on consistency effect in arbitration (>0).
        interference_rate: Load-interference strength in arbitration and decay (>0).
        temp_wm          : Scales WM policy sharpness (>=0; 0 = uniform WM policy).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr_pos, wm_weight, softmax_beta, wm_consistency, interference_rate, temp_wm = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    eps = 1e-12

    def clip01(x):
        return np.clip(x, 0.0, 1.0)

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    def inv_logit(z):
        return 1 / (1 + np.exp(-z))

    # Negative learning rate for negative PEs derived to keep total params <=6:
    # We transform lr_pos into two rates via a sigmoid-shaped split anchored at 0.5:
    # lr_neg = lr_pos / (1 + lr_pos) ensures 0<lr_neg<0.5 while lr_pos can be larger.
    lr_neg = lr_pos / (1.0 + lr_pos)

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Per-state consistency trace (initialized at neutral 0.5)
        cons = 0.5 * np.ones(nS)

        load_factor = max(0.0, nS - 3.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy with adjustable sharpness via temp_wm
            wm_scale = softmax_beta_wm * max(0.0, temp_wm)
            p_wm = 1.0 / np.sum(np.exp(wm_scale * (W_s - W_s[a])))

            # Consistency-based arbitration adjustment
            # Favor WM when the chosen action matches WM preference and rewards have been consistent.
            wm_pref = int(np.argmax(W_s))
            # Effective arbitration weight via logit modulation
            w_base = inv_logit(logit(clip01(wm_weight)) +
                               wm_consistency * (cons[s] - 0.5) -
                               interference_rate * load_factor)
            wm_weight_eff = clip01(w_base)
            wm_weight = wm_weight_eff  # feed into template mixture

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with valence asymmetry
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s][a] += alpha * pe

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Global interference/decay toward uniform (stronger with load)
            decay = 1.0 - np.exp(-interference_rate * (1.0 + load_factor))
            decay = clip01(decay)
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            if r > 0:
                # Reward: encode chosen action
                eta = 0.7  # fixed strong push; effective sharpness controlled by temp_wm in policy
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
                # Increase consistency if reward aligns with WM preference
                cons[s] = clip01(0.8 * cons[s] + 0.2 * (1.0 if a == wm_pref else 0.0))
            else:
                # No reward: downweight chosen action, distribute mass to others
                leak = 0.3
                redistribute = leak * w[s, a] / (nA - 1)
                for aa in range(nA):
                    if aa == a:
                        w[s, aa] *= (1.0 - leak)
                    else:
                        w[s, aa] += redistribute
                # Decrease consistency on errors
                cons[s] = clip01(0.9 * cons[s])

            # Normalize WM row to keep numerical stability
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM as a fast associative cache with novelty-biased policy and load-tuned arbitration.

    Mechanism
    - RL: standard Rescorla–Wagner update (template).
    - WM policy: softmax over w[s,:], but with novelty bias that keeps policy
      diffuse when memory is weak (reduces overcommitment under uncertainty).
    - Arbitration: template mixture uses wm_weight; we modulate it by set size and
      by the state's WM strength (variance from uniform).
    - WM update:
        • Store on reward with gain.
        • Mild redistribution on errors away from chosen action.
        • Load-dependent bleed toward uniform.

    Parameters
    ----------
    model_parameters : list/tuple of 6 floats
        lr             : RL learning rate in (0,1].
        wm_weight      : Baseline WM arbitration weight in [0,1].
        softmax_beta   : RL inverse temperature; scaled by *10 internally.
        store_gain     : Strength of WM update toward the rewarded action (0..1).
        novelty_bias   : Degree to flatten WM policy when memory is weak (>=0).
        load_slope     : How strongly larger set sizes reduce WM influence and increase bleed (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, store_gain, novelty_bias, load_slope = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50

    eps = 1e-12

    def clip01(x):
        return np.clip(x, 0.0, 1.0)

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    def inv_logit(z):
        return 1 / (1 + np.exp(-z))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        load_factor = max(0.0, nS - 3.0)

        # Baseline arbitration adjusted by load in logit space
        base_w = inv_logit(logit(clip01(wm_weight)) - load_slope * load_factor)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # Novelty-biased WM policy: flatten when memory is weak
            mem_strength = np.max(W_s) - 1.0 / nA
            mem_strength = clip01(mem_strength / (1.0 - 1.0 / nA + eps))
            # Interpolate WM logits toward uniform based on novelty_bias*(1-mem_strength)
            blend = clip01(novelty_bias * (1.0 - mem_strength))
            W_mix = (1.0 - blend) * W_s + blend * (1.0 / nA) * np.ones(nA)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_mix - W_mix[a])))

            # Arbitration: scale base_w by current WM strength
            wm_weight = clip01(base_w * mem_strength)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Load-dependent bleed toward uniform
            bleed = 1.0 - np.exp(-load_slope * (1.0 + 0.25 * load_factor))
            bleed = clip01(bleed)
            w[s, :] = (1.0 - bleed) * w[s, :] + bleed * w_0[s, :]

            if r > 0:
                # Store rewarded action with gain
                eta = clip01(store_gain)
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - eta) * w[s, :] + eta * one_hot
            else:
                # On error, redistribute mass away from chosen action
                penal = 0.2 + 0.1 * clip01(load_factor)  # slightly stronger under load
                taken = penal * w[s, a]
                w[s, a] *= (1.0 - penal)
                w[s, :] += taken / (nA - 1) * (1 - np.eye(1, nA, a).flatten())

            # Normalize row
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum
            else:
                w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p