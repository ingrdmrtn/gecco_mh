def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited working memory with uncertainty-weighted arbitration and age-modulated capacity.

    The model blends:
      - Incremental Q-learning (softmax policy).
      - A capacity-limited working-memory (WM) store that encodes rewarded state->action associations.
        WM has a limited number of "slots" (wm_chunk). Older adults have an effective reduction in
        capacity via age_boost. When the number of encoded states exceeds capacity, the least recent
        is forgotten (recency-based eviction).
      - Arbitration weight favors WM when RL is uncertain (high entropy). WM weight also scales with
        how much of the set can be covered by capacity (cap_eff / set_size).

    Parameters
    ----------
    states : np.ndarray (int)
        State index at each trial (0..nS-1 within a block).
    actions : np.ndarray (int)
        Chosen action at each trial (0..2). Out-of-range actions are treated as lapses (uniform).
    rewards : np.ndarray (float)
        Feedback (typically 0/1). Negative rewards are treated as invalid for learning.
    blocks : np.ndarray (int)
        Block identifier per trial.
    set_sizes : np.ndarray (int)
        Set size per trial (3 or 6) for the current block.
    age : np.ndarray (float)
        Participant age array (use age[0]); age>=45 => older group.
    model_parameters : sequence of float
        [lr, beta, wm_chunk, age_boost, slip]
        - lr: Q-learning rate (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - wm_chunk: nominal WM capacity in number of states (1..6).
        - age_boost: multiplicative reduction of WM capacity for older adults via exp(-age_boost) (>=0).
        - slip: lapse/slip probability; with prob 'slip' choice is uniform (1/3).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, wm_chunk, age_boost, slip = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM store: action per state (-1 means empty), plus recency order
        wm_action = -np.ones(nS, dtype=int)
        # maintain a simple recency list of states currently in WM
        wm_recency = []  # list of state indices, most recent at end

        # Effective capacity reduced for older adults
        cap_eff = wm_chunk * np.exp(-age_boost * is_older)
        cap_eff = int(np.clip(np.round(cap_eff), 1, max(1, nS)))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL softmax policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy: if we have an entry, place high weight on stored action; else uniform
            p_wm = np.ones(nA) / nA
            if 0 <= wm_action[s] < nA:
                p_wm = np.zeros(nA)
                p_wm[wm_action[s]] = 1.0

            # Uncertainty of RL: normalized entropy in [0,1]
            eps = 1e-12
            entropy = -np.sum(np.clip(p_rl, eps, 1.0) * np.log(np.clip(p_rl, eps, 1.0))) / np.log(nA)
            # Capacity coverage: how much of set size can WM cover
            cover = np.clip(cap_eff / max(1, ss), 0.0, 1.0)

            # Arbitration weight: more WM when RL is uncertain and capacity can cover set size
            w_wm = np.clip(entropy * cover, 0.0, 1.0)

            p_mix = w_wm * p_wm + (1.0 - w_wm) * p_rl
            p_final = (1.0 - slip) * p_mix + slip * (np.ones(nA) / nA)

            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                # out-of-range action -> treat as unmodeled lapse; uniform probability
                total_logp += np.log(1.0 / nA)
                continue

            # Learning updates only for valid feedback
            if r >= 0.0:
                # RL update
                Q[s, a] += lr * (r - Q[s, a])

                # WM encoding upon reward, with capacity enforcement
                if r > 0.0:
                    # insert/update recency
                    if s in wm_recency:
                        wm_recency.remove(s)
                    wm_recency.append(s)
                    wm_action[s] = a
                    # evict least recent if over capacity
                    while len(wm_recency) > cap_eff:
                        evict_state = wm_recency.pop(0)
                        wm_action[evict_state] = -1
                else:
                    # no positive feedback: do not encode; weak forgetting via recency demotion
                    if s in wm_recency:
                        wm_recency.remove(s)
                        wm_recency.insert(0, s)  # make it less recent

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Valence-asymmetric RL with age- and load-driven decision noise and state-wise stickiness.

    Mechanisms:
      - Q-learning with a single base learning rate split into positive/negative components
        via an asymmetry parameter (alpha_pos = alpha*(1+asym), alpha_neg = alpha*(1-asym)).
      - Inverse temperature (beta) is reduced by an additive noise term that increases with set size
        and with being older (age>=45), modeling broader decision noise under load and aging.
      - State-conditional stickiness: a bias to repeat the last action chosen in the same state.
        Stickiness is amplified for older adults via stick_age.

    Parameters
    ----------
    states : np.ndarray (int)
        State index per trial.
    actions : np.ndarray (int)
        Chosen action per trial (0..2). Out-of-range treated as uniform.
    rewards : np.ndarray (float)
        Outcome per trial (0/1). Negative values treated as invalid for learning.
    blocks : np.ndarray (int)
        Block index per trial.
    set_sizes : np.ndarray (int)
        Set size per trial (3 or 6).
    age : np.ndarray (float)
        Participant age array (use age[0]); age>=45 => older group.
    model_parameters : sequence of float
        [alpha, beta, asym, noise0, stick_age]
        - alpha: base learning rate (0..1).
        - beta: base inverse temperature (>0).
        - asym: valence asymmetry in [-1,1]; splits alpha into pos/neg.
        - noise0: baseline decision noise (>=0) that reduces effective beta.
        - stick_age: stickiness gain applied if older (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, asym, noise0, stick_age = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    # derive valence-specific learning rates
    alpha_pos = np.clip(alpha * (1.0 + asym), 0.0, 1.0)
    alpha_neg = np.clip(alpha * (1.0 - asym), 0.0, 1.0)

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        last_action = -np.ones(nS, dtype=int)  # per-state last chosen action

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Effective inverse temperature reduced by noise (load + aging)
            # noise term: baseline + load (0 for 3, ~1 for 6) + age contribution
            load_term = (max(ss - 3, 0)) / 3.0
            noise = noise0 + load_term + 0.5 * is_older
            beta_eff = beta * np.exp(-np.clip(noise, 0.0, 10.0))

            # Stickiness bias (only if a last action exists)
            stick = stick_age * is_older
            pref = Q[s, :].copy()
            if 0 <= last_action[s] < nA:
                pref[last_action[s]] += stick

            # Softmax choice
            pref_center = pref - np.max(pref)
            p = np.exp(beta_eff * pref_center)
            p = p / np.sum(p)

            if 0 <= a < nA:
                pa = float(np.clip(p[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Update last action
            last_action[s] = a

            # Learning update (valid rewards only)
            if r >= 0.0:
                delta = r - Q[s, a]
                lr = alpha_pos if delta >= 0.0 else alpha_neg
                Q[s, a] += lr * delta

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dirichlet-Bayesian associative learning with age/load-shaped priors, decay, and softmax sensitivity.

    Mechanisms:
      - For each state, maintain Dirichlet concentration counts over the 3 actions.
      - Prior strength (kappa) is lower under higher load and for older adults, reflecting weaker
        initial confidence/precision in associations. kappa = prior0 * exp(-age_load_tradeoff * (is_older + load)).
      - Action selection uses the posterior predictive (normalized counts) passed through a softmax
        with inverse temperature beta to capture stochasticity.
      - Feedback updating:
          * If reward==1: increment chosen action's count.
          * If reward==0: distribute increment to the two non-chosen actions (evidence against chosen).
      - Counts decay toward the prior each trial at rate 'decay' to capture forgetting/interference.
      - Includes a lapse rate 'lapse' that mixes a uniform policy.

    Parameters
    ----------
    states : np.ndarray (int)
        State index per trial.
    actions : np.ndarray (int)
        Chosen action per trial (0..2). Out-of-range treated as uniform.
    rewards : np.ndarray (float)
        Binary feedback (0/1). Negative rewards are ignored for learning.
    blocks : np.ndarray (int)
        Block identifier per trial.
    set_sizes : np.ndarray (int)
        Set size per trial (3 or 6).
    age : np.ndarray (float)
        Participant age array (use age[0]); age>=45 => older group.
    model_parameters : sequence of float
        [prior0, beta, age_load_tradeoff, decay, lapse]
        - prior0: base prior strength (Dirichlet total concentration) per state (>0).
        - beta: inverse temperature applied to log predictive probs (>0).
        - age_load_tradeoff: scales how much age and load reduce prior precision (>=0).
        - decay: per-trial decay toward prior in [0,1).
        - lapse: lapse probability; with prob 'lapse' choose uniformly (1/3).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    prior0, beta, age_load_tradeoff, decay, lapse = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Compute block-specific prior precision from age and load
        load = (max(nS - 3, 0)) / 3.0
        kappa = prior0 * np.exp(-age_load_tradeoff * (is_older + load))
        kappa = float(np.clip(kappa, 1e-6, 1e6))
        prior_vec = (kappa / nA) * np.ones(nA)

        # Initialize Dirichlet counts per state
        counts = np.tile(prior_vec, (nS, 1))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Posterior predictive
            total_c = np.sum(counts[s, :])
            p_pred = counts[s, :] / max(total_c, 1e-12)

            # Softmax over log predictive to allow sharper choices with beta
            logp = np.log(np.clip(p_pred, 1e-12, 1.0))
            logits = beta * (logp - np.max(logp))
            p = np.exp(logits)
            p = p / np.sum(p)

            p_final = (1.0 - lapse) * p + lapse * (np.ones(nA) / nA)

            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Learning with decay toward prior
            # First decay counts toward prior
            counts[s, :] = (1.0 - decay) * counts[s, :] + decay * prior_vec

            # Then incorporate feedback if valid
            if r >= 0.0:
                if r > 0.0:
                    counts[s, a] += 1.0
                else:
                    # distribute evidence against chosen action to non-chosen options
                    for a2 in range(nA):
                        if a2 != a:
                            counts[s, a2] += 0.5  # total increment 1.0 split across two

    return -float(total_logp)