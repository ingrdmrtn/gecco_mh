Here are three alternative hybrid RL+WM models tailored to the RLWM task, each as a standalone function returning the negative log-likelihood of the observed choices. Each model uses a different arbitration/update mechanism and parameterization, with explicit working-memory policies and updates, and with set-size effects.

Note: These functions assume numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Capacity-limited WM with interference and decay, mixed with RL.

    Idea:
    - RL: standard delta rule.
    - WM: one-hot storage on rewarded trials; decays toward uniform; subject to cross-state interference.
    - Arbitration: WM influence scales with capacity factor min(K_slots / set_size, 1) times base wm_weight.
      This captures reduced WM efficacy under higher load (nS=6).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: base mixture weight for WM (0..1), modulated by capacity factor per set size.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - K_slots: WM capacity (in number of associations; >=1).
    - wm_decay: WM decay rate toward uniform each trial (0..1).
    - interference: cross-state interference when storing in WM (0..1).
      When an item is stored for state s, a fraction "interference" of that signal spills
      to the same chosen action in other states.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, K_slots, wm_decay, interference = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic WM readout
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Precompute WM capacity factor for this block
        cap_factor = min(float(K_slots) / float(max(1, nS)), 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise arbitration weight: capacity-limited WM influence
            wm_weight = np.clip(wm_weight * cap_factor, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated one-hot storage with interference and decay
            if r > 0.5:
                # Store correct mapping strongly for current state
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])

                # Interference: spill a fraction of this storage to the same action across other states
                if nS > 1 and interference > 0.0:
                    leak = interference
                    for s_other in range(nS):
                        if s_other == s:
                            continue
                        # Add leaked signal on the same action index
                        w[s_other, a] = (1 - leak) * w[s_other, a] + leak * (1.0 / nA)
                        # Renormalize each affected state's WM distribution
                        w[s_other, :] /= np.sum(w[s_other, :])

            # Global WM decay toward uniform (applied every trial)
            lam = np.clip(wm_decay, 0.0, 1.0)
            w = (1.0 - lam) * w + lam * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Error-driven WM gating plus entropy-based arbitration with asymmetric RL learning.

    Idea:
    - RL: delta-rule with separate learning rates for positive and negative outcomes.
    - WM: stores one-hot on surprise-driven (high |PE|) rewarded events; otherwise slight rehearsal.
    - Arbitration: WM weight is boosted when surprise (|PE|) is high and when RL policy is uncertain
      (high entropy), and is reduced with larger set size. This makes WM dominate early/surprising
      trials and in small set sizes.

    Parameters (tuple):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_weight: base WM mixture weight (0..1), modulated per trial.
    - pe_gate_gain: sensitivity of WM gating to absolute PE (>=0).
    - entropy_gate: scales the influence of RL entropy on boosting WM (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, pe_gate_gain, entropy_gate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last absolute PE per state to drive surprise gating
        last_abs_pe = np.ones(nS) * 0.5  # moderate surprise initially

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute RL policy entropy (normalized to [0,1])
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            p_rl_vec = exp_logits / np.sum(exp_logits)
            H_rl = -np.sum(p_rl_vec * np.log(np.clip(p_rl_vec, eps, 1.0))) / np.log(nA)  # in [0,1]

            # Surprise gate based on last absolute PE at this state
            # Threshold increases with set size to reflect harder gating in high load.
            thresh = (max(3, nS) - 3) / 3.0  # 0 for nS=3, ~1 for nS=6
            gate_surprise = 1.0 / (1.0 + np.exp(-pe_gate_gain * (last_abs_pe[s] - thresh)))

            # Entropy-based WM boost when RL is uncertain; set-size penalty
            size_factor = 3.0 / float(max(3, nS))
            wm_weight = np.clip(wm_weight * size_factor * (1.0 + entropy_gate * H_rl), 0.0, 1.0)
            wm_weight = np.clip(wm_weight * gate_surprise, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - q[s, a]
            eta = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += eta * pe

            # Update surprise trace
            last_abs_pe[s] = abs(pe)

            # WM update: reward-gated storage with light rehearsal when not gated
            if r > 0.5 and gate_surprise > 0.5:
                # Strong storage
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            else:
                # Mild rehearsal toward current choice for that state
                alpha_wm = 0.1 * size_factor
                w[s, :] = (1.0 - alpha_wm) * w[s, :] + alpha_wm * w_0[s, :]
                w[s, a] += alpha_wm * (1.0 - 1.0 / nA)
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Reliability-weighted arbitration with RL forgetting and noisy WM storage.

    Idea:
    - RL: delta-rule plus global forgetting toward uniform (simulating interference/decay).
    - WM: on rewarded trials, store a one-hot mapping with noise; global WM decay toward uniform.
    - Arbitration: base wm_weight dynamically adjusted by relative reliability of WM vs RL at the current state
      and penalized by set size. Reliability is measured as "peakiness" over uniform.
      This allows flexible dominance of the more confident system.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: base WM mixture weight (0..1) to be adapted per trial.
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - rl_forget: RL forgetting rate toward uniform each trial (0..1).
    - wm_noise: WM storage noise (0..1). Also scales arbitration sensitivity to reliability difference.
    - wm_decay: WM decay rate toward uniform each trial (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_noise, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p) - np.log(1 - p)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        base_logit = logit(np.clip(wm_weight, 0.0, 1.0))
        size_factor = 3.0 / float(max(3, nS))  # penalize WM when nS is large

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy prob for chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy prob for chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Reliability signals: "peakiness over uniform"
            peak_rl = np.max(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - np.max(Q_s))))
            p_rl_vec = peak_rl / denom_rl
            rel_rl = np.clip(p_rl_vec - 1.0 / nA, 0.0, 1.0 - 1.0 / nA)

            rel_wm = np.clip(np.max(W_s) - 1.0 / nA, 0.0, 1.0 - 1.0 / nA)

            # Dynamic arbitration using reliability difference, scaled by set size penalty
            sens = max(0.0, wm_noise)  # reuse wm_noise as sensitivity scale (higher noise -> softer arbitration)
            wm_logit = base_logit + size_factor * sens * (rel_wm - rel_rl)
            wm_weight = np.clip(sigmoid(wm_logit), 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            lam_rl = np.clip(rl_forget, 0.0, 1.0)
            q = (1.0 - lam_rl) * q + lam_rl * (1.0 / nA)

            # WM update: reward-gated noisy one-hot plus global decay
            if r > 0.5:
                # Noisy storage: blend one-hot with uniform according to wm_noise
                noise = np.clip(wm_noise, 0.0, 1.0)
                w[s, :] = (1.0 - noise) * w_0[s, :]
                w[s, a] = (1.0 - noise) * 1.0 + noise * (1.0 / nA)
                # Add the uniform portion to other actions
                for a_other in range(nA):
                    if a_other != a:
                        w[s, a_other] += noise * (1.0 / nA)
                w[s, :] = np.clip(w[s, :], 0.0, None)
                w[s, :] /= np.sum(w[s, :])

            # Global WM decay
            lam_wm = np.clip(wm_decay, 0.0, 1.0)
            w = (1.0 - lam_wm) * w + lam_wm * w_0

        blocks_log_p += log_p

    return -blocks_log_p