def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with confidence- and load-gated arbitration; age increases WM decay and reduces WM influence.

    Overview:
    - RL learns Q-values with a single learning rate and softmax policy.
    - WM stores a categorical distribution over actions per state. On reward, it shifts toward a one-hot
      code for the chosen action; on non-reward, it decays toward uniform. Decay is stronger for larger set sizes
      and in older adults.
    - Arbitration between WM and RL is a mixture. WM weight increases with:
        (a) lower set size (lower load), and
        (b) WM confidence (difference between top-2 WM probabilities).
      Older adults have reduced WM influence.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - wm_weight0: baseline WM mixing weight (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_decay_older: additional WM decay added if older (>=0)
    - load_sensitivity: scales how much set size modulates WM weight (>=0)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single participant age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_decay_older, load_sensitivity = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:].copy()
            W_s = w[s,:].copy()

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute WM confidence as difference between top-1 and top-2 probabilities
            sorted_W = np.sort(W_s)[::-1]
            conf = np.clip(sorted_W[0] - sorted_W[1], 0.0, 1.0)

            # Load modulation: stronger WM at low set size; scaled by load_sensitivity
            load_gain = 1.0 + load_sensitivity * (3.0 / float(nS) - 0.5)  # ~ >1 for nS=3, <1 for nS=6

            # Age penalty to WM weight
            age_penalty = 1.0 - 0.3 * older

            wm_weight = np.clip(wm_weight0 * load_gain * age_penalty * (0.5 + 0.5 * conf), 0.0, 1.0)

            p_total = np.clip(wm_weight * p_wm + (1 - wm_weight) * p_rl, eps, 1.0)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Reward strengthens chosen action toward one-hot; no-reward decays toward uniform
            if r > 0.5:
                # Move WM distribution toward a one-hot code for the chosen action
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                alpha_w = 0.6 + 0.2 * conf  # stronger update when WM is already confident
                w[s,:] = (1 - alpha_w) * W_s + alpha_w * peaked
            else:
                # Decay toward uniform; stronger decay with higher set size and in older adults
                base_decay = 0.25 + 0.25 * ((nS - 3) / 3.0)  # 0.25 for nS=3, 0.5 for nS=6
                age_decay = wm_decay_older * older
                decay = np.clip(base_decay + age_decay, 0.0, 1.0)
                w[s,:] = (1 - decay) * W_s + decay * w_0[s,:]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Dual-rate RL + episodic WM recall with age-reduced recall probability.

    Overview:
    - RL uses asymmetric learning rates for positive vs. negative outcomes.
    - WM behaves as an episodic store that encodes a sharp distribution after rewarded trials.
      On unrewarded trials, memory decays and may partially persist.
    - Arbitration: the probability of using WM depends on a recall probability that decreases with set size
      and with age. When recall succeeds, WM policy dominates more.
    - WM policy is a softmax over its stored distribution.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive reward (0..1)
    - lr_neg: RL learning rate for zero reward (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - recall_base: baseline recall probability (0..1)
    - age_recall_drop: additional drop in recall for older adults (>=0)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single participant age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, softmax_beta, recall_base, age_recall_drop = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track a per-state recall strength (0..1), reflecting memory availability
        recall_strength = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:].copy()
            W_s = w[s,:].copy()

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Recall probability depends on base, set size (lower for 6), and age (older lower)
            load_factor = 3.0 / float(nS)  # 1.0 (nS=3), 0.5 (nS=6)
            p_recall = np.clip(recall_base * load_factor - age_recall_drop * older, 0.0, 1.0)

            # Effective WM mixing weight combines recall probability and current recall strength of the state
            wm_weight = np.clip(0.5 * p_recall + 0.5 * recall_strength[s], 0.0, 1.0)

            p_total = np.clip(wm_weight * p_wm + (1 - wm_weight) * p_rl, eps, 1.0)
            log_p += np.log(p_total)
      
            # RL update with asymmetric learning rates
            delta = r - Q_s[a]
            eff_lr = lr_pos if delta > 0 else lr_neg
            q[s][a] += eff_lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # Encode a strong memory trace for the chosen action in this state
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                encode_rate = 0.8  # strong episodic push
                w[s,:] = (1 - encode_rate) * W_s + encode_rate * peaked
                # Boost recall strength toward 1 on reward
                recall_strength[s] = np.clip(0.7 * recall_strength[s] + 0.3 + 0.3 * load_factor, 0.0, 1.0)
            else:
                # Memory decays when no reward is obtained
                # Decay is stronger at large set size and for older adults
                decay = 0.2 + 0.3 * (nS - 3) / 3.0 + 0.2 * older
                decay = np.clip(decay, 0.0, 1.0)
                w[s,:] = (1 - decay) * W_s + decay * w_0[s,:]
                # Recall strength also decays
                recall_strength[s] = np.clip((1 - decay) * recall_strength[s], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-gated arbitration: WM takes over when RL is uncertain; age amplifies perceived uncertainty.

    Overview:
    - RL learns Q-values with a single learning rate and maintains an uncertainty (variance-like) estimate
      per state-action via an exponential tracker.
    - WM stores action distributions; reward sharpens, non-reward decays toward uniform with interference that
      scales with set size.
    - Arbitration: WM weight increases when RL uncertainty is high, when set size is small, and decreases with age-driven
      WM reliability drop. Older age increases RL uncertainty scaling, shifting arbitration toward WM only if load allows.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1)
    - softmax_beta: RL inverse temperature (scaled internally by 10)
    - wm_weight0: baseline WM mixing weight (0..1)
    - sigma_lr: learning rate for uncertainty tracker (0..1)
    - age_uncertainty_gain: multiplicative gain on uncertainty for older adults (>=0)

    Inputs:
    - states, actions, rewards: arrays per trial
    - blocks: block id per trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with single participant age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_weight0, sigma_lr, age_uncertainty_gain = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    age = age[0]
    older = 1.0 if age >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b].astype(int)
        block_rewards = rewards[blocks == b].astype(float)
        block_states = states[blocks == b].astype(int)
        block_set_sizes = set_sizes[blocks == b].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Initialize RL uncertainty tracker per state-action
        sigma = 0.25 * np.ones((nS, nA))  # start moderately uncertain

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:].copy()
            W_s = w[s,:].copy()
            Sig_s = sigma[s,:].copy()

            # RL softmax probability of chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta*(Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax probability of chosen action
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Uncertainty for current state: take action-specific or average
            rl_unc = np.clip(Sig_s[a], 0.0, 1.0)

            # Older adults experience amplified uncertainty
            rl_unc *= (1.0 + age_uncertainty_gain * older)

            # Load factor: reduce WM contribution when nS=6
            load_factor = 3.0 / float(nS)  # 1 for 3, 0.5 for 6

            # Combine: WM weight increases with RL uncertainty and low load; capped by wm_weight0
            wm_weight = np.clip(wm_weight0 * load_factor * (0.5 + 0.5 * rl_unc), 0.0, 1.0)

            p_total = np.clip(wm_weight * p_wm + (1 - wm_weight) * p_rl, eps, 1.0)
            log_p += np.log(p_total)
      
            # RL update and uncertainty update
            delta = r - Q_s[a]
            q[s][a] += lr * delta
            # Uncertainty tracker: higher when recent squared error is high
            sigma[s][a] = (1 - sigma_lr) * Sigma_s[a] + sigma_lr * (delta ** 2)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            if r > 0.5:
                # Reward sharpens WM toward one-hot for chosen action
                peaked = (1.0 / nA) * np.ones(nA)
                peaked[a] = 1.0
                peaked /= np.sum(peaked)
                sharpen = 0.7 * load_factor  # more effective sharpening at low load
                # Age reduces effective sharpening
                sharpen *= (1.0 - 0.2 * older)
                w[s,:] = (1 - sharpen) * W_s + sharpen * peaked
                # Interference to other actions in same state is implicit via normalization
            else:
                # Decay toward uniform with interference scaling with set size (higher interference at nS=6)
                interference = 0.2 + 0.3 * (nS - 3) / 3.0  # 0.2 at 3, 0.5 at 6
                # Older adults have slightly higher interference
                interference *= (1.0 + 0.3 * older)
                interference = np.clip(interference, 0.0, 1.0)
                w[s,:] = (1 - interference) * W_s + interference * w_0[s,:]

        blocks_log_p += log_p

    return -blocks_log_p