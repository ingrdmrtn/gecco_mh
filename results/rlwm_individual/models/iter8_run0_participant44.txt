def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + WM-entropy arbitration.

    Idea:
    - RL uses eligibility traces to allow temporally extended credit assignment within a block.
    - WM forms a high-precision item-specific policy that decays over time.
    - Arbitration is governed by comparative uncertainty: when WM is more certain (lower entropy) than RL,
      its weight increases via a logistic function.
    - Captures load effects because WM entropy tends to be higher when set size is large, shifting control to RL.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr            : RL learning rate (0..1).
      beta_rl       : RL inverse temperature; internally scaled by 10.
      wm_gain       : Intercept term of the logistic arbitration in favor of WM (can be negative/positive).
      lambda_trace  : Eligibility-trace decay (0..1); higher => broader credit assignment.
      wm_decay      : Per-trial WM decay toward uniform (0..1).
      entropy_temp  : Slope on entropy-difference in the logistic arbitration (>0 increases sensitivity).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, beta_rl, wm_gain, lambda_trace, wm_decay, entropy_temp = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        e   = np.zeros((nS, nA))  # eligibility traces

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            logits_rl = softmax_beta * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Arbitration by comparative entropy (lower entropy => higher weight)
            eps = 1e-12
            H_rl = -np.sum(p_rl_vec * np.log(p_rl_vec + eps))
            H_wm = -np.sum(p_wm_vec * np.log(p_wm_vec + eps))
            # Positive when WM is more certain than RL
            dH = H_rl - H_wm
            wm_mix = 1.0 / (1.0 + np.exp(-(wm_gain + entropy_temp * dH)))
            wm_mix = np.clip(wm_mix, 0.0, 1.0)

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with eligibility traces
            delta = r - q[s, a]
            # Accumulate trace for current (s,a) and decay others
            e *= lambda_trace
            e[s, a] += 1.0
            q += lr * delta * e

            # WM decay toward uniform baseline
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: reward-locked strengthening; slight suppression on errors
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Blend in a strong item/action association
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                # Mildly reduce the chosen action's WM trace and redistribute to others
                reduce = 0.25 * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += reduce / (nA - 1)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM + WSLS heuristic with load-sensitive WM weight.

    Idea:
    - RL learns state-action values via delta rule; policy is softmax.
    - WM stores strong mappings for rewarded state-action pairs and decays over time.
    - WSLS (win-stay/lose-shift) heuristic operates per state: after a win, repeat; after a loss, avoid.
    - Mixing weights:
        * WM weight decreases with set size (load), capturing capacity limits.
        * WSLS has a fixed strength parameter.
        * RL receives the remaining weight.
    - This tests whether participants rely on a simple WSLS strategy, particularly early in learning,
      and whether WM contribution is reduced under higher load.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr            : RL learning rate (0..1).
      beta_rl       : RL inverse temperature; internally scaled by 10.
      wm_base       : Base WM mixture weight at set size = 1; scaled down by load.
      wsls_strength : Mixture weight for WSLS heuristic (0..1).
      wm_decay      : Per-trial WM decay toward uniform (0..1).
      size_slope    : Load sensitivity (>0): WM weight = wm_base / (1 + size_slope*(nS-1)).

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, beta_rl, wm_base, wsls_strength, wm_decay, size_slope = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and reward per state for WSLS
        last_action = -np.ones(nS, dtype=int)
        last_reward = -np.ones(nS, dtype=float)

        # Compute WM mix weight under load
        wm_weight = wm_base / (1.0 + size_slope * max(0, nS - 1))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)
        wsls_w = np.clip(wsls_strength, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax
            logits_rl = softmax_beta * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM softmax
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # WSLS policy for this state
            p_wsls_vec = np.ones(nA) / nA
            la = last_action[s]
            lr_prev = last_reward[s]
            if la >= 0:
                if lr_prev > 0.0:
                    # Win-stay: concentrate probability on last action
                    p_wsls_vec = np.zeros(nA)
                    p_wsls_vec[la] = 1.0
                elif lr_prev == 0.0:
                    # Lose-shift: avoid last action, distribute to others
                    p_wsls_vec = np.ones(nA)
                    p_wsls_vec[la] = 0.0
                    p_wsls_vec /= np.sum(p_wsls_vec)
                else:
                    # If unknown (-1), keep uniform
                    p_wsls_vec = np.ones(nA) / nA
            p_wsls = p_wsls_vec[a]

            # Combine three policies
            alpha_wm = wm_weight
            alpha_wsls = wsls_w
            alpha_rl = 1.0 - (alpha_wm + alpha_wsls)
            if alpha_rl < 0.0:
                # Renormalize to ensure non-negative mixture weights
                total = max(alpha_wm, 0.0) + max(alpha_wsls, 0.0)
                if total > 1.0:
                    alpha_wm = max(alpha_wm, 0.0) / total
                    alpha_wsls = max(alpha_wsls, 0.0) / total
                    alpha_rl = 1.0 - (alpha_wm + alpha_wsls)
                else:
                    alpha_rl = 0.0

            p_total = alpha_wm * p_wm + alpha_wsls * p_wsls + alpha_rl * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward baseline
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update: strengthen on reward, slight suppression on error
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                reduce = 0.25 * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += reduce / (nA - 1)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

            # Update WSLS memory
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with value decay + WM with load-dependent interference (cross-talk).

    Idea:
    - RL learns via delta rule but also decays toward uniform baseline (forgetting) each trial.
    - WM encodes rewarded items but suffers cross-talk interference that blends WM traces across states.
      Interference grows with set size, simulating binding failures under load.
    - Arbitration weight for WM decreases with load (set size), independently from interference.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
      lr            : RL learning rate (0..1).
      beta_rl       : RL inverse temperature; internally scaled by 10.
      wm_weight0    : Base WM mixture weight at low load.
      q_decay       : Per-trial RL decay toward uniform (0..1).
      load_penalty  : Controls how WM weight shrinks with set size: wm_w = wm_weight0 / (1 + load_penalty*(nS-1)).
      cross_talk    : WM cross-state interference strength (0..1); scaled by set size each trial.

    Returns
    -------
    neg_log_likelihood : float
      Negative log-likelihood of the observed action sequence under the model.
    """
    lr, beta_rl, wm_weight0, q_decay, load_penalty, cross_talk = model_parameters
    softmax_beta = beta_rl * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states  = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q   = (1.0 / nA) * np.ones((nS, nA))
        w   = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-adjusted WM mixture weight
        wm_weight = wm_weight0 / (1.0 + load_penalty * max(0, nS - 1))
        wm_weight = np.clip(wm_weight, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL softmax
            logits_rl = softmax_beta * q[s, :]
            max_rl = np.max(logits_rl)
            exp_rl = np.exp(logits_rl - max_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)
            p_rl = p_rl_vec[a]

            # WM softmax
            logits_wm = softmax_beta_wm * w[s, :]
            max_wm = np.max(logits_wm)
            exp_wm = np.exp(logits_wm - max_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)
            p_wm = p_wm_vec[a]

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with decay toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA)

            # WM update: reward strengthening / mild error suppression
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.6 * w[s, :] + 0.4 * one_hot
            else:
                reduce = 0.2 * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += reduce / (nA - 1)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

            # WM cross-talk interference increases with load
            if nS > 1:
                # Scale interference by relative load (0 for nS=1, up to 1 for nS=6 when using (nS-1)/5)
                load_scale = (nS - 1) / 5.0
                alpha = np.clip(cross_talk * load_scale, 0.0, 1.0)
                avg_w = np.mean(w, axis=0, keepdims=True)  # blend toward average over states
                w = (1.0 - alpha) * w + alpha * avg_w
                # Renormalize each state's WM row
                row_sums = np.sum(w, axis=1, keepdims=True)
                w = np.divide(w, row_sums, out=w, where=row_sums > 0)

        blocks_log_p += log_p

    return -blocks_log_p