def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Hebbian WM with load-driven decay and reliability-based arbitration.

    Mechanism
    - RL: delta-rule Q-learning with softmax action selection.
    - WM: fast Hebbian binding toward the rewarded action and gentle unbinding on errors.
      WM decays toward uniform each trial, and the decay rate increases with set size (load).
    - Arbitration: blends WM and RL using a logistic transform of WM vs RL confidence
      (action-value spread) plus a baseline WM weight. An additional load-dependent
      noise floor steals weight from WM under higher load.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight0: float
            Baseline WM mixture weight (0..1) before confidence and load adjustments.
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        wm_learn: float
            WM Hebbian learning rate for strengthening the chosen action (0..1).
        decay_wm_base: float
            Baseline WM decay toward uniform per trial (0..1).
        load_noise: float
            Additional mixture weight that shifts mass from WM to RL as load increases.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_learn, decay_wm_base, load_noise = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load-sensitive WM decay and load-driven noise floor
        decay_wm = np.clip(decay_wm_base + (max(0, nS - 3) / 3.0) * decay_wm_base, 0.0, 1.0)
        noise_floor = np.clip(load_noise * (max(0, nS - 3) / 3.0), 0.0, 0.5)

        eps = 1e-12
        base_logit = np.log((wm_weight0 + eps) / (1.0 - wm_weight0 + eps))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (given)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Confidence-based arbitration with load penalty (steals weight to RL via noise_floor)
            conf_rl = np.max(Q_s) - np.min(Q_s)
            conf_wm = np.max(W_s) - np.min(W_s)
            wm_adv = conf_wm - conf_rl
            wm_mix = 1.0 / (1.0 + np.exp(-(base_logit + wm_adv)))
            # Reduce WM mix by the noise floor under higher load
            wm_mix = np.clip(wm_mix * (1.0 - noise_floor), 0.0, 1.0)

            # Mixture policy
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM Hebbian update: strengthen chosen action on reward, slight re-normalization on error
            if r > 0.5:
                # Move row toward one-hot on chosen action
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
            else:
                # On errors, gently reduce confidence in the chosen action
                w[s, a] = (1.0 - wm_learn) * w[s, a] + wm_learn * (1.0 / nA)

            # Normalize row to sum to 1
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Recency-gated WM with state-specific repetition bias and load interference.

    Mechanism
    - RL: delta-rule learning with softmax.
    - WM: fast binding on rewards with decay; interference grows with set size.
    - Arbitration: WM weight increases when a state is seen recently (shorter gap),
      and decreases with set size. Additionally, a state-specific repetition bias
      mixes in a policy that repeats the last action for that state.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight0: float
            Baseline WM weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        phi_rec: float
            Recency sensitivity: larger values decay WM weight faster with inter-visit gaps.
        wm_learn: float
            WM update strength toward the rewarded action (0..1).
        rho_rep: float
            Mixture weight for a state-specific repetition policy (0..1).

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, phi_rec, wm_learn, rho_rep = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last visit time and last action per state for recency and repetition
        last_visit = -np.ones(nS, dtype=int)
        last_action = -np.ones(nS, dtype=int)

        # Load interference implemented as stronger decay with larger set size
        base_decay = 0.05
        decay_wm = np.clip(base_decay * (1.0 + (max(0, nS - 3) / 3.0)), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Recency-based WM mixture: closer in time => higher WM usage
            gap = 0 if last_visit[s] < 0 else (t - last_visit[s])
            rec_factor = np.exp(-phi_rec * gap)
            # Load penalty multiplies the WM weight
            load_penalty = 1.0 / (1.0 + max(0, nS - 3))
            wm_mix = np.clip(wm_weight0 * rec_factor * load_penalty, 0.0, 1.0)

            # Base mixture
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl

            # State-specific repetition bias mixture
            rep_policy = 1.0 if (last_action[s] == a and last_action[s] >= 0) else 0.0
            p_total = (1.0 - rho_rep) * p_total + rho_rep * rep_policy

            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - decay_wm) * w + decay_wm * w_0

            # WM update: bind rewarded action, mild redistribution on errors
            if r > 0.5:
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn
            else:
                w[s, a] = (1.0 - wm_learn) * w[s, a] + wm_learn * (1.0 / nA)

            # Normalize
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

            # Update recency and repetition trackers
            last_visit[s] = t
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Surprise-stabilized WM consolidation with load-gated arbitration.

    Mechanism
    - RL: delta-rule with softmax.
    - WM: a consolidation trace c[s,a] that accumulates when the state is stable
      (low surprise on the previous encounter of that state). The WM policy comes from
      the normalized trace. Global decay toward uniform erodes c over time.
    - Arbitration: WM weight increases when the state's previous surprise was low,
      and decreases with set size via a logistic load gate.

    Notes
    - Surprise used for arbitration at trial t is the last observed surprise for that state
      (from its previous visit), preserving proper temporal order for choice probabilities.

    Parameters
    ----------
    model_parameters: tuple/list of 6 floats
        lr: float
            RL learning rate (0..1).
        wm_weight0: float
            Baseline WM mixture weight (0..1).
        softmax_beta: float
            RL inverse temperature; internally scaled by 10.
        theta_surprise: float
            Surprise sensitivity: larger values make arbitration more sensitive to prior surprise.
        decay_bind: float
            Decay rate of WM consolidation trace toward uniform per trial (0..1).
        load_gate: float
            Strength of the logistic penalty on WM as set size increases.

    Returns
    -------
    nll: float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, theta_surprise, decay_bind, load_gate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        c = (1.0 / nA) * np.ones((nS, nA))  # WM consolidation trace
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track previous surprise per state (initialize medium)
        prev_surprise = 0.5 * np.ones(nS)

        # Load gate: logistic penalty on WM with larger set sizes
        load_penalty = 1.0 / (1.0 + np.exp(load_gate * (max(0, nS - 3))))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = c[s, :]  # WM policy derived from consolidation trace

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Surprise-based arbitration uses the state's last observed surprise (lower => more WM)
            # Map surprise in [0,1+] through a logistic transform around theta_surprise
            surprise_signal = prev_surprise[s] - theta_surprise
            wm_mix_raw = wm_weight0 * (1.0 / (1.0 + np.exp(10.0 * surprise_signal)))
            wm_mix = np.clip(wm_mix_raw * load_penalty, 0.0, 1.0)

            # Mixture probability
            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # Update surprise for this state (for next time it's visited)
            prev_surprise[s] = min(1.0, abs(delta))

            # WM decay toward uniform
            c = (1.0 - decay_bind) * c + decay_bind * w_0

            # WM consolidation: strengthen current association proportional to (1 - current surprise)
            # Larger positive updates when outcome was unsurprising (stable mapping)
            gain = (1.0 - min(1.0, abs(delta)))
            c[s, :] = (1.0 - gain) * c[s, :]
            c[s, a] += gain

            # Normalize row
            row_sum = np.sum(c[s, :])
            if row_sum > 0:
                c[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p