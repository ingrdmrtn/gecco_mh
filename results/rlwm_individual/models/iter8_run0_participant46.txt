def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + recency-weighted WM with load-scaled WM temperature

    Idea:
    - RL: tabular Q-learning with softmax (from template).
    - WM: associative weights per state w[s,:] that are attracted toward one-hot for the
      last rewarded action in that state and otherwise decay toward uniform with a recency
      timescale. WM policy is a softmax whose inverse temperature decreases with set size.
    - Arbitration: fixed WM mixture weight scaled by load (3/nS).

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight0: float in [0,1], baseline WM mixture weight (before load scaling).
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - tau_rec: float >= 0, WM recency/learning strength; governs both attraction to the last
               rewarded action and decay to uniform (higher => faster).
    - wm_beta_base: float >= 0, base inverse temperature for WM softmax at set size 3.
    - ss_temp_gain: float, exponent controlling how WM temperature scales with set size:
                    beta_wm = wm_beta_base * (3/nS) ** ss_temp_gain.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight0, softmax_beta, tau_rec, wm_beta_base, ss_temp_gain = model_parameters
    softmax_beta *= 10
    softmax_beta_wm_base = wm_beta_base

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute load-dependent components
        load_scale = 3.0 / float(nS)
        beta_wm = softmax_beta_wm_base * (load_scale ** ss_temp_gain)

        # Map tau_rec to a bounded step size in (0,1)
        lam = 1.0 - np.exp(-max(0.0, tau_rec))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax with load-scaled temperature
            p_wm = 1.0 / np.sum(np.exp(beta_wm * (W_s - W_s[a])))

            # Mixture with load scaling
            wm_weight_eff = np.clip(wm_weight0 * load_scale, 0.0, 1.0)
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # Global WM decay toward uniform (recency)
            w = (1.0 - lam) * w + lam * w_0

            # Reward-contingent WM attraction
            if r > 0.5:
                # Pull the chosen action toward 1 in that state
                w[s, :] = (1.0 - lam) * w[s, :]
                w[s, a] += lam
            else:
                # A small corrective push away from the chosen action
                w[s, a] = (1.0 - 0.5 * lam) * w[s, a]
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL + capacity- and uncertainty-weighted WM with decay-by-load

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: associative weights per state; learns toward one-hot on reward and weakly repels on no-reward.
    - Arbitration: WM weight scales with
        wm_weight0 * min(1, K/nS) * ((max_entropy - H(p_rl_state))/max_entropy) ** gamma
      where p_rl_state is the RL softmax over actions for the current state, H is entropy,
      and max_entropy = log(nA). Thus, more WM use when: (a) set size within capacity K and
      (b) RL is more certain (lower entropy).
    - WM also decays to uniform more when nS > K.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - wm_weight0: float in [0,1], baseline WM mixture weight.
    - K: float >= 0, effective WM capacity in number of states.
    - gamma: float >= 0, exponent controlling how RL certainty modulates WM weight.
    - wm_alpha: float in [0,1], WM learning rate and decay magnitude.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight0, K, gamma, wm_alpha = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50.0

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        cap_scale = min(1.0, float(K) / max(1.0, float(nS)))
        max_entropy = np.log(nA + 1e-12)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # RL softmax distribution for entropy calculation
            # Compute p over all actions by normalizing exp(beta*Q)
            q_shift = Q_s - np.max(Q_s)
            exp_q = np.exp(softmax_beta * q_shift)
            p_rl_vec = exp_q / np.sum(exp_q)
            entropy = -np.sum(p_rl_vec * (np.log(p_rl_vec + 1e-12)))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: capacity and RL certainty
            certainty = (max_entropy - entropy) / max_entropy
            wm_weight_eff = wm_weight0 * cap_scale * (max(0.0, certainty) ** max(0.0, gamma))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reinforce on reward, gentle repulsion on no-reward
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                w[s, a] = (1.0 - 0.5 * wm_alpha) * w[s, a]
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

            # Capacity-dependent decay to uniform across all states
            decay = (1.0 - cap_scale) * wm_alpha
            if decay > 0:
                w = (1.0 - decay) * w + decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: RL + consistency-adaptive WM gating with set-size damping

    Idea:
    - RL: tabular Q-learning with softmax (template).
    - WM: associative matrix over actions updated toward one-hot on reward and mildly
      weakened otherwise.
    - Arbitration: a per-state gating variable m[s] in [0,1] that tracks recent reward
      consistency via exponential smoothing. The WM mixture weight is:
          wm_weight_eff = sigmoid(beta_m * (m[s] - theta_m)) * (3/nS)
      where m[s] is updated by m[s] <- (1 - kappa)*m[s] + kappa*(1 if reward else 0).
      Thus, when outcomes are consistently rewarding in a state, WM control increases,
      but it is damped under higher load.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, RL inverse temperature (internally scaled by 10).
    - kappa: float in [0,1], smoothing rate for the consistency trace m[s].
    - beta_m: float >= 0, slope of the logistic gate mapping m[s] to WM weight.
    - theta_m: float in [0,1], threshold for the logistic gate (center point).
    - wm_alpha: float in [0,1], WM learning rate.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, kappa, beta_m, theta_m, wm_alpha = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50.0

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Consistency trace initialized to neutral 0.5
        m = 0.5 * np.ones(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability (template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic-like)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Load-damped, consistency-gated WM weight
            load_scale = 3.0 / float(nS)
            gate = 1.0 / (1.0 + np.exp(-beta_m * (m[s] - theta_m)))
            wm_weight_eff = np.clip(gate * load_scale, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update
            if r > 0.5:
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # Weaken the chosen action a bit and renormalize
                w[s, a] = (1.0 - 0.5 * wm_alpha) * w[s, a]
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

            # Consistency trace update toward observed outcome (1 for reward, 0 otherwise)
            target = 1.0 if r > 0.5 else 0.0
            m[s] = (1.0 - kappa) * m[s] + kappa * target

        blocks_log_p += log_p

    return -blocks_log_p