def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with constant arbitration, load-penalized WM weight, and reward-gated WM storage.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM arbitration weight is a logistic of a baseline minus a load penalty.
      Heavier loads (larger set size) reduce reliance on WM.
    - WM stores rewarded associations with strength wm_eta; unrewarded trials decay WM toward uniform.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - wm_logit_base: real, baseline logit controlling WM mixture weight.
    - wm_eta: float in [0,1], WM learning rate toward one-hot target on reward, and toward uniform on no reward.
    - load_sigma: float >= 0, penalizes WM weight as set size increases; effective logit -= load_sigma*(nS-3).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, wm_logit_base, wm_eta, load_sigma = model_parameters
    softmax_beta *= 10.0  # RL temperature scaling
    softmax_beta_wm = 50.0  # deterministic WM readout
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Load effect on WM reliance
        load_scaled = max(0.0, nS - 3)  # 0 for 3, 3 for 6 (assuming sizes in {3,6})
        wm_logit = wm_logit_base - load_sigma * load_scaled
        wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy: softmax over WM values
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated storage vs. decay
            if r > 0.0:
                # Move WM for state s toward the one-hot of chosen action
                target = w_0[s, :].copy()
                target[:] = 0.0
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target
            else:
                # No reward -> decay WM toward uniform prior
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with load-reduced RL temperature, constant arbitration, and passive WM decay.

    Idea:
    - Choices are a mixture of RL and WM policies with a constant arbitration weight.
    - RL softmax temperature decreases with load (exploration increases as set size grows).
    - WM decays passively toward uniform on every trial; rewarded trials write a one-hot pattern.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - mix_kappa: real, converted via sigmoid into the constant WM mixture weight.
    - explore_load: float >= 0, increases exploration (reduces effective beta) as set size increases.
      Effective beta_rl = softmax_beta*10 / (1 + explore_load*(nS-3)).
    - wm_decay_rate: float in [0,1], passive decay rate of WM toward uniform each time state is visited.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, mix_kappa, explore_load, wm_decay_rate = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    wm_weight = 1.0 / (1.0 + np.exp(-mix_kappa))

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        # Load-dependent RL temperature
        load_scaled = max(0.0, nS - 3)
        beta_rl_eff = base_beta / (1.0 + explore_load * load_scaled)

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with load-reduced temperature
            denom_rl = np.sum(np.exp(beta_rl_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive decay toward uniform on visit
            w[s, :] = (1.0 - wm_decay_rate) * w[s, :] + wm_decay_rate * w_0[s, :]

            # Rewarded write strengthens the chosen action's representation
            if r > 0.0:
                target = w_0[s, :].copy()
                target[:] = 0.0
                target[a] = 1.0
                # Simple overwrite-style blend
                w[s, :] = 0.5 * w[s, :] + 0.5 * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with PE-trace-based arbitration and cross-state WM interference.

    Idea:
    - Arbitration weight for WM depends on a state-specific running unsigned PE trace:
      larger recent PEs (uncertainty) reduce WM reliance (or vice versa) via a logistic.
    - WM updates via delta rule toward one-hot on reward, with cross-state interference:
      storing an association partially overwrites the same action channel in other states.

    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - softmax_beta: float >= 0, base inverse temperature for RL; internally scaled by 10.
    - pe_slope: real, slope for how strongly PE trace modulates WM weight.
    - pe_bias: real, baseline logit for WM weight independent of PE.
    - interference: float in [0,1], fraction of WM weight for the chosen action that leaks to other states.
    - wm_alpha: float in [0,1], WM learning rate toward the target vector on reward; small decay on no reward.

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, softmax_beta, pe_slope, pe_bias, interference, wm_alpha = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # State-specific running unsigned PE trace for arbitration (initialized low)
        pe_trace = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # PE-trace-based arbitration: higher recent PE reduces WM reliance (via slope sign)
            wm_logit = pe_bias - pe_slope * pe_trace[s]
            wm_weight = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update and PE trace update
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # Update unsigned PE trace for this state (simple exponential averaging)
            pe_trace[s] = 0.7 * pe_trace[s] + 0.3 * abs(delta)

            # WM update:
            if r > 0.0:
                # Move w[s,:] toward one-hot of chosen action
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * target

                # Cross-state interference: leak chosen action channel to other states
                if interference > 0.0 and nS > 1:
                    for sp in range(nS):
                        if sp == s:
                            continue
                        # Pull w[sp, a] slightly toward uniform to mimic overwriting
                        w[sp, a] = (1.0 - interference) * w[sp, a] + interference * (1.0 / nA)
            else:
                # No reward -> mild decay toward uniform for current state
                w[s, :] = (1.0 - 0.5 * wm_alpha) * w[s, :] + 0.5 * wm_alpha * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p