def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with eligibility traces and age- and load-modulated WM gating.

    Idea
    - A Q-learner is augmented with an eligibility trace within each state that persists across time.
    - The trace increases credit assignment to recently chosen actions; it decays faster for larger set sizes
      and for older adults (reflecting reduced maintenance).
    - A working-memory (WM) gate transiently boosts the probability of the most recently rewarded action in a state.
      The gate is stronger in small sets and for younger adults.
    - Policy is a mixture of RL softmax and WM-gated choice.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    age : array-like of float
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (lr, beta, trace_lambda, wm_gate0, age_gain)
        - lr: learning rate for Q updates (0..1).
        - beta: inverse temperature for softmax (>0).
        - trace_lambda: base eligibility trace persistence (0..1).
        - wm_gate0: base WM gate strength (0..1), mixed as a probability weight.
        - age_gain: scales age and load modulation of both trace decay and WM gate.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, trace_lambda, wm_gate0, age_gain = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q and eligibility traces per state-action
        Q = (1.0 / nA) * np.ones((nS, nA))
        e = np.zeros((nS, nA))  # eligibility traces

        # WM memory: last rewarded action per state and its strength
        last_hit_action = -1 * np.ones(nS, dtype=int)
        last_hit_strength = np.zeros(nS)  # decays over time; refreshed by rewards

        # Load term: 0 at set size 3, 0.5 at set size 6
        load_term = 1.0 - (3.0 / float(nS))
        # Age-load modulation factor
        mod = 1.0 + age_gain * (is_older + load_term)

        # Effective parameters after modulation
        # Older and larger load => faster decay (i.e., smaller effective lambda)
        lam_eff = np.clip(trace_lambda / mod, 0.0, 1.0)
        # WM gate is weaker under load and for older adults
        wm_eff = np.clip(wm_gate0 / mod, 0.0, 1.0)

        logp = 0.0
        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # RL policy (softmax over Q with beta)
            prefs = beta * Q[s, :]
            # avoid overflow by subtracting chosen pref in denominator
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_rl = 1.0 / max(denom, 1e-12)

            # WM-gated policy: if we have a recent rewarded action in this state, favor it
            p_wm = 1.0 / nA
            if last_hit_action[s] >= 0:
                # Probability mass allocated to remembered action based on its strength
                wm_pref = np.zeros(nA)
                remembered = last_hit_action[s]
                # Form a deterministic preference scaled by memory strength
                # Convert to a categorical prob: mass to remembered action, rest uniform
                m = np.clip(last_hit_strength[s], 0.0, 1.0)
                p_wm_vec = (1.0 - m) * (np.ones(nA) / nA)
                p_wm_vec[remembered] += m * (1.0 - 1.0 / nA)
                p_wm = p_wm_vec[a]
            # Mixture of WM and RL
            p = wm_eff * p_wm + (1.0 - wm_eff) * p_rl
            p = max(p, 1e-12)
            logp += np.log(p)

            # Update eligibility traces: decay all, set chosen to 1
            e[s, :] *= lam_eff
            e[s, a] = 1.0

            # TD error and Q update using traces within the same state (credit to chosen, small to others via decay)
            pe = r - Q[s, a]
            # Update only this state's actions via its traces (others unaffected as traces=0)
            Q[s, :] += lr * pe * e[s, :]

            # Update WM memory: if reward, store and strengthen; otherwise decay
            # Strength decays with load and age
            decay_wm = np.clip(0.1 * mod, 0.0, 0.95)
            last_hit_strength[s] *= (1.0 - decay_wm)
            if r > 0.5:
                last_hit_action[s] = a
                # Boost strength proportional to surprise and bounded
                last_hit_strength[s] = np.clip(last_hit_strength[s] + (1.0 - last_hit_strength[s]) * (0.5 + 0.5 * abs(pe)), 0.0, 1.0)

        total_logp += logp

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Bayesian action-belief learner with age-shaped priors and load-dependent forgetting.

    Idea
    - For each state, maintain a Dirichlet belief over which of the 3 actions is correct.
    - Prior concentration is higher for older adults (strong priors, slower adaptation).
    - Beliefs undergo exponential forgetting that is stronger in larger set sizes.
    - Choices follow a "probability matching softmax": posterior action probabilities are
      sharpened by an inverse temperature and mixed with a small lapse.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    age : array-like of float
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (prior_base, beta, rho_forget, age_weight, lapse)
        - prior_base: base Dirichlet prior concentration per action (>0).
        - beta: inverse temperature applied to posterior probs (>0).
        - rho_forget: base forgetting rate toward the prior per trial (0..1).
        - age_weight: scales how much age and load increase prior strength and forgetting.
        - lapse: lapse probability mixed with uniform choice (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    prior_base, beta, rho_forget, age_weight, lapse = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        load_term = 1.0 - (3.0 / float(nS))  # 0 for 3-set, 0.5 for 6-set

        # Age-load modulation: stronger priors and forgetting for older/larger sets
        prior_gain = 1.0 + age_weight * (is_older + load_term)
        forget_gain = 1.0 + age_weight * (is_older + 0.5 * load_term)

        # Prior concentration per action
        a0 = np.full((nS, nA), prior_base * prior_gain, dtype=float)
        alpha_dir = a0.copy()

        logp = 0.0
        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # Posterior mean over actions
            probs = alpha_dir[s, :] / np.sum(alpha_dir[s, :])
            # Apply inverse temperature to sharpen; implement softmax on log-probs
            prefs = beta * np.log(np.clip(probs, 1e-12, 1.0))
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_choice = 1.0 / max(denom, 1e-12)
            # Lapse mixture
            p_final = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp += np.log(p_final)

            # Forgetting toward prior (per trial, state-specific)
            rho_eff = np.clip(rho_forget * forget_gain, 0.0, 1.0)
            alpha_dir[s, :] = (1.0 - rho_eff) * alpha_dir[s, :] + rho_eff * a0[s, :]

            # Update counts based on outcome
            if r > 0.5:
                # Reward: increase belief that chosen action is correct
                alpha_dir[s, a] += 1.0
            else:
                # No reward: redistribute evidence to the two other actions
                inc = 0.5  # split 1 unit across the two non-chosen actions
                for aa in range(nA):
                    if aa != a:
                        alpha_dir[s, aa] += inc

        total_logp += logp

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    WSLS-RL mixture with age- and load-dependent arbitration.

    Idea
    - Combine a model-free Q-learner with a win-stay/lose-shift (WSLS) heuristic.
    - Arbitration weight for WSLS decreases with set size and increases with age,
      reflecting heavier reliance on heuristics by older adults under load.
    - Final policy is a mixture of WSLS and softmax over Q, with a small lapse.

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Reward outcome (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size for the block (3 or 6).
    age : array-like of float
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (lr, beta, ws0, k_age_load, epsilon)
        - lr: learning rate for Q updates (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - ws0: baseline WSLS weight (0..1).
        - k_age_load: modulation of WSLS weight by age and set size.
                      Positive => more WSLS weight for older and larger set sizes.
        - epsilon: lapse probability mixed with uniform (0..1).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, beta, ws0, k_age_load, epsilon = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        b_states = states[mask].astype(int)
        b_actions = actions[mask].astype(int)
        b_rewards = rewards[mask].astype(float)
        nS = int(set_sizes[mask][0])

        # Initialize Q
        Q = (1.0 / nA) * np.ones((nS, nA))

        # Track last action and reward per state
        last_a = -1 * np.ones(nS, dtype=int)
        last_r = np.zeros(nS, dtype=float)

        # WSLS weight adjusted by age and load
        load_term = 1.0 - (3.0 / float(nS))  # 0 for 3-set, 0.5 for 6-set
        ws_weight = np.clip(ws0 * (1.0 + k_age_load * (is_older + load_term)), 0.0, 1.0)

        logp = 0.0
        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])

            # RL softmax policy
            prefs = beta * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_rl = 1.0 / max(denom, 1e-12)

            # WSLS policy vector
            p_ws = 1.0 / nA
            if last_a[s] >= 0:
                p_vec = np.ones(nA) / nA
                if last_r[s] > 0.5:
                    # Win-stay: put all extra mass on last action
                    la = last_a[s]
                    p_vec = np.ones(nA) * (0.0)
                    p_vec[la] = 1.0
                else:
                    # Lose-shift: split equally among non-last actions
                    la = last_a[s]
                    p_vec = np.zeros(nA)
                    for aa in range(nA):
                        if aa != la:
                            p_vec[aa] = 0.5
                p_ws = p_vec[a]

            # Mixture of WSLS and RL with lapse
            p_mix = ws_weight * p_ws + (1.0 - ws_weight) * p_rl
            p_final = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_final = max(p_final, 1e-12)
            logp += np.log(p_final)

            # Q update
            pe = r - Q[s, a]
            Q[s, a] += lr * pe

            # Update WSLS memory
            last_a[s] = a
            last_r[s] = r

        total_logp += logp

    return -total_logp