def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with eligibility traces + capacity-limited WM mixture and load-sensitive forgetting.

    Core ideas:
    - RL updates are temporally smoothed via replacing eligibility traces (lambda_elig),
      which can capture rapid generalization of credit within a block.
    - WM contributes a near-deterministic policy based on the most recently rewarded association.
    - Arbitration weight for WM is reduced when set size exceeds WM capacity (K_capacity).
    - WM associations decay toward uniform each trial at rate wm_decay (load-insensitive in this model).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_weight_base: baseline WM mixture weight (0..1).
    - K_capacity: WM capacity in "slots" (can be real-valued; effective scaling is min(1, K/nS)).
    - wm_decay: WM forgetting rate toward uniform each trial (0..1).
    - lambda_elig: eligibility trace decay (0..1), replacing trace.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, K_capacity, wm_decay, lambda_elig = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))
        # Eligibility traces for RL (replacing)
        e = np.zeros((nS, nA))

        # Load-dependent WM arbitration via capacity scaling
        cap_scale = min(1.0, float(K_capacity) / max(1.0, float(nS)))
        wm_weight_eff_block = np.clip(wm_weight_base * cap_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]
            # RL policy probability of chosen action (softmax trick)
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: near-deterministic softmax over WM strengths
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture policy
            p_total = p_wm * wm_weight_eff_block + (1 - wm_weight_eff_block) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update with replacing eligibility traces
            pe = r - Q_s[a]
            e *= lambda_elig
            e[s, :] = 0.0
            e[s, a] = 1.0
            q += lr * pe * e

            # WM forgetting (global toward uniform)
            w = (1 - wm_decay) * w + wm_decay * w_0
            # WM one-shot storage on reward
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with meta-learning of learning rate from surprise + WM confidence reduced by load-driven interference,
    and arbitration by RL uncertainty and WM confidence.

    Core ideas:
    - RL learning rate increases with surprise (unsigned prediction error), enabling faster adaptation when
      outcomes are unexpected (meta_lr_gain).
    - WM confidence (precision) decays with interference generated by the number of distinct states engaged
      within the block (load_interference_rate), stronger under larger set sizes.
    - Arbitration increases WM's contribution when RL is confident (low entropy) and WM is precise; otherwise
      RL dominates.
    - WM is near-deterministic, with decay toward uniform accelerated by interference.

    Parameters (model_parameters):
    - lr_base: base RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_weight_base: baseline WM mixture weight (0..1).
    - wm_precision_base: baseline WM precision/confidence (0..1).
    - load_interference_rate: scales how much each newly encountered state reduces WM precision (>=0).
    - meta_lr_gain: sensitivity of learning rate to unsigned PE (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr_base, softmax_beta, wm_weight_base, wm_precision_base, load_interference_rate, meta_lr_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track interference as the fraction of unique states engaged so far
        seen_states = np.zeros(nS, dtype=bool)
        unique_count = 0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            if not seen_states[s]:
                seen_states[s] = True
                unique_count += 1

            # Interference proportional to fraction of set encountered times load_interference_rate
            interference = np.clip(load_interference_rate * (unique_count / max(1.0, float(nS))), 0.0, 1.0)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL uncertainty via policy entropy
            logits = softmax_beta * (Q_s - np.max(Q_s))
            probs = np.exp(logits) / np.sum(np.exp(logits))
            entropy = -np.sum(probs * np.log(np.clip(probs, 1e-12, 1.0)))
            entropy_norm = entropy / np.log(nA)  # in [0,1]

            # WM precision reduced by interference
            wm_precision_eff = np.clip(wm_precision_base * (1.0 - interference), 0.0, 1.0)

            # Arbitration weight increases when RL is confident (low entropy) and WM is precise
            wm_weight_eff = np.clip(wm_weight_base * wm_precision_eff * (1.0 - entropy_norm), 0.0, 1.0)

            p_total = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # Meta-learning of RL learning rate by surprise
            pe = r - Q_s[a]
            lr_eff = np.clip(lr_base + meta_lr_gain * np.abs(pe), 0.0, 1.0)
            q[s, a] += lr_eff * pe

            # WM forgetting accelerated by interference
            wm_forgetting = np.clip(interference, 0.0, 1.0)
            w = (1 - wm_forgetting) * w + wm_forgetting * w_0
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with choice stickiness and lapse + WM boosted by local correctness streak and load-dependent lapses.

    Core ideas:
    - RL includes a perseveration bias: repeating the last action in a state adds a bonus (stickiness).
    - A load-dependent lapse probability mixes a uniform random policy into the final choice.
    - WM contribution is boosted when the current state has a recent streak of correct responses
      (captures rapid one-shot learning and exploitation).
    - WM decays toward uniform at a rate tied to load_lapse_gain (higher load -> faster decay).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: inverse temperature for RL policy (scaled internally by 10).
    - wm_weight: baseline WM mixture weight (0..1).
    - stickiness: bonus added to the last chosen action for the current state in RL policy space (>=0).
    - lapse_base: baseline lapse probability mixed with uniform (0..0.5).
    - load_lapse_gain: increases lapse and WM decay with load (>=0).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight, stickiness, lapse_base, load_lapse_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness (-1 means none yet)
        last_action = -1 * np.ones(nS, dtype=int)
        # Track correctness streak per state
        streak = np.zeros(nS, dtype=int)

        # Load-dependent lapse and WM decay
        load_factor = max(0.0, (float(nS) - 3.0) / 3.0)  # 0 for 3, ~1 for 6
        epsilon_lapse = np.clip(lapse_base + load_lapse_gain * load_factor, 0.0, 0.5)
        wm_decay_eff = np.clip(0.1 + 0.4 * load_factor + 0.5 * load_lapse_gain, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with stickiness added to last action for this state
            bias = np.zeros(nA)
            if last_action[s] >= 0:
                bias[last_action[s]] = stickiness
            V = Q_s + bias
            p_rl = 1 / np.sum(np.exp(softmax_beta * (V - V[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM weight boosted by correctness streak (capped)
            streak_boost = 1.0 + 0.5 * min(2, max(0, streak[s]))
            wm_weight_eff = np.clip(wm_weight * streak_boost, 0.0, 1.0)

            # Mixture of WM and RL
            p_mix = p_wm * wm_weight_eff + (1 - wm_weight_eff) * p_rl
            # Lapse to uniform with load-dependent epsilon
            p_total = (1 - epsilon_lapse) * p_mix + epsilon_lapse * (1.0 / nA)

            log_p += np.log(max(p_total, 1e-12))

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM decay
            w = (1 - wm_decay_eff) * w + wm_decay_eff * w_0
            # WM update on reward: one-shot store
            if r > 0:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

            # Update last action and streak
            last_action[s] = a
            streak[s] = streak[s] + 1 if r > 0 else 0

        blocks_log_p += log_p

    return -blocks_log_p