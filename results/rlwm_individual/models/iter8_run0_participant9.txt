def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM mixture with uncertainty- and load-weighted arbitration.

    Core ideas:
    - RL: delta-rule Q-learning within block.
    - WM: fast associative table that stores rewarded actions; decays toward uniform.
    - Arbitration: WM weight is adapted on each trial by (a) set-size load and (b) RL surprise
      (absolute prediction error). Larger set size and larger surprise reduce reliance on WM.

    Parameters (6 total, in order):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM weight (0..1), transformed internally via logit and sigmoid.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_decay: Per-trial WM decay toward uniform (0..1).
    - surprise_sensitivity: How strongly absolute RL prediction error reduces WM weight (>0).
    - ns_scale: How strongly set size reduces WM reliance (>0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, surprise_sensitivity, ns_scale = model_parameters[:6]
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM channel
    nA = 3
    eps = 1e-12

    # helper: stable sigmoid with clipping
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))

    # helper: logit of a probability in (0,1)
    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        base_logit = logit(wm_weight_base)
        log_p = 0.0

        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy (given by template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values (deterministic channel)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise arbitration weight
            # Compute RL surprise before updating (absolute prediction error)
            delta_prior = r - Q_s[a]
            wm_logit_t = base_logit - ns_scale * (nS - 3.0) - surprise_sensitivity * np.abs(delta_prior)
            wm_weight_t = sigmoid(wm_logit_t)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM passive decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0
            # WM learns rewarded associations quickly by boosting chosen action if rewarded
            if r > 0.5:
                # allocate mass to chosen action in the current state
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with global forgetting + capacity/recency-limited WM and load/interference arbitration.

    Core ideas:
    - RL: delta-rule with global drift toward uniform (forgetting), capturing volatility across trials.
    - WM: item-and-order buffer storing up to K rewarded state-action pairs (recency-based).
           Stored state rows are set to one-hot on the stored action; evicted rows reset to uniform.
    - Arbitration: baseline WM weight adjusted by (a) effective capacity relative to set size
                   and (b) interference that grows with set size.

    Parameters (6 total, in order):
    - lr: RL learning rate (0..1).
    - wm_weight: Baseline WM weight (0..1), transformed internally via logit and sigmoid.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - q_forgetting: Per-trial RL drift toward uniform (0..1).
    - K_cap: WM capacity (number of state-action bindings that can be held; >=1).
    - interference: Set-size interference strength (>0), reduces WM weight as nS grows.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, q_forgetting, K_cap, interference = model_parameters[:6]
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM channel
    nA = 3
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM buffer for up to K_cap items: list of (state -> action)
        buffer_states = []
        buffer_map = {}  # state -> action

        # Block-level arbitration baseline adjusted by capacity vs set size and interference
        # If capacity < set size, downweight WM; if capacity >= set size, maintain higher weight.
        cap_term = np.log((K_cap + 1e-6) / (nS + 1e-6))  # positive if K >= nS
        w_logit_block = logit(wm_weight_base) + cap_term - interference * max(0.0, (nS - 3.0))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL forgetting toward uniform each trial
            q = (1.0 - q_forgetting) * q + q_forgetting * (1.0 / nA)

            Q_s = q[s, :].copy()
            # If the state is stored in WM, ensure WM row is one-hot; otherwise it remains whatever it is
            if s in buffer_map:
                w[s, :] = 0.0
                w[s, buffer_map[s]] = 1.0
            W_s = w[s, :].copy()

            # RL policy (given by template form)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_weight_t = sigmoid(w_logit_block)
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: on rewarded trials, store binding with recency and capacity K_cap
            if r > 0.5:
                # store/update mapping
                if s in buffer_map:
                    buffer_states.remove(s)  # move to most recent
                buffer_map[s] = a
                buffer_states.append(s)
                # Evict oldest if over capacity
                while len(buffer_states) > int(max(1, np.round(K_cap))):
                    evict_s = buffer_states.pop(0)
                    buffer_map.pop(evict_s, None)
                    # reset evicted row to uniform
                    w[evict_s, :] = w_0[evict_s, :]

                # set current row to one-hot
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                # On non-reward, do not store; leave as is (already handled above if previously stored)
                pass

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL-WM mixture with streak-gated WM reliance and load scaling.

    Core ideas:
    - RL: delta-rule Q-learning.
    - WM: fast associative table that strengthens on rewarded choices and decays toward uniform.
    - Gating: Per-state win-streak counter promotes WM reliance when consistent reward is observed.
              The WM weight increases with streak length but is divided by a set-size exponent term.

    Parameters (6 total, in order):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM weight (0..1), transformed via logit and sigmoid.
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_decay: Per-trial WM decay toward uniform (0..1).
    - streak_gain: Gain controlling how much streak length boosts WM reliance (>0).
    - ns_exponent: Exponent controlling how strongly set size suppresses WM (>=0).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight_base, softmax_beta, wm_decay, streak_gain, ns_exponent = model_parameters[:6]
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-np.clip(x, -20, 20)))

    def logit(p):
        p = np.clip(p, 1e-6, 1 - 1e-6)
        return np.log(p / (1 - p))

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state win streak (counts consecutive rewarded trials for that state)
        streak = np.zeros(nS)

        base_logit = logit(wm_weight_base)
        load_divisor = max(1.0, (float(nS) ** max(0.0, ns_exponent)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Streak-gated WM weight (higher streak -> more WM, scaled down by set size)
            wm_logit_t = base_logit + (streak_gain * streak[s]) / load_divisor
            wm_weight_t = sigmoid(wm_logit_t)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM learns from reward by boosting chosen action
            if r > 0.5:
                w[s, :] = (1.0 - wm_decay) * w[s, :]
                w[s, a] += wm_decay
                streak[s] += 1.0
            else:
                # reset streak for this state on failure
                streak[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p