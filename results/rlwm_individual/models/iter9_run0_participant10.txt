def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with age- and load-modulated perseveration, global interference (decay), and lapses.

    Idea:
    - Model-free Q-learning drives choices.
    - A perseveration (choice stickiness) bias adds to action logits, stronger under low load and for younger adults.
    - A global interference/decay pulls Q-values toward uniform each trial, stronger under higher set size and for older adults.
    - A lapse process mixes in a uniform random policy, more likely under higher load and for older adults.

    Parameters (model_parameters):
    - alpha: scalar in [0,1], RL learning rate.
    - beta: inverse temperature (>0) for softmax choice.
    - stick_gain: strength (>0) of perseveration bias; effective bias decreases with load and age.
    - decay_gain: strength (>0) of global interference per trial; increases with load and age.
    - lapse_base: base lapse rate in [0,1]; effective lapse grows with load and age.

    Inputs:
    - states: array of ints (trial-wise states; 0..set_size-1 within each block).
    - actions: array of ints (chosen actions; 0..2).
    - rewards: array of floats (0 or 1).
    - blocks: array of ints (block identifiers; learning resets by block).
    - set_sizes: array of ints (set size per trial; typically 3 or 6).
    - age: array-like with a single entry, participant age in years.
    - model_parameters: [alpha, beta, stick_gain, decay_gain, lapse_base].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, stick_gain, decay_gain, lapse_base = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))

        prev_action = None  # within-block perseveration reference

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Load- and age-modulated components
            load_factor = float(curr_set) / 3.0  # 1 for set=3, 2 for set=6
            inv_load = 3.0 / float(curr_set)
            # Perseveration effective strength decreases with load and age
            stick_eff = stick_gain * inv_load * (1.0 - 0.4 * is_older)
            # Global interference increases with load and age
            decay_eff = decay_gain * load_factor * (0.6 + 0.4 * is_older)
            decay_eff = min(max(decay_eff, 0.0), 1.0)
            # Lapse increases with load and age
            lapse = lapse_base * load_factor * (0.7 + 0.6 * is_older)
            lapse = min(max(lapse, 0.0), 0.4)  # cap lapses

            # Build logits with perseveration bias
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            if prev_action is not None:
                bias = np.zeros(nA)
                bias[int(prev_action)] = 1.0
                logits = logits + stick_eff * bias

            # Softmax
            p = np.exp(logits)
            p /= np.sum(p)
            # Lapse mixture
            p = (1.0 - lapse) * p + lapse * (1.0 / nA)

            # Accumulate NLL
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # Global interference pulling toward uniform (applied to entire Q table)
            Q = (1.0 - decay_eff) * Q + decay_eff * (1.0 / nA)

            # Track previous action
            prev_action = a

    return nll


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + capacity-limited, decaying WM with probabilistic arbitration and age/load-modulated exploration.

    Idea:
    - Two systems:
      1) RL Q-learning.
      2) A WM store that encodes one-hot policies for rewarded state-action pairs.
    - WM capacity is limited to K_eff items; whether the current state is effectively in WM depends
      on K_eff relative to set size and on a per-state activation that decays over time.
    - Arbitration weight equals the probability the current state is in WM: w = min(1, K_eff/set_size) * activation_s.
    - Exploration (epsilon) increases with set size and for older adults.

    Parameters (model_parameters):
    - alpha: RL learning rate in [0,1].
    - beta: inverse temperature (>0) for RL softmax.
    - K_base: baseline WM capacity (in items), >0.
    - decay_rate: per-trial decay rate of WM activation in [0,1].
    - eps_base: baseline exploration rate in [0,1].

    Inputs:
    - states, actions, rewards, blocks, set_sizes: trial-wise arrays (see task description).
    - age: array-like with a single entry, participant age in years.
    - model_parameters: [alpha, beta, K_base, decay_rate, eps_base].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha, beta, K_base, decay_rate, eps_base = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])

        # RL Q-table
        Q = (1.0 / nA) * np.ones((nS, nA))
        # WM policy table (rows sum to 1 initially) and activations
        WM = (1.0 / nA) * np.ones((nS, nA))
        act = np.zeros(nS)  # activation 0..1; 0 means not held, 1 means freshly stored

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Effective capacity: reduced for older adults
            K_eff = K_base * (1.0 - 0.4 * is_older)
            # Probability that current state is in WM given capacity and current activation
            cap_prob = min(1.0, max(0.0, K_eff / float(curr_set)))
            # Activation decays with load and age
            decay_eff = decay_rate * (float(curr_set) / 3.0) * (0.6 + 0.4 * is_older)
            decay_eff = min(max(decay_eff, 0.0), 1.0)
            # Update activations globally each trial (interference)
            act = (1.0 - decay_eff) * act

            w = cap_prob * act[s]
            w = min(max(w, 0.0), 1.0)

            # RL policy
            q_s = Q[s, :]
            logits = beta * (q_s - np.max(q_s))
            p_rl = np.exp(logits)
            p_rl /= np.sum(p_rl)

            # WM policy for this state (already normalized)
            p_wm = WM[s, :]
            p_wm = p_wm / np.sum(p_wm)

            # Exploration increases with load and age
            epsilon = eps_base * (float(curr_set) / 3.0) * (0.7 + 0.6 * is_older)
            epsilon = min(max(epsilon, 0.0), 0.4)

            # Arbitration and final policy
            p_mix = w * p_wm + (1.0 - w) * p_rl
            p = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            # NLL
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM write: if rewarded, write one-hot; else leave as is
            if r > 0.5:
                WM[s, :] = 0.0
                WM[s, a] = 1.0
                act[s] = 1.0  # freshly encoded

    return nll


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates and meta-control of decision temperature via surprise,
    modulated by age and load.

    Idea:
    - Standard Q-learning with separate learning rates for positive and negative prediction errors.
    - The softmax inverse temperature beta_t adapts over time based on surprise |r - p(a)|:
      beta increases when surprise is below a target (commitment), decreases when above (explore).
    - The surprise target depends on set size (higher target under higher load) and age
      (older adults tolerate higher surprise before committing).
    - This captures load- and age-dependent exploration-exploitation control without extra noise terms.

    Parameters (model_parameters):
    - alpha_pos: learning rate for positive PE (r - Q) > 0, in [0,1].
    - alpha_neg: learning rate for negative PE (r - Q) < 0, in [0,1].
    - beta_init: initial inverse temperature (>0) at the start of each block.
    - k_meta: meta-learning rate (>0) controlling how strongly beta adapts to surprise.
    - target_base: baseline surprise target in [0,1].

    Inputs:
    - states, actions, rewards, blocks, set_sizes: trial-wise arrays.
    - age: array-like with a single entry, participant age in years.
    - model_parameters: [alpha_pos, alpha_neg, beta_init, k_meta, target_base].

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    alpha_pos, alpha_neg, beta_init, k_meta, target_base = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45 else 0.0

    nll = 0.0
    nA = 3
    eps = 1e-12

    for b in np.unique(blocks):
        idx = (blocks == b)
        block_actions = actions[idx]
        block_rewards = rewards[idx]
        block_states = states[idx]
        block_set_sizes = set_sizes[idx]

        nS = int(block_set_sizes[0])
        Q = (1.0 / nA) * np.ones((nS, nA))
        beta_t = max(beta_init * (1.0 - 0.2 * is_older), eps)  # slightly lower initial beta for older adults

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            curr_set = int(block_set_sizes[t])

            # Softmax with current beta_t
            q_s = Q[s, :]
            logits = beta_t * (q_s - np.max(q_s))
            p = np.exp(logits)
            p /= np.sum(p)

            # NLL
            p_choice = max(p[a], eps)
            nll -= np.log(p_choice)

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            if pe >= 0.0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # Surprise of chosen action and target depending on load and age
            surprise = abs(r - p[a])  # in [0,1]
            target = target_base * (float(curr_set) / 3.0) * (0.7 + 0.6 * is_older)
            target = min(max(target, 0.0), 1.0)

            # Meta-update of beta: push toward lower surprise than target -> higher beta (exploit)
            # Scale meta-rate down slightly for older adults (slower adaptation)
            k_eff = k_meta * (1.0 - 0.3 * is_older)
            beta_t = beta_t + k_eff * (target - surprise)
            beta_t = max(beta_t, eps)  # keep positive

    return nll