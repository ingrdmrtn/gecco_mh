def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with decaying Q-values and size-sensitive WM sharpening.

    Mechanism:
    - RL: standard delta rule with per-trial decay of Q toward uniform (forgetting).
    - WM: fast, reward-gated update toward a one-hot action code, with an update gain that
      scales with set size (rehearsal_gain). WM policy is sharpened more in small set sizes.
    - Arbitration: fixed base mixture (wm_weight0) down-weighted in larger set sizes (3/nS).

    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight0 : float
            Base mixture weight on WM (0-1), reduced in larger set sizes.
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        q_decay : float
            Per-trial RL forgetting rate toward uniform (0-1).
        rehearsal_gain : float
            WM learning gain (0-1) that is scaled by 3/nS on each rewarded trial.
        wm_confidence_temp : float
            Scales WM policy sharpening; effective WM beta = 50 * (1 + wm_confidence_temp * 3/nS).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, q_decay, rehearsal_gain, wm_confidence_temp = model_parameters
    softmax_beta *= 10.0  # RL temperature scaling per template
    softmax_beta_wm = 50.0  # base WM sharpness per template
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))  # RL values
        w = (1.0 / nA) * np.ones((nS, nA))  # WM weights
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # WM baseline

        # Set-size adjusted mixture weight and WM sharpening
        base_w = np.clip(wm_weight0 * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
        beta_wm_eff = softmax_beta_wm * (1.0 + wm_confidence_temp * (3.0 / max(3.0, float(nS))))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy (softmax probability of chosen action)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (sharpened in smaller set sizes)
            p_wm_core = 1.0 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))
            p_wm = p_wm_core  # no extra lapse here; sharpening already size-dependent

            # Mixture
            eff_w = base_w
            p_total = eff_w * p_wm + (1.0 - eff_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting
            delta = r - Q_s[a]
            q[s, a] += lr * delta
            # decay toward uniform after each trial
            q = (1.0 - q_decay) * q + q_decay * (1.0 / nA)

            # WM update: reward-gated, size-scaled gain
            if r > 0.0:
                eta_wm = np.clip(rehearsal_gain * (3.0 / max(3.0, float(nS))), 0.0, 1.0)
                # move the entire row toward one-hot on chosen action
                w[s, :] = (1.0 - eta_wm) * w[s, :]
                w[s, a] += eta_wm
                # renormalize to keep a proper distribution
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with uncertainty bonus and WM one-shot learning with size-dependent decay.

    Mechanism:
    - RL: delta rule plus a directed exploration bonus added to Q during choice:
      bonus(s,a) = explore_bonus / sqrt(N(s,a)+1). This increases choice probabilities for
      less-visited actions and is folded into the RL softmax.
    - WM: one-shot mapping after reward (set near one-hot on the rewarded action) with
      decay toward uniform between trials; decay increases with set size.
    - Arbitration: mixture weight scales with current WM certainty for the state and with
      3/nS (WM less trusted at larger set sizes).

    Parameters
    ----------
    model_parameters : tuple/list of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight0 : float
            Base WM weight (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        explore_bonus : float
            Magnitude of RL directed exploration bonus (>0 recommended).
        wm_decay : float
            Baseline WM decay toward uniform per trial (0-1), amplified by set size.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, explore_bonus, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # RL visit counts for directed exploration

        # size-dependent WM decay (more decay at larger set sizes)
        wm_decay_eff = 1.0 - (1.0 - wm_decay) * (3.0 / max(3.0, float(nS)))
        wm_decay_eff = np.clip(wm_decay_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply between-trial WM decay toward uniform
            w = (1.0 - wm_decay_eff) * w + wm_decay_eff * w_0

            Q_s = q[s, :]

            # RL policy with exploration bonus
            bonus = explore_bonus / np.sqrt(N[s, :] + 1.0)
            Q_aug = Q_s + bonus
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_aug - Q_aug[a])))

            # WM policy (deterministic readout of current WM row)
            W_s = w[s, :]
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # mixture weight scales with WM certainty (how peaked W_s is) and set size
            certainty = np.max(W_s) - (1.0 / nA)
            eff_w = np.clip(wm_weight0 * certainty * (3.0 / max(3.0, float(nS))), 0.0, 1.0)

            p_total = eff_w * p_wm_core + (1.0 - eff_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL updates
            N[s, a] += 1.0
            q[s, a] += lr * (r - q[s, a])

            # WM one-shot learning on reward, mild corrective on non-reward
            if r > 0.0:
                eta_pos = 1.0  # one-shot capture into WM
                w[s, :] = (1.0 - eta_pos) * w[s, :]
                w[s, a] += eta_pos
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                # small push away from the chosen action on errors
                eta_neg = 0.1
                mass = min(w[s, a] - eps, eta_neg * w[s, a])
                w[s, a] -= mass
                redistribute = mass / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with logistic arbitration driven by set size and WM confidence; WM lapse increases with set size.

    Mechanism:
    - RL: standard delta rule.
    - WM: fast learning with its own learning rate; small unlearning on errors; no between-trial global decay.
    - Arbitration: trial-wise mixing weight is a logistic transform of:
        arbit_bias + size_slope*(3 - nS) + conf_term,
      where conf_term measures WM confidence for the current state (inverse entropy).
    - WM policy includes a size-dependent lapse that mixes WM with uniform choice.

    Parameters
    ----------
    model_parameters : tuple/list of length 6
        lr : float
            RL learning rate (0-1).
        wm_lr : float
            WM learning rate toward the chosen action when rewarded (0-1).
        softmax_beta : float
            RL inverse temperature (scaled by 10 internally).
        arbit_bias : float
            Bias term in the logistic arbitration (can be negative or positive).
        size_slope : float
            Weight on (3 - nS) in arbitration; positive values favor WM in small set sizes.
        lapse_base : float
            Base WM lapse that increases with set size (0-1).

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices.
    """
    lr, wm_lr, softmax_beta, arbit_bias, size_slope, lapse_base = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    eps = 1e-12

    def inv_entropy_conf(p_row):
        # Confidence proxy: 1 - normalized entropy in [0,1]
        p = np.clip(p_row, eps, 1.0)
        H = -np.sum(p * np.log(p))
        H_max = np.log(len(p))
        return np.clip(1.0 - H / H_max, 0.0, 1.0)

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Size-dependent lapse grows with set size
        size_factor = (float(nS) - 3.0) / max(1.0, 6.0 - 3.0)
        lapse = np.clip(lapse_base * np.clip(size_factor, 0.0, 1.0), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with lapse
            p_wm_core = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1.0 - lapse) * p_wm_core + lapse * (1.0 / nA)

            # Arbitration via logistic function
            conf = inv_entropy_conf(W_s)
            lin = arbit_bias + size_slope * (3.0 - float(nS)) + conf
            eff_w = 1.0 / (1.0 + np.exp(-lin))
            eff_w = np.clip(eff_w, 0.0, 1.0)

            p_total = eff_w * p_wm + (1.0 - eff_w) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * (r - q[s, a])

            # WM update: reward-driven learning with its own rate; mild forgetting on error
            if r > 0.0:
                eta = np.clip(wm_lr, 0.0, 1.0)
                w[s, :] = (1.0 - eta) * w[s, :]
                w[s, a] += eta
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])
            else:
                # small correction away from chosen action on errors
                eta_err = 0.1 * np.clip(wm_lr, 0.0, 1.0)
                take = min(w[s, a] - eps, eta_err * w[s, a])
                w[s, a] -= take
                add = take / (nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += add
                w[s, :] = np.clip(w[s, :], eps, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p