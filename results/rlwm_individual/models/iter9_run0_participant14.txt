def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Certainty-weighted RL+WM with load-penalized arbitration and load-driven WM decay.

    Idea:
    - Model-free RL with a single learning rate and softmax action selection.
    - A WM store that tracks action weights per state and is used through a high-precision softmax.
    - Arbitration favors the more certain system (lower entropy), but is penalized by load (set size).
    - WM decays toward uniform each trial; decay increases with set size (load).

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate.
    - wm_weight: [0,1] baseline arbitration weight favoring WM (used as a base logit).
    - softmax_beta: >=0 RL inverse temperature (internally scaled by 10).
    - entropy_gain: >=0 sensitivity of arbitration to relative certainty (RL vs WM entropy).
    - decay_load: >=0 scales how strongly larger set sizes accelerate WM decay per trial.
    - wm_suppress: [0,1] additional WM decay applied after negative outcomes (error-driven suppression).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, entropy_gain, decay_load, wm_suppress = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent baseline decay per trial
        load_term = max(0.0, (nS - 3))
        base_decay = np.clip(decay_load * (load_term / max(1.0, nS)), 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Per-trial WM decay toward uniform (stronger with load)
            w = (1.0 - base_decay) * w + base_decay * w_0

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            q_shift = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * q_shift)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            # WM policy (high precision)
            w_shift = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * w_shift)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = pi_wm[a]

            # Arbitration by relative certainty (lower entropy -> higher weight) penalized by load
            eps = 1e-12
            H_rl = -np.sum(pi_rl * np.log(pi_rl + eps))
            H_wm = -np.sum(pi_wm * np.log(pi_wm + eps))

            # Convert baseline wm_weight in [0,1] to logit
            wm_logit_base = np.log((wm_weight + 1e-6) / (1 - wm_weight + 1e-6))
            wm_logit = wm_logit_base + entropy_gain * (H_rl - H_wm) - decay_load * load_term
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update
            delta = r - q[s][a]
            q[s][a] += lr * delta

            # WM update:
            # - On reward: sharpen toward a peaked distribution (one-shot binding).
            # - On no reward: suppress the chosen action and diffuse toward uniform, scaled by wm_suppress.
            if r > 0:
                target = w_0[s, :].copy()
                target[a] = 1.0
                target /= np.sum(target)
                # Use the dynamic arbitration weight to determine WM write strength
                eta = np.clip(wm_weight_dyn, 0.0, 1.0)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target
            else:
                # Error-driven suppression of chosen action and diffusion to uniform
                eta = np.clip(wm_suppress * (0.5 + 0.5 * wm_weight_dyn), 0.0, 1.0)
                suppress = w[s, :].copy()
                suppress[a] = 0.0
                if suppress.sum() > 0:
                    suppress = suppress / suppress.sum()
                else:
                    suppress = w_0[s, :].copy()
                # Interpolate first toward uniform (diffusion), then re-normalize via convex combo with suppress
                w[s, :] = (1.0 - eta) * w[s, :] + eta * (0.5 * w_0[s, :] + 0.5 * suppress)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Reward-rate-gated RL+WM with error-driven WM reset and load-sensitive gating.

    Idea:
    - RL with a single learning rate; negative errors scaled by a 'neg_scale' factor.
    - WM stores recent action preferences per state; on errors, WM for that state partially resets.
    - Arbitration weight depends on a leaky reward-rate estimate (higher -> more WM), but is reduced by load.
    - Load sensitivity via a logistic penalty centered at a 'load_mid' set size.

    Parameters (model_parameters):
    - lr: [0,1] RL learning rate (applied to positive errors).
    - wm_weight: [0,1] baseline arbitration weight (used as a base logit).
    - softmax_beta: >=0 RL inverse temperature (internally scaled by 10).
    - neg_scale: >=0 scales RL learning from negative prediction errors (effective lr_neg = lr * neg_scale).
    - arb_tau: (0,1] reward-rate integration timescale; smaller -> faster to adapt.
    - load_mid: >0 midpoint of the logistic load penalty; larger -> weaker penalty for nS near 3.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, neg_scale, arb_tau, load_mid = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Initialize leaky reward-rate estimate
        rew_rate = 0.5

        # Precompute load penalty via logistic
        load_term = float(nS)
        load_penalty = 1.0 / (1.0 + np.exp(-(load_term - load_mid)))  # in (0,1)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # RL policy
            q_shift = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * q_shift)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            # WM policy
            w_shift = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * w_shift)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = pi_wm[a]

            # Arbitration: base logit + reward-rate drive - load penalty
            wm_logit_base = np.log((wm_weight + 1e-6) / (1 - wm_weight + 1e-6))
            wm_logit = wm_logit_base + np.log((rew_rate + 1e-6) / (1 - rew_rate + 1e-6)) - 3.0 * load_penalty
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update with asymmetric scaling for negative errors
            pe = r - q[s][a]
            lr_eff = lr if pe >= 0 else lr * neg_scale
            q[s][a] += lr_eff * pe

            # WM update:
            # - If rewarded: sharpen to a peaked distribution on chosen action.
            # - If not rewarded: partially reset WM for this state toward uniform (error-driven reset).
            if r > 0:
                target = w_0[s, :].copy()
                target[a] = 1.0
                target /= np.sum(target)
                eta_wm = np.clip(wm_weight_dyn, 0.0, 1.0)  # trust WM more when we plan to rely on it
                w[s, :] = (1.0 - eta_wm) * w[s, :] + eta_wm * target
            else:
                reset_strength = np.clip(0.5 + 0.5 * wm_weight_dyn, 0.0, 1.0)
                w[s, :] = (1.0 - reset_strength) * w[s, :] + reset_strength * w_0[s, :]

            # Update leaky reward-rate
            rew_rate = (1.0 - arb_tau) * r + arb_tau * rew_rate

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    WM-confidence-modulated RL with state-ageing WM and load penalty.

    Idea:
    - RL uses a learning rate modulated by WM confidence (max probability in WM for the state).
    - WM is updated rapidly on reward and mildly on non-reward; WM confidence decays with
      state-specific 'age' since last visit and with set size (load).
    - Arbitration weight increases with WM confidence but is penalized by load.

    Parameters (model_parameters):
    - lr: [0,1] base RL learning rate.
    - wm_weight: [0,1] baseline arbitration weight (used as a base logit).
    - softmax_beta: >=0 RL inverse temperature (internally scaled by 10).
    - wm_learn: [0,1] WM encoding strength on reward; also scales mild anti-update on non-reward.
    - age_decay: >=0 controls how fast state-specific age reduces WM confidence (higher -> faster decay).
    - load_penalty: >=0 how strongly larger set sizes reduce both WM confidence and arbitration.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_learn, age_decay, load_penalty = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track state-specific age since last visit to modulate WM confidence
        state_age = np.zeros(nS)

        log_p = 0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Age all states by 1 and reset current state's age to 0 after using it
            state_age += 1.0

            # Compute WM confidence for current state from its distribution max, reduced by age and load
            W_s = w[s, :].copy()
            wm_conf = np.max(W_s)
            # Age and load reduce effective WM confidence
            age_factor = np.exp(-age_decay * state_age[s])
            load_factor = 1.0 / (1.0 + load_penalty * max(0.0, nS - 3))
            wm_conf_eff = np.clip(wm_conf * age_factor * load_factor, 0.0, 1.0)

            Q_s = q[s, :].copy()

            # RL and WM policies
            q_shift = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * q_shift)
            pi_rl = pi_rl / np.sum(pi_rl)
            p_rl = pi_rl[a]

            w_shift = W_s - np.max(W_s)
            pi_wm = np.exp(softmax_beta_wm * w_shift)
            pi_wm = pi_wm / np.sum(pi_wm)
            p_wm = pi_wm[a]

            # Arbitration: baseline + WM confidence (promote WM) - load penalty
            wm_logit_base = np.log((wm_weight + 1e-6) / (1 - wm_weight + 1e-6))
            wm_logit = wm_logit_base + np.log((wm_conf_eff + 1e-6) / (1 - wm_conf_eff + 1e-6)) - load_penalty * max(0.0, nS - 3)
            wm_weight_dyn = 1.0 / (1.0 + np.exp(-wm_logit))

            p_total = wm_weight_dyn * p_wm + (1.0 - wm_weight_dyn) * p_rl
            log_p += np.log(p_total + 1e-12)

            # RL update with WM-confidence-modulated learning rate
            pe = r - q[s][a]
            lr_eff = np.clip(lr * (0.5 + 0.5 * wm_conf_eff), 0.0, 1.0)
            q[s][a] += lr_eff * pe

            # WM update:
            # Reward -> strong binding to chosen action; No reward -> mild diffusion away from chosen action.
            if r > 0:
                target = w_0[s, :].copy()
                target[a] = 1.0
                target /= np.sum(target)
                eta = np.clip(wm_learn * (0.5 + 0.5 * wm_conf_eff), 0.0, 1.0)
                w[s, :] = (1.0 - eta) * w[s, :] + eta * target
            else:
                eta = np.clip(0.25 * wm_learn * (1.0 - wm_conf_eff), 0.0, 1.0)
                spread = w[s, :].copy()
                spread[a] = 0.0
                if spread.sum() > 0:
                    spread = spread / spread.sum()
                else:
                    spread = w_0[s, :].copy()
                w[s, :] = (1.0 - eta) * w[s, :] + eta * (0.5 * w_0[s, :] + 0.5 * spread)

            # After updating, reset current state's age
            state_age[s] = 0.0

        blocks_log_p += log_p

    return -blocks_log_p