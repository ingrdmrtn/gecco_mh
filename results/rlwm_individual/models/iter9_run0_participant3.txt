def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with adaptive temperature and load-sensitive WM precision.

    Mechanism
    - RL: Q-learning with delta rule. The RL inverse temperature adapts online based on
      recent unsigned prediction error magnitude: higher recent surprise lowers beta
      (more exploration).
    - WM: fast table supporting near-deterministic retrieval, but WM precision (beta_wm)
      decreases with set size (load). WM contributes via a mixture weight wm_weight.
    - Arbitration: fixed wm_weight mixture (but effective stochasticity of WM depends on load).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta0, beta_adapt_rate, wm_beta0, wm_beta_load_slope)
        - lr: RL learning rate in [0,1].
        - wm_weight: mixture weight for WM in [0,1].
        - softmax_beta0: baseline RL inverse temperature (>0); internally scaled by 10.
        - beta_adapt_rate: rate in [0,1] controlling how quickly recent unsigned PE
          (surprise) updates the beta adaptation signal.
        - wm_beta0: baseline WM inverse temperature for small set size.
        - wm_beta_load_slope: slope (>0) that reduces WM precision as set size increases.
          Effective beta_wm_t = max(1, wm_beta0 - wm_beta_load_slope*(nS-3)).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, beta_adapt_rate, wm_beta0, wm_beta_load_slope = model_parameters
    softmax_beta *= 10  # baseline scaling
    softmax_beta_wm = 50  # will be overridden per trial
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # running estimate of recent unsigned PE (surprise)
        surpr = 0.0

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # adapt RL beta using recent surprise (higher surpr -> lower beta)
            beta_eff = softmax_beta / (1.0 + surpr + 1e-8)
            softmax_beta = beta_eff  # inject trial-specific beta into RL policy

            # load-sensitive WM precision
            softmax_beta_wm = max(1.0, wm_beta0 - wm_beta_load_slope * (nS - 3))

            Q_s = q[s,:]
            W_s = w[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy: softmax with load-adjusted precision
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            p_vec_wm = np.exp(wm_logits) / np.sum(np.exp(wm_logits))
            p_wm = p_vec_wm[a]

            p_total = p_wm*wm_weight + (1-wm_weight)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Update recent surprise tracker
            surpr = (1.0 - beta_adapt_rate) * surpr + beta_adapt_rate * abs(delta)

            # WM update: rewarded trials push toward one-hot; otherwise slight decay to uniform
            one_hot = np.zeros(nA); one_hot[a] = 1.0
            if r > 0.5:
                w[s,:] = 0.5 * w[s,:] + 0.5 * one_hot
            else:
                w[s,:] = 0.9 * w[s,:] + 0.1 * w_0[s,:]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with time-based decay and set-size interference on WM recall, plus load-dependent lapse.

    Mechanism
    - RL: standard Q-learning with softmax.
    - WM: stores the last rewarded action per state (one-hot), but recall probability
      decays with time since last rewarded encoding and with set size (interference).
    - Arbitration: effective WM mixture weight equals the current recall probability
      further reduced by a load-dependent lapse that mixes in uniform choices.

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight0, softmax_beta, decay_time, setsize_interf, lapse_base)
        - lr: RL learning rate in [0,1].
        - wm_weight0: base WM reliance in [0,1] scaling recall probability.
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - decay_time: time-decay rate (>0) for WM recall with elapsed trials since last rewarded encoding.
        - setsize_interf: interference rate (>0) that reduces WM recall with larger set size.
        - lapse_base: scales a load-dependent lapse; lapse(nS) = clamp(lapse_base*(nS-3)/3, 0, 0.45).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, decay_time, setsize_interf, lapse_base = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # deterministic WM when recalled
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Track last rewarded-encoding time for each state; initialize as None (-1)
        last_reward_time = -1 * np.ones(nS, dtype=int)
        trial_counter = 0

        # precompute load-dependent lapse
        lapse = np.clip(lapse_base * max(0, (nS - 3)) / 3.0, 0.0, 0.45)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s,:]
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM recall probability depends on time since last rewarded encoding and set size
            if last_reward_time[s] >= 0:
                lag = max(0, trial_counter - last_reward_time[s])
            else:
                lag = 1e9  # essentially no recall before first rewarded encoding

            p_recall_time = np.exp(-decay_time * lag)
            p_recall_load = np.exp(-setsize_interf * max(0, (nS - 3)))
            wm_recall_prob = wm_weight0 * p_recall_time * p_recall_load

            # WM policy: if recalled, near-deterministic from w; else uniform
            W_s = w[s,:]
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            p_vec_wm_det = np.exp(wm_logits) / np.sum(np.exp(wm_logits))
            p_vec_wm = wm_recall_prob * p_vec_wm_det + (1 - wm_recall_prob) * (1.0 / nA) * np.ones(nA)
            p_wm = p_vec_wm[a]

            # Combine with RL; add load-dependent lapse toward uniform via lowering WM contribution
            wm_eff = wm_recall_prob * (1.0 - lapse)
            p_total = p_wm*wm_eff + (1-wm_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: encode on reward; gentle decay otherwise
            one_hot = np.zeros(nA); one_hot[a] = 1.0
            if r > 0.5:
                w[s,:] = 0.0 * w[s,:] + 1.0 * one_hot  # overwrite with strong encoding
                last_reward_time[s] = trial_counter
            else:
                # small passive decay toward uniform when no reward
                w[s,:] = 0.95 * w[s,:] + 0.05 * w_0[s,:]

            trial_counter += 1

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with conflict-gated arbitration, passive WM decay, and asymmetric WM learning.

    Mechanism
    - RL: Q-learning with softmax; we compute the full RL policy per state.
    - WM: fast table with passive decay toward uniform each trial; phasic WM learning
      is stronger on rewarded outcomes than on non-rewarded outcomes (pos/neg asymmetry).
    - Arbitration: mixture weight for WM decreases with policy conflict between WM and RL
      (measured via symmetrical KL divergence) and with set size (scaled conflict penalty).

    Parameters
    ----------
    model_parameters : tuple
        (lr, wm_weight, softmax_beta, conflict_sensitivity, wm_decay, wm_posneg_ratio)
        - lr: RL learning rate in [0,1].
        - wm_weight: baseline WM mixture weight in [0,1].
        - softmax_beta: RL inverse temperature (>0); internally scaled by 10.
        - conflict_sensitivity: scales how much policy conflict reduces WM reliance (>=0).
        - wm_decay: passive decay rate per trial toward uniform for WM in [0,1].
        - wm_posneg_ratio: multiplicative factor (>0) making WM learning stronger on
          rewarded trials than on non-rewarded trials. Positive rate = base_kappa*wm_posneg_ratio,
          negative rate = base_kappa, where base_kappa = 0.5*wm_weight.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, conflict_sensitivity, wm_decay, wm_posneg_ratio = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy (full vector)
            rl_logits = softmax_beta * (Q_s - np.max(Q_s))
            p_vec_rl = np.exp(rl_logits) / np.sum(np.exp(rl_logits))
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy (full vector)
            wm_logits = softmax_beta_wm * (W_s - np.max(W_s))
            p_vec_wm = np.exp(wm_logits) / np.sum(np.exp(wm_logits))
            p_wm = p_vec_wm[a]

            # Conflict measure: symmetric KL divergence
            eps = 1e-12
            P = np.clip(p_vec_wm, eps, 1.0)
            Q = np.clip(p_vec_rl, eps, 1.0)
            kl_wm_rl = np.sum(P * (np.log(P) - np.log(Q)))
            kl_rl_wm = np.sum(Q * (np.log(Q) - np.log(P)))
            conflict = 0.5 * (kl_wm_rl + kl_rl_wm)

            # Load-scaled conflict gating of WM reliance
            wm_eff = wm_weight / (1.0 + conflict_sensitivity * conflict * (nS / 3.0))
            wm_eff = np.clip(wm_eff, 0.0, 1.0)

            p_total = p_wm*wm_eff + (1-wm_eff)*p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM passive decay toward uniform
            w[s,:] = (1.0 - wm_decay) * w[s,:] + wm_decay * w_0[s,:]

            # WM phasic update with asymmetry
            base_kappa = 0.5 * wm_weight
            one_hot = np.zeros(nA); one_hot[a] = 1.0
            if r > 0.5:
                k_pos = base_kappa * wm_posneg_ratio
                w[s,:] = (1.0 - k_pos) * w[s,:] + k_pos * one_hot
            else:
                k_neg = base_kappa
                w[s,:] = (1.0 - k_neg) * w[s,:] + k_neg * w_0[s,:]

        blocks_log_p += log_p

    return -blocks_log_p