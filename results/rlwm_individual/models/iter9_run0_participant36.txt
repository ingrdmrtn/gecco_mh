def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with interference- and recency-gated arbitration.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy.
    - WM system: associative cache (per-state action preferences) that is sharpened by reward,
      globally decays toward uniform due to interference, and is up-weighted on repeated states.
    - Arbitration: WM weight decreases with load (set size) and increases when the same state
      repeats (recency facilitation). A small lapse adds uniform noise.

    Parameters (model_parameters):
    - lr: RL learning rate for Q-values (0..1).
    - wm_base: Baseline WM mixture weight at low load (0..1).
    - softmax_beta: Inverse temperature for RL softmax (scaled by 10 internally).
    - interference_rate: Global decay rate of WM content per trial, amplified by load (>=0).
    - revisit_boost: Extra WM weight added when current state equals the previous state (>=0).
    - lapse: Lapse probability mixing in a uniform random choice (0..0.5).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_base, softmax_beta, interference_rate, revisit_boost, lapse = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        prev_s = -1
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM preferences (near-deterministic)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: down-weight WM with load, up-weight if state repeats
            load_pen = max(0, nS - 3)
            wm_weight_t = wm_base * np.exp(-interference_rate * load_pen)
            if prev_s == s:
                wm_weight_t += revisit_boost
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Mixture with lapse
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = (1.0 - lapse) * p_total + lapse * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward sharpens chosen action; no-reward mildly suppresses chosen
            # Global interference/decay toward uniform is applied each trial
            # Apply global decay toward w_0 scaled by load
            decay = 1.0 - np.clip(interference_rate * (1 + 0.5 * load_pen), 0.0, 1.0)
            w = decay * w + (1.0 - decay) * w_0

            if r > 0.0:
                # Move W_s toward a one-hot vector on a
                target = np.zeros(nA)
                target[a] = 1.0
                # Learning rate for WM depends on how confident WM already is (bigger when uncertain)
                wm_eta = 0.5 + 0.5 * (1.0 - np.max(W_s))
                w[s, :] = (1 - wm_eta) * W_s + wm_eta * target
            else:
                # Penalize chosen action slightly; renormalize
                penal = 0.1
                W_new = W_s.copy()
                W_new[a] = max(0.0, W_new[a] - penal * W_new[a])
                # Re-normalize to sum to 1 (keep within simplex)
                sumW = np.sum(W_new)
                if sumW <= 1e-8:
                    W_new = w_0[s, :].copy()
                else:
                    W_new = W_new / sumW
                w[s, :] = W_new

            prev_s = s

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Capacity-limited WM buffer with recency and noise.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy.
    - WM system: a limited-capacity buffer storing reliable state->action bindings when rewarded.
      Membership is determined by per-state memory strength that undergoes recency decay.
      If the current state is in WM, a near-deterministic WM policy is used with small noise;
      otherwise choices rely more on RL.
    - Arbitration: base WM weight scaled by fraction of states within capacity; effective WM usage
      shrinks as set size exceeds capacity.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight when state is in buffer (0..1).
    - softmax_beta: RL softmax inverse temperature (scaled by 10 internally).
    - capacity_C: WM capacity in number of states (>=1).
    - recency_decay: Per-trial decay of memory strength (0..1).
    - wm_noise: Probability of random action when using WM (0..0.5).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, capacity_C, recency_decay, wm_noise = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM preferences for each state
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Per-state memory strength controlling buffer membership
        m_strength = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Decay memory strengths (recency)
            m_strength *= (1.0 - np.clip(recency_decay, 0.0, 1.0))

            # Determine buffer membership: top-K states by strength
            C = int(max(1, round(capacity_C)))
            C = min(C, nS)
            # Compute rank threshold
            if C < nS:
                # find threshold for top-C
                thresh = np.partition(m_strength, -C)[-C]
                in_buffer = (m_strength >= thresh) & (m_strength > 0)
            else:
                in_buffer = m_strength > 0

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: if state in buffer, use WM softmax with small noise; else fallback uniform
            if in_buffer[s]:
                p_wm_det = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = (1.0 - wm_noise) * p_wm_det + wm_noise * (1.0 / nA)
                wm_available = 1.0
            else:
                # WM unavailable for this state; policy defaults to uniform
                p_wm = 1.0 / nA
                wm_available = 0.0

            # Arbitration scales with whether WM is available and load relative to capacity
            load_factor = min(1.0, C / max(1.0, float(nS)))
            wm_weight_t = wm_weight * wm_available * load_factor
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updates:
            # - If rewarded, strengthen binding for this state and move w[s] toward one-hot
            # - If not rewarded, reduce confidence slightly toward uniform
            if r > 0.0:
                # Increase memory strength and sharpen preference
                m_strength[s] += 1.0
                target = np.zeros(nA)
                target[a] = 1.0
                wm_eta = 0.7
                w[s, :] = (1 - wm_eta) * W_s + wm_eta * target
            else:
                # Mild forgetting toward uniform and slight decrement in strength
                w[s, :] = 0.9 * W_s + 0.1 * w_0[s, :]
                m_strength[s] = max(0.0, m_strength[s] - 0.2)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with learned block-level rule (action chunk) under load-sensitive arbitration.

    Overview:
    - RL system: delta-rule Q-learning with softmax policy.
    - WM system: per-state bindings (w) combined with a block-level "rule" vector g over actions.
      The rule captures a shared action tendency across states (e.g., when many states map to
      the same action), aiding especially in small set sizes.
    - Arbitration: WM usage decreases with load; within WM, the contribution of the rule vs.
      per-state memory is controlled by rule_gain.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight at low load (0..1).
    - softmax_beta: RL softmax inverse temperature (scaled by 10 internally).
    - rule_gain: Mixture weight of global rule vs. per-state WM within WM policy (0..1).
    - rule_learn: Learning rate for updating the global rule vector (0..1).
    - load_drop: Strength of WM down-weighting per extra item above 3 (>=0).

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, rule_gain, rule_learn, load_drop = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Global rule vector initialized uniform
        g = (1.0 / nA) * np.ones(nA)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: combine per-state WM and global rule
            # Preference vector inside WM
            wm_pref = np.clip((1.0 - rule_gain) * W_s + rule_gain * g, 1e-8, None)
            wm_pref /= np.sum(wm_pref)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_pref - wm_pref[a])))

            # Arbitration: WM down-weighted by load
            load_pen = max(0, nS - 3)
            wm_weight_t = wm_weight * np.exp(-load_drop * load_pen)
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM updates
            # Per-state: move toward one-hot on reward; gentle forgetting toward uniform otherwise
            if r > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                wm_eta = 0.6
                w[s, :] = (1 - wm_eta) * W_s + wm_eta * target
            else:
                w[s, :] = 0.9 * W_s + 0.1 * w_0[s, :]

            # Global rule update: reward shifts g toward chosen action; normalize
            g = (1.0 - rule_learn) * g
            g[a] += rule_learn * r
            g = np.clip(g, 1e-8, None)
            g = g / np.sum(g)

        blocks_log_p += log_p

    return -blocks_log_p