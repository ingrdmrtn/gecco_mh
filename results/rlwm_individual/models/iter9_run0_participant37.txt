def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated Working Memory with set-size-modulated gate and WM decay.

    Overview:
    - Choices arise from a mixture of an RL policy and a WM policy.
    - RL updates via delta rule (single learning rate).
    - WM storage is gated by the absolute prediction error (|PE|): when surprising
      outcomes occur, WM encodes the chosen action if rewarded; if not rewarded, it clears.
    - The WM gate becomes stricter and WM decays faster with larger set sizes (higher load).

    Policy:
    - RL: softmax over Q-values with inverse temperature softmax_beta (scaled by 10 internally).
    - WM: softmax over WM weights with high inverse temperature (deterministic recall).
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    Set-size effects:
    - Gate threshold increases with set size (harder to store at larger nS).
    - WM decay toward uniform increases with set size.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Constant WM mixture weight (0..1).
    - softmax_beta: Baseline RL inverse temperature (scaled by 10 internally).
    - pe_gate: Baseline absolute PE threshold for WM gating (>=0).
    - gate_size_gain: Multiplier controlling how gate tightens with set size (>=0).
    - wm_decay: Baseline WM decay toward uniform per trial (0..1). Set-size amplifies this decay.
    """
    lr, wm_weight, softmax_beta, pe_gate, gate_size_gain, wm_decay = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size effects
        # Gate gets stricter with set size (more load -> higher threshold)
        pe_gate_eff = pe_gate * (1.0 + max(0, nS - 3) * gate_size_gain)
        # WM decay amplifies with set size using a compounding form
        wm_decay_eff = 1.0 - (1.0 - np.clip(wm_decay, 0.0, 1.0)) ** (1 + max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update
            # Leak/decay toward uniform (interference)
            w[s, :] = (1.0 - wm_decay_eff) * w[s, :] + wm_decay_eff * w_0[s, :]

            # Gated encoding by surprise
            if abs(pe) >= pe_gate_eff:
                if r > 0.5:
                    # Store rewarded mapping as one-hot
                    w[s, :] = 0.0
                    w[s, a] = 1.0
                else:
                    # Clear/forget on surprising negative outcome
                    w[s, :] = w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Reward-locked WM with set-size-depressed confidence and lapse.

    Overview:
    - RL: standard delta rule with softmax policy.
    - WM: stores the last rewarded action per state; recall has a confidence that
      decreases with set size and a lapse component that injects random choice.

    Policy:
    - RL: softmax over Q-values (beta scaled by 10).
    - WM: softmax over a confidence-weighted WM distribution with lapse:
        p_wm = (1 - lapse) * softmax(beta_wm * W_conf) + lapse * uniform.
    - Mixture: p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl.

    WM dynamics:
    - On reward=1, WM moves toward a one-hot at chosen action with step size = conf_eff.
    - On reward=0, WM drifts toward uniform with step size = (1 - conf_eff)/2.
    - No explicit parameter for WM learning rate; confidence controls both strength
      of recall and strength of encoding/forgetting.

    Set-size effects:
    - WM confidence conf_eff = max(0, wm_conf_base - size_conf_drop * (nS - 3)).
      Larger set sizes reduce confidence (both policy precision and encoding strength).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Constant WM mixture weight (0..1).
    - softmax_beta: Baseline RL inverse temperature (scaled by 10 internally).
    - wm_conf_base: Baseline WM confidence (0..1).
    - size_conf_drop: How much confidence drops per +3 items (>=0).
    - lapse: WM lapse probability mixing with uniform (0..1).
    """
    lr, wm_weight, softmax_beta, wm_conf_base, size_conf_drop, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        conf_eff = max(0.0, wm_conf_base - max(0, nS - 3) * size_conf_drop)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy with confidence shaping and lapse
            # Confidence-weighted content (blend with uniform before softmax)
            W_conf = conf_eff * W_s + (1.0 - conf_eff) * w_0[s, :]
            denom_wm_det = np.sum(np.exp(softmax_beta_wm * (W_conf - W_conf[a])))
            p_wm_det = 1.0 / max(denom_wm_det, 1e-12)
            p_wm = (1.0 - lapse) * p_wm_det + lapse * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update
            if r > 0.5:
                # Hebbian move toward one-hot with step size = conf_eff
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - conf_eff) * w[s, :] + conf_eff * target
            else:
                # On errors, drift toward uniform with a small step
                forget_step = (1.0 - conf_eff) * 0.5
                w[s, :] = (1.0 - forget_step) * w[s, :] + forget_step * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + Uncertainty-based arbitration and WM leak with set-size penalty.

    Overview:
    - RL: standard delta rule with softmax policy.
    - WM: associative store updated Hebbianly on rewards, with leak toward uniform.
    - Arbitration: the mixture weight is adjusted on each trial as a function of the
      relative uncertainty (entropy) of RL vs WM, penalized by set size.

    Policy:
    - RL: softmax over Q-values (beta scaled by 10).
    - WM: softmax over WM weights (high precision).
    - Trial-wise mixture:
        wm_mix_t = sigmoid(wm_weight0 + uncertainty_gain * (H_rl - H_wm) - size_uncertainty_penalty * (nS - 3))
        p_total = wm_mix_t * p_wm + (1 - wm_mix_t) * p_rl

    WM dynamics:
    - Leak toward uniform with rate wm_leak_eff (increasing with set size).
    - On reward=1, Hebbian increment toward one-hot for chosen action with step (1 - wm_leak_eff).

    Set-size effects:
    - Larger nS reduces the WM mixture via the penalty term in the sigmoid.
    - Larger nS increases WM leak via wm_leak_eff = 1 - (1 - wm_leak)^(1 + (nS - 3)).

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight0: Baseline logit for WM mixture (can be negative/positive).
    - softmax_beta: Baseline RL inverse temperature (scaled by 10 internally).
    - uncertainty_gain: Sensitivity to entropy difference H_rl - H_wm (>=0).
    - size_uncertainty_penalty: Set-size penalty on WM mixture (>=0).
    - wm_leak: Baseline WM leak toward uniform per trial (0..1), amplified by set size.
    """
    lr, wm_weight0, softmax_beta, uncertainty_gain, size_uncertainty_penalty, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0

    def softmax_probs(x, beta):
        ex = np.exp(beta * (x - np.max(x)))
        return ex / max(np.sum(ex), 1e-12)

    def entropy_from_values(vals, beta):
        # Convert to softmax distribution, then compute entropy
        p = softmax_probs(vals, beta)
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    for b in np.unique(blocks):
        block_mask = (blocks == b)

        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        wm_leak_eff = 1.0 - (1.0 - np.clip(wm_leak, 0.0, 1.0)) ** (1 + max(0, nS - 3))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action using the provided invariant form
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM policy probability of chosen action
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Trial-wise arbitration using entropies
            H_rl = entropy_from_values(Q_s, softmax_beta)
            H_wm = entropy_from_values(W_s, softmax_beta_wm)
            logit = wm_weight0 + uncertainty_gain * (H_rl - H_wm) - size_uncertainty_penalty * max(0, nS - 3)
            wm_mix_t = 1.0 / (1.0 + np.exp(-logit))

            p_total = wm_mix_t * p_wm + (1.0 - wm_mix_t) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            pe = r - Q_s[a]
            q[s, a] += lr * pe

            # WM update: leak then reward-based Hebbian increment
            w[s, :] = (1.0 - wm_leak_eff) * w[s, :] + wm_leak_eff * w_0[s, :]
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                # Strength of encoding complements the leak
                enc = 1.0 - wm_leak_eff
                w[s, :] = (1.0 - enc) * w[s, :] + enc * target

        blocks_log_p += log_p

    return -blocks_log_p