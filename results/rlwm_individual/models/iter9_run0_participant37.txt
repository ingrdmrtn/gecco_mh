def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + reward-gated WM bindings with load-dependent gating.

    Model idea:
    - RL: standard Q-learning with learning rate lr and softmax choice (beta scaled by 10).
    - WM: a fast associative store that binds the currently chosen action to the state when rewarded.
      On reward, WM for that state moves toward a peaked distribution on the chosen action.
      On no reward, WM for that state partially decays toward uniform.
      A persistence parameter controls how strongly WM updates in either case.
      A PE-based boost increases WM binding when surprising positive outcomes occur.
    - Mixture: WM contribution is down-weighted exponentially with set size (load_beta).

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Baseline WM mixture weight.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled by 10 internally).
    - wm_persist: scalar in [0,1]. Strength/persistence of WM updates per trial.
    - load_beta: scalar >= 0. Controls how quickly WM weight decays with higher set size.
    - pe_boost: scalar >= 0. Multiplier for PE-enhanced WM binding on positive surprise.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_persist, load_beta, pe_boost = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent WM mixture (more load -> less WM)
        wm_weight_eff = wm_weight * np.exp(-load_beta * max(0, nS - 3))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action a
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy probability of chosen action a
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update:
            # - If rewarded: move toward a peaked distribution at chosen action.
            #   The strength is boosted by positive PE magnitude.
            # - If not rewarded: partial decay toward uniform.
            if r > 0.5:
                # Create a target one-hot-like distribution with small mass on others
                target = np.full(nA, 1e-6)
                target[a] = 1.0 - (nA - 1) * 1e-6
                # Positive surprise boosts binding
                pe = max(0.0, delta)
                eta = wm_persist * (1.0 + pe_boost * pe)
                w[s, :] = (1 - eta) * w[s, :] + eta * target
            else:
                # No reward: decay the policy toward uniform (forgetting)
                eta = wm_persist
                w[s, :] = (1 - eta) * w[s, :] + eta * w_0[s, :]

            # Normalize to avoid numerical drift
            w_sum = np.sum(w[s, :])
            if w_sum > 0:
                w[s, :] /= w_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + fast WM value table with asymmetric WM learning and sigmoidal load gating.

    Model idea:
    - RL: standard Q-learning with lr and softmax.
    - WM: a fast value-like table (w) learned with separate learning rates for positive and negative outcomes,
      allowing quick acquisition and forgetting of stimulus-action mappings.
      Only the chosen action is updated toward 1 (if rewarded) or 0 (if not), while non-chosen actions
      softly regress toward 1/nA (neutral prior) to maintain a stochastic policy.
    - Mixture: WM mixture weight is scaled by a sigmoid of set size, decreasing with larger set sizes.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Baseline WM mixture weight before load gating.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled by 10 internally).
    - wm_eta_pos: scalar in [0,1]. WM learning rate after reward.
    - wm_eta_neg: scalar in [0,1]. WM learning rate after no reward.
    - gate_shift: real scalar. Controls where the sigmoidal load gate transitions with set size.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_eta_pos, wm_eta_neg, gate_shift = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Sigmoidal gating as a function of set size: higher nS -> smaller weight
        # gate in (0,1): sigmoid(-(nS - 3) - gate_shift)
        gate_input = -(nS - 3) - gate_shift
        wm_weight_eff = wm_weight / (1.0 + np.exp(-gate_input))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen probability
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen probability
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update (fast asymmetric):
            if r > 0.5:
                # move chosen toward 1
                w[s, a] += wm_eta_pos * (1.0 - w[s, a])
                # regress non-chosen toward neutral 1/nA
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += 0.5 * wm_eta_pos * (w_0[s, aa] - w[s, aa])
            else:
                # move chosen toward 0
                w[s, a] += wm_eta_neg * (0.0 - w[s, a])
                # regress non-chosen toward neutral 1/nA
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += 0.5 * wm_eta_neg * (w_0[s, aa] - w[s, aa])

            # Normalize to maintain a valid policy-like vector
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM win-stay/biased-lose-shift policy with power-law capacity gating.

    Model idea:
    - RL: standard Q-learning with lr and softmax.
    - WM: policy w implements win-stay / biased-lose-shift for each state:
        after reward: place high mass on last chosen action (stay);
        after no reward: distribute probability to other actions with bias ws_bias,
        leaving a smaller mass on the last chosen action (shift).
      A small lapse ensures non-zero probabilities for all actions.
    - Mixture: WM mixture weight is scaled by a power-law of set size, capturing capacity limitations.

    Parameters (model_parameters):
    - lr: scalar in [0,1]. RL learning rate.
    - wm_weight: scalar in [0,1]. Baseline WM mixture weight.
    - softmax_beta: scalar >= 0. RL inverse temperature (scaled by 10 internally).
    - ws_bias: scalar in [0,1]. On no-reward trials, fraction of mass assigned to the best alternative
               (i.e., a single non-chosen action gets extra weight; others share the rest).
    - capacity_gamma: scalar >= 0. Exponent controlling how WM weight decays with set size as (3/nS)^gamma.
    - wm_lapse: scalar in [0, 0.5). Lapse to keep WM policy from being fully deterministic.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, ws_bias, capacity_gamma, wm_lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Power-law capacity gating
        wm_weight_eff = wm_weight * (3.0 / max(3.0, nS)) ** capacity_gamma

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen probability
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM chosen probability
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM win-stay / biased-lose-shift update
            if r > 0.5:
                # Win-stay: concentrate mass on chosen action with lapse
                wm_pref = np.full(nA, wm_lapse / (nA - 1))
                wm_pref[a] = 1.0 - wm_lapse
                w[s, :] = 0.5 * w[s, :] + 0.5 * wm_pref
            else:
                # Lose-shift: bias one alternative (the currently best RL action) with ws_bias
                # Identify best alternative by RL Q (excluding chosen)
                alts = [aa for aa in range(nA) if aa != a]
                best_alt = alts[int(np.argmax(Q_s[alts]))]
                wm_pref = np.zeros(nA)
                # Assign small residual to chosen action (lapse)
                wm_pref[a] = wm_lapse
                # Assign ws_bias to best alternative
                wm_pref[best_alt] = (1.0 - wm_lapse) * ws_bias
                # Remaining mass to the other alternative
                other_alt = [aa for aa in alts if aa != best_alt][0]
                wm_pref[other_alt] = (1.0 - wm_lapse) * (1.0 - ws_bias)
                w[s, :] = 0.5 * w[s, :] + 0.5 * wm_pref

            # Normalize to ensure valid distribution
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p