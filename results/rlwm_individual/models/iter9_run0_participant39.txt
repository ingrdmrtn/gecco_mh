def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with set-size dependent WM retrieval and RL perseveration bias.

    Mechanism
    - RL: Q-learning with softmax action selection. Adds a perseveration bonus to the most recent action.
    - WM: a fast, reward-gated Hebbian store that aims to encode the correct action for each state.
          At decision time, WM is retrieved with probability that depends on set size (higher load -> lower retrieval).
          When WM retrieval fails, WM contributes a uniform policy (a lapse within WM).
    - Mixture: Combine WM and RL policies by wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_retrieval_3: probability of successful WM retrieval at set size 3 (0..1).
        - wm_retrieval_6: probability of successful WM retrieval at set size 6 (0..1).
        - pers: perseveration bonus added to the last chosen action in RL values (in Q units, >=0).

    Set-size effects
    ----------------
    - WM retrieval probability uses wm_retrieval_3 for nS=3 and wm_retrieval_6 for nS=6, capturing load-dependent WM access.
    """
    lr, wm_weight, softmax_beta, wm_retrieval_3, wm_retrieval_6, pers = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # set-size specific WM retrieval probability
        p_retrieval = wm_retrieval_3 if nS == 3 else wm_retrieval_6

        log_p = 0
        last_action = None
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy with perseveration bonus on last chosen action
            Qb = Q_s.copy()
            if last_action is not None:
                Qb[last_action] += pers
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Qb - Qb[a])))

            # WM policy: retrieval succeeds with p_retrieval, else uniform lapse within WM
            p_wm_det = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = p_retrieval * p_wm_det + (1 - p_retrieval) * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-gated Hebbian toward the chosen action (encode only on reward)
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Full overwrite toward one-hot (fast WM storage)
                w[s] = onehot

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with entropy-based arbitration and load-dependent WM decay.

    Mechanism
    - RL: standard Q-learning with softmax.
    - WM: reward-gated Hebbian updates; between trials, WM decays toward uniform at a rate that increases with set size.
    - Arbitration: dynamic wm_weight based on WM certainty (low entropy increases WM influence) and load penalty.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: baseline mixture weight (logit-space intercept for arbitration).
        - softmax_beta: RL inverse temperature; internally scaled by 10.
        - wm_alpha: WM learning rate toward one-hot on rewarded trials (0..1).
        - entropy_temp: gain (>0) converting WM certainty (1 - entropy) into added WM weight.
        - load_bias: load penalty (>0 increases penalty) that reduces WM weight as set size grows.

    Set-size effects
    ----------------
    - WM decay per trial: decay = sigmoid(load_bias * (nS/6)) - 0.5, mapped to [0,0.5]; larger sets decay more.
    - Arbitration weight reduces with set size via the load_bias term; the effective weight is
      wm_weight_eff = sigmoid(wm_weight + entropy_temp*(Hmax - H(W_s)) - load_bias*((nS-3)/3)).
    """
    lr, wm_weight, softmax_beta, wm_alpha, entropy_temp, load_bias = model_parameters
    softmax_beta *= 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-dependent decay mapped to [0, 0.5]
        def sigmoid(x):
            return 1.0 / (1.0 + np.exp(-x))
        wm_decay = sigmoid(load_bias * (float(nS) / 6.0)) - 0.5
        wm_decay = np.clip(wm_decay, 0.0, 0.5)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy (deterministic softmax)
            p_wm_core = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Entropy-based arbitration with load penalty
            eps = 1e-12
            W_norm = np.clip(W_s, eps, 1.0)
            W_norm = W_norm / np.sum(W_norm)
            H = -np.sum(W_norm * np.log(W_norm))  # natural log entropy
            Hmax = np.log(nA)
            wm_weight_eff = sigmoid(wm_weight + entropy_temp * (Hmax - H) - load_bias * ((nS - 3.0) / 3.0))

            p_total = wm_weight_eff * p_wm_core + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: reward-gated Hebbian
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = (1 - wm_alpha) * w[s] + wm_alpha * onehot

            # WM decay toward uniform applied globally each trial, load-dependent
            if wm_decay > 0:
                w = (1 - wm_decay) * w + wm_decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with meta-control of RL temperature by recent performance and load-dependent WM lapse.

    Mechanism
    - RL: Q-learning with an adaptive inverse temperature. Beta increases when recent outcomes are better,
      decreases when they are worse (meta-control of exploration vs exploitation).
    - WM: reward-gated Hebbian store with a set-size-specific lapse that mixes uniform noise into WM policy.
    - Mixture: Combine WM and RL with wm_weight.

    Parameters
    ----------
    model_parameters : tuple
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight of WM vs RL (0..1).
        - softmax_beta0: base RL inverse temperature; internally scaled by 10.
        - beta_meta_gain: gain of beta adaptation to recent success (>=0).
        - wm_lapse_3: WM lapse at set size 3 (0..1), mixed with uniform policy.
        - wm_lapse_6: WM lapse at set size 6 (0..1), mixed with uniform policy.

    Set-size effects
    ----------------
    - WM lapse is higher at larger set size via wm_lapse_6; for nS=3 uses wm_lapse_3.
    """
    lr, wm_weight, softmax_beta0, beta_meta_gain, wm_lapse_3, wm_lapse_6 = model_parameters
    softmax_beta = softmax_beta0 * 10
    softmax_beta_wm = 50
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Choose WM lapse by set size
        wm_lapse = wm_lapse_3 if nS == 3 else wm_lapse_6

        # Running estimate of recent performance (exponentially weighted)
        perf = 0.5  # initialize neutral
        perf_alpha = 0.2  # fixed smoothing inside model (not a free parameter)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # Meta-controlled beta
            beta_eff = max(0.0, softmax_beta + beta_meta_gain * (perf - 0.5) * 10.0)

            # RL policy with adaptive beta
            p_rl = 1 / np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))

            # WM deterministic policy mixed with lapse (uniform)
            p_wm_core = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = (1 - wm_lapse) * p_wm_core + wm_lapse * (1.0 / nA)

            # Mixture
            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update on reward
            if r > 0:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                w[s] = onehot

            # Update performance trace for meta-control
            perf = (1 - perf_alpha) * perf + perf_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p