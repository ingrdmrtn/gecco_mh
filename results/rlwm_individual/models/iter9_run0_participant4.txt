def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with count-based novelty bonus (UCB-like) modulated by age and set size.

    Mechanism:
    - Standard Q-learning within block.
    - A directed exploration bonus is added based on inverse square root of visit counts for each state-action.
      This encourages sampling less-visited actions (novelty/UCB).
    - The overall scale of the bonus is multiplicatively modulated by age group (older vs younger)
      and cognitive load (set size), with larger bonus under lower load and for older adults.
    - Choices follow a softmax over Q + bonus.

    Parameters
    ----------
    states : 1D array-like of int
        State indices on each trial (0..set_size-1 within a block).
    actions : 1D array-like of int
        Chosen action indices on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Binary feedback on each trial.
    blocks : 1D array-like of int
        Block index for each trial (trials from the same block share the same set size and state space).
    set_sizes : 1D array-like of int
        Set size per trial (constant within block; typically 3 or 6).
    age : 1D array-like of float
        Participant age; age[0] used to determine age group (>=45 older, <45 younger).
    model_parameters : tuple/list
        (alpha, beta, novelty_base, age_load_gain, nu)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for softmax over augmented values.
        - novelty_base: base strength of the novelty/UCB bonus.
        - age_load_gain: gain that scales how strongly age and load modulate the bonus.
        - nu: positive stabilizer added inside the sqrt for counts (prevents infinite bonus at zero counts).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, novelty_base, age_load_gain, nu = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize Q-values and visit counts per block
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))

        # Age-load modulation of novelty bonus:
        # - load_factor: higher at small set sizes (1.0 at 3, 0.5 at 6).
        # - age_factor: older adults receive stronger directed exploration.
        load_factor = 3.0 / float(max(1, nS))
        age_factor = 1.0 + 0.5 * is_older
        mod_bonus = novelty_base * (1.0 + age_load_gain * (age_factor * load_factor - 1.0))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Count-based bonus for each action in this state
            bonus_s = mod_bonus / np.sqrt(N[s, :] + max(1e-8, nu))

            V_s = Q[s, :] + bonus_s
            V_centered = V_s - np.max(V_s)
            expV = np.exp(beta * V_centered)
            p_vec = expV / np.sum(expV)
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # Update counts and Q
            N[s, a] += 1.0
            td = r - Q[s, a]
            Q[s, a] += alpha * td

    return -float(total_log_p)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + fast Hebbian mapping with age- and load-gated arbitration.

    Mechanism:
    - Model-free Q-learning runs within block.
    - In parallel, a fast Hebbian mapping M captures one-shot learning of rewarded state-action pairs.
      M decays over time (usage-dependent) and is strengthened only by rewarded choices.
    - Action values used for choice are a convex combination: (1 - w) * Q + w * M at the current state.
      The gate w is a logistic function of a base parameter, with fixed positive age offset and negative load offset:
        w = sigmoid(gate_base + 0.5*is_older - (nS - 3)/3)
      Thus, older adults and smaller set sizes rely more on the Hebbian map.
    - Choices follow a softmax over the combined value.

    Parameters
    ----------
    states : 1D array-like of int
        State indices on each trial (0..set_size-1 within block).
    actions : 1D array-like of int
        Chosen action indices on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Binary feedback on each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size per trial (constant within block; e.g., 3 or 6).
    age : 1D array-like of float
        Participant age; age[0] determines age group (>=45 older, <45 younger).
    model_parameters : tuple/list
        (alpha, beta, hebb_strength, decay_hebb, gate_base)
        - alpha: learning rate for Q-learning (0..1).
        - beta: inverse temperature for the softmax.
        - hebb_strength: learning rate for Hebbian mapping updates after reward (0..1).
        - decay_hebb: decay per encounter applied to Hebbian mapping at the visited state (0..1).
        - gate_base: baseline gating term that is transformed via sigmoid to weight Hebbian vs RL.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha, beta, hebb_strength, decay_hebb, gate_base = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))
        M = np.zeros((nS, nA))  # fast Hebbian map (bounded between 0 and 1 via updates)

        # Age- and load-gated weighting of Hebbian map
        # Older => +0.5 in logit; larger set sizes reduce Hebbian reliance by (nS-3)/3
        logit_w = gate_base + 0.5 * is_older - (max(3, nS) - 3) / 3.0
        w = 1.0 / (1.0 + np.exp(-logit_w))

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            V_s = (1.0 - w) * Q[s, :] + w * M[s, :]
            V_centered = V_s - np.max(V_s)
            expV = np.exp(beta * V_centered)
            p_vec = expV / np.sum(expV)
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # RL update
            td = r - Q[s, a]
            Q[s, a] += alpha * td

            # Hebbian map: decay at visited state, then reinforce if rewarded
            M[s, :] *= (1.0 - np.clip(decay_hebb, 0.0, 1.0))
            if r >= 0.5:
                # Move M[s,a] toward 1 with rate hebb_strength (bounded within [0,1])
                M[s, a] += hebb_strength * (1.0 - M[s, a])

    return -float(total_log_p)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with surprise-driven adaptive learning rate and load-sensitive leak, modulated by age.

    Mechanism:
    - Q-learning within block, but the learning rate is adapted on each trial based on surprise (squared PE).
    - Adaptive rate: alpha_t = sigmoid(alpha0 + k_surprise * PE^2 * m_age_load), ensuring 0..1.
      Older adults and lower load (set size 3) produce larger m_age_load, thus stronger adaptation.
    - Additionally, a per-trial leak toward the uniform prior is applied at the visited state to model
      working-memory limitations under higher load.
    - Choices follow softmax over current Q.

    Parameters
    ----------
    states : 1D array-like of int
        State indices on each trial (0..set_size-1 within block).
    actions : 1D array-like of int
        Chosen action indices on each trial (0..2).
    rewards : 1D array-like of {0,1}
        Binary feedback on each trial.
    blocks : 1D array-like of int
        Block index for each trial.
    set_sizes : 1D array-like of int
        Set size per trial (constant within block).
    age : 1D array-like of float
        Participant age; age[0] determines age group (>=45 older, <45 younger).
    model_parameters : tuple/list
        (alpha0, beta, k_surprise, mod_base, leak)
        - alpha0: baseline logit for the adaptive learning rate (maps via sigmoid to (0,1)).
        - beta: inverse temperature for softmax choice.
        - k_surprise: gain on squared prediction error that scales adaptive learning rate.
        - mod_base: base multiplier for age-load modulation of surprise effect.
        - leak: per-encounter leak toward uniform at the visited state (0..1); larger leak = stronger forgetting.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    alpha0, beta, k_surprise, mod_base, leak = model_parameters
    age_value = age[0]
    is_older = 1 if age_value >= 45 else 0

    nA = 3
    total_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        Q = (1.0 / nA) * np.ones((nS, nA))

        # Age-load modulation for surprise-driven adaptation:
        # older => +50%; lower load => larger factor (1.0 at 3, 0.5 at 6).
        load_factor = 3.0 / float(max(1, nS))
        age_factor = 1.0 + 0.5 * is_older
        m_age_load = mod_base * age_factor * load_factor

        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Softmax policy over Q
            V_s = Q[s, :]
            V_centered = V_s - np.max(V_s)
            expV = np.exp(beta * V_centered)
            p_vec = expV / np.sum(expV)
            p_choice = max(p_vec[a], 1e-12)
            total_log_p += np.log(p_choice)

            # Apply load-sensitive leak toward uniform prior at visited state
            if leak > 0.0:
                Q[s, :] = (1.0 - leak) * Q[s, :] + leak * (1.0 / nA)

            # Surprise-driven adaptive learning rate
            pe = r - Q[s, a]
            alpha_t = 1.0 / (1.0 + np.exp(-(alpha0 + k_surprise * (pe ** 2) * m_age_load)))

            # Q update with adaptive alpha
            Q[s, a] += alpha_t * pe

    return -float(total_log_p)