def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with visit- and time-based WM arbitration.

    Mechanism
    - RL: standard delta-rule learning with softmax choice.
    - WM store: reward-gated, item-specific memory that decays as a function of
      time since the state was last encountered.
    - Arbitration: WM influence increases with the number of visits to a state
      (state familiarity) and decreases with set size. Thus, WM helps more in
      small set-size blocks and after a few repetitions of the same state.

    Parameters
    ----------
    model_parameters : list or array of length 5
        lr : float
            RL learning rate (0-1).
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        wm_prior : float
            Baseline WM learning strength and mixture cap (0-1).
        wm_time_tau : float
            Time constant for WM decay (in trials between successive visits to the state).
            Larger values decay more slowly.
        count_slope : float
            Controls how quickly WM arbitration grows with state visit count
            via a sigmoid on visits.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, softmax_beta, wm_prior, wm_time_tau, count_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        visits = np.zeros(nS, dtype=int)
        last_t = -np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Apply WM decay for this state based on time since last seen
            if last_t[s] >= 0 and wm_time_tau > 0:
                dt = t - last_t[s]
                decay = np.exp(-float(dt) / max(wm_time_tau, 1e-6))
                w[s, :] = w_0[s, :] + decay * (w[s, :] - w_0[s, :])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: depends on set size and visits to state
            visits_s = visits[s]
            fam = 1.0 / (1.0 + np.exp(-count_slope * (visits_s - 1.0)))  # grows with visits
            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(wm_prior * fam * size_scale, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (reward-gated strengthening toward chosen action)
            if r > 0:
                target = np.zeros(nA)
                target[a] = 1.0
                # Encode with strength tied to wm_prior; stronger when familiar
                enc_gain = np.clip(wm_prior * (0.5 + 0.5 * fam), 0.0, 1.0)
                w[s, :] = (1.0 - enc_gain) * w[s, :] + enc_gain * target
            else:
                # On non-reward, slight relaxation toward uniform
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # bookkeeping
            visits[s] += 1
            last_t[s] = t

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with action stickiness + WM gated by prediction-error magnitude.

    Mechanism
    - RL: standard delta-rule; softmax with action stickiness in the current state
      (bias to repeat the last action in that state).
    - WM: stores a strong one-shot memory when prediction error magnitude is large
      (surprising outcome), with overwrite proportional to a gate on |PE|.
    - Arbitration: WM mixture weight shrinks with set size and with recent PE
      magnitude (more WM when outcomes are predictable/low PE).

    Parameters
    ----------
    model_parameters : list or array of length 5
        lr : float
            RL learning rate (0-1).
        wm_weight : float
            Baseline WM mixture weight (0-1), further scaled by 3/nS and by the PE gate.
        softmax_beta : float
            RL inverse temperature; internally scaled by 10.
        pe_gate : float
            Controls sensitivity of WM encoding and arbitration to |prediction error|.
            Larger values, steeper gating.
        action_stick : float
            Additive logit bias for repeating the last action taken in the same state.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, pe_gate, action_stick = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -np.ones(nS, dtype=int)
        prev_abs_pe = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with state-specific stickiness
            Q_s = q[s, :].copy()
            logits = softmax_beta * (Q_s - np.max(Q_s))
            if last_action[s] >= 0:
                logits[last_action[s]] += action_stick
            exp_rl = np.exp(logits - np.max(logits))
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight: lower when set size is large and when recent PE was large
            size_scale = 3.0 / float(nS)
            # Use previous absolute PE to decide reliance on WM now (predictable -> more WM)
            gate = 1.0 / (1.0 + np.exp(pe_gate * (prev_abs_pe[s] - 0.25)))
            eff_wm_weight = np.clip(wm_weight * size_scale * gate, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM update: overwrite strength grows with |PE|
            abs_pe = abs(delta)
            enc_gain = 1.0 / (1.0 + np.exp(-pe_gate * (abs_pe - 0.25)))
            # Slight constant leak toward uniform to prevent saturation
            w[s, :] = 0.99 * w[s, :] + 0.01 * w_0[s, :]
            target = np.zeros(nA)
            if r > 0:
                target[a] = 1.0
                w[s, :] = (1.0 - enc_gain) * w[s, :] + enc_gain * target
            else:
                # On non-reward, bias away from chosen action slightly
                anti = np.ones(nA) / (nA - 1.0)
                anti[a] = 0.0
                w[s, :] = 0.95 * w[s, :] + 0.05 * anti

            # bookkeeping
            last_action[s] = a
            prev_abs_pe[s] = abs_pe

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with entropy-adaptive learning rate + WM encoding by advantage threshold
    and beta modulation by cognitive load.

    Mechanism
    - RL: delta-rule with trial-wise learning rate adapted by current choice
      entropy (more certain -> lower learning rate; more diffuse -> higher).
      Softmax inverse temperature is reduced under higher set size.
    - WM: encodes strongly when the chosen action's advantage over the next-best
      action exceeds a threshold and reward is positive; otherwise encodes weakly.
      WM also exhibits small continuous leak toward uniform.
    - Arbitration: fixed WM weight scaled by set size (3/nS).

    Parameters
    ----------
    model_parameters : list or array of length 6
        lr0 : float
            Base RL learning rate (0-1) subject to entropy-driven modulation.
        softmax_beta : float
            Base inverse temperature; internally scaled by 10 and downscaled by set size.
        wm_weight : float
            Baseline WM mixture weight (0-1), scaled by 3/nS.
        adv_thresh : float
            Advantage threshold for strong WM encoding; operates on (Q[a] - second_best_Q).
        wm_refresh : float
            WM leak/refresh rate toward uniform (per trial, 0-1). Also sets the scale
            of encoding gains.
        beta_cogload : float
            Strength of softmax beta downscaling with set size (>=0).

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    lr0, softmax_beta, wm_weight, adv_thresh, wm_refresh, beta_cogload = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    blocks_log_p = 0.0
    nA = 3

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask].astype(int)
        block_rewards = rewards[block_mask]
        block_states = states[block_mask].astype(int)
        block_set_sizes = set_sizes[block_mask]

        nS = int(block_set_sizes[0])

        # Load-dependent beta: reduce with larger set size
        beta_scale = 1.0 / (1.0 + beta_cogload * max((nS - 3), 0) / 3.0)
        beta_block = softmax_beta * beta_scale

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :].copy()
            logits = beta_block * (Q_s - np.max(Q_s))
            exp_rl = np.exp(logits - np.max(logits))
            pvec_rl = exp_rl / np.sum(exp_rl)
            p_rl = pvec_rl[a]

            # WM policy
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration
            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(wm_weight * size_scale, 0.0, 1.0)

            p_total = eff_wm_weight * p_wm + (1.0 - eff_wm_weight) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update with entropy-adaptive learning rate
            # Entropy of current policy in this state (normalized to [0,1] by log(nA))
            eps = 1e-12
            H = -np.sum(pvec_rl * np.log(pvec_rl + eps)) / np.log(nA)
            eff_lr = np.clip(lr0 * (0.5 + 0.5 * H), 0.0, 1.0)
            delta = r - q[s, a]
            q[s, a] += eff_lr * delta

            # WM leak
            w = (1.0 - wm_refresh) * w + wm_refresh * w_0

            # WM encoding based on advantage threshold
            # Compute advantage of chosen over second-best
            sorted_Q = np.sort(Q_s)
            second_best = sorted_Q[-2]
            advantage = Q_s[a] - second_best
            # Encoding gains: stronger if advantage exceeds threshold and reward is positive
            base_gain = np.clip(5.0 * wm_refresh, 0.0, 1.0)
            strong = (advantage >= adv_thresh) and (r > 0)
            gain = base_gain if strong else 0.5 * base_gain * max(r, 0.0)
            if gain > 0.0:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - gain) * w[s, :] + gain * target

        blocks_log_p += log_p

    return -blocks_log_p