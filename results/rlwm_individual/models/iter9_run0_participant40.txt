def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with uncertainty-directed exploration (UCB-like) modulated by age and set size.

    Mechanism:
      - Q-learning for action values per state.
      - An exploration bonus inversely proportional to the (choice) count of each state-action.
      - The exploration bonus is amplified/suppressed by age and by set size (load).
      - A small 'slip' mixes in uniform choice probability.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). Out-of-range actions are treated as lapses (uniform).
    rewards : np.ndarray of float
        Binary feedback (0 or 1). Negative values are treated as invalid updates.
    blocks : np.ndarray of int
        Block index for each trial.
    set_sizes : np.ndarray of int
        Set size for each trial (3 or 6).
    age : np.ndarray of float
        Participant age in a one-element array; age[0] >= 45 => older group.
    model_parameters : sequence of float
        [learn_rate, temp, ucb_gain, age_factor, slip]
        - learn_rate: learning rate for Q-learning (0..1).
        - temp: inverse temperature for softmax choice.
        - ucb_gain: base weight of the uncertainty bonus.
        - age_factor: multiplicative modulation of exploration for older adults
                      (effective gain = ucb_gain * (1 + age_factor) if older, else ucb_gain).
        - slip: lapse/slip probability mixed with a uniform policy.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    learn_rate, temp, ucb_gain, age_factor, slip = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize Q-values and choice counts
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = np.zeros((nS, nA))  # counts of how often each action was chosen in each state

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL softmax
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(temp * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # Uncertainty bonus (UCB-like): larger for less-sampled actions.
            # Load modulation: stronger directed exploration at low load (ss=3), weaker at high load (ss=6).
            load_mod = 3.0 / float(ss)  # 1.0 for ss=3, 0.5 for ss=6
            age_mod = 1.0 + age_factor * is_older
            eff_gain = max(0.0, ucb_gain) * load_mod * age_mod
            bonus = eff_gain / np.sqrt(N[s, :] + 1.0)

            # Combine Q and bonus inside softmax by adding to scaled utilities
            util = temp * q_s + bonus
            util = util - np.max(util)
            p_choice = np.exp(util)
            p_choice = p_choice / np.sum(p_choice)

            # Mix with slip (uniform)
            p_final = (1.0 - slip) * p_choice + slip * (np.ones(nA) / nA)

            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)

                # Update counts and values
                N[s, a] += 1.0
                if r >= 0.0:
                    Q[s, a] += learn_rate * (r - Q[s, a])
            else:
                # Uninterpretable action (e.g., -2): assign uniform likelihood; skip learning
                total_logp += np.log(1.0 / nA)

    return -float(total_logp)


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Deterministic hypothesis elimination with noisy reinstatement, modulated by age and load.

    Mechanism:
      - For each state, maintain a belief distribution over the 3 actions, initialized uniform.
      - On negative feedback, eliminate (downweight) the sampled action with a reliability that
        decreases with set size and with an age-dependent factor.
      - On positive feedback, reinstate a sharp belief on the chosen action (one-shot memory),
        blended with prior beliefs by a reinstatement gain.
      - Choices are drawn from a softmax over log-beliefs (Luce choice), with small slip to uniform.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). Out-of-range treated as lapse (uniform).
    rewards : np.ndarray of float
        Feedback (0 or 1). Negative values treated as invalid for updating.
    blocks : np.ndarray of int
        Block index for each trial.
    set_sizes : np.ndarray of int
        Set size for each trial (3 or 6).
    age : np.ndarray of float
        Participant age in a one-element array; age[0] >= 45 => older group.
    model_parameters : sequence of float
        [elim_rate, temp, wm_reinst, age_shift, slip]
        - elim_rate: base elimination strength for incorrect action on negative feedback (0..1).
        - temp: inverse temperature applied to log-beliefs in softmax.
        - wm_reinst: gain for reinstating the chosen action on positive feedback (0..1).
        - age_shift: reduces elimination reliability for older adults (elim_eff *= (1 - age_shift) if older).
        - slip: lapse probability mixed with uniform policy.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    elim_rate, temp, wm_reinst, age_shift, slip = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # Initialize beliefs P(s,a) uniform
        P = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # Softmax over log-beliefs (Luce's choice rule)
            p_s = np.clip(P[s, :], 1e-12, 1.0)
            logits = temp * (np.log(p_s) - np.max(np.log(p_s)))
            p_choice = np.exp(logits)
            p_choice = p_choice / np.sum(p_choice)

            # Slip to uniform
            p_final = (1.0 - slip) * p_choice + slip * (np.ones(nA) / nA)

            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)
            else:
                total_logp += np.log(1.0 / nA)
                continue

            # Update beliefs based on feedback
            if r >= 0.0:
                if r > 0.0:
                    # Reinstatement: move belief toward a point mass on the rewarded action
                    one_hot = np.zeros(nA)
                    one_hot[a] = 1.0
                    P[s, :] = (1.0 - wm_reinst) * P[s, :] + wm_reinst * one_hot
                else:
                    # Elimination with load and age modulation
                    load_mod = 3.0 / float(ss)  # lower when ss=6
                    age_mod = (1.0 - age_shift * is_older)
                    elim_eff = np.clip(elim_rate * load_mod * age_mod, 0.0, 1.0)

                    # Downweight the incorrect action, renormalize
                    P[s, a] = P[s, a] * (1.0 - elim_eff)
                    # If all mass collapses numerically, re-spread a tiny uniform mass
                    total_mass = np.sum(P[s, :])
                    if total_mass <= 1e-12:
                        P[s, :] = (1.0 / nA)
                    else:
                        P[s, :] = P[s, :] / total_mass

    return -float(total_logp)


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with memory-binding (swap) errors: WM-guided policy is corrupted by load and age.

    Mechanism:
      - Incremental RL (Q-learning) forms a baseline policy via softmax.
      - A working-memory (WM) trace stores the most recently rewarded action per state.
      - Due to binding/interference, a fraction of WM guidance is 'swapped' from other states:
        the WM policy for a state becomes a mixture of its own stored action and the distribution
        of stored actions from other states. The swap fraction increases with set size and age.
      - Arbitration: higher swap leads to reduced effective WM weight (more reliance on RL).
      - A small noise_floor mixes uniform choice into the final policy.

    Parameters
    ----------
    states : np.ndarray of int
        State index at each trial (0..nS-1 within block).
    actions : np.ndarray of int
        Chosen action at each trial (0..2). Out-of-range treated as lapse (uniform).
    rewards : np.ndarray of float
        Feedback (0 or 1). Negative values are not used for updating.
    blocks : np.ndarray of int
        Block index for each trial.
    set_sizes : np.ndarray of int
        Set size per trial (3 or 6).
    age : np.ndarray of float
        Participant age in a one-element array; age[0] >= 45 => older group.
    model_parameters : sequence of float
        [eta, beta, swap_base, age_suscept, noise_floor]
        - eta: Q-learning rate (0..1).
        - beta: inverse temperature for RL softmax.
        - swap_base: baseline fraction of WM guidance coming from other (incorrect) states at ss=3.
        - age_suscept: additive factor increasing swap fraction for older adults (>=0 increases swaps).
        - noise_floor: mixes uniform policy into the final choice distribution.

    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    eta, beta, swap_base, age_suscept, noise_floor = model_parameters
    age_val = float(age[0])
    is_older = 1.0 if age_val >= 45.0 else 0.0

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        idx = (blocks == b)
        b_states = states[idx]
        b_actions = actions[idx]
        b_rewards = rewards[idx]
        b_set_sizes = set_sizes[idx]
        nS = int(b_set_sizes[0])

        # RL values
        Q = (1.0 / nA) * np.ones((nS, nA))

        # WM traces: -1 means no stored action yet
        wm_trace = -np.ones(nS, dtype=int)

        for t in range(len(b_states)):
            s = int(b_states[t])
            a = int(b_actions[t])
            r = float(b_rewards[t])
            ss = int(b_set_sizes[t])

            # RL policy
            q_s = Q[s, :]
            q_center = q_s - np.max(q_s)
            p_rl = np.exp(beta * q_center)
            p_rl = p_rl / np.sum(p_rl)

            # Build WM policy with swap interference
            # Aggregate distribution of actions stored in other states
            other_idx = [j for j in range(nS) if j != s and wm_trace[j] >= 0]
            p_other = np.zeros(nA)
            if len(other_idx) > 0:
                for j in other_idx:
                    aj = wm_trace[j]
                    if 0 <= aj < nA:
                        p_other[aj] += 1.0
                p_other = p_other / np.sum(p_other)
            else:
                p_other = np.ones(nA) / nA

            # Own state's stored action distribution
            if wm_trace[s] >= 0 and wm_trace[s] < nA:
                p_self = np.zeros(nA)
                p_self[wm_trace[s]] = 1.0
            else:
                p_self = np.ones(nA) / nA

            # Swap fraction increases with load (ss/3) and age
            swap_eff = np.clip(swap_base * (float(ss) / 3.0) * (1.0 + age_suscept * is_older), 0.0, 1.0)
            p_wm = (1.0 - swap_eff) * p_self + swap_eff * p_other

            # Arbitration: reduce WM weight as swap increases
            wm_weight = np.clip(1.0 - swap_eff, 0.0, 1.0)
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl

            # Add uniform noise floor
            p_final = (1.0 - noise_floor) * p_mix + noise_floor * (np.ones(nA) / nA)

            if 0 <= a < nA:
                pa = float(np.clip(p_final[a], 1e-12, 1.0))
                total_logp += np.log(pa)

                # RL update
                if r >= 0.0:
                    Q[s, a] += eta * (r - Q[s, a])

                    # WM trace update on positive feedback: store the rewarded mapping
                    if r > 0.0:
                        wm_trace[s] = a
            else:
                total_logp += np.log(1.0 / nA)
                # Do not update on invalid actions

    return -float(total_logp)