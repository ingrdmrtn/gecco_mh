def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + capacity-limited WM with interference and RL forgetting.

    Idea:
    - RL: delta-rule with small forgetting toward uniform each trial.
    - WM: one-hot storage upon rewarded outcome; subject to global interference that increases with set size.
    - Arbitration: fixed base wm_weight scaled down by an effective capacity factor (capacity / set size)
      and further reduced by interference pressure.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Base arbitration weight for WM channel (0..1) before set-size scaling.
    - softmax_beta: Baseline inverse temperature for RL (scaled by 10 internally).
    - wm_capacity: Effective WM capacity in number of items (>=1).
    - interference_strength: Strength of cross-item interference pushing WM toward uniform (0..1).
    - rl_forget: RL forgetting rate toward uniform per trial (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    wm_capacity, interference_strength, rl_forget = model_parameters[3:]
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Precompute effective arbitration weight for this block
        cap_factor = np.clip(float(wm_capacity) / max(1.0, float(nS)), 0.0, 1.0)
        overload = np.clip(float(nS) / max(1.0, float(wm_capacity)), 0.0, 1.0)
        inter_penalty = np.clip(1.0 - interference_strength * overload, 0.0, 1.0)
        wm_w_eff = np.clip(wm_weight * cap_factor * inter_penalty, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy (probability of chosen action, computed in a numerically stable way)
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: fixed mixture with set-size adjusted WM weight
            p_total = np.clip(wm_w_eff, 0.0, 1.0)*p_wm + (1-np.clip(wm_w_eff, 0.0, 1.0))*p_rl
            log_p += np.log(np.clip(p_total, 1e-12, 1.0))
      
            # RL update with forgetting
            delta = r-Q_s[a]
            q[s][a] += lr*delta
            # RL forgetting toward uniform across all state-action values
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Store on rewarded trials: one-hot, then global interference toward uniform
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            # Interference toward uniform increases with overload
            w = (1.0 - interference_strength * overload) * w + (interference_strength * overload) * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Hybrid RL + WM with meta-control based on WM strength, RL entropy, and set size, plus WM decay and lapses.

    Idea:
    - RL: standard delta-rule.
    - WM: sharpened distribution when rewarded; decays toward uniform at rate wm_decay.
    - Arbitration: dynamic wm_weight via a logistic meta-controller that favors WM when:
        (a) WM strength is high, (b) RL action entropy is high (uncertain), (c) set size is small.
      The meta_temp parameter scales the sharpness of this gating.
    - Lapse: with probability 'lapse', choices are uniform random.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Base bias term for WM in the logistic meta-controller (can be negative/positive).
    - softmax_beta: Baseline inverse temperature for RL (scaled by 10 internally).
    - meta_temp: Gain of the meta-controller (>=0), larger means more decisive gating.
    - wm_decay: WM decay per trial toward uniform (0..1).
    - lapse: Lapse probability (0..1) of choosing uniformly at random.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, meta_temp, wm_decay, lapse = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        size_factor = 3.0 / max(3.0, float(nS))  # in [0.5, 1], smaller set -> larger factor

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]
            W_s = w[s,:]

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # WM policy
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # RL uncertainty via normalized entropy of softmax over Q_s at current beta
            logits = softmax_beta * (Q_s - np.max(Q_s))
            exp_logits = np.exp(logits)
            prob_rl_vec = exp_logits / np.sum(exp_logits)
            entropy = -np.sum(prob_rl_vec * np.log(np.clip(prob_rl_vec, 1e-12, 1.0)))
            max_entropy = np.log(len(Q_s))
            entropy_norm = entropy / max_entropy  # 0..1

            # WM strength in [0,1]: deviation from uniform
            wm_strength = np.max(W_s) - 1.0 / nA
            wm_strength_norm = np.clip(wm_strength / (1.0 - 1.0 / nA), 0.0, 1.0)

            # Meta-control: logistic gating
            # Advantage term balances WM strength (+), RL entropy (+), and small set size (+)
            advantage = wm_weight + meta_temp * (wm_strength_norm + entropy_norm + size_factor - 1.0)
            wm_w_eff = 1.0 / (1.0 + np.exp(-advantage))
            wm_w_eff = np.clip(wm_w_eff, 0.0, 1.0)

            mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            p_total = (1.0 - lapse) * mix + lapse * (1.0 / nA)
            log_p += np.log(np.clip(p_total, 1e-12, 1.0))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # WM update: write on reward, then decay toward uniform
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0
                w[s, :] /= np.sum(w[s, :])
            # global decay
            w = (1.0 - wm_decay) * w + wm_decay * w_0

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WSLS-style WM heuristic with set-size-dependent arbitration and recency.

    Idea:
    - RL: standard delta-rule.
    - WM channel implements a win-stay/lose-shift heuristic per state:
        when last outcome in that state was rewarded, bias toward repeating last action;
        when not rewarded, bias away from last action.
      This heuristic is encoded in W and sharpened by wsls_bias; W decays with 'recency'.
    - Arbitration: WM reliance increases for small set sizes and with base wm_weight.

    Parameters (tuple):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM weight (can be any real; passed through sigmoid after size scaling).
    - softmax_beta: Baseline inverse temperature for RL (scaled by 10 internally).
    - wsls_bias: Positive values strengthen the WM/WSLS policy precision; negative softens it.
    - size_slope: Scales how much smaller sets boost WM reliance (>=0).
    - recency: Decay rate per trial of WM heuristic toward uniform (0..1).

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wsls_bias, size_slope, recency = model_parameters
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # base; will be modulated by wsls_bias

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # For WSLS we need to remember last action and outcome per state
        last_action = -1 * np.ones(nS, dtype=int)
        last_reward = 0.0 * np.ones(nS)

        # Set-size dependent WM reliance via logistic transform
        size_boost = size_slope * (3.0 / max(3.0, float(nS)))  # in [size_slope*0.5, size_slope]
        wm_bias = wm_weight + size_boost
        wm_weight_eff_block = 1.0 / (1.0 + np.exp(-wm_bias))

        # WM precision adjustment via wsls_bias
        beta_wm_eff = max(1.0, softmax_beta_wm * np.exp(wsls_bias))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s,:]

            # Build WSLS heuristic distribution in W_s
            # Start from decay toward uniform
            w[s, :] = (1.0 - recency) * w[s, :] + recency * w_0[s, :]

            if last_action[s] >= 0:
                # If last trial in this state existed, set heuristic
                if last_reward[s] > 0.5:
                    # Win-stay: bias toward last_action
                    w[s, :] = w_0[s, :]
                    w[s, last_action[s]] = 1.0
                    w[s, :] /= np.sum(w[s, :])
                else:
                    # Lose-shift: bias away from last_action -> spread mass on other actions
                    w[s, :] = w_0[s, :]
                    others = [i for i in range(nA) if i != last_action[s]]
                    for i in others:
                        w[s, i] = 1.0
                    w[s, :] /= np.sum(w[s, :])
            # WM policy using sharpened beta
            W_s = w[s, :]
            p_wm = 1 / np.sum(np.exp(beta_wm_eff * (W_s - W_s[a])))

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta*(Q_s-Q_s[a])))

            # Mixture with block-level effective WM weight
            p_total = np.clip(wm_weight_eff_block, 0.0, 1.0)*p_wm + (1-np.clip(wm_weight_eff_block, 0.0, 1.0))*p_rl
            log_p += np.log(np.clip(p_total, 1e-12, 1.0))
      
            # RL update
            delta = r-Q_s[a]
            q[s][a] += lr*delta

            # Update WSLS memory for state s with current outcome
            last_action[s] = a
            last_reward[s] = r

        blocks_log_p += log_p

    return -blocks_log_p