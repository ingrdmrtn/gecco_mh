def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-based arbitration (entropy vs. WM confidence).

    Mechanism:
    - RL: tabular Q-learning with constant learning rate.
    - WM: fast encoding toward the rewarded action with decay toward uniform baseline.
    - Arbitration: dynamic, based on a comparison between RL uncertainty (entropy) and WM confidence.
        wm_weight_eff = wm_base * sigmoid(arb_gain * (wm_confidence - normalized_RL_entropy))
      where WM confidence = max probability in the WM row for the current state, and RL entropy is
      the entropy of the RL softmax policy normalized by log(3).

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_base: Baseline WM mixture scaling in [0,1] (upper bound on WM weight).
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - wm_enc: WM encoding gain in [0,1]; how strongly WM shifts toward rewarded action.
    - wm_forget: WM decay toward uniform in [0,1] applied on each visit to the state.
    - arb_gain: Arbitration gain controlling sensitivity of the gate to WM confidence vs RL entropy.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_base, softmax_beta, wm_enc, wm_forget, arb_gain = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Clips for numerical safety
    lr = np.clip(lr, 0.0, 1.0)
    wm_base = np.clip(wm_base, 0.0, 1.0)
    wm_enc = np.clip(wm_enc, 0.0, 1.0)
    wm_forget = np.clip(wm_forget, 0.0, 1.0)
    arb_gain = float(arb_gain)

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy (full vector to compute entropy)
            Q_shift = Q_s - np.max(Q_s)
            exp_Q = np.exp(softmax_beta * Q_shift)
            p_rl_vec = exp_Q / np.sum(exp_Q + 1e-12)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = exp_W / np.sum(exp_W + 1e-12)
            p_wm = p_wm_vec[a]

            # Arbitration: entropy vs. confidence
            rl_entropy = -np.sum(p_rl_vec * (np.log(p_rl_vec + 1e-12)))
            rl_entropy_norm = rl_entropy / np.log(nA)  # in [0,1]
            wm_conf = np.max(W_s)  # in [1/nA, 1]
            gate = 1.0 / (1.0 + np.exp(-arb_gain * (wm_conf - rl_entropy_norm)))
            wm_weight_eff = np.clip(wm_base * gate, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform and reward-driven encoding
            # Forget (toward uniform) on visit
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            if r > 0.0:
                # Encode rewarded mapping: push probability mass toward chosen action
                w[s, :] = (1.0 - wm_enc) * w[s, :]
                w[s, a] += wm_enc

            # Renormalize
            w_row_sum = np.sum(w[s, :])
            if w_row_sum > 0:
                w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with surprise-modulated learning rate + WM with error-gated encoding and load-dependent precision.

    Mechanism:
    - RL: tabular Q-learning with trial-by-trial learning rate:
          alpha_t = clip(lr_base + lr_gain * |delta_t|, 0, 1)
      capturing Pearce-Hall style surprise-driven learning.
    - WM: encodes the rewarded choice only when the state's recent prediction error
      (EWMA) is below a gate threshold (i.e., mapping is stable), otherwise it refrains from encoding.
      WM precision and mixture weight degrade with set size.
    - Arbitration: fixed WM weight scaled down by set size.

    Parameters (in order):
    - lr_base: Baseline RL learning rate in [0,1].
    - lr_gain: Surprise scaling for RL learning rate in [0,1] (effective alpha_t is clipped to [0,1]).
    - wm_weight: Baseline WM mixture weight in [0,1].
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - gate_thresh: Threshold on recent abs prediction error to allow WM encoding (lower -> stricter).
    - load_scale: >=0 scales how strongly set size reduces WM influence and precision.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_base, lr_gain, wm_weight, softmax_beta, gate_thresh, load_scale = model_parameters
    softmax_beta *= 10.0
    base_softmax_beta_wm = 50.0

    lr_base = np.clip(lr_base, 0.0, 1.0)
    lr_gain = np.clip(lr_gain, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    gate_thresh = max(0.0, float(gate_thresh))
    load_scale = max(0.0, float(load_scale))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track recent prediction error per state for WM gating
        err_trace = np.zeros(nS)
        err_decay = 0.8  # fixed EWMA decay within-block

        # Load-dependent WM precision and weight
        load_factor = 1.0 + load_scale * max(0, nS - 3)
        softmax_beta_wm = base_softmax_beta_wm / load_factor
        wm_weight_eff = wm_weight / load_factor
        wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]

            # RL policy
            Q_shift = Q_s - np.max(Q_s)
            exp_Q = np.exp(softmax_beta * Q_shift)
            p_rl_vec = exp_Q / np.sum(exp_Q + 1e-12)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = exp_W / np.sum(exp_W + 1e-12)
            p_wm = p_wm_vec[a]

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with surprise-modulated alpha
            delta = r - Q_s[a]
            alpha_t = np.clip(lr_base + lr_gain * abs(delta), 0.0, 1.0)
            q[s, a] += alpha_t * delta

            # Update error trace for gating
            err_trace[s] = err_decay * err_trace[s] + (1.0 - err_decay) * abs(delta)

            # WM update: encode only if mapping is stable and rewarded
            if (r > 0.0) and (err_trace[s] <= gate_thresh):
                # One-shot push toward chosen action
                w[s, :] = 0.0 * w[s, :] + 0.0  # explicit for clarity
                w[s, :] = w_0[s, :]  # start from uniform
                w[s, :] = 0.0 * w[s, :] + 0.0
                # Equivalent to: initialize then push strongly
                w[s, :] = (1.0 / nA) * np.ones(nA)
                w[s, a] = 1.0  # cache chosen action
                # small smoothing to avoid degeneracy
                eps = 1e-6
                w[s, :] = (1.0 - nA * eps) * w[s, :] + eps

            # Renormalize WM row in any case
            w_row_sum = np.sum(w[s, :])
            if w_row_sum > 0:
                w[s, :] = np.clip(w[s, :], 1e-12, 1.0)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM "hypothesis with confidence" model and load-scaled arbitration.

    Mechanism:
    - RL: standard Q-learning.
    - WM: for each state, maintain a hypothesized best action a_star[s] and a confidence c[s] in [0,1].
      On reward:
        a_star[s] <- chosen action; c[s] increases by conf_up*(1-c[s]).
      On non-reward:
        if chosen action == a_star[s], c[s] decreases by conf_down*c[s].
      WM policy row is constructed as:
        w[s,:] = (1 - c[s]) * uniform + c[s] * one_hot(a_star[s])
    - Arbitration: wm_weight is downscaled by set size with a power-law and scaled by current confidence:
        wm_weight_eff = wm_weight * (c[s]**1) / (1 + (max(0, nS-1))**load_exp)

    Parameters (in order):
    - lr: RL learning rate in [0,1].
    - wm_weight: Baseline WM mixture weight in [0,1].
    - softmax_beta: RL inverse temperature; scaled by 10 internally.
    - conf_up: Confidence increment rate on reward in [0,1].
    - conf_down: Confidence decrement rate on non-reward in [0,1].
    - load_exp: Exponent >=0 controlling set-size penalty on WM influence.

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, conf_up, conf_down, load_exp = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    lr = np.clip(lr, 0.0, 1.0)
    wm_weight = np.clip(wm_weight, 0.0, 1.0)
    conf_up = np.clip(conf_up, 0.0, 1.0)
    conf_down = np.clip(conf_down, 0.0, 1.0)
    load_exp = max(0.0, float(load_exp))

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM hypothesis store
        a_star = np.zeros(nS, dtype=int)  # initial hypothesis arbitrary (0)
        c = np.zeros(nS)                  # initial confidence 0 -> uniform

        load_penalty = 1.0 + (max(0, nS - 1))**load_exp

        log_p = 0.0
        for t in range(len(block_states)):

            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # Construct WM row from hypothesis and confidence
            w_row = (1.0 - c[s]) * w_0[s, :].copy()
            oh = np.zeros(3)
            oh[a_star[s]] = 1.0
            w_row = w_row + c[s] * oh
            # Normalize for stability
            w_row = np.clip(w_row, 1e-12, 1.0)
            w_row = w_row / np.sum(w_row)
            w[s, :] = w_row

            Q_s = q[s, :]

            # RL policy
            Q_shift = Q_s - np.max(Q_s)
            exp_Q = np.exp(softmax_beta * Q_shift)
            p_rl_vec = exp_Q / np.sum(exp_Q + 1e-12)
            p_rl = p_rl_vec[a]

            # WM policy
            W_s = w[s, :]
            W_shift = W_s - np.max(W_s)
            exp_W = np.exp(softmax_beta_wm * W_shift)
            p_wm_vec = exp_W / np.sum(exp_W + 1e-12)
            p_wm = p_wm_vec[a]

            # Arbitration: confidence-scaled WM with load penalty
            wm_weight_eff = wm_weight * (c[s] ** 1.0) / load_penalty
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM hypothesis/confidence updates
            if r > 0.0:
                a_star[s] = a
                c[s] = c[s] + conf_up * (1.0 - c[s])
            else:
                if a == a_star[s]:
                    c[s] = c[s] * (1.0 - conf_down)
                # if a != a_star, keep c; future rewards will update a_star

            # Bound c to [0,1]
            c[s] = np.clip(c[s], 0.0, 1.0)

        blocks_log_p += log_p

    return -blocks_log_p