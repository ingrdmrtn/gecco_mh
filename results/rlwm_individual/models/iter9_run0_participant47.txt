def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 1: RL + WM with uncertainty-based arbitration and Dirichlet-like WM representation.
    - RL: single-rate Q-learning.
    - WM: per-state action distribution behaves like a Dirichlet belief; reward increases
          the chosen action's concentration, and a forgetting term pulls toward uniform.
    - Arbitration: WM weight increases when WM is more certain (lower entropy) than RL
      and decreases with set size (load) via a 3/nS scaling.

    Parameters:
      lr:               RL learning rate for Q updates (0..1)
      softmax_beta:     RL inverse temperature (scaled internally by 10; >=0)
      wm_conc_init:     Initial WM concentration per action (>=0); sets starting sharpness
      wm_forget:        WM forgetting rate toward uniform on each visit (0..1)
      arb_unc_slope:    Slope converting (uncertainty difference) to WM weight via sigmoid
      wm_reward_boost:  Increment to WM concentration for rewarded chosen action (>=0)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_conc_init, wm_forget, arb_unc_slope, wm_reward_boost = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        # WM represented as "concentrations" that translate into a probability vector
        w = np.full((nS, nA), max(1e-6, wm_conc_init))
        w_0 = np.full((nS, nA), max(1e-6, wm_conc_init))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            # Convert WM concentrations to a probability vector for policy
            W_conc = w[s, :].clip(1e-12)
            W_s = W_conc / np.sum(W_conc)

            # RL policy
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from WM distribution with high inverse temperature
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Uncertainty-based arbitration: compare entropies (higher entropy = more uncertain)
            eps = 1e-12
            # RL implied distribution via softmax over Q (for entropy)
            rl_probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            rl_probs = rl_probs / np.sum(rl_probs)
            H_rl = -np.sum(rl_probs * np.log(rl_probs + eps))
            H_wm = -np.sum(W_s * np.log(W_s + eps))
            # Positive when WM is more certain than RL
            delta_unc = H_rl - H_wm
            # Load scaling: WM weight reduced as set size increases
            load_scale = 3.0 / max(3.0, float(nS))
            wm_weight = load_scale / (1.0 + np.exp(-arb_unc_slope * delta_unc))

            p_total = wm_weight * p_wm + (1 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM decay toward prior (uniform via initial concentrations)
            w[s, :] = (1.0 - wm_forget) * w[s, :] + wm_forget * w_0[s, :]

            # WM storage: increase concentration for rewarded chosen action
            if r > 0:
                w[s, a] += wm_reward_boost

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 2: RL with eligibility traces + PE-gated WM storage.
    - RL: Q-learning with eligibility traces over state-action pairs within a block.
          Although the task is bandit-like, traces capture short-term carryover.
    - WM: deterministic map that stores rewarded actions when a PE-gated gate opens.
          The gate probability increases with the magnitude of the PE.
          WM decays toward uniform otherwise.
    - Arbitration: mixture of RL and WM where WM becomes more deterministic when the
          gate has stored a strong trace; WM temperature scaled by wm_temp_gain.

    Parameters:
      lr:               RL learning rate (0..1)
      softmax_beta:     RL inverse temperature (scaled internally by 10; >=0)
      trace_lambda:     Eligibility trace decay (0..1); controls persistence of e
      gate_sensitivity: Controls how PE magnitude opens the WM gate (>=0)
      wm_decay:         WM decay toward uniform on each visit (0..1)
      wm_temp_gain:     Gain on WM inverse temperature (multiplies base 50; >=0)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, trace_lambda, gate_sensitivity, wm_decay, wm_temp_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50 * max(0.0, wm_temp_gain)  # scale WM determinism
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Eligibility traces over state-action pairs
        e = np.zeros((nS, nA))

        # Fixed WM weight that also decreases with load (more items -> less WM influence)
        base_wm_weight = 3.0 / max(3.0, float(nS))

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM policies
            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            p_total = base_wm_weight * p_wm + (1 - base_wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # Compute PE
            delta = r - Q_s[a]

            # Update eligibility traces: decay then set the chosen pair to 1
            e *= trace_lambda
            e[s, a] = 1.0

            # RL update with traces
            q += lr * delta * e

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # PE-gated WM storage: gate opens more with larger |PE|
            gate_prob = 1.0 / (1.0 + np.exp(-gate_sensitivity * abs(float(delta))))
            if np.random.rand() < gate_prob and r > 0:
                # Store a strong, near-deterministic preference for the rewarded action
                w[s, :] = (1.0 - 1.0) * w[s, :]  # zero out before placing a point mass
                w[s, a] = 1.0
                # Numerical safety
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    Model 3: Asymmetric RL + fixed WM weight with load-dependent interference across states.
    - RL: Q-learning with separate learning rates for gains and losses.
    - WM: rewarded actions are stored; representations decay toward uniform and also
          suffer cross-state interference that increases with set size.
    - Arbitration: fixed WM mixture weight scaled down by load.

    Parameters:
      lr_pos:            RL learning rate for positive PE (0..1)
      lr_neg:            RL learning rate for negative PE (0..1)
      softmax_beta:      RL inverse temperature (scaled internally by 10; >=0)
      wm_weight:         Base WM mixture weight (0..1) before load scaling
      wm_decay:          WM decay toward uniform on each visit (0..1)
      interference_gain: Strength of cross-state WM interference increasing with load (>=0)

    Returns:
      Negative log-likelihood of observed actions.
    """
    lr_pos, lr_neg, softmax_beta, wm_weight, wm_decay, interference_gain = model_parameters

    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Load-adjusted WM weight
        load_scale = 3.0 / max(3.0, float(nS))
        wm_weight_eff = np.clip(wm_weight * load_scale, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            wm_logits = W_s
            p_wm = 1 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetry
            delta = r - Q_s[a]
            lr = lr_pos if delta >= 0 else lr_neg
            q[s][a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # Rewarded storage: sharpen the chosen action if rewarded
            if r > 0:
                # Move mass toward the chosen action
                gain = 0.7  # strong storage; implicit but derived from deterministic WM policy
                w[s, :] = (1.0 - gain) * w[s, :]
                w[s, a] += gain
                w[s, :] = np.clip(w[s, :], 1e-12, None)
                w[s, :] /= np.sum(w[s, :])

            # Cross-state interference increases with set size
            if nS > 1:
                k = interference_gain * max(0.0, (nS - 3.0) / 3.0)
                if k > 0:
                    mean_w = np.mean(w, axis=0, keepdims=True)
                    w = (1.0 - k) * w + k * mean_w
                    # Renormalize per state
                    w = np.clip(w, 1e-12, None)
                    w = w / np.sum(w, axis=1, keepdims=True)

        blocks_log_p += log_p

    return -blocks_log_p