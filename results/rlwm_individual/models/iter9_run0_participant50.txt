def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with uncertainty-weighted arbitration and set-size gating.

    Idea:
    - RL learns action values per state with delta rule and softmax policy.
    - WM stores near one-shot associations, but its contribution is reduced as set size increases.
    - Arbitration favors WM more when RL is uncertain (high entropy), and favors RL when RL is confident.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Baseline WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL policy (scaled internally by 10).
    - entropy_slope: Controls how strongly RL uncertainty (entropy) increases WM weight (>0 increases WM usage when RL is uncertain).
    - load_gate: Controls how quickly WM contribution declines with set size (higher => stronger decline with larger nS).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, entropy_slope, load_gate = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size gate for WM availability
        # Larger nS -> smaller gate factor
        load_factor = 1.0 / (1.0 + np.exp(load_gate * (float(nS) - 3.0)))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1.0 / nA

            # RL full distribution to compute entropy
            # softmax with numerical stabilization
            z = Q_s - np.max(Q_s)
            pi_rl = np.exp(softmax_beta * z)
            pi_rl = pi_rl / np.sum(pi_rl)
            H_rl = -np.sum(pi_rl * (np.log(pi_rl + 1e-12)))

            # WM policy probability (soft, near-deterministic)
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # Uncertainty-weighted arbitration:
            # More WM weight when RL entropy is high.
            # Use a sigmoid transform around a mid-entropy reference (~log(nA)/2)
            H_ref = 0.5 * np.log(nA)
            uncert = 1.0 / (1.0 + np.exp(-entropy_slope * (H_rl - H_ref)))
            wm_weight_eff = wm_weight * load_factor * uncert

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # Rewarded trials: write close to one-hot; Unrewarded: decay to baseline.
            # Both modulated by load_factor (less effective under high load).
            if r > 0.0:
                # Write strength increases with uncertainty (if RL uncertain, rely more on WM writing)
                write_strength = min(1.0, 0.3 + 0.7 * uncert) * load_factor
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * one_hot
            else:
                decay_strength = (0.2 + 0.6 * (1.0 - load_factor))
                w[s, :] = (1.0 - decay_strength) * w[s, :] + decay_strength * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM as a win-stay associative buffer with load-dependent precision.

    Idea:
    - RL uses delta learning and softmax.
    - WM acts like a win-stay memory: when rewarded, store the chosen action for that state.
      On subsequent visits, WM pushes policy toward that stored action.
    - The sharpness of WM policy declines with set size (retrieval precision drops).

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Mixture weight for WM (0..1).
    - softmax_beta: Inverse temperature for RL (scaled internally by 10).
    - ws_bias: Strength of the win-stay imprint in WM values (higher => more peaked WM preference).
    - load_sensitivity: Controls how strongly WM precision decreases with set size.

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, ws_bias, load_sensitivity = model_parameters
    softmax_beta *= 10.0
    base_softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM precision declines with set size
        precision_scale = 1.0 / (1.0 + load_sensitivity * max(0.0, float(nS) - 3.0))
        softmax_beta_wm = base_softmax_beta_wm * precision_scale

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy prob for chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1.0 / nA

            # WM policy: softmax over WM values
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: win-stay imprint on reward; mild leak otherwise
            if r > 0.0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                # Strength of imprint controlled by ws_bias and precision decline
                write = min(1.0, 0.25 + 0.75 * ws_bias * precision_scale)
                w[s, :] = (1.0 - write) * w[s, :] + write * one_hot
            else:
                # Leak back toward uniform baseline; leak stronger when set size is large
                leak = min(1.0, 0.2 + 0.6 * (1.0 - precision_scale))
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + PE-gated WM writing with capacity scaling and WM leak.

    Idea:
    - RL learns with delta rule; policy via softmax.
    - WM writes more strongly when unsigned prediction error is large (surprising outcomes),
      and less when it is small (confirmatory), capturing attention/salience-driven memory.
    - WM availability declines with set size via a capacity-like scaling.
    - WM has a leak parameter that pulls it back toward uniform when not reinforced.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight_base: Baseline WM mixture weight (0..1).
    - softmax_beta: Inverse temperature for RL (scaled internally by 10).
    - pe_to_wm: Gain transforming unsigned PE into WM write strength (higher => stronger PE-driven WM storage).
    - cap_alpha: Capacity factor (>0). Effective WM availability scales as cap_alpha / (cap_alpha + nS).
    - wm_leak: Leak rate of WM toward baseline when not reinforced (0..1).

    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight_base, softmax_beta, pe_to_wm, cap_alpha, wm_leak = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-like availability
        avail = cap_alpha / (cap_alpha + float(nS))
        wm_weight_eff_base = wm_weight_base * avail

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]

            # RL policy prob of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl if denom_rl > 0 else 1.0 / nA

            # WM policy prob of chosen action
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm if denom_wm > 0 else 1.0 / nA

            # PE-gated arbitration: more WM when the unsigned PE is large (surprise)
            pe = r - Q_s[a]
            upe = abs(pe)
            gate = 1.0 / (1.0 + np.exp(-pe_to_wm * (upe - 0.5)))  # centered near 0.5 surprise
            wm_weight_eff = wm_weight_eff_base * gate

            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            q[s, a] += lr * pe

            # WM update:
            # Write toward one-hot proportional to surprise and availability; otherwise leak
            one_hot = np.zeros(nA)
            one_hot[a] = 1.0
            write_strength = avail * gate
            if r > 0.0:
                w[s, :] = (1.0 - write_strength) * w[s, :] + write_strength * one_hot
            else:
                # Leak toward baseline with wm_leak; also small surprise-driven partial write
                w[s, :] = (1.0 - wm_leak) * w[s, :] + wm_leak * w_0[s, :]
                if write_strength > 0.0:
                    w[s, :] = (1.0 - 0.5 * write_strength) * w[s, :] + (0.5 * write_strength) * one_hot

        blocks_log_p += log_p

    return -blocks_log_p