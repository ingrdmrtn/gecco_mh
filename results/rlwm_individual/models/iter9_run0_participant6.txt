def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Uncertainty-tempered softmax RL with age-by-load modulation.

    Idea
    - Model-free Q-learning is used.
    - The decision temperature is reduced when the agent is more certain about a state
      (more visits), and increased when uncertain (few visits).
    - The strength of uncertainty tempering is modulated by set size and age (older and larger sets
      cause stronger reduction of effective inverse temperature, encouraging exploration).

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial (0..nS-1 within a block).
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary reward feedback (0/1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for each trial's block (e.g., 3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha, beta0, k_unc_temp, age_load_bias, prior_c)
        - alpha: learning rate for Q updates (0..1).
        - beta0: baseline inverse temperature for softmax (>0).
        - k_unc_temp: scales how much uncertainty adjusts temperature (>=0).
        - age_load_bias: multiplies the combined effect of being older and larger set sizes
          on the uncertainty-tempered temperature.
        - prior_c: prior pseudo-count per action for visit counts (>0).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta0, k_unc_temp, age_load_bias, prior_c = model_parameters
    age_val = age[0]
    is_older = (age_val >= 45)

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize Q and visit counts (Dirichlet-like prior)
        Q = (1.0 / nA) * np.ones((nS, nA))
        N = prior_c * np.ones((nS, nA))  # counts per state-action

        # Age-by-load modulation term
        load_term = (float(nS) - 3.0) / 3.0  # 0 for 3-set, 1 for 6-set
        age_term = 1.0 if is_older else 0.0
        mod = 1.0 + age_load_bias * (load_term + age_term)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # State uncertainty from counts: more total visits -> less uncertainty
            total_visits_s = np.sum(N[s, :])
            # Normalize uncertainty to (0,1): with prior, uncertainty high early, low later
            unc_s = 1.0 / (1.0 + total_visits_s / (nA * prior_c))

            # Effective inverse temperature: lower when uncertainty is high
            k_eff = k_unc_temp * mod
            beta_eff = beta0 * max(1e-6, (1.0 - k_eff * unc_s))
            # Numerical safety lower bound
            beta_eff = max(beta_eff, 1e-6)

            # Softmax probability of chosen action
            prefs = beta_eff * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p = 1.0 / max(denom, 1e-12)
            logp += np.log(max(p, 1e-12))

            # Learning update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update counts
            N[s, a] += 1.0

        total_logp += logp

    return -total_logp


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL blended with capacity-limited working memory (WM) store-and-retrieve.

    Idea
    - Model-free Q-learning runs continuously.
    - A simple WM store holds rewarded state-action pairs with probabilistic encoding.
      WM traces decay over time.
    - The probability of storing/retrieving is higher in small set sizes and for younger adults;
      older age and larger sets reduce WM efficacy.
    - Policy is a mixture: WM-based categorical choice from current WM strengths and RL softmax.

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial (0..nS-1 within a block).
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary reward feedback (0/1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for each trial's block (e.g., 3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha, beta, p_slot3, age_effect, rho)
        - alpha: learning rate for RL value updates (0..1).
        - beta: inverse temperature for RL softmax (>0).
        - p_slot3: baseline probability to successfully store a rewarded pair in WM at set size 3 (0..1).
        - age_effect: modulates WM efficacy by age and load (>=0). Positive => older/large sets reduce WM.
        - rho: per-trial WM decay rate toward zero (0..1). Higher => faster forgetting.

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha, beta, p_slot3, age_effect, rho = model_parameters
    age_val = age[0]
    is_older = (age_val >= 45)
    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize RL and WM
        Q = (1.0 / nA) * np.ones((nS, nA))
        WM = np.zeros((nS, nA))  # strength of WM trace per state-action in [0,1]

        # Compute WM store efficacy for this block
        load_factor = 3.0 / float(nS)  # 1.0 for 3-set, 0.5 for 6-set
        # Age/load modulation: older and larger set => lower store prob
        age_load_penalty = 1.0 + age_effect * ((1.0 - load_factor) + (1.0 if is_older else -0.5))
        p_store = np.clip(p_slot3 * load_factor / max(age_load_penalty, 1e-6), 0.0, 1.0)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # WM decay toward zero
            WM = (1.0 - rho) * WM

            # WM policy: categorical over WM strengths in the current state
            wm_sum = np.sum(WM[s, :])
            if wm_sum > 1e-12:
                p_wm_vec = WM[s, :] / wm_sum
                p_wm_a = p_wm_vec[a]
                wm_weight = np.clip(wm_sum, 0.0, 1.0)  # more total WM strength => stronger reliance
            else:
                p_wm_vec = np.ones(nA) / nA
                p_wm_a = p_wm_vec[a]
                wm_weight = 0.0

            # RL policy
            prefs = beta * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p_rl_a = 1.0 / max(denom, 1e-12)

            # Mixture of WM and RL
            p_choice = wm_weight * p_wm_a + (1.0 - wm_weight) * p_rl_a
            logp += np.log(max(p_choice, 1e-12))

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM encoding: only reward-consistent storage, probabilistic
            if r >= 0.5:
                # Store rewarded action with probability p_store by boosting its trace toward 1
                WM[s, a] += p_store * (1.0 - WM[s, a])
                # Optionally reduce competing actions slightly to sharpen WM code
                for aa in range(nA):
                    if aa != a:
                        WM[s, aa] *= (1.0 - 0.5 * p_store)
            else:
                # On non-reward, we gently suppress the chosen action trace
                WM[s, a] *= (1.0 - 0.5 * p_store)

        total_logp += logp

    return -total_logp


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Attention-modulated learning and decision temperature with age-by-load effects.

    Idea
    - Each state has a latent attention variable A_s in [0,1] tracking recent surprise (unsigned PE).
    - Learning rate and decision inverse temperature are scaled by A_s: higher attention -> faster learning,
      more exploitation; lower attention -> slower learning, more exploration.
    - Attention updates via leaky integration toward current surprise, with strength modulated by
      set size and age (older/larger sets change how quickly attention adapts).

    Parameters
    ----------
    states : 1D array-like of int
        State index per trial (0..nS-1 within a block).
    actions : 1D array-like of int
        Chosen action per trial (0..2).
    rewards : 1D array-like of int
        Binary reward feedback (0/1).
    blocks : 1D array-like of int
        Block index per trial.
    set_sizes : 1D array-like of int
        Set size for each trial's block (e.g., 3 or 6).
    age : 1D array-like of float or int
        Participant age; age[0] is used.
    model_parameters : tuple/list
        (alpha0, beta, att_drift, k_age_load)
        - alpha0: baseline learning rate (0..1).
        - beta: baseline inverse temperature (>0).
        - att_drift: base rate at which attention tracks surprise (0..1).
        - k_age_load: scales how set size and age modulate attention drift and decision gain.
          Positive => older and larger sets increase drift impact (more volatility).

    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    alpha0, beta, att_drift, k_age_load = model_parameters
    age_val = age[0]
    is_older = (age_val >= 45)

    nA = 3
    total_logp = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_states = states[mask]
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        nS = int(set_sizes[mask][0])

        # Initialize Q and state attention A_s
        Q = (1.0 / nA) * np.ones((nS, nA))
        A = 0.5 * np.ones(nS)  # start with medium attention

        # Age-by-load modulation for attention dynamics and temperature scaling
        load_term = (float(nS) / 3.0) - 1.0  # 0 for 3-set, 1 for 6-set
        age_term = 1.0 if is_older else -0.5
        mod = 1.0 + k_age_load * (load_term + age_term)

        logp = 0.0
        for t in range(len(block_states)):
            s = int(block_states[t])
            a = int(block_actions[t])
            r = float(block_rewards[t])

            # Effective learning rate and temperature from attention
            alpha_eff = np.clip(alpha0 * (0.5 + A[s]), 1e-6, 1.0)
            beta_eff = max(beta * (0.5 + A[s]), 1e-6)

            # Softmax probability for chosen action
            prefs = beta_eff * Q[s, :]
            denom = np.sum(np.exp(prefs - prefs[a]))
            p = 1.0 / max(denom, 1e-12)
            logp += np.log(max(p, 1e-12))

            # Compute surprise and update Q
            pe = r - Q[s, a]
            Q[s, a] += alpha_eff * pe

            # Update attention toward current surprise (unsigned PE)
            surprise = abs(pe)
            drift_eff = np.clip(att_drift * mod, 0.0, 1.0)
            A[s] = (1.0 - drift_eff) * A[s] + drift_eff * surprise
            # Keep A within [0,1]
            A[s] = np.clip(A[s], 0.0, 1.0)

        total_logp += logp

    return -total_logp