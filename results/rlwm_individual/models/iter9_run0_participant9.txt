def cognitive_model1(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with asymmetric learning rates, choice stickiness, and capacity-limited WM mixture.

    Idea:
    - RL: delta-rule with separate learning rates for positive vs. negative prediction errors
      and a choice-stickiness bias (per block) added to the RL policy.
    - WM: fast, reward-tagged table; arbitration weight is reduced by set size via a capacity
      ratio and reduced further when WM is uncertain (high entropy).
    - Arbitration: trial-wise mixture weight = wm_weight_base * capacity(nS) * (1 - normalized entropy).

    Parameters (6 total):
    - lr_pos: RL learning rate for positive prediction errors (0..1).
    - wm_weight_base: Base WM mixture weight before capacity/entropy modulation (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10 for sensitivity.
    - lr_neg: RL learning rate for negative prediction errors (0..1).
    - capacity_c: WM capacity (in number of state-action pairs effectively usable; >0).
    - stickiness: Choice stickiness added to the last chosen action in Q-policy (can be +/-).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, wm_weight_base, softmax_beta, lr_neg, capacity_c, stickiness = model_parameters
    softmax_beta *= 10.0  # per template
    softmax_beta_wm = 50.0  # very deterministic WM channel
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -1  # for stickiness within block

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy with stickiness
            Q_s = q[s, :].copy()
            if last_action >= 0 and last_action < nA:
                Q_s[last_action] += stickiness
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy and arbitration:
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity modulation: effective WM share scales with min(1, C/nS)
            cap_ratio = min(1.0, float(capacity_c) / max(1.0, float(nS)))

            # Uncertainty modulation: entropy of WM state distribution
            Ps = W_s / max(eps, np.sum(W_s))
            H = -np.sum(Ps * np.log(max(eps, 1.0) * Ps + eps))
            Hmax = np.log(nA)
            conf = 1.0 - (H / max(eps, Hmax))  # 0 when uniform, 1 when one-hot

            wm_weight = np.clip(wm_weight_base * cap_ratio * conf, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            delta = r - q[s, a]
            if delta >= 0:
                q[s, a] += lr_pos * delta
            else:
                q[s, a] += lr_neg * delta

            # WM update: reward-tagged fast sharpening at state s
            # Move a fraction of mass toward the chosen action if rewarded,
            # otherwise mildly suppress it (toward uniform prior).
            if r > 0.5:
                # Sharpen at chosen action
                gain = 0.8  # strong, but implicit (no extra parameter)
                w[s, :] = (1.0 - gain) * w[s, :] + gain * w_0[s, :]
                w[s, a] += gain  # push chosen action up
                # renormalize row
                w[s, :] = np.maximum(w[s, :], 0.0)
                w[s, :] /= np.sum(w[s, :]) + eps
            else:
                # small relaxation toward uniform prior when error occurs
                leak = 0.2
                w[s, :] = (1.0 - leak) * w[s, :] + leak * w_0[s, :]

            last_action = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL with Q-forgetting + confirmatory WM and lapse.

    Idea:
    - RL: delta-rule with per-trial forgetting of Q-values toward uniform (captures load/decay).
    - WM: confirmatory store counting successful hits per state-action; WM reliability scales with
      the max hit count relative to a confirmation threshold.
    - Arbitration: WM weight decreases with set size (inverse with nS) and increases with WM confirmation.
      Final choice is mixed with a lapse to uniform.

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight0: Base WM weight before set-size and confirmation scaling (0..1).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - q_forget: Per-trial Q forgetting rate toward uniform (0..1).
    - confirm_thresh: WM confirmation threshold (>=1); higher requires more hits to be confident.
    - lapse: Lapse probability mixing the final policy with uniform (0..1).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, q_forget, confirm_thresh, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM hit counters per state-action
        hits = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy from confirmatory counts (soft preference for higher hit action)
            # Convert hits into a distribution by adding a small prior (from w_0).
            W_s = hits[s, :].copy()
            W_s = W_s + 1e-6  # avoid all-zero
            W_s = W_s / (np.sum(W_s) + eps)
            # Blend with prior template
            W_s = 0.5 * W_s + 0.5 * w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration weight:
            # - Set-size scaling: inverse with nS
            ns_scale = 1.0 / max(1.0, float(nS))
            # - Confirmation scaling: relative to threshold
            conf = min(1.0, np.max(hits[s, :]) / max(1.0, float(confirm_thresh)))
            wm_weight = np.clip(wm_weight0 * ns_scale * conf, 0.0, 1.0)

            # Mixture with lapse to uniform
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update with forgetting toward uniform
            delta = r - q[s, a]
            q[s, a] += lr * delta
            # apply global small forgetting toward uniform prior
            q = (1.0 - q_forget) * q + q_forget * (1.0 / nA) * np.ones_like(q)

            # WM updates: increment hits on rewarded actions, small decay otherwise
            if r > 0.5:
                hits[s, a] += 1.0
                # Make w row reflect this sharper association as well
                w[s, :] = 0.8 * w[s, :] + 0.2 * w_0[s, :]
                w[s, a] += 0.2
                w[s, :] = np.maximum(w[s, :], 0.0)
                w[s, :] /= (np.sum(w[s, :]) + eps)
            else:
                # mild decay of hits and WM toward prior
                hits[s, :] *= 0.9
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, model_parameters):
    """
    RL + WM with value-based arbitration and interference scaling with set size.

    Idea:
    - RL: standard delta-rule.
    - WM: rapid learning toward the rewarded action with learning rate wm_eta.
    - Interference: WM table decays toward uniform at a rate that increases with set size and
      a base interference parameter.
    - Arbitration: Trial-wise WM weight is a logistic function of a baseline term, a set-size term,
      and the current advantage of WM over RL (max(W_s) - max(Q_s)).

    Parameters (6 total):
    - lr: RL learning rate (0..1).
    - wm_weight0: Baseline term controlling WM tendency in the logistic arbitration (can be +/-).
    - softmax_beta: RL inverse temperature; internally scaled by 10.
    - wm_eta: WM learning rate for moving the state row toward the chosen action on reward (0..1).
    - wm_interference: Base WM interference/decay toward uniform per trial (0..1).
    - ns_slope: Sensitivity of arbitration to set size (negative values reduce WM at larger nS).

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight0, softmax_beta, wm_eta, wm_interference, ns_slope = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    nA = 3
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask].astype(int)
        block_rewards = rewards[mask].astype(float)
        block_states = states[mask].astype(int)
        block_set_sizes = set_sizes[mask].astype(int)

        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            W_s = w[s, :].copy()
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: logistic of baseline + set-size + value-advantage
            Vadv = np.max(W_s) - np.max(Q_s)  # WM advantage over RL at this state
            z = wm_weight0 + ns_slope * float(nS) + Vadv
            wm_weight = 1.0 / (1.0 + np.exp(-z))
            wm_weight = float(np.clip(wm_weight, 0.0, 1.0))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, eps)
            log_p += np.log(p_total)

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM interference: decay entire table toward uniform more when nS is larger
            inter_rate = np.clip(wm_interference * (float(nS) / max(1.0, float(nS))), 0.0, 1.0)
            w = (1.0 - inter_rate) * w + inter_rate * w_0

            # WM learning: if rewarded, move row s toward one-hot on chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_eta) * w[s, :] + wm_eta * target

        blocks_log_p += log_p

    return -blocks_log_p