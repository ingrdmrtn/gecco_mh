def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using:
    Asymmetric RL with novelty bonus and simple recall, with age- and load-modulated exploration.

    Inputs
    - stimulus: array of int, state index per trial.
    - blocks: array of int, block index per trial.
    - set_sizes: array of int, set size for each trial's block.
    - correct_answer: array of int, correct action per trial (0..2).
    - age: float, participant age in years.

    Returns
    - simulated_actions: array of int (0..2)
    - simulated_rewards: array of int (0/1)
    """

    alpha_pos, alpha_neg, beta_base, novelty_bonus, age_explore_penalty, load_temp_penalty = parameters


    n_trials = len(stimulus)
    nA = 3

    # Age group flag
    age_group = 1.0 if age >= 45 else 0.0

    # Guard/transform as in fitting model
    beta_base = max(beta_base, 1e-6) * 3.0
    novelty_bonus = max(novelty_bonus, 1e-6)
    age_explore_penalty = max(0.0, age_explore_penalty)
    load_temp_penalty = max(0.0, load_temp_penalty)

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Trial counter over the whole session
    tr = 0

    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx]
        block_correct = correct_answer[block_idx]
        nS = int(set_sizes[block_idx][0])

        # Map each state in this block to its correct action (assumes stationary mapping)
        # If states are 0..nS-1 within block, this picks the first seen correct action for each state
        state_ids = np.unique(block_states).astype(int)
        max_state = int(state_ids.max())
        # Ensure arrays size nS, states assumed contiguous 0..nS-1
        correct_by_state = np.zeros(nS, dtype=int)
        for s in range(nS):
            first_idx = np.where(block_states == s)[0][0]
            correct_by_state[s] = int(block_correct[first_idx])

        # Initialize RL variables
        Q = (1.0 / nA) * np.ones((nS, nA))
        visits = np.zeros((nS, nA))
        last_rewarded = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])

            # Load index in [0, ~1] with 3 as baseline set size
            load = (max(nS, 3) - 3) / 3.0

            # Effective temperature with load and age penalties (as in fitting)
            beta_eff = beta_base
            beta_eff *= np.exp(-load_temp_penalty * load)
            beta_eff *= np.exp(-age_group * age_explore_penalty)
            beta_eff = max(beta_eff, 1e-6) * 3.0

            # Novelty bonus scales with age (younger preserve more novelty)
            novelty_scale = novelty_bonus * (1.0 - 0.5 * age_group)
            bon = novelty_scale / (1.0 + visits[s, :])

            # RL policy
            logits = beta_eff * (Q[s, :] + bon)
            logits = logits - np.max(logits)
            p_rl = np.exp(logits)
            p_rl = p_rl / np.sum(p_rl)

            # Recall mechanism: bias toward last rewarded action in state s
            if last_rewarded[s] >= 0:
                recall_base = 1.0 / (1.0 + np.exp(-novelty_scale))  # sigmoid of novelty scale
                recall_load = np.exp(-2.0 * load)                   # more load, less recall
                recall_age = 1.0 / (1.0 + age_group * age_explore_penalty)  # older, less recall
                p_recall = np.clip(recall_base * recall_load * recall_age, 0.0, 1.0)

                eps = 1e-6
                p_recall_vec = np.full(nA, eps)
                p_recall_vec[last_rewarded[s]] = 1.0 - (nA - 1) * eps
                # Mixture and renormalize for safety
                p_mix = p_recall * p_recall_vec + (1.0 - p_recall) * p_rl
                p_mix = p_mix / np.sum(p_mix)
            else:
                p_mix = p_rl

            # Sample action
            a = int(np.random.choice(nA, p=p_mix))
            simulated_actions[block_idx[t]] = a

            # Reward from correct mapping
            r = 1 if a == correct_by_state[s] else 0
            simulated_rewards[block_idx[t]] = r

            # Book-keeping
            visits[s, a] += 1.0

            # Asymmetric RL update
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # Update recall cache on reward
            if r > 0.5:
                last_rewarded[s] = a

            tr += 1

    return simulated_actions, simulated_rewards