def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards for an RL model with set-size-specific learning rates,
    state-wise perseveration, and age-dependent exploration.

    This simulation follows the same logic as the provided fitting model (cognitive_model2):
      - Separate learning rates for small vs large set sizes
      - Softmax policy over Q-values with a perseveration bias toward repeating the last action in a state
      - Older adults have reduced inverse temperature (more exploratory)
      - Value decay increases with set size

    Parameters
    ----------
    stimulus : array-like of int
        State index on each trial (0..nS-1 within a block).
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size for each trial's block (e.g., 3 or 6). Assumed constant within a block.
    correct_answer : array-like of int
        Correct action on each trial (0..nA-1). Assumed constant within a state inside a block.
    age : float
        Participant age in years. Younger: age < 45, Older: age >= 45.

    Returns
    -------
    simulated_actions : np.ndarray of int
        Simulated chosen action on each trial (0..nA-1).
    simulated_rewards : np.ndarray of int
        Simulated binary feedback on each trial (0 or 1).
    """


    alpha_small, alpha_large, beta_base, perseveration_kappa, age_beta_bonus, decay_base = parameters

    n_trials = len(stimulus)
    nA = 3

    age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Process each block independently
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_idx = np.where(blocks == b)[0]
        # Preserve within-block trial order
        block_idx.sort()
        b_states = np.array(stimulus)[block_idx]
        b_correct = np.array(correct_answer)[block_idx]
        b_set_sizes = np.array(set_sizes)[block_idx]
        nS = int(b_set_sizes[0])  # assumed constant set size within block

        # Choose learning rate by set size
        alpha = alpha_small if nS <= 3 else alpha_large

        # Inverse temperature with age adjustment
        beta = max(1e-6, beta_base) * 10.0
        beta = beta * (1.0 - is_older * np.clip(age_beta_bonus, 0.0, 1.0))
        beta = max(1e-6, beta)

        # Initialize Q-values and perseveration memory
        Q = np.zeros((nS, nA))
        last_action = -np.ones(nS, dtype=int)

        # Effective decay increases with set size beyond 3
        eff_decay = np.clip(decay_base, 0.0, 1.0) * max(0.0, (nS - 3) / 3.0)

        # Optional: map each state to its correct action in this block
        # (assumes state numbering is 0..nS-1 within block)
        # If stimuli are labeled 0..nS-1 per block, this works directly.
        # If not, we construct a mapping based on first occurrence.
        state_to_correct = {}
        for s_val in np.unique(b_states):
            first_idx = np.where(b_states == s_val)[0][0]
            state_to_correct[int(s_val)] = int(b_correct[first_idx])

        # Simulate trials in this block
        for t_local, t_global in enumerate(block_idx):
            s = int(b_states[t_local])
            correct_a = state_to_correct[s]

            # Action preferences: Q plus perseveration bias on last action for this state
            pref = Q[s, :].copy()
            if last_action[s] >= 0:
                pref[last_action[s]] += perseveration_kappa

            # Softmax choice
            pref_centered = pref - np.max(pref)
            expp = np.exp(beta * pref_centered)
            p_all = expp / np.sum(expp)

            a = int(np.random.choice(nA, p=p_all))
            r = 1 if a == correct_a else 0

            simulated_actions[t_global] = a
            simulated_rewards[t_global] = r

            # RL update
            Q[s, a] += alpha * (r - Q[s, a])

            # Decay toward zero, stronger in larger set sizes
            Q[s, :] *= (1.0 - eff_decay)

            # Update perseveration memory
            last_action[s] = a

    return simulated_actions, simulated_rewards