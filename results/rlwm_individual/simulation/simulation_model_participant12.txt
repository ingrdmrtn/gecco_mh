def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL+WM mixture with capacity- and decay-limited working memory, modulated by set size and age.
    Simulation counterpart of cognitive_model1.

    Inputs:
    - stimulus: array of state indices per trial (ints; expected 0..nS-1 within block)
    - blocks: array of block indices per trial (ints); learning resets across blocks
    - set_sizes: array of set sizes per trial (ints; typically constant within a block)
    - correct_answer: array of correct actions per trial (ints in 0..2)
    - age: participant's age (float)

    Returns:
    - simulated_actions: array of sampled actions (ints in 0..2)
    - simulated_rewards: array of binary rewards (0/1)
    """

    alpha, beta, C, rho, w0, eps = parameters

    # Internal preprocessing to match fitting code behavior
    beta = max(1e-6, beta) * 10.0
    eps = np.clip(eps, 0.0, 0.5)
    alpha = np.clip(alpha, 0.0, 1.0)
    rho = np.clip(rho, 0.0, 1.0)
    C = max(1e-6, C)
    w0 = np.clip(w0, 0.0, 1.0)

    # Age effect on WM weight
    younger = 1 if age < 45 else 0
    age_wm_boost = 1.20 if younger else 0.85

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3

    # Iterate over blocks and simulate trial-by-trial choices and outcomes
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_idx = np.where(block_mask)[0]
        block_states = stimulus[block_mask]
        block_set_sizes = set_sizes[block_mask]
        block_correct = correct_answer[block_mask]

        # Determine number of states in this block
        # In the fitting code, nS is set to the block set size
        ss_block = int(block_set_sizes[0]) if len(block_set_sizes) > 0 else (int(np.max(block_states)) + 1)
        nS = ss_block

        # For each state, get its correct action (assumes consistent mapping within block)
        correct_by_state = np.zeros(nS, dtype=int)
        for s in range(nS):
            # If a state s might not appear (edge case), default to 0
            s_trials = np.where(block_states == s)[0]
            if len(s_trials) > 0:
                correct_by_state[s] = int(block_correct[s_trials[0]])
            else:
                correct_by_state[s] = 0

        # Initialize RL and WM
        Q = np.zeros((nS, nA))            # RL action values
        W = np.ones((nS, nA)) / nA        # WM policy store (probabilities)

        # Simulate each trial in block
        for local_t, tr in enumerate(block_idx):
            s = int(block_states[local_t])
            ss = int(block_set_sizes[local_t]) if local_t < len(block_set_sizes) else nS

            # WM decay toward uniform for current state
            W[s, :] = (1 - rho) * W[s, :] + rho * (1.0 / nA)

            # RL softmax policy
            Qs = Q[s, :]
            Qs_centered = Qs - np.max(Qs)
            pi_rl = np.exp(beta * Qs_centered)
            sum_pi_rl = np.sum(pi_rl)
            if sum_pi_rl <= 0 or not np.isfinite(sum_pi_rl):
                pi_rl = np.ones(nA) / nA
            else:
                pi_rl /= sum_pi_rl

            # WM policy is current W row (normalized)
            pi_wm = W[s, :].copy()
            pi_wm = np.maximum(pi_wm, 1e-8)
            pi_wm /= np.sum(pi_wm)

            # Effective WM mixture weight with capacity and age modulation
            cap_factor = min(1.0, C / max(1, ss))
            w_eff = np.clip(w0 * cap_factor * age_wm_boost, 0.0, 1.0)

            # Mixture and lapse
            pi_mix = (1.0 - w_eff) * pi_rl + w_eff * pi_wm
            pi_mix = (1.0 - eps) * pi_mix + eps * (1.0 / nA)
            pi_mix = np.maximum(pi_mix, 1e-12)
            pi_mix /= np.sum(pi_mix)

            # Sample action
            a = int(np.random.choice(nA, p=pi_mix))
            simulated_actions[tr] = a

            # Generate reward based on correct action
            correct_a = int(correct_by_state[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update on rewarded trials (store last correct response with decay structure)
            if r >= 0.5:
                # As in fitting code: concentrate mass on chosen action with decay component
                W[s, :] = (1 - rho) * 0.0
                W[s, a] = (1 - rho) * 1.0 + rho * (1.0 / nA)
                # Normalize for safety
                W[s, :] = np.maximum(W[s, :], 1e-8)
                W[s, :] /= np.sum(W[s, :])

    return simulated_actions, simulated_rewards