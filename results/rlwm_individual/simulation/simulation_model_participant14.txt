def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL+WM mixture with age- and set-sizeâ€“modulated working-memory weight and perseveration.

    Parameters:
    - stimulus: sequence of state identities (0-indexed within a block) for each trial (array-like)
    - blocks: block index for each trial (array-like)
    - set_sizes: set size (number of distinct states in the current block) for each trial (array-like)
    - correct_answer: correct action for each trial (0,1,2) (array-like)
    - age: participant age (float or single-element array)

    Returns:
    - simulated_actions: simulated action choices (array of int)
    - simulated_rewards: simulated binary rewards (array of int)
    """


    alpha, beta, wm_weight_base, wm_decay, stickiness, gamma_setsize = parameters

    beta *= 10.0

    # Age handling and age factor
    try:
        age_val = float(age[0])  # if array-like with one element
    except Exception:
        age_val = float(age)     # if scalar
    is_younger = 1.0 if age_val < 45 else 0.0
    age_wm_factor = 1.0 + 0.3 * (is_younger - (1.0 - is_younger))  # +0.3 if younger, -0.3 if older

    stimulus = np.asarray(stimulus, dtype=int)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)
    correct_answer = np.asarray(correct_answer, dtype=int)

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx]
        block_set_sizes = set_sizes[block_idx]
        block_correct = correct_answer[block_idx]

        nA = 3
        # Use set size provided for each trial; assume first entry represents the block set size
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))  # uniform

        prev_action = None

        for t in range(len(block_states)):
            s = int(block_states[t])
            ss = float(block_set_sizes[t])

            # Set-size scaling of WM weight
            ss_factor = (3.0 / ss) ** max(0.0, gamma_setsize)
            wm_weight = np.clip(wm_weight_base * age_wm_factor * ss_factor, 0.0, 1.0)

            # RL policy with stickiness
            prefs_rl = beta * Q[s, :].copy()
            if prev_action is not None:
                prefs_rl[prev_action] += stickiness
            max_pref_rl = np.max(prefs_rl)
            pi_rl = np.exp(prefs_rl - max_pref_rl)
            pi_rl /= np.sum(pi_rl)

            # WM policy with stronger inverse temperature and stickiness
            beta_wm = 2.0 * beta
            prefs_wm = beta_wm * W[s, :].copy()
            if prev_action is not None:
                prefs_wm[prev_action] += stickiness
            max_pref_wm = np.max(prefs_wm)
            pi_wm = np.exp(prefs_wm - max_pref_wm)
            pi_wm /= np.sum(pi_wm)

            # Mixture policy
            pi = wm_weight * pi_wm + (1.0 - wm_weight) * pi_rl
            pi = np.clip(pi, 1e-12, 1.0)
            pi /= np.sum(pi)

            # Sample action
            a = int(np.random.choice(nA, p=pi))
            simulated_actions[block_idx[t]] = a

            # Reward based on correct answer for this trial
            r = 1 if a == int(block_correct[t]) else 0
            simulated_rewards[block_idx[t]] = r

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM decay toward uniform
            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)

            # If rewarded, store deterministic association in WM for that state
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0
            else:
                # No explicit additional update on errors beyond decay (to match fitting code)
                pass

            # Normalize WM row (defensive; should already sum to 1 after rewarded update)
            row_sum = np.sum(W[s, :])
            if row_sum > 0:
                W[s, :] /= row_sum

            prev_action = a
            tr += 1

    return simulated_actions, simulated_rewards