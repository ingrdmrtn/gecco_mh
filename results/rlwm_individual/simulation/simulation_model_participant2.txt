def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards from the count-based uncertainty bonus model with
    age-modulated directed exploration, load-dependent interference, and perseveration.

    Inputs:
    - stimulus: array of state indices per trial (0..nS-1 within each block)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (should be constant within a block; typically 3 or 6)
    - correct_answer: array of correct action (0..2) per trial
    - age: participant age (float)

    Returns:
    - simulated_actions: array of simulated actions (0..2) per trial
    - simulated_rewards: array of simulated rewards (0/1) per trial
    """

    alpha, beta0, bonus0, age_bonus_shift, size_forget, perseveration = parameters

    n_trials = len(stimulus)
    nA = 3

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    age_group = 1 if age >= 45 else 0  # 1=older, 0=younger

    tr = 0
    for b in np.unique(blocks):
        idx = np.where(blocks == b)[0]
        b_states = stimulus[idx].astype(int)
        b_corr = correct_answer[idx].astype(int)
        b_set_sizes = set_sizes[idx].astype(int)
        nS = int(b_set_sizes[0])

        # Map each within-block state to its correct action (assumes stable within block)
        state_correct = np.zeros(nS, dtype=int)
        for s in range(nS):
            first_match = np.where(b_states == s)[0]
            if len(first_match) > 0:
                state_correct[s] = b_corr[first_match[0]]
            else:
                state_correct[s] = 0  # fallback (shouldn't happen)

        # Initialize Q-values, visit counts, and last action per state
        Q = np.zeros((nS, nA))
        N = np.zeros((nS, nA))
        last_act = -1 * np.ones(nS, dtype=int)

        beta_eff = 10.0 * beta0
        load_level = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        bonus_eff = bonus0 + age_bonus_shift * (0.5 - age_group)
        pers_eff = perseveration * (1.0 - 0.5 * load_level)
        decay_rate = np.clip(size_forget * load_level, 0.0, 0.5)

        for t in range(len(b_states)):
            s = b_states[t]

            # Apply load-dependent interference (decay) to Q for currently visited state
            Q[s, :] *= (1.0 - decay_rate)

            # Epistemic uncertainty bonus U(s,a) = 1/sqrt(N+1)
            U = 1.0 / np.sqrt(N[s, :] + 1.0)

            # Compute logits with Q, bonus, and perseveration
            logits = beta_eff * Q[s, :] + bonus_eff * U
            if last_act[s] >= 0:
                logits[last_act[s]] += pers_eff

            # Softmax policy
            logits = logits - np.max(logits)
            pol = np.exp(logits)
            pol = pol / np.sum(pol)

            # Sample action
            a = int(np.random.choice(nA, p=pol))
            simulated_actions[tr] = a

            # Generate reward based on correct action
            r = 1 if a == state_correct[s] else 0
            simulated_rewards[tr] = r

            # Update visit counts and Q-learning
            N[s, a] += 1.0
            Q[s, a] += alpha * (r - Q[s, a])

            # Update perseveration memory
            last_act[s] = a

            tr += 1

    return simulated_actions, simulated_rewards