def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards from the RL model with asymmetrical learning rates
    and age/load-modulated perseveration bias (matching cognitive_model2 logic).

    Inputs:
    - stimulus: array of state indices per trial (0..nS-1 within each block)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (e.g., 3 or 6); first value in a block gives nS
    - correct_answer: array of correct action per trial (0,1,2)
    - age: participant age (float or array-like)

    Returns:
    - simulated_actions: array of sampled actions (0,1,2)
    - simulated_rewards: array of rewards (0/1)
    """

    alpha_pos, alpha_neg, beta, kappa_base, kappa_age_slope, lapse = parameters

    # Match fitting code's transforms/bounds
    beta = max(1e-6, beta) * 5.0
    alpha_pos = min(max(alpha_pos, 0.0), 1.0)
    alpha_neg = min(max(alpha_neg, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    nA = 3
    eps = 1e-12

    age_val = age[0] if hasattr(age, "__len__") else float(age)
    older = 1.0 if age_val >= 45 else 0.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_idx = np.where(blocks == b)[0]
        block_states = np.asarray(stimulus)[block_idx]
        block_correct = np.asarray(correct_answer)[block_idx]
        block_set_sizes = np.asarray(set_sizes)[block_idx]

        # Number of states in this block (as in fitting code)
        nS = int(block_set_sizes[0])

        # Map each state to its correct action (take the first occurrence)
        # Assumes states are labeled 0..nS-1 within the block.
        correct_by_state = np.zeros(nS, dtype=int)
        for s in range(nS):
            first_occ = np.where(block_states == s)[0]
            if len(first_occ) == 0:
                # If a state index doesn't appear (shouldn't happen), default to 0
                correct_by_state[s] = 0
            else:
                correct_by_state[s] = int(block_correct[first_occ[0]])

        # Initialize Q-values
        Q = np.zeros((nS, nA))
        prev_action = None

        # Iterate trials in this block
        for t in range(len(block_states)):
            s = int(block_states[t])
            load_factor = float(block_set_sizes[t]) / 6.0  # 0.5 for set size 3; 1.0 for 6

            # Perseveration strength with age and load modulation
            kappa = kappa_base + kappa_age_slope * older
            kappa_eff = kappa * load_factor

            # Build perseveration bonus vector
            pers = np.zeros(nA)
            if prev_action is not None and 0 <= prev_action < nA:
                pers[prev_action] = kappa_eff

            # Softmax over Q + perseveration
            vals = Q[s] + pers
            m = np.max(beta * vals)
            expv = np.exp(beta * vals - m)
            pi = expv / np.sum(expv)
            # Lapse mixing with uniform
            pi = (1.0 - lapse) * pi + lapse * (np.ones(nA) / nA)
            # Numerical safety
            pi = np.maximum(pi, eps)
            pi = pi / np.sum(pi)

            # Sample action
            a = int(np.random.choice(nA, p=pi))
            simulated_actions[tr + t] = a

            # Generate reward from ground-truth correct action
            correct_a = int(correct_by_state[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr + t] = r

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0 else alpha_neg
            Q[s, a] += lr * pe

            # Update perseveration memory
            prev_action = a

        tr += len(block_states)

    return simulated_actions, simulated_rewards