def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards under Model 1: RL + rewarded-episode retrieval with set-size and age-modulated retrieval probability and decay.

    Parameters:
    - stimulus: array of state identifiers per trial (can be any integers; remapped within block)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (e.g., 3 or 6); assumed constant within a block
    - correct_answer: array of correct action per trial (0..2). If constant per state within a block, repeated across trials of that state.
    - age: participant's age (float)
    - parameters: list or tuple of 6 parameters in order:
        alpha, beta, decay_base, retr_base, age_retr_shift, ss_retr_slope

    Returns:
    - simulated_actions: array of simulated action choices (0..2) per trial
    - simulated_rewards: array of simulated binary rewards (0/1) per trial
    """
    import numpy as np

    nA = 3
    eps = 1e-12

    alpha, beta, decay_base, retr_base, age_retr_shift, ss_retr_slope = parameters
    beta = 5.0 * max(beta, eps)

    is_older = 1.0 if age >= 45 else 0.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Iterate over blocks
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states_raw = stimulus[block_idx]
        block_correct = correct_answer[block_idx]
        # Determine set size from input (assumed constant within block)
        nS = int(set_sizes[block_idx][0])

        # Map observed state labels within block to 0..nS-1
        unique_states = np.unique(block_states_raw)
        # If unique_states size differs from nS (due to missing states), still map whatever appears
        state_to_idx = {s: i for i, s in enumerate(unique_states)}
        # Safety: if fewer unique states than nS, we still size Q/M by nS, but only accessed indices that appear

        # Initialize RL and episodic memory arrays
        Q = np.zeros((nS, nA))
        M = np.zeros((nS, nA))

        # Set-size and age dependent parameters for this block
        ss_factor = (nS - 3) / 3.0  # 0 for 3, 1 for 6
        decay = np.clip(decay_base * (1.0 + 0.5 * ss_factor) * (1.0 + 0.25 * is_older), 0.0, 1.0)
        logit_rho = retr_base - ss_retr_slope * ss_factor + age_retr_shift * is_older
        rho = 1.0 / (1.0 + np.exp(-logit_rho))

        # Trial loop within block
        for t_local, tr in enumerate(block_idx):
            # State index for arrays
            s_raw = block_states_raw[t_local]
            # If an unseen state occurs and exceeds nS, map modulo (very rare); else map using dict
            s = state_to_idx.get(s_raw, None)
            if s is None:
                # Fallback: assign next available index if space remains, else wrap
                if len(state_to_idx) < nS:
                    s = len(state_to_idx)
                    state_to_idx[s_raw] = s
                else:
                    s = int(hash(s_raw) % nS)

            # Apply decay to episodic memory (global per trial)
            if decay > 0:
                M *= (1.0 - decay)

            # Compute RL policy
            q_s = Q[s, :].copy()
            q_s -= np.max(q_s)
            p_rl = np.exp(beta * q_s)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # Compute episodic retrieval policy
            m_s = M[s, :].copy()
            m_s -= np.max(m_s)
            p_ep = np.exp(beta * m_s)
            p_ep = p_ep / (np.sum(p_ep) + eps)

            # Mixture policy
            p = rho * p_ep + (1.0 - rho) * p_rl
            p = np.clip(p, eps, 1.0)
            p = p / np.sum(p)

            # Sample action
            a = int(np.random.choice(nA, p=p))
            simulated_actions[tr] = a

            # Simulate reward from correct answer on this trial
            correct_a = int(block_correct[t_local])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Episodic memory update on rewarded trials
            if r > 0.5:
                M[s, :] *= 0.5   # partial suppression of competitors
                M[s, a] = 1.0    # mark successful action strongly

    return simulated_actions, simulated_rewards