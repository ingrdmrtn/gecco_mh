def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    RL + Bayesian WM with load- and surprise-adaptive arbitration (simulation).

    Parameters:
    - stimulus: sequence of stimuli (state indices) for each trial (array-like, int)
    - blocks: block index for each trial (array-like, int)
    - set_sizes: set size for each trial's block (array-like, int) [assumed constant within block]
    - correct_answer: correct action for each trial (array-like, int in {0,1,2})
    - parameters: [lr, wm_weight0, softmax_beta, wm_precision, surprise_gain, load_sens]

    Returns:
    - simulated_actions: simulated action choices (array of int)
    - simulated_rewards: simulated binary rewards (array of int)
    """
    lr, wm_weight0, softmax_beta, wm_precision, surprise_gain, load_sens = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM policy over its action probs

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices].astype(int)
        block_correct = correct_answer[block_indices].astype(int)

        nA = 3
        # Use provided set_sizes (assumed constant within block) to match fitting logic
        nS = int(set_sizes[block_indices[0]])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))  # unused but kept for parity

        # Surprise memory per state (causal proxy): updated after outcomes, used on next visit
        surprise_state = np.zeros(nS)

        # Load-dependent base WM weight
        load_pen = max(0, nS - 3)
        # Same transformation as in fitting code
        wm_base = 1.0 / (1.0 + np.exp(-np.log(wm_weight0 + 1e-9) + np.log(1 - wm_weight0 + 1e-9) - load_sens * load_pen))

        for t_local, t_idx in enumerate(block_indices):
            s = int(block_states[t_local])

            # RL policy
            Q_s = q[s, :]
            q_shift = Q_s - np.max(Q_s)
            p_rl = np.exp(softmax_beta * q_shift)
            p_rl /= np.sum(p_rl)

            # WM policy: Dirichlet-like concentration to probs, then near-deterministic softmax
            conc = np.maximum(w[s, :], 1e-8)
            wm_probs = conc / np.sum(conc)
            wm_shift = wm_probs - np.max(wm_probs)
            p_wm = np.exp(softmax_beta_wm * wm_shift)
            p_wm /= np.sum(p_wm)

            # Arbitration weight with causal surprise (from last visit to state s)
            wm_weight_t = wm_base + surprise_gain * np.abs(surprise_state[s])
            wm_weight_t = np.clip(wm_weight_t, 0.0, 1.0)

            # Mixture policy
            p_total = wm_weight_t * p_wm + (1.0 - wm_weight_t) * p_rl
            p_total = np.maximum(p_total, 1e-12)
            p_total = p_total / np.sum(p_total)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[t_idx] = a

            # Outcome (deterministic mapping from state to correct action)
            correct_a = int(block_correct[t_local])
            r = 1 if a == correct_a else 0
            simulated_rewards[t_idx] = r

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update (Dirichlet-like)
            if r > 0.0:
                w[s, a] += wm_precision
            else:
                spill = wm_precision / max(1, nA - 1)
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += spill

            # Update surprise for this state for next visits (causal use)
            surprise_state[s] = abs(delta)

            tr += 1

    return simulated_actions, simulated_rewards