def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Simulate choices and rewards from the RL + valence-dependent learning + stickiness + WM mixture model.

    Parameters
    ----------
    stimulus : array-like (n_trials,)
        State (stimulus) index for each trial (assumed 0..nS-1 within block).
    blocks : array-like (n_trials,)
        Block index for each trial.
    set_sizes : array-like (n_trials,)
        Set size for each trial (constant within a block).
    correct_answer : array-like (n_trials,)
        Correct action for each trial (0..2).
    parameters : list or array-like of length 6
        [alpha_pos, alpha_neg, wm_weight_base, softmax_beta, tau_stick, wm_decay]

    Returns
    -------
    simulated_actions : np.ndarray (n_trials,)
    simulated_rewards : np.ndarray (n_trials,)
    """
    import numpy as np

    alpha_pos, alpha_neg, wm_weight_base, softmax_beta, tau_stick, wm_decay = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3
    eps = 1e-12

    # Process by block to reset latent states each block
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_indices = np.where(blocks == b)[0]
        block_states = np.array(stimulus)[block_indices].astype(int)
        block_correct = np.array(correct_answer)[block_indices].astype(int)
        # Use provided set size for the block (assumed constant within block)
        nS = int(np.array(set_sizes)[block_indices][0])

        # Map per-state correct action from first occurrence in the block
        correct_actions = np.zeros(nS, dtype=int)
        for s in range(nS):
            idxs = np.where(block_states == s)[0]
            if len(idxs) > 0:
                correct_actions[s] = block_correct[idxs[0]]
            else:
                # Fallback (should not happen if all states appear)
                correct_actions[s] = 0

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        last_action = -1 * np.ones(nS, dtype=int)  # -1 = no prior action in this state

        for local_t, tr in enumerate(block_indices):
            s = int(block_states[local_t])

            # Stickiness: add bias to last action's logit within this state
            Q_s = q[s, :].copy()
            if last_action[s] >= 0:
                Q_s[last_action[s]] += tau_stick

            W_s = w[s, :]

            # Softmax policies
            p_rl_logits = softmax_beta * Q_s
            # numerical stability
            p_rl_logits -= np.max(p_rl_logits)
            p_rl = np.exp(p_rl_logits)
            p_rl /= np.sum(p_rl)

            p_wm_logits = softmax_beta_wm * W_s
            p_wm_logits -= np.max(p_wm_logits)
            p_wm = np.exp(p_wm_logits)
            p_wm /= np.sum(p_wm)

            # WM mixture weight scales with set size: stronger in small sets
            set_size_scale = 3.0 / nS
            wm_weight = np.clip(wm_weight_base * set_size_scale, 0.0, 1.0)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)  # guard
            p_total /= np.sum(p_total)

            # Sample action
            a = np.random.choice(nA, p=p_total)
            simulated_actions[tr] = a

            # Outcome based on correct action
            r = 1 if a == correct_actions[s] else 0
            simulated_rewards[tr] = r

            # RL update with valence-dependent learning rates
            pe = r - q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            q[s, a] += lr * pe

            # WM decay toward uniform, then one-shot write on reward
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0.5:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update last action for stickiness
            last_action[s] = a

    return simulated_actions, simulated_rewards