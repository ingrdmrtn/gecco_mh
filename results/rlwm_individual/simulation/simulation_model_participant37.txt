def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Bayesian RL (Beta-Bernoulli) with WM win-stay/lose-shift heuristic and capacity gating.

    Parameters:
    - stimulus: sequence of stimuli for each trial (array-like)
    - blocks: block index for each trial (array-like)
    - set_sizes: block-dependent set sizes for each trial (array-like)
    - correct_answer: correct action on each trial (array-like)
    - parameters: [prior_a, prior_b, softmax_beta, wm_base, lapse, capacity_K]

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """
    import numpy as np

    prior_a, prior_b, softmax_beta, wm_base, lapse, capacity_K = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    stimulus = np.asarray(stimulus)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)
    correct_answer = np.asarray(correct_answer)

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3
    tr = 0

    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states_raw = stimulus[block_idx]
        block_correct = correct_answer[block_idx]

        # Map any arbitrary state labels within block to 0..nS-1
        unique_states = np.unique(block_states_raw)
        nS = len(unique_states)
        state_to_idx = {sval: i for i, sval in enumerate(unique_states)}
        block_states = np.array([state_to_idx[sval] for sval in block_states_raw], dtype=int)

        # Per-state correct action (assume consistent across trials)
        per_state_correct = np.zeros(nS, dtype=int)
        for i, sval in enumerate(unique_states):
            first_occ = np.where(block_states == i)[0][0]
            per_state_correct[i] = int(block_correct[first_occ])

        # Initialize RL posteriors and WM preference
        a_post = np.full((nS, nA), prior_a, dtype=float)
        b_post = np.full((nS, nA), prior_b, dtype=float)
        w = (1.0 / nA) * np.ones((nS, nA), dtype=float)  # WM policy state

        # Capacity gating based on block set size
        base_gate = wm_base * min(1.0, capacity_K / max(1.0, nS))

        for t in range(len(block_states)):
            s = block_states[t]

            # RL policy (softmax over expected values from Beta posterior)
            Q_s = a_post[s, :] / (a_post[s, :] + b_post[s, :])
            x_rl = softmax_beta * (Q_s - np.max(Q_s))
            p_rl = np.exp(x_rl)
            p_rl /= np.sum(p_rl)

            # WM policy (softmax over WM preference vector)
            W_s = w[s, :]
            x_wm = softmax_beta_wm * (W_s - np.max(W_s))
            p_wm = np.exp(x_wm)
            p_wm /= np.sum(p_wm)

            # Mixture
            p_total = base_gate * p_wm + (1.0 - base_gate) * p_rl
            p_total = np.clip(p_total, 1e-12, None)
            p_total /= np.sum(p_total)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[block_idx[t]] = a

            # Reward (deterministic correctness)
            r = 1 if a == per_state_correct[s] else 0
            simulated_rewards[block_idx[t]] = r

            # RL update (Beta-Bernoulli posterior)
            a_post[s, a] += r
            b_post[s, a] += (1 - r)

            # WM update: win-stay/lose-shift with lapse, partial overwrite
            if r >= 1:
                wm_pref = np.full(nA, lapse / (nA - 1))
                wm_pref[a] = 1.0 - lapse
            else:
                wm_pref = np.full(nA, (1.0 - lapse) / (nA - 1))
                wm_pref[a] = lapse

            w[s, :] = 0.5 * w[s, :] + 0.5 * wm_pref

            tr += 1

    return simulated_actions, simulated_rewards