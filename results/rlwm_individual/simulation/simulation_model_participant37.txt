def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using the asymmetric learning model with
    surprise-dependent decision noise and age-/load-modulated credit spread.

    Inputs:
    - stimulus: array of state indices per trial (ints)
    - blocks: array of block ids per trial (ints)
    - set_sizes: array of set size (number of states) per trial (ints; constant within a block)
    - correct_answer: array of correct action per trial (ints in [0,2])
    - age: participant age (float)
    - parameters: [alpha_pos, alpha_neg, beta, spread, load_sens, age_fac]

    Returns:
    - simulated_actions: array of chosen actions (ints)
    - simulated_rewards: array of binary rewards (0/1)
    """
    import numpy as np

    alpha_pos, alpha_neg, beta, spread, load_sens, age_fac = parameters
    # Enforce bounds/transformations as in fitting code
    alpha_pos = min(max(alpha_pos, 0.0), 1.0)
    alpha_neg = min(max(alpha_neg, 0.0), 1.0)
    beta = max(1e-6, beta) * 10.0
    spread = max(0.0, spread)
    load_sens = max(0.0, load_sens)
    age_fac = max(0.0, age_fac)

    older = 1 if age > 45 else 0
    nA = 3
    eps = 1e-12

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Iterate by block
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_set_sizes = set_sizes[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)

        # Determine number of states in this block
        nS = int(block_set_sizes[0])
        # Initialize Q-values
        Q = np.zeros((nS, nA))

        # Map each state to its correct action (assumed stable within block)
        correct_map = np.zeros(nS, dtype=int)
        for s in range(nS):
            # take the first occurrence in the block for that state
            pos = np.where(block_states == s)[0]
            if len(pos) > 0:
                correct_map[s] = int(block_correct[pos[0]])
            else:
                # Fallback (should not happen if states appear in the block)
                correct_map[s] = 0

        # Maintain state-specific previous surprise for decision noise
        prev_surprise = np.zeros(nS)

        # Trial loop within block
        for local_t, tr in enumerate(block_idx):
            s = int(block_states[local_t])
            ss = float(block_set_sizes[local_t])

            # Decision noise depends on previous surprise in this state
            beta_eff = beta / (1.0 + prev_surprise[s])

            # Softmax over current Q-values
            logits = beta_eff * Q[s, :]
            m = np.max(logits)
            exps = np.exp(logits - m)
            probs = exps / max(np.sum(exps), eps)

            # Sample action
            a = int(np.random.choice(nA, p=probs))
            simulated_actions[tr] = a

            # Outcome based on correct action for this state
            r = 1 if a == int(correct_map[s]) else 0
            simulated_rewards[tr] = r

            # Compute PE and update Q for chosen action (asymmetric learning rates)
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # Credit spread to non-chosen actions within the same state
            spread_eff = spread * (1.0 + load_sens * ((ss - 3.0) / 3.0)) * (1.0 + age_fac * older)
            if spread_eff > 0:
                others = [aa for aa in range(nA) if aa != a]
                denom = len(others) if len(others) > 0 else 1.0
                for ao in others:
                    Q[s, ao] -= (spread_eff * pe) / denom

            # Update surprise for next decision in this state
            prev_surprise[s] = abs(pe)

    return simulated_actions, simulated_rewards