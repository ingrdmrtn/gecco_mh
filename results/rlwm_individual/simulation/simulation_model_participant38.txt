def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL + capacity-limited working memory mixture with age-modulated WM capacity and lapses (simulation).

    Inputs:
    - stimulus: array of state indices per trial (0..set_size-1 within each block)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (e.g., 3 or 6)
    - correct_answer: array of correct actions per trial (values in {0,1,2}); assumed consistent per state within a block
    - age: participant's age in years (float)

    Returns:
    - simulated_actions: array of simulated choices per trial (values in {0,1,2})
    - simulated_rewards: array of simulated rewards per trial (0/1)
    """

    alpha_rl, beta, wm_weight_base, K_wm, lapse = parameters

    beta *= 10.0
    age_val = float(age)
    is_older = age_val >= 45.0

    # Age effects (as in the fitting code)
    age_capacity_factor = 0.6 if is_older else 1.0
    alpha_rl_eff = alpha_rl * (0.8 if is_older else 1.0)

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3
    eps = 1e-12
    tr = 0

    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)
        block_set_sizes = set_sizes[block_idx].astype(int)

        # Determine number of states in this block
        nS = int(block_set_sizes[0]) if len(block_set_sizes) > 0 else len(np.unique(block_states))

        # Map from state -> correct action (assume stationary within block)
        # If multiple trials per state, take the first occurrence
        correct_by_state = np.zeros(nS, dtype=int)
        seen = np.zeros(nS, dtype=bool)
        for t_local in range(len(block_states)):
            s = block_states[t_local]
            if 0 <= s < nS and not seen[s]:
                correct_by_state[s] = block_correct[t_local]
                seen[s] = True

        # Initialize RL and WM stores
        Q = np.zeros((nS, nA))                 # RL values start at 0
        W = (1.0 / nA) * np.ones((nS, nA))     # WM policy starts uniform

        for t_local in range(len(block_states)):
            s = int(block_states[t_local])
            nS_curr = int(block_set_sizes[t_local])

            # Effective WM weight given set size and age
            K_eff = max(0.0, K_wm * age_capacity_factor)
            wm_weight = wm_weight_base * min(1.0, K_eff / max(1.0, float(nS_curr)))

            # RL policy
            q_s = Q[s, :]
            pref = beta * (q_s - np.max(q_s))
            exp_pref = np.exp(pref)
            p_rl = exp_pref / (np.sum(exp_pref) + eps)

            # WM policy (directly uses W row as probabilities)
            p_wm = W[s, :]

            # Mixture and lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_final = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p_final))
            simulated_actions[tr] = a

            # Outcome given correct action for this state
            correct_a = int(correct_by_state[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - q_s[a]
            Q[s, a] = q_s[a] + alpha_rl_eff * delta

            # WM update: store rewarded action deterministically; otherwise reset to uniform
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0
            else:
                W[s, :] = (1.0 / nA)

            tr += 1

    return simulated_actions, simulated_rewards