def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using Model 2:
    Model-free RL with action-generalization across states, age-modulated generalization,
    and load-sensitive forgetting.

    Inputs:
    - stimulus: array of state indices per trial (ints starting at 0)
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (e.g., 3 or 6)
    - correct_answer: array of correct action per trial (0..2)
    - age: participant age (float or array-like with 1 number)
    - parameters: [alpha, beta, gen_gain, age_gen_boost, rho_forget]

    Returns:
    - simulated_actions: array of chosen actions (0..2)
    - simulated_rewards: array of binary rewards (0/1)
    """
    import numpy as np

    alpha, beta, gen_gain, age_gen_boost, rho_forget = parameters
    nA = 3
    beta_eff = max(1e-6, beta) * 10.0

    # Age handling
    try:
        age_val = float(age[0])
    except (TypeError, IndexError):
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = np.asarray(stimulus)[block_idx].astype(int)
        block_correct = np.asarray(correct_answer)[block_idx].astype(int)
        block_set_sizes = np.asarray(set_sizes)[block_idx].astype(int)

        # Number of states in this block is given by set size
        nS = int(block_set_sizes[0])
        # Map each state to its correct action within the block
        correct_actions_by_state = np.zeros(nS, dtype=int)
        for s in range(nS):
            # find first occurrence of this state in the block
            first_idx = np.where(block_states == s)[0][0]
            correct_actions_by_state[s] = block_correct[first_idx]

        # Initialize Q-values
        Q = np.zeros((nS, nA), dtype=float)

        for t in range(len(block_states)):
            s = block_states[t]
            ss = float(block_set_sizes[t])

            # Softmax policy
            q_s = Q[s, :].copy()
            logits = beta_eff * (q_s - np.max(q_s))
            p = np.exp(logits)
            p_sum = np.sum(p)
            if p_sum <= 0 or not np.isfinite(p_sum):
                p = np.ones(nA) / nA
            else:
                p /= p_sum

            # Sample action
            a = np.random.choice(nA, p=p)
            simulated_actions[tr] = a

            # Reward based on state-specific correct action
            correct_a = correct_actions_by_state[s]
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # Q-learning update
            pe = r - Q[s, a]
            Q[s, a] += np.clip(alpha, 1e-6, 1.0) * pe

            # Action-generalization to other states
            g_eff = max(0.0, gen_gain) * (1.0 + is_older * max(0.0, age_gen_boost)) / max(1.0, ss)
            if nS > 1 and g_eff != 0.0:
                for s2 in range(nS):
                    if s2 == s:
                        continue
                    Q[s2, a] += g_eff * pe

            # Load-sensitive forgetting
            forget_scale = 1.0 + max(0.0, ss - 3.0) / 3.0
            forget_rate = np.clip(rho_forget * forget_scale, 0.0, 0.999)
            Q *= (1.0 - forget_rate)

            tr += 1

    return simulated_actions, simulated_rewards