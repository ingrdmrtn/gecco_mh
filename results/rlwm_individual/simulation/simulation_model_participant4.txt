def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate actions and rewards from the RL model with age-dependent directed exploration bonus,
    set-size-dependent forgetting, and lapse, mirroring cognitive_model2.

    Parameters
    ----------
    stimulus : array-like of int
        State index on each trial (0..nS-1 within a block).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (typically constant within block, e.g., 3 or 6).
    correct_answer : array-like of int
        Correct action per trial (0..2). Used to generate rewards.
    age : float or array-like
        Participant age.

    Returns
    -------
    simulated_actions : array of int
        Simulated action choices per trial (0..2).
    simulated_rewards : array of int
        Simulated binary rewards per trial (0/1).
    """

    alpha, beta, bonus_y, bonus_o, decay, lapse = parameters
    eps = 1e-12
    alpha = max(0.0, min(1.0, float(alpha)))
    beta = max(eps, float(beta)) * 10.0  # scale as in fitting
    bonus_y = max(0.0, float(bonus_y))
    bonus_o = max(0.0, float(bonus_o))
    decay = max(0.0, min(1.0, float(decay)))
    lapse = max(0.0, min(1.0, float(lapse)))

    # Determine age group
    try:
        age_val = float(age[0])
    except (TypeError, IndexError):
        age_val = float(age)
    is_older = age_val >= 45
    bonus_group = bonus_o if is_older else bonus_y

    stimulus = np.asarray(stimulus, dtype=int)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes, dtype=int)
    correct_answer = np.asarray(correct_answer, dtype=int)

    n_trials = len(stimulus)
    nA = 3

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    # Iterate blocks in temporal order they appear
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx]
        block_set_sizes = set_sizes[block_idx]
        block_correct = correct_answer[block_idx]

        # Set size for this block (assumed constant within block)
        nS = int(block_set_sizes[0])

        # Build a mapping from state -> correct action using first occurrence within the block
        # This ensures consistent reward structure by state within the block.
        correct_by_state = np.zeros(nS, dtype=int)
        seen = np.zeros(nS, dtype=bool)
        for i in range(len(block_states)):
            s = int(block_states[i])
            if 0 <= s < nS and not seen[s]:
                correct_by_state[s] = int(block_correct[i])
                seen[s] = True

        # Initialize value estimates and visit counts
        Q = np.zeros((nS, nA), dtype=float)
        N = np.ones((nS, nA), dtype=float)  # start at 1 to avoid div by zero and huge initial bonuses

        # Simulate trials within this block in order
        for t in range(len(block_states)):
            s = int(block_states[t])
            sz = float(block_set_sizes[t])

            # Apply set-size dependent decay to all actions in current state
            decay_eff = decay * ((max(3.0, sz) - 3.0) / 3.0)
            Q[s, :] *= (1.0 - decay_eff)

            # Directed exploration bonus (count-based, age-dependent magnitude)
            bonus_vec = bonus_group / np.sqrt(N[s, :])

            # Preferences, softmax with inverse temperature beta
            prefs = Q[s, :] + bonus_vec
            prefs -= np.max(prefs)  # numerical stability
            p_soft = np.exp(beta * prefs)
            p_soft /= (np.sum(p_soft) + eps)

            # Lapse mixture with uniform random choice
            p = (1.0 - lapse) * p_soft + lapse * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p))
            simulated_actions[block_idx[t]] = a

            # Generate reward based on correct action for this state
            correct_a = int(correct_by_state[s])
            r = 1 if a == correct_a else 0
            simulated_rewards[block_idx[t]] = r

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Update visit count
            N[s, a] += 1.0

        tr += len(block_states)

    return simulated_actions, simulated_rewards