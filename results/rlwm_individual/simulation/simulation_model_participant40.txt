def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    RL with meta-learned inverse temperature + WM that encodes only after confirmation.

    Parameters:
    - stimulus: sequence of stimuli (state indices) for each trial (array-like, int)
    - blocks: block index for each trial (array-like, int)
    - set_sizes: set size for the block on each trial (array-like, int)
    - correct_answer: correct action for each trial (array-like, int)
    - parameters: [lr, wm_weight, softmax_beta, meta_beta_lr, win_stay_bias, wm_confirm_gain]

    Returns:
    - simulated_actions: simulated action choices (array, int)
    - simulated_rewards: simulated binary rewards (array, int)
    """
    import numpy as np

    lr, wm_weight, softmax_beta, meta_beta_lr, win_stay_bias, wm_confirm_gain = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    stimulus = np.asarray(stimulus, dtype=int)
    blocks = np.asarray(blocks)
    set_sizes = np.asarray(set_sizes)
    correct_answer = np.asarray(correct_answer, dtype=int)

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3
    tr = 0

    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)
        block_set_sizes = set_sizes[block_idx]

        # Use provided set size for the block (assumed constant within block)
        nS = int(block_set_sizes[0])

        # Map each state to its correct action using first occurrence in block
        correct_by_state = np.zeros(nS, dtype=int)
        seen_state = np.zeros(nS, dtype=bool)
        for i in range(len(block_states)):
            s = block_states[i]
            if not seen_state[s]:
                correct_by_state[s] = block_correct[i]
                seen_state[s] = True

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_rew_action = -np.ones(nS, dtype=int)
        last_rew_was_positive = np.zeros(nS, dtype=bool)

        beta_t = softmax_beta

        for t in range(len(block_states)):
            s = block_states[t]

            # RL policy with win-stay bias
            Q_s = q[s, :].copy()
            logits_Q = beta_t * (Q_s - np.max(Q_s))
            if last_rew_was_positive[s] and last_rew_action[s] >= 0:
                logits_Q[last_rew_action[s]] += win_stay_bias
            exp_rl = np.exp(logits_Q - np.max(logits_Q))
            pvec_rl = exp_rl / np.sum(exp_rl)

            # WM policy
            W_s = w[s, :].copy()
            logits_W = softmax_beta_wm * (W_s - np.max(W_s))
            exp_wm = np.exp(logits_W - np.max(logits_W))
            pvec_wm = exp_wm / np.sum(exp_wm)

            # Arbitration weight scaled by set size
            size_scale = 3.0 / float(nS)
            eff_wm_weight = np.clip(wm_weight * size_scale, 0.0, 1.0)

            # Mixture policy and action sampling
            p_total = eff_wm_weight * pvec_wm + (1.0 - eff_wm_weight) * pvec_rl
            p_total = p_total / np.sum(p_total)
            a = np.random.choice(nA, p=p_total)
            simulated_actions[tr] = a

            # Generate reward based on ground-truth correct action for state
            r = 1 if a == correct_by_state[s] else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # Meta-learned beta update (bounded below)
            beta_t = max(1e-6, beta_t + meta_beta_lr * (r - 0.5))

            # WM decay toward uniform
            w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

            # WM confirmation-based consolidation
            confirm = (r > 0) and last_rew_was_positive[s] and (last_rew_action[s] == a)
            if confirm:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_confirm_gain) * w[s, :] + wm_confirm_gain * target

            # Update last rewarded action flags
            if r > 0:
                last_rew_action[s] = a
                last_rew_was_positive[s] = True
            else:
                last_rew_was_positive[s] = False

            tr += 1

    return simulated_actions, simulated_rewards