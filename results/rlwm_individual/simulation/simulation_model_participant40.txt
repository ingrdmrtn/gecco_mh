def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards from the RL + capacity-limited WM mixture model with age and load effects.

    Inputs:
    - stimulus: array of state indices per trial (within-block indices 0..nS-1)
    - blocks: array of block indices per trial (int)
    - set_sizes: array of set size for each trial (int), constant within a block
    - correct_answer: array of correct action (0..2) for each trial
    - age: participant's age (float or int)

    Returns:
    - simulated_actions: array of chosen actions (0..2)
    - simulated_rewards: array of rewards (0/1)
    """

    alpha, beta, k_wm, wm_decay, lapse = parameters

    # Parameter sanitization/same transforms as in fitting code
    beta_eff_base = max(1e-6, beta) * 5.0
    k_wm = max(1e-6, k_wm)
    wm_decay = min(max(wm_decay, 0.0), 1.0)
    lapse = min(max(lapse, 0.0), 0.5)

    age_val = age
    is_older = 1.0 if age_val >= 45 else 0.0

    # Age effects
    k_wm_age = k_wm * (1.0 - 0.3 * is_older)
    beta_age_scale = 1.0 - 0.2 * is_older
    beta_eff_base *= max(0.1, beta_age_scale)

    nA = 3
    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        idx = np.where(blocks == b)[0]
        block_states = np.asarray(stimulus)[idx]
        block_correct = np.asarray(correct_answer)[idx]
        block_set_sizes = np.asarray(set_sizes)[idx]
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        Q = np.zeros((nS, nA), dtype=float)               # RL Q-values
        W = np.ones((nS, nA), dtype=float) / nA           # WM associative strengths
        last_correct = -np.ones(nS, dtype=int)            # last correct action per state (-1 unknown)

        for t in range(len(block_states)):
            s = int(block_states[t])
            set_sz = float(block_set_sizes[t])

            # Compute WM weight based on capacity load and age overload penalty
            wm_load = min(1.0, max(0.0, k_wm_age / max(1.0, set_sz)))
            overload = max(0.0, (set_sz - k_wm_age) / max(1.0, set_sz))
            wm_weight = wm_load * (1.0 - 0.25 * is_older * overload)
            wm_weight = min(max(wm_weight, 0.0), 1.0)

            # RL policy
            Qs = Q[s, :]
            beta_eff = beta_eff_base
            Qs_center = Qs - np.max(Qs)
            expQ = np.exp(beta_eff * Qs_center)
            p_rl = expQ / np.sum(expQ)

            # WM decay and update from previous trial's outcome will happen after action/reward in simulation
            # For action selection, we use current W (after decay applied each trial)
            # Apply decay now (as in fitting code, before incorporating current trial outcome)
            W[s, :] = (1.0 - wm_decay) * W[s, :]

            # WM policy from current W
            beta_wm = 10.0 * beta_eff
            Ws_center = W[s, :] - np.max(W[s, :])
            p_wm = np.exp(beta_wm * Ws_center)
            p_wm = p_wm / np.sum(p_wm)

            # Last-correct spike-in
            if last_correct[s] >= 0:
                lc = last_correct[s]
                spike = np.zeros(nA); spike[lc] = 1.0
                p_wm = 0.8 * p_wm + 0.2 * spike

            # Mixture with lapse
            p_mix = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[tr + t] = a

            # Determine reward from correct answer
            correct_a = int(block_correct[t])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr + t] = r

            # WM learning given outcome (mirrors fitting code)
            if r > 0.5:
                eta = 0.7
                target = np.zeros(nA); target[a] = 1.0
                W[s, :] = (1 - eta) * W[s, :] + eta * target
                last_correct[s] = a
            else:
                eta_neg = 0.2
                W[s, :] = (1 - eta_neg) * W[s, :] + eta_neg * (1.0 / nA) * np.ones(nA)

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

        tr += len(block_states)

    return simulated_actions, simulated_rewards