def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL+WM capacity-limited mixture with age- and set-size-modulated WM contribution (simulation).

    Parameters:
    - stimulus: array-like, state index on each trial (0..nS-1 within a block)
    - blocks: array-like, block index for each trial
    - set_sizes: array-like, set size for each trial (e.g., 3 or 6)
    - correct_answer: array-like, correct action for each trial (0..2)
    - age: float, participant age

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """

    alpha, beta, wm_base, capacity_K, decay, epsilon = parameters

    # Internal scalings and bounds consistent with fitting code
    beta = max(1e-6, beta) * 10.0
    epsilon = max(0.0, min(0.2, epsilon))
    age_val = age
    is_older = 1.0 if age_val >= 45 else 0.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3

    # Iterate through blocks
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx]
        block_set_sizes = set_sizes[block_idx]
        block_correct = correct_answer[block_idx]

        # The model uses the set size of the block (assumed constant within block)
        nS = int(block_set_sizes[0])

        # Initialize RL and WM states for the block
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = (1.0 / nA) * np.ones((nS, nA))  # WM policy-like table
        WM_conf = np.zeros(nS)              # WM confidence/availability

        for local_t, tr in enumerate(block_idx):
            s = int(block_states[local_t])
            ss = int(block_set_sizes[local_t])

            # Effective WM availability given capacity and age
            K_eff = max(1e-6, capacity_K * (0.7 if is_older > 0.5 else 1.0))
            wm_retrieval = 1.0 / (1.0 + (ss / K_eff))

            wm_weight = wm_base * (1.0 - 0.25 * is_older) * wm_retrieval
            wm_weight = max(0.0, min(1.0, wm_weight))

            # RL policy
            Qs = Q[s, :]
            Qs_center = Qs - np.max(Qs)
            p_rl = np.exp(beta * Qs_center)
            p_rl = p_rl / np.sum(p_rl)

            # WM policy
            beta_wm = 5.0 * beta
            Ws = W[s, :]
            Ws = np.maximum(Ws, 1e-8)
            Ws = Ws / np.sum(Ws)
            logit_wm = beta_wm * (Ws - np.max(Ws))
            p_wm = np.exp(logit_wm)
            p_wm = p_wm / np.sum(p_wm)

            # Mixture with WM confidence
            wm_mix = wm_weight * (0.3 + 0.7 * WM_conf[s])
            wm_mix = max(0.0, min(1.0, wm_mix))
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl

            # Lapses
            p_final = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p_final))
            simulated_actions[tr] = a

            # Outcome given correct answer
            correct_a = int(block_correct[local_t])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # Learning updates
            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM update
            if r > 0.5:
                # Store a focused WM trace for this state
                W[s, :] = (1e-6) * np.ones(nA)
                W[s, a] = 1.0
                WM_conf[s] = 1.0
            else:
                # Decay toward uniform
                W[s, :] = (1.0 - decay) * W[s, :] + decay * (1.0 / nA)
                WM_conf[s] = (1.0 - decay) * WM_conf[s]

    return simulated_actions, simulated_rewards