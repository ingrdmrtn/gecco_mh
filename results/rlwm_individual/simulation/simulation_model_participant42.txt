def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate actions and rewards from the RL + decaying WM mixture model
    with age- and load-dependent WM weighting.

    Inputs:
    - stimulus: array of state indices per trial (0..nS-1 within each block)
    - blocks: array of block indices per trial (integers)
    - set_sizes: array of set sizes per trial (first value within a block defines nS)
    - correct_answer: array of correct action (0,1,2) per trial
    - age: participant's age (float)

    Returns:
    - simulated_actions: array of chosen actions (0,1,2)
    - simulated_rewards: array of rewards (0 or 1)
    """

    alpha, beta, wm_capacity, wm_decay, lapse = parameters

    # Apply same internal scaling/clipping as fitting code
    beta = 10.0 * beta
    alpha = np.clip(alpha, 0.0, 1.0)
    wm_decay = np.clip(wm_decay, 0.0, 1.0)
    lapse = np.clip(lapse, 0.0, 1.0)
    age_val = float(age)

    n_trials = len(stimulus)
    nA = 3
    eps = 1e-12

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        idx = np.where(blocks == b)[0]
        b_states = stimulus[idx].astype(int)
        b_set_sizes = set_sizes[idx].astype(int)
        b_correct = correct_answer[idx].astype(int)
        nS = int(b_set_sizes[0])

        # Initialize controllers
        Q = np.zeros((nS, nA))
        W = np.ones((nS, nA)) / nA

        # Age-adjusted effective WM capacity
        age_penalty = max(0.0, (age_val - 45.0) / 30.0)  # 0 at 45 or below, ~1 at 75
        k_eff = max(0.0, wm_capacity - age_penalty)

        for t in range(len(b_states)):
            s = int(b_states[t])
            ss = int(b_set_sizes[t])

            # RL softmax policy
            q_s = Q[s, :]
            z = beta * (q_s - np.max(q_s))
            p_rl_vec = np.exp(z)
            p_rl_vec /= np.sum(p_rl_vec)

            # WM policy is the WM distribution directly
            p_wm_vec = W[s, :].copy()
            p_wm_vec /= max(np.sum(p_wm_vec), eps)  # ensure normalized

            # Load- and age-dependent WM weight
            wm_weight = np.clip(k_eff / max(1, ss), 0.0, 1.0)

            # Mixture and lapse
            p_choice = wm_weight * p_wm_vec + (1.0 - wm_weight) * p_rl_vec
            p_final = (1.0 - lapse) * p_choice + lapse * (1.0 / nA)
            p_final = np.clip(p_final, eps, 1.0)
            p_final /= np.sum(p_final)

            # Sample action
            a = int(np.random.choice(nA, p=p_final))
            simulated_actions[tr] = a

            # Generate reward from correct answer on this trial
            r = 1 if a == int(b_correct[t]) else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM decay toward uniform
            W = (1.0 - wm_decay) * W + wm_decay * (1.0 / nA)

            # If rewarded, store chosen action in WM for that state
            if r > 0.5:
                W[s, :] = 0.0
                W[s, a] = 1.0

            tr += 1

    return simulated_actions, simulated_rewards