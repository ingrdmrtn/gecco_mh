import numpy as np

def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate actions and rewards using an Actor-Critic model with load- and age-modulated
    eligibility traces and lapse, matching the provided fitting model logic.

    Inputs:
    - stimulus: array of state indices per trial (0..set_size-1)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (e.g., 3 or 6)
    - correct_answer: array of correct actions per trial (0,1,2)
    - age: scalar age in years

    Returns:
    - simulated_actions: array of simulated actions (0,1,2)
    - simulated_rewards: array of simulated binary rewards (0/1)
    """


    alpha_actor, alpha_critic, beta, lambda_base, load_decay, age_lapse = parameters

    n_trials = len(stimulus)
    nA = 3
    eps = 1e-12

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Age handling
    age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    tr = 0
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx]
        block_set_sizes = set_sizes[block_idx]
        block_correct = correct_answer[block_idx]

        # Number of states in this block determined by set size on first trial of block
        nS = int(block_set_sizes[0])

        # Initialize actor (preferences) and critic (state values)
        pref = np.zeros((nS, nA))
        V = np.zeros(nS)

        # Eligibility traces
        e_actor = np.zeros((nS, nA))
        e_critic = np.zeros(nS)

        for t in range(len(block_states)):
            s = int(block_states[t])
            # Compute current load (beyond 3 items)
            load = max(0, int(block_set_sizes[t]) - 3)

            # Effective eligibility decay and temperature
            lambda_eff = lambda_base * max(0.0, 1.0 - load_decay * load) * (1.0 - 0.3 * is_older)
            lambda_eff = min(max(lambda_eff, 0.0), 1.0)

            beta_eff = beta * 10.0

            # Policy from softmax over preferences for current state
            logits = beta_eff * pref[s, :]
            logits = logits - np.max(logits)  # stabilize
            p_vec = np.exp(logits)
            p_vec = p_vec / max(eps, np.sum(p_vec))

            # Lapse mixture increases with load and age
            lapse_eff = np.clip(0.02 * load + age_lapse * is_older, 0.0, 0.5)
            p_mix = (1.0 - lapse_eff) * p_vec + lapse_eff * (np.ones(nA) / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p_mix))
            simulated_actions[block_idx[t]] = a

            # Determine reward from correct answer on this trial
            correct_a = int(block_correct[t])
            r = 1 if a == correct_a else 0
            simulated_rewards[block_idx[t]] = r

            # TD error (episodic; no next-state value)
            v_s = V[s]
            delta = r - v_s

            # Decay eligibility traces
            e_actor *= lambda_eff
            e_critic *= lambda_eff

            # Accumulate gradient eligibility at current state-action
            grad = -p_vec.copy()
            grad[a] += 1.0
            e_actor[s, :] += grad
            e_critic[s] += 1.0

            # Update actor preferences and critic values
            pref += alpha_actor * delta * e_actor
            V += alpha_critic * delta * e_critic

            tr += 1

    return simulated_actions, simulated_rewards