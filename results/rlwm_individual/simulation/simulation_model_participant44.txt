def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Simulate choices and rewards using:
    - RL with asymmetric learning rates
    - WM contribution gated by surprise (|PE|)
    - Lapse probability of random choice

    Inputs:
    - stimulus: array of state ids per trial (0..nS-1 within each block)
    - blocks: array of block ids per trial
    - set_sizes: array of set sizes per trial (constant within a block)
    - correct_answer: array of correct actions per trial (0..nA-1)
    - parameters: [lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, lapse]

    Outputs:
    - simulated_actions: array of chosen actions per trial
    - simulated_rewards: array of binary rewards per trial
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_decay, lapse = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr_global = 0
    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices]
        block_correct = correct_answer[block_indices]

        nA = 3
        # Prefer provided set size for the block if consistent; otherwise infer from states present
        if len(block_indices) > 0:
            nS = int(set_sizes[block_indices][0])
        else:
            nS = len(np.unique(block_states))

        # Map each state to its correct action (take the first occurrence per state)
        state_to_correct = np.zeros(nS, dtype=int)
        seen = np.zeros(nS, dtype=bool)
        for t in range(len(block_states)):
            s = int(block_states[t])
            if not seen[s]:
                state_to_correct[s] = int(block_correct[t])
                seen[s] = True

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            s = int(block_states[t])

            # Compute RL policy
            logits_rl = softmax_beta * q[s, :]
            logits_rl -= np.max(logits_rl)
            exp_rl = np.exp(logits_rl)
            p_rl_vec = exp_rl / np.sum(exp_rl)

            # Compute WM policy
            logits_wm = softmax_beta_wm * w[s, :]
            logits_wm -= np.max(logits_wm)
            exp_wm = np.exp(logits_wm)
            p_wm_vec = exp_wm / np.sum(exp_wm)

            # Action-dependent surprise gating for simulation:
            # For each hypothetical action a, compute surprise if that action were taken and rewarded accordingly.
            correct_a = state_to_correct[s]
            # Deterministic reward given environment rule
            r_vec = np.zeros(nA)
            r_vec[correct_a] = 1.0

            delta_vec = r_vec - q[s, :]
            surprise_vec = np.abs(delta_vec)
            wm_mix_vec = np.clip(wm_weight * surprise_vec, 0.0, 1.0)

            p_mix_vec = wm_mix_vec * p_wm_vec + (1.0 - wm_mix_vec) * p_rl_vec
            p_total_vec = (1.0 - lapse) * p_mix_vec + lapse * (1.0 / nA)
            p_total_vec = np.maximum(p_total_vec, 1e-12)
            p_total_vec = p_total_vec / np.sum(p_total_vec)

            # Sample action
            a = int(np.random.choice(nA, p=p_total_vec))
            simulated_actions[block_indices[t]] = a

            # Realized reward from environment
            r = 1 if a == correct_a else 0
            simulated_rewards[block_indices[t]] = r

            # RL update with asymmetric learning
            delta = r - q[s, a]
            lr_eff = lr_pos if delta >= 0.0 else lr_neg
            q[s, a] += lr_eff * delta

            # WM decay toward baseline
            w = (1.0 - wm_decay) * w + wm_decay * w_0

            # WM update based on outcome
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = 0.5 * w[s, :] + 0.5 * one_hot
            else:
                reduce = 0.25 * w[s, a]
                w[s, a] -= reduce
                if nA > 1:
                    others = [i for i in range(nA) if i != a]
                    w[s, others] += reduce / (nA - 1)
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum
                else:
                    w[s, :] = w_0[s, :].copy()

            tr_global += 1

    return simulated_actions, simulated_rewards