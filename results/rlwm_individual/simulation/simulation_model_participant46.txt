def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate behavior from the RL-WM mixture model with age- and set-size-dependent
    WM engagement, perseveration (stickiness), and lapses.

    Inputs:
    - stimulus: array of state indices per trial (0..nS-1 within each block)
    - blocks: array of block indices per trial (learning resets per block)
    - set_sizes: array of set size per trial (e.g., 3 or 6); used to set nS per block and m(t)
    - correct_answer: array of correct action per trial (0..2)
    - age: participant age (scalar)

    Returns:
    - simulated_actions: array of sampled actions per trial (0..2)
    - simulated_rewards: array of rewards per trial (0/1), based on matching correct_answer
    """

    alpha, beta, K, rho, phi, epsilon_base = parameters

    # Apply same internal scalings as in the fitting code
    beta = max(1e-6, beta) * 8.0
    is_older = 1.0 if (np.asarray(age).item() >= 45) else 0.0

    # Effective WM capacity with age penalty; avoid collapse
    K_eff_factor = 1.0 - 0.35 * is_older
    K_eff_min = 0.5
    K_eff = max(K_eff_min, K * K_eff_factor)

    # Lapse increases with older age
    epsilon = np.clip(epsilon_base * (1.0 + 1.5 * is_older), 0.0, 0.49)

    # Slightly lower precision for older adults
    beta_eff = beta / (1.0 + 0.25 * is_older)

    n_trials = len(stimulus)
    simulated_actions = -1 * np.ones(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3

    # Iterate over blocks, reset learning each block
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        idx = np.where(block_mask)[0]

        block_states = stimulus[idx].astype(int)
        block_correct = correct_answer[idx].astype(int)
        block_set_sizes = set_sizes[idx].astype(int)

        # Determine number of states in this block (assumed equal to set size for the block)
        nS = int(block_set_sizes[0])
        if nS <= 0:
            # Skip malformed blocks
            continue

        # Initialize RL and WM value tables and last action per state
        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        W = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        last_action = -1 * np.ones(nS, dtype=int)

        for k, t in enumerate(idx):
            s = int(block_states[k])
            a_star = int(block_correct[k])
            nS_t = int(block_set_sizes[k])

            # Skip invalid trials
            if s < 0 or s >= nS or a_star < 0 or a_star >= nA:
                continue

            # RL policy with perseveration
            prefs = beta_eff * Q[s, :].copy()
            if last_action[s] >= 0:
                prefs[last_action[s]] += phi
            prefs -= np.max(prefs)
            exp_prefs = np.exp(prefs)
            p_rl_vec = exp_prefs / max(1e-12, np.sum(exp_prefs))

            # WM policy (sharpened WM distribution)
            wm_prefs = 6.0 * W[s, :].copy()
            wm_prefs -= np.max(wm_prefs)
            exp_wm = np.exp(wm_prefs)
            p_wm_vec = exp_wm / max(1e-12, np.sum(exp_wm))

            # Set-size and age dependent WM mixture weight
            size_term = (K_eff - nS_t) / max(1e-6, K_eff)
            m_raw = 1.0 / (1.0 + np.exp(-4.0 * size_term))  # (0,1)
            m = rho * m_raw

            # Lapse-mixture with uniform
            p_mix = (1.0 - epsilon) * (m * p_wm_vec + (1.0 - m) * p_rl_vec) + epsilon * (1.0 / nA)
            p_mix = np.clip(p_mix, 1e-12, 1.0)
            p_mix /= np.sum(p_mix)

            # Sample action
            a = int(np.random.choice(nA, p=p_mix))
            simulated_actions[t] = a

            # Generate reward from correct answer
            r = 1 if a == a_star else 0
            simulated_rewards[t] = r

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # WM update
            if r > 0:
                eta_wm = min(1.0, 0.5 + 0.5 * rho)
                target = np.zeros(nA)
                target[a] = 1.0
                W[s, :] = (1.0 - eta_wm) * W[s, :] + eta_wm * target
            else:
                decay_wm = 0.25
                W[s, :] = (1.0 - decay_wm) * W[s, :] + decay_wm * (1.0 / nA)

            # Update perseveration state
            last_action[s] = a

    return simulated_actions, simulated_rewards