def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Simulate choices and rewards for:
    Model 3: RL + surprise-gated WM strength with deterministic WM policy and load-dependent decay

    Inputs:
    - stimulus: array of state indices per trial
    - blocks: array of block indices per trial
    - set_sizes: array of set size per trial (constant within a block)
    - correct_answer: array of correct action per trial
    - parameters: [lr, softmax_beta, wm_weight0, gate_beta, theta, wm_alpha]

    Returns:
    - simulated_actions: array of simulated action choices
    - simulated_rewards: array of simulated binary rewards
    """
    lr, softmax_beta, wm_weight0, gate_beta, theta, wm_alpha = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices].astype(int)
        block_correct_actions = correct_answer[block_indices].astype(int)
        block_set_sizes = set_sizes[block_indices]

        nA = 3
        nS = int(block_set_sizes[0])

        # Build per-state correct action map from trials in this block
        # Assumes states are labeled 0..nS-1
        correct_actions = np.zeros(nS, dtype=int)
        for st in range(nS):
            idx = np.where(block_states == st)[0]
            if len(idx) > 0:
                correct_actions[st] = block_correct_actions[idx[0]]
            else:
                # Fallback (shouldn't happen if data are consistent)
                correct_actions[st] = 0

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        load_scale = 3.0 / float(nS)
        decay = np.clip(wm_alpha * (1.0 - load_scale), 0.0, 1.0)
        gate_beta_eff = max(0.0, gate_beta)

        for t in range(len(block_states)):
            s = int(block_states[t])

            Q_s = q[s, :].copy()
            W_s = w[s, :].copy()

            # Softmax policies
            # RL
            Q_s_shift = Q_s - np.max(Q_s)
            p_rl = np.exp(softmax_beta * Q_s_shift)
            p_rl /= np.sum(p_rl)
            # WM (highly deterministic)
            W_s_shift = W_s - np.max(W_s)
            p_wm = np.exp(softmax_beta_wm * W_s_shift)
            p_wm /= np.sum(p_wm)

            # Arbitration weight via surprise gate
            # Use expected action by sampling from mixture
            # For sampling we compute mixture first, but gating needs surprise from RL w.r.t chosen action.
            # We compute gate using the action's RL surprise after sampling.
            p_total = wm_weight0 * 0.5 * p_wm + (1.0 - wm_weight0 * 0.5) * p_rl  # temporary mix to sample action
            a = np.random.choice(nA, p=p_total)

            # Compute reward from environment
            r = 1 if a == int(correct_actions[s]) else 0
            simulated_actions[tr] = a
            simulated_rewards[tr] = r

            # Compute surprise and final arbitration weight (based on chosen action)
            delta = r - Q_s[a]
            surprise = abs(delta)
            gate = 1.0 / (1.0 + np.exp(-(surprise - theta) * gate_beta_eff))
            wm_weight_eff = np.clip(wm_weight0 * gate * load_scale, 0.0, 1.0)

            # Recompute mixture with correct arbitration weight and resample? No: action already taken.
            # Learning updates:
            # RL update
            q[s, a] += lr * delta

            # WM global decay toward uniform
            w = (1.0 - decay) * w + decay * w_0

            # WM local update based on outcome
            if r > 0.5:
                # Draw row toward one-hot on rewarded action
                w[s, :] = (1.0 - wm_alpha) * w[s, :]
                w[s, a] += wm_alpha
            else:
                # Penalize chosen action slightly and renormalize row
                w[s, a] = (1.0 - wm_alpha) * w[s, a]
                row_sum = np.sum(w[s, :])
                if row_sum <= 0:
                    w[s, :] = w_0[s, :].copy()
                else:
                    w[s, :] /= row_sum

            tr += 1

    return simulated_actions, simulated_rewards