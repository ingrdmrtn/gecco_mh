def simulate_model(stimulus, blocks, set_sizes, correct_answer, parameters):
    """
    Simulation for: Hybrid RL + WM with set-sizeâ€“scaled WM decay and state-specific choice stickiness.

    Parameters:
    - stimulus: sequence of stimuli (states) for each trial (array of ints, 0..nS-1 within block)
    - blocks: block index for each trial (array)
    - set_sizes: set size for each trial (array; constant within a block)
    - correct_answer: correct action for each trial (array of ints, 0..2)
    - parameters: [lr, softmax_beta, omega0, rho, kappa_stick]

    Returns:
    - simulated_actions: simulated action choices (array)
    - simulated_rewards: simulated binary rewards (array)
    """
    lr, softmax_beta, omega0, rho, kappa_stick = parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    tr = 0
    for b in np.unique(blocks):
        block_indices = np.where(blocks == b)[0]
        block_states = stimulus[block_indices].astype(int)
        block_set_sizes = set_sizes[block_indices]
        block_correct = correct_answer[block_indices].astype(int)

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # WM decay increases with set size
        wm_forget = 1.0 - (1.0 - rho) ** max(1, (nS - 1))

        # Track last action per state for stickiness
        last_action = -1 * np.ones(nS, dtype=int)

        for t in range(len(block_states)):
            s = int(block_states[t])

            # RL policy
            Q_s = q[s, :].copy()
            p_rl = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            p_rl /= np.sum(p_rl)

            # WM policy with within-state stickiness bias
            W_s = w[s, :].copy()
            if last_action[s] >= 0:
                bias_vec = np.zeros(nA)
                bias_vec[last_action[s]] = kappa_stick
                W_s = W_s + bias_vec
            p_wm = np.exp(softmax_beta_wm * (W_s - np.max(W_s)))
            p_wm /= np.sum(p_wm)

            # Arbitration
            p_total = omega0 * p_wm + (1.0 - omega0) * p_rl
            p_total = p_total / np.sum(p_total)

            # Sample action
            a = int(np.random.choice(nA, p=p_total))
            simulated_actions[tr + t] = a

            # Outcome
            correct_a = int(block_correct[t])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr + t] = r

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay
            w = (1.0 - wm_forget) * w + wm_forget * w_0

            # WM one-shot storage on rewarded trials
            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0

            # Update stickiness memory
            last_action[s] = a

        tr += len(block_states)

    return simulated_actions, simulated_rewards