import numpy as np

def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate actions and rewards using the meta-control RL+WM model with asymmetric learning and stickiness.

    Inputs:
    - stimulus: array of state indices per trial (0..nS-1 within block)
    - blocks: array of block IDs per trial; learning resets per block
    - set_sizes: array of set size per trial (3 or 6); constant within a block
    - correct_answer: array of correct action per trial (0..2)
    - age: participant's age (float or scalar)
    - parameters: sequence of 6 floats
        alpha_pos, alpha_neg, beta, stickiness, gate_intercept, gamma_load

    Returns:
    - simulated_actions: array of simulated action choices (0..2)
    - simulated_rewards: array of simulated binary rewards (0/1)
    """
    alpha_pos, alpha_neg, beta, stickiness, gate_intercept, gamma_load = parameters
    beta = beta * 10.0

    # Handle age (older adults reduce WM gating)
    try:
        age_val = float(age[0])
    except Exception:
        age_val = float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    n_trials = len(stimulus)
    nA = 3
    eps = 1e-12

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Iterate over blocks; reset learning per block
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = np.asarray(stimulus)[block_idx]
        block_correct = np.asarray(correct_answer)[block_idx]
        block_set_sizes = np.asarray(set_sizes)[block_idx]

        # Determine set size for this block
        nS = int(block_set_sizes[0])

        # Map each state to its correct action (use first occurrence in block)
        correct_actions_by_state = np.zeros(nS, dtype=int)
        for s in range(nS):
            inds_s = np.where(block_states == s)[0]
            if len(inds_s) == 0:
                # If state was not visited (shouldn't happen), default to 0
                correct_actions_by_state[s] = 0
            else:
                correct_actions_by_state[s] = int(block_correct[inds_s[0]])

        # Initialize RL and WM
        Q = (1.0 / nA) * np.ones((nS, nA))
        W = np.zeros((nS, nA))  # last-outcome memory (0..1)
        last_action = -1

        for local_t, tr in enumerate(block_idx):
            s = int(block_states[local_t])

            # RL preferences with stickiness
            prefs_rl = Q[s, :].copy()
            if last_action >= 0:
                prefs_rl[last_action] += stickiness
            prefs_rl = prefs_rl - np.max(prefs_rl)
            p_rl = np.exp(beta * prefs_rl)
            p_rl = p_rl / (np.sum(p_rl) + eps)

            # WM preferences with stickiness
            prefs_wm = W[s, :].copy()
            if last_action >= 0:
                prefs_wm[last_action] += stickiness
            prefs_wm = prefs_wm - np.max(prefs_wm)
            p_wm = np.exp(beta * prefs_wm)
            p_wm = p_wm / (np.sum(p_wm) + eps)

            # Gating: depends on load (set size) and age
            load_term = gamma_load * (3.0 - float(nS))  # 0 for 3, -3 for 6 if gamma_load>0
            age_term = -0.5 * is_older
            z = gate_intercept + load_term + age_term
            gate = 1.0 / (1.0 + np.exp(-z))
            gate = min(max(gate, 0.0), 1.0)

            # Mixed policy
            p_mix = gate * p_wm + (1.0 - gate) * p_rl
            p_mix = p_mix / (np.sum(p_mix) + eps)

            # Sample action
            a = int(np.random.choice(nA, p=p_mix))
            simulated_actions[tr] = a

            # Generate reward from task rule
            correct_a = correct_actions_by_state[s]
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update with asymmetric learning rates
            pe = r - Q[s, a]
            if pe >= 0:
                Q[s, a] += alpha_pos * pe
            else:
                Q[s, a] += alpha_neg * pe

            # WM update: decay within state, then store last outcome for chosen action
            W[s, :] *= 0.9
            W[s, a] = r

            # Update stickiness memory
            last_action = a

    return simulated_actions, simulated_rewards