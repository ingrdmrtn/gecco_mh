def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL+WM simulation with capacity-limited working memory, decay, and age-modulated WM reliance and decay.

    Inputs:
    - stimulus: array of state indices per trial (int)
    - blocks: array of block index per trial (int)
    - set_sizes: array of set size per trial (int; used to set number of states in the block)
    - correct_answer: array of correct action per trial (int in {0,1,2})
    - age: participant age (float or array-like with one element)

    Returns:
    - simulated_actions: array of sampled actions per trial (int)
    - simulated_rewards: array of binary rewards per trial (int in {0,1})
    """

    alpha, beta, wm_weight_base, wm_capacity_k, wm_decay, lapse = parameters

    # Apply model's internal transforms and safeguards
    beta = 10.0 * beta
    wm_capacity_k = max(0.1, wm_capacity_k)
    lapse = float(np.clip(lapse, 0.0, 0.49))

    # Age handling (allow float or array-like with one value)
    try:
        ag = float(age[0]) if hasattr(age, "__len__") else float(age)
    except Exception:
        ag = float(age)
    is_older = 1.0 if ag >= 45 else 0.0

    # Age-dependent multipliers
    wm_weight_mult = 0.8 if is_older else 1.1
    decay_mult = 1.3 if is_older else 0.9

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3  # number of actions

    # Iterate over blocks
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_set_sizes = set_sizes[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)

        # Determine number of states from provided set size (assumed constant within block)
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        Q = (1.0 / nA) * np.ones((nS, nA), dtype=float)
        W = (1.0 / nA) * np.ones((nS, nA), dtype=float)

        # Effective WM base weight and decay after age modulation
        wm_w_eff_base = float(np.clip(wm_weight_base * wm_weight_mult, 0.0, 1.0))
        wm_decay_eff = float(np.clip(wm_decay * decay_mult, 0.0, 1.0))

        for local_t, tr in enumerate(block_idx):
            s = int(block_states[local_t])
            # Safety: bound state index
            if s < 0 or s >= nS:
                # If invalid state slips through, pick uniformly and skip learning
                probs = np.ones(nA) / nA
                a = int(np.random.choice(nA, p=probs))
                r = 1 if a == int(block_correct[local_t]) else 0
                simulated_actions[tr] = a
                simulated_rewards[tr] = r
                continue

            # Capacity gating of WM contribution
            phi = min(1.0, wm_capacity_k / float(nS))
            wm_mix = float(np.clip(wm_w_eff_base * phi, 0.0, 1.0))

            # RL policy
            q_s = Q[s, :]
            q_s_center = q_s - np.max(q_s)
            exp_q = np.exp(beta * q_s_center)
            p_rl = exp_q / np.sum(exp_q)

            # WM policy (probabilities from normalized WM row)
            w_s = W[s, :].copy()
            w_s = np.clip(w_s, 1e-12, None)
            p_wm = w_s / np.sum(w_s)

            # Mixture and lapse
            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)

            # Sample action
            a = int(np.random.choice(nA, p=p))
            simulated_actions[tr] = a

            # Outcome based on correctness
            correct_a = int(block_correct[local_t])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            delta = r - Q[s, a]
            Q[s, a] += alpha * delta

            # WM decay toward uniform for the current state
            W[s, :] = (1.0 - wm_decay_eff) * W[s, :] + wm_decay_eff * (1.0 / nA)

            # WM one-shot storage on correct feedback
            if r == 1:
                W[s, :] = (1e-6) * np.ones(nA)
                W[s, a] = 1.0

    return simulated_actions, simulated_rewards