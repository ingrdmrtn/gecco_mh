def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    RL with asymmetric learning rates, perseveration, age- and load-modulated exploration, and lapse-to-nonresponse.

    Inputs
    - stimulus: state index per trial (array-like of int)
    - blocks: block index per trial (array-like of int)
    - set_sizes: set size per trial (array-like of int, e.g., 3 or 6)
    - correct_answer: correct action per trial (array-like of int in {0,1,2})
    - age: participant age (float)

    Returns
    - simulated_actions: simulated action choices per trial (array of int; -2 denotes lapse)
    - simulated_rewards: simulated binary rewards per trial (array of int in {0,1})
    """

    alpha_pos, alpha_neg, beta, kappa, age_noise_scale, lapse = parameters

    is_old = 1.0 if age >= 45 else 0.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3  # number of actions
    trial_counter = 0

    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = np.asarray(stimulus)[block_idx]
        block_correct = np.asarray(correct_answer)[block_idx]
        block_set_sizes = np.asarray(set_sizes)[block_idx]

        # Determine number of states in this block
        # Prefer the provided set size if consistent, else infer from unique states in block
        if len(block_set_sizes) > 0 and np.all(block_set_sizes == block_set_sizes[0]):
            nS = int(block_set_sizes[0])
        else:
            nS = int(len(np.unique(block_states)))

        # Build per-state correct action mapping using first occurrence in block
        state_ids = np.unique(block_states)
        # Map observed state labels to a compact 0..nS-1 index if needed
        # Create dictionary from observed state label to compact index
        state_to_compact = {s_lab: i for i, s_lab in enumerate(state_ids)}
        # per-compact-state correct action
        correct_by_state = np.zeros(nS, dtype=int)
        for s_lab in state_ids:
            s_comp = state_to_compact[s_lab]
            first_idx = np.where(block_states == s_lab)[0][0]
            correct_by_state[s_comp] = int(block_correct[first_idx])

        # Initialize Q-values and perseveration memory
        q = (1.0 / nA) * np.ones((nS, nA))
        last_a = -1 * np.ones(nS, dtype=int)  # last action taken in each state

        for t in range(len(block_states)):
            s_lab = block_states[t]
            s = state_to_compact[s_lab]  # compact state index
            nS_t = int(block_set_sizes[t]) if t < len(block_set_sizes) else nS

            # Effective inverse temperature scales down for older adults when set size > 3
            load_ratio = max(1.0, nS_t) / 3.0  # 1 for 3, 2 for 6
            beta_eff = beta * 10.0 / (1.0 + is_old * age_noise_scale * (load_ratio - 1.0))

            # Action values with perseveration bias
            Q_s = q[s, :].copy()
            if last_a[s] in [0, 1, 2]:
                Q_s[last_a[s]] += kappa

            # Softmax over biased Q-values
            Qc = Q_s - np.max(Q_s)
            expQ = np.exp(beta_eff * Qc)
            p_task = expQ / np.sum(expQ)

            # Lapse vs. task-driven decision
            if np.random.rand() < lapse:
                a = -2  # lapse (nonresponse)
            else:
                a = int(np.random.choice(nA, p=p_task))

            # Reward based on correctness (no reward for lapse)
            if a in [0, 1, 2]:
                correct_a = correct_by_state[s]
                r = 1 if a == correct_a else 0
            else:
                r = 0  # lapse gets zero reward and no learning

            # Store outputs
            simulated_actions[trial_counter] = a
            simulated_rewards[trial_counter] = r

            # RL update only if a valid action was made
            if a in [0, 1, 2]:
                r_pos = 1.0 if r > 0 else 0.0
                alpha_use = alpha_pos if r_pos > 0.5 else alpha_neg
                delta = r_pos - q[s, a]
                q[s, a] = q[s, a] + alpha_use * delta
                last_a[s] = a

            trial_counter += 1

    return simulated_actions, simulated_rewards