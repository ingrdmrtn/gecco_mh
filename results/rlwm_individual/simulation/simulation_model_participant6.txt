def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards using the RL + global forgetting + rule system with age- and set-size-modulated rule engagement.

    Inputs:
    - stimulus: array of state indices per trial (0..nS-1 within each block)
    - blocks: array of block indices per trial (block-wise segmentation)
    - set_sizes: array of set sizes per trial (e.g., 3 or 6); assumed constant within block
    - correct_answer: array of correct actions per trial (0..2)
    - age: scalar or array-like with participant age

    Returns:
    - simulated_actions: array of simulated action choices (0..2)
    - simulated_rewards: array of simulated binary rewards (0/1)
    """


    alpha, beta, phi, rule_base, age_effect, epsilon = parameters

    # Internal scaling and clipping consistent with fitting code
    beta = beta * 10.0
    epsilon = np.clip(epsilon, 1e-6, 0.2)
    alpha = np.clip(alpha, 0.0, 1.0)
    phi = np.clip(phi, 0.0, 1.0)

    # Handle age input
    if isinstance(age, (list, tuple, np.ndarray)):
        age_val = float(age[0])
    else:
        age_val = float(age)
    age_sign = 1.0 if age_val < 45 else -1.0

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    nA = 3

    # Iterate over blocks to reset latent states per block
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states = stimulus[block_idx].astype(int)
        block_correct = correct_answer[block_idx].astype(int)
        block_set_sizes = set_sizes[block_idx].astype(int)

        # Determine number of states in this block (assumed constant within block)
        nS = int(block_set_sizes[0])
        # Initialize RL and rule memory
        Q = np.zeros((nS, nA))
        rule_action = -1 * np.ones(nS, dtype=int)  # -1 indicates no rule learned yet

        for i, tr in enumerate(block_idx):
            s = int(block_states[i])
            correct_a = int(block_correct[i])
            set_size = float(block_set_sizes[i])

            # Effective forgetting for this trial
            older = 1.0 if age_sign < 0 else 0.0
            phi_eff = phi * (set_size / 6.0) * (1.0 + 0.5 * older * max(age_effect, 0.0))
            phi_eff = np.clip(phi_eff, 0.0, 1.0)

            # Global decay toward 0
            Q *= (1.0 - phi_eff)

            # RL policy (softmax)
            q_s = Q[s, :].copy()
            q_s = q_s - np.max(q_s)  # numerical stability
            exp_q = np.exp(beta * q_s)
            prl = exp_q / (np.sum(exp_q) + 1e-12)

            # Rule engagement weight
            w_rule = np.clip((rule_base * (1.0 + age_sign * age_effect)) / max(set_size, 1.0), 0.0, 1.0)

            # Rule policy
            if rule_action[s] >= 0:
                rule_vec = np.zeros(nA)
                rule_vec[rule_action[s]] = 1.0
            else:
                rule_vec = np.ones(nA) / nA

            # Lapse-augmented mixture policy
            p = (1.0 - epsilon) * (w_rule * rule_vec + (1.0 - w_rule) * prl) + epsilon * (1.0 / nA)
            p = np.clip(p, 1e-12, 1.0)
            p = p / p.sum()

            # Sample action
            a = int(np.random.choice(nA, p=p))
            simulated_actions[tr] = a

            # Generate reward
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update
            pe = r - Q[s, a]
            Q[s, a] += alpha * pe

            # Rule acquisition on reward
            if r > 0.5:
                rule_action[s] = a

    return simulated_actions, simulated_rewards