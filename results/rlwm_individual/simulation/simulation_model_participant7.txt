def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards under:
    RL with success-memory boost and age-modulated exploration bonus (UCB-like).

    Parameters:
    - stimulus: array of state identifiers per trial (can be arbitrary labels; remapped within each block)
    - blocks: array of block indices per trial
    - set_sizes: array of set sizes per trial (expected constant within a block; values like 3 or 6)
    - correct_answer: array of correct action (0,1,2) per trial
    - age: scalar or length-1 array with age in years
    - parameters: [alpha, beta, wm_success_boost, explore_bonus_base, age_explore_shift]

    Returns:
    - simulated_actions: array of simulated choices (0,1,2)
    - simulated_rewards: array of simulated rewards (0/1)
    """
    import numpy as np

    alpha, beta, wm_success_boost, explore_bonus_base, age_explore_shift = parameters
    beta = max(1e-6, float(beta)) * 5.0

    # Handle age input
    try:
        age_years = float(age[0]) if hasattr(age, "__len__") else float(age)
    except Exception:
        age_years = float(age)
    age_group = 1.0 if age_years < 45.0 else -1.0

    nA = 3
    eps = 1e-12

    n_trials = len(stimulus)
    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Iterate blocks in the order they appear
    for b in np.unique(blocks):
        block_idx = np.where(blocks == b)[0]
        block_states_raw = stimulus[block_idx]
        block_correct = correct_answer[block_idx]

        # Map arbitrary state labels within block to 0..nS-1
        unique_states = np.unique(block_states_raw)
        state_map = {s: i for i, s in enumerate(unique_states)}
        block_states = np.array([state_map[s] for s in block_states_raw], dtype=int)
        nS = len(unique_states)

        # Derive a single correct action per state (assumed stationary within block)
        correct_by_state = np.zeros(nS, dtype=int)
        for s_local in range(nS):
            idx_s = np.where(block_states == s_local)[0]
            # Take the first occurrence's correct action
            correct_by_state[s_local] = int(block_correct[idx_s[0]])

        # Initialize RL and bookkeeping
        Q = np.zeros((nS, nA), dtype=float)
        has_success = np.zeros((nS, nA), dtype=bool)
        N = np.zeros((nS, nA), dtype=float) + 1e-6  # for exploration bonus denominator

        # Exploration bonus calibrated by age and set size
        load_scale = float(nS) / 3.0  # 1 for 3, 2 for 6, etc.
        explore_bonus_mag = max(0.0, float(explore_bonus_base) + float(age_explore_shift) * age_group) * load_scale

        # Simulate trials within the block
        for i, tr in enumerate(block_idx):
            s = block_states[i]

            # Base preference from Q
            pref = Q[s, :].copy()

            # Add WM-like success boost for actions that have ever produced reward in this state
            if has_success[s, :].any():
                boost_vec = np.zeros(nA, dtype=float)
                boost_vec[has_success[s, :]] = float(wm_success_boost)
                pref += boost_vec

            # Add exploration bonus (UCB-like) for under-sampled actions
            bonus = explore_bonus_mag / np.sqrt(N[s, :] + 0.0)
            pref += bonus

            # Softmax choice
            logits = beta * (pref - np.max(pref))
            pi = np.exp(logits)
            pi /= np.sum(pi)
            # Numerical safety
            pi = np.clip(pi, eps, 1.0)
            pi /= np.sum(pi)

            a = int(np.random.choice(nA, p=pi))
            simulated_actions[tr] = a

            # Generate reward based on correct action for this state
            r = 1 if a == int(correct_by_state[s]) else 0
            simulated_rewards[tr] = r

            # Update counts and values
            N[s, a] += 1.0
            Q[s, a] += float(alpha) * (r - Q[s, a])

            if r > 0:
                has_success[s, a] = True

    return simulated_actions, simulated_rewards