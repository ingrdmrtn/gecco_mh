import numpy as np

def simulate_model(stimulus, blocks, set_sizes, correct_answer, age, parameters):
    """
    Simulate choices and rewards under the valence-asymmetric RL model with
    set-sizeâ€“dependent forgetting and choice stickiness, modulated by age.

    Parameters:
    - stimulus: array of state indices per trial (0..nS-1 within each block)
    - blocks: array of block index per trial
    - set_sizes: array of set size per trial (constant within a block; e.g., 3 or 6)
    - correct_answer: array of correct action per trial (0,1,2)
    - age: participant age (float or array-like of length 1)
    - parameters: list/tuple
        (alpha_pos, alpha_neg, beta, decay_base, stickiness, age_forget)

    Returns:
    - simulated_actions: array of simulated choices (0,1,2)
    - simulated_rewards: array of simulated binary rewards (0/1)
    """
    alpha_pos, alpha_neg, beta, decay_base, stickiness, age_forget = parameters

    # Parameter constraints as in the fitting code
    beta = max(1e-6, float(beta)) * 10.0
    alpha_pos = float(np.clip(alpha_pos, 0.0, 1.0))
    alpha_neg = float(np.clip(alpha_neg, 0.0, 1.0))
    decay_base = float(np.clip(decay_base, 0.0, 1.0))
    age_val = float(age[0]) if np.ndim(age) > 0 else float(age)
    is_older = 1.0 if age_val >= 45 else 0.0

    n_trials = len(stimulus)
    nA = 3
    prior = np.ones(nA) / nA

    simulated_actions = np.zeros(n_trials, dtype=int)
    simulated_rewards = np.zeros(n_trials, dtype=int)

    # Iterate block by block to maintain within-block state/action histories
    unique_blocks = np.unique(blocks)
    for b in unique_blocks:
        block_indices = np.where(blocks == b)[0]

        # Derive set size for this block (constant within block)
        nS = int(set_sizes[block_indices[0]])
        # Initialize Q and prev_action for each state in this block
        Q = np.ones((nS, nA)) / nA
        prev_action = -np.ones(nS, dtype=int)

        # Effective decay depends on set size and age
        set_scale = float(nS) / 6.0
        decay_eff = float(np.clip(decay_base * (1.0 + age_forget * is_older) * set_scale, 0.0, 1.0))

        # Simulate each trial in this block in temporal order
        for tr in block_indices:
            s = int(stimulus[tr])
            # Apply per-trial decay toward uniform prior for the current state's Q-values
            Q[s, :] = (1.0 - decay_eff) * Q[s, :] + decay_eff * prior

            # Choice stickiness on the previous action for this state
            stick_vec = np.zeros(nA)
            if prev_action[s] >= 0:
                stick_vec[prev_action[s]] = stickiness

            # Softmax with numerical stabilization (subtract max Q)
            logits = beta * (Q[s, :] - np.max(Q[s, :])) + stick_vec
            p = np.exp(logits)
            p = p / np.sum(p)

            # Sample an action according to softmax probabilities
            a = int(np.random.choice(nA, p=p))
            simulated_actions[tr] = a

            # Generate reward: 1 if action matches correct answer, else 0
            correct_a = int(correct_answer[tr])
            r = 1 if a == correct_a else 0
            simulated_rewards[tr] = r

            # RL update with valence asymmetry
            pe = r - Q[s, a]
            lr = alpha_pos if pe >= 0.0 else alpha_neg
            Q[s, a] += lr * pe

            # Update stickiness memory for this state
            prev_action[s] = a

    return simulated_actions, simulated_rewards