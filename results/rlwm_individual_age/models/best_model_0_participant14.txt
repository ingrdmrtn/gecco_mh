def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity limit: WM available only up to a (age-modulated) capacity K.

    Idea:
    - WM can store deterministic mappings only for up to K states in a block; beyond that,
      WM falls back toward uniform responding for those states.
    - Younger adults have higher effective capacity than older adults.
    - WM decays over time; RL provides a parallel graded value estimate across all states.
    - Final policy is a mixture of WM and RL.

    Parameters
    ----------
    states : array-like, shape (T,)
        State on each trial.
    actions : array-like, shape (T,)
        Chosen action (0..2).
    rewards : array-like, shape (T,)
        Reward (0/1).
    blocks : array-like, shape (T,)
        Block index.
    set_sizes : array-like, shape (T,)
        Set size for each trial's block (3 or 6).
    age : array-like, shape (1,)
        Participant age in years.
    model_parameters : iterable of length 5
        [lr, wm_weight, softmax_beta, wm_decay, K_param]
        - lr: RL learning rate.
        - wm_weight: mixture weight for WM.
        - softmax_beta: RL inverse temperature (scaled internally).
        - wm_decay: WM decay rate toward baseline.
        - K_param: base capacity parameter; mapped to [1,6] then adjusted by age group.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, K_param = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    is_older = 1.0 if age >= 45 else 0.0

    K_cont = 1.0 + 5.0 * (1.0 / (1.0 + np.exp(-K_param)))  # in [1,6]
    K_eff = np.clip(K_cont - 1.0 * is_older, 1.0, 6.0)

    eps_safe = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))


        seen_order = []
        wm_eligible = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            s_all = int(block_states[t])
            if s_all not in seen_order:
                seen_order.append(s_all)

                if np.sum(wm_eligible) < int(np.floor(K_eff)):
                    wm_eligible[s_all] = True


        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))



            if wm_eligible[s]:
                p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = p_wm_soft
            else:
                p_wm = 1.0 / nA  # no WM for this state

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps_safe, 1.0)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s, a] += lr * delta


            if wm_eligible[s]:

                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
                if r > 0.5:

                    learn = 0.7
                    w[s, :] = (1.0 - learn) * w[s, :]
                    w[s, a] += learn
                else:

                    pass
            else:

                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p