def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + reward-gated WM and uncertainty-based arbitration, with age-dependent noise.
    
    The model mixes RL and WM policies, where:
    - RL includes global forgetting toward uniform (captures larger set-size demands).
    - WM encodes rewarded actions and decays otherwise.
    - Arbitration weight for WM is modulated by (a) set size relative to WM capability,
      and (b) the current WM certainty (max value in W_s).
    - Older adults have lower effective inverse temperature and higher lapse.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial. States reset between blocks.
    set_sizes : array-like of int
        The set size for the block each trial belongs to (3 or 6).
    age : array-like of int or float
        Participant age; age[0] used to determine age group (>=45 is older).
    model_parameters : list or array-like of float (length 6)
        [lr, wm_weight, softmax_beta, rl_forget, wm_decay, lapse]
        lr: RL learning rate (0..1).
        wm_weight: Base arbitration weight on WM (0..1).
        softmax_beta: Base RL inverse temperature (scaled internally by 10).
        rl_forget: RL forgetting rate toward uniform each trial (0..1).
        wm_decay: WM decay/update rate (0..1).
        lapse: Base lapse probability (0..1), increased for older adults.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_decay, lapse = model_parameters

    age_val = age[0]
    is_old = age_val >= 45

    beta_scale = 0.7 if is_old else 1.0
    lapse_scale = 1.5 if is_old else 1.0

    softmax_beta = (softmax_beta * 10.0) * beta_scale
    softmax_beta_wm = 50.0  # deterministic WM
    lapse_eff_base = np.clip(lapse * lapse_scale, 0.0, 1.0)

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        size_factor = 1.0 - max(0.0, (nS - 3) / 3.0)  # 1 at nS=3, 0 at nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            wm_certainty = float(np.max(W_s))  # in [~1/nA, 1]
            wm_w_eff = np.clip(wm_weight * size_factor * wm_certainty, 0.0, 1.0)

            p_mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl

            lapse_eff = lapse_eff_base
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            delta = r - Q_s[a]
            q[s, a] += lr * delta

            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p