def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Exponential-decay Working Memory with load- and age-modulated arbitration.

    Idea:
    - RL learns Q-values via a standard delta rule.
    - WM stores a near-deterministic action for a state after rewarded feedback, but the WM trace decays
      exponentially with time since last successful storage.
    - Arbitration between WM and RL depends on set size (lower WM influence in larger sets) and age group
      (older adults have reduced WM influence).
    - WM decay rate is controlled by a time constant.

    Parameters (tuple): (lr, wm_weight, softmax_beta, wm_decay_tau, gamma)
        lr: RL learning rate (0..1).
        wm_weight: Baseline WM contribution weight (0..1).
        softmax_beta: RL inverse temperature, internally scaled by x10.
        wm_decay_tau: Time constant for WM exponential decay in trials; larger means slower decay (>0).
        gamma: Load sensitivity exponent; WM weight scales as (3/nS)^gamma.

    Returns:
        Negative log-likelihood of the observed choices.

    Age use:
        - Older (>=45) group has reduced WM influence (multiplied by 0.75).
    """
    lr, wm_weight, softmax_beta, wm_decay_tau, gamma = model_parameters
    softmax_beta *= 10.0  # beta has a higher upper bound
    softmax_beta_wm = 50.0  # very deterministic
    age_val = age[0]
    age_scale = 0.75 if age_val >= 45 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        time_since_update = np.full(nS, np.inf)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)



            dt = time_since_update[s] if np.isfinite(time_since_update[s]) else np.inf
            if np.isfinite(dt):
                d = 1.0 - np.exp(-max(dt, 0.0) / max(wm_decay_tau, 1e-6))
                w[s, :] = (1.0 - d) * w[s, :] + d * w_0[s, :]

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            load_scale = (3.0 / float(nS)) ** gamma
            eta = wm_weight * load_scale * age_scale
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            delta = r - Q_s[a]
            q[s, a] += lr * delta


            if r > 0:
                w[s, :] = 0.0
                w[s, a] = 1.0
                time_since_update[s] = 0.0
            else:

                if np.isfinite(time_since_update[s]):
                    time_since_update[s] += 1.0
                else:
                    time_since_update[s] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p