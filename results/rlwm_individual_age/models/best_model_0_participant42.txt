def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with decay, limited capacity, and state-specific perseveration (stickiness) bias.
    
    WM stores rewarded mappings and decays; capacity is reduced under larger set sizes,
    with an additional age-related capacity reduction. A state-wise perseveration bias
    (stronger in older adults) is implemented in the WM channel to capture tendency to
    repeat previous actions in a given state.
    
    Parameters
    ----------
    states : 1D array of int
        State identifiers per trial (0..nS-1 within a block).
    actions : 1D array of int
        Chosen action per trial (0..2).
    rewards : 1D array of int
        Binary feedback per trial (0 or 1).
    blocks : 1D array of int
        Block identifier per trial.
    set_sizes : 1D array of int
        Set size (3 or 6) for each trial/block.
    age : 1D array of float
        Participant age, first element used.
    model_parameters : list/tuple of floats
        [lr, wm_weight, softmax_beta, phi, k, stickiness]
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight of WM vs RL in action selection (0..1)
        - softmax_beta: inverse temperature for RL (scaled by 10 internally)
        - phi: WM decay rate (0..1)
        - k: baseline WM capacity (items)
        - stickiness: perseveration strength added to WM logits for last action in a state

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi, k_base, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = age_val >= 45

    capacity_scale = 0.6 if is_older else 1.0

    stickiness_eff = stickiness * (1.3 if is_older else 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            nS_current = int(block_set_sizes[t])
            k_eff = max(0.0, capacity_scale * k_base)
            cap_factor = 1.0 if k_eff >= nS_current else (k_eff / max(1.0, nS_current))

            wm_logits = w[s, :].copy()
            if last_action[s] >= 0:
                wm_logits[last_action[s]] += stickiness_eff

            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            p_wm = cap_factor * p_wm_soft + (1.0 - cap_factor) * (1.0 / nA)

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            delta = r - Q_s[a]
            q[s][a] += lr * delta

            w = (1.0 - phi) * w + phi * w_0
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:


                w[s, a] = max(0.0, w[s, a] - 0.5)
                w[s, :] = w[s, :] / max(w[s, :].sum(), 1e-12)
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p