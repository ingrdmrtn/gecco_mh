def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM arbitration and one-shot WM learning.
    
    Idea:
    - Decisions are a mixture of incremental RL and a fast Working Memory (WM) system.
    - WM contribution is stronger when set size is small relative to an internal capacity K,
      and is reduced for older adults.
    - WM stores the most recent outcome for the chosen action in a state (one-shot learning),
      with a fast learning rate and mild decay toward a uniform prior.
    
    Parameters (model_parameters):
    - lr: float in [0,1], RL learning rate.
    - wm_weight: float in [0,1], baseline arbitration weight for WM.
    - softmax_beta: float >= 0, inverse temperature for RL softmax (internally scaled).
    - wm_alpha: float in [0,1], WM update strength (one-shot learning toward observed outcome).
    - K: float > 0, effective WM capacity (in number of state-action mappings).
    - age_old_factor: float in [0,1], multiplicative reduction of WM weight for older adults (>=45).
    
    Inputs:
    - states: array of state indices (0..set_size-1 for each block).
    - actions: array of chosen actions (0,1,2).
    - rewards: array of rewards (0 or 1).
    - blocks: array of block indices.
    - set_sizes: array of set size for each trial (3 or 6).
    - age: array-like, first element is participant age (years).
    - model_parameters: list or array of the 6 parameters above.
    
    Returns:
    - Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_alpha, K, age_old_factor = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_old = 1.0 if age_val >= 45 else 0.0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])
            nS_now = float(block_set_sizes[t])

            Q_s = q[s, :]


            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, eps)

            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, eps)


            set_factor = min(1.0, max(0.0, K / max(nS_now, 1.0)))

            age_factor = (1.0 - is_old) + is_old * max(0.0, min(1.0, age_old_factor))
            wm_w_eff = max(0.0, min(1.0, wm_weight * set_factor * age_factor))

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            delta = r - Q_s[a]
            q[s, a] += lr * delta


            w[s, :] = (1.0 - wm_alpha) * w[s, :] + wm_alpha * w_0[s, :]

            w[s, a] = (1.0 - wm_alpha) * w[s, a] + wm_alpha * r

        blocks_log_p += log_p

    return -blocks_log_p