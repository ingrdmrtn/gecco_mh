def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RLâ€“WM arbitration by uncertainty with age bias and action perseveration.

    Arbitration weight is computed from relative uncertainty (entropy) of RL vs WM
    policies, biased by age group. WM is updated rapidly on correct trials and
    decays toward uniform. RL has a single learning rate but we use separate
    learning for positive vs negative outcomes via lr and (1-lr).

    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of int
        Binary reward per trial (0/1).
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size per trial (3 or 6).
    age : array-like of float
        Participant age; age[0] used to determine age group (<45 vs >=45).
    model_parameters : sequence of 6 floats
        lr : Base RL learning rate (used for positive prediction errors; negative uses 1-lr)
        wm_weight : Baseline arbitration bias toward WM (used as an intercept term)
        softmax_beta : Inverse temperature for RL softmax (scaled by 10 internally)
        age_bias_wm : Additive bias toward WM if younger and away if older (enters arbitration)
        perseveration : Action stickiness added to last chosen action within a state
        wm_noise : Inverse precision factor for WM policy (higher -> noisier WM)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, age_bias_wm, perseveration, wm_noise = model_parameters

    softmax_beta *= 10
    softmax_beta_wm = 50  # baseline; we will reduce by wm_noise in WM policy
    age = age[0]
    is_older = 1 if age >= 45 else 0

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):

        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        prev_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()
            if prev_action[s] >= 0:
                Q_s[prev_action[s]] += perseveration

            Qs_shift = Q_s - np.max(Q_s)
            exp_rl = np.exp(softmax_beta * Qs_shift)
            probs_rl = exp_rl / np.sum(exp_rl)
            p_rl = np.clip(probs_rl[a], eps, 1.0)


            decay = 0.05  # small fixed decay per trial
            w = (1.0 - decay) * w + decay * w_0

            W_s = w[s, :].copy()
            if prev_action[s] >= 0:

                W_s[prev_action[s]] += 0.25 * perseveration
            W_shift = W_s - np.max(W_s)
            beta_wm_eff = softmax_beta_wm / (1.0 + wm_noise)
            exp_wm = np.exp(beta_wm_eff * W_shift)
            probs_wm = exp_wm / np.sum(exp_wm)
            p_wm = np.clip(probs_wm[a], eps, 1.0)

            H_rl = -np.sum(probs_rl * np.log(np.clip(probs_rl, eps, 1.0)))
            H_wm = -np.sum(probs_wm * np.log(np.clip(probs_wm, eps, 1.0)))

            age_term = (1.0 - is_older) - is_older  # +1 if younger, -1 if older
            intercept = wm_weight  # baseline bias toward WM

            arb_input = intercept + (H_rl - H_wm) + age_bias_wm * age_term
            wm_weight_eff = 1.0 / (1.0 + np.exp(-arb_input))
            wm_weight_eff = np.clip(wm_weight_eff, 0.0, 1.0)

            wm_weight = wm_weight_eff  # use effective arbitration in the template mixture

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            delta = r - q[s][a]
            if delta >= 0:
                q[s][a] += lr * delta
            else:
                q[s][a] += (1.0 - lr) * delta


            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - lr) * w[s, :] + lr * one_hot
            else:

                w[s, a] = (1.0 - lr) * w[s, a] + lr * (1.0 / nA)

            prev_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p