def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-modulated WM weight and decay-based WM encoding.

    Idea:
    - Choice is a mixture of reinforcement learning (RL) policy and working memory (WM) policy.
    - WM is a fast, decaying associative store that can one-shot encode rewarded associations.
    - WM contribution is reduced when set size exceeds WM capacity, and by older age.
    - Younger adults (age < 45) are assumed to have stronger WM contribution than older adults.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: Baseline mixing weight of WM policy in [0,1]
    - softmax_beta: RL inverse temperature before scaling; will be scaled by 10 as in template
    - wm_decay: WM decay/encoding rate in [0,1]; also used as the encoding gain on rewarded trials
    - wm_capacity: WM capacity proxy (e.g., between 1 and 6)
    - age_mod: Age modulation factor for WM weight (e.g., between 0 and 1), reduces WM weight for older adults

    Returns:
    - Negative log-likelihood of the observed sequence of actions under the model.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    # fixed per template
    softmax_beta *= 10  # beta has a higher upper bound
    softmax_beta_wm = 50  # very deterministic
    # extra parameters
    wm_decay = model_parameters[3]
    wm_capacity = model_parameters[4]
    age_mod = model_parameters[5]

    age = age[0]
    is_older = 1 if age >= 45 else 0

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective WM weight depends on set size (capacity effect) and age
        cap_effect = min(1.0, wm_capacity / max(1, nS))
        age_effect = 1.0 - is_older * age_mod  # reduce WM for older adults
        wm_weight_eff_block = np.clip(wm_weight * cap_effect * age_effect, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability of chosen action (softmax)
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy (softmax with high inverse temperature)
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = p_wm * wm_weight_eff_block + (1 - wm_weight_eff_block) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # 1) Decay WM contents in current state toward uniform
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # 2) Encode rewarded associations one-shot: push chosen action up, others down
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty-based arbitration modulated by load and age.

    Idea:
    - Arbitration between RL and WM depends on state-wise uncertainty:
      greater RL uncertainty (high entropy) and lower WM uncertainty (low entropy) increase WM reliance.
    - WM also decays over time; encoding is fast on rewarded trials.
    - WM contribution decreases with higher set size and with older age.

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_bias: Baseline bias toward WM in arbitration (real-valued; mapped via sigmoid)
    - softmax_beta: RL inverse temperature before scaling; will be scaled by 10 as in template
    - wm_decay: WM decay/encoding rate in [0,1]
    - unc_weight: Weight on (H_RL - H_WM) in the arbitration
    - age_shift: Reduction in WM preference for older adults (additive in arbitration logit)

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_bias, softmax_beta = model_parameters[:3]
    softmax_beta *= 10
    softmax_beta_wm = 50

    wm_decay = model_parameters[3]
    unc_weight = model_parameters[4]
    age_shift = model_parameters[5]

    age = age[0]
    is_older = 1 if age >= 45 else 0

    def softmax_prob_of_choice(vals, beta, a_idx):
        return 1.0 / np.sum(np.exp(beta * (vals - vals[a_idx])))

    def entropy_from_logits(vals, beta):
        # compute entropy of softmax(beta * vals)
        logits = beta * vals
        logits = logits - np.max(logits)
        p = np.exp(logits)
        p = p / np.sum(p)
        p = np.clip(p, 1e-12, 1.0)
        return -np.sum(p * np.log(p))

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Set-size penalty for WM: larger sets reduce the effective WM bias
        setsize_penalty = np.log(max(1, nS))  # mild penalty growth with set size
        logit_offset_age = -is_older * age_shift

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM action probabilities for chosen action
            p_rl = softmax_prob_of_choice(Q_s, softmax_beta, a)
            p_wm = softmax_prob_of_choice(W_s, softmax_beta_wm, a)

            # Uncertainty-based arbitration
            H_rl = entropy_from_logits(Q_s, softmax_beta)
            H_wm = entropy_from_logits(W_s, softmax_beta_wm)
            # Arbitration logit combines baseline bias, uncertainty contrast, set-size penalty, and age
            arb_logit = wm_bias + unc_weight * (H_rl - H_wm) - setsize_penalty + logit_offset_age
            wm_weight_state = 1.0 / (1.0 + np.exp(-arb_logit))  # sigmoid to [0,1]

            p_total = wm_weight_state * p_wm + (1 - wm_weight_state) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update: decay toward uniform, then encode on reward
            w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - wm_decay) * w[s, :] + wm_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Gated WM control over RL: WM takes over when confident; otherwise RL dominates.
    Gate depends on WM confidence, set size, and age-adjusted forgetting.

    Idea:
    - WM stores one-shot rewarded associations but forgets with decay.
    - A confidence gate (difference between top-1 and top-2 WM values) determines whether WM controls choice.
    - Gate is harder to open in larger sets; older adults show greater forgetting (higher effective decay).

    Parameters (model_parameters):
    - lr: RL learning rate in [0,1]
    - wm_weight: Maximum influence of WM when gate is open in [0,1]
    - softmax_beta: RL inverse temperature before scaling; will be scaled by 10 as in template
    - wm_decay: Baseline WM decay/encoding rate in [0,1]
    - gate_threshold: Confidence threshold for WM gate in [0,1]
    - age_forget_rate: Additional decay applied if older (>=45), increasing forgetting

    Returns:
    - Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta = model_parameters[:3]
    softmax_beta *= 10
    softmax_beta_wm = 50

    wm_decay = model_parameters[3]
    gate_threshold = model_parameters[4]
    age_forget_rate = model_parameters[5]

    age = age[0]
    is_older = 1 if age >= 45 else 0

    blocks_log_p = 0
    for b in np.unique(blocks):

        block_actions = actions[blocks == b]
        block_rewards = rewards[blocks == b]
        block_states = states[blocks == b]
        block_set_sizes = set_sizes[blocks == b]

        nA = 3
        nS = block_set_sizes[0]

        q = (1 / nA) * np.ones((nS, nA))
        w = (1 / nA) * np.ones((nS, nA))
        w_0 = (1 / nA) * np.ones((nS, nA))

        # Effective decay includes an age-dependent increment if older
        eff_decay = np.clip(wm_decay + is_older * age_forget_rate, 0.0, 1.0)

        log_p = 0
        for t in range(len(block_states)):

            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Gate by WM confidence and set size
            sorted_vals = np.sort(W_s)[::-1]
            top = sorted_vals[0]
            second = sorted_vals[1] if len(sorted_vals) > 1 else 0.0
            confidence = max(0.0, top - second)  # in [0,1] approximately

            # Set-size penalty makes gate harder to open in larger sets
            setsize_penalty = np.tanh((nS - 3) / 3.0)  # 0 at set size 3, ~0.5 at 6
            gated = 1.0 if confidence > (gate_threshold + setsize_penalty * 0.2) else 0.0

            wm_weight_eff = wm_weight * gated
            p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, 1e-12, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM update with age-adjusted forgetting and one-shot encoding
            w[s, :] = (1 - eff_decay) * w[s, :] + eff_decay * w_0[s, :]
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1 - eff_decay) * w[s, :] + eff_decay * one_hot

        blocks_log_p += log_p

    return -blocks_log_p