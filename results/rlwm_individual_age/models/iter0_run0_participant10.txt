Below are three standalone cognitive models that follow the provided template structure. Each model blends reinforcement learning (RL) and working memory (WM), uses all parameters meaningfully, incorporates age group, and modulates WM contribution by set size (cognitive load). All models return the negative log-likelihood of observed choices.

Note: Assumes numpy as np is already imported.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + decaying WM mixture with age- and set-size-dependent WM weight and lapse.

    Idea:
    - Choices are a mixture of RL and WM policies.
    - WM stores recent rewarded actions per state and decays toward uniform.
    - WM is more influential for smaller set sizes and for younger adults.
    - Includes a small lapse rate for stimulus-independent random choices.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Base WM mixture weight before adjustments.
    - softmax_beta: Inverse temperature for RL softmax (internally scaled by 10).
    - wm_decay: WM decay rate toward target (0..1). Higher = faster update/decay.
    - age_bias: Additive bias to WM weight for younger vs. older (younger gets +age_bias, older gets -age_bias).
    - lapse: Lapse probability of choosing uniformly at random (0..1).

    Age:
    - Younger (<45) increases WM mixture via age_bias; older decreases.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, wm_weight, softmax_beta, wm_decay, age_bias, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM policy
    age_val = age[0]
    is_young = 1.0 if age_val < 45 else -1.0

    # helper: stable sigmoid
    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    tiny = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # compute effective WM weight (depends on age and set size)
        base = wm_weight + age_bias * is_young
        # More WM weight for smaller set sizes; scale by 3/nS (equals 1 for nS=3, 0.5 for nS=6)
        setsize_factor = 3.0 / float(nS)
        wm_mix_base = np.clip(sigmoid(base) * setsize_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, tiny)

            # WM policy: softmax on WM values
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, tiny)

            # Mixture + lapse
            mix = wm_mix_base
            p_mix = mix * p_wm + (1.0 - mix) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(max(p_total, tiny))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: move toward one-hot if rewarded, else decay toward uniform
            target = np.copy(w_0[s, :])
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
            # convex update toward target; also mild relaxation toward uniform on each step
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with dual learning rates + capacity-limited WM gating modulated by age.

    Idea:
    - RL uses separate learning rates for positive/negative prediction errors.
    - WM stores last rewarded action per state (deterministic recall), with slow decay.
    - WM engagement decreases when set size exceeds capacity K.
    - Age modulates effective WM mixture: younger increases, older decreases.

    Parameters (model_parameters):
    - alpha_pos: RL learning rate for positive prediction errors (0..1).
    - alpha_neg: RL learning rate for negative prediction errors (0..1).
    - softmax_beta: Inverse temperature for RL softmax (internally scaled by 10).
    - wm_weight_base: Base WM mixture weight before capacity and age adjustments.
    - K: WM capacity (in number of state-action pairs storable).
    - age_effect: Age modulation term added to base WM weight (+ for younger, - for older).

    Age:
    - Younger (<45) adds +age_effect to WM weight; older adds -age_effect.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    alpha_pos, alpha_neg, softmax_beta, wm_weight_base, K, age_effect = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_young = 1.0 if age_val < 45 else -1.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    tiny = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity gating: if nS > K, reduce WM weight proportionally
        cap_factor = np.clip((K - nS + 0.5) / max(K, 0.5), 0.0, 1.0)
        # Age-adjusted base weight
        base = wm_weight_base + is_young * age_effect
        wm_mix_base = np.clip(sigmoid(base) * cap_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, tiny)

            # WM policy probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, tiny)

            p_total = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            log_p += np.log(max(p_total, tiny))

            # RL update with dual learning rates
            pe = r - Q_s[a]
            alpha = alpha_pos if pe >= 0 else alpha_neg
            q[s, a] += alpha * pe

            # WM update: if rewarded, write one-hot; otherwise light decay toward uniform
            if r == 1:
                w[s, :] = 0.0
                w[s, a] = 1.0
            else:
                w[s, :] = 0.9 * w[s, :] + 0.1 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting and uncertainty-based arbitration with WM.
    
    Idea:
    - RL values forget toward uniform; uncertainty (entropy) in RL policy increases reliance on WM.
    - WM stores rewarded actions with decay; set size down-weights WM.
    - Age modulates overall WM reliance.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - softmax_beta: Inverse temperature for RL softmax (internally scaled by 10).
    - wm_weight_base: Base WM weight before arbitration/age/load.
    - rl_forget: RL forgetting rate toward uniform (0..1) per trial for the visited state.
    - arb_sensitivity: Strength of uncertainty-based arbitration (scales entropy contribution to WM mix).
    - age_effect: Age modulation term for WM weight (+ for younger, - for older).

    Age:
    - Younger (<45) adds +age_effect to WM weight; older adds -age_effect.

    Returns:
    - Negative log-likelihood of observed actions.
    """
    lr, softmax_beta, wm_weight_base, rl_forget, arb_sensitivity, age_effect = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_young = 1.0 if age_val < 45 else -1.0

    def sigmoid(x):
        return 1.0 / (1.0 + np.exp(-x))

    tiny = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Base WM mix with age and set size
        base = wm_weight_base + is_young * age_effect
        setsize_factor = 3.0 / float(nS)
        base_mix = np.clip(sigmoid(base) * setsize_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, tiny)

            # RL uncertainty (entropy of RL softmax)
            probs = np.exp(softmax_beta * (Q_s - np.max(Q_s)))
            probs = probs / max(np.sum(probs), tiny)
            entropy = -np.sum(probs * np.log(np.clip(probs, tiny, 1.0))) / np.log(nA)  # normalized 0..1

            # WM policy probability
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, tiny)

            # Arbitration: increase WM weight when RL is uncertain
            arb = np.clip(arb_sensitivity * entropy, 0.0, 1.0)
            wm_mix = np.clip(base_mix * arb + base_mix * (1.0 - arb) * 0.5, 0.0, 1.0)
            # The above keeps some base WM contribution even when entropy is low, but boosts it when high.

            p_total = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            log_p += np.log(max(p_total, tiny))

            # RL update with forgetting toward uniform each time state is visited
            pe = r - Q_s[a]
            q[s, a] += lr * pe
            q[s, :] = (1.0 - rl_forget) * q[s, :] + rl_forget * (1.0 / nA)

            # WM update: decay toward uniform; pull toward one-hot if rewarded
            target = np.copy(w_0[s, :])
            if r == 1:
                target = np.zeros(nA)
                target[a] = 1.0
            # modest decay/update
            w[s, :] = 0.85 * w[s, :] + 0.15 * target

        blocks_log_p += log_p

    return -blocks_log_p