Here are three standalone cognitive models that combine reinforcement learning (RL) with a working-memory (WM) component and incorporate age-group effects and set-size-dependent load. Each function returns the negative log-likelihood of the observed choices.

Note: Assume numpy as np is already imported by the caller.

def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-gated WM access and decay-based WM maintenance.
    - RL: single learning rate, softmax choice.
    - WM: decaying, reward-gated encoding of the last correct action per state,
          softmax choice with high inverse temperature.
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl,
      where wm_weight_eff is down/up-weighted by a capacity gate based on set size and age group.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1 within block).
    actions : array-like, int
        Chosen action (0..2).
    rewards : array-like, {0,1}
        Binary feedback.
    blocks : array-like, int
        Block index per trial (states do not repeat across blocks).
    set_sizes : array-like, int
        Set size for the trial's block (3 or 6).
    age : array-like or scalar
        Participant age in years. Age >= 45 = older group, else younger.
    model_parameters : sequence of 6 floats
        lr            : RL learning rate in [0,1].
        wm_weight     : Baseline WM mixture weight in [0,1].
        softmax_beta  : RL inverse temperature (will be scaled internally).
        phi_decay     : WM decay/encoding rate in [0,1].
        K_capacity    : WM capacity (effective states reliably maintained).
        age_bias      : Capacity bias added for younger and subtracted for older.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi_decay, K_capacity, age_bias = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # near-deterministic WM policy

    # Determine age group and age-modulated capacity
    if np.ndim(age) > 0:
        age_val = age[0]
    else:
        age_val = age
    is_older = age_val >= 45

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjusted effective capacity
        K_eff = K_capacity - age_bias if is_older else K_capacity + age_bias
        K_eff = np.clip(K_eff, 1.0, 6.0)

        # Compute capacity gate for this block's set size
        SS = float(nS)
        # Smooth capacity gate: larger when SS << K_eff, smaller when SS >> K_eff
        gate = 1.0 / (1.0 + np.exp(SS - K_eff))
        wm_weight_eff = np.clip(wm_weight * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over decaying memory weights
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with decay and reward-gated encoding
            if r > 0.5:
                # Encode rewarded action toward a one-hot trace
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * one_hot
            else:
                # No encoding; decay toward baseline uncertainty
                w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM (Win-Stay/Lose-Shift flavored) with age-scaled WM weighting and WM forgetting.
    - RL: single learning rate, softmax choice.
    - WM: encodes last rewarded action as a strong attractor (win-stay),
          and shifts weight away from last action after loss (lose-shift).
          Maintains a decaying trace per state.
    - Mixture: p_total = wm_weight_eff * p_wm + (1 - wm_weight_eff) * p_rl,
      where wm_weight_eff is age- and set-size-modulated.

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1 within block).
    actions : array-like, int
        Chosen action (0..2).
    rewards : array-like, {0,1}
        Binary feedback.
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size for the trial's block (3 or 6).
    age : array-like or scalar
        Participant age in years. Age >= 45 = older group.
    model_parameters : sequence of 6 floats
        lr            : RL learning rate in [0,1].
        wm_weight     : Baseline WM mixture weight in [0,1].
        softmax_beta  : RL inverse temperature (scaled internally).
        tau_wsls      : WM WSLS strength in [0,1] (how strongly to encode win and to shift after loss).
        phi_decay     : WM forgetting rate in [0,1].
        age_scale     : Age scaling factor in [0,1]; increases WM weight if younger, decreases if older.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, tau_wsls, phi_decay, age_scale = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    # Determine age group and compute age-scaled mixture weight
    if np.ndim(age) > 0:
        age_val = age[0]
    else:
        age_val = age
    is_older = age_val >= 45

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age scaling of WM mixture weight
        if is_older:
            wm_age = wm_weight * (1.0 - age_scale)
        else:
            wm_age = wm_weight * (1.0 + age_scale)
        wm_age = np.clip(wm_age, 0.0, 1.0)

        # Set-size gate (WM less influential at larger set sizes)
        SS = float(nS)
        gate = 1.0 / (1.0 + np.exp(SS - 4.5))  # midpoint between 3 and 6
        wm_weight_eff = np.clip(wm_age * gate, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM WSLS policy via softmax over w[s]
            W_s = w[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update with WSLS flavor and forgetting
            # Start with decay toward baseline
            w[s, :] = (1.0 - phi_decay) * w[s, :] + phi_decay * w_0[s, :]

            if r > 0.5:
                # Win-stay: push weight toward the chosen action
                incr = np.zeros(nA)
                incr[a] = 1.0
                w[s, :] = (1.0 - tau_wsls) * w[s, :] + tau_wsls * incr
            else:
                # Lose-shift: reduce chosen action's weight, redistribute to others
                decr = np.ones(nA) / (nA - 1)
                decr[a] = 0.0
                w[s, :] = (1.0 - tau_wsls) * w[s, :] + tau_wsls * decr

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with valence-asymmetric learning and WM with set-size-dependent interference noise.
    - RL: separate learning rates for positive and negative outcomes.
    - WM: encodes rewarded action as one-hot; representation is corrupted by interference
          that increases with set size and more so in older adults.
    - Mixture: p_total = wm_weight * p_wm(noisy) + (1 - wm_weight) * p_rl.
    - Age: also scales RL temperature (older -> lower beta via age_temp_scale).

    Parameters
    ----------
    states : array-like, int
        State index on each trial (0..nS-1 within block).
    actions : array-like, int
        Chosen action (0..2).
    rewards : array-like, {0,1}
        Binary feedback.
    blocks : array-like, int
        Block index per trial.
    set_sizes : array-like, int
        Set size for the trial's block (3 or 6).
    age : array-like or scalar
        Participant age in years. Age >= 45 = older group.
    model_parameters : sequence of 6 floats
        alpha_pos       : RL learning rate after reward in [0,1].
        alpha_neg       : RL learning rate after no reward in [0,1].
        wm_weight       : WM mixture weight in [0,1].
        softmax_beta    : Base RL inverse temperature (scaled internally).
        sigma_noise     : Max WM interference noise level in [0,1].
        age_temp_scale  : Temperature scaling in [0,1]; reduces beta if older, increases if younger.

    Returns
    -------
    nll : float
        Negative log-likelihood of observed choices under the model.
    """
    alpha_pos, alpha_neg, wm_weight, softmax_beta, sigma_noise, age_temp_scale = model_parameters
    # Determine age group and age-scaled temperature
    if np.ndim(age) > 0:
        age_val = age[0]
    else:
        age_val = age
    is_older = age_val >= 45

    # Age scaling: older -> lower beta, younger -> higher beta
    if is_older:
        softmax_beta = softmax_beta * (1.0 - age_temp_scale)
    else:
        softmax_beta = softmax_beta * (1.0 + age_temp_scale)
    softmax_beta = max(softmax_beta, 1e-3) * 10.0

    softmax_beta_wm = 50.0
    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size dependent interference scaling (0 at SS=3, ~1 at SS=6)
        SS = float(nS)
        g = (SS - 3.0) / (6.0 - 3.0)  # 0 for 3, 1 for 6
        g = np.clip(g, 0.0, 1.0)

        # Older adults experience more WM interference
        if is_older:
            noise = np.clip(sigma_noise * (g + 0.2), 0.0, 1.0)
        else:
            noise = np.clip(sigma_noise * g, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy with interference noise: convex combination with uniform
            W_s_clean = w[s, :]
            W_s_noisy = (1.0 - noise) * W_s_clean + noise * w_0[s, :]
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s_noisy - W_s_noisy[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with valence-asymmetric learning
            if r > 0.5:
                q[s, a] += alpha_pos * (1.0 - Q_s[a])
            else:
                q[s, a] += alpha_neg * (0.0 - Q_s[a])

            # WM update: store rewarded action strongly; otherwise drift to baseline
            if r > 0.5:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # After no reward, revert partially to uncertainty (encodes that memory is uninformative)
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p