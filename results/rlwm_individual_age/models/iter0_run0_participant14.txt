def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with decay and age-/load-driven WM lapses.

    Idea:
    - Choices are generated by a mixture of reinforcement learning (RL) and a fast, capacity-limited
      working memory (WM) system.
    - WM acts like a one-shot learner with decay toward baseline and is more lapse-prone under
      higher load (set size = 6) and in older adults.
    - Younger adults (age < 45) are assumed to have fewer WM lapses than older adults, especially under low load.

    Parameters
    ----------
    states : array-like, shape (T,)
        State index on each trial (0..nS-1 for block).
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Binary reward on each trial (0/1).
    blocks : array-like, shape (T,)
        Block index for each trial.
    set_sizes : array-like, shape (T,)
        Set size for each trial's block (3 or 6).
    age : array-like, shape (1,)
        Participant age in years.
    model_parameters : iterable of length 6
        [lr, wm_weight, softmax_beta, wm_decay, wm_learn, lapse_base]
        - lr: RL learning rate (0..1).
        - wm_weight: mixture weight for WM contribution (0..1).
        - softmax_beta: RL softmax inverse temperature (scaled internally).
        - wm_decay: WM decay rate toward baseline each trial (0..1).
        - wm_learn: WM learning strength on rewarded trials (0..1).
        - lapse_base: baseline WM lapse log-odds, modulated by set size and age group.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_learn, lapse_base = model_parameters
    softmax_beta *= 10.0  # higher dynamic range
    softmax_beta_wm = 50.0  # highly deterministic WM policy
    age = age[0]
    is_older = 1.0 if age >= 45 else 0.0

    eps_safe = 1e-12
    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action a
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: softmax over WM values with a lapse that increases with set size and age
            # Lapse epsilon computed via logistic of base + load term + age group term
            load_term = (nS - 3.0)  # 0 for set size 3, 3 for set size 6 (scaled)
            lapse_logit = lapse_base + 0.8 * load_term + 0.5 * is_older
            eps_lapse = 1.0 / (1.0 + np.exp(-lapse_logit))
            # Deterministic WM softmax
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            # Mix with uniform under lapse
            p_wm = (1.0 - eps_lapse) * p_wm_soft + eps_lapse * (1.0 / nA)

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps_safe, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward baseline each trial for this state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
            # Rewarded trials "imprint" the chosen action (one-shot-like learning)
            if r > 0.5:
                # Move distribution toward one-hot on chosen action
                w[s, :] = (1.0 - wm_learn) * w[s, :]
                w[s, a] += wm_learn

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with PE-gated WM access and age-modulated gating; WM decays and learns from outcomes.

    Idea:
    - WM access is gated by a combination of recent unsigned prediction error (PE) and age.
      Larger PE and younger age increase WM gating (more reliance on WM), especially helpful
      when outcomes provide clear information.
    - WM stores action associations with decay and is updated on both rewarded and unrewarded trials.
      Unrewarded trials reduce the WM value of the chosen action (learn-to-avoid).
    - Mixture of RL and WM policies forms the final choice probability.

    Parameters
    ----------
    states : array-like, shape (T,)
        State index on each trial.
    actions : array-like, shape (T,)
        Chosen action on each trial (0..2).
    rewards : array-like, shape (T,)
        Reward on each trial (0/1).
    blocks : array-like, shape (T,)
        Block index.
    set_sizes : array-like, shape (T,)
        Set size for the block (3 or 6).
    age : array-like, shape (1,)
        Participant age in years.
    model_parameters : iterable of length 5
        [lr, wm_weight, softmax_beta, wm_decay, gate_pe]
        - lr: RL learning rate.
        - wm_weight: base WM mixture weight (subject-level).
        - softmax_beta: RL softmax inverse temperature (scaled internally).
        - wm_decay: WM decay rate toward baseline.
        - gate_pe: sensitivity of WM gating to unsigned prediction error (>=0).

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, gate_pe = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    is_older = 1.0 if age >= 45 else 0.0

    eps_safe = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Keep a per-state running estimate of recent unsigned PE for gating
        pe_trace = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM softmax (deterministic)
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Compute a dynamic WM gate from recent unsigned PE and age
            # Larger PE and younger age -> more WM access.
            # We map gate to a lapse-like quantity then to an effective WM probability.
            # gate_raw in R -> gate in [0,1] via sigmoid
            load_term = (nS - 3.0)  # greater load makes WM access harder
            gate_bias = -0.5 - 0.6 * is_older - 0.3 * load_term  # older and higher load reduce gate
            gate_raw = gate_bias + gate_pe * pe_trace[s]
            gate = 1.0 / (1.0 + np.exp(-gate_raw))  # 0..1

            # Interpret gate as probability of using WM softmax vs uniform fallback
            p_wm = gate * p_wm_soft + (1.0 - gate) * (1.0 / nA)

            p_total = p_wm * wm_weight + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps_safe, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # Decay toward baseline each trial for the current state
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            if r > 0.5:
                # Strengthen chosen action toward one-hot
                strengthen = 0.8  # fixed strong push toward chosen item
                w[s, :] = (1.0 - strengthen) * w[s, :]
                w[s, a] += strengthen
            else:
                # Learn-to-avoid: reduce chosen action's WM value slightly relative to others
                weaken = 0.2
                take = min(weaken, w[s, a])  # ensure non-negativity
                redistribute = take / (nA - 1.0)
                w[s, a] -= take
                for aa in range(nA):
                    if aa != a:
                        w[s, aa] += redistribute

            # Update PE trace (for next trial's gating). Use mild decay to keep it recent.
            pe_trace[s] = 0.6 * pe_trace[s] + 0.4 * abs(delta)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with capacity limit: WM available only up to a (age-modulated) capacity K.

    Idea:
    - WM can store deterministic mappings only for up to K states in a block; beyond that,
      WM falls back toward uniform responding for those states.
    - Younger adults have higher effective capacity than older adults.
    - WM decays over time; RL provides a parallel graded value estimate across all states.
    - Final policy is a mixture of WM and RL.

    Parameters
    ----------
    states : array-like, shape (T,)
        State on each trial.
    actions : array-like, shape (T,)
        Chosen action (0..2).
    rewards : array-like, shape (T,)
        Reward (0/1).
    blocks : array-like, shape (T,)
        Block index.
    set_sizes : array-like, shape (T,)
        Set size for each trial's block (3 or 6).
    age : array-like, shape (1,)
        Participant age in years.
    model_parameters : iterable of length 5
        [lr, wm_weight, softmax_beta, wm_decay, K_param]
        - lr: RL learning rate.
        - wm_weight: mixture weight for WM.
        - softmax_beta: RL inverse temperature (scaled internally).
        - wm_decay: WM decay rate toward baseline.
        - K_param: base capacity parameter; mapped to [1,6] then adjusted by age group.

    Returns
    -------
    nll : float
        Negative log-likelihood of the observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, K_param = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    is_older = 1.0 if age >= 45 else 0.0

    # Map K_param to [1,6] via a sigmoid transform and add an age effect (older -> lower K)
    K_cont = 1.0 + 5.0 * (1.0 / (1.0 + np.exp(-K_param)))  # in [1,6]
    K_eff = np.clip(K_cont - 1.0 * is_older, 1.0, 6.0)

    eps_safe = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Determine which states are assigned WM slots: simple heuristic by first-come, first-served
        # We'll track the first time each state appears; up to K_eff states will be WM-eligible.
        seen_order = []
        wm_eligible = np.zeros(nS, dtype=bool)

        log_p = 0.0
        for t in range(len(block_states)):
            s_all = int(block_states[t])
            if s_all not in seen_order:
                seen_order.append(s_all)
                # Assign eligibility if capacity not exceeded
                if np.sum(wm_eligible) < int(np.floor(K_eff)):
                    wm_eligible[s_all] = True

        # Now run the block again to compute likelihood with eligibility fixed for the block
        # Re-initialize values to ensure proper simulation from block start
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))

        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # [FILL IN THE POLICY FOR THE WORKING MEMORY]
            # WM policy: if state is WM-eligible, use deterministic softmax over WM values;
            # otherwise, WM contributes only uniform policy (no memory available).
            if wm_eligible[s]:
                p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
                p_wm = p_wm_soft
            else:
                p_wm = 1.0 / nA  # no WM for this state

            p_total = p_wm * wm_weight + (1 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps_safe, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # [FILL IN THE VALUE UPDATING FOR THE WORKING MEMORY]
            # WM updates only if the state is WM-eligible; otherwise, let it decay toward uniform
            if wm_eligible[s]:
                # Decay toward baseline slightly each trial to reflect maintenance cost
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]
                if r > 0.5:
                    # On reward, imprint chosen action (one-shot like)
                    learn = 0.7
                    w[s, :] = (1.0 - learn) * w[s, :]
                    w[s, a] += learn
                else:
                    # On no reward, mild decay only (do not reinforce incorrect mapping)
                    pass
            else:
                # For ineligible states, maintain near-uniform (decay back each opportunity)
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p