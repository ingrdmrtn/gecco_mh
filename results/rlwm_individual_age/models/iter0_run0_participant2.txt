def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity-limited WM and decay, with age-dependent WM weighting.

    Idea:
    - Choices are a mixture of RL (Q-learning) and a fast working-memory (WM) store.
    - Effective WM contribution is scaled by set size (capacity K) and age group.
    - WM stores the last rewarded action per state with high precision and decays toward uniform.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Baseline WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled up internally).
    - K: WM capacity in number of items (e.g., around 3â€“4).
    - decay: WM decay/encoding strength (0..1). Higher -> stronger encoding and faster decay to new info.

    Inputs:
    - states, actions, rewards: trial-wise arrays
    - blocks: block index for each trial
    - set_sizes: set size per trial (3 or 6)
    - age: array-like with a single value; age[0] is used

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, K, decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM effective weight scales with capacity and age
            cap_scale = min(1.0, K / max(1, nS))
            age_scale = 1.0 if younger == 1 else 0.75
            wm_weight_eff = np.clip(wm_weight * cap_scale * age_scale, 0.0, 1.0)

            # WM policy: near-deterministic softmax over WM weights
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight_eff * p_wm + (1.0 - wm_weight_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # - Decay of current row toward uniform
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # - If rewarded, encode chosen action strongly (overwrite-like)
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # blend toward onehot with strength proportional to decay
                w[s, :] = (1.0 - decay) * w[s, :] + decay * onehot
            else:
                # If not rewarded, slightly suppress chosen action relative to uniform
                suppress = np.copy(w[s, :])
                suppress[a] *= (1.0 - 0.5 * decay)
                suppress = suppress / suppress.sum()
                w[s, :] = suppress

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with reward-gated WM encoding and set size/age-dependent WM engagement.

    Idea:
    - WM is engaged with a probability that decreases with set size and (slightly) with age.
    - WM encodes only after rewarded trials; otherwise it simply decays toward uniform.
    - WM policy is high-precision but its actual contribution is scaled on each trial by the engagement gate.

    Parameters (model_parameters):
    - lr: RL learning rate (0..1).
    - wm_weight: Baseline WM weight (0..1).
    - softmax_beta: RL inverse temperature (scaled internally).
    - gate: Baseline WM engagement factor (0..1) that is further scaled by set size and age.
    - beta_wm_base: Base WM inverse temperature prior to set size/age scaling.
    - decay: WM decay rate toward uniform (0..1).

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, gate, beta_wm_base, decay = model_parameters
    softmax_beta *= 10.0
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM softmax precision scales with set size and age
            size_scale = 3.0 / max(1.0, float(nS))  # lower precision at larger set sizes
            age_scale = 1.0 if younger == 1 else 0.7
            softmax_beta_wm = beta_wm_base * size_scale * age_scale

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Trial-wise WM engagement gate scales with set size and age
            gate_eff = np.clip(wm_weight * gate * size_scale * (1.0 if younger == 1 else 0.85), 0.0, 1.0)

            # Mixture
            p_total = gate_eff * p_wm + (1.0 - gate_eff) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            # Always decay toward uniform a bit
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]
            # Reward-gated encoding: only after correct feedback, store chosen action
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                # Strong overwrite when rewarded
                w[s, :] = (1.0 - decay) * w[s, :] + decay * onehot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with asymmetric RL learning rates and perseveration bias, and age/set-size dependent WM noise.

    Idea:
    - RL has separate learning rates for positive and negative outcomes.
    - A perseveration bias adds a transient benefit to repeating recent actions within a state.
    - WM contributes with precision that decreases with set size and more strongly for older adults.
    - WM stores rewarded actions strongly and suffers interference (extra decay) at larger set sizes.

    Parameters (model_parameters):
    - lr_pos: RL learning rate for rewards (0..1).
    - lr_neg: RL learning rate for non-rewards (0..1).
    - wm_weight: WM mixture weight (0..1).
    - softmax_beta: RL inverse temperature (scaled internally).
    - persev: Perseveration strength added to RL values (>=0).
    - noise_wm: Base WM noise (>0); effective beta_wm ~ 1/noise scaled by set size and age.

    Inputs:
    - states, actions, rewards, blocks, set_sizes, age

    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, persev, noise_wm = model_parameters
    softmax_beta *= 10.0
    age_val = age[0]
    younger = 1 if age_val < 45 else 0

    eps = 1e-12
    blocks_log_p = 0.0

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # State-wise perseveration traces
        chi = np.zeros((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]
            chi_s = chi[s, :]

            # RL policy with perseveration bias
            Q_eff = Q_s + persev * chi_s
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_eff - Q_eff[a])))

            # WM precision from noise, age, and set size
            base_beta_wm = 1.0 / max(1e-6, noise_wm)
            size_scale = 3.0 / max(1.0, float(nS))
            age_scale = 1.0 if younger == 1 else 0.6
            softmax_beta_wm = base_beta_wm * size_scale * age_scale

            # WM policy
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Mixture
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = np.clip(p_total, eps, 1.0)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            if r > 0.5:
                q[s, a] += lr_pos * (r - Q_s[a])
            else:
                q[s, a] += lr_neg * (r - Q_s[a])

            # Perseveration trace update (simple decay and increment on chosen action)
            chi[s, :] *= 0.5  # fixed decay
            chi[s, a] += 1.0

            # WM update with set-size-dependent interference
            # Interference scales extra decay beyond uniform
            interference = max(0.0, (nS - 3) / 3.0)  # 0 at 3, 1 at 6
            # Age makes interference more harmful for older adults
            age_interf = interference * (0.5 if younger == 1 else 1.0)

            # Decay toward uniform due to interference
            w[s, :] = (1.0 - age_interf) * w[s, :] + age_interf * w_0[s, :]

            # If rewarded, strongly encode chosen action
            if r > 0.5:
                onehot = np.zeros(nA)
                onehot[a] = 1.0
                encode_strength = 0.8  # strong overwrite on reward
                w[s, :] = (1.0 - encode_strength) * w[s, :] + encode_strength * onehot
            else:
                # If not rewarded, small normalization to avoid collapse
                w[s, :] = w[s, :] / np.sum(w[s, :])

        blocks_log_p += log_p

    return -blocks_log_p