def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM capacity model with age-dependent WM capacity.
    
    The model mixes a model-free RL policy with a working-memory (WM) policy.
    WM is capacity-limited and more effective at small set sizes; capacity differs by age group.
    WM stores the last rewarded action for each state with decay toward a uniform prior when feedback is 0.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial. States reset between blocks.
    set_sizes : array-like of int
        The set size for the block each trial belongs to (3 or 6).
    age : array-like of int or float
        Participant age; age[0] used to determine age group (>=45 is older).
    model_parameters : list or array-like of float (length 6)
        [lr, wm_weight, softmax_beta, wm_decay, K_young, K_old]
        lr: RL learning rate (0..1).
        wm_weight: Base arbitration weight on WM (0..1).
        softmax_beta: RL inverse temperature (scaled internally by 10).
        wm_decay: WM update/decay rate toward target (0..1).
        K_young: WM capacity (items) for younger participants.
        K_old: WM capacity (items) for older participants.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, wm_decay, K_young, K_old = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM policy
    age_val = age[0]
    is_old = age_val >= 45

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM value tables
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity for this participant
        K = K_old if is_old else K_young

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy: probability of chosen action under softmax
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: softmax over WM values with high inverse temperature
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Arbitration: capacity-limited contribution of WM
            # Effective WM weight scaled by usable capacity relative to set size
            cap_factor = min(1.0, max(0.0, K / float(nS)))
            wm_w_eff = wm_weight * cap_factor

            p_total = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update
            # If rewarded: imprint chosen action; if not: decay toward uniform prior
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning + recency-based WM with set-size interference and age sensitivity.
    
    The model combines:
    - RL with separate learning rates for positive vs. negative outcomes.
    - WM that encodes the most recent chosen action per state (recency), regardless of reward,
      but suffers interference that increases with set size and is stronger for older adults.
    Arbitration between RL and WM is controlled by wm_weight.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial. States reset between blocks.
    set_sizes : array-like of int
        The set size for the block each trial belongs to (3 or 6).
    age : array-like of int or float
        Participant age; age[0] used to determine age group (>=45 is older).
    model_parameters : list or array-like of float (length 6)
        [lr_pos, lr_neg, wm_weight, softmax_beta, recency_alpha, interference]
        lr_pos: RL learning rate after reward=1 (0..1).
        lr_neg: RL learning rate after reward=0 (0..1).
        wm_weight: Base arbitration weight on WM (0..1).
        softmax_beta: RL inverse temperature (scaled internally by 10).
        recency_alpha: Strength of adding the most recent action to WM (0..1).
        interference: Baseline interference/decay in WM (0..1); grows with set size and age.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, recency_alpha, interference = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_old = age_val >= 45

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Interference scaling: greater for larger set sizes; amplified for older adults
        size_factor = max(0.0, (nS - 3) / 3.0)  # 0 at nS=3, 1 at nS=6
        age_factor = 1.5 if is_old else 1.0
        inter_eff = np.clip(interference * size_factor * age_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM policy: more "one-hot" if the recent choice is strongly encoded
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            log_p += np.log(max(p_total, eps))

            # RL update with asymmetric learning rates
            lr = lr_pos if r > 0.5 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: recency encoding with interference/decay and renormalization
            # First, apply interference decay toward uniform
            w[s, :] = (1.0 - inter_eff) * w[s, :] + inter_eff * w_0[s, :]

            # Then, add recency trace for the chosen action
            w[s, a] += recency_alpha

            # Normalize to a probability distribution to use with softmax policy
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] = w[s, :] / row_sum
            else:
                w[s, :] = w_0[s, :].copy()

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with forgetting + reward-gated WM and uncertainty-based arbitration, with age-dependent noise.
    
    The model mixes RL and WM policies, where:
    - RL includes global forgetting toward uniform (captures larger set-size demands).
    - WM encodes rewarded actions and decays otherwise.
    - Arbitration weight for WM is modulated by (a) set size relative to WM capability,
      and (b) the current WM certainty (max value in W_s).
    - Older adults have lower effective inverse temperature and higher lapse.
    
    Parameters
    ----------
    states : array-like of int
        State indices per trial within each block (0..nS-1).
    actions : array-like of int
        Chosen action indices per trial (0..2).
    rewards : array-like of {0,1}
        Binary feedback per trial.
    blocks : array-like of int
        Block index per trial. States reset between blocks.
    set_sizes : array-like of int
        The set size for the block each trial belongs to (3 or 6).
    age : array-like of int or float
        Participant age; age[0] used to determine age group (>=45 is older).
    model_parameters : list or array-like of float (length 6)
        [lr, wm_weight, softmax_beta, rl_forget, wm_decay, lapse]
        lr: RL learning rate (0..1).
        wm_weight: Base arbitration weight on WM (0..1).
        softmax_beta: Base RL inverse temperature (scaled internally by 10).
        rl_forget: RL forgetting rate toward uniform each trial (0..1).
        wm_decay: WM decay/update rate (0..1).
        lapse: Base lapse probability (0..1), increased for older adults.
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    lr, wm_weight, softmax_beta, rl_forget, wm_decay, lapse = model_parameters
    # Age-dependent noise adjustments
    age_val = age[0]
    is_old = age_val >= 45

    # Older adults: lower effective beta, higher lapse
    beta_scale = 0.7 if is_old else 1.0
    lapse_scale = 1.5 if is_old else 1.0

    softmax_beta = (softmax_beta * 10.0) * beta_scale
    softmax_beta_wm = 50.0  # deterministic WM
    lapse_eff_base = np.clip(lapse * lapse_scale, 0.0, 1.0)

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Base set-size factor for WM effectiveness (lower at nS=6)
        size_factor = 1.0 - max(0.0, (nS - 3) / 3.0)  # 1 at nS=3, 0 at nS=6

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # Apply RL forgetting globally toward uniform each trial
            q = (1.0 - rl_forget) * q + rl_forget * (1.0 / nA)

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL and WM choice probabilities for the chosen action
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_wm = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # WM certainty (max entry in W_s) modulates arbitration
            wm_certainty = float(np.max(W_s))  # in [~1/nA, 1]
            wm_w_eff = np.clip(wm_weight * size_factor * wm_certainty, 0.0, 1.0)

            p_mix = wm_w_eff * p_wm + (1.0 - wm_w_eff) * p_rl

            # Lapse to uniform, with age-dependent scaling
            lapse_eff = lapse_eff_base
            p_total = (1.0 - lapse_eff) * p_mix + lapse_eff * (1.0 / nA)

            log_p += np.log(max(p_total, eps))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update: reward-gated imprint, else decay to uniform
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target
            else:
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p