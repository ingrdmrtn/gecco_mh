def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM capacity-gated mixture with age-related WM effectiveness.

    Idea:
    - Choices arise from a mixture of a slow RL system and a fast working-memory (WM) system.
    - WM contribution is gated by set size (capacity K vs. current nS) and reduced in older adults.
    - WM stores the most recent rewarded action per state with fast one-shot strengthening and ongoing decay.
    - RL updates with a standard delta rule.
    
    Parameters (tuple): (lr, wm_weight, softmax_beta, K, wm_decay)
        lr: Scalar in (0,1], RL learning rate for Q-values.
        wm_weight: Base mixture weight of WM versus RL (0..1).
        softmax_beta: Inverse temperature for RL softmax; internally scaled by 10.
        K: WM capacity (in items); WM engagement scales with min(1, K / set_size).
        wm_decay: Per-trial decay of WM toward uniform (0..1). Also sets WM insertion strength as (1 - wm_decay).
    Returns:
        Negative log-likelihood of the observed choices.
    Age use:
        - Older group (age >= 45) has an additional multiplicative reduction of WM engagement (age_factor < 1).
    """
    lr, wm_weight, softmax_beta, K, wm_decay = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic WM
    age_val = age[0]

    # Age-specific WM effectiveness attenuation
    age_factor = 0.85 if age_val >= 45 else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability under softmax
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM chosen-action probability under (near) deterministic softmax
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Capacity-gated WM engagement, attenuated in older adults
            cap_term = min(1.0, max(0.0, K / float(nS)))
            eta = wm_weight * cap_term * age_factor
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM one-shot strengthening for rewarded action; neutral otherwise
            insert_alpha = 1.0 - wm_decay
            if insert_alpha > 0.0:
                if r > 0:
                    # push probability mass toward chosen action
                    w[s, :] = (1.0 - insert_alpha) * w[s, :]
                    w[s, a] += insert_alpha
                # ensure normalization
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with uncertainty/arbitration and perseveration bias; age reduces WM use and RL precision.

    Idea:
    - Arbitration weight for WM increases when set size is small (via capacity K) and decreases in older adults.
    - Includes action perseveration within state (stickiness) to capture habitual tendencies under load.
    - RL uses standard delta rule; WM decays and stores recent rewarded actions.
    - Older adults show lower WM engagement and slightly lower RL inverse temperature.

    Parameters (tuple): (lr, wm_weight, softmax_beta, wm_decay, K, perseveration)
        lr: RL learning rate (0..1).
        wm_weight: Base WM arbitration weight (0..1).
        softmax_beta: RL inverse temperature; internally scaled by 10 and slightly reduced if older.
        wm_decay: WM decay toward uniform (0..1).
        K: WM capacity parameter controlling set-size dependence (higher K => more WM use at larger set sizes).
        perseveration: Stickiness weight added to the previously chosen action within a state.
    Returns:
        Negative log-likelihood of the observed choices.
    Age use:
        - Older (>=45): Reduce WM engagement (age_factor < 1) and RL precision (beta_age < 1).
    """
    lr, wm_weight, softmax_beta, wm_decay, K, perseveration = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]

    age_factor = 0.8 if age_val >= 45 else 1.0      # WM attenuation
    beta_age = 0.9 if age_val >= 45 else 1.0        # RL precision attenuation
    beta_rl = base_beta * beta_age

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Track last action per state for perseveration; initialize to -1 (none)
        last_a = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_curr = int(block_set_sizes[t])  # should be constant within block

            Q_s = q[s, :].copy()

            # Add perseveration bias to RL values for the last action in this state
            if last_a[s] >= 0:
                Q_s[last_a[s]] += perseveration

            # RL chosen-action probability
            denom_rl = np.sum(np.exp(beta_rl * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM chosen-action probability
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Arbitration: WM weight decreases with set size via an exponential capacity function, and with age
            cap_term = np.exp(-max(0.0, (nS_curr - 1.0)) / max(K, 1e-6))
            eta = wm_weight * cap_term * age_factor
            eta = min(max(eta, 0.0), 1.0)

            p_total = eta * p_wm + (1.0 - eta) * p_rl
            log_p += np.log(max(p_total, 1e-12))

            # RL update
            delta = r - q[s, a]
            q[s, a] += lr * delta

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM update: strengthen rewarded chosen action
            insert_alpha = 1.0 - wm_decay
            if insert_alpha > 0.0 and r > 0:
                w[s, :] = (1.0 - insert_alpha) * w[s, :]
                w[s, a] += insert_alpha
                # normalize
                row_sum = np.sum(w[s, :])
                if row_sum > 0:
                    w[s, :] /= row_sum

            # Update last action for perseveration
            last_a[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    Asymmetric RL with win-stay WM and age-dependent lapses under load.

    Idea:
    - RL has separate learning rates for positive and negative outcomes (captures age-related sensitivity asymmetries).
    - WM operates as a win-stay store: after a rewarded choice, it strongly favors repeating that action in that state.
    - Arbitration combines WM and RL, but includes an age- and load-dependent lapse probability to uniform choice.
      Older adults exhibit higher lapses, especially at larger set sizes.
    
    Parameters (tuple): (lr_pos, lr_neg, wm_weight, softmax_beta, wm_lapse)
        lr_pos: RL learning rate for rewards = 1.
        lr_neg: RL learning rate for rewards = 0.
        wm_weight: Base WM weight in arbitration (0..1).
        softmax_beta: RL inverse temperature; internally scaled by 10.
        wm_lapse: Baseline lapse probability that scales up with set size and more so for older adults.
    Returns:
        Negative log-likelihood of the observed choices.
    Age use:
        - Older (>=45): Larger lapse scaling with set size; also reduced WM weight under high load.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, wm_lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]

    # Age-specific parameters
    lapse_age_scale = 1.5 if age_val >= 45 else 1.0  # lapses increase more in older adults
    wm_age_scale = 0.85 if age_val >= 45 else 1.0    # WM weight reduced for older adults

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track whether we have a recent rewarded action per state for win-stay
        # Represent via WM table directly (w)
        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = block_rewards[t]
            nS_curr = int(block_set_sizes[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL chosen-action probability
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / max(denom_rl, 1e-12)

            # WM chosen-action probability (win-stay preference in W_s)
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / max(denom_wm, 1e-12)

            # Load- and age-dependent lapse to uniform choice
            # Lapse increases with set size; older adults lapse more.
            lapse = wm_lapse * (nS_curr / 6.0) * lapse_age_scale
            lapse = min(max(lapse, 0.0), 0.5)  # cap to keep mixture valid and modest

            # WM arbitration weight reduced under high load and in older adults
            load_scale = 3.0 / float(nS_curr)  # more WM when set size small (3) vs large (6)
            eta = wm_weight * load_scale * wm_age_scale
            eta = min(max(eta, 0.0), 1.0)

            # Mixture with lapse: p = (1-lapse)*(eta*WM + (1-eta)*RL) + lapse*(uniform)
            p_uniform = 1.0 / 3.0
            p_mix = eta * p_wm + (1.0 - eta) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * p_uniform
            log_p += np.log(max(p_total, 1e-12))

            # RL update with asymmetric learning rates
            lr_use = lr_pos if r > 0 else lr_neg
            delta = r - Q_s[a]
            q[s, a] += lr_use * delta

            # WM decay slightly toward uniform (implicit via conservative update)
            # Win-stay: strongly store rewarded action; lose: softly move toward uniform
            if r > 0:
                # Overwrite to favor chosen action
                w[s, :] = 0.05 * w_0[s, :]  # small baseline everywhere
                w[s, a] = 0.95              # strong win-stay preference
            else:
                # After loss, reduce any extreme preference (move toward uniform)
                w[s, :] = 0.7 * w[s, :] + 0.3 * w_0[s, :]

            # Normalize to avoid drift
            row_sum = np.sum(w[s, :])
            if row_sum > 0:
                w[s, :] /= row_sum

        blocks_log_p += log_p

    return -blocks_log_p