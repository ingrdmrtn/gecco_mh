def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity- and age-adjusted WM, and WM decay.
    
    Model idea:
    - Choices are a mixture of an incremental RL system and a fast one-shot working memory (WM).
    - WM stores rewarded stimulus-action mappings and decays toward uniform.
    - WM influence is down-weighted when set size exceeds WM capacity K and for older adults.
    
    Parameters (model_parameters):
    - lr: RL learning rate (0-1)
    - wm_weight: Base weight of WM contribution in the policy mixture (0-1)
    - softmax_beta: Inverse temperature for RL action selection (scaled internally)
    - wm_decay: Per-trial decay of WM values toward uniform (0-1)
    - capacity_K: WM capacity (e.g., 1-6)
    - age_bias: How strongly age group modulates WM weight (0-1). Older: down-weight by (1 - age_bias), Younger: up-weight by (1 + age_bias)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, capacity_K, age_bias = model_parameters

    # Scale temperatures (as in template)
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # very deterministic
    
    # Determine age group
    age_val = age[0]
    is_older = age_val >= 45

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL Q-values and WM store
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-adjust WM weight
        if is_older:
            wm_weight_age = wm_weight * (1.0 - age_bias)
        else:
            wm_weight_age = wm_weight * (1.0 + age_bias)
        wm_weight_age = np.clip(wm_weight_age, 0.0, 1.0)

        # Capacity-adjust WM weight for this block
        capacity_factor = min(1.0, max(0.0, capacity_K) / max(1.0, nS))
        wm_weight_eff_block = wm_weight_age * capacity_factor

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy probability of chosen action
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy: softmax over WM weights for this state
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture policy
            p_total = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM update:
            # 1) Decay WM for this state toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # 2) Store rewarded mapping one-shot: on reward, make chosen action highly preferred
            if r > 0.5:  # reward == 1
                w[s, :] = w_0[s, :]  # reset to uniform baseline
                w[s, a] = 1.0  # mark chosen as correct

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + WM implementing win-stay/lose-shift with decay.
    
    Model idea:
    - RL uses separate learning rates for positive/negative prediction errors and a softmax policy.
    - WM implements a fast win-stay/lose-shift controller with decay; it can also avoid the last punished action.
    - WM weight scales with set size via a power law and is modulated by age group.
    - A small lapse epsilon injects uniform random choice.
    
    Parameters (model_parameters):
    - lr_pos: RL learning rate for positive prediction errors (0-1)
    - lr_neg: RL learning rate for negative prediction errors (0-1)
    - wm_weight: Base WM mixture weight (0-1)
    - softmax_beta: Inverse temperature for RL (scaled internally)
    - epsilon: Lapse rate mixed with uniform (0-0.2)
    - size_gamma: Exponent controlling how WM declines with set size: wm_eff ~ nS^(-gamma)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr_pos, lr_neg, wm_weight, softmax_beta, epsilon, size_gamma = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM

    age_val = age[0]
    is_older = age_val >= 45

    # Age modulation: older have lower decision temperature and reduced WM weight
    temp_age_factor = 0.8 if is_older else 1.0
    wm_age_factor = 0.8 if is_older else 1.0

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))  # WM preference store
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Set-size scaling of WM influence: power law
        wm_size_factor = (nS ** (-max(0.0, size_gamma))) if nS > 0 else 1.0
        wm_weight_eff_block = np.clip(wm_weight * wm_age_factor * wm_size_factor, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            beta_eff = softmax_beta * temp_age_factor
            denom_rl = np.sum(np.exp(beta_eff * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy (win-stay/lose-shift):
            # W_s holds preferences; we map them to a softmax for a probability.
            W_s = w[s, :]
            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # Mixture of WM and RL, with lapse
            p_mix = wm_weight_eff_block * p_wm + (1.0 - wm_weight_eff_block) * p_rl
            p_total = (1.0 - epsilon) * p_mix + epsilon * (1.0 / nA)
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            alpha = lr_pos if pe >= 0 else lr_neg
            q[s, a] += alpha * pe

            # WM update implementing win-stay/lose-shift, with decay toward uniform
            # Decay a bit each trial (state-specific)
            decay = 0.2  # modest built-in decay for WM
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # Win-stay: if rewarded, push chosen action preference up
            if r > 0.5:
                w[s, a] = 1.0
                other_idx = [i for i in range(nA) if i != a]
                w[s, other_idx] = 0.0
            else:
                # Lose-shift: decrease chosen action preference a bit (avoid it)
                w[s, a] = 0.0  # explicitly mark as non-preferred; others rise via normalization/decay

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL-WM arbitration by WM reliability and gating of RL learning.
    
    Model idea:
    - WM stores rewarded mappings with decay. Its reliability depends on how peaked its distribution is.
    - The mixture weight is trial-wise: wm_weight * reliability * capacity * age_factor.
    - RL learning is gated: when WM is reliable, RL updates are reduced to avoid double counting.
    
    Parameters (model_parameters):
    - lr: Base RL learning rate (0-1)
    - wm_weight: Base WM weight (0-1)
    - softmax_beta: RL inverse temperature (scaled internally)
    - wm_decay: WM decay toward uniform per trial (0-1)
    - wm_noise: Additive noise on WM values before softmax (retrieval noise; >=0)
    - age_factor: Multiplier controlling how age modulates WM reliability (0-1). Older: reliability *= (1 - age_factor); Younger: *= (1 + age_factor)
    
    Returns:
    - Negative log-likelihood of observed choices.
    """
    lr, wm_weight, softmax_beta, wm_decay, wm_noise, age_factor = model_parameters

    softmax_beta *= 10.0
    softmax_beta_wm = 50.0

    age_val = age[0]
    is_older = age_val >= 45

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age modulation of WM reliability
        if is_older:
            age_rel = (1.0 - age_factor)
        else:
            age_rel = (1.0 + age_factor)
        age_rel = np.clip(age_rel, 0.0, 2.0)  # cap to reasonable range

        # Capacity factor (simple inverse dependence on set size)
        capacity_factor = 1.0 / max(1.0, nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            # RL policy
            Q_s = q[s, :]
            denom_rl = np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))
            p_rl = 1.0 / denom_rl

            # WM policy with retrieval noise
            W_s = w[s, :].copy()
            if wm_noise > 0.0:
                # zero-mean noise; assume np.random is available (no imports added)
                noise = np.random.normal(0.0, wm_noise, size=W_s.shape)
                W_s = W_s + noise

            denom_wm = np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm = 1.0 / denom_wm

            # WM reliability from entropy of W_s (lower entropy => higher reliability)
            P_w = np.exp(softmax_beta_wm * W_s)
            P_w = P_w / np.sum(P_w)
            entropy = -np.sum(P_w * np.log(np.clip(P_w, 1e-12, 1.0)))
            max_entropy = np.log(len(P_w))
            rel = 1.0 - (entropy / max_entropy)  # in [0,1]

            # Trial-wise mixture weight
            w_mix = wm_weight * rel * capacity_factor * age_rel
            w_mix = np.clip(w_mix, 0.0, 1.0)

            p_total = w_mix * p_wm + (1.0 - w_mix) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update gated by (1 - w_mix): if WM is reliable, suppress RL learning
            pe = r - Q_s[a]
            q[s, a] += lr * (1.0 - w_mix) * pe

            # WM decay toward uniform
            w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * w_0[s, :]

            # WM storage: on reward, store the mapping deterministically
            if r > 0.5:
                w[s, :] = w_0[s, :]
                w[s, a] = 1.0

        blocks_log_p += log_p

    return -blocks_log_p