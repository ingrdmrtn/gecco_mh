def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM mixture with capacity limit and age-adjusted WM, plus forgetting and lapse.
    
    The model assumes choices arise from a mixture of:
    - Reinforcement learning (RL): state-action Q-values updated with a learning rate.
    - Working memory (WM): one-shot storage of rewarded associations with decay.
    
    Mixture weight is modulated by set size (capacity pressure) and age group (older adults
    have effectively lower WM capacity). We also include a small lapse probability.
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial (0..set_size-1 within each block).
    actions : array-like of int
        Chosen action on each trial (0,1,2).
    rewards : array-like of {0,1}
        Binary feedback following the action.
    blocks : array-like of int
        Block index for each trial. States do not carry over across blocks.
    set_sizes : array-like of int
        The set size applicable to the current block (3 or 6).
    age : array-like (length 1)
        Participant age in years.
    model_parameters : iterable of floats
        [lr, wm_weight, softmax_beta, K, wm_decay, lapse]
        - lr: RL learning rate (0..1)
        - wm_weight: base WM mixture weight (0..1)
        - softmax_beta: inverse temperature for RL policy (scaled inside)
        - K: WM capacity proxy (in items)
        - wm_decay: WM decay rate toward new target on update (0..1), and toward uniform each trial
        - lapse: lapse probability mixing in uniform random choice (0..0.2)
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices under the model.
    """
    import numpy as np

    lr, wm_weight, softmax_beta, K, wm_decay, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]
    # Age adjustment for capacity: older adults have reduced effective capacity
    # Scale penalty from 0 at 45 to 0.5 at 90 years
    age_norm = np.clip((age - 45.0) / 45.0, 0.0, 1.0)
    K_eff_factor = 1.0 - 0.5 * age_norm  # between 1.0 (<=45) and 0.5 (>=90)

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # Initialize RL and WM stores
        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))   # WM current policy per state
        w_0 = (1.0 / nA) * np.ones((nS, nA)) # Uniform prior for forgetting baseline

        # Set-size and age dependent WM mixture scaling
        cap_ratio = np.clip((K * K_eff_factor) / max(nS, 1), 0.0, 1.0)
        wm_mix_block = np.clip(wm_weight * cap_ratio, 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy softmax probability of chosen action via normalization trick
            diff = softmax_beta * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(diff))

            # WM policy: softmax over WM weights (high beta makes it near-deterministic)
            W_centered = W_s - np.max(W_s)
            wm_probs = np.exp(softmax_beta_wm * W_centered)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Mixture with lapse
            p_mix = wm_mix_block * p_wm + (1.0 - wm_mix_block) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            delta = r - Q_s[a]
            q[s, a] += lr * delta

            # WM forgetting toward uniform each trial (state-specific passive decay)
            w[s, :] = (1.0 - 0.25 * wm_decay) * w[s, :] + (0.25 * wm_decay) * w_0[s, :]

            # WM update: if rewarded, move distribution toward a one-hot at the chosen action
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = (1.0 - wm_decay) * w[s, :] + wm_decay * target
            else:
                # Mild corrective learning on errors: push away from chosen action slightly
                err_push = np.full(nA, 1.0 / (nA - 1))
                err_push[a] = 0.0
                w[s, :] = (1.0 - 0.25 * wm_decay) * w[s, :] + (0.25 * wm_decay) * err_push

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL with asymmetric learning rates + gated WM store with age- and load-dependent gating/forgetting.
    
    The model blends:
    - RL: two learning rates for positive and negative prediction errors.
    - WM: on rewarded trials, WM may store the chosen action for the current state (gated).
      WM then yields near-deterministic choices when available; WM decays with set size and age.
    
    Mixture weight is the product of:
    - wm_weight: base WM weight
    - cap_ratio: capacity-based factor 1 if set_size<=capacity, else <1
    - wm_gate: probability that WM is engaged when available
    
    Parameters
    ----------
    states : array-like of int
        State index on each trial.
    actions : array-like of int
        Chosen action on each trial (0..2).
    rewards : array-like of {0,1}
        Feedback.
    blocks : array-like of int
        Block index for each trial.
    set_sizes : array-like of int
        Set size of the block (3 or 6).
    age : array-like (length 1)
        Participant age in years.
    model_parameters : iterable of floats
        [lr_pos, lr_neg, wm_weight, softmax_beta, wm_gate, lapse]
        - lr_pos: RL learning rate for positive PE
        - lr_neg: RL learning rate for negative PE
        - wm_weight: base WM contribution (0..1)
        - softmax_beta: RL inverse temperature (scaled inside)
        - wm_gate: base probability WM is accessible/engaged when available (0..1)
        - lapse: lapse probability mixing in uniform random choice
    
    Returns
    -------
    float
        Negative log-likelihood of the observed choices.
    """
    import numpy as np

    lr_pos, lr_neg, wm_weight, softmax_beta, wm_gate, lapse = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age = age[0]

    # Age and load influence on forgetting and capacity gating (no extra parameters)
    age_norm = np.clip((age - 45.0) / 45.0, 0.0, 1.0)  # 0 at 45 or younger, up to 1 at 90+
    K_eff_factor = 1.0 - 0.4 * age_norm  # mild capacity reduction with age

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Capacity-based factor relative to a nominal capacity of 4 items
        cap_ratio = np.clip((4.0 * K_eff_factor) / max(nS, 1), 0.0, 1.0)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :]
            W_s = w[s, :]

            # RL policy
            diff = softmax_beta * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(diff))

            # WM policy
            W_centered = W_s - np.max(W_s)
            wm_probs = np.exp(softmax_beta_wm * W_centered)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Engagement and mixture
            # Forgetting increases with set size and age (heuristic, parameter-free)
            base_forget = 0.10 + 0.15 * max(0, nS - 3)  # 0.10 at set size 3, 0.25 at 6
            age_boost = 0.20 * age_norm                 # up to +0.20 for older
            f = np.clip(base_forget + age_boost, 0.0, 0.9)

            # Effective WM mixture weight
            wm_engage = np.clip(wm_gate * cap_ratio, 0.0, 1.0)
            wm_mix = np.clip(wm_weight * wm_engage, 0.0, 1.0)

            p_mix = wm_mix * p_wm + (1.0 - wm_mix) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update with asymmetric learning rates
            pe = r - Q_s[a]
            lr = lr_pos if pe >= 0.0 else lr_neg
            q[s, a] += lr * pe

            # WM forgetting toward uniform each trial
            w[s, :] = (1.0 - f) * w[s, :] + f * w_0[s, :]

            # WM gated one-shot storage on rewarded trials
            if r > 0.5:
                if np.random.rand() < wm_gate:  # stochastic store gate
                    target = np.zeros(nA)
                    target[a] = 1.0
                    # Strong overwrite on successful learning
                    w[s, :] = 0.9 * target + 0.1 * (w[s, :] - target)

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+WM with choice stickiness and age-modulated temperature; WM uses recency-weighted traces.
    
    Components:
    - RL: single learning rate, softmax temperature modulated by age (older -> lower beta).
    - Stickiness: adds a bias to repeat the last action taken in the same state.
    - WM: recency-weighted action traces updated by outcome; decays with set size and age.
    
    Mixture of RL and WM depends on set size (smaller sets favor WM) and age (older -> weaker WM).
    
    Parameters
    ----------
    states : array-like of int
        State index per trial.
    actions : array-like of int
        Chosen action per trial (0..2).
    rewards : array-like of {0,1}
        Feedback.
    blocks : array-like of int
        Block index per trial.
    set_sizes : array-like of int
        Set size (3 or 6).
    age : array-like (length 1)
        Participant age in years.
    model_parameters : iterable of floats
        [lr, wm_weight, softmax_beta, stickiness, wm_tau]
        - lr: RL learning rate (0..1)
        - wm_weight: base WM mixture weight (0..1)
        - softmax_beta: base RL inverse temperature (scaled inside)
        - stickiness: bias added to the previously chosen action in a state
        - wm_tau: WM trace retention factor (0..1), larger = slower decay
    
    Returns
    -------
    float
        Negative log-likelihood of observed choices.
    """
    import numpy as np

    lr, wm_weight, softmax_beta, stickiness, wm_tau = model_parameters
    base_beta = softmax_beta * 10.0
    softmax_beta_wm = 50.0
    age = age[0]

    # Age effect on temperature: older -> lower effective beta (more noise)
    age_norm = np.clip((age - 45.0) / 45.0, 0.0, 1.0)
    beta_age_factor = 1.0 - 0.4 * age_norm  # up to 40% reduction for older adults
    lapse = 0.02 + 0.08 * age_norm          # small age-related lapse

    blocks_log_p = 0.0
    eps = 1e-12

    for b in np.unique(blocks):
        mask = (blocks == b)
        block_actions = actions[mask]
        block_rewards = rewards[mask]
        block_states = states[mask]
        block_set_sizes = set_sizes[mask]

        nA = 3
        nS = int(block_set_sizes[0])

        q = (1.0 / nA) * np.ones((nS, nA))

        # WM traces and prior
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for stickiness (initialize to None/-1)
        last_action = -1 * np.ones(nS, dtype=int)

        # Set-size and age modulation of WM mixture and decay
        cap_ratio = np.clip(4.0 / max(nS, 1), 0.0, 1.0)  # 1.0 at size 3, ~0.67 at size 6
        wm_age = np.clip(wm_weight * (1.0 - 0.5 * age_norm), 0.0, 1.0)
        wm_mix_base = np.clip(wm_age * cap_ratio, 0.0, 1.0)

        # WM decay factor built from wm_tau, set size, and age
        # Higher nS and older age -> stronger decay toward uniform
        extra_forgetting = 0.15 * max(0, nS - 3) + 0.20 * age_norm
        decay = np.clip((1.0 - wm_tau) + extra_forgetting, 0.0, 0.95)

        log_p = 0.0
        for t in range(len(block_states)):
            a = int(block_actions[t])
            s = int(block_states[t])
            r = float(block_rewards[t])

            Q_s = q[s, :].copy()

            # Add stickiness bias to RL values for the last action in this state
            if last_action[s] >= 0:
                Q_s[last_action[s]] += stickiness

            # RL policy with age-modulated beta
            beta_eff = base_beta * beta_age_factor
            diff = beta_eff * (Q_s - Q_s[a])
            p_rl = 1.0 / np.sum(np.exp(diff))

            # WM policy from traces
            W_s = w[s, :]
            W_centered = W_s - np.max(W_s)
            wm_probs = np.exp(softmax_beta_wm * W_centered)
            wm_probs /= np.sum(wm_probs)
            p_wm = wm_probs[a]

            # Mixture and lapse
            p_mix = wm_mix_base * p_wm + (1.0 - wm_mix_base) * p_rl
            p_total = (1.0 - lapse) * p_mix + lapse * (1.0 / nA)
            log_p += np.log(np.clip(p_total, eps, 1.0))

            # RL update
            pe = r - q[s, a]
            q[s, a] += lr * pe

            # WM trace decay toward uniform
            w[s, :] = (1.0 - decay) * w[s, :] + decay * w_0[s, :]

            # WM trace update: reinforce chosen action, scaled by reward
            # Reward strengthens trace, no-reward slightly weakens it
            if r > 0.5:
                target = np.zeros(nA)
                target[a] = 1.0
                w[s, :] = 0.8 * w[s, :] + 0.2 * target
            else:
                # On errors, shift a bit away from chosen action
                weaken = np.full(nA, 1.0 / (nA - 1))
                weaken[a] = 0.0
                w[s, :] = 0.9 * w[s, :] + 0.1 * weaken

            # Update last action for stickiness
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p