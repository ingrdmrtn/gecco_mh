def cognitive_model1(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL+Capacity-limited Working Memory with age-dependent capacity and decay.
    
    This model mixes a standard RL policy with a capacity-limited Working Memory (WM) policy.
    WM stores stimulus-action mappings when rewarded, but memories decay over time and are
    capacity-limited. Older adults are assumed to have lower WM capacity.
    
    Parameters
    ----------
    states : 1D array of int
        State identifiers per trial (0..nS-1 within a block).
    actions : 1D array of int
        Chosen action per trial (0..2).
    rewards : 1D array of int
        Binary feedback per trial (0 or 1).
    blocks : 1D array of int
        Block identifier per trial.
    set_sizes : 1D array of int
        Set size (3 or 6) for each trial/block.
    age : 1D array of float
        Participant age, first element used.
    model_parameters : list/tuple of floats
        [lr, wm_weight, softmax_beta, k_young, k_old, phi]
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight of WM vs RL in action selection (0..1)
        - softmax_beta: inverse temperature for RL (scaled by 10 internally)
        - k_young: WM capacity (items) for younger group
        - k_old: WM capacity (items) for older group
        - phi: WM decay rate per trial toward uniform (0..1)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, k_young, k_old, phi = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0  # deterministic WM when item is within capacity and not decayed
    age_val = age[0]
    is_older = age_val >= 45

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM values (probabilities over actions per state)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Age-dependent WM capacity
        k = k_old if is_older else k_young
        k = max(0.0, k)  # ensure non-negative

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WORKING MEMORY POLICY:
            # Capacity factor: if set size exceeds capacity, WM policy is diluted toward uniform.
            nS_current = int(block_set_sizes[t])
            cap_factor = 1.0 if k >= nS_current else (k / max(1.0, nS_current))

            # Softmax over WM weights for state s
            W_s = w[s, :]
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))

            # Capacity-limited WM probability
            p_wm = cap_factor * p_wm_soft + (1.0 - cap_factor) * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)  # numerical guard
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WORKING MEMORY UPDATE:
            # Global decay toward uniform
            w = (1.0 - phi) * w + phi * w_0
            # Encode rewarded mapping strongly (one-shot learning)
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model2(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with decay, limited capacity, and state-specific perseveration (stickiness) bias.
    
    WM stores rewarded mappings and decays; capacity is reduced under larger set sizes,
    with an additional age-related capacity reduction. A state-wise perseveration bias
    (stronger in older adults) is implemented in the WM channel to capture tendency to
    repeat previous actions in a given state.
    
    Parameters
    ----------
    states : 1D array of int
        State identifiers per trial (0..nS-1 within a block).
    actions : 1D array of int
        Chosen action per trial (0..2).
    rewards : 1D array of int
        Binary feedback per trial (0 or 1).
    blocks : 1D array of int
        Block identifier per trial.
    set_sizes : 1D array of int
        Set size (3 or 6) for each trial/block.
    age : 1D array of float
        Participant age, first element used.
    model_parameters : list/tuple of floats
        [lr, wm_weight, softmax_beta, phi, k, stickiness]
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight of WM vs RL in action selection (0..1)
        - softmax_beta: inverse temperature for RL (scaled by 10 internally)
        - phi: WM decay rate (0..1)
        - k: baseline WM capacity (items)
        - stickiness: perseveration strength added to WM logits for last action in a state

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi, k_base, stickiness = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = age_val >= 45

    # Age-dependent effective capacity scaling (older adults reduced capacity)
    capacity_scale = 0.6 if is_older else 1.0

    # Age-dependent perseveration boost
    stickiness_eff = stickiness * (1.3 if is_older else 1.0)

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM values (probabilities over actions per state)
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))

        # Track last action per state for perseveration (initialize with -1 meaning none)
        last_action = -1 * np.ones(nS, dtype=int)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy probability of chosen action
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # WM POLICY with capacity and perseveration
            nS_current = int(block_set_sizes[t])
            k_eff = max(0.0, capacity_scale * k_base)
            cap_factor = 1.0 if k_eff >= nS_current else (k_eff / max(1.0, nS_current))

            # Build WM logits from w[s,:] and add perseveration to last action
            wm_logits = w[s, :].copy()
            if last_action[s] >= 0:
                wm_logits[last_action[s]] += stickiness_eff

            # Softmax probability of chosen action under WM logits
            p_wm_soft = 1.0 / np.sum(np.exp(softmax_beta_wm * (wm_logits - wm_logits[a])))

            # Capacity-diluted WM probability
            p_wm = cap_factor * p_wm_soft + (1.0 - cap_factor) * (1.0 / nA)

            # Mixture policy
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM UPDATE: decay, then encode outcome
            w = (1.0 - phi) * w + phi * w_0
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
            else:
                # On negative feedback, reduce weight on the chosen action (avoidance)
                # and renormalize softly by pushing slightly toward uniform
                w[s, a] = max(0.0, w[s, a] - 0.5)
                w[s, :] = w[s, :] / max(w[s, :].sum(), 1e-12)
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

            # Update last action for perseveration
            last_action[s] = a

        blocks_log_p += log_p

    return -blocks_log_p


def cognitive_model3(states, actions, rewards, blocks, set_sizes, age, model_parameters):
    """
    RL + WM with uncertainty-lapse and age-sensitive lapses under high load.
    
    WM stores mappings when rewarded, but policy is mixed with a lapse component that
    increases with set size and more strongly for older adults. WM memory strength is
    tracked via a confidence variable and decays over time.
    
    Parameters
    ----------
    states : 1D array of int
        State identifiers per trial (0..nS-1 within a block).
    actions : 1D array of int
        Chosen action per trial (0..2).
    rewards : 1D array of int
        Binary feedback per trial (0 or 1).
    blocks : 1D array of int
        Block identifier per trial.
    set_sizes : 1D array of int
        Set size (3 or 6) for each trial/block.
    age : 1D array of float
        Participant age, first element used.
    model_parameters : list/tuple of floats
        [lr, wm_weight, softmax_beta, phi, epsilon_base, age_mult]
        - lr: RL learning rate (0..1)
        - wm_weight: mixture weight of WM vs RL in action selection (0..1)
        - softmax_beta: inverse temperature for RL (scaled by 10 internally)
        - phi: WM decay on memory strength (0..1)
        - epsilon_base: baseline WM lapse probability (0..1)
        - age_mult: multiplicative factor on lapses for older adults (>0)

    Returns
    -------
    float
        Negative log-likelihood of observed choices under the model.
    """
    lr, wm_weight, softmax_beta, phi, epsilon_base, age_mult = model_parameters
    softmax_beta *= 10.0
    softmax_beta_wm = 50.0
    age_val = age[0]
    is_older = age_val >= 45

    blocks_log_p = 0.0
    for b in np.unique(blocks):
        block_mask = (blocks == b)
        block_actions = actions[block_mask]
        block_rewards = rewards[block_mask]
        block_states = states[block_mask]
        block_set_sizes = set_sizes[block_mask]

        nA = 3
        nS = int(block_set_sizes[0])

        # RL Q-values
        q = (1.0 / nA) * np.ones((nS, nA))

        # WM values and confidence per state
        w = (1.0 / nA) * np.ones((nS, nA))
        w_0 = (1.0 / nA) * np.ones((nS, nA))
        # Confidence m[s] in having a correct WM mapping for state s (0..1)
        m = np.zeros(nS)

        log_p = 0.0
        for t in range(len(block_states)):
            a = block_actions[t]
            s = block_states[t]
            r = block_rewards[t]

            # RL policy
            Q_s = q[s, :]
            p_rl = 1.0 / np.sum(np.exp(softmax_beta * (Q_s - Q_s[a])))

            # Set-size and age-dependent lapse in WM channel
            nS_current = int(block_set_sizes[t])
            load_factor = (nS_current - 3) / 3.0  # 0 at 3, 1 at 6
            eps = epsilon_base * (1.0 + load_factor)
            if is_older:
                eps *= age_mult
            eps = min(max(eps, 0.0), 1.0)

            # WM policy based on memory confidence m[s]
            # If confident, WM is sharp; if not, closer to uniform
            W_s = w[s, :]
            p_wm_sharp = 1.0 / np.sum(np.exp(softmax_beta_wm * (W_s - W_s[a])))
            p_wm_uncertain = 1.0 / nA
            p_wm_mem = m[s] * p_wm_sharp + (1.0 - m[s]) * p_wm_uncertain

            # Apply lapse within WM channel
            p_wm = (1.0 - eps) * p_wm_mem + eps * (1.0 / nA)

            # Mixture with RL
            p_total = wm_weight * p_wm + (1.0 - wm_weight) * p_rl
            p_total = max(p_total, 1e-12)
            log_p += np.log(p_total)

            # RL update
            delta = r - Q_s[a]
            q[s][a] += lr * delta

            # WM UPDATE:
            # Decay memory confidence
            m = (1.0 - phi) * m
            # Decay WM action distributions toward uniform
            w = (1.0 - phi) * w + phi * w_0

            # If rewarded, encode mapping strongly and boost confidence
            if r > 0:
                one_hot = np.zeros(nA)
                one_hot[a] = 1.0
                w[s, :] = one_hot
                # Confidence update toward 1
                m[s] = 1.0 - (1.0 - m[s]) * (1.0 - 0.8)  # fast increase
            else:
                # On error, reduce confidence for that state
                m[s] *= 0.5
                # Reduce weight on chosen action slightly
                w[s, a] = 0.0
                # Renormalize softly
                w[s, :] = w[s, :] / max(w[s, :].sum(), 1e-12)
                w[s, :] = 0.5 * w[s, :] + 0.5 * w_0[s, :]

        blocks_log_p += log_p

    return -blocks_log_p